{
  "metadata": {
    "last_updated": "2026-02-16 09:18:16",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 20,
    "total_comments": 133,
    "file_size_bytes": 125062
  },
  "items": [
    {
      "id": "1r5712p",
      "title": "Scrapling v0.4 is here - Effortless Web Scraping for the Modern Web",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/chi2m8gjjljg1.png",
      "author": "0xReaper",
      "created_utc": "2026-02-15 06:01:04",
      "score": 184,
      "num_comments": 22,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5712p/scrapling_v04_is_here_effortless_web_scraping_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5h0uzf",
          "author": "Reddit_User_Original",
          "text": "Nice job. Been familiar with your project since v0.3. It's the best of its kind as far as i can tell. I use scrapling when using curl cffi is insufficient, and i need something more powerful. How do you stay on top of the anti bot tech? Have you had to implement changes in response to any new anti bot tech recently? Thanks so much for building this tool.",
          "score": 14,
          "created_utc": "2026-02-15 07:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i40el",
              "author": "0xReaper",
              "text": "Thanks, mate. That means a lot to me. \n\nThe thing is, I have been working in the Web Scraping field for years, and since I made the library, I use it every day. So it's always under heavy testing from me; most of the time, I find issues before users report them because of that.\n\nRegarding security, before switching to Web Scraping, I spent about 8 years in the information security field, including bug hunting. So I was an ethical hacker before all of that. And I spent some time working as backend.",
              "score": 13,
              "created_utc": "2026-02-15 13:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hjjyo",
          "author": "Satobarri",
          "text": "Why can‚Äôt I decline your cookies on your page?",
          "score": 3,
          "created_utc": "2026-02-15 10:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i30l4",
              "author": "0xReaper",
              "text": "I have fixed it, thanks for pointing that out",
              "score": 8,
              "created_utc": "2026-02-15 13:12:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5hwu1b",
              "author": "0xReaper",
              "text": "Oh, I didn‚Äôt notice that. Let me have a look at it, I have just switched to zensical with this update so I might have missed something in the configuration.",
              "score": 3,
              "created_utc": "2026-02-15 12:25:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5i31e4",
                  "author": "Satobarri",
                  "text": "Thanks. Not a biggie but makes it suspicious for European visitors.",
                  "score": 3,
                  "created_utc": "2026-02-15 13:12:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jn3on",
          "author": "JerryBond106",
          "text": "Should i use some vpn for this as well, so i don't get ip banned? (I'm new to this, i read proxy is included but don't know the big picture in scraping yet, as it changes rapidly and i wasn't ready to start safely yet)",
          "score": 2,
          "created_utc": "2026-02-15 18:06:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kepjy",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-15 20:23:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5mfpfp",
                  "author": "webscraping-ModTeam",
                  "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-16 03:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kfwlh",
          "author": "24props",
          "text": "I‚Äôm currently on my phone and will review this later. I believe that for many people today, due to the widespread use of AI coding, it will be beneficial to create a skill (agentskills.io) to assist users who utilize AI for development or integration. Only because LLMs are never trained on immediate new versions of anything and have knowledge gaps/cutoffs.",
          "score": 2,
          "created_utc": "2026-02-15 20:29:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kk0cb",
              "author": "0xReaper",
              "text": "Yes, I agree, I will work on this soon. I'm just taking a well-deserved rest before working on the next version. There is a lot more to add.",
              "score": 3,
              "created_utc": "2026-02-15 20:51:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kyzi6",
          "author": "515051505150",
          "text": "One thing I‚Äôve struggled with is determining the maximum number of requests per minute I can send to a site before getting rate limited or blocked. Is there a feature within scrapling that can help automatically determine the max threshold of scrapes before a site‚Äôs counter-measures kick in?",
          "score": 2,
          "created_utc": "2026-02-15 22:07:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i11sy",
          "author": "Overall-Suit-5531",
          "text": "Interesting! Does it manage JavaScript too?",
          "score": 2,
          "created_utc": "2026-02-15 12:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i31c0",
              "author": "0xReaper",
              "text": "yup",
              "score": 2,
              "created_utc": "2026-02-15 13:12:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hchnq",
          "author": "One-Spend379",
          "text": "Great job üëç \nCan it scrap allegro. pl ?",
          "score": 1,
          "created_utc": "2026-02-15 09:17:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jynxn",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-15 19:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mfrf7",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-16 03:27:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k4fsw",
          "author": "NoN4meBoy",
          "text": "Does it handle datadome ?",
          "score": 1,
          "created_utc": "2026-02-15 19:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kdbhy",
          "author": "strasbourg69",
          "text": "Could i use this to scan for emails and phone numbers of for example plumbers, regionally targetted",
          "score": 1,
          "created_utc": "2026-02-15 20:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5krjs1",
          "author": "saadcarnot",
          "text": "Can it avoid anti bot stuff like google enterprise v3 captcha?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nfd1o",
          "author": "ChallengeEmergency11",
          "text": "How free?",
          "score": 1,
          "created_utc": "2026-02-16 08:15:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0p4j2",
      "title": "Scrape Zillow Data - 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r0p4j2/scrape_zillow_data_2026/",
      "author": "retardracer",
      "created_utc": "2026-02-10 02:56:50",
      "score": 72,
      "num_comments": 28,
      "upvote_ratio": 0.95,
      "text": "I recently accidentally found out a whitelisted way to scrape Zillow.\n\nI took the time to properly file a bug with them and let them resolve.\n\nThey told me to kick rocks, that they did not have enough information to reproduce.\n\nSo I told them I would come here and post less then what I gave them and see if other people can reproduce :)\n\nZillow has whitelisted a section of google servers. Anyone can utilize google sheets (or any google app with gapps scripting or similar), to fetch/scrape and pull into your sheets, from there you can do anything with your CSV/sheet from there.\n\nMy example was a spreadsheet I made to track houses during house hunting. Before I turned it into a cool proof of concept web app(that doesnt use Zillow)\n\nGod speed!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r0p4j2/scrape_zillow_data_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4k22gz",
          "author": "ContributionEasy6513",
          "text": "Thanks OP\n\nGotta love organisations that whitelist AWS or an entire cloudprovider completely past their firewalls.",
          "score": 27,
          "created_utc": "2026-02-10 03:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v6ljt",
              "author": "Transformand",
              "text": "  \nWas anyone able to get a whitelisted VM via GCP? I couldnt, tried a few US regions...\n\nSeems only the IPs running app scripts are whitelisted, but those are limited to 6 minute runs, few runs per day\n\n",
              "score": 2,
              "created_utc": "2026-02-11 20:50:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wgf2r",
                  "author": "retardracer",
                  "text": "How are you able to reproduce but bugcrowd couldn't?  :) and the type of details you got are exactly what bug crowd should be doing.  Zillow get a new bug vendor!",
                  "score": 1,
                  "created_utc": "2026-02-12 00:50:32",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kdmkk",
          "author": "AdministrativeHost15",
          "text": "Same trick as scraping LinkedIn via running scripts on Azure. (Microsoft owns LinkedIn).",
          "score": 22,
          "created_utc": "2026-02-10 05:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndf06",
              "author": "jinef_john",
              "text": "I don't think this works per se, I've gone and set up a script on azure but I get status 999. I'll probably try with internal endpoints but I don't see how this route is different from building a custom scraping pipeline ü§î",
              "score": 4,
              "created_utc": "2026-02-10 17:31:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4m5std",
              "author": "letopeto",
              "text": "Can you explain more? All you need is a Azure server or is there more to it?",
              "score": 3,
              "created_utc": "2026-02-10 13:58:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n3u65",
                  "author": "AdministrativeHost15",
                  "text": "PM/QA with Microsoft/LinkedIn often run scripts against LinkedIn for various purposes. Internal tools run on Azure so LinkedIn can't block the IP address ranges of the Azure data centers.",
                  "score": 5,
                  "created_utc": "2026-02-10 16:46:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nhwis",
              "author": "yauhenrachkouski",
              "text": "I tried it also via azure vm and see auth wall. When did you try it last time? Tried with bing bot UA and with real",
              "score": 2,
              "created_utc": "2026-02-10 17:51:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ot5yr",
                  "author": "Transformand",
                  "text": "what Auth wall do you mean?",
                  "score": 1,
                  "created_utc": "2026-02-10 21:29:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4jw1r3",
          "author": "letopeto",
          "text": "how do you use google sheets to do the scraping itself? a bit confused by what you mean",
          "score": 3,
          "created_utc": "2026-02-10 03:16:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jxfp0",
              "author": "retardracer",
              "text": "Google Apps Script (GAS) enables advanced automation, custom functions, and interactive features within Google Sheets using JavaScript. Accessible via Extensions > Apps Script, it automates tasks like data formatting, menu creation, and connecting to other Google services (Gmail, Drive). Key capabilities include custom functions, macros, and API interactions. ",
              "score": 13,
              "created_utc": "2026-02-10 03:24:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qpr1k",
          "author": "SharpRule4025",
          "text": "Classic case of whitelisting an entire cloud provider's IP range. The same pattern exists with LinkedIn and Azure (Microsoft owns LinkedIn, so Azure IPs get trusted differently). Companies whitelist Google/AWS/Azure at the network level for legitimate integrations and forget that anyone can spin up a VM on those same networks.\n\nThe Google Sheets approach works because the request originates from Google's infrastructure, which Zillow trusts implicitly. Same reason Apps Script can hit APIs that block datacenter IPs from other providers.\n\nNot surprised they dismissed the bug report. Fixing it would mean auditing their entire IP whitelist, which probably breaks a dozen internal integrations.",
          "score": 5,
          "created_utc": "2026-02-11 03:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l5q6l",
          "author": "copyfringe",
          "text": "Thanks for sharing, OP. If they have whitelisted all of Google, then maybe one could spin up a linux VM in GCP and run a traditional wget based scraper script from there.",
          "score": 3,
          "created_utc": "2026-02-10 09:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k73o3",
          "author": "Forward_Tackle_6487",
          "text": "frekking awesome",
          "score": 2,
          "created_utc": "2026-02-10 04:28:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k9ojf",
          "author": "Bitter_Caramel305",
          "text": "You know what, you should also inform them to hide their backend API or at least obfuscate it using protobuf or something, as it's laying in open getting used by people like me.\n\nHowever, I don't think they'll be interested in fixing this either as most companies just don't bother to care about such insignificant acts, but rather prefers to focus on shipping new features, no matter how insignificant.\n\nOn a related note, remember their dev team might be to understaff and also their devs probably have a huge backlog.",
          "score": 3,
          "created_utc": "2026-02-10 04:46:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54fwv0",
              "author": "Even-Recording-1886",
              "text": "If they hide their backend api them What is the fallback process to scrape data from it?",
              "score": 1,
              "created_utc": "2026-02-13 06:46:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54i000",
                  "author": "Bitter_Caramel305",
                  "text": "Uhh..., have you ever heard about browser automation?",
                  "score": 1,
                  "created_utc": "2026-02-13 07:04:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4knv0f",
          "author": "Fit_Temperature680",
          "text": "Thanks for the tip \n\n",
          "score": 1,
          "created_utc": "2026-02-10 06:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nh7ri",
          "author": "Even-Recording-1886",
          "text": "Don‚Äôt we need proxy?",
          "score": 1,
          "created_utc": "2026-02-10 17:48:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nj62p",
              "author": "PresidentHoaks",
              "text": "Not if youre going through a google system that whitelists Google's IP block. They do this because they want Google to index their pages so theyre the top search result for homes. They would have to get stricter about their whitelist if they wanted to solve this",
              "score": 2,
              "created_utc": "2026-02-10 17:57:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v6mcu",
          "author": "Transformand",
          "text": "  \nWas anyone able to get a whitelisted VM via GCP? I couldnt, tried a few US regions...\n\nSeems only the IPs running app scripts are whitelisted, but those are limited to 6 minute runs, few runs per day",
          "score": 1,
          "created_utc": "2026-02-11 20:50:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52i8n9",
          "author": "Ok-Taste-5844",
          "text": "I'm still running into 403, even when running it in Apps Script. I'm in Canada, could that be why?\n\nHere's my code. Thanks for help.\n\n    function testZillow() {\n    ¬† var response = UrlFetchApp.fetch(\"https://www.zillow.com/homes/for_rent/792680_rid/\", {muteHttpExceptions: true});\n    ¬† var code = response.getResponseCode();\n    ¬† var html = response.getContentText();\n    ¬† \n    ¬† Logger.log(\"Response Code: \" + code);\n    ¬† Logger.log(\"First 100 chars: \" + html.substring(0, 100));\n    ¬† Logger.log(\"Contains 'denied': \" + (html.indexOf(\"denied\") > -1));\n    ¬† Logger.log(\"Contains 'zpid': \" + (html.indexOf(\"zpid\") > -1));\n    }",
          "score": 1,
          "created_utc": "2026-02-12 23:10:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kstqw",
          "author": "error1212",
          "text": "That's interesting, thanks for sharing. I'm afraid you may be risking a lawsuit.",
          "score": -5,
          "created_utc": "2026-02-10 07:22:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5ptfe",
      "title": "I can scrape anything",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 20:57:12",
      "score": 68,
      "num_comments": 103,
      "upvote_ratio": 0.71,
      "text": "No selenium, playwright or puppeteer shit, I can scrape anything in full request mode, bypassing every bot protection. It doesn't matter if is Cloudflare, Akamai, PerimeterX etc  \nAfter years in this filed I believe it's time to give something back to the community. I'll start to release open source stuff for the people who want to learn. Let me know which one is the most interesting topic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5kqgqc",
          "author": "Raidrew",
          "text": "My body is ready",
          "score": 36,
          "created_utc": "2026-02-15 21:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kufee",
              "author": "heelstoo",
              "text": "Can you scrape me, Focker?",
              "score": 22,
              "created_utc": "2026-02-15 21:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5l5g7s",
                  "author": "emprezario",
                  "text": "You can scrape anything with nipples. ü§∑‚Äç‚ôÇÔ∏è",
                  "score": 12,
                  "created_utc": "2026-02-15 22:42:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kx6se",
          "author": "mizhgun",
          "text": "I can type 1000 characters per minute. It's just total gibberish, though.",
          "score": 17,
          "created_utc": "2026-02-15 21:58:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kytyf",
              "author": "0xMassii",
              "text": "No jokes here, do a check on me",
              "score": -8,
              "created_utc": "2026-02-15 22:07:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l2skh",
          "author": "indicava",
          "text": "Why the hell is this crap being upvoted?",
          "score": 32,
          "created_utc": "2026-02-15 22:28:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l3vi5",
              "author": "0xMassii",
              "text": "The real question is why not?",
              "score": -27,
              "created_utc": "2026-02-15 22:34:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l3e21",
          "author": "beachandbyte",
          "text": "Press X to doubt. \n\nAkami and Cloudflare inspect the TLS stack. It would be possible to spoof that but challenging. But then they pass back js that profiles the browser apis to suspicious requests and the payload in that js is constantly changing. So at least for those requests you would need to either mock the entire surface of the browser Api and return what they would consider reasonable values (at that point just using a browser is likely far easier). Won‚Äôt say it‚Äôs 100% not possible but unless your trick is proxying through some other process that is actually getting around all the protection you won‚Äôt be bypassing  cloudflare or Akami purely with requests.",
          "score": 8,
          "created_utc": "2026-02-15 22:31:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l45m1",
              "author": "0xMassii",
              "text": "Bro, bro, bro bot protection are my bitches\nI‚Äôm the man who destroy the whole ‚Ä¶‚Ä¶‚Ä¶‚Ä¶ website :)",
              "score": -16,
              "created_utc": "2026-02-15 22:35:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9umx",
                  "author": "You_Cant_Win_This",
                  "text": "ok no we know you are fraud for sure",
                  "score": 5,
                  "created_utc": "2026-02-15 23:07:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kmtz5",
          "author": "EffectiveSeat1505",
          "text": "CF",
          "score": 3,
          "created_utc": "2026-02-15 21:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kymuh",
              "author": "0xMassii",
              "text": "I have a custom solver for CF, soon will be OS, I‚Äôll start from tomorrow",
              "score": 3,
              "created_utc": "2026-02-15 22:06:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5lqmym",
                  "author": "MurkBRA",
                  "text": "Everyone who tried to make this open source received a DMCA takedown notice.",
                  "score": 3,
                  "created_utc": "2026-02-16 00:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kr2r4",
              "author": "UnlikelyLikably",
              "text": "Second that",
              "score": 1,
              "created_utc": "2026-02-15 21:27:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l632w",
          "author": "who_am_i_to_say_so",
          "text": "Your mom scraped deezenuts with her snaggletooth. Scraping must run in the family.",
          "score": 11,
          "created_utc": "2026-02-15 22:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6f8f",
              "author": "0xMassii",
              "text": "No sense but good for u ü§£",
              "score": -3,
              "created_utc": "2026-02-15 22:47:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l6jx3",
                  "author": "who_am_i_to_say_so",
                  "text": "I couldn‚Äôt resist. Haha",
                  "score": 1,
                  "created_utc": "2026-02-15 22:48:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kllaj",
          "author": "yyavuz",
          "text": "let's gooooooo",
          "score": 8,
          "created_utc": "2026-02-15 20:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kp5vy",
          "author": "PTBKoo",
          "text": "Cloudflare turnstile, I have to open a chrome browser for a every single turnstile solve and scraping over 1k daily and my poor server can‚Äôt keep up.",
          "score": 3,
          "created_utc": "2026-02-15 21:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lnyvm",
          "author": "BossDailyGaming",
          "text": "Talk is cheap, make that gh repo",
          "score": 3,
          "created_utc": "2026-02-16 00:30:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kntfi",
          "author": "askolein",
          "text": "Interested. Def the non selenium request thing. How come? Direct webassembly?",
          "score": 2,
          "created_utc": "2026-02-15 21:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpbqg",
          "author": "No-Exchange2961",
          "text": "Please post the link to the open source!",
          "score": 2,
          "created_utc": "2026-02-15 21:18:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ksxhi",
          "author": "sudbull",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 21:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyolu",
              "author": "0xMassii",
              "text": "Ez brother",
              "score": 2,
              "created_utc": "2026-02-15 22:06:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lb6q1",
          "author": "EntrepreneurSea4283",
          "text": "What's the the hardest thing to scrape",
          "score": 2,
          "created_utc": "2026-02-15 23:14:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbgki",
              "author": "0xMassii",
              "text": "It depends, for me few POST requests on Akamai gave me hard times",
              "score": 1,
              "created_utc": "2026-02-15 23:16:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lufaj",
          "author": "crawford5002",
          "text": "Teach me your ways",
          "score": 2,
          "created_utc": "2026-02-16 01:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpi7j",
          "author": "san-vicente",
          "text": "Let me know",
          "score": 1,
          "created_utc": "2026-02-15 21:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kq3i9",
          "author": "Any-Dig-3384",
          "text": "DM if legit",
          "score": 1,
          "created_utc": "2026-02-15 21:22:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kri7l",
          "author": "Ladytron2",
          "text": "Facebook events?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ks7mg",
          "author": "Srijaa",
          "text": "I‚Äôm in!",
          "score": 1,
          "created_utc": "2026-02-15 21:33:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ku6zv",
          "author": "uneatenbreakfast",
          "text": "The chosen one",
          "score": 1,
          "created_utc": "2026-02-15 21:43:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kup2g",
          "author": "puzz-User",
          "text": "Let‚Äôs go!",
          "score": 1,
          "created_utc": "2026-02-15 21:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kvsse",
          "author": "living_david_aloca",
          "text": "Well let‚Äôs see it",
          "score": 1,
          "created_utc": "2026-02-15 21:51:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kw0gx",
          "author": "Sensitive_Nobody409",
          "text": "Recaptcha v3 pls ‚ù§Ô∏èüëèüèø",
          "score": 1,
          "created_utc": "2026-02-15 21:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyx8n",
              "author": "0xMassii",
              "text": "Ez, but if you want to start from somewhere check solver online",
              "score": 1,
              "created_utc": "2026-02-15 22:07:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kw4m0",
          "author": "Sensitive_Nobody409",
          "text": "Build your own chromium?",
          "score": 1,
          "created_utc": "2026-02-15 21:53:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kx1va",
          "author": "slumdogbi",
          "text": "Amazon with sponsored products",
          "score": 1,
          "created_utc": "2026-02-15 21:57:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyy8v",
              "author": "0xMassii",
              "text": "Ez",
              "score": 1,
              "created_utc": "2026-02-15 22:07:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ld0o2",
                  "author": "slumdogbi",
                  "text": "üëèüëèüëèüëè",
                  "score": 1,
                  "created_utc": "2026-02-15 23:25:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5lh0ov",
                  "author": "RealAmerik",
                  "text": "I'm interested in this as well.",
                  "score": 1,
                  "created_utc": "2026-02-15 23:49:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kydj8",
          "author": "Overall-Suit-5531",
          "text": "Do it",
          "score": 1,
          "created_utc": "2026-02-15 22:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l0cai",
          "author": "Dorkits",
          "text": "Where",
          "score": 1,
          "created_utc": "2026-02-15 22:15:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l0f36",
              "author": "0xMassii",
              "text": "Everywhere",
              "score": 0,
              "created_utc": "2026-02-15 22:15:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l0kvi",
                  "author": "Dorkits",
                  "text": "The link bro",
                  "score": 1,
                  "created_utc": "2026-02-15 22:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l1vnf",
          "author": "sojufles",
          "text": "How not to get rate limited by Cloudflare, with a scraper in a container using 1 IP?",
          "score": 1,
          "created_utc": "2026-02-15 22:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2hlv",
              "author": "0xMassii",
              "text": "That‚Äôs is related to the website, you need to find vulns on the target or misconfig, otherwise cf will block you",
              "score": 1,
              "created_utc": "2026-02-15 22:26:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l34c6",
                  "author": "sojufles",
                  "text": "Thanks for the reply, it also seems that whenever my scraper in container is rate limited. I can access the site from my local browser. Do you have any idea or experiences why this happens?",
                  "score": 1,
                  "created_utc": "2026-02-15 22:29:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2jsb",
          "author": "Antop90",
          "text": "The problem isn't emulating the browser, there are already plenty of super effective libraries for that. The real issue is having a pool of very expensive IPv4 addresses to rotate.",
          "score": 1,
          "created_utc": "2026-02-15 22:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2xi4",
              "author": "0xMassii",
              "text": "Yeah, but you can scrape also with cheap resi, but you will be slower obv",
              "score": 1,
              "created_utc": "2026-02-15 22:28:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l3hye",
                  "author": "Antop90",
                  "text": "Sometimes speed is essential, and the only way to achieve it is with a solid proxy pool. no magical alternatives exist",
                  "score": 1,
                  "created_utc": "2026-02-15 22:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2stb",
          "author": "Eudaimonic_me",
          "text": "Google trends",
          "score": 1,
          "created_utc": "2026-02-15 22:28:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l3qzm",
          "author": "saadcarnot",
          "text": "I am working on automating an time critical workflow, however it's protected with Recaptcha v3 Enterprise. I can't afford to wait for solver to get back. I need to avoid it all together. \n\nAny suggestions for me?",
          "score": 1,
          "created_utc": "2026-02-15 22:33:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l4a1w",
              "author": "0xMassii",
              "text": "Look for vulns on the website target",
              "score": 2,
              "created_utc": "2026-02-15 22:36:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l54i8",
                  "author": "saadcarnot",
                  "text": "Couldn't find any, can you list down what to look for? Site is recreation.gov",
                  "score": 1,
                  "created_utc": "2026-02-15 22:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lawia",
          "author": "Inner_Grape_211",
          "text": "can u explain each of ur bypasses? with general tips pls?",
          "score": 1,
          "created_utc": "2026-02-15 23:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbhn3",
              "author": "0xMassii",
              "text": "I‚Äôll do",
              "score": 1,
              "created_utc": "2026-02-15 23:16:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ls78m",
          "author": "TheKillerScope",
          "text": "Can you scrape Dune or GMGN? PolyMarket? If so, I'd be VERY interested in connecting with you and discussing a possible partnership.",
          "score": 1,
          "created_utc": "2026-02-16 00:55:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lus8f",
          "author": "sunrise_zc",
          "text": "customized browserÔºü",
          "score": 1,
          "created_utc": "2026-02-16 01:12:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m1ddn",
          "author": "RaiseRuntimeError",
          "text": "Is this r/webscrapingcirclejerk",
          "score": 1,
          "created_utc": "2026-02-16 01:54:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m7qli",
          "author": "Villain_99",
          "text": "LinkedIn please",
          "score": 1,
          "created_utc": "2026-02-16 02:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mas5b",
          "author": "G_S_7_wiz",
          "text": "How do you get all amzon reviews..you need cookies for all reviews",
          "score": 1,
          "created_utc": "2026-02-16 02:54:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mb88o",
          "author": "LifeShmucksSoMuch",
          "text": "can you bypass shape?",
          "score": 1,
          "created_utc": "2026-02-16 02:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l60ag",
          "author": "DotEnvironmental4718",
          "text": "Can you scrape my butt? It‚Äôs itching",
          "score": 1,
          "created_utc": "2026-02-15 22:45:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6atr",
              "author": "0xMassii",
              "text": "Crazy statement",
              "score": 2,
              "created_utc": "2026-02-15 22:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l9cbf",
          "author": "tradegreek",
          "text": "How do you scrape stuff which requires constantly refreshed cookies (say every 5-10 mins) without using a tool like selenium to obtain the new cookies?",
          "score": 1,
          "created_utc": "2026-02-15 23:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l9ki9",
              "author": "0xMassii",
              "text": "Maybe i don‚Äôt need cookies either to get the data :)",
              "score": 2,
              "created_utc": "2026-02-15 23:05:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9n2v",
                  "author": "tradegreek",
                  "text": "I don‚Äôt get how is that possible?",
                  "score": 0,
                  "created_utc": "2026-02-15 23:05:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5la1z9",
                  "author": "You_Cant_Win_This",
                  "text": "You do though",
                  "score": 0,
                  "created_utc": "2026-02-15 23:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lg4cf",
          "author": "Afriendlywhiteguy",
          "text": "Teach me your ways",
          "score": 1,
          "created_utc": "2026-02-15 23:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lgsti",
          "author": "dca12345",
          "text": "What‚Äôs wrong with PlayWright?",
          "score": 1,
          "created_utc": "2026-02-15 23:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l1e5u",
          "author": "hulleyrob",
          "text": "Docker support?",
          "score": 0,
          "created_utc": "2026-02-15 22:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2be9",
              "author": "0xMassii",
              "text": "We don‚Äôt need, but we can",
              "score": 2,
              "created_utc": "2026-02-15 22:25:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l2s5l",
                  "author": "hulleyrob",
                  "text": "Could be very interesting if it will still beat all the tech in a docker container I don‚Äôt think there is anything out there that can do that right now.",
                  "score": 0,
                  "created_utc": "2026-02-15 22:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l790o",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 0,
          "created_utc": "2026-02-15 22:52:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7aws",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn seems to be the most difficult",
          "score": 0,
          "created_utc": "2026-02-15 22:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ldt2m",
          "author": "TillOk5563",
          "text": "The ServiceNow variables fields?",
          "score": 0,
          "created_utc": "2026-02-15 23:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5liydm",
          "author": "oizysplutus",
          "text": "Enterprise hcap",
          "score": 0,
          "created_utc": "2026-02-16 00:01:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lk4i8",
          "author": "tuttipazzo",
          "text": "Ok.  You got me interested as well.",
          "score": 0,
          "created_utc": "2026-02-16 00:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ll7qd",
          "author": "Sensitive-Finger-404",
          "text": "facebook marketplace",
          "score": 0,
          "created_utc": "2026-02-16 00:14:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lmb1u",
          "author": "bgj556",
          "text": "‚Ä¶ yes please",
          "score": 0,
          "created_utc": "2026-02-16 00:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7598",
          "author": "AdhesivenessEven7287",
          "text": "Reddit",
          "score": -1,
          "created_utc": "2026-02-15 22:51:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1tmm1",
      "title": "Web scraping sandbox website - scrapingsandbox.com",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r1tmm1/web_scraping_sandbox_website_scrapingsandboxcom/",
      "author": "vickyrathee",
      "created_utc": "2026-02-11 10:14:57",
      "score": 30,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "Hey guys,\n\nI have published a scraping sandbox website- [scrapingsandbox.com](https://scrapingsandbox.com/) to learn and practice web scraping using Playwright, Puppeteer etc.\n\nFor now, I just kept 500 products in e-commerce styled list, filter, pagination and details page to practicing product website scraping, later I will be adding more pages for different scenarios.\n\nIt's [open source on Github](https://github.com/Agenty/scrapingsandbox), please let me know if have have any suggestion or feedback.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r1tmm1/web_scraping_sandbox_website_scrapingsandboxcom/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4tovor",
          "author": "sakozzy",
          "text": "This is cool man! One suggestion: add a few ‚Äúreal world pain‚Äù scenarios people actually run into. Stuff like lazy loading/infinite scroll, rate limits, basic anti-bot behavior, and messy HTML. That‚Äôs where most beginners get stuck",
          "score": 10,
          "created_utc": "2026-02-11 16:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xj4rs",
              "author": "vickyrathee",
              "text": "Sure, I will add infinite scroll today and other example in upcoming week. PR welcome if you want to contribute. [https://github.com/Agenty/scrapingsandbox](https://github.com/Agenty/scrapingsandbox) ",
              "score": 6,
              "created_utc": "2026-02-12 04:57:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o55viwg",
              "author": "Gold_Emphasis1325",
              "text": "You can introduce simulated poor network conditions (latency, bandwidth, drops) on the client side.",
              "score": 1,
              "created_utc": "2026-02-13 13:46:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4s0ckj",
          "author": "lp435",
          "text": "Cool idea. How does it work? Have you implemented challenges?",
          "score": 1,
          "created_utc": "2026-02-11 10:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s0xu2",
              "author": "vickyrathee",
              "text": "it's hosed on Cloudflare workers to build scraping agents with any language you want. No copy right claim, legal issue. Learn, make videos, blog post etc.",
              "score": 1,
              "created_utc": "2026-02-11 10:44:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5hpe58",
                  "author": "navid_ta",
                  "text": "One of my major challenges was dealing with Cloudflare verification. Using common scraping libraries and headless browsers, is it possible to practice on this service in order to understand how Cloudflare verification behaves, or not?\nWhen I accessed it myself, no verification challenge was triggered.",
                  "score": 1,
                  "created_utc": "2026-02-15 11:21:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tveec",
          "author": "AdministrativeHost15",
          "text": "Consider giving a prize to the user who can scrape the highly protected secret word.",
          "score": 1,
          "created_utc": "2026-02-11 17:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xizyf",
              "author": "vickyrathee",
              "text": "Who will pay for the prize? :-)",
              "score": 2,
              "created_utc": "2026-02-12 04:56:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xugxs",
                  "author": "AdministrativeHost15",
                  "text": "Prize can just be bragging rights that will help you get freelance gigs scraping highly protected sites.",
                  "score": 1,
                  "created_utc": "2026-02-12 06:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u80ee",
          "author": "Gold_Emphasis1325",
          "text": "Legal / TOS issues with provider? Otherwise, it would be nice to have a practice area for Captcha, and the \"click this box\" to prove you're human. Either way, it's all just an arms race between the scrapers and the scrape-ees....",
          "score": 1,
          "created_utc": "2026-02-11 18:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjdx6",
              "author": "vickyrathee",
              "text": "Adding captcha etc on sandbox will be a TOS issue and might risk our account, so will add general examples for learning instead like infinit scroll, form submission, pagination, rate limit etc.",
              "score": 1,
              "created_utc": "2026-02-12 04:59:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o55vdcd",
                  "author": "Gold_Emphasis1325",
                  "text": "Yeah and the TOS are changing constantly. Allowed / gray area today and violation and possibly legal action later...",
                  "score": 1,
                  "created_utc": "2026-02-13 13:45:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4x17cu",
          "author": "Objective-Fun-4533",
          "text": "Cool. Like the UI",
          "score": 1,
          "created_utc": "2026-02-12 02:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjfig",
              "author": "vickyrathee",
              "text": "thanks to Claud Opus 4.6",
              "score": 1,
              "created_utc": "2026-02-12 04:59:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y8su5",
          "author": "Nel549",
          "text": "Add cloudfare to it or on specific pages",
          "score": 1,
          "created_utc": "2026-02-12 08:46:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yim0a",
          "author": "maxim-kulgin",
          "text": "add products variations? for example like size/color? Thanks man! ",
          "score": 1,
          "created_utc": "2026-02-12 10:23:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3hy8h",
      "title": "Chrome extension that auto-detects and extracts data from any webpage",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "author": "mjiqbal",
      "created_utc": "2026-02-13 06:33:11",
      "score": 12,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "we got tired of writing one-off scripts for simple scraping jobs (product listings, search results, directories, tables ) so we built a Chrome extension called Detect and Extract that handles the common cases without writing a single line of code.\n\nHow it works:\n\n1.  Click the extension icon on any page, it automatically detects data structures (tables, lists, grids, card layouts, etc.)\n2.   Preview the data in a grid, rename or delete columns you don't need\n3.   Export as CSV, Excel, JSON, or copy straight to clipboard\n\n The part we are most proud of, multi-page crawling:\n\n*   Point it at the \"Next\" button or page numbers, and it'll auto-crawl through all pages\n*   Handles numbered pagination (1, 2, 3...) and \"Next >\" button patterns\n*   Auto-detects and dismisses modal popups that block the page during crawls (upgrade prompts, cookie banners, etc.)\n*   Built-in deduplication so you don't get repeat rows across pages\n\n  Other stuff:\n\n*  Visual element picker ‚Äî if auto-detection misses something, click any element on the page and it finds all similar items\n*  Presets ‚Äî save your scraping config per site so you don't have to set it up again next time\n*  Minimal permissions ‚Äî only uses activeTab, no background data collection, no account required\n*  Works on most sites ‚Äî e-commerce, directories, forums, search results, dashboards\n\n  What it's NOT:\n\n*   Not a Selenium/Puppeteer replacement ‚Äî this is for visual, interactive scraping\n*   Won't bypass anti-bot measures or CAPTCHAs\n*   Not great for SPAs that require authentication flows or infinite nested navigation\n\n\n\nTrying to make it a solid free tool for people who need quick data without spinning up a whole scraping pipeline.\n\nWould love feedback from this community. What features would make this more useful for your workflows?\n\n[Detect and Extract](https://chromewebstore.google.com/detail/detect-and-extract/kkmibnjkdelljnkoibconbnaenpliefi)\n\nhttps://preview.redd.it/lj6mmc2xg7jg1.png?width=1681&format=png&auto=webp&s=9a0545a48f68145697e2a74d99da783ae07320da\n\n  \nHeads up about the install warning: If you have Chrome's Enhanced Safe Browsing turned on (Chrome Settings > Privacy), you'll see a \"Proceed with caution\" dialog when installing. This is completely normal for all newly published extensions it's not a security issue with the extension itself. Google flags every new extension until it builds up a trust score over time through installs and automated safety reviews. Just click \"Continue to install\" and it works fine. Users with Standard Safe Browsing won't see this at all.  \nReference URL [Regarding that warning](https://support.google.com/chrome/answer/2664769?visit_id=639065601437510841-3127678855&p=cws_enhanced_safe_browsing&rd=1#10745467)  \n",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55xo30",
          "author": "Gold_Emphasis1325",
          "text": "I made a few versions of these and settled on one that I sometimes use. I was thinking about trying to productize and ideally make money or at least make connections, but with vibe coding out these days, a whole genre of apps and utilities and projects are now \"useless\" in the customer sense, but gold for the individual -- hyper personalized, \"free (minus your time and tokens)\" and something to showcase....\n\nPrivate-source open roadmap for anyone building their own:  \n\\- cloudflare box interaction  \n\\- old and 2025+ style captchas (and beta unseen human detection bypass)  \n\\- secure interaction with API layer / MCP - LLM - RAG - persistence  \n\\- auth/auth  \n\\- payments system  \n\\- thought through TOS of the plugin/API, privacy (what users will accept) and payments  \n\\- plan for dealing with fragility, constant updates",
          "score": 1,
          "created_utc": "2026-02-13 13:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5awbvi",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-14 06:51:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4cax",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-14 08:06:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5bd2mc",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-14 09:32:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r16ix2",
      "title": "Is cookie reuse risky or necessary for modern scraping?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r16ix2/is_cookie_reuse_risky_or_necessary_for_modern/",
      "author": "JosephPRO_",
      "created_utc": "2026-02-10 17:07:40",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.92,
      "text": "I keep seeing very mixed advice when it comes to cookies in scraping workflows. Some people say reusing cookies is important if you want sessions to look normal and avoid getting blocked, especially on sites with logins or multi-step flows. Others warn that reusing cookies is risky and can actually cause more problems if you carry over bad state or get a session flagged.\n\nFrom what I‚Äôve seen so far, starting with a fresh session every time sometimes works, but other times it feels like sites expect continuity. Reusing cookies seems to make things more stable in those cases, but I‚Äôm never sure how long is too long or when a session should be thrown away and rebuilt.\n\nI‚Äôm trying to figure out what actually works in real-world scraping, not just in theory. Do people here mostly reuse cookies, rotate them often, or avoid them unless absolutely needed? Have cookies been a bigger source of trouble than things like IPs or headers over time?\n\nCurious to hear how others approach this and what you‚Äôve learned from experience.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r16ix2/is_cookie_reuse_risky_or_necessary_for_modern/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4n90y3",
          "author": "v_maria",
          "text": "This honestly depends on the site, thats the fun thing about scraping. There is no real silver bullet you need to learn and understand web",
          "score": 10,
          "created_utc": "2026-02-10 17:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n9lh7",
          "author": "army_of_wan",
          "text": "You throw the session away when it expires  and you'll know it expires when you get a 403 instead of a 200. \n\nCatch the exception\n\nInitiate a solving process -> could be cookie farming api or something else\n\nUpdate your session with new cookies\n\nYou may also consider proxy rotation.\n\nGo again.",
          "score": 8,
          "created_utc": "2026-02-10 17:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nljif",
          "author": "RandomPantsAppear",
          "text": "Not in the slightest, especially as most sites won‚Äôt block you solely for being an unfamiliar entity. No history + one of (bad ip, bad history, bad headers) will get you blocked though. \n\nThere are multiple ways to handle this\n\n1) Just accept your 50% success rate and abuse celery‚Äôs retry functionality \n\n2) Forge the cookies (integer, timestamp, uuid)\n\n3) Build a history - easier than it sounds. I‚Äôve actually had scrapers that zip up their chrome profile and upload to s3 when they‚Äôre done, then redownload it when they start again. \n\n‚Äî‚Äî‚Äî‚Äî‚Äî-\n\nThe biggest advantage a scraper has vs history based blocking is that **they can only query data they can access in milliseconds**. It is not acceptable virtually anywhere to add 200ms to every request. \n\nWhat this means is that you only need the basics - a number of requests previously seen, an ideally distant start date, and an ideally recent end date. That‚Äôs the kind of data most places can quickly cache in volume. \n\nSimilarly (for cookie forging) a remarkable number of places will accept a uuid as legitimate just because it should be a uuid and is a uuid, and cache misses and syncing between multiple nodes is a common issue with real users. \n\nThis can get a little hairy once captchas start rendering and you don‚Äôt solve them. But for most solutions, a pretty basic history will go a long way.",
          "score": 3,
          "created_utc": "2026-02-10 18:08:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qoqqp",
          "author": "SharpRule4025",
          "text": "It depends entirely on the site and what their anti-bot is looking for. There's no universal rule.\n\nFor sites with Cloudflare or similar protection, reusing a valid session cookie avoids re-solving the challenge on every request. That's a net positive, fewer challenge pages, faster requests, lower detection risk. The key is rotating them before expiry and not sharing a single cookie across multiple IPs.\n\nFor simpler sites, fresh sessions work fine. The overhead of cookie management isn't worth it if the site doesn't track session state for bot detection.\n\nThe real risk with cookie reuse is carrying a flagged session forward. If one request gets soft-blocked (CAPTCHA, rate limit warning), that cookie is burned. You need logic to detect when a session goes stale and swap it out immediately rather than keep hammering with a poisoned session.",
          "score": 2,
          "created_utc": "2026-02-11 03:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zcirk",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-12 13:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zj75h",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-12 14:35:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o567v2g",
          "author": "sohailglt",
          "text": "It depends on the website you‚Äôre scraping. You‚Äôll need to test and try the cookies one by one to see which ones are required.",
          "score": 1,
          "created_utc": "2026-02-13 14:51:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4kdoy",
      "title": "scraping vinted - TLS fingerprinting + session rotation",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4kdoy/scraping_vinted_tls_fingerprinting_session/",
      "author": "DataKazKN",
      "created_utc": "2026-02-14 13:16:31",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "spent the last couple weeks trying to scrape vinted (european secondhand marketplace, ~50M users). figured i'd share what i learned because their anti-bot setup is genuinely impressive and i haven't seen much written about it.\n\n**the stack they're running**\n\nvinted sits behind cloudflare with a pretty aggressive bot management config. but the interesting part isn't cloudflare itself ‚Äî it's what happens after you get past the initial challenge.\n\ntheir API requires a valid session with specific cookies generated through a multi-step auth flow. you can't just grab a token and go. the cookies are tied to your TLS fingerprint, so if your http client doesn't match what a real browser would send, the session gets invalidated silently. no error, no 403 ‚Äî just empty responses or stale data.\n\n**what didn't work**\n\nstarted with plain axios/undici. instant blocks. moved to got-scraping (which uses header-generator under the hood for realistic TLS). got further but still inconsistent ‚Äî about 40% of requests would return empty arrays even with valid-looking sessions.\n\nturned out the issue was TLS fingerprint rotation. if your JA3/JA4 hash changes between requests but your session cookie stays the same, they flag it. so you need to either:\n- keep a consistent TLS fingerprint per session\n- or rotate both session AND fingerprint together\n\n**what actually worked**\n\nended up using a playwright-based cookie factory that spins up a real browser context, completes the oauth flow, captures the authenticated cookies, then passes those to a lightweight http client (got-scraping) that maintains the same TLS fingerprint for the lifetime of that session.\n\nthe key insight was treating sessions as disposable units ‚Äî each one gets ~50-100 requests before you burn it and create a new one. tried pushing to 200+ and the ban rate went from ~2% to ~30%.\n\nalso had to handle their per-country domain routing. vinted merged most of western EU into one catalog but the API endpoints still differ by country (fr, de, es, it, nl, be, pt, etc). each needs its own session pool.\n\n**results**\n\nmanaged to pull ~960 items for a single query (\"nike air max\", france) in one run with full metadata ‚Äî prices, seller stats, item condition, photos, timestamps. the data quality is actually really good once you get past the anti-bot layer.\n\nthe whole session rotation + cookie factory approach runs without proxies for moderate volume. for heavier loads you'd obviously want to add proxy rotation but the TLS fingerprint consistency matters way more than IP rotation in my experience.\n\n**lessons learned**\n\n- TLS fingerprinting is the real gatekeeper now, not just IP bans\n- silent failures (empty responses instead of 403s) are way harder to debug than explicit blocks\n- session lifetime management matters more than raw request volume\n- got-scraping > undici/axios for anything cloudflare-protected\n- playwright for auth, lightweight client for data ‚Äî don't use a full browser for every request\n\ncurious if anyone else has tackled vinted or similar cloudflare-protected marketplaces. the TLS fingerprint binding to sessions was a new one for me.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4kdoy/scraping_vinted_tls_fingerprinting_session/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r3uw0e",
      "title": "Built two scrapers for european markets what should I learn next?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "author": "Free-Path-5550",
      "created_utc": "2026-02-13 17:10:14",
      "score": 7,
      "num_comments": 3,
      "upvote_ratio": 0.74,
      "text": "Been working on scrapers for a couple of European marketplaces this past week. My first attempt in this space. I finished two that successfully \"work\"... for now.  Some things I picked up so far:\n\n\\- Raw data isn't enough. Adding computed fields like deal detection, engagement scoring, and price tracking across runs makes the output way more useful than a static dump.\n\n\\- European platforms have aggressive anti-bot compared to US sites from my research. Took real effort to get stable.\n\n\\- You don't need a browser for everything. Keeping it lightweight makes a huge difference.\n\n\\- Biggest lesson learned... was how much I hate DataDome. I was able to slip past it a few times, but usually blocked the next run. I eventually learned to just go around it if possible.\n\nStill early in this spcraping. What should I be learning next? What separates a decent scraper from a great one?  Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o59r43b",
          "author": "Bitter_Caramel305",
          "text": "Try running your existing scrapers on a few hundred thousand product/items and you'll realize that scrapers without proxies and regular maintains are not stable at all. ",
          "score": 7,
          "created_utc": "2026-02-14 01:48:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4phlg",
      "title": "Payment processors for a scraping SaaS (high-risk niche)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "author": "Accomplished_Mood766",
      "created_utc": "2026-02-14 16:50:20",
      "score": 7,
      "num_comments": 22,
      "upvote_ratio": 0.82,
      "text": "Hi everyone,  \nI‚Äôm running a SaaS that provides scraping services, and I‚Äôm currently struggling with payment processing.\n\nStripe, Paddle, and Lemonsqueezy have all declined us due to the nature of the business. I understand that this niche is often classified as high-risk, but in practice we‚Äôve been operating for 5 months with **zero chargebacks or disputes**. Unfortunately, that doesn‚Äôt seem to matter much to decision-makers at most payment platforms ‚Äî scraping services are automatically flagged as high risk.\n\nI‚Äôd like to ask those of you who are running SaaS products in similar areas (scraping, data extraction, automation, etc.):\n\n* Which payment processors or merchant accounts are you using to accept credit card payments?\n* Are there providers that are more tolerant or experienced with this type of business?\n* Any recommendations or experiences you‚Äôre willing to share would be greatly appreciated.\n\nThanks in advance ‚Äî I‚Äôd really value hearing from others who‚Äôve dealt with this problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5d5tuf",
          "author": "JohnnyOmmm",
          "text": "Lmao crypto or porno processors",
          "score": 5,
          "created_utc": "2026-02-14 16:54:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6da7",
              "author": "Accomplished_Mood766",
              "text": "They accept cryptocurrency, but our customers don‚Äôt use crypto. Our users are mostly recruiters, marketers, and regular consumers who are accustomed to traditional credit and debit card payments.",
              "score": 3,
              "created_utc": "2026-02-14 16:57:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5dke6t",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 6,
                  "created_utc": "2026-02-14 18:07:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5dcjg8",
                  "author": "RobSm",
                  "text": "Send invoices and let them pay monthly wiretransfer, etc. You can automate email sending and if not paid - email notifications, etc. Also, paypal? Which has option to use CC directly?",
                  "score": 2,
                  "created_utc": "2026-02-14 17:28:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e4rl6",
          "author": "convicted_redditor",
          "text": "Try polar.sh",
          "score": 2,
          "created_utc": "2026-02-14 19:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eoyik",
          "author": "Dry_Illustrator977",
          "text": "Why not try crypto?",
          "score": 2,
          "created_utc": "2026-02-14 21:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gg8cf",
          "author": "Quantum_Rage",
          "text": "I don't what kind of scraping SaaS you're building, but I have seen SaaS apps built on web scraping accept payments via Stripe or Paddle.",
          "score": 1,
          "created_utc": "2026-02-15 04:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5imxka",
              "author": "Accomplished_Mood766",
              "text": "I run a SaaS focused on LinkedIn data scraping. Stripe declined us immediately. We worked with Paddle for five months with zero chargebacks, but their risk management team eventually decided to terminate our payment account.",
              "score": 1,
              "created_utc": "2026-02-15 15:09:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kcp1x",
          "author": "awebscrapingguy",
          "text": "I think you are not saying everything, all scraping saas run on Stripe or Paddle without issues",
          "score": 1,
          "created_utc": "2026-02-15 20:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kpe1p",
              "author": "Accomplished_Mood766",
              "text": "Stripe explicitly prohibits scraping-related services. Paddle‚Äôs policies were more flexible, and we were able to work with them for five months; however, their risk team eventually decided to stop supporting our account.",
              "score": 1,
              "created_utc": "2026-02-15 21:18:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kjygm",
          "author": "Round_Method_5140",
          "text": "Paypal?",
          "score": 1,
          "created_utc": "2026-02-15 20:50:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r103lp",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r103lp/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-10 13:01:11",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1r103lp/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4u7yks",
          "author": "Finding_Away40",
          "text": "webscraping newbie here.  How do I scrape Google maps, or can I scrape google business profiles?  Scraping for a list of residential and commercial paining companies in the US.  Or, is it easiest to use a tool like Outscraper?  thx",
          "score": 1,
          "created_utc": "2026-02-11 18:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vaoah",
          "author": "skinonmychin",
          "text": "How would I scrape the files behind an old website (late aughties) from WebArchive? I recall seeing somewhere a way to download the text files behind it without fragmenting the content.",
          "score": 1,
          "created_utc": "2026-02-11 21:10:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w4vur",
              "author": "HLCYSWAP",
              "text": "    wget \\\n      --mirror \\                # Mirror the site recursively\n      --convert-links \\         # Convert links to work offline\n      --adjust-extension \\      # Save files with proper extensions (like .html)\n      --page-requisites \\       # Download all resources (CSS, JS, images)\n      --no-parent \\             # Don‚Äôt ascend to parent directories\n      -e robots=off \\           # Ignore robots.txt restrictions\n      -P ./local-copy \\         # Save to this local folder\n      \"https://example.com/path/\"",
              "score": 1,
              "created_utc": "2026-02-11 23:43:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50bzja",
          "author": "Eyoba_19",
          "text": "Currently building a social media (tiktok and instagram mostly) to scrape profiles. The idea would be you can input specific keywords or description of what kind of influencers you‚Äôre looking for with specific criteria like min/max follower count, region, engagement rate and so on, and retrieve a set of profiles.\n\nOver time a knowledge graph of profiles will be built and you can thus just fetch from a db instead of scraping.\n\nYou can then ofcourse sell the data back or train AI or whatever, you own your data.\n\nI‚Äôm planning it as a Saas, but open to do it as a one-off product(although not sure how to handle constant support for when sites change their API).\n\nLet me know if anyone is interested, would love to talk to some people to kinda hit where this hurts and make sth out of it, DMs are open",
          "score": 1,
          "created_utc": "2026-02-12 16:53:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r06vvb",
      "title": "Help needed: scraping product URLs from El Corte Ingl√©s website",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r06vvb/help_needed_scraping_product_urls_from_el_corte/",
      "author": "frogmaxi",
      "created_utc": "2026-02-09 15:18:05",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.81,
      "text": "Hi all,\n\nI work for Jack & Jones and manage the El Corte Ingl√©s (Spain) account. I‚Äôm trying to build a reliable way to list which styles and color variants are currently published on ECI‚Äôs website, and extract two fields shown on the product page.\n\nSearch page (Jack & Jones men‚Äôs clothing): via this [link ](https://www.elcorteingles.es/moda-hombre/ropa/search-nwx/1/?s=jack+%26+jones&stype=past_search)\n\n**What I need (output = CSV):**  \nFor each product on that search result (about **229 products/styles** when fully loaded), I need one row per **color variant** with:\n\n* **Modelo** (style code)\n* **Referencia** (variant/reference code)\n\nExample issue:  \nOne product can have multiple color swatches. Clicking swatches changes the URL query (e.g. `...&color=Negro`, `...&color=Azul`), and each color has a different ‚ÄúReferencia‚Äù while ‚ÄúModelo‚Äù is shared.\n\n[for each colorway, we need the \\\\\"referencia\\\\\"](https://preview.redd.it/ggx6vi3uihig1.png?width=1901&format=png&auto=webp&s=6f82a326a98284309fb1778a55570ff02332ecdc)\n\n[This is found under \\\\\"detalle y cuidado\\\\\"](https://preview.redd.it/n6rdi0ywihig1.png?width=1817&format=png&auto=webp&s=e370f51ff77ac1bd1613caa7a4164c05830a1630)\n\n\n\nI have 0.5/10 python/web scraping knowledge but I guess this could be automated. I spent the last 8 hours trying back and forth with the help of chatGPT 5.2 (thinking) but without any luck. Is this achivable?\n\nThanks in advance!  \nCarlos",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r06vvb/help_needed_scraping_product_urls_from_el_corte/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4hl0rv",
          "author": "jeheda",
          "text": "I was bored so:\n\n[https://docs.google.com/spreadsheets/d/1McYT-ulwmsUp7zBmaTTXvlD-cMJBmYBBds\\_wqHAoKis/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1McYT-ulwmsUp7zBmaTTXvlD-cMJBmYBBds_wqHAoKis/edit?usp=sharing)",
          "score": 4,
          "created_utc": "2026-02-09 19:46:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hli0j",
              "author": "frogmaxi",
              "text": "What a fucking genius!! \nWould you pleeease be so kind and let me know how you did it??",
              "score": 1,
              "created_utc": "2026-02-09 19:49:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4homd1",
                  "author": "jeheda",
                  "text": "They have an API, go to the website search and product page press F12 and go to the network tab and you will see the calls the website does to these api endpoints.\n\nIn this case i used these two endpoints:\n\nThis one requires authorization but the web generates one you can just copy and paste it:  \n/products/ecommerce-pdp-product/v1/pdp/  \n\n\nThis is the search pagination doesn't seem to require authorization:  \n/api/firefly/vuestore/new-search/moda-hombre/ropa/1/?s=jack%20%26%20jones&showDimensions=none&stype=past\\_search&isBookSearch=false&isMultiSearchCategory=false",
                  "score": 2,
                  "created_utc": "2026-02-09 20:05:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4g1w6f",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-09 15:24:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g7x84",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 1,
              "created_utc": "2026-02-09 15:53:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gmcfb",
          "author": "CouldBeNapping",
          "text": "Totally achievable, but you should know that Jack&Jones already have an agency for this. You should talk to your UK counterparts.",
          "score": 1,
          "created_utc": "2026-02-09 17:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gwpla",
              "author": "frogmaxi",
              "text": "I thought so :)\nI don‚Äôt know anyone at the UK office. In fact, I barely know anyone outside the Spanish branch. \nAny idea how difficult is to do what I need?\nThanks!!",
              "score": 1,
              "created_utc": "2026-02-09 17:51:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j0m9g",
          "author": "SharpRule4025",
          "text": "For ongoing monitoring like this you want to hit their internal API endpoints directly instead of scraping rendered pages. Open the network tab in your browser while navigating the Jack & Jones section, filter by XHR, and you'll probably see product catalog API calls returning JSON with all the data you need.\n\nIf they serve it through a GraphQL endpoint thats even better since you can query exactly the fields you want. Way more stable than parsing HTML which breaks every time they redesign.",
          "score": 1,
          "created_utc": "2026-02-10 00:12:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3lfdv",
      "title": "How do you handle scraping directory sites that cap results at ~200?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "author": "deadlynightshade_x",
      "created_utc": "2026-02-13 10:08:34",
      "score": 5,
      "num_comments": 17,
      "upvote_ratio": 0.69,
      "text": "Hey \n\nI've been trying to pull data from a large online directory/phone book site (Swiss one, but I think the issue is pretty common across similar services like yellow pages, local directories, etc.).The site claims tens of thousands of matching entries (e.g., \\~50k private records for a region), but in practice:\n\n* URL params like &pages=500, &maxnum=500, or whatever don't actually fetch more and it hard-caps visible/returned results around 200.\n\nHas anyone here successfully scraped large volumes from these kinds of directory/phone book sites recently ?\n\n* Do most of them still enforce strict \\~100‚Äì200 result caps per query/page?\n* What tricks actually work to get around it without getting banned quickly?\n\nJust curious if it's still feasible for bigger datasets or if these sites have mostly locked it down. \n\n\n\nTips, tools, or experiences appreciated !Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55w5y8",
          "author": "datamizer",
          "text": "They all set max results per query to normal defaults like 100. Even if you try to request more, the API basically has logic to protect it from serving a very large result set. \n\nAll you do is set a page number or offset, figure out the max result size, then send paginated requests at some interval. That's very standard.\n\nDepends on the site, some just have basic rate limits like no more than 10 requests over 10 seconds from the same IP. Most don't have robust prevention. Most you just pass an origin and a referer and that's good enough.",
          "score": 7,
          "created_utc": "2026-02-13 13:49:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o565cf3",
              "author": "leros",
              "text": "Lots of sites cap the results and don't let you paginate through them all. You need be more creative to scrape everything.¬†",
              "score": 2,
              "created_utc": "2026-02-13 14:38:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56d6i8",
                  "author": "datamizer",
                  "text": "It depends on the specifics of how they implemented it. Some sites there is no mechanism other than having a full sitemap if they offer one. Checking robots.txt to see what they are trying to obscure, using search engines that have their pages indexed etc. It's really just site by site basis.",
                  "score": 1,
                  "created_utc": "2026-02-13 15:17:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bqpts",
              "author": "deadlynightshade_x",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-14 11:44:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o595at0",
          "author": "forklingo",
          "text": "a lot of those caps aren‚Äôt just ui limits, they‚Äôre intentional backend limits to stop bulk extraction. if the api itself only returns \\~200 per query, tweaking page params usually won‚Äôt help.\n\nwhat tends to work better is slicing the query space instead of trying to increase the page size. for example, split by smaller geographic areas, postal codes, name prefixes, or categories so each query stays under the cap but collectively covers more ground.\n\nalso be careful with ‚Äúgetting around‚Äù protections. many directory sites have pretty strict terms and rate limits, and they do monitor unusual access patterns. rotating ips or hammering endpoints might work short term but usually gets you blocked fast.\n\nfor larger datasets, sometimes it‚Äôs more realistic to look for official data sources, public datasets, or paid access if it‚Äôs business critical. scraping big directories at scale has gotten a lot harder over the years.",
          "score": 2,
          "created_utc": "2026-02-13 23:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bquvy",
              "author": "deadlynightshade_x",
              "text": "I appreciate your input",
              "score": 1,
              "created_utc": "2026-02-14 11:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57vvg6",
          "author": "Free-Path-5550",
          "text": "Break your one big query into many smaller ones that each return under the cap., then deduplicate after merging since slices will overlap. Well this is how i solved against the site i was trying.",
          "score": 1,
          "created_utc": "2026-02-13 19:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br7pq",
              "author": "deadlynightshade_x",
              "text": "That what i did and it worked ! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:48:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5a85n5",
          "author": "pauldm7",
          "text": "You will likely need to make a lot more queries. Ie instead of searching for widgets, you will search for widgets a, widgets b, widgets aa widgets ab and so on. Cover every combo of letters until the results stop.\n\nIf you can click on the individual results, how is the URL? Numerical IDs?",
          "score": 1,
          "created_utc": "2026-02-14 03:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bra4d",
              "author": "deadlynightshade_x",
              "text": "That's exactly what I did and it gave me good results! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:49:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bfme2",
          "author": "uncivilized_human",
          "text": "one thing i've found helps - check if there's an internal api the frontend uses. the caps are often enforced at the ui layer but the underlying api might have different limits. open network tab, search, and see what endpoints get hit. mobile apps are worth checking too since their apis tend to be less locked down.",
          "score": 1,
          "created_utc": "2026-02-14 09:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5brd9y",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gvbay",
                  "author": "shipping_sideways",
                  "text": "np! good luck with it - the internal apis on sites like that sometimes have cursor-based pagination that lets you get around the arbitrary caps. if you find the actual endpoint the frontend uses, it's often way less restrictive than what they expose to users",
                  "score": 1,
                  "created_utc": "2026-02-15 06:32:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56bp02",
          "author": "Hour_Analyst_7765",
          "text": "Yes most sites cap request sites.. whether its too big for a client (probably historic reasons), or anti-scraping measure.\n\nHowever, 100 to 200 per call is imo quite good still. If the dataset is 50k, thats 500 pages, and even if you wait a minute per call you'll have that dataset in just over 8 hours. If you refresh daily thats quick enough.\n\nFor me <10 hours is plenty quick. But I often don't have very strict latency requirements. Most of my jobs need to keep up on a \"daily basis\"\n\nThe difficulty is often:\n\n1. Walking pages linearly may get you blocked on WAF nowadays.\n2. Sites that stop serving data after N amount of records, because they don't expect anyone to realistically scan through literal hunrdeds or thousands of records.\n\nIn this case, you'll have to deal with navigation through categories, parametric filters, etc. to reduce the dataset that can be crawled completely.\n\n3. Sites that don't have a way of putting new/modified records in front. I fetch quite a lot of listings and always will look for a \"newest first\" kind of sorting. Fortunately for many sites this is their default. I stop crawling as soon as I see data that I've already crawled.",
          "score": 1,
          "created_utc": "2026-02-13 15:10:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58kdvb",
              "author": "ObjectIndependent827",
              "text": "Yeah this matches what most people run into now it‚Äôs less about brute forcing pages and more about breaking the dataset into smaller slices through filters or categories\n\nNewest first sorting is huge too since it lets you maintain coverage without re crawling everything and helps avoid triggering WAF limits over time",
              "score": 2,
              "created_utc": "2026-02-13 21:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5br5j3",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r61a90",
      "title": "Need tought websites to scrape for testing",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "author": "Transformand",
      "created_utc": "2026-02-16 05:39:35",
      "score": 4,
      "num_comments": 8,
      "upvote_ratio": 0.83,
      "text": "I've been developing my own piece of code, that so far has been able to bypass anti-bot security I had a tough time cracking before at scale (such as PerimiterX).\n\nCan you share what sites you think are difficult to access/scrape?\n\nI want to test out my scraper more before open sourcing it",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5mz52h",
          "author": "ry8",
          "text": "Try BassProShop.",
          "score": 3,
          "created_utc": "2026-02-16 05:50:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mz7p3",
          "author": "Lemon_eats_orange",
          "text": "Jokingly if you can get past the anti bot defenses on piracy anime sites that would be a site to see. Though like seriously you open the chrome developer tools on some of those sites and they will feedback loop you and run your resources dry. \n\nOff the top of my head these sites might be difficult. \nWalmart.com, zoro.com, naver.com and it's subdomains can be incredibly difficult, hermes.com, safeway.com. Definitely shopee.",
          "score": 2,
          "created_utc": "2026-02-16 05:51:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n45v8",
          "author": "thePsychonautDad",
          "text": "Protonmail email creation steps.",
          "score": 2,
          "created_utc": "2026-02-16 06:33:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5naddf",
          "author": "LT823",
          "text": "Instagram followers of profile",
          "score": 1,
          "created_utc": "2026-02-16 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc08b",
          "author": "GuNiKz",
          "text": "reuters, I tried once, but I wasn't successful ",
          "score": 1,
          "created_utc": "2026-02-16 07:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n08hq",
          "author": "nofilmincamera",
          "text": "Sounds kind of insane but I like banks for this. Specifically self service portals that are public. Tended to have recaptcha 3 plus latered protection.",
          "score": 0,
          "created_utc": "2026-02-16 05:59:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n7b3a",
          "author": "scorpiock",
          "text": "Try payment, social media sites",
          "score": 0,
          "created_utc": "2026-02-16 07:01:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0xvwc",
      "title": "IMDB data scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r0xvwc/imdb_data_scraping/",
      "author": "pavankalyanre",
      "created_utc": "2026-02-10 11:06:49",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "**My old post:**\n\n[ https://www.reddit.com/r/webscraping/comments/o7l9fw/here\\_is\\_how\\_i\\_scraped\\_everything\\_on\\_imdb\\_website/ ](https://www.reddit.com/r/webscraping/comments/o7l9fw/here_is_how_i_scraped_everything_on_imdb_website/?utm_source=chatgpt.com)\n\nHi webscrapers,\n\nA few years ago, I posted about how I scraped around 500k IMDb movies. I‚Äôve since updated the package to extract data directly from imdb apis instead of using BeautifulSoup or Selenium.\n\nI created scripts to extract IMDb data.\n\nCheck out the package and feel free to fork it and try it out. If you find it useful, a ‚≠ê on the repo would be appreciated.\n\nGithub :  [ https://github.com/pavan412kalyan/imdb-movie-scraper/tree/main/ImdbDataExtraction ](https://github.com/pavan412kalyan/imdb-movie-scraper/tree/main/ImdbDataExtraction)\n\nHere is the IMDb movie dataset on Kaggle: updated with 700k + movies now\n\n[ https://www.kaggle.com/datasets/pavan4kalyan/imdb-dataset-of-600k-international-movies/data ](https://www.kaggle.com/datasets/pavan4kalyan/imdb-dataset-of-600k-international-movies/data)\n\nYou can also see the website I built using this data (with some AI):\n\n[ https://realimdb.com/ ](https://realimdb.com/)\n\nScripts included:\n\nImdbDataExtraction/\n\n‚îú‚îÄ‚îÄ pages\\_dowloader/           # Movie/TV bulk scraping\n\n‚îú‚îÄ‚îÄ search\\_by\\_id/              # Individual lookups\n\n‚îú‚îÄ‚îÄ search\\_by\\_string/          # Text-based search\n\n‚îú‚îÄ‚îÄ people\\_downloader/         # Celebrity/crew data\n\n‚îú‚îÄ‚îÄ videos\\_downloader/         # Video content\n\n‚îú‚îÄ‚îÄ images\\_dowloader/          # Image content\n\n‚îú‚îÄ‚îÄ review\\_downl",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r0xvwc/imdb_data_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r4mkpv",
      "title": "Data extraction quality / LLM cost",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "author": "Playgroundmob",
      "created_utc": "2026-02-14 14:53:02",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.72,
      "text": "I'm trying to get an idea of how much people/companies would pay per result when scraping websites that require structured data with a reliable JSON schema and high data quality - for example, e-commerce or job listings. especially when dealing with unknown sources.\n\nLLMs like Flash 2.5/3.0, which are actually reliable for consistent results, are not cheap - sometimes we might even reach $0.01‚Äì$0.03 per extraction.\n\nI'm trying to understand, in real world terms, how much people would pay for solutions that just work.\n\nIf anyone can share real world use cases, that would be awesome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5czh07",
          "author": "RandomPantsAppear",
          "text": "I think the biggest issue you will run into is people doing it cheaper, not using an LLM. You are kind of limiting yourself to one offs, because for any recurring task people will find something more traditional and exponentially cheaper.",
          "score": 1,
          "created_utc": "2026-02-14 16:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d1jzs",
              "author": "Playgroundmob",
              "text": "You're right. I'm wondering if there's a use case for massive scrape operation, where we just can't handle building specific scrapers per domain, as we don't know what volume of different websites to investigate and scrape.",
              "score": 1,
              "created_utc": "2026-02-14 16:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5d246w",
                  "author": "RandomPantsAppear",
                  "text": "If I were you, I‚Äôd focus on integrating. There‚Äôs lots of options but I would first build something in that let it work with Google docs, pdf, and images as well as HTML. I would also add several export types. \n\nAfter that I would make a chrome extension that allowed people to either subscribe or top up their balances, and extract data on demand. \n\nI would name the app something that business people would likely search for, not something a coder would search for.",
                  "score": 2,
                  "created_utc": "2026-02-14 16:35:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3htxr",
      "title": "[Selenium/C#] \"Cannot start the driver service\" in Windows Service",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "author": "AromaticLocksmith662",
      "created_utc": "2026-02-13 06:26:28",
      "score": 3,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôve been banging my head against a wall for a week with a Selenium ChromeDriver issue and could use some fresh eyes.\n\nThe Context:\n\nI have a web scraping tool running as a background Windows Service. It processes license data for different states.\n\nScale: We have about 20 separate Windows Services running in parallel on the same server, each scraping different data sources.\n\nTech Stack: C# .NET, Selenium WebDriver, Chrome (Headless).\n\nVersion: Chrome & Driver are both version 144.0.x.x (Versions are matched).\n\nThe Issue:\n\nEverything was running smoothly until recently. Now, I am getting a WebDriverException claiming it cannot start the driver service on a specific localhost port.\n\nthe exception:\n\nCannot start the driver service on http://localhost:54853/\n\nThe Stack Trace:\n\nat OpenQA.Selenium.DriverService.Start()\n\nat OpenQA.Selenium.Remote.DriverServiceCommandExecutor.Execute(Command commandToExecute)\n\nat OpenQA.Selenium.WebDriver.Execute(String driverCommandToExecute, Dictionary\\`2 parameters)\n\nat OpenQA.Selenium.WebDriver.StartSession(ICapabilities desiredCapabilities)\n\nat OpenQA.Selenium.Chromium.ChromiumDriver..ctor(ChromiumDriverService service, ChromiumOptions options, TimeSpan commandTimeout)\n\nat MyNamespace.LicenseProject.Business.Vermont.VermontLicenseService.ProcessLicense() in ...\\\\VermontLicenseService.cs:line 228\n\ncode:\n\nvar options = new ChromeOptions();\n\noptions.AddArgument(\"--headless\");\n\noptions.AddArgument(\"--no-sandbox\");\n\noptions.AddArgument(\"--disable-dev-shm-usage\");\n\n// I am explicitly setting the driver directory\n\nvar service = ChromeDriverService.CreateDefaultService(driverPath);\n\nservice.HideCommandPromptWindow = true;\n\n// Error implies it fails right here:\n\nusing (var driver = new ChromeDriver(service, options, TimeSpan.FromMinutes(2)))\n\n{\n\n// scraping logic\n\n}\n\nWhat I've Tried/Verified:\n\nVersion Mismatch: Double-checked that the chromedriver.exe version matches the installed Chrome browser version (144.0.x.x).\n\nManual Run: The scraper works fine when I run it as a console app/user mode. It only fails when running as a Windows Service.\n\nCleanup: I suspected \"zombie\" chrome processes were eating up ports, so I added logic to kill orphaned chrome processes, but the issue persists.\n\nHas anyone managed high-volume Selenium instances in a Windows Service environment and seen this port binding error?\n\nAny pointers would be appreciated!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5b4f4r",
          "author": "jinef_john",
          "text": "I would explicitly enable verbose logging and log that to a file. But here is what I think is happening, with 20 parallel services, they're likely clashing on the default user data directory. You should try and set unique user data directories per service instance.\n\nAlso I suspect the driver might be failing to bind, you could let chromedriver pick its own port, by setting service.Port=0.\n\nIf neither of these help, I'd bet it's a session 0 isolation problem. I believe even in headless, chrome sometimes needs to create a window station or access GDI resources that are restricted in Session 0. So you could try running the service under a specific user account with 'allow service to interact with service'\n\nOr it could even be the headless flags too. \nTry the new headless mode\n```\n--headless=new\n```\nYou could also add other options, disable gpu, extensions,software rasterizer,force color profile to be srgb\n\nBut I would mostly look into the user data directory or session 0 isolation.",
          "score": 1,
          "created_utc": "2026-02-14 08:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dpbq2",
          "author": "CuriousCat7871",
          "text": "I know it is windows, but can you run the browser in a docker container instead?",
          "score": 1,
          "created_utc": "2026-02-14 18:32:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n5vqw",
          "author": "THenrich",
          "text": "You should consider switching to using Playwright instead of Selenium. Selenium is quite old and there are advantages to using Playwright.",
          "score": 1,
          "created_utc": "2026-02-16 06:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o568ljp",
          "author": "Stunning_Cry_6673",
          "text": "Lol. Selenium was a technology popular 15 years ago . Cant believe someone would use it today from scraping üòÇüòÇüòÇ",
          "score": -3,
          "created_utc": "2026-02-13 14:54:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4tkt",
              "author": "jinef_john",
              "text": "Your favorite tool today, probably uses selenium under the hood üòâ",
              "score": 0,
              "created_utc": "2026-02-14 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5b52xt",
                  "author": "Stunning_Cry_6673",
                  "text": "Lol. In these days when is so simple to get information you come with this garbage affirmation ü§£ü§£ü§£",
                  "score": 0,
                  "created_utc": "2026-02-14 08:14:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r5rrds",
      "title": "Let‚Äôs move by step",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 22:14:31",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "https://www.reddit.com/r/webscraping/s/0aCA0m6ioo\n\nJust use this post to comment with a website, so send in this format\n\nWebsite and what you want to achieve \n\nAnd then we will make a list so we can proceed by step and start with website with more upvotes \n\nWhat do you think?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5l6t90",
          "author": "No-Exchange2961",
          "text": "Linkedin!!!",
          "score": 2,
          "created_utc": "2026-02-15 22:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7dh3",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 22:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lw6n6",
          "author": "Mysterious_Tip_6793",
          "text": "LinkedIn and Reddit",
          "score": 1,
          "created_utc": "2026-02-16 01:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m5c12",
          "author": "lechiffreqc",
          "text": "CRA: Want to maintain auth for allowing automating tax filling \nRevenu Quebec: Same as CRA\nTD: Same, but also automating downloading transactions \nAll other canadian banks for same reason",
          "score": 1,
          "created_utc": "2026-02-16 02:19:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5ca6r",
      "title": "Pull URL with WebScraper",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5ca6r/pull_url_with_webscraper/",
      "author": "HyperfocusedSoul",
      "created_utc": "2026-02-15 11:22:49",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.81,
      "text": "Hi all,\n\n  \nI will openly admit that I am not code-savvy... that said, I need some help.\n\nI am trying to use WebScraper to pull data from multiple pages linked to a starting page. Basically, it is designed to start at a particular website's history (the starting page), and click through/scrape the listed data from every \"new\" page linked in the history. It is doing that fine, but I also want to grab the URL associated with each new entry/page. While it is grabbing the Starting URL, it is not grabbing the individual URLs for each page it is going through. For the life of me, I can not figure out the correct Type and Selector. Does anyone have any tips or advice?\n\n  \nP.s. I would want to add this to an existing Parent Selector that already has all my other selectors/data points as subs...",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5ca6r/pull_url_with_webscraper/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5hwz3v",
          "author": "Equivalent-Brain-234",
          "text": "Which web scraper are you using? Is it custom built?",
          "score": 1,
          "created_utc": "2026-02-15 12:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i48du",
          "author": "akashpanda29",
          "text": "What is the website you are trying to scrape and what's your setup ? \nI can help you",
          "score": 1,
          "created_utc": "2026-02-15 13:20:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0mir4",
      "title": "FB group post scraping",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/helvk0jdgkig1.png",
      "author": "Capital_Towel_9219",
      "created_utc": "2026-02-10 01:01:51",
      "score": 2,
      "num_comments": 4,
      "upvote_ratio": 0.63,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r0mir4/fb_group_post_scraping/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4jn1v5",
          "author": "RandomPantsAppear",
          "text": "With FB and tasks like this the only real route I‚Äôve found for selectors is finding what you *can* make a selector for that is nearby the element you‚Äôre targeting, then going into parent/children and looking at the length of the non html text. \n\nSometimes this is more doable if you compress the dom down - so (as an example) if element and element.parent have the same total text rendered to them and both are div/span, eliminate either tag (in a way that maintains the rendered text). \n\nIt (again, sometimes) helps make the gratuitous and redundant html fb renders more manageable and makes selectors matching that you later call parent or child on a bit more predictable.",
          "score": 2,
          "created_utc": "2026-02-10 02:22:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jnip7",
              "author": "RandomPantsAppear",
              "text": "Another option might be the screenshot and send to AI approach but that gets pricey at scale",
              "score": 3,
              "created_utc": "2026-02-10 02:25:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mk51f",
                  "author": "Capital_Towel_9219",
                  "text": "It is really time consuming, thats the main issue.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:14:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l6gpl",
          "author": "Hot_District_1164",
          "text": "I'm trying to solve exactly this issue right now. The UI contains deliberately malicious bait attributes, that make you think you can use them to extract the content, but new attributes are appearing an disappearing on a daily basis. Links might not have the href attribute at all unless the post is within viewport and hovered by mose. Crazy stuff. Most success I had was by just getting all the posts, getting innerText and processing by AI. I've also reverse engineered some successful browser extensions doing this Facebook stuff and apparently they are using the Facebook GraphQL to get the data.",
          "score": 1,
          "created_utc": "2026-02-10 09:34:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1wc9z",
      "title": "scrape data from site that loads data dynamically with javascript???",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r1wc9z/scrape_data_from_site_that_loads_data_dynamically/",
      "author": "Quiet_Dasy",
      "created_utc": "2026-02-11 12:41:37",
      "score": 2,
      "num_comments": 9,
      "upvote_ratio": 0.63,
      "text": "Project Overview: DeckMaster Scraper\n\nLive Site: domain-rec.web.app\n\nTech Stack: Flutter frontend with a Supabase backend.\n\nCurrent Access: Public REST API endpoint (No direct DB credentials).\n\n\nTarget Endpoint:\n https://kxkpdonptbxenljethns.supabase.co/rest/v1/PopularDeckMasters?select=*&limit=50\n\nThe Goal\n\nInstead of just pulling all cards\n, I need to extract the specific card name  ,not card data, contained within each individual page.\n\nThe Challenge\n\nI need a method to iterate through the IDs provided by the main API and scrape the specific card details associated with each entry.\n\nHow to Scrape the Data??\n\nSince the site uses Supabase, i don't actually need to \"scrape\" the HTML. \n",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r1wc9z/scrape_data_from_site_that_loads_data_dynamically/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4srk51",
          "author": "BeforeICry",
          "text": "Generally one of the best formats. Static sites require full HTML parsing whereas these you can look for the backend API and likely use a much smaller traffic bandwidth.",
          "score": 2,
          "created_utc": "2026-02-11 13:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wbfd8",
          "author": "WouldaCouldaBetta",
          "text": "Cant u just let the page load and scrape the loaded html?",
          "score": 1,
          "created_utc": "2026-02-12 00:21:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yt62h",
          "author": "SharpRule4025",
          "text": "This is actually way easier than you think. Flutter apps with Supabase backends expose the REST API directly, you already found the endpoint. You don't need to scrape the rendered page at all.\n\nJust hit that Supabase endpoint, grab the IDs from the main listing, then iterate through them hitting the detail endpoint for each one. Supabase REST follows PostgREST conventions so you can filter with ?id=eq.{id} to get individual records. No browser, no JS rendering needed.",
          "score": 1,
          "created_utc": "2026-02-12 11:55:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5676nq",
          "author": "sohailglt",
          "text": "Monitor the endpoints, headers, and cookies, then replicate the request exactly as they send it to retrieve the data.",
          "score": 1,
          "created_utc": "2026-02-13 14:47:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}