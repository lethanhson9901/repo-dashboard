{
  "metadata": {
    "last_updated": "2026-01-20 08:59:58",
    "time_filter": "week",
    "subreddit": "sre",
    "total_items": 8,
    "total_comments": 64,
    "file_size_bytes": 60677
  },
  "items": [
    {
      "id": "1qfhnox",
      "title": "Datadog pricing aside, how good is it during real incidents",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfhnox/datadog_pricing_aside_how_good_is_it_during_real/",
      "author": "HotelBrilliant2508",
      "created_utc": "2026-01-17 16:29:29",
      "score": 74,
      "num_comments": 25,
      "upvote_ratio": 0.96,
      "text": " Considering Datadog setting aside the pricing debate for a second - how does it actually perform when things are on fire?\n\nIs the correlation between metrics and traces actually useful?\n\nWant to hear from people who've used it during actual incidents. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfhnox/datadog_pricing_aside_how_good_is_it_during_real/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o055utj",
          "author": "rolexboxers",
          "text": "The real question is does your team actually know how to use it under pressure? That matters way more than which specific platform you're on.",
          "score": 30,
          "created_utc": "2026-01-17 17:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bcpvm",
          "author": "AccountEngineer",
          "text": "Used a few different ones at different companies. They all have strengths and weaknesses. None of them are going to magically solve incidents for you if that's what you're asking.\n\nWhat specific use case are you trying to optimize for?",
          "score": 29,
          "created_utc": "2026-01-18 16:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04v7h2",
          "author": "Mmmm618",
          "text": "Yeah, it holds up. The correlation stuff isn't bullshit - you can actually track an issue from alert to root cause without wanting to throw your laptop.",
          "score": 15,
          "created_utc": "2026-01-17 17:07:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06miro",
              "author": "Mangudai_11",
              "text": "And what about predicting events? Before they happen and before I throw away my laptop xD",
              "score": 1,
              "created_utc": "2026-01-17 22:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o07or8f",
          "author": "Grandpabart",
          "text": "If incident response is your actual priority, Datadog will tell you something is broken, but you're going to need an action layer (e.g. an IDP like Port) to make sure the right owners of the problem are on it and everyone knows their role/what to do/how.",
          "score": 14,
          "created_utc": "2026-01-18 01:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04sxzs",
          "author": "engineered_academic",
          "text": "Datadog is probably the best tool but like any tool you have to configure it properly and be familiar with it so that you can wield it during incidents. Doing chaos engineering experiments to test your runbooks and exercise those incident muscles is important. It definitely isn't fire and forget, you need to have a proper strategy and vision around logs and traces as well as controlling and justifying costs.",
          "score": 52,
          "created_utc": "2026-01-17 16:57:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o054l4w",
              "author": "hijinks",
              "text": "i 2nd this.. i run a consulting company that specializes in o11y and helping people with spend.\n\nIf DD was half the price then most would be using it. In all my time I've never heard a company say we hate DD the service. It's always we hate DD pricing.",
              "score": 22,
              "created_utc": "2026-01-17 17:51:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08ko8z",
                  "author": "tcpWalker",
                  "text": "I feel like I have heard about companies being talked into it by management firms and then having the agents cause incidents. But I am not sure why and was not running those incidents. I suppose like with any new major service, do your PoCs and manage your risk as you gain familiarity with the project.  \n  \n(And the ordinary tradeoffs about building in-house vs outsourcing to a vendor apply)",
                  "score": 1,
                  "created_utc": "2026-01-18 04:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04zeep",
              "author": "stikko",
              "text": "100% this. Donâ€™t expect to be able to spew a bunch of garbage in and get magic incident resolutions back out.  Strategy from SRE and discipline from the app teams are key.",
              "score": 3,
              "created_utc": "2026-01-17 17:27:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o050u6x",
              "author": "Fattswindstorm",
              "text": "Yes itâ€™s a great tool. Just be mindful when you are about to start a trial, they are gonna be up your ass to get agents installed so be prepared.  Have the proper doors open, and a strategy to get the agents installed.  \n\n- Have a basic tagging scheme ready.  Like define your services, applications, environments, etc. \n- Know what logs you want ingested, and determine which ones are important and which ones are noise.  Youâ€™ll want to filter out the noise fore ingesting the logs. \n\nWe enabled DataDog the same time we were doing a lift an shift into Aws and both landed on my lap.  It was very chaotic and hard trying to figure all this out during the trial period. So if you have the room, give an engineer or two the room to set this up with some sort of framework ready for guiding the implementation",
              "score": 2,
              "created_utc": "2026-01-17 17:34:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06vqeu",
              "author": "Hebrewhammer8d8",
              "text": "It is fire and forget if the lead engineer decides to leave and take another job with little documentation, and management expects that we can do what lead engineer did.",
              "score": 1,
              "created_utc": "2026-01-17 23:01:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o04nihz",
          "author": "whitechapel8733",
          "text": "The best IMO.",
          "score": 10,
          "created_utc": "2026-01-17 16:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o059lgn",
          "author": "jimmyjohns69420xl",
          "text": "at the user layer itâ€™s fantastic. great UI and tools.\n\nat the admin layer its ok, but youâ€™ll spend more time than you expect pulling your hair out over undocumented agent configs and such. youâ€™ll also likely end up doing annoying cost control exercises.\n\nI think the best argument against DD today is that with LLMs, the UI layer is becoming less important, and DDâ€™s pricing model disincentivizes sending complete and high-cardinality data, which is what LLMs need to behave well. itâ€™s still early on that front but something to consider.",
          "score": 5,
          "created_utc": "2026-01-17 18:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04u6rl",
          "author": "theubster",
          "text": "Its better than most at anomaly detection. But, I don't trust any software enough to have thay kind of thing be the core of our detection or root cause discovery. \n\nHonestly, its dashboards are what sets it apart. The widget options & display are second to none. Plus, the sheer number of integrations, allowing it to be the one main hub of information about what's happening in our platform. \n\nUltimately, incident recovery comes down to platform knowledge and timely detection. Both of which can work with any tool. Why I like datadog for incidents is that their incident tooling make data capture easy, and gives fantastic visibility into incident trends over time.",
          "score": 2,
          "created_utc": "2026-01-17 17:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0581xs",
          "author": "IS300FANATIC",
          "text": "In my experience- Datadog has been an amazing observability tool. \n\nThe event correlation was a big deal and difference maker when reviewing their stack against others like new relic, app lnsights, signalfx and even dynatrace.  Dynatrace was actually a pretty close second imo but where they lost me is they failed to remove unused or empty data selectors from selection scope. For example - if im an Azure shop - i dont want to have to scroll past empty non-used AWS metrics and features just to find data I am ingesting and needing to filter down to. (Atleast 3ish years ago it was this way, not sure about now) + a few other small things. \nAnyway - back to the question: that quick data isolation moment in time saves time when needing to jump in quick if you didnt do the firedrill work to make those correlations or build the visibility before hand\n\nOne other piece that im quite impressed with Datadog tooling before and during an issue is how light their continuous profiler is.  Super cheap on resources and just seems to work in most cases - making it more comfortable to turn it on, let it ride in a close lower and curve some of that production inefficiency or spotting edge case bugs in production based on conditions that may not have been possible to reproduce in a lower (vendors didnt offer sandbox or test envs so production is your testing bed for feature with live traffic, not just mock data)\n\nI'm not a super huge fan of their RUM though and their syntetics could be a little more flexible. - that product suite just feels a bit clunky imo.\n\nLastly - npm has been a life saver in a k8s environment- it builds huge context into what's talking to what and can filter down pretty nicely to isolate traffic patterns so let's say \"podx is in imagepullbackoff\" - why? Well typically first thing you think is it not being able to pull its image from whatever container registry. Well I've actually caught an issue in scenarios like this where the pod was trying to hit the ACR at its public endpoint and not its private connection endpoint configure - forcing us to open up to public with restrictions to satisfy the incident, open ticket with msft and better understand the conditions which turned out to be msft side issue. - without npm in our pocket, looking at DNS queries and outbound connection attempts would have been a lot tough/slower.\n\nOthers may disagree with a few points here but they have all helped myself and my teams in their journey for reliability and they seem to win the race mostly every time. \nThey are pricey a f though nonetheless lol",
          "score": 2,
          "created_utc": "2026-01-17 18:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05ffbk",
          "author": "Ecestu",
          "text": "We've found that simpler is usually better during incidents. Too many features and fancy visualizations just add confusion when you're stressed and need to act fast.",
          "score": 2,
          "created_utc": "2026-01-17 18:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08o8y5",
          "author": "inferix",
          "text": "Im mainly into Dynatrace and wondered how it compares to Datadog?",
          "score": 1,
          "created_utc": "2026-01-18 05:00:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bdvf0",
          "author": "Relative_Taro_1384",
          "text": "Been on Datadog for like 2 years. It's fine. Does what it needs to do most of the time. The APM stuff is useful if your services are instrumented well. Ours aren't in some places so YMMV.",
          "score": 1,
          "created_utc": "2026-01-18 16:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dd4fy",
          "author": "AxiomOfLife",
          "text": "I prefer grafana but you gotta fiddle with it more, datadog seems more user friendly",
          "score": 1,
          "created_utc": "2026-01-18 22:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0f1p8n",
          "author": "safetytrick",
          "text": "I've used Datadog at two different places and I've never loved it. I prefer Prometheus and Grafana.\n\nFor logs Splunk is amazing but pricey. Graylog is okay. Elk looks nice but I don't have deep experience with it.",
          "score": 1,
          "created_utc": "2026-01-19 03:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g97hn",
          "author": "samsuthar",
          "text": "100%, datadog is the best tool out there to get metrics and trace correlations during incidents. But I would suggest trying 2 to 3 tools and see which works best, especially if you check whether they give predictions before an incident happens based on predictive analysis.",
          "score": 1,
          "created_utc": "2026-01-19 09:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06r2wg",
          "author": "badaccount99",
          "text": "We just going to ignore their sales team?  Their product might be the best in the world, and I've got a huge budget I'm spending with New Relic right now, but their sales guys.... sheesh.  No thanks.\n\nNew Relic is pretty decent though.\n\nEdit:  Downvoted maybe from their sales team.  They might have the greatest product of all time, but I had them calling my work number every other day, then they started calling my personal cell.  Seriously, I've got like $200k to spend on monitoring, but you blow up my personal phone you're never getting our business, and I'm talking crap about you to my parent company too which is worth way more than I can spend.",
          "score": 0,
          "created_utc": "2026-01-17 22:38:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05gy4r",
          "author": "NeuralNexus",
          "text": "Datadog is a great tool (the best on the market in many areas, competitive and best in ecosystem in others) but it's not really worth the price at scale. \n\nYou should really consider open telemetry and what features you need and how that pipeline can look. You can get a lot of serviceability and observability with open source tooling now. You do not need every single bell and whistle. \n\nDatadog is a great 'quick fix' kind of solution that works pretty well with the defaults. But I'd really recommend you think about otel and how you can build the kind of observation stack your app needs for incident management.",
          "score": 1,
          "created_utc": "2026-01-17 18:48:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0523vy",
          "author": "danukefl2",
          "text": "Being a place with mostly vendor provided software, it's not great on that side but works fine for metrics and logs. If you have OTEL and the visibility it does work much better. The trick is not throwing too much crap into it so it can actually work, plus that can help keep prices down too.",
          "score": 1,
          "created_utc": "2026-01-17 17:40:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbszyl",
      "title": "Whatâ€™s the worst part of being on-call ?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qbszyl/whats_the_worst_part_of_being_oncall/",
      "author": "IndiBuilder",
      "created_utc": "2026-01-13 14:38:29",
      "score": 15,
      "num_comments": 30,
      "upvote_ratio": 0.78,
      "text": "For me itâ€™s often the first few minutes after the page, before I know whatâ€™s actually broken, and getting paged on weekends when I would have stepped out.\n\nCurious what that moment feels like for others?",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qbszyl/whats_the_worst_part_of_being_oncall/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzd4353",
          "author": "tobylh",
          "text": "Thankfully my on-call is *usually* pretty quiet (though I've probably jinxed it now) so for me it's less about burnout or getting woken up. \n\nIt's more the back of the mind niggling that I am on call so I'd better not go out and do that thing, or be too far from home, just on the off chance, even though nothing normally happens. It grates a bit.",
          "score": 24,
          "created_utc": "2026-01-13 15:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdpd7t",
              "author": "wampey",
              "text": "Agreed itâ€™s typically the mentality you have to take when on-call. That said, hopefully you have a back up which you can keep in touch with and depend on if the need arises. While I was doing primary/secondary on-call (now forever tertiary) I got myself to just go see movies and do things. Would notify my secondary who was cool with it. A few hours a day of a mental break is good.",
              "score": 1,
              "created_utc": "2026-01-13 16:51:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzd1deq",
          "author": "Twirrim",
          "text": "The disruption to my day/week is always the worst part. Having to lug around my laptop, having to make sure I'm not committing to anything that would make it impossible to get online quickly enough to deal with things. It gets annoying, particularly as a father. It forces my wife to have to do even more work during my on-call shifts, as it makes me unreliable. I can't commit to picking up the kid from school or activities because I can't be sure I won't be in the middle of an incident.\n\n\nWeirdly I enjoy actually handling pages. I don't dread finding out what has paged me, I get excited to go figure it out.Â  I'm very strongly problem motivated, if you throw a set of problems at me that I've got the skills to start tackling, I'm happy as can be. Doubly so if it means I get a chance to learn something.Â  A page is just a fresh challenge.Â  The flip side of that is I get extremely pissed off by false positive alarms, because I go in to the situation expecting a problem only to find nothing!\n\n\nI've been on-call in one form or another for approaching 20 years now, so it's not like the experience is new or novel either.Â ",
          "score": 19,
          "created_utc": "2026-01-13 14:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzg953c",
              "author": "realitythreek",
              "text": ">Â Weirdly I enjoy actually handling pages. I don't dread finding out what has paged me, I get excited to go figure it out.\n\nSame but mine is because ADHD. I thrive under pressure.",
              "score": 5,
              "created_utc": "2026-01-14 00:18:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlxraf",
                  "author": "NetInfused",
                  "text": "Hey fellow ADHDer :) \n\nAt least that's a useful trait for us: we handle well pressure and crisis.",
                  "score": 1,
                  "created_utc": "2026-01-14 20:56:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzoywha",
              "author": "IndiBuilder",
              "text": "For me itâ€™s spending 15-20 mins to investigate just to realise,The issue is a transient one cause by some dependencies or due to cloud provider.\n\nThat time isnâ€™t just lost investigation, itâ€™s the mental cost of uncertainty and context switching when thereâ€™s nothing actionable to do.",
              "score": 1,
              "created_utc": "2026-01-15 07:54:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdk7m2",
          "author": "hijinks",
          "text": "ya'll understand this guy is farming for market research right?\n\nthis new age of AI sucks.. anyone with a $200 sub now thinks they can topple pagerduty",
          "score": 15,
          "created_utc": "2026-01-13 16:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzok5f7",
              "author": "captainPigggy",
              "text": "does anyone actually likes being on-call, If something can reduce the pain why wouldnâ€™t we want that?",
              "score": 1,
              "created_utc": "2026-01-15 05:47:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzol0rq",
                  "author": "hijinks",
                  "text": "Because it would be nice if they said. Hey I'm building a product and would be interested in people's pains and struggles and not hiding",
                  "score": 1,
                  "created_utc": "2026-01-15 05:54:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzd5fkj",
          "author": "happyn6s1",
          "text": "3AM was the worst",
          "score": 5,
          "created_utc": "2026-01-13 15:19:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd5h6v",
          "author": "ningyakbekadu69",
          "text": "Getting paged at 3am. Also the flood of alerts sometimes.",
          "score": 4,
          "created_utc": "2026-01-13 15:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzddvc1",
          "author": "Cheap_Explorer_6883",
          "text": "For me it's mainly not being able to have my life.  \nMy sleep is lighter cause I'm \"ready\" to be called.  \nIn our case we have to be 1 hour away physically from the company in case the remote connection doesn't work. Which means not being able to do anything.  \nCant even go to the restaurant, concert, cinema or any paying stuff in case im called.  \n\nWork wise, the useless colleague/team leader who doesnt share info, takes all the technical decision and make a mess out f everything, so you have to debug undocumented things that makes no logical sense. Already navigate in a repository and know which non sense keyword to look for",
          "score": 3,
          "created_utc": "2026-01-13 15:59:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzesleh",
          "author": "tamara_henson",
          "text": "Iâ€™ve been on-call for over 20 years.  My family and I are so used to it, itâ€™s no big.  Itâ€™s part of the job and working in Tech.  Someone has to keep the lights on.  \n\nMy last job was a bit rough.  I was on-call every 3rd week in a startup.  And working 12 days in a row once a month was a bit rough.  But I was making good money so I just dealt with it.",
          "score": 3,
          "created_utc": "2026-01-13 20:00:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze15yg",
          "author": "sigmoia",
          "text": "Being on call itself. We don't build rocketships but like to pretend we do.Â \n\n\nThis 24/7 availability expectation is broken. You could argue that in case of multi region apps, deciding on downtime is hard.Â \n\n\nBut if your app runs across three regions then on call could be only during work time.Â ",
          "score": 2,
          "created_utc": "2026-01-13 17:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze1tf9",
          "author": "rmullig2",
          "text": "I don't mind the overnight calls to resolve an outage. That's how you make your bones in this profession. When you are the primary person who resolves system wide outages then it makes your job a lot more secure.",
          "score": 2,
          "created_utc": "2026-01-13 18:00:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzex99u",
          "author": "zajdee",
          "text": "The worst part for me was getting paged repeatedly for the same issues over and over again, because management did not see \"improving the systems\" a priority.",
          "score": 2,
          "created_utc": "2026-01-13 20:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfl8w9",
          "author": "bigvalen",
          "text": "The worst page is the one you get at 06:00. It wakes the kids. They get up, wander around, hassling you while you are on the laptop, on the floor, stressed, in your underwear. Three hours later, you are still there, cold, and they are crying that they haven't been fed, but you are still chatting to your bosses boss about why you are still losing $1000 a second, and no one has any idea why the rollback keeps crashing. \n\nTotally worth the $100 a weekend bonus.",
          "score": 2,
          "created_utc": "2026-01-13 22:14:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhhcsq",
          "author": "Trosteming",
          "text": "Getting page just after you felt into sleep. The adrenaline rush just ruin me for getting into sleep afterward.",
          "score": 2,
          "created_utc": "2026-01-14 04:36:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz9xxs",
              "author": "Thump241",
              "text": "That adrenaline and staying in that \"is the pager going to go off again? what about now? Please let me sleep tonight...\" headspace and just hover in that not-asleep and not-awake state till the morning alarm goes off. Lack of sleep leads me to burnout. Luckily our on-call week is offset by 1 comp day.",
              "score": 1,
              "created_utc": "2026-01-16 19:45:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlxcu9",
          "author": "NetInfused",
          "text": "The existential dread of not knowing what went wrong, and then my head starts playing the worst possible scenarios. Maybe it's my PTSD from many ransomware attacks I have seen.\n\nThat, and that i literally JUMP out of the bed scared when the pager hits.",
          "score": 2,
          "created_utc": "2026-01-14 20:54:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdeaxg",
          "author": "Pandabol",
          "text": "I used to work in a company where you were activated most of the time.\n\nThereâ€™s one time where we are opening a new market in a new country and we were activated all the time.\n\nI remember during this period of time, I was activated at 2am to look at an issue. Later we found out that it wasnâ€™t my domain but I was told to stay in the call while they find the software engineer that could possible solve this issue. \n\nThe incident IC try to call the person on duty and couldnâ€™t get him. \n\nEnded up I have to wait till 6 am and then we manage to solve this issue.\n\nIt feel like shit working the next day.",
          "score": 1,
          "created_utc": "2026-01-13 16:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdr8cz",
              "author": "IndiBuilder",
              "text": "Thats very common in large orgs, I have been in similar shit, my team handles the api gateway, no matter where the fault is we are the first once to get pulled in just to iron out that its not a gateway issue. \nEven though traces and logs all are accessible across orgs, discovering right indicators and interpreting is  still a challenge.",
              "score": 1,
              "created_utc": "2026-01-13 17:10:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0c86jx",
                  "author": "Pandabol",
                  "text": "I'm working in a relatively small company. \n\nSuck to hear that you're in a similar situation to me when I was working in my previous company. Hope you can get out as soon as possible.",
                  "score": 1,
                  "created_utc": "2026-01-18 19:05:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdqcyq",
          "author": "ScudsCorp",
          "text": "Non actionable alarms from dependent services I have no control over. \n\nOh too many connections to the user table? Ah, yes, weâ€™re all fucked then, arenâ€™t we? You want me to hop on a bridge call just to show face. Fine.",
          "score": 1,
          "created_utc": "2026-01-13 17:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzds4o6",
              "author": "IndiBuilder",
              "text": "Thats very relatable, often I have seen certain engineers are expected to be there just in case theres a need to investigate some other aspect of the incident",
              "score": 1,
              "created_utc": "2026-01-13 17:15:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzf2a3e",
          "author": "kiddj1",
          "text": "When it's your last second on call and it goes off",
          "score": 1,
          "created_utc": "2026-01-13 20:46:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfn7ga",
          "author": "418NotATeapot",
          "text": "Hands down itâ€™s that back-the-mind thought that my weekend or evening could be interrupted at any moment and I have to perform without question. Big mental tax.",
          "score": 1,
          "created_utc": "2026-01-13 22:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkdyis",
          "author": "awesomeplenty",
          "text": "Getting a P0 paged right when you are about to take a dump in the toilet. Your shit will go reverse back into you and shit time is effectively cancelled.",
          "score": 1,
          "created_utc": "2026-01-14 16:45:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbwy59",
      "title": "Operation toil increased to 30% in 2025, despite AI",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qbwy59/operation_toil_increased_to_30_in_2025_despite_ai/",
      "author": "Dramatic_Sky456",
      "created_utc": "2026-01-13 17:14:45",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.7,
      "text": "Operational toil rose to 30% in 2025 (from 25%), the first increase in five years.\nWhat resonated most from this report: the work isnâ€™t just fixing incidents anymore. Itâ€™s the extra layer around them:\n- verifying AI suggestions\n- reviewing changes more heavily\n- handling alert fatigue / ignored alerts\n- coordination overhead during incidents\n\nReport is here\nhttps://runframe.io/blog/state-of-incident-management-2025\n\nDoes this match what youâ€™re seeing on-call? Whatâ€™s driving toil up (or down) for your team in the last 12 months?",
      "is_original_content": false,
      "link_flair_text": "BLOG",
      "permalink": "https://reddit.com/r/sre/comments/1qbwy59/operation_toil_increased_to_30_in_2025_despite_ai/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzdw9s5",
          "author": "siddharthnibjiya",
          "text": "Itâ€™s not despite AI, itâ€™s also sometimes because of AI tbh.\n\nâ€” more code, accelerated feature development, lesser confidence and control on whatâ€™s going into prod. \n\nAI in ops wasnâ€™t mature enough in 2025, things will be very different by end of 2026 though based on my current usage and recent experiences.",
          "score": 7,
          "created_utc": "2026-01-13 17:35:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwm3t",
          "author": "theubster",
          "text": "AI has just made my job harder. Its good for doing stuff like suggesting ways to clean up language for documentation or announcements. Even it's prototyping capabilities are pretty rad.\n\nBut, the fundamental issue is that it's reducing organizational knowledge about how our systems and services work. Engineers aren't able to reason about a service they know little about, and only had a slight hand in building. \n\nI would much prefer that AI didn't try to touch code.",
          "score": 6,
          "created_utc": "2026-01-13 17:36:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwfvv",
          "author": "siddharthnibjiya",
          "text": "Also, regarding the data/article â€” I wouldnâ€™t cite this anywhere â€” article was much like AI aggregation of existing reports in the article â€” while itâ€™s a great narrative to build, if one brings together/cherrypicks the most stark statistics from different report (each on different set of users), the new report will very likely make it look more drastic than it is.",
          "score": 2,
          "created_utc": "2026-01-13 17:36:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl2zjm",
              "author": "Dramatic_Sky456",
              "text": "Fair critique on face value. The full methodology is in the report: 20+ industry reports analyzed + 25 team interviews (July-Dec 2025). Each stat in the index table is sourced. We tried to be transparent about what came from where. Appreciate you keeping us honest.",
              "score": 1,
              "created_utc": "2026-01-14 18:37:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzoluqz",
          "author": "iliasd15",
          "text": "How dare you ğŸ˜­ğŸ˜­ğŸ˜­",
          "score": 1,
          "created_utc": "2026-01-15 06:01:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfc42f",
      "title": "What usually causes observability cost spikes in your setup?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfc42f/what_usually_causes_observability_cost_spikes_in/",
      "author": "jopsguy",
      "created_utc": "2026-01-17 12:34:45",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.73,
      "text": "Weâ€™ve seen a few cases where observability cost suddenly jumps without an obvious infra change.\n\nIn hindsight, itâ€™s usually one of:\n\n* a new high-cardinality label\n* log level changes\n* sampling changes that werenâ€™t coordinated\n\nFor people running OpenTelemetry in production:\n\n1. how do you detect these issues early?\n2. do you have any ownership model for telemetry cost?\n\nInterested in real-world approaches, not vendor answers.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfc42f/what_usually_causes_observability_cost_spikes_in/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o03krml",
          "author": "hawtdawtz",
          "text": "You monitor it with more observability and alerts.",
          "score": 10,
          "created_utc": "2026-01-17 13:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03mc95",
          "author": "Sea_Refrigerator5622",
          "text": "Definitely high cardinality labels. Monitoring and aggressive filtering are the only way. Tell teams you will filter first and ask questions later.",
          "score": 5,
          "created_utc": "2026-01-17 13:18:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03prk5",
          "author": "jjneely",
          "text": "I build dashboards around ownership and link the volume of telemetry from a service to how much that costs from the vendors.  This usually gets the team and the manager's attention.",
          "score": 3,
          "created_utc": "2026-01-17 13:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ap1pq",
          "author": "kusanagiblade331",
          "text": "One of my experience with cost spike was due to Splunk. High log ingestion spike event can be quite random. Usually, it can be due new logs being added, new features being added or some special incident events related.\n\nAs for some of the techniques that I have used in the past to control log ingestion spike, you can find them [here](https://www.linkedin.com/pulse/how-reduce-splunk-cloud-cost-without-losing-yao-hong-kok-zueac/?trackingId=Xi4K2gDCQzKrwI1tQZGzSA%3D%3D).",
          "score": 2,
          "created_utc": "2026-01-18 14:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g8q80",
          "author": "samsuthar",
          "text": "I think there are multiple ways to control costs.   \n1. Look at one week pattern and see which host or services affecting   \n2. Have ingestion control in place   \n3. Set up alerts with a threshold so you can get notified.",
          "score": 2,
          "created_utc": "2026-01-19 09:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06r113",
          "author": "foobarstrap",
          "text": "top issue: we do a node rollout in our Kubernetes cluster: replacing all the nodes, moving all pods around.",
          "score": 1,
          "created_utc": "2026-01-17 22:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dk972",
          "author": "maxfields2000",
          "text": "We have monitors on our collectors as well as on ingest (we use a vendor, but are converting the pipeline to OpenTel).\n\nThat said, our collectors block unknown/unapproved labels/tags on metrics in production and all tags/labels/new metrics that go to production go through a cardinality review, so it's rare we are \"surprised\" by a new tag/label.  We can be surprised by a sudden uptick in unique values used on already approved tags.  (side note, in QA/test environments tags/labels are essentially unique value rate limited so they can be actively developed without approval).\n\nSo our causes tend to be new services launching using existing tags/labels that increase cardinality.  Sudden spikes on logs for services that don't use sampling (we have a few.. it's usually actually loadtests, not production, that have gone sideways/spiraled out of control).\n\nWe do use some host based vendor monitoring as well, and someone accidentally deploying/activating hosts for monitoring can be another cause.\n\nIn all cases we have monitors/alerts that catch unexpected rate changes within minutes to hours depending on the type of cost and within 24/48 hours we've worked out containment. Our team has authority to immediately block/disable any high cost monitoring that lacks approval or we can't find the owner of as well. \n\nBy catching all cost issues within 12-24 hours we rarely have a surprise bill/cost.",
          "score": 1,
          "created_utc": "2026-01-18 23:04:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcq3t4",
      "title": "I'm building a Python CLI tool to test Google Cloud alerts/dashboards. It generates historical or live logs/metrics based on a simple YAML config. Is this useful or am I reinventing the wheel unnecessarily?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qcq3t4/im_building_a_python_cli_tool_to_test_google/",
      "author": "fedmest",
      "created_utc": "2026-01-14 15:25:03",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nIâ€™ve been working on an open-source Python tool I decided to call theÂ **Observability Testing Tool**Â for Google Cloud, and Iâ€™m at a point where Iâ€™d love some community feedback before I sink more time into it.\n\n**The Problem the tool aims to solve:**Â I am a Google Cloud trainer and I was writing course material for an advanced observability querying/alerting course. I needed to be able to easily generate great amounts of logs and metrics for the labs. I started writing this Python tool and then realised it could probably be useful more widely. I'm thinking when needing to validate complex LQL / Log Analytics SQL / PromQL queries or when testing PagerDuty/email alerting policies for systems where \"waiting for an error\" isn't a strategy, and manually inserting log entries via the Console is tedious.\n\nI looked at tools likeÂ `flog`Â (which is great), but I needed something that could natively talk to the Google Cloud API, handle authentication, and generateÂ **metrics**Â (Time Series data) alongside logs.\n\n**What I built:**Â It's a CLI tool where you define \"Jobs\" in a YAML file. It has two main modes:\n\n1. **Historical Backfill:**Â \"Fill the last 24 hours with error logs.\" Great for testing dashboards and retrospective queries.\n2. **Live Mode:**Â \"Generate a Critical error every 10 seconds for the next 5 minutes.\" Great for testing live alert triggers.\n\nIt supports variables, so you can randomize IPs or fetch real GCE metadata (like instance IDs) to make the logs look realistic.\n\n**A simple config looks like this:**\n\n    loggingJobs:\n      - frequency: \"30s ~ 1m\"\n        startTime: \"2025-01-01T00:00:00\"\n        endOffset: \"5m\"\n        logName: \"application.log\"\n        level: \"ERROR\"\n        textPayload: \"An error has occurred\"\n\nBut things can get way more complex.\n\n**My questions for you:**\n\n1. **Does this already exist?**Â Is there a standard tool for \"observability seeding\" on GCP that I missed? If thereâ€™s an industry standard that does this better, Iâ€™d rather contribute to that than maintain a separate tool.\n2. **Is this a real pain point?**Â Do you find yourselves wishing you had a way to \"generate noise\" on demand? Or is the standard \"deploy and tune later\" approach usually good enough for your teams?\n3. **How would you actually use it?**Â Where would a tool like this fit in your workflow? Would you use it manually, or would you expect to put it in a CI pipeline to \"smoke test\" your monitoring stack before a rollout?\n\n**Repo is here:**Â [https://github.com/fmestrone/observability-testing-tool](https://github.com/fmestrone/observability-testing-tool)\n\n**Overview article on medium.com:**Â [https://blog.federicomestrone.com/dont-wait-for-an-outage-stress-test-your-google-cloud-observability-setup-today-a987166fcd68](https://blog.federicomestrone.com/dont-wait-for-an-outage-stress-test-your-google-cloud-observability-setup-today-a987166fcd68)\n\nThanks for roasting my code (or the idea)! ğŸ˜€",
      "is_original_content": false,
      "link_flair_text": "HELP",
      "permalink": "https://reddit.com/r/sre/comments/1qcq3t4/im_building_a_python_cli_tool_to_test_google/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzmbt1p",
          "author": "StrangeStrider",
          "text": "Not sure if it applies, but in the cybersecurity role of Detection Engineering, we test all of our security rules by using things like Atomic Red Team or sample intrusion logs that we inject on demand to test the rule.\n\nhttps://github.com/redcanaryco/atomic-red-team\nhttps://redcanary.com/blog/testing-and-validation/detection-validation/",
          "score": 1,
          "created_utc": "2026-01-14 21:59:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfof5o",
      "title": "How many meetings / ad-hoc calls do you have per week in your role?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfof5o/how_many_meetings_adhoc_calls_do_you_have_per/",
      "author": "Ok_Discipline3753",
      "created_utc": "2026-01-17 20:48:55",
      "score": 5,
      "num_comments": 10,
      "upvote_ratio": 0.73,
      "text": "Iâ€™m trying to get a realistic picture of what the day-to-day looks like. Iâ€™m mostly interested in:\n\n1. number of scheduled meetings per week\n2. how often you get ad-hoc calls or â€œcan you jump on a call now?â€ interruptions\n3. how often you have to explain your work to non-technical stakeholders?\n4. how often you lose half a day due to meetings / interruptions\n\nhow many hours per week are spent in meetings or calls?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfof5o/how_many_meetings_adhoc_calls_do_you_have_per/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o066r6y",
          "author": "interrupt_hdlr",
          "text": "big tech numbers:  \n  \n1. about 6-7 scheduled meetings per week  \n2. every other day  \n3. rarely  \n4. 2-3 days per week",
          "score": 5,
          "created_utc": "2026-01-17 20:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06lf50",
          "author": "hawtdawtz",
          "text": "1. On average 6-7 outside of standups like the other guy said. Iâ€™m in two standups which adds another ~3 hours a week\n2. Roughly 1-3 times a week\n3. Varies, idk 0-3 times a week \n4. 30% of the time",
          "score": 2,
          "created_utc": "2026-01-17 22:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06ym0p",
          "author": "Escatotdf",
          "text": "1. 6-7 a week\n2. 2-3 a week\n3. About once a week\n4. 2-3 times a week\n\n8-11 hours of meetings total per week according to my calendar. As a senior SRE",
          "score": 2,
          "created_utc": "2026-01-17 23:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06a5i8",
          "author": "Ecstatic-Minimum-252",
          "text": "30 minutes meeting once a week",
          "score": 1,
          "created_utc": "2026-01-17 21:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08pz5b",
          "author": "SomeGuyNamedPaul",
          "text": "1. about 7, including daily stand-ups and I'm absolutely including them because they suck down 90 minutes and parts of my soul.  It's so terrible.\n\n2. very infrequently.  It's basically only a text chat with a co-worker extends past 4 exchanges \n\n3. Never\n\n4. Infrequently, but when it does happen my morale is so crushed that the rest of the day is wildly unproductive anyway.",
          "score": 1,
          "created_utc": "2026-01-18 05:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b9uik",
          "author": "Sufficient-Bad-7037",
          "text": "1. Once in a week\n2. Very infrequently \n3. Rarely\n4. Mostly async slack interruptions",
          "score": 1,
          "created_utc": "2026-01-18 16:25:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bq62a",
          "author": "Fearless_Meat_1655",
          "text": "1. 15-17 a week. In my previous org we had 20 in a weekÂ \n2. Ad hoc is 2-3 and lasts few minsÂ ",
          "score": 1,
          "created_utc": "2026-01-18 17:43:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0d81w6",
          "author": "raisputin",
          "text": "Too often",
          "score": 1,
          "created_utc": "2026-01-18 22:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ekf8z",
          "author": "FullStackDestroyer",
          "text": "1.\tâ number of scheduled meetings per week\n> 10-15 hrs\n\n2.\tâ how often you get ad-hoc calls or â€œcan you jump on a call now?â€ interruptions\n> 2-3x per day\n\n3.\tâ how often you have to explain your work to non-technical stakeholders?\n> every single day.  Seriously.\n\n4.\tâ how often you lose half a day due to meetings / interruptions\n> 2-3x per week.  There are ways to improve, which Iâ€™m working on, but change is hard.  Get a helmet. \n\nCreds: lead SRE/release at a public company, lots of AI dev, hundreds of services, dozens of external dependencies, software fails SO regularly, but we make it workâ€¦well.",
          "score": 1,
          "created_utc": "2026-01-19 02:19:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lpl7z",
          "author": "FreshEmd",
          "text": "I run two teams while also being hands on IC work, and I probably have on average 6-8 meetings per day. Ad-hoc calls would be difficult to count, but it's often.  These are not just standard meetings, they are often specific to a topic for solution design. All my hands on work happens after hours for 15-30 min I get between meetings. Which means if it wasn't for AI tools I don't think I'd be able to get anything meaningful done. Don't be me.",
          "score": 1,
          "created_utc": "2026-01-20 03:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdfauz",
      "title": "I need to vent about process",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qdfauz/i_need_to_vent_about_process/",
      "author": "raslan81",
      "created_utc": "2026-01-15 09:40:40",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.6,
      "text": "Let's moan about process.  \n  \nProcess in tech feels like an onion. As products mature, more and more layers get added, usually after incidents or post mortems. Each layer is meant to make things safer, but we almost never measure what that extra process actually costs.\n\nWhen a post mortem leads to a new process, what we are really doing is slowing everyone down a little bit more. We do not track the impact on developer frustration, speed of execution, or the people who quietly leave because getting anything done has become painful.\n\nIf you hire good people, you should be able to accept that some things will go wrong and move on, rather than trying to process every failure out of existence. Most companies only reward the people who add process, because it looks responsible and is easy to defend. The people who remove process take the risk, and if anything goes wrong they get the blame, even if the team delivers faster and with fewer people afterwards.\n\nThat imbalance is why process only ever seems to grow, and why innovation slowly gets squeezed out.\n\n\n\n**Note:** thank you to Chatgpt for summarising my thoughts so eloquently \n\nEx SRE, now a Product Manager in tech.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qdfauz/i_need_to_vent_about_process/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzr3bj1",
          "author": "pdp10",
          "text": "> more and more layers get added, usually after incidents or post mortems. Each layer is meant to make things safer, but we almost never measure what that extra process actually costs.\n\nExactly correct.\n\n> Most companies only reward the people who add process, because it looks responsible and is easy to defend.\n\nAlso exactly correct.\n\nAlthough it seems a bit cliche for SRE, I do have something of an answer for you: writing code. When we've added automation to a process, it's usually been successful in the long run. Whereas when we add manual human steps, it's most often been a failure in the long run.\n\nInterestingly, in a well-lubricated modern team, it usually only requires one Individual Contributor to add automation, whereas it typically requires a higher authority to assign additional process to humans.\n\nI'm happily surprised that ChatGPT produced such an eloquent result.",
          "score": 13,
          "created_utc": "2026-01-15 16:26:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hefeo",
              "author": "FostWare",
              "text": "More layers and more process should usually mean additional preflight checks and clearer documentation until four confluence migrations and a bunch of staff turnover later, and you now canâ€™t remember why the test is there but it damn well runs right every time.",
              "score": 3,
              "created_utc": "2026-01-19 14:44:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrue9p",
          "author": "daedalus_structure",
          "text": "Customers expect their data to be secure, the SLAs provided to them to be fulfilled, and investors expect that the money they are spending on engineering hours, the most expensive part of making software, isn't wasted. \n\nWhen you just depend on \"good people will do the right thing\", you get as many different ideas of what that right thing is as you have people. \n\nDoing things 50 different right ways is more damaging to an engineering organization than establishing a process that 45 will follow with professional attention and 5 will buck, because dealing with non-compliance is straightforward because expectations have been clearly set instead of \"I dunno, do what you want\". \n\nChildren always want to run with scissors and sometimes adults in the room need to tell them no. We completely understand that you want to cut fast. But Jimothy over there has one eye, and so we're not doing that anymore.",
          "score": 7,
          "created_utc": "2026-01-15 18:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00ma6a",
          "author": "WheredTheSquirrelGo",
          "text": "I hear you on the pain point, but 98% of people that are authorized to implement a process have no actual experience of good process design.\n\nGood process design eliminates waste and reduces output variation.Â Good process design champions enablement in a manner that is mindful of innovation.Â Itâ€™s an optimization model balancing the constraints to get the most value.\n\nRyan f9 does an excellent job explaining this in relation to Hondas manufacturing story.Â https://youtu.be/0LfbsW-5tAk?si=A72vlZWIBBQ4eMFt\n\nDonâ€™t blame process for your pain, blame the human that doesnt understand process design principles. But ultimately participate in enabling good process design, be the change.",
          "score": 3,
          "created_utc": "2026-01-16 23:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqd8l4",
          "author": "the_packrat",
          "text": "Process is almost always the wrong way to make things safer as well. Where folks are adding process, it's usually because really improving things can't be conceived of by non-technical people, or because it's not considered worth the cost, so process is the plan B.\n\nIt has all the costs you mention, but typically is either neutral or more commonly actually makes things worse.",
          "score": -2,
          "created_utc": "2026-01-15 14:23:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc2k2u",
      "title": "Looking for a test system that can run in microK8s or Kind that produces mock data.",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qc2k2u/looking_for_a_test_system_that_can_run_in/",
      "author": "trudesea",
      "created_utc": "2026-01-13 20:35:55",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "Hi,\n\nWeird question I know, but the reason is I was laid off end of last month after 27yrs as an Architect/Platform Engineer.   I was basically an SRE but didn't have the title.  \n\nBefore I separated from the company I was working on implementing istio/opentelemetry/prometheus/graphana/tempo and integrating with JIRA and Gitlab \n\nIt was just in the design phase but the systems where there GKE/AWS test clusters running our platform so I had plenty of data to build this out.\n\nSo now all I have is my home lab and I want to build it out so I can test and improve my design.  Also buff up on my Python as we didn't really use it.  \n\nIs there such a thing that just runs in the cluster and produces logs, simulates issues including OOMs/pod restarts/etc so you can test/rate your design?\n\nThanks for any info.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qc2k2u/looking_for_a_test_system_that_can_run_in/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzi584m",
          "author": "s5n_n5n",
          "text": "Not a weird question at all, apps to simulate load / issues, etc. is something a lot of teams and people require. I started to collect those [in this repository](https://github.com/causely-oss/awesome-synthetic-apps), here are a few you might be the most interested in:\n\n* [https://github.com/open-telemetry/opentelemetry-demo](https://github.com/open-telemetry/opentelemetry-demo), official OpenTelemetry demo, has some feature flags to simulate issues (most of them are application related, not infra)\n* [https://github.com/causely-oss/chaosmania](https://github.com/causely-oss/chaosmania), has some mocks for CPU and Memory issues, which you can use to create OOms etc.\n\nThere are a lot more, and the list on that repo is most likely incomplete, so if you find more or anyone knows more, I am happy to build it out.\n\nAdditionally, what you can do is run any of those demo apps and combine it with some chaos testing solutions, I used pumba (https://github.com/alexei-led/pumba/) in the past, since it fit my needs and was easy to get started, but there are also solutions like Litmus or ChaosMesh, but I have not a lot of experience with them.",
          "score": 3,
          "created_utc": "2026-01-14 07:49:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl1sn2",
          "author": "Charming_Speaker_908",
          "text": "https://coralogix.github.io/workshops/#opentelemetry-collector-tools\n\nThis might help",
          "score": 1,
          "created_utc": "2026-01-14 18:32:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}