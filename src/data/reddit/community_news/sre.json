{
  "metadata": {
    "last_updated": "2026-01-19 16:49:57",
    "time_filter": "week",
    "subreddit": "sre",
    "total_items": 11,
    "total_comments": 84,
    "file_size_bytes": 87287
  },
  "items": [
    {
      "id": "1qbd255",
      "title": "New term \"Claude Hole\"",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qbd255/new_term_claude_hole/",
      "author": "kellven",
      "created_utc": "2026-01-13 00:54:28",
      "score": 256,
      "num_comments": 22,
      "upvote_ratio": 0.97,
      "text": "I run SRE/Ops at a small tech company and we had a doozy today.   \n  \nA \"Claude Hole\"  is when engineer is troubleshooting or developing code with Claud/llm that they don't understand and end up in a different zip code from the actual solution.   \n\nExample: We had an engineer today run into a bug with  CNPG template, due to a really simple value miss they didn't set the AWS account number correctly in the service account annotation. Fairly easy to spot due to the cluster throwing IAM errors. \n\nThey somehow ended up submitting a PR changing the OICD for EVERY SERVICE ACCOUNT in there org. SRE blocked the PR and spent the next hour trying to figure out what the hell this engineer was actually trying to do. \n\nOn of the SRE's described it as goaltending which I thought was apt. \n\nStay safe our there buddies , shits getting weird. \n\nSide note, mods we need a horror story flare . ",
      "is_original_content": false,
      "link_flair_text": "HORROR STORY",
      "permalink": "https://reddit.com/r/sre/comments/1qbd255/new_term_claude_hole/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nz9pnfv",
          "author": "Affectionate-Bit6525",
          "text": "I almost ended up in a Claude hole today when it recommended adding some very esoteric code to work around a bug.  Turns out Iâ€™d added some unintentional white space in a string.",
          "score": 40,
          "created_utc": "2026-01-13 01:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza5ih8",
          "author": "kennetheops",
          "text": "shits getting nutty. I just had to help consult a large company on what the hell their dev team did to spent $250k on aws. Claude Crater might be the more accurate term for what we are about to face",
          "score": 43,
          "created_utc": "2026-01-13 02:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzb60z1",
              "author": "Pethron",
              "text": "A whole new meaning for â€œblast radiusâ€",
              "score": 9,
              "created_utc": "2026-01-13 06:25:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdafg3",
                  "author": "kennetheops",
                  "text": "Shady Claudes",
                  "score": 2,
                  "created_utc": "2026-01-13 15:43:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz9r2jv",
          "author": "nullset_2",
          "text": "Goaltending is actually the right behavior. All of this is a team effort, we're all accountable for AI code shit.",
          "score": 33,
          "created_utc": "2026-01-13 01:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzduuvb",
          "author": "hawtdawtz",
          "text": "Added the flare just for you ðŸ˜Š",
          "score": 12,
          "created_utc": "2026-01-13 17:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbl7h7",
          "author": "fuckingredditman",
          "text": "people need to realize that the autoregressive nature of LLMs makes them terrible at long-term troubleshooting sessions where you may get misled/run into red herings frequently. you can use them for short sessions and they might spot something, but if they don't: don't keep trying. it will only mislead you.",
          "score": 10,
          "created_utc": "2026-01-13 08:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdjxzd",
              "author": "Sure_Stranger_6466",
              "text": "I agree with this take. Wasting valuable tokens debugging code that doesn't work created by humans. If you can't debug it on your own Claude makes for a terrible debugger, although the code it writes usually works for my purposes.",
              "score": 2,
              "created_utc": "2026-01-13 16:26:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzafjmj",
          "author": "potatohead00",
          "text": "Maybe \"Claude hole\" should be the derogatory term used for the human who pulls this garbage?",
          "score": 17,
          "created_utc": "2026-01-13 03:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzd4fy7",
              "author": "Unscene",
              "text": "Devs sending me terraform snippets of obvious AI BS that are completely wrong and make no sense, and asking me \"can't you just do it this way?\" with douchey confidence. That guy is a Claude-Hole.",
              "score": 5,
              "created_utc": "2026-01-13 15:14:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdmv8j",
                  "author": "kellven",
                  "text": "I like when it hallucinates IAM polices/actions that donâ€™t exist , which seems to be all the god damn time.",
                  "score": 5,
                  "created_utc": "2026-01-13 16:40:10",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzbreqp",
          "author": "bhannik-itiswatitis",
          "text": "rule is, if itâ€™s too complicated, youâ€™re missing something",
          "score": 6,
          "created_utc": "2026-01-13 09:43:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd2zx9",
          "author": "duebina",
          "text": "It's important to tell it that you want to do TDD development methods. It keeps it on track and in scope.",
          "score": 3,
          "created_utc": "2026-01-13 15:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzup8f0",
          "author": "HacksYouMe",
          "text": "we recently had a nightmare where our engineers wrote entire cleanup cron service for oidc token that can delete tests connections, ended up deleting refresh token for all the services that are 8 hours older. so Horror flare is a fair request ðŸ¤£",
          "score": 1,
          "created_utc": "2026-01-16 03:02:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05cvsy",
          "author": "Intrepid-Stand-8540",
          "text": "\\> On of the SRE's described it as goaltending which I thought was apt.\n\nWhat does \"goaltending\" mean?",
          "score": 1,
          "created_utc": "2026-01-17 18:30:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07grjb",
              "author": "Tren898",
              "text": "Getting in front of the net. Stopping the organization from getting scored on, by in this case, a really bad PR",
              "score": 1,
              "created_utc": "2026-01-18 00:52:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbjc6e",
          "author": "ceasars_wreath",
          "text": "Best way when using AI is to ask it reason out every change in case you donâ€™t review it, this is more of a negligent SRE than AI issue(which canâ€™t do much without entire context)",
          "score": 1,
          "created_utc": "2026-01-13 08:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg9s29",
          "author": "__WalterWeizen__",
          "text": "Good grief, why isn't there linting, guardrails, and testing in place to avoid this very thing? Why wasn't the LLM self documenting the changes being made? \n\nðŸ™„",
          "score": -1,
          "created_utc": "2026-01-14 00:22:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzganw9",
              "author": "kellven",
              "text": "In fairness the system worked and our process blocked it , but the change wouldnâ€™t really be picked up by a linter , and the llms documentation on the change was baiscly technobabble about k8 service accounts",
              "score": 2,
              "created_utc": "2026-01-14 00:26:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzgfj2h",
                  "author": "__WalterWeizen__",
                  "text": "So, there was no one there capable of correcting the LLM or refining the prompts? Also, k8s is my particular specialty, so now I'm *really* intrigued. \n\nFrankly, if the documentation generated by the LLM is that far off base, then I'm concerned about prompt engineering. \n\nThe linter & associated testing suite could easily be configured to pick up issues in CI.",
                  "score": -1,
                  "created_utc": "2026-01-14 00:53:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qfhnox",
      "title": "Datadog pricing aside, how good is it during real incidents",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfhnox/datadog_pricing_aside_how_good_is_it_during_real/",
      "author": "HotelBrilliant2508",
      "created_utc": "2026-01-17 16:29:29",
      "score": 70,
      "num_comments": 25,
      "upvote_ratio": 0.96,
      "text": " Considering Datadog setting aside the pricing debate for a second - how does it actually perform when things are on fire?\n\nIs the correlation between metrics and traces actually useful?\n\nWant to hear from people who've used it during actual incidents. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfhnox/datadog_pricing_aside_how_good_is_it_during_real/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o055utj",
          "author": "rolexboxers",
          "text": "The real question is does your team actually know how to use it under pressure? That matters way more than which specific platform you're on.",
          "score": 29,
          "created_utc": "2026-01-17 17:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bcpvm",
          "author": "AccountEngineer",
          "text": "Used a few different ones at different companies. They all have strengths and weaknesses. None of them are going to magically solve incidents for you if that's what you're asking.\n\nWhat specific use case are you trying to optimize for?",
          "score": 29,
          "created_utc": "2026-01-18 16:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04v7h2",
          "author": "Mmmm618",
          "text": "Yeah, it holds up. The correlation stuff isn't bullshit - you can actually track an issue from alert to root cause without wanting to throw your laptop.",
          "score": 14,
          "created_utc": "2026-01-17 17:07:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06miro",
              "author": "Mangudai_11",
              "text": "And what about predicting events? Before they happen and before I throw away my laptop xD",
              "score": 1,
              "created_utc": "2026-01-17 22:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o07or8f",
          "author": "Grandpabart",
          "text": "If incident response is your actual priority, Datadog will tell you something is broken, but you're going to need an action layer (e.g. an IDP like Port) to make sure the right owners of the problem are on it and everyone knows their role/what to do/how.",
          "score": 14,
          "created_utc": "2026-01-18 01:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04sxzs",
          "author": "engineered_academic",
          "text": "Datadog is probably the best tool but like any tool you have to configure it properly and be familiar with it so that you can wield it during incidents. Doing chaos engineering experiments to test your runbooks and exercise those incident muscles is important. It definitely isn't fire and forget, you need to have a proper strategy and vision around logs and traces as well as controlling and justifying costs.",
          "score": 51,
          "created_utc": "2026-01-17 16:57:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o054l4w",
              "author": "hijinks",
              "text": "i 2nd this.. i run a consulting company that specializes in o11y and helping people with spend.\n\nIf DD was half the price then most would be using it. In all my time I've never heard a company say we hate DD the service. It's always we hate DD pricing.",
              "score": 19,
              "created_utc": "2026-01-17 17:51:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08ko8z",
                  "author": "tcpWalker",
                  "text": "I feel like I have heard about companies being talked into it by management firms and then having the agents cause incidents. But I am not sure why and was not running those incidents. I suppose like with any new major service, do your PoCs and manage your risk as you gain familiarity with the project.  \n  \n(And the ordinary tradeoffs about building in-house vs outsourcing to a vendor apply)",
                  "score": 1,
                  "created_utc": "2026-01-18 04:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04zeep",
              "author": "stikko",
              "text": "100% this. Donâ€™t expect to be able to spew a bunch of garbage in and get magic incident resolutions back out.  Strategy from SRE and discipline from the app teams are key.",
              "score": 3,
              "created_utc": "2026-01-17 17:27:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o050u6x",
              "author": "Fattswindstorm",
              "text": "Yes itâ€™s a great tool. Just be mindful when you are about to start a trial, they are gonna be up your ass to get agents installed so be prepared.  Have the proper doors open, and a strategy to get the agents installed.  \n\n- Have a basic tagging scheme ready.  Like define your services, applications, environments, etc. \n- Know what logs you want ingested, and determine which ones are important and which ones are noise.  Youâ€™ll want to filter out the noise fore ingesting the logs. \n\nWe enabled DataDog the same time we were doing a lift an shift into Aws and both landed on my lap.  It was very chaotic and hard trying to figure all this out during the trial period. So if you have the room, give an engineer or two the room to set this up with some sort of framework ready for guiding the implementation",
              "score": 2,
              "created_utc": "2026-01-17 17:34:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06vqeu",
              "author": "Hebrewhammer8d8",
              "text": "It is fire and forget if the lead engineer decides to leave and take another job with little documentation, and management expects that we can do what lead engineer did.",
              "score": 1,
              "created_utc": "2026-01-17 23:01:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o059lgn",
          "author": "jimmyjohns69420xl",
          "text": "at the user layer itâ€™s fantastic. great UI and tools.\n\nat the admin layer its ok, but youâ€™ll spend more time than you expect pulling your hair out over undocumented agent configs and such. youâ€™ll also likely end up doing annoying cost control exercises.\n\nI think the best argument against DD today is that with LLMs, the UI layer is becoming less important, and DDâ€™s pricing model disincentivizes sending complete and high-cardinality data, which is what LLMs need to behave well. itâ€™s still early on that front but something to consider.",
          "score": 4,
          "created_utc": "2026-01-17 18:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04nihz",
          "author": "whitechapel8733",
          "text": "The best IMO.",
          "score": 7,
          "created_utc": "2026-01-17 16:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04u6rl",
          "author": "theubster",
          "text": "Its better than most at anomaly detection. But, I don't trust any software enough to have thay kind of thing be the core of our detection or root cause discovery. \n\nHonestly, its dashboards are what sets it apart. The widget options & display are second to none. Plus, the sheer number of integrations, allowing it to be the one main hub of information about what's happening in our platform. \n\nUltimately, incident recovery comes down to platform knowledge and timely detection. Both of which can work with any tool. Why I like datadog for incidents is that their incident tooling make data capture easy, and gives fantastic visibility into incident trends over time.",
          "score": 2,
          "created_utc": "2026-01-17 17:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0581xs",
          "author": "IS300FANATIC",
          "text": "In my experience- Datadog has been an amazing observability tool. \n\nThe event correlation was a big deal and difference maker when reviewing their stack against others like new relic, app lnsights, signalfx and even dynatrace.  Dynatrace was actually a pretty close second imo but where they lost me is they failed to remove unused or empty data selectors from selection scope. For example - if im an Azure shop - i dont want to have to scroll past empty non-used AWS metrics and features just to find data I am ingesting and needing to filter down to. (Atleast 3ish years ago it was this way, not sure about now) + a few other small things. \nAnyway - back to the question: that quick data isolation moment in time saves time when needing to jump in quick if you didnt do the firedrill work to make those correlations or build the visibility before hand\n\nOne other piece that im quite impressed with Datadog tooling before and during an issue is how light their continuous profiler is.  Super cheap on resources and just seems to work in most cases - making it more comfortable to turn it on, let it ride in a close lower and curve some of that production inefficiency or spotting edge case bugs in production based on conditions that may not have been possible to reproduce in a lower (vendors didnt offer sandbox or test envs so production is your testing bed for feature with live traffic, not just mock data)\n\nI'm not a super huge fan of their RUM though and their syntetics could be a little more flexible. - that product suite just feels a bit clunky imo.\n\nLastly - npm has been a life saver in a k8s environment- it builds huge context into what's talking to what and can filter down pretty nicely to isolate traffic patterns so let's say \"podx is in imagepullbackoff\" - why? Well typically first thing you think is it not being able to pull its image from whatever container registry. Well I've actually caught an issue in scenarios like this where the pod was trying to hit the ACR at its public endpoint and not its private connection endpoint configure - forcing us to open up to public with restrictions to satisfy the incident, open ticket with msft and better understand the conditions which turned out to be msft side issue. - without npm in our pocket, looking at DNS queries and outbound connection attempts would have been a lot tough/slower.\n\nOthers may disagree with a few points here but they have all helped myself and my teams in their journey for reliability and they seem to win the race mostly every time. \nThey are pricey a f though nonetheless lol",
          "score": 2,
          "created_utc": "2026-01-17 18:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05ffbk",
          "author": "Ecestu",
          "text": "We've found that simpler is usually better during incidents. Too many features and fancy visualizations just add confusion when you're stressed and need to act fast.",
          "score": 1,
          "created_utc": "2026-01-17 18:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08o8y5",
          "author": "inferix",
          "text": "Im mainly into Dynatrace and wondered how it compares to Datadog?",
          "score": 1,
          "created_utc": "2026-01-18 05:00:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bdvf0",
          "author": "Relative_Taro_1384",
          "text": "Been on Datadog for like 2 years. It's fine. Does what it needs to do most of the time. The APM stuff is useful if your services are instrumented well. Ours aren't in some places so YMMV.",
          "score": 1,
          "created_utc": "2026-01-18 16:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dd4fy",
          "author": "AxiomOfLife",
          "text": "I prefer grafana but you gotta fiddle with it more, datadog seems more user friendly",
          "score": 1,
          "created_utc": "2026-01-18 22:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0f1p8n",
          "author": "safetytrick",
          "text": "I've used Datadog at two different places and I've never loved it. I prefer Prometheus and Grafana.\n\nFor logs Splunk is amazing but pricey. Graylog is okay. Elk looks nice but I don't have deep experience with it.",
          "score": 1,
          "created_utc": "2026-01-19 03:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g97hn",
          "author": "samsuthar",
          "text": "100%, datadog is the best tool out there to get metrics and trace correlations during incidents. But I would suggest trying 2 to 3 tools and see which works best, especially if you check whether they give predictions before an incident happens based on predictive analysis.",
          "score": 1,
          "created_utc": "2026-01-19 09:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06r2wg",
          "author": "badaccount99",
          "text": "We just going to ignore their sales team?  Their product might be the best in the world, and I've got a huge budget I'm spending with New Relic right now, but their sales guys.... sheesh.  No thanks.\n\nNew Relic is pretty decent though.\n\nEdit:  Downvoted maybe from their sales team.  They might have the greatest product of all time, but I had them calling my work number every other day, then they started calling my personal cell.  Seriously, I've got like $200k to spend on monitoring, but you blow up my personal phone you're never getting our business, and I'm talking crap about you to my parent company too which is worth way more than I can spend.",
          "score": 1,
          "created_utc": "2026-01-17 22:38:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05gy4r",
          "author": "NeuralNexus",
          "text": "Datadog is a great tool (the best on the market in many areas, competitive and best in ecosystem in others) but it's not really worth the price at scale. \n\nYou should really consider open telemetry and what features you need and how that pipeline can look. You can get a lot of serviceability and observability with open source tooling now. You do not need every single bell and whistle. \n\nDatadog is a great 'quick fix' kind of solution that works pretty well with the defaults. But I'd really recommend you think about otel and how you can build the kind of observation stack your app needs for incident management.",
          "score": 2,
          "created_utc": "2026-01-17 18:48:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0523vy",
          "author": "danukefl2",
          "text": "Being a place with mostly vendor provided software, it's not great on that side but works fine for metrics and logs. If you have OTEL and the visibility it does work much better. The trick is not throwing too much crap into it so it can actually work, plus that can help keep prices down too.",
          "score": 1,
          "created_utc": "2026-01-17 17:40:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb9til",
      "title": "Took me back to the Black Friday weekend I was on-call. Fml",
      "subreddit": "sre",
      "url": "https://i.redd.it/jlkpsh45yzcg1.jpeg",
      "author": "psuedored",
      "created_utc": "2026-01-12 22:42:31",
      "score": 61,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "HUMOR",
      "permalink": "https://reddit.com/r/sre/comments/1qb9til/took_me_back_to_the_black_friday_weekend_i_was/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nz9n326",
          "author": "moggg",
          "text": "[linkedin.com](http://linkedin.com)",
          "score": 6,
          "created_utc": "2026-01-13 00:54:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz91wdz",
          "author": "Cryptobee07",
          "text": "I feel youâ€¦. Glad Iâ€™m not working on one of those companies who sell stuff on thxgiving and holidays time",
          "score": 3,
          "created_utc": "2026-01-12 23:01:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz94ypu",
          "author": "hawtdawtz",
          "text": "Me working at a brokerage and seeing whenever Trump posts literally anything.",
          "score": 3,
          "created_utc": "2026-01-12 23:17:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb4qxz",
          "author": "Shama_lala",
          "text": "Yep, it's  3am just to remind you itâ€™s still Black Friday somewhere lol",
          "score": 1,
          "created_utc": "2026-01-13 06:14:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzc05v5",
              "author": "psuedored",
              "text": "It's always a SEV0 somewhere",
              "score": 1,
              "created_utc": "2026-01-13 11:03:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzda94f",
                  "author": "Shama_lala",
                  "text": "SEV0 doesnâ€™t respect time zones or holidays. it only knows vibes and pain.",
                  "score": 3,
                  "created_utc": "2026-01-13 15:42:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxpqgu",
          "author": "IndiBuilder",
          "text": "Sharing their side of stories:\n\n[On call experiences](https://www.reddit.com/r/sre/comments/1qbszyl/whats_the_worst_part_of_being_oncall/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
          "score": 1,
          "created_utc": "2026-01-16 15:35:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbszyl",
      "title": "Whatâ€™s the worst part of being on-call ?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qbszyl/whats_the_worst_part_of_being_oncall/",
      "author": "IndiBuilder",
      "created_utc": "2026-01-13 14:38:29",
      "score": 13,
      "num_comments": 30,
      "upvote_ratio": 0.76,
      "text": "For me itâ€™s often the first few minutes after the page, before I know whatâ€™s actually broken, and getting paged on weekends when I would have stepped out.\n\nCurious what that moment feels like for others?",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qbszyl/whats_the_worst_part_of_being_oncall/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzd4353",
          "author": "tobylh",
          "text": "Thankfully my on-call is *usually* pretty quiet (though I've probably jinxed it now) so for me it's less about burnout or getting woken up. \n\nIt's more the back of the mind niggling that I am on call so I'd better not go out and do that thing, or be too far from home, just on the off chance, even though nothing normally happens. It grates a bit.",
          "score": 24,
          "created_utc": "2026-01-13 15:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdpd7t",
              "author": "wampey",
              "text": "Agreed itâ€™s typically the mentality you have to take when on-call. That said, hopefully you have a back up which you can keep in touch with and depend on if the need arises. While I was doing primary/secondary on-call (now forever tertiary) I got myself to just go see movies and do things. Would notify my secondary who was cool with it. A few hours a day of a mental break is good.",
              "score": 1,
              "created_utc": "2026-01-13 16:51:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzd1deq",
          "author": "Twirrim",
          "text": "The disruption to my day/week is always the worst part. Having to lug around my laptop, having to make sure I'm not committing to anything that would make it impossible to get online quickly enough to deal with things. It gets annoying, particularly as a father. It forces my wife to have to do even more work during my on-call shifts, as it makes me unreliable. I can't commit to picking up the kid from school or activities because I can't be sure I won't be in the middle of an incident.\n\n\nWeirdly I enjoy actually handling pages. I don't dread finding out what has paged me, I get excited to go figure it out.Â  I'm very strongly problem motivated, if you throw a set of problems at me that I've got the skills to start tackling, I'm happy as can be. Doubly so if it means I get a chance to learn something.Â  A page is just a fresh challenge.Â  The flip side of that is I get extremely pissed off by false positive alarms, because I go in to the situation expecting a problem only to find nothing!\n\n\nI've been on-call in one form or another for approaching 20 years now, so it's not like the experience is new or novel either.Â ",
          "score": 19,
          "created_utc": "2026-01-13 14:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzg953c",
              "author": "realitythreek",
              "text": ">Â Weirdly I enjoy actually handling pages. I don't dread finding out what has paged me, I get excited to go figure it out.\n\nSame but mine is because ADHD. I thrive under pressure.",
              "score": 4,
              "created_utc": "2026-01-14 00:18:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlxraf",
                  "author": "NetInfused",
                  "text": "Hey fellow ADHDer :) \n\nAt least that's a useful trait for us: we handle well pressure and crisis.",
                  "score": 1,
                  "created_utc": "2026-01-14 20:56:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzoywha",
              "author": "IndiBuilder",
              "text": "For me itâ€™s spending 15-20 mins to investigate just to realise,The issue is a transient one cause by some dependencies or due to cloud provider.\n\nThat time isnâ€™t just lost investigation, itâ€™s the mental cost of uncertainty and context switching when thereâ€™s nothing actionable to do.",
              "score": 1,
              "created_utc": "2026-01-15 07:54:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdk7m2",
          "author": "hijinks",
          "text": "ya'll understand this guy is farming for market research right?\n\nthis new age of AI sucks.. anyone with a $200 sub now thinks they can topple pagerduty",
          "score": 16,
          "created_utc": "2026-01-13 16:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzok5f7",
              "author": "captainPigggy",
              "text": "does anyone actually likes being on-call, If something can reduce the pain why wouldnâ€™t we want that?",
              "score": 1,
              "created_utc": "2026-01-15 05:47:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzol0rq",
                  "author": "hijinks",
                  "text": "Because it would be nice if they said. Hey I'm building a product and would be interested in people's pains and struggles and not hiding",
                  "score": 1,
                  "created_utc": "2026-01-15 05:54:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzd5fkj",
          "author": "happyn6s1",
          "text": "3AM was the worst",
          "score": 5,
          "created_utc": "2026-01-13 15:19:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd5h6v",
          "author": "ningyakbekadu69",
          "text": "Getting paged at 3am. Also the flood of alerts sometimes.",
          "score": 5,
          "created_utc": "2026-01-13 15:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzddvc1",
          "author": "Cheap_Explorer_6883",
          "text": "For me it's mainly not being able to have my life.  \nMy sleep is lighter cause I'm \"ready\" to be called.  \nIn our case we have to be 1 hour away physically from the company in case the remote connection doesn't work. Which means not being able to do anything.  \nCant even go to the restaurant, concert, cinema or any paying stuff in case im called.  \n\nWork wise, the useless colleague/team leader who doesnt share info, takes all the technical decision and make a mess out f everything, so you have to debug undocumented things that makes no logical sense. Already navigate in a repository and know which non sense keyword to look for",
          "score": 3,
          "created_utc": "2026-01-13 15:59:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzesleh",
          "author": "tamara_henson",
          "text": "Iâ€™ve been on-call for over 20 years.  My family and I are so used to it, itâ€™s no big.  Itâ€™s part of the job and working in Tech.  Someone has to keep the lights on.  \n\nMy last job was a bit rough.  I was on-call every 3rd week in a startup.  And working 12 days in a row once a month was a bit rough.  But I was making good money so I just dealt with it.",
          "score": 3,
          "created_utc": "2026-01-13 20:00:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze15yg",
          "author": "sigmoia",
          "text": "Being on call itself. We don't build rocketships but like to pretend we do.Â \n\n\nThis 24/7 availability expectation is broken. You could argue that in case of multi region apps, deciding on downtime is hard.Â \n\n\nBut if your app runs across three regions then on call could be only during work time.Â ",
          "score": 2,
          "created_utc": "2026-01-13 17:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze1tf9",
          "author": "rmullig2",
          "text": "I don't mind the overnight calls to resolve an outage. That's how you make your bones in this profession. When you are the primary person who resolves system wide outages then it makes your job a lot more secure.",
          "score": 2,
          "created_utc": "2026-01-13 18:00:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzex99u",
          "author": "zajdee",
          "text": "The worst part for me was getting paged repeatedly for the same issues over and over again, because management did not see \"improving the systems\" a priority.",
          "score": 2,
          "created_utc": "2026-01-13 20:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfl8w9",
          "author": "bigvalen",
          "text": "The worst page is the one you get at 06:00. It wakes the kids. They get up, wander around, hassling you while you are on the laptop, on the floor, stressed, in your underwear. Three hours later, you are still there, cold, and they are crying that they haven't been fed, but you are still chatting to your bosses boss about why you are still losing $1000 a second, and no one has any idea why the rollback keeps crashing. \n\nTotally worth the $100 a weekend bonus.",
          "score": 2,
          "created_utc": "2026-01-13 22:14:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhhcsq",
          "author": "Trosteming",
          "text": "Getting page just after you felt into sleep. The adrenaline rush just ruin me for getting into sleep afterward.",
          "score": 2,
          "created_utc": "2026-01-14 04:36:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz9xxs",
              "author": "Thump241",
              "text": "That adrenaline and staying in that \"is the pager going to go off again? what about now? Please let me sleep tonight...\" headspace and just hover in that not-asleep and not-awake state till the morning alarm goes off. Lack of sleep leads me to burnout. Luckily our on-call week is offset by 1 comp day.",
              "score": 1,
              "created_utc": "2026-01-16 19:45:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlxcu9",
          "author": "NetInfused",
          "text": "The existential dread of not knowing what went wrong, and then my head starts playing the worst possible scenarios. Maybe it's my PTSD from many ransomware attacks I have seen.\n\nThat, and that i literally JUMP out of the bed scared when the pager hits.",
          "score": 2,
          "created_utc": "2026-01-14 20:54:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdeaxg",
          "author": "Pandabol",
          "text": "I used to work in a company where you were activated most of the time.\n\nThereâ€™s one time where we are opening a new market in a new country and we were activated all the time.\n\nI remember during this period of time, I was activated at 2am to look at an issue. Later we found out that it wasnâ€™t my domain but I was told to stay in the call while they find the software engineer that could possible solve this issue. \n\nThe incident IC try to call the person on duty and couldnâ€™t get him. \n\nEnded up I have to wait till 6 am and then we manage to solve this issue.\n\nIt feel like shit working the next day.",
          "score": 1,
          "created_utc": "2026-01-13 16:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdr8cz",
              "author": "IndiBuilder",
              "text": "Thats very common in large orgs, I have been in similar shit, my team handles the api gateway, no matter where the fault is we are the first once to get pulled in just to iron out that its not a gateway issue. \nEven though traces and logs all are accessible across orgs, discovering right indicators and interpreting is  still a challenge.",
              "score": 1,
              "created_utc": "2026-01-13 17:10:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0c86jx",
                  "author": "Pandabol",
                  "text": "I'm working in a relatively small company. \n\nSuck to hear that you're in a similar situation to me when I was working in my previous company. Hope you can get out as soon as possible.",
                  "score": 1,
                  "created_utc": "2026-01-18 19:05:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdqcyq",
          "author": "ScudsCorp",
          "text": "Non actionable alarms from dependent services I have no control over. \n\nOh too many connections to the user table? Ah, yes, weâ€™re all fucked then, arenâ€™t we? You want me to hop on a bridge call just to show face. Fine.",
          "score": 1,
          "created_utc": "2026-01-13 17:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzds4o6",
              "author": "IndiBuilder",
              "text": "Thats very relatable, often I have seen certain engineers are expected to be there just in case theres a need to investigate some other aspect of the incident",
              "score": 1,
              "created_utc": "2026-01-13 17:15:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzf2a3e",
          "author": "kiddj1",
          "text": "When it's your last second on call and it goes off",
          "score": 1,
          "created_utc": "2026-01-13 20:46:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfn7ga",
          "author": "418NotATeapot",
          "text": "Hands down itâ€™s that back-the-mind thought that my weekend or evening could be interrupted at any moment and I have to perform without question. Big mental tax.",
          "score": 1,
          "created_utc": "2026-01-13 22:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkdyis",
          "author": "awesomeplenty",
          "text": "Getting a P0 paged right when you are about to take a dump in the toilet. Your shit will go reverse back into you and shit time is effectively cancelled.",
          "score": 1,
          "created_utc": "2026-01-14 16:45:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbwy59",
      "title": "Operation toil increased to 30% in 2025, despite AI",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qbwy59/operation_toil_increased_to_30_in_2025_despite_ai/",
      "author": "Dramatic_Sky456",
      "created_utc": "2026-01-13 17:14:45",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.7,
      "text": "Operational toil rose to 30% in 2025 (from 25%), the first increase in five years.\nWhat resonated most from this report: the work isnâ€™t just fixing incidents anymore. Itâ€™s the extra layer around them:\n- verifying AI suggestions\n- reviewing changes more heavily\n- handling alert fatigue / ignored alerts\n- coordination overhead during incidents\n\nReport is here\nhttps://runframe.io/blog/state-of-incident-management-2025\n\nDoes this match what youâ€™re seeing on-call? Whatâ€™s driving toil up (or down) for your team in the last 12 months?",
      "is_original_content": false,
      "link_flair_text": "BLOG",
      "permalink": "https://reddit.com/r/sre/comments/1qbwy59/operation_toil_increased_to_30_in_2025_despite_ai/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzdw9s5",
          "author": "siddharthnibjiya",
          "text": "Itâ€™s not despite AI, itâ€™s also sometimes because of AI tbh.\n\nâ€” more code, accelerated feature development, lesser confidence and control on whatâ€™s going into prod. \n\nAI in ops wasnâ€™t mature enough in 2025, things will be very different by end of 2026 though based on my current usage and recent experiences.",
          "score": 7,
          "created_utc": "2026-01-13 17:35:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwm3t",
          "author": "theubster",
          "text": "AI has just made my job harder. Its good for doing stuff like suggesting ways to clean up language for documentation or announcements. Even it's prototyping capabilities are pretty rad.\n\nBut, the fundamental issue is that it's reducing organizational knowledge about how our systems and services work. Engineers aren't able to reason about a service they know little about, and only had a slight hand in building. \n\nI would much prefer that AI didn't try to touch code.",
          "score": 6,
          "created_utc": "2026-01-13 17:36:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwfvv",
          "author": "siddharthnibjiya",
          "text": "Also, regarding the data/article â€” I wouldnâ€™t cite this anywhere â€” article was much like AI aggregation of existing reports in the article â€” while itâ€™s a great narrative to build, if one brings together/cherrypicks the most stark statistics from different report (each on different set of users), the new report will very likely make it look more drastic than it is.",
          "score": 2,
          "created_utc": "2026-01-13 17:36:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl2zjm",
              "author": "Dramatic_Sky456",
              "text": "Fair critique on face value. The full methodology is in the report: 20+ industry reports analyzed + 25 team interviews (July-Dec 2025). Each stat in the index table is sourced. We tried to be transparent about what came from where. Appreciate you keeping us honest.",
              "score": 1,
              "created_utc": "2026-01-14 18:37:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzoluqz",
          "author": "iliasd15",
          "text": "How dare you ðŸ˜­ðŸ˜­ðŸ˜­",
          "score": 1,
          "created_utc": "2026-01-15 06:01:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcq3t4",
      "title": "I'm building a Python CLI tool to test Google Cloud alerts/dashboards. It generates historical or live logs/metrics based on a simple YAML config. Is this useful or am I reinventing the wheel unnecessarily?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qcq3t4/im_building_a_python_cli_tool_to_test_google/",
      "author": "fedmest",
      "created_utc": "2026-01-14 15:25:03",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nIâ€™ve been working on an open-source Python tool I decided to call theÂ **Observability Testing Tool**Â for Google Cloud, and Iâ€™m at a point where Iâ€™d love some community feedback before I sink more time into it.\n\n**The Problem the tool aims to solve:**Â I am a Google Cloud trainer and I was writing course material for an advanced observability querying/alerting course. I needed to be able to easily generate great amounts of logs and metrics for the labs. I started writing this Python tool and then realised it could probably be useful more widely. I'm thinking when needing to validate complex LQL / Log Analytics SQL / PromQL queries or when testing PagerDuty/email alerting policies for systems where \"waiting for an error\" isn't a strategy, and manually inserting log entries via the Console is tedious.\n\nI looked at tools likeÂ `flog`Â (which is great), but I needed something that could natively talk to the Google Cloud API, handle authentication, and generateÂ **metrics**Â (Time Series data) alongside logs.\n\n**What I built:**Â It's a CLI tool where you define \"Jobs\" in a YAML file. It has two main modes:\n\n1. **Historical Backfill:**Â \"Fill the last 24 hours with error logs.\" Great for testing dashboards and retrospective queries.\n2. **Live Mode:**Â \"Generate a Critical error every 10 seconds for the next 5 minutes.\" Great for testing live alert triggers.\n\nIt supports variables, so you can randomize IPs or fetch real GCE metadata (like instance IDs) to make the logs look realistic.\n\n**A simple config looks like this:**\n\n    loggingJobs:\n      - frequency: \"30s ~ 1m\"\n        startTime: \"2025-01-01T00:00:00\"\n        endOffset: \"5m\"\n        logName: \"application.log\"\n        level: \"ERROR\"\n        textPayload: \"An error has occurred\"\n\nBut things can get way more complex.\n\n**My questions for you:**\n\n1. **Does this already exist?**Â Is there a standard tool for \"observability seeding\" on GCP that I missed? If thereâ€™s an industry standard that does this better, Iâ€™d rather contribute to that than maintain a separate tool.\n2. **Is this a real pain point?**Â Do you find yourselves wishing you had a way to \"generate noise\" on demand? Or is the standard \"deploy and tune later\" approach usually good enough for your teams?\n3. **How would you actually use it?**Â Where would a tool like this fit in your workflow? Would you use it manually, or would you expect to put it in a CI pipeline to \"smoke test\" your monitoring stack before a rollout?\n\n**Repo is here:**Â [https://github.com/fmestrone/observability-testing-tool](https://github.com/fmestrone/observability-testing-tool)\n\n**Overview article on medium.com:**Â [https://blog.federicomestrone.com/dont-wait-for-an-outage-stress-test-your-google-cloud-observability-setup-today-a987166fcd68](https://blog.federicomestrone.com/dont-wait-for-an-outage-stress-test-your-google-cloud-observability-setup-today-a987166fcd68)\n\nThanks for roasting my code (or the idea)! ðŸ˜€",
      "is_original_content": false,
      "link_flair_text": "HELP",
      "permalink": "https://reddit.com/r/sre/comments/1qcq3t4/im_building_a_python_cli_tool_to_test_google/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzmbt1p",
          "author": "StrangeStrider",
          "text": "Not sure if it applies, but in the cybersecurity role of Detection Engineering, we test all of our security rules by using things like Atomic Red Team or sample intrusion logs that we inject on demand to test the rule.\n\nhttps://github.com/redcanaryco/atomic-red-team\nhttps://redcanary.com/blog/testing-and-validation/detection-validation/",
          "score": 1,
          "created_utc": "2026-01-14 21:59:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfc42f",
      "title": "What usually causes observability cost spikes in your setup?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfc42f/what_usually_causes_observability_cost_spikes_in/",
      "author": "jopsguy",
      "created_utc": "2026-01-17 12:34:45",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 0.75,
      "text": "Weâ€™ve seen a few cases where observability cost suddenly jumps without an obvious infra change.\n\nIn hindsight, itâ€™s usually one of:\n\n* a new high-cardinality label\n* log level changes\n* sampling changes that werenâ€™t coordinated\n\nFor people running OpenTelemetry in production:\n\n1. how do you detect these issues early?\n2. do you have any ownership model for telemetry cost?\n\nInterested in real-world approaches, not vendor answers.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfc42f/what_usually_causes_observability_cost_spikes_in/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o03krml",
          "author": "hawtdawtz",
          "text": "You monitor it with more observability and alerts.",
          "score": 8,
          "created_utc": "2026-01-17 13:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03mc95",
          "author": "Sea_Refrigerator5622",
          "text": "Definitely high cardinality labels. Monitoring and aggressive filtering are the only way. Tell teams you will filter first and ask questions later.",
          "score": 5,
          "created_utc": "2026-01-17 13:18:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03prk5",
          "author": "jjneely",
          "text": "I build dashboards around ownership and link the volume of telemetry from a service to how much that costs from the vendors.  This usually gets the team and the manager's attention.",
          "score": 2,
          "created_utc": "2026-01-17 13:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ap1pq",
          "author": "kusanagiblade331",
          "text": "One of my experience with cost spike was due to Splunk. High log ingestion spike event can be quite random. Usually, it can be due new logs being added, new features being added or some special incident events related.\n\nAs for some of the techniques that I have used in the past to control log ingestion spike, you can find them [here](https://www.linkedin.com/pulse/how-reduce-splunk-cloud-cost-without-losing-yao-hong-kok-zueac/?trackingId=Xi4K2gDCQzKrwI1tQZGzSA%3D%3D).",
          "score": 2,
          "created_utc": "2026-01-18 14:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g8q80",
          "author": "samsuthar",
          "text": "I think there are multiple ways to control costs.   \n1. Look at one week pattern and see which host or services affecting   \n2. Have ingestion control in place   \n3. Set up alerts with a threshold so you can get notified.",
          "score": 2,
          "created_utc": "2026-01-19 09:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06r113",
          "author": "foobarstrap",
          "text": "top issue: we do a node rollout in our Kubernetes cluster: replacing all the nodes, moving all pods around.",
          "score": 1,
          "created_utc": "2026-01-17 22:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dk972",
          "author": "maxfields2000",
          "text": "We have monitors on our collectors as well as on ingest (we use a vendor, but are converting the pipeline to OpenTel).\n\nThat said, our collectors block unknown/unapproved labels/tags on metrics in production and all tags/labels/new metrics that go to production go through a cardinality review, so it's rare we are \"surprised\" by a new tag/label.  We can be surprised by a sudden uptick in unique values used on already approved tags.  (side note, in QA/test environments tags/labels are essentially unique value rate limited so they can be actively developed without approval).\n\nSo our causes tend to be new services launching using existing tags/labels that increase cardinality.  Sudden spikes on logs for services that don't use sampling (we have a few.. it's usually actually loadtests, not production, that have gone sideways/spiraled out of control).\n\nWe do use some host based vendor monitoring as well, and someone accidentally deploying/activating hosts for monitoring can be another cause.\n\nIn all cases we have monitors/alerts that catch unexpected rate changes within minutes to hours depending on the type of cost and within 24/48 hours we've worked out containment. Our team has authority to immediately block/disable any high cost monitoring that lacks approval or we can't find the owner of as well. \n\nBy catching all cost issues within 12-24 hours we rarely have a surprise bill/cost.",
          "score": 1,
          "created_utc": "2026-01-18 23:04:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbansu",
      "title": "[Mod Post] Community Update: Proposed Rule Changes & Feedback Wanted",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qbansu/mod_post_community_update_proposed_rule_changes/",
      "author": "thecal714",
      "created_utc": "2026-01-12 23:15:51",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.78,
      "text": "Hey everyone! Hope youâ€™re all doing well so far in 2026.\n\nAs part of our ongoing effort to keep r/sre a valuable, welcoming, and engaging space for discussions around Site Reliability Engineering, weâ€™ve been reviewing how the subreddit is working and thinking about how to make it even better. Over the past year, this community has grown in some really exciting ways:\n\n* We've grown by \\~9.9k members\n* There were \\~1k more posts made last year than the year before\n* There were \\~12.9k more comments made last year than the year before\n\n# Proposed Rules Changes\n\nAlthough against the rules, there seems to be a lot of engagement with posts asking for interview prep advice and how to get into SRE. As such, we're creating a survey to see how you feel about modifying the rules around there.\n\nAdditionally, we're seeing *lots* of reports on promotional content and posts that seem to be farming for feedback to improve products. The survey will cover these as well.\n\nPlease see that here: [https://docs.google.com/forms/d/e/1FAIpQLSds751nKsP3nb1lFOiAdkwXVtmAO2e4rzuPGNJ9y9gZ-ksZ7A/viewform?usp=dialog](https://docs.google.com/forms/d/e/1FAIpQLSds751nKsP3nb1lFOiAdkwXVtmAO2e4rzuPGNJ9y9gZ-ksZ7A/viewform?usp=dialog)\n\n# What Topics Would You Like to See More?\n\nWeâ€™re always looking to make the subreddit more useful and relevant to you. Let us know what topics youâ€™d like to see more of. Ideas we've spitballed include:\n\n* Incident retrospectives and blameless learning\n* Career advice & SRE job-related content\n* Deep dives into reliability engineering practices\n* Case studies and war stories\n* Weekly/monthly discussion threads\n\nDrop a comment below with ideas â€” the more specific, the better!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qbansu/mod_post_community_update_proposed_rule_changes/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzdiicx",
          "author": "pdp10",
          "text": "Reddit has both /r/CScareerquestions and /r/ITcareerquestions, where it seems like most of the career-path questions should be channeled. After all, SRE (and DevOps) aren't so much roles, as practices.",
          "score": 3,
          "created_utc": "2026-01-13 16:20:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr481b",
          "author": "wfrced_bot",
          "text": "> Over the past year, this community has grown in some really exciting ways\n\nThere are fewer active people here than there were a year ago. I wouldn't say the community is completely dead, but organic posts get drowned in noise. I'm not even talking about quality posts (which, frankly, I don't remember seeing many of), just something that isn't generated by an LLM or fishing for engagement with low-effort slop. I don't know how this sub became a bot honeypot considering it's not even that active, but logic clearly isn't a constraint for people engaging in such practices.\n\nThe user/post/comment statistics are misleading and could be (and probably are) bots. Out of ten random posts at least five will be bots talking to each other trying to sell something (revolutionary idea of an incident management platform with AI insights).\n\nKarma/account age requirements should at least put out the fire.\n\n> Although against the rules, there seems to be a lot of engagement with posts asking for interview prep advice and how to get into SRE.\n\nI don't want to be rude, but this is a bad thing and a clear failure on the moderation side. Of course these posts get engagement - anyone can chime in with generic advice, and they are broadly relevant for everyone. That is presumably why the rule exists in the first place.\n\n> What Topics Would You Like to See More?\n\nI would like to see anything that's not breaking rule #2. The bar is low.\n\nI want something that's at least tangentially relevant to SRE. Not [/r/sysadmin-tier bitching](https://www.reddit.com/r/sre/comments/1qbd255/new_term_claude_hole/), not [vague posts about what NYC market looks like](https://www.reddit.com/r/sre/comments/1q9b6wl/sre_market_in_nyc/), not the regular vibecoded cheatsheets ([1](https://www.reddit.com/r/sre/comments/1qa9fxt/i_mapped_out_how_debugging_actually_works_during/), [2](https://www.reddit.com/r/sre/comments/1q7xqhd/a_production_engineering_knowledge_base_called/)). These are the top posts of the week, the rest is worse! LinkedIn has a better signal-to-noise ratio.\n\n> Let us know what topics you'd like to see more of.\n\nBut to what end? It's users who create discussions. Unless the moderation team plans to actively invite speakers or write the posts, this feedback will not change anything.",
          "score": 1,
          "created_utc": "2026-01-15 16:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztj75s",
              "author": "thecal714",
              "text": "> Karma/account age requirements should at least put out the fire.\n\nMany of the reported posts which we remove have significant positive karma and account age. That _used_ to be an easy and effective switch for mods to flip, but isn't a particularly effective one any longer. \n\n> I don't want to be rude, but this is a bad thing and a clear failure on the moderation side.\n\nNot rude, but I would ask for clarification on what you mean by moderation failure, so I can ensure that we're thinking the same things. These post get removed, but are often posted in the middle of the night in American time zones, so are actioned upon in the morning. Recruiting outside of American time zones is quite difficult. \n\n> I want something that's at least tangentially relevant to SRE. Not... \n\nOkay. We're pretty aware of what people _don't_ want to see based on user reports, but specifics on what you want to see are what we asked for and what would actually help.\n\n> But to what end? ...Unless the moderation team plans to actively invite speakers or write the posts\n\nWell, we can invite people to post on topics or conduct AMAs and write some content ourselves, but it also helps to identify what _is_ relevant to SREs. Since SRE means something different everywhere, what's relevant to SRE is also equally varying.",
              "score": 1,
              "created_utc": "2026-01-15 23:11:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzu0da1",
                  "author": "wfrced_bot",
                  "text": "> That used to be an easy and effective switch for mods to flip, but isn't a particularly effective one any longer.\n\nI don't agree that this *used to be* an easy and effective switch, because in my view it still is. I went through this week's posts, and these are examples that remain:\n\n* [https://www.reddit.com/r/sre/comments/1qbkuxl/how_does_your_team_retain_alert_resolution/](https://www.reddit.com/r/sre/comments/1qbkuxl/how_does_your_team_retain_alert_resolution/)\n* [https://www.reddit.com/r/sre/comments/1qc97re/duckdb_and_object_storage_for_reducing/](https://www.reddit.com/r/sre/comments/1qc97re/duckdb_and_object_storage_for_reducing/)\n* [https://www.reddit.com/r/sre/comments/1qaoc94/for_practicing_sres_which_learning_resources_best/](https://www.reddit.com/r/sre/comments/1qaoc94/for_practicing_sres_which_learning_resources_best/)\n* [https://www.reddit.com/r/sre/comments/1q9bskc/vendor_selection_enterprise_vs_startup_vs_build/](https://www.reddit.com/r/sre/comments/1q9bskc/vendor_selection_enterprise_vs_startup_vs_build/)\n* [https://www.reddit.com/r/sre/comments/1q7xo4u/former_cloudflare_sre_building_a_tool_to_keep_a/](https://www.reddit.com/r/sre/comments/1q7xo4u/former_cloudflare_sre_building_a_tool_to_keep_a/)\n* [https://www.reddit.com/r/sre/comments/1q9w030/someone_wrote_an_emo_anthem_about_the_2012_leap/](https://www.reddit.com/r/sre/comments/1q9w030/someone_wrote_an_emo_anthem_about_the_2012_leap/) - *nb: this one is genuinely bizarre*\n* [https://www.reddit.com/r/sre/comments/1qcclyk/how_much_do_you_get_paid_for_oncall_duty_in_india/](https://www.reddit.com/r/sre/comments/1qcclyk/how_much_do_you_get_paid_for_oncall_duty_in_india/)\n* [https://www.reddit.com/r/sre/comments/1q9l3nn/global_cdn_misconfiguration_in_a_giant_like_nike/](https://www.reddit.com/r/sre/comments/1q9l3nn/global_cdn_misconfiguration_in_a_giant_like_nike/)\n* [https://www.reddit.com/r/sre/comments/1qbqbrk/how_we_stopped_ai_from_hallucinating_during_log/](https://www.reddit.com/r/sre/comments/1qbqbrk/how_we_stopped_ai_from_hallucinating_during_log/)\n\nThat's `9/26 â‰ˆ 35%` of posts this week coming from accounts that look suspicious (<1 year old OR <100 karma) and that, in my opinion, could be removed without any downside. I would also expect some posts meeting this criterion to have already been removed and not appear here, so the ratio is likely higher.\n\nThere is [one post that could be considered a false positive](https://www.reddit.com/r/sre/comments/1qcq3t4/im_building_a_python_cli_tool_to_test_google/), but that also reads as self-promotion to me (and the tool itself appears completely vibecoded).\n\n> These posts get removed, but are often posted in the middle of the night in American time zones, so are actioned upon in the morning. Recruiting outside of American time zones is quite difficult.\n\nRelying primarily on user reports feels analogous to relying on customer complaints instead of implementing monitoring. Given a churn of 5â€“6 posts per day, options like scheduled posting freezes or manual approval seem ok to me.\n\nI don't follow the sub extremely closely (though probably more than most, since I follow only a small set of smaller subs), but I don't recall seeing a call for additional moderators; may explain the recruiting difficulty. This isn't meant to be snarky - just how it comes across from the outside.\n\n> We're pretty aware of what people don't want to see based on user reports, but specifics on what you want to see are what we asked for and what would actually help.\n\n> Since SRE means something different everywhere, what's relevant to SRE is also equally varying.\n\nAwareness alone doesn't seem sufficient to me; posts still need to be actively curated, and reports alone don't fully substitute for having a clear stance on what belongs. The posts I listed above are still up - I don't know whether that was a conscious decision or simply a matter of time, but I would have removed them regardless of engagement, even if it resulted in a completely empty feed. Likewise, I wouldn't [have added a \"HORROR STORY\" flair](https://www.reddit.com/r/sre/comments/1qbd255/new_term_claude_hole/nzduuvb/) to honor a shitpost.\n\nSRE is a technical discipline. It's reasonable to expect that people are interested in anything that helps them do that job better. Your proposed topic list is good, and everything in the Google workbooks clearly belongs. I also think \"reliability engineering\" is already a sufficiently precise framing - e.g. it's far narrower than DevOps. At its core, SRE is about measuring reliability and doing the work required to bring it to an acceptable standard. Anything that supports that goal belongs; anything that doesn't probably doesn't.",
                  "score": 1,
                  "created_utc": "2026-01-16 00:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qfof5o",
      "title": "How many meetings / ad-hoc calls do you have per week in your role?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfof5o/how_many_meetings_adhoc_calls_do_you_have_per/",
      "author": "Ok_Discipline3753",
      "created_utc": "2026-01-17 20:48:55",
      "score": 5,
      "num_comments": 9,
      "upvote_ratio": 0.73,
      "text": "Iâ€™m trying to get a realistic picture of what the day-to-day looks like. Iâ€™m mostly interested in:\n\n1. number of scheduled meetings per week\n2. how often you get ad-hoc calls or â€œcan you jump on a call now?â€ interruptions\n3. how often you have to explain your work to non-technical stakeholders?\n4. how often you lose half a day due to meetings / interruptions\n\nhow many hours per week are spent in meetings or calls?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfof5o/how_many_meetings_adhoc_calls_do_you_have_per/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o066r6y",
          "author": "interrupt_hdlr",
          "text": "big tech numbers:  \n  \n1. about 6-7 scheduled meetings per week  \n2. every other day  \n3. rarely  \n4. 2-3 days per week",
          "score": 4,
          "created_utc": "2026-01-17 20:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06lf50",
          "author": "hawtdawtz",
          "text": "1. On average 6-7 outside of standups like the other guy said. Iâ€™m in two standups which adds another ~3 hours a week\n2. Roughly 1-3 times a week\n3. Varies, idk 0-3 times a week \n4. 30% of the time",
          "score": 2,
          "created_utc": "2026-01-17 22:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06ym0p",
          "author": "Escatotdf",
          "text": "1. 6-7 a week\n2. 2-3 a week\n3. About once a week\n4. 2-3 times a week\n\n8-11 hours of meetings total per week according to my calendar. As a senior SRE",
          "score": 2,
          "created_utc": "2026-01-17 23:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06a5i8",
          "author": "Ecstatic-Minimum-252",
          "text": "30 minutes meeting once a week",
          "score": 1,
          "created_utc": "2026-01-17 21:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08pz5b",
          "author": "SomeGuyNamedPaul",
          "text": "1. about 7, including daily stand-ups and I'm absolutely including them because they suck down 90 minutes and parts of my soul.  It's so terrible.\n\n2. very infrequently.  It's basically only a text chat with a co-worker extends past 4 exchanges \n\n3. Never\n\n4. Infrequently, but when it does happen my morale is so crushed that the rest of the day is wildly unproductive anyway.",
          "score": 1,
          "created_utc": "2026-01-18 05:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b9uik",
          "author": "Sufficient-Bad-7037",
          "text": "1. Once in a week\n2. Very infrequently \n3. Rarely\n4. Mostly async slack interruptions",
          "score": 1,
          "created_utc": "2026-01-18 16:25:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bq62a",
          "author": "Fearless_Meat_1655",
          "text": "1. 15-17 a week. In my previous org we had 20 in a weekÂ \n2. Ad hoc is 2-3 and lasts few minsÂ ",
          "score": 1,
          "created_utc": "2026-01-18 17:43:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0d81w6",
          "author": "raisputin",
          "text": "Too often",
          "score": 1,
          "created_utc": "2026-01-18 22:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ekf8z",
          "author": "FullStackDestroyer",
          "text": "1.\tâ number of scheduled meetings per week\n> 10-15 hrs\n\n2.\tâ how often you get ad-hoc calls or â€œcan you jump on a call now?â€ interruptions\n> 2-3x per day\n\n3.\tâ how often you have to explain your work to non-technical stakeholders?\n> every single day.  Seriously.\n\n4.\tâ how often you lose half a day due to meetings / interruptions\n> 2-3x per week.  There are ways to improve, which Iâ€™m working on, but change is hard.  Get a helmet. \n\nCreds: lead SRE/release at a public company, lots of AI dev, hundreds of services, dozens of external dependencies, software fails SO regularly, but we make it workâ€¦well.",
          "score": 1,
          "created_utc": "2026-01-19 02:19:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc2k2u",
      "title": "Looking for a test system that can run in microK8s or Kind that produces mock data.",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qc2k2u/looking_for_a_test_system_that_can_run_in/",
      "author": "trudesea",
      "created_utc": "2026-01-13 20:35:55",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi,\n\nWeird question I know, but the reason is I was laid off end of last month after 27yrs as an Architect/Platform Engineer.   I was basically an SRE but didn't have the title.  \n\nBefore I separated from the company I was working on implementing istio/opentelemetry/prometheus/graphana/tempo and integrating with JIRA and Gitlab \n\nIt was just in the design phase but the systems where there GKE/AWS test clusters running our platform so I had plenty of data to build this out.\n\nSo now all I have is my home lab and I want to build it out so I can test and improve my design.  Also buff up on my Python as we didn't really use it.  \n\nIs there such a thing that just runs in the cluster and produces logs, simulates issues including OOMs/pod restarts/etc so you can test/rate your design?\n\nThanks for any info.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qc2k2u/looking_for_a_test_system_that_can_run_in/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzi584m",
          "author": "s5n_n5n",
          "text": "Not a weird question at all, apps to simulate load / issues, etc. is something a lot of teams and people require. I started to collect those [in this repository](https://github.com/causely-oss/awesome-synthetic-apps), here are a few you might be the most interested in:\n\n* [https://github.com/open-telemetry/opentelemetry-demo](https://github.com/open-telemetry/opentelemetry-demo), official OpenTelemetry demo, has some feature flags to simulate issues (most of them are application related, not infra)\n* [https://github.com/causely-oss/chaosmania](https://github.com/causely-oss/chaosmania), has some mocks for CPU and Memory issues, which you can use to create OOms etc.\n\nThere are a lot more, and the list on that repo is most likely incomplete, so if you find more or anyone knows more, I am happy to build it out.\n\nAdditionally, what you can do is run any of those demo apps and combine it with some chaos testing solutions, I used pumba (https://github.com/alexei-led/pumba/) in the past, since it fit my needs and was easy to get started, but there are also solutions like Litmus or ChaosMesh, but I have not a lot of experience with them.",
          "score": 3,
          "created_utc": "2026-01-14 07:49:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl1sn2",
          "author": "Charming_Speaker_908",
          "text": "https://coralogix.github.io/workshops/#opentelemetry-collector-tools\n\nThis might help",
          "score": 1,
          "created_utc": "2026-01-14 18:32:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbkuxl",
      "title": "How does your team retain alert resolution knowledge beyond Slack?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qbkuxl/how_does_your_team_retain_alert_resolution/",
      "author": "Unlucky_Spread_6653",
      "created_utc": "2026-01-13 07:14:20",
      "score": 0,
      "num_comments": 14,
      "upvote_ratio": 0.5,
      "text": "Not talking about routing or escalation.\n\nOnce an alert fires and hits Slack:\n\n* Where do you *actually* look first?\n* How do you know if this exact alert has happened before?\n* Does the outcome change based on *who* is on call?\n\nIn a lot of teams Iâ€™ve seen, resolution boils down to:\n\n* Someone remembering the fix\n* Searching old Slack threads\n* Or starting from scratch\n\nIs that reality for most teams, or am I just seeing badly run setups?\n\nWhat does your team do differently (if anything)?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qbkuxl/how_does_your_team_retain_alert_resolution/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzbfjvn",
          "author": "robscomputer",
          "text": "Alert > PagerDuty > Alert metadata contains playbook > oncall uses playbook > If no resolution or playbook lacking > create action item to improve playbook > repeat as needed. \n\nSlack has some features like pages which act as a mini wiki but the search has been so bad, it's a hard sell to teams. We use Gitlab and store the playbooks in markdown, but some teams use Google Docs/Confluence. It really doesn't matter as long as it's a location that supports rich text and allows easy updating.",
          "score": 10,
          "created_utc": "2026-01-13 07:49:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbgg3z",
              "author": "Unlucky_Spread_6653",
              "text": "Understood. So pagerduty is a crucial part of your on call however the challenge of a RCA inventory is still there. We are also thinking to add rca of each service in that particular repo for easy access.",
              "score": 1,
              "created_utc": "2026-01-13 07:57:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzbgyvy",
                  "author": "robscomputer",
                  "text": "We use PagerDuty but that could be any platform, what it does have is the alert payload contains the link to the playbook for addressing the issue. \n\nYes, we have a large backlog of issues to fix, as each sprint is developed, certain epics or stories are pulled to the front for work, including some of the playbook improvements. Most playbook issues involve taking tribal knowledge and documenting it.",
                  "score": 3,
                  "created_utc": "2026-01-13 08:02:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzbgfiv",
          "author": "itasteawesome",
          "text": "Jesus this is terrifying to me to think that there are companies out there who use slack as their runbook/knowledge base.\n\nIsn't documenting the problem and fix like the most basic gut reaction of a professional when fixing problems or have i just been in a fever dream these many years?",
          "score": 5,
          "created_utc": "2026-01-13 07:57:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbgt2h",
              "author": "Unlucky_Spread_6653",
              "text": "Not exactly runbooks in slack. Conversation thread that has context to a incident. Yes documentation is done but where is your documentation live and how fast you can retrieve it at the time of crises. That is my concern to solve.",
              "score": 1,
              "created_utc": "2026-01-13 08:01:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzbjs6j",
                  "author": "itasteawesome",
                  "text": "I'm a visual guy, so I usually tightly couple my runbooks and my o11y and operational dashboards.   The place you look to see if the thing is fucked tells you what to do when the thing is fucked (assuming I havent jumped ahead and automated remediation of this failure mode, but in this case the dashboard should be showing logs from the remediation pipeline to let the person know that the remediation did/did not run as expected).\n\nIf we come into a novel failure mode then as soon as I get the thing un-fucked i write it up and add it to the dashboard. If I dont write it down right away I never will.  Iterate into infinity.  \nMy dashboards about a given object will include the data from the ticketing system so I can see if this is a recurring pattern, and what people may have done in the past, but i prefer to pull the resolution up into the dashboard to save future engineers from the extra clicks.\n\nI've done similar workflows using a half dozen tools over the years as the stacks and companies change, but the general strokes tend to be similar.",
                  "score": 1,
                  "created_utc": "2026-01-13 08:29:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdly56",
              "author": "pdp10",
              "text": "I suspect that these problems people report are less about the documentation of issues, and more about efficiently propagating knowledge of the issues and fixes, to the other relevant parties.",
              "score": 1,
              "created_utc": "2026-01-13 16:35:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdlito",
          "author": "pdp10",
          "text": "*Post mortem* actionables:\n\n* Additional error-handling and logging in existing code.\n* New integration and unit test code.\n* Changes to metrics and monitoring.\n* Additional scanning and remediation code.\n* If all automation options are impractical: additional checklist items for humans.\n\nSimplified examples:\n\n* Instead of \"tribal knowledge\" about how to rotate X.509 certs, or a docbase, or figuring it all out again each time, we write automation.\n* Put the automation (or, failing that, the documentation) in the most-obvious place. Put it in one canonical place, then everything else is a pointer to it -- either an explicit reference, or a literal filesystem symlink where feasible. One of our local conventions is to have a `Makefile` as the obvious entry point. Sometimes the makefile just prints some instructions when it's run! Another convention is to have a `README.rst` file in the system's root directory explaining what's going on, or put the same information in `/etc/motd`.\n* Other docs live in the Git tree with the relevant code, if they can't be comments or runtime `--help` in the code itself.",
          "score": 3,
          "created_utc": "2026-01-13 16:34:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzckkz5",
          "author": "lefos123",
          "text": "Runbook should be in the alert message, or in a single place that folks can look at. Markdown in GitHub, a wiki, etc.",
          "score": 2,
          "created_utc": "2026-01-13 13:30:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfd84m",
          "author": "founders_keepers",
          "text": "urh it's ok to use slack as your run book or as central spot to coordinate and distribute alerts but you really need something like a SRE platform (we use Rootly, but do your own research) to triage or thigns will get completely lost over time. Docs here so you can see at least the possibilities of what might an integration look like: [https://docs.rootly.com/integrating-with-slack](https://docs.rootly.com/integrating-with-slack)",
          "score": 1,
          "created_utc": "2026-01-13 21:36:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzt6jet",
          "author": "Skylis",
          "text": "Step 1, don't just fire and forget alerts to slack. Use a system actually meant for this.",
          "score": 1,
          "created_utc": "2026-01-15 22:07:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}