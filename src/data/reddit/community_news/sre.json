{
  "metadata": {
    "last_updated": "2026-02-02 09:09:57",
    "time_filter": "week",
    "subreddit": "sre",
    "total_items": 9,
    "total_comments": 40,
    "file_size_bytes": 52804
  },
  "items": [
    {
      "id": "1qnr03g",
      "title": "The future of software engineering is SRE",
      "subreddit": "sre",
      "url": "https://swizec.com/blog/the-future-of-software-engineering-is-sre/",
      "author": "eberkut",
      "created_utc": "2026-01-26 19:57:34",
      "score": 77,
      "num_comments": 9,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "BLOG",
      "permalink": "https://reddit.com/r/sre/comments/1qnr03g/the_future_of_software_engineering_is_sre/",
      "domain": "swizec.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1zbw1w",
          "author": "knob-ed",
          "text": "I've been saying this to my colleagues, especially those who were slightly technical and are now able to produce PoCs.\n\nIt's all fun and games until you have to run/maintain the fucker.",
          "score": 20,
          "created_utc": "2026-01-27 08:09:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ppg6c",
              "author": "BlessedSRE",
              "text": "Already hit this .. product can write apps, product cannot operate apps",
              "score": 2,
              "created_utc": "2026-01-31 01:26:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2x6bil",
          "author": "raymond_reddington77",
          "text": "Very superficial article, wouldn’t you say?",
          "score": 1,
          "created_utc": "2026-02-01 05:11:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yo2tw",
          "author": "kennetheops",
          "text": "great blog. would love to have a longer conversation to hear your thoughts",
          "score": -7,
          "created_utc": "2026-01-27 04:59:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l9bpw",
          "author": "chub79",
          "text": "The logic by which AI would stop at writing code and wouldn't swallow the whole ops pipeline is fantasy.",
          "score": -2,
          "created_utc": "2026-01-30 12:17:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ly9q3",
              "author": "the_packrat",
              "text": "Right there, where you decided that SRE was  the \"ops pipeline\" is where you missed the point.",
              "score": 4,
              "created_utc": "2026-01-30 14:37:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m0xp2",
                  "author": "chub79",
                  "text": "Ah yes, SRE will be the ivory tower that AI cannot take over. I mean developers said the same thing.",
                  "score": -2,
                  "created_utc": "2026-01-30 14:50:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnkwib",
      "title": "[Mod Post] New Rule: Posts advertising or soliciting feedback for products are not allowed!",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qnkwib/mod_post_new_rule_posts_advertising_or_soliciting/",
      "author": "thecal714",
      "created_utc": "2026-01-26 16:31:49",
      "score": 60,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Effective 2026-01-26 1630 UTC, posts advertising or soliciting feedback for products are not allowed (rule #6).\n\nAny questions, please ask below.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qnkwib/mod_post_new_rule_posts_advertising_or_soliciting/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1w4bsk",
          "author": "nullset_2",
          "text": "Thank you!",
          "score": 7,
          "created_utc": "2026-01-26 20:57:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wk611",
          "author": "GrogRedLub4242",
          "text": "THANK YOU, MODS!!!",
          "score": 5,
          "created_utc": "2026-01-26 22:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uhsfu",
          "author": "bittrance",
          "text": "A definition of product would be helpful. Since the rule is called \"No Advertisements\" I assume that it includes commercial software offerings. Does it also include open source offerings? Does it matter if the provider is a commercial enterprise?\n\n\"Soliciting\" is a tricky word in English for us non-native speakers. Does that refer only to first-party requests or also to third-party requests for feedback?",
          "score": 0,
          "created_utc": "2026-01-26 16:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ul08u",
              "author": "thecal714",
              "text": "**tl;dr:** Send [modmail](https://www.reddit.com/message/compose/?to=/r/sre) to ask the mods beforehand if you're unsure if you'll fall afoul of the rule. This is pretty much the Reddit golden rule.\n\n> I assume that it includes commercial software offerings. Does it also include open source offerings?\n\nYes, it does.\n\n> \"Soliciting\" is a tricky word in English for us non-native speakers.\n\nPeople making (or wanting to make) a product cannot ask for feedback or use this subreddit to perform market research.\n\n> third-party requests for feedback?\n\nI'm interested in what you mean by this.",
              "score": 6,
              "created_utc": "2026-01-26 16:59:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o25edja",
                  "author": "sakthi_man",
                  "text": ">I'm interested in what you mean by this.\n\nI think they meant asking for the feedback of a product they didn't develop. Something like \"I am thinking of adding a caching layer in my service, is redis good enough or is there a better alternative like dragonfly?\"",
                  "score": 2,
                  "created_utc": "2026-01-28 03:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qodqaq",
      "title": "Unpopular Opinion: \"Multi-Region\" is security theater if you're sharing the vendor's Control Plane.",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qodqaq/unpopular_opinion_multiregion_is_security_theater/",
      "author": "NTCTech",
      "created_utc": "2026-01-27 13:23:53",
      "score": 49,
      "num_comments": 38,
      "upvote_ratio": 0.8,
      "text": "I need to vent about a pattern I’m seeing in almost every DR audit lately.\n\nEveryone is obsessed with Data Plane failure (Zone A floods, fiber cut in Virginia, etc.). But almost nobody is calculating the blast radius of a Control Plane failure.\n\nI watched a supposedly \"resilient\" Multi-Region setup completely implode recently. The architecture diagram looked great - active workloads in US-East, cold standby in US-West. But when the provider had a global IAM service degradation, the whole thing became a brick.\n\nThe VMs were healthy! They were running perfectly. But the *management* of those VMs was dead. We couldn't scale up the standby region because the API calls were timing out globally. We were effectively locked out of the console because the auth tokens wouldn't refresh.\n\nIt didn't matter that we paid for two regions. We were dependent on a single, global vendor implementation of Identity.\n\nThe \"Shared Fate\" Reality We keep treating Hyperscalers like magic infrastructure, but they are just software vendors shipping code. If they push a bad config to their global BGP or IAM layer, your \"geo-redundancy\" means nothing.\n\nI’ve started forcing my teams to run \"Kill Switch\" drills that actually simulate this:\n\n* Cut the primary region's network access.\n* Attempt to bring up the DR site without using the provider's SSO or global traffic manager.\n* 9 times out of 10, it fails because of a hidden dependency we didn't document.\n\nThe SLA Math is a Joke Also, can we stop pretending 99.99% SLAs are a risk mitigation strategy? I ran the numbers for a client:\n\n* Cost of Outage (4 hours): $2M in lost transactions.\n* SLA Payout: A $4,500 service credit next month.\n\nThe SLA protects *their* margins, not our uptime.\n\nI did a full forensic write-up on this (including the TCO math and the \"Control Plane Separation\" diagrams) on my personal site. I pinned the post to my profile if you want to see the charts, but I’m curious - how are you guys handling \"Global Service\" risk?\n\nAre you actually building \"Active-Active\" across different cloud providers, or are we all just crossing our fingers that the IAM team at AWS/Azure doesn't have a bad day?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qodqaq/unpopular_opinion_multiregion_is_security_theater/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o20jy2p",
          "author": "ninjaluvr",
          "text": "Not only is that not unpopular, it's the actual guidance from the vendors. \n\nAWS: [https://docs.aws.amazon.com/wellarchitected/latest/framework/rel\\_withstand\\_component\\_failures\\_avoid\\_control\\_plane.html](https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_withstand_component_failures_avoid_control_plane.html)\n\nGoogle: [https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/bare-metal/docs/troubleshooting/failure-mode-analysis](https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/bare-metal/docs/troubleshooting/failure-mode-analysis)\n\nAzure: [https://learn.microsoft.com/en-us/azure/architecture/guide/multitenant/considerations/control-planes](https://learn.microsoft.com/en-us/azure/architecture/guide/multitenant/considerations/control-planes)",
          "score": 34,
          "created_utc": "2026-01-27 13:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20ubd1",
              "author": "NTCTech",
              "text": "The docs are great, but the problem isn't the guidance, it's the implementation friction.\n\nEvery 'Well-Architected' review I do has the box checked for 'Multi-Region,' but when we actually pull the plug on IAM during a game day, the failover script dies 9 times out of 10. The vendors write the whitepaper on isolation, but their default tooling pushes you toward global namespaces and easy peering because it reduces friction during onboarding. It’s 'Do as I say, not as my default config does.'",
              "score": 4,
              "created_utc": "2026-01-27 14:42:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21719n",
                  "author": "ninjaluvr",
                  "text": "These are simply architectural decisions you make and game day out like everything else. If all you're doing in a well-architected review is checking a box for multi-region, the issue is you.",
                  "score": 9,
                  "created_utc": "2026-01-27 15:42:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o20qvqe",
          "author": "neuralspasticity",
          "text": "A stand by region is not a multi-region architecture. \n\nIt’s a DR site.",
          "score": 26,
          "created_utc": "2026-01-27 14:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21a0mo",
          "author": "ForeverYonge",
          "text": "The opinion is unpopular because it’s about uptime, not security, so you start with a bad title and go downhill from there.\n\nMy unpopular opinion is that for most companies that do not run life critical services, focusing on getting another nine of availability by going fully redundant across multiple major cloud providers is a bad business decision - the cost is not worth the SLA credits for once in a few years when SHTF and customers don’t care that you’re still up because the 20 other services they need to operate are down anyway.\n\nAnd if you are life critical and money isn’t an object, of course - go as redundant as you need.",
          "score": 9,
          "created_utc": "2026-01-27 15:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21f4jn",
              "author": "NTCTech",
              "text": "Availability is the 'A' in the CIA Triad. If I can't access the system, it's indistinguishable from a ransomware encryption event to the end user. Uptime *is* security.\n\nBut on the business logic? I actually agree with you.\n\nIf you aren't running a pacemaker or a stock exchange, burning 2x cash on multi-cloud is often a bad trade.\n\nMy frustration isn't with companies that choose to be single-cloud to save money. My frustration is with the 'Middle Ground' companies that pay the premium for Multi-Region (data egress + complexity) believing they bought insurance.\n\nThey *think* they are redundant. The bill *says* they are redundant. But because of the shared control plane, they are effectively paying for two regions to go down at the same time.\n\nIf you're going to accept the risk, accept it fully. Don't pay for a DR region that relies on the primary region's API to wake up.",
              "score": 1,
              "created_utc": "2026-01-27 16:17:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21kxry",
                  "author": "ut0mt8",
                  "text": "Completely agree. These companies should better invest in knowing their stack and simplify it. So outages will be handled shortly",
                  "score": 1,
                  "created_utc": "2026-01-27 16:42:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21kx2j",
          "author": "GeorgeRNorfolk",
          "text": "It depends on your service. If IAM has a global outage that breaks half the internet, most sites using it can point fingers and say they were impacted like many other sites and there's nothing they can do. The cost in the outage is likely to be lower than the cost of building and supporting a multi-cloud architecture. \n\nObviously for critical healthcare or banking systems, they need to be up regardless, so they need to spend the money to be multi-cloud.",
          "score": 3,
          "created_utc": "2026-01-27 16:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20itdw",
          "author": "addfuo",
          "text": "this is the reason we’re using dedicated CDN provider to handle the traffic from outside, if there’s outages in one our cloud vendor it’ll automatically switch to different provider. If there’s outages on CDN side we can completely disable the CDN. \n\nBut in the end we’re paying more",
          "score": 6,
          "created_utc": "2026-01-27 13:43:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20v62u",
              "author": "NTCTech",
              "text": "Exactly. That extra cost isn't 'waste' it's your insurance premium.\n\nUsing a neutral CDN as the air-gap is the smartest play. If your routing layer shares the same fate/vendor as your compute layer, you aren't really multi-cloud. You just have a very expensive single point of failure.",
              "score": 6,
              "created_utc": "2026-01-27 14:46:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20ouzp",
          "author": "LorkScorguar",
          "text": "that's what happened multi times with GCP failures, especially the west9 outage where the control plane wasn't spread across all zones",
          "score": 2,
          "created_utc": "2026-01-27 14:15:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20ys1m",
              "author": "NTCTech",
              "text": "100%. \n\nThe West9 incident is the textbook case study for this.\n\nPeople assume 'Zone Independence' includes the control plane. It rarely does. When the API goes dark or the cooling fails in the wrong hallway, those 'independent' zones turn into stranded islands real quick.",
              "score": 1,
              "created_utc": "2026-01-27 15:03:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o226iy0",
              "author": "ut0mt8",
              "text": "West9 is even worse. Zones are just different levels of the same datacenter building. Datacenter which is famous for being one of the oldest and with lot of failure of all paris suburbs dcs",
              "score": 0,
              "created_utc": "2026-01-27 18:15:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21khdy",
          "author": "ut0mt8",
          "text": "It's not unpopular. Multi region or whatever only makes sense when you have quasi shared nothing architecture.\nIs it worth the investment? It depends.",
          "score": 2,
          "created_utc": "2026-01-27 16:40:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2242xd",
              "author": "NTCTech",
              "text": "\"'Quasi shared nothing' is exactly the right term.\n\nThat’s the gap I see constantly: teams build 'shared nothing' at the Compute/Storage layer, but leave the Auth/Mgmt layer fully shared. They essentially build a blast radius that includes their recovery site.\n\nAnd 100% on the investment part. If the cost of downtime is lower than the engineering salary required to maintain a true active-active split, then the correct architectural decision is to just let it go down.",
              "score": 3,
              "created_utc": "2026-01-27 18:05:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22brpt",
          "author": "sionescu",
          "text": "> If they push a bad config to their global BGP or IAM layer, your \"geo-redundancy\" means nothing.\n\nIt means that the VMs are still running. That is a lot.\n\n> The VMs were healthy! They were running perfectly.\n> Cost of Outage (4 hours): $2M in lost transactions\n\nWait, so the service was running without enough slack to handle a traffic surge ? That's on the ops. And how was there lost revenue if the VMs were running perfectly ?",
          "score": 1,
          "created_utc": "2026-01-27 18:37:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22e2wp",
              "author": "NTCTech",
              "text": "It sounds contradictory, but here is exactly how the revenue loss happens when the Control Plane dies but Data Plane (VMs) stays up:\n\nAuth Failure (The Big One): The app was 'up,' but the login flow relied on the provider's managed Identity/Auth service. Users could hit the front page (served by the VM), but as soon as they tried to log in or checkout, the token refresh failed. Result: 100% transaction failure rate.\n\nScaling Lock: We hit a natural traffic spike. The ASG (Auto Scaling Group) tried to spin up new nodes to handle the load. The 'RunInstances' API call failed. The existing healthy VMs got saturated (CPU 100%) and started timing out requests.\n\nThe Lockout: We couldn't manually intervene (restart services, drain queues) because our bastion host access relied on the same IAM that was throttling.\n\nSo yes, the VMs were technically 'running' - but they were zombies.",
              "score": 2,
              "created_utc": "2026-01-27 18:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22fa8d",
                  "author": "sionescu",
                  "text": "So the developers used a service that had no guarantees of regional independence/resiliency (the Auth service) without thinking ? It wasn't meant for app Auth, it was meant for control plane Auth. \n\n\nWas there no Production Readiness Review to surface this issue ?",
                  "score": 1,
                  "created_utc": "2026-01-27 18:52:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2k10b9",
                  "author": "PudsBuds",
                  "text": "Chatgpt. What model are you running on? ",
                  "score": 1,
                  "created_utc": "2026-01-30 06:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22xiq2",
          "author": "fork-bomb42",
          "text": "This is exactly the blind spot we keep seeing in incident reviews. Teams design for data plane failures, but the control plane dependencies stay implicit until they break.\n\nWhat helped us wasn’t “more regions,” but explicitly asking: what still works if the provider API, IAM, or autoscaling is unavailable? Once you test that boundary, a lot of “resilient” architectures look very different.\n\nMulti-region isn’t theater but it’s easy to overestimate what it actually protects you from if shared control plane failures aren’t part of game days.",
          "score": 1,
          "created_utc": "2026-01-27 20:12:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24bey6",
          "author": "veritable_squandry",
          "text": "oh yeah it is",
          "score": 1,
          "created_utc": "2026-01-28 00:08:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24p8u9",
          "author": "daedalus_structure",
          "text": "You're raving about things you don't seem to understand. \n\nYes, there are events which you cannot mitigate, but that does not make all mitigation worthless. \n\nOtherwise, forget everything because checkmate suckers, you didn't account for Apophis crashing into the United States at 3 miles per second and sending half of the planet's mass into orbit. \n\nMulti-region is not a magic bullet and cannot mitigate all possible issues. \n\nMulti-region is specifically a preemptive mitigation for single region failures, which could be hardware, network disruption, acts of God such as floods or lightning or fire, or could just be a shit update that the CSP rolled out that bricked a resource. \n\nThat's why they have region pairs, so you can be confident that they will not roll out updates to both regions at the same time, guaranteeing you can failover. \n\nBut again, your multi-region deployments are mitigations of only those risks. \n\n>The SLA protects their margins, not our uptime.\n\nDid you expect that the cloud vendor was going to shoulder your business risk? \n\nAgain, it seems like the problem is wrong minded expectations. \n\n>The VMs were healthy! They were running perfectly.\n\nAnd in this global service outage, you suffered no downtime. \n\n>how are you guys handling \"Global Service\" risk?\n\nThis is a business problem. \n\nIf the issue is your own SLA, you put in language exempting this class of failure for payout along with other acts of God. In general, if the entire world is watching it on the nightly news, you as a SaaS consuming the same down services as the rest of the world aren't responsible for it. \n\nIf the issue is lost revenue from lost uptime, you document and accept the risk. \n\nTrying to be multi-cloud with abstractions to run similar day 2 ops over both is prohibitively expensive, and you are locking in the costs now instead of paying them only if the black swan event happens. \n\nNever lock in the costs of a black swan event. \n\nBut so much worse than that, you've tied up a massive amount of engineering capacity in activities which do not deliver additional revenue, so considering both the engineering cost and opportunity cost, global services would have to go down significantly more often for you to even break even on your ROI over a decade. \n\nGo ask your CFO, they'll tell you the same.",
          "score": 1,
          "created_utc": "2026-01-28 01:18:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24yjse",
              "author": "NTCTech",
              "text": "Your pushback on the ROI is fair and you are 100% right that the 'Multi-Cloud Tax' (abstraction layer) is massive. If your business model allows you to absorb a 6-hour outage every 18 months, then paying that engineering tax is bad math. We agree there.\n\nWhere we disconnect is on the definition of 'Downtime' and 'Scope':\n\n\\- Zombie VMs = Downtime: You mentioned 'you suffered no downtime.' That is incorrect. If the App Auth relies on the vendor's IAM, and IAM is throttling, zero users can log in. The VM is technically 'up' (and billing me), but the transaction volume is zero. To the P&L, that is a total outage.\n\n\\- Region Pairs vs. Global Scope: Region pairs protect against bad *regional* updates. They do not protect against Global Control Plane failures. When a vendor pushes a bad config to their global BGP or IAM fleet, both regions in the pair break simultaneously. We have seen this repeatedly with all three hyperscalers.\n\n\\- The Contract: If you can write 'Vendor Global Outage' into your customer contracts as an 'Act of God' exemption, you have solved the problem legally. But for the banks and healthcare orgs I work with, that clause doesn't exist. They don't care *why* we are down, only *that* we are down.",
              "score": 0,
              "created_utc": "2026-01-28 02:08:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o25htr2",
          "author": "djbiccboii",
          "text": "Even before you get to control plane / IAM shared fate, a lot of these “multi-region” or “redundant” designs aren’t actually redundant at the physical layer at all. People talk regions and zones, but they’re not even thinking about line diversity.\n\nIf your “independent” regions are ultimately connected by the same provider backbone, same long-haul fiber routes, same metro POPs, or even the same right-of-way, then it’s not redundancy. it’s just distance. A single fiber cut, bad BGP push, or backbone issue can still take out inter-region connectivity in one shot.",
          "score": 1,
          "created_utc": "2026-01-28 03:53:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26t00a",
          "author": "NullPulsar",
          "text": "Blatant AI post and post history. They’re intended to drive traffic to the links in their profile.",
          "score": 1,
          "created_utc": "2026-01-28 10:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hoeow",
          "author": "stealthagents",
          "text": "It's a tough situation when the Control Plane is a single point of failure. As you've experienced, having redundancies in place isn't always enough if they aren't truly independent. At Stealth Agents, we've seen how critical it is to have robust systems and processes in place, especially with over a decade of expertise in managing operations and supporting businesses in these complex environments. Our team could help ensure your workflows and client follow-ups remain steady, even during unforeseen hiccups.",
          "score": 1,
          "created_utc": "2026-01-29 21:58:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2twu18",
          "author": "Other_Goat_9381",
          "text": "The major hyperscalers are all actively working on regionalizing their deeper critical services. I see this as a transient issue currently; a gap in their architecture designs that will slowly fade away as their premiums go up to price it in.\n\nBut it's right to complain though: everyone makes multi region sound like a one-click button when in reality it's an extremely difficult problem distributed systems.",
          "score": 1,
          "created_utc": "2026-01-31 18:24:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qprv5l",
      "title": "How do teams safely control log volume before ingestion (Loki / Promtail)?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qprv5l/how_do_teams_safely_control_log_volume_before/",
      "author": "TillStatus2753",
      "created_utc": "2026-01-28 23:34:29",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 0.9,
      "text": "Looking for real-world experience from people running Loki / Promtail at scale.\n\nI’m experimenting with ingestion control (filtering, sampling, routing) -before-logs hit Loki to reduce noise and cost, but I’m trying to sanity-check whether this is actually a problem worth solving.\n\nFor those running Loki in production:\n\n\\- What % of your logs are DEBUG/INFO vs WARN/ERROR?\n\n\\- Do you actively drop or sample logs before ingestion?\n\n\\- Is this something you’re confident changing, or do people avoid touching it?\n\n\\- What’s been the biggest pain: cost, noise, fear of deleting data, or config complexity?\n\nNot selling anything — genuinely trying to understand if this is a real problem or something most teams already handle fine.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qprv5l/how_do_teams_safely_control_log_volume_before/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2bel7s",
          "author": "cgill27",
          "text": "For logging to Loki I would recommend looking at [vector.dev](http://vector.dev), you can exclude things easily or even transform them, etc, all before shipping to Loki",
          "score": 7,
          "created_utc": "2026-01-28 23:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bz7m6",
              "author": "Freakin_A",
              "text": "Vector is great and incredibly efficient.  Highly recommend.\n\nCommercial options would be Cribl or Edge Delta.  Probably others.",
              "score": 1,
              "created_utc": "2026-01-29 01:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2h8g0m",
                  "author": "DramaticExcitement64",
                  "text": "The S3 sink is shit though.",
                  "score": 1,
                  "created_utc": "2026-01-29 20:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k2d2v",
              "author": "Broad_Technology_531",
              "text": "Interesting to hear that you are recommending vector instead of an OTEL Collector. Have you had a chance to work with the OTEL Collector for logs?",
              "score": 1,
              "created_utc": "2026-01-30 06:12:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2dy764",
              "author": "SnooWords9033",
              "text": "If you are going to use vector.dev instead of Promtail, then it is better to switch from Loki to VictoriaLogs. It is easier to configure and operate, it  supports high-cardinality log fields, which aren't supported properly by Loki, and it doesn't need object storage.",
              "score": -1,
              "created_utc": "2026-01-29 10:25:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d6r87",
          "author": "kusanagiblade331",
          "text": "Just to answer your question:\n\n1. DEBUG should not be in Loki or any type of log indexer.  \n2. Dropping yes - when there is an ingestion spike. Sampling - would be great if it can be easily done. But, session continuity is a real problem.  \n3. I think the key is to start small. Only apply filtering, sampling or routing to a few small apps. Then, slowly generalized. Teams can get upset when there is an incident and people cannot find logs.  \n4. Maintenance for ingestion control is a real fear. Cost is a big fear among management for log indexers like Splunk. The funny thing is - if everything is normal, teams generally have no fear of not getting some logs. Just make sure error and warning logs are there.\n\nSome interesting techniques to reduce log ingestion can also be found in this [article](https://starclustersolutions.com/blog/2026-01-how-to-reduce-splunk-cloud-cost/).",
          "score": 4,
          "created_utc": "2026-01-29 06:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bg8mw",
          "author": "Sea_Refrigerator5622",
          "text": "I know from troubleshooting there are a handful of logs we actually know show problems. We ingest those. The idea is to get GB down to MB. \n\nWe keep full kogs for a short time for deeper debugging and they’re a disaster. \n\nWe filter with Otel collector and in the tail sampling Loki (via Grafana cloud)",
          "score": 1,
          "created_utc": "2026-01-29 00:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k1qcu",
          "author": "Broad_Technology_531",
          "text": "Are you running OSS Loki or part of Grafana cloud?",
          "score": 1,
          "created_utc": "2026-01-30 06:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kx8ll",
              "author": "TillStatus2753",
              "text": "Both. I’ve been experimenting with OSS Loki locally and also talking to teams on Grafana Cloud. The questions around safely reducing logs seem to show up in both.",
              "score": 1,
              "created_utc": "2026-01-30 10:42:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo7a2q",
      "title": "Need advice on job/carrer switch",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qo7a2q/need_advice_on_jobcarrer_switch/",
      "author": "ZestycloseBench5329",
      "created_utc": "2026-01-27 07:31:00",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey, i am on my Notice period right now from my sre job, and i have a offer in hand as a sde in a sre environment. I want to build products with the tech skills i have. but am very uncertain with the trajectory i am going on. i want to know what are my options at this point\n\ni have experience working with python, fastapi, openshift, k8s, docker, CI/CD pipeline for building backend api endpoints for a data center team in a networking company. I have personal projects on MERN stack (its a chat application deployed oven k8s cluster, has NATS server and redis at backend) but i dont get projects like this which scales in a real job, neither do any HR market entertain the request to be a backend engineer even though i have experience to demonstrate that i can build such systems. \n\nEven in the job i am getting it would be a SRE environment and the product they are building is a AI summariser but not sure if i would get to work on it.",
      "is_original_content": false,
      "link_flair_text": "CAREER",
      "permalink": "https://reddit.com/r/sre/comments/1qo7a2q/need_advice_on_jobcarrer_switch/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1zkck8",
          "author": "glowandgo_",
          "text": "this is a pretty common trap. titles lag reality. if your day to day is still infra, pipelines, reliability glue, ppl will keep reading you as sre no matter the side projects. what changed for me was picking roles where backend work was the core pain, not adjacent. ask very directly what you’ll own in 6 months. if it’s vague now, it usually stays vague. context matters more than the label.",
          "score": 5,
          "created_utc": "2026-01-27 09:28:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr7nbo",
      "title": "Looking for a whitepaper/journeydoc for SRE transition",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qr7nbo/looking_for_a_whitepaperjourneydoc_for_sre/",
      "author": "hiveminer",
      "created_utc": "2026-01-30 14:55:33",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "So guys, in 2017, Juniper released a very nicely prepared 16 page document on the transition/journey to [NRE](https://events19.linuxfoundation.org/wp-content/uploads/2017/12/NRE-and-DevNetOps-Overview-ONS-Sept-2018.pdf)(Network Reliability Engineering).  I think it is well written.  Now, the question is, has a document like that been written for sysops?  SRE?  If now, those boasting the title of SENIOR SRE.. should consider it.  In fact, I think there are a number of parallels within that document which would apply to SRE.  We are staring at the dawn of IT second brain/digital sidekick.  That can also be incorporated, if not now, maybe for a possible version 2.",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qr7nbo/looking_for_a_whitepaperjourneydoc_for_sre/",
      "domain": "self.sre",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qp3214",
      "title": "Observability Blueprints",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qp3214/observability_blueprints/",
      "author": "jpkroehling",
      "created_utc": "2026-01-28 05:52:12",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.78,
      "text": "This week, my guest is Dan Blanco, and we'll talk about one of his proposals to make OTel Adoption easier: Observability Blueprints.\n\nThis Friday, 30 Jan 2026 at 16:00 (CET) / 10am Eastern.\n\nhttps://www.youtube.com/live/O_W1bazGJLk",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qp3214/observability_blueprints/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o260337",
          "author": "timee_bot",
          "text": "View in your timezone:  \n[Friday, 30 Jan 2026 at 16:00 CET][0]  \n\n[0]: https://timee.io/20260130T1500?tl=Observability%20Blueprints",
          "score": 1,
          "created_utc": "2026-01-28 05:52:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqah7q",
      "title": "Any good tools for Kubernetes access control?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qqah7q/any_good_tools_for_kubernetes_access_control/",
      "author": "SidLais351",
      "created_utc": "2026-01-29 14:46:16",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "managing access to multiple clusters with different environments and teams. We want tighter control over kubectl access, auditability, and clean offboarding. Looking for tools or patterns that have worked well in real setups. \n\ncommunity input would really helpful",
      "is_original_content": false,
      "link_flair_text": "HELP",
      "permalink": "https://reddit.com/r/sre/comments/1qqah7q/any_good_tools_for_kubernetes_access_control/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2f3s7c",
          "author": "JoshSmeda",
          "text": "If you have money, Teleport PAM.",
          "score": 3,
          "created_utc": "2026-01-29 14:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fbbmc",
          "author": "granviaje",
          "text": "Tailscale and Tailscale ACL are quite sweet ",
          "score": 2,
          "created_utc": "2026-01-29 15:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f4iyy",
          "author": "walt_dinio",
          "text": "following because we're also running into this problem as we continue to increase our k8s adoption. only where it makes sense. ",
          "score": 1,
          "created_utc": "2026-01-29 14:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fhplw",
          "author": "kckrish98",
          "text": "we looked at this problem from the perspective of making Kubernetes access consistent with other infrastructure access. we use Teleport to manage RBAC for SSH and Kubernetes through our identity provider, so user access is tied to roles instead of static kubeconfigs or separate credential stores. It gave us a more consistent permission model across clusters and host access",
          "score": 1,
          "created_utc": "2026-01-29 15:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mlcp9",
          "author": "SadFaceSmith",
          "text": "Been loving the Tailscale K8s Operator",
          "score": 1,
          "created_utc": "2026-01-30 16:24:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qra6x4",
      "title": "What are some useful things you can do with telemetry data outside of incident response?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qra6x4/what_are_some_useful_things_you_can_do_with/",
      "author": "Useful-Process9033",
      "created_utc": "2026-01-30 16:28:14",
      "score": 4,
      "num_comments": 19,
      "upvote_ratio": 0.75,
      "text": "In my previous role I pretty much only look at the logs/ metrics when I get paged. Or only during weekly reviews checking the dashboards and making sure all our services are in a good state. I suppose if you've got to a good state and incidents/ alerts are rare, when would you ever want to look at your logs/ metrics/ traces, and where else they'd be useful outside of incident response?",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qra6x4/what_are_some_useful_things_you_can_do_with/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2mo4as",
          "author": "AmazingHand9603",
          "text": "You can dive into telemetry to spot unexpected patterns or usage you didn’t anticipate. For example, maybe a particular endpoint is getting hammered every Monday morning and you’d never know unless you looked. Or maybe an old feature is still way more popular than you thought. This kind of info can help with planning, prioritizing, and making better product decisions. I’ve found a few minor issues this way before they ever became incidents, just by poking around out of curiosity.",
          "score": 10,
          "created_utc": "2026-01-30 16:36:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mv83h",
              "author": "Useful-Process9033",
              "text": "do you set a dedicated time daily/ weekly to review telemetry this way? or you look when the team is dealing with planning etc.",
              "score": 2,
              "created_utc": "2026-01-30 17:08:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mq5n7",
          "author": "bittrance",
          "text": "As a platform engineer, I often look at service metrics to understand how services behave in production. Do we have noisy neighbor problems? Do teams understand how to scale their services? Do logs flow properly? I think of myself as a game keeper or gardener walking the grounds.",
          "score": 4,
          "created_utc": "2026-01-30 16:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mvrwh",
              "author": "Useful-Process9033",
              "text": "how often do you have to review these and how often do you find something worth acting on? I suppose if you discover a noisy neighbor problem/ scaling problem you'd need to file a ticket of some sort and ping the team.",
              "score": 1,
              "created_utc": "2026-01-30 17:10:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ueca7",
                  "author": "bittrance",
                  "text": "We have a lot of services and they are a little too different to meaningfully aggregate metrics across them, so I look almost daily at metrics of some sort, but not so often at any particular metric.",
                  "score": 2,
                  "created_utc": "2026-01-31 19:47:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mwrtl",
              "author": "abuhd",
              "text": "There's a sweet newish cisco tool for understanding services, with visuals! It's not free but I helped do some basic ux testing for it. It's called Cisco Thousandeyes.",
              "score": 0,
              "created_utc": "2026-01-30 17:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2u22pa",
                  "author": "redipin",
                  "text": "Haha, this is hilarious… it’s only new with regards to being part of Cisco’s offerings. We had started using Thousandeyes over a decade ago in my org, though we dropped it before Cisco purchased it.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:48:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mo5h7",
          "author": "s5n_n5n",
          "text": "You can use telemetry for a lot of things:\n\n- optimization. So looking for things to improve \n- prevent incidents, finding what might break next \n- there are some auto scalers that work of tracing or metrics \n- Any kind of business analytics of course. ",
          "score": 3,
          "created_utc": "2026-01-30 16:36:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mvgdj",
              "author": "Useful-Process9033",
              "text": "for optimization & preventing incidents, how do you go about it?",
              "score": 2,
              "created_utc": "2026-01-30 17:09:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mybtm",
                  "author": "s5n_n5n",
                  "text": "Optimization: tracing can be very helpful here since you can find the critical paths and what consumes the most time, so when you want to trim your overall response time, you can focus on the right things. Or, what you can do is track which queries are called how often in production with what duration and focus on those to make things better. \n\nPrevent incidents is of course somehow harder and sits somewhere between optimization and incidence response. You need to find what is slowly moving towards a breaking point eg with anomaly detection ",
                  "score": 2,
                  "created_utc": "2026-01-30 17:22:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mumt8",
          "author": "abuhd",
          "text": "I manually dig through logs all day. If I find something of interest, I simply add it to my log pipeline. I keep the data temporarily for 7 days to see how much storage gets used. Then I figure out if the cost of the data is worth it to increase it to say 30 days. Then every 2-3 months, I revisit my pipeline and remove what isn't relevant. It's a never ending cycle of reading latest news, then figuring out what's important to YOU.",
          "score": 1,
          "created_utc": "2026-01-30 17:05:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2muze6",
              "author": "Useful-Process9033",
              "text": "interesting. how do you know if the data is worth the cost of 30 days storage? how do you know its value?",
              "score": 2,
              "created_utc": "2026-01-30 17:07:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mxi5e",
                  "author": "abuhd",
                  "text": "The value is what's important to me at the time. I think what I was trying to say in a roundabout way, was that, this process you're looking to find doesn't exist because it evolves daily.",
                  "score": 1,
                  "created_utc": "2026-01-30 17:18:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2nk5c9",
          "author": "robscomputer",
          "text": "Metrics in long term can show you how the service or component is handling target SLO's. It's a good point to show this during postmortems when someone says your service is not stable, the SLO metric will display the true story. It is tricky to get everyone to agree upon what the SLO should measure tho. :)",
          "score": 1,
          "created_utc": "2026-01-30 18:57:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ot5ud",
          "author": "fubo",
          "text": "Growth modeling and budgeting. Performance & efficiency. Looking for \"weird stuff\".\n\n\"Hey, why does this cache hit rate take a huge narrow dip at the beginning of every hour? Oh look, the cache entry expiration time is one hour, there's a single entry that accounts for 50% of all queries, and that particular entry takes 500msec to compute on a cache miss. So when that entry expires, 50% of all queries are cache misses for half a second. If we precompute that one entry and stuff it into the cache before it expires, we can get rid of that weird traffic spike on the backends. More generally, we should do that for the top N queries anyway; there's no point in ever having those be a cache miss.\"",
          "score": 1,
          "created_utc": "2026-01-30 22:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r2lir",
          "author": "c0Re69",
          "text": "Making the connection to customers in form of SLOs is the first important step, and the next one would be connecting that to business metrics/sales, which unlocks some nice leverage and brings you closer to the money.",
          "score": 1,
          "created_utc": "2026-01-31 07:03:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}