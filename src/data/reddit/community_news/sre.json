{
  "metadata": {
    "last_updated": "2026-01-30 08:59:57",
    "time_filter": "week",
    "subreddit": "sre",
    "total_items": 11,
    "total_comments": 62,
    "file_size_bytes": 76672
  },
  "items": [
    {
      "id": "1qnr03g",
      "title": "The future of software engineering is SRE",
      "subreddit": "sre",
      "url": "https://swizec.com/blog/the-future-of-software-engineering-is-sre/",
      "author": "eberkut",
      "created_utc": "2026-01-26 19:57:34",
      "score": 74,
      "num_comments": 2,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "BLOG",
      "permalink": "https://reddit.com/r/sre/comments/1qnr03g/the_future_of_software_engineering_is_sre/",
      "domain": "swizec.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1zbw1w",
          "author": "knob-ed",
          "text": "I've been saying this to my colleagues, especially those who were slightly technical and are now able to produce PoCs.\n\nIt's all fun and games until you have to run/maintain the fucker.",
          "score": 17,
          "created_utc": "2026-01-27 08:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yo2tw",
          "author": "kennetheops",
          "text": "great blog. would love to have a longer conversation to hear your thoughts",
          "score": -7,
          "created_utc": "2026-01-27 04:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnkwib",
      "title": "[Mod Post] New Rule: Posts advertising or soliciting feedback for products are not allowed!",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qnkwib/mod_post_new_rule_posts_advertising_or_soliciting/",
      "author": "thecal714",
      "created_utc": "2026-01-26 16:31:49",
      "score": 61,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Effective 2026-01-26 1630 UTC, posts advertising or soliciting feedback for products are not allowed (rule #6).\n\nAny questions, please ask below.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qnkwib/mod_post_new_rule_posts_advertising_or_soliciting/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1w4bsk",
          "author": "nullset_2",
          "text": "Thank you!",
          "score": 7,
          "created_utc": "2026-01-26 20:57:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wk611",
          "author": "GrogRedLub4242",
          "text": "THANK YOU, MODS!!!",
          "score": 5,
          "created_utc": "2026-01-26 22:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uhsfu",
          "author": "bittrance",
          "text": "A definition of product would be helpful. Since the rule is called \"No Advertisements\" I assume that it includes commercial software offerings. Does it also include open source offerings? Does it matter if the provider is a commercial enterprise?\n\n\"Soliciting\" is a tricky word in English for us non-native speakers. Does that refer only to first-party requests or also to third-party requests for feedback?",
          "score": 0,
          "created_utc": "2026-01-26 16:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ul08u",
              "author": "thecal714",
              "text": "**tl;dr:** Send [modmail](https://www.reddit.com/message/compose/?to=/r/sre) to ask the mods beforehand if you're unsure if you'll fall afoul of the rule. This is pretty much the Reddit golden rule.\n\n> I assume that it includes commercial software offerings. Does it also include open source offerings?\n\nYes, it does.\n\n> \"Soliciting\" is a tricky word in English for us non-native speakers.\n\nPeople making (or wanting to make) a product cannot ask for feedback or use this subreddit to perform market research.\n\n> third-party requests for feedback?\n\nI'm interested in what you mean by this.",
              "score": 6,
              "created_utc": "2026-01-26 16:59:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o25edja",
                  "author": "sakthi_man",
                  "text": ">I'm interested in what you mean by this.\n\nI think they meant asking for the feedback of a product they didn't develop. Something like \"I am thinking of adding a caching layer in my service, is redis good enough or is there a better alternative like dragonfly?\"",
                  "score": 2,
                  "created_utc": "2026-01-28 03:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlqm8j",
      "title": "Honeycomb EU outage write-up is a good reminder that humans are still the bottleneck",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qlqm8j/honeycomb_eu_outage_writeup_is_a_good_reminder/",
      "author": "TellersTech",
      "created_utc": "2026-01-24 15:58:02",
      "score": 57,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "Just read it and yeah… it hit a nerve.\n\nLong incidents aren’t just “fix the thing.” It’s handoffs, fatigue, context getting dropped, people accidentally doing the same work twice, status updates eating cycles, and everyone getting a little more cooked as the hours pile up.\n\nIt also made me think about the curl bug bounty thing this week. Different domain, same failure mode. Once the input stream turns into noise (AI slop reports, alert spam, ticket spam), you don’t just lose time. You lose trust in the channel. Then the real signal shows up and gets missed.\n\nHow are you all handling this lately? Not just outages, but the “too much inbound” problem in general.\n\nHoneycomb report:  [ https://status.honeycomb.io/incidents/pjzh0mtqw3vt ](https://status.honeycomb.io/incidents/pjzh0mtqw3vt)\n\ncurl context: [https://github.com/curl/curl/pull/20312](https://github.com/curl/curl/pull/20312)",
      "is_original_content": false,
      "link_flair_text": "POSTMORTEM",
      "permalink": "https://reddit.com/r/sre/comments/1qlqm8j/honeycomb_eu_outage_writeup_is_a_good_reminder/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1g4oje",
          "author": "jdizzle4",
          "text": "my palms are sweaty after reading that. Those poor SREs, what a battle. So much respect for the transparency in writing up and publishing this much detail. Having been in similar long incidents where it feels like defeat after defeat... my heart rate was elevated while reading it.",
          "score": 17,
          "created_utc": "2026-01-24 16:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gtgbf",
          "author": "TellersTech",
          "text": "Quick follow-up: I talked about this same ‘inbound noise kills trust’ thing on Ship It Weekly this week, using curl + the Honeycomb outage as examples. \n\nIf anyone wants the audio version: https://www.tellerstech.com/ship-it-weekly/curl-shuts-down-bug-bounties-due-to-ai-slop-aws-rds-blue-green-cuts-switchover-downtime-to-5-seconds-and-amazon-ecr-adds-cross-repository-layer-sharing/\n\nBut I’m more interested in what patterns actually work for teams here.",
          "score": 14,
          "created_utc": "2026-01-24 18:10:44",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1iuekp",
          "author": "wfrced_bot",
          "text": "> Long incidents aren’t just “fix the thing.” It’s handoffs, fatigue, context getting dropped, people accidentally doing the same work twice, status updates eating cycles, and everyone getting a little more cooked as the hours pile up.\n\nI'm not following. The report was about the issue with restoring distributed self-managed metadata on a broken cluster. There is no mention of humans being a bottleneck anywhere.\n\n> It also made me think about the curl bug bounty thing this week. Different domain, same failure mode.\n\nWhat failure mode?",
          "score": 5,
          "created_utc": "2026-01-24 23:53:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pk0x2",
              "author": "AvoidSpirit",
              "text": "You’re talking to AI",
              "score": 1,
              "created_utc": "2026-01-25 22:45:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ixvot",
              "author": "TellersTech",
              "text": "Yeah that’s fair. The write-up is basically all technical details, not “humans are the bottleneck” explicitly.\n\nWhat I meant was more the *pattern* you see in any long incident. Even when the root cause is 100% technical, after a few hours the limiting factor becomes people… handoffs, context loss, duplicate work, comms/status updates, fatigue. From what I’ve seen multi-region recovery stuff like this is where that really shows up.\n\nAnd the curl thing was the same vibe from a different angle. Once the input stream turns into noise, triage stops scaling and the channel loses trust.",
              "score": 3,
              "created_utc": "2026-01-25 00:11:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qodqaq",
      "title": "Unpopular Opinion: \"Multi-Region\" is security theater if you're sharing the vendor's Control Plane.",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qodqaq/unpopular_opinion_multiregion_is_security_theater/",
      "author": "NTCTech",
      "created_utc": "2026-01-27 13:23:53",
      "score": 43,
      "num_comments": 37,
      "upvote_ratio": 0.79,
      "text": "I need to vent about a pattern I’m seeing in almost every DR audit lately.\n\nEveryone is obsessed with Data Plane failure (Zone A floods, fiber cut in Virginia, etc.). But almost nobody is calculating the blast radius of a Control Plane failure.\n\nI watched a supposedly \"resilient\" Multi-Region setup completely implode recently. The architecture diagram looked great - active workloads in US-East, cold standby in US-West. But when the provider had a global IAM service degradation, the whole thing became a brick.\n\nThe VMs were healthy! They were running perfectly. But the *management* of those VMs was dead. We couldn't scale up the standby region because the API calls were timing out globally. We were effectively locked out of the console because the auth tokens wouldn't refresh.\n\nIt didn't matter that we paid for two regions. We were dependent on a single, global vendor implementation of Identity.\n\nThe \"Shared Fate\" Reality We keep treating Hyperscalers like magic infrastructure, but they are just software vendors shipping code. If they push a bad config to their global BGP or IAM layer, your \"geo-redundancy\" means nothing.\n\nI’ve started forcing my teams to run \"Kill Switch\" drills that actually simulate this:\n\n* Cut the primary region's network access.\n* Attempt to bring up the DR site without using the provider's SSO or global traffic manager.\n* 9 times out of 10, it fails because of a hidden dependency we didn't document.\n\nThe SLA Math is a Joke Also, can we stop pretending 99.99% SLAs are a risk mitigation strategy? I ran the numbers for a client:\n\n* Cost of Outage (4 hours): $2M in lost transactions.\n* SLA Payout: A $4,500 service credit next month.\n\nThe SLA protects *their* margins, not our uptime.\n\nI did a full forensic write-up on this (including the TCO math and the \"Control Plane Separation\" diagrams) on my personal site. I pinned the post to my profile if you want to see the charts, but I’m curious - how are you guys handling \"Global Service\" risk?\n\nAre you actually building \"Active-Active\" across different cloud providers, or are we all just crossing our fingers that the IAM team at AWS/Azure doesn't have a bad day?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qodqaq/unpopular_opinion_multiregion_is_security_theater/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o20jy2p",
          "author": "ninjaluvr",
          "text": "Not only is that not unpopular, it's the actual guidance from the vendors. \n\nAWS: [https://docs.aws.amazon.com/wellarchitected/latest/framework/rel\\_withstand\\_component\\_failures\\_avoid\\_control\\_plane.html](https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_withstand_component_failures_avoid_control_plane.html)\n\nGoogle: [https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/bare-metal/docs/troubleshooting/failure-mode-analysis](https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/bare-metal/docs/troubleshooting/failure-mode-analysis)\n\nAzure: [https://learn.microsoft.com/en-us/azure/architecture/guide/multitenant/considerations/control-planes](https://learn.microsoft.com/en-us/azure/architecture/guide/multitenant/considerations/control-planes)",
          "score": 31,
          "created_utc": "2026-01-27 13:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20ubd1",
              "author": "NTCTech",
              "text": "The docs are great, but the problem isn't the guidance, it's the implementation friction.\n\nEvery 'Well-Architected' review I do has the box checked for 'Multi-Region,' but when we actually pull the plug on IAM during a game day, the failover script dies 9 times out of 10. The vendors write the whitepaper on isolation, but their default tooling pushes you toward global namespaces and easy peering because it reduces friction during onboarding. It’s 'Do as I say, not as my default config does.'",
              "score": 6,
              "created_utc": "2026-01-27 14:42:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21719n",
                  "author": "ninjaluvr",
                  "text": "These are simply architectural decisions you make and game day out like everything else. If all you're doing in a well-architected review is checking a box for multi-region, the issue is you.",
                  "score": 8,
                  "created_utc": "2026-01-27 15:42:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o20qvqe",
          "author": "neuralspasticity",
          "text": "A stand by region is not a multi-region architecture. \n\nIt’s a DR site.",
          "score": 26,
          "created_utc": "2026-01-27 14:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21a0mo",
          "author": "ForeverYonge",
          "text": "The opinion is unpopular because it’s about uptime, not security, so you start with a bad title and go downhill from there.\n\nMy unpopular opinion is that for most companies that do not run life critical services, focusing on getting another nine of availability by going fully redundant across multiple major cloud providers is a bad business decision - the cost is not worth the SLA credits for once in a few years when SHTF and customers don’t care that you’re still up because the 20 other services they need to operate are down anyway.\n\nAnd if you are life critical and money isn’t an object, of course - go as redundant as you need.",
          "score": 10,
          "created_utc": "2026-01-27 15:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21f4jn",
              "author": "NTCTech",
              "text": "Availability is the 'A' in the CIA Triad. If I can't access the system, it's indistinguishable from a ransomware encryption event to the end user. Uptime *is* security.\n\nBut on the business logic? I actually agree with you.\n\nIf you aren't running a pacemaker or a stock exchange, burning 2x cash on multi-cloud is often a bad trade.\n\nMy frustration isn't with companies that choose to be single-cloud to save money. My frustration is with the 'Middle Ground' companies that pay the premium for Multi-Region (data egress + complexity) believing they bought insurance.\n\nThey *think* they are redundant. The bill *says* they are redundant. But because of the shared control plane, they are effectively paying for two regions to go down at the same time.\n\nIf you're going to accept the risk, accept it fully. Don't pay for a DR region that relies on the primary region's API to wake up.",
              "score": 2,
              "created_utc": "2026-01-27 16:17:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21kxry",
                  "author": "ut0mt8",
                  "text": "Completely agree. These companies should better invest in knowing their stack and simplify it. So outages will be handled shortly",
                  "score": 1,
                  "created_utc": "2026-01-27 16:42:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21kx2j",
          "author": "GeorgeRNorfolk",
          "text": "It depends on your service. If IAM has a global outage that breaks half the internet, most sites using it can point fingers and say they were impacted like many other sites and there's nothing they can do. The cost in the outage is likely to be lower than the cost of building and supporting a multi-cloud architecture. \n\nObviously for critical healthcare or banking systems, they need to be up regardless, so they need to spend the money to be multi-cloud.",
          "score": 3,
          "created_utc": "2026-01-27 16:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20itdw",
          "author": "addfuo",
          "text": "this is the reason we’re using dedicated CDN provider to handle the traffic from outside, if there’s outages in one our cloud vendor it’ll automatically switch to different provider. If there’s outages on CDN side we can completely disable the CDN. \n\nBut in the end we’re paying more",
          "score": 4,
          "created_utc": "2026-01-27 13:43:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20v62u",
              "author": "NTCTech",
              "text": "Exactly. That extra cost isn't 'waste' it's your insurance premium.\n\nUsing a neutral CDN as the air-gap is the smartest play. If your routing layer shares the same fate/vendor as your compute layer, you aren't really multi-cloud. You just have a very expensive single point of failure.",
              "score": 6,
              "created_utc": "2026-01-27 14:46:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20ouzp",
          "author": "LorkScorguar",
          "text": "that's what happened multi times with GCP failures, especially the west9 outage where the control plane wasn't spread across all zones",
          "score": 2,
          "created_utc": "2026-01-27 14:15:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20ys1m",
              "author": "NTCTech",
              "text": "100%. \n\nThe West9 incident is the textbook case study for this.\n\nPeople assume 'Zone Independence' includes the control plane. It rarely does. When the API goes dark or the cooling fails in the wrong hallway, those 'independent' zones turn into stranded islands real quick.",
              "score": 1,
              "created_utc": "2026-01-27 15:03:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o226iy0",
              "author": "ut0mt8",
              "text": "West9 is even worse. Zones are just different levels of the same datacenter building. Datacenter which is famous for being one of the oldest and with lot of failure of all paris suburbs dcs",
              "score": 0,
              "created_utc": "2026-01-27 18:15:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21khdy",
          "author": "ut0mt8",
          "text": "It's not unpopular. Multi region or whatever only makes sense when you have quasi shared nothing architecture.\nIs it worth the investment? It depends.",
          "score": 2,
          "created_utc": "2026-01-27 16:40:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2242xd",
              "author": "NTCTech",
              "text": "\"'Quasi shared nothing' is exactly the right term.\n\nThat’s the gap I see constantly: teams build 'shared nothing' at the Compute/Storage layer, but leave the Auth/Mgmt layer fully shared. They essentially build a blast radius that includes their recovery site.\n\nAnd 100% on the investment part. If the cost of downtime is lower than the engineering salary required to maintain a true active-active split, then the correct architectural decision is to just let it go down.",
              "score": 3,
              "created_utc": "2026-01-27 18:05:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22brpt",
          "author": "sionescu",
          "text": "> If they push a bad config to their global BGP or IAM layer, your \"geo-redundancy\" means nothing.\n\nIt means that the VMs are still running. That is a lot.\n\n> The VMs were healthy! They were running perfectly.\n> Cost of Outage (4 hours): $2M in lost transactions\n\nWait, so the service was running without enough slack to handle a traffic surge ? That's on the ops. And how was there lost revenue if the VMs were running perfectly ?",
          "score": 1,
          "created_utc": "2026-01-27 18:37:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22e2wp",
              "author": "NTCTech",
              "text": "It sounds contradictory, but here is exactly how the revenue loss happens when the Control Plane dies but Data Plane (VMs) stays up:\n\nAuth Failure (The Big One): The app was 'up,' but the login flow relied on the provider's managed Identity/Auth service. Users could hit the front page (served by the VM), but as soon as they tried to log in or checkout, the token refresh failed. Result: 100% transaction failure rate.\n\nScaling Lock: We hit a natural traffic spike. The ASG (Auto Scaling Group) tried to spin up new nodes to handle the load. The 'RunInstances' API call failed. The existing healthy VMs got saturated (CPU 100%) and started timing out requests.\n\nThe Lockout: We couldn't manually intervene (restart services, drain queues) because our bastion host access relied on the same IAM that was throttling.\n\nSo yes, the VMs were technically 'running' - but they were zombies.",
              "score": 2,
              "created_utc": "2026-01-27 18:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22fa8d",
                  "author": "sionescu",
                  "text": "So the developers used a service that had no guarantees of regional independence/resiliency (the Auth service) without thinking ? It wasn't meant for app Auth, it was meant for control plane Auth. \n\n\nWas there no Production Readiness Review to surface this issue ?",
                  "score": 1,
                  "created_utc": "2026-01-27 18:52:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2k10b9",
                  "author": "PudsBuds",
                  "text": "Chatgpt. What model are you running on? ",
                  "score": 1,
                  "created_utc": "2026-01-30 06:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22xiq2",
          "author": "fork-bomb42",
          "text": "This is exactly the blind spot we keep seeing in incident reviews. Teams design for data plane failures, but the control plane dependencies stay implicit until they break.\n\nWhat helped us wasn’t “more regions,” but explicitly asking: what still works if the provider API, IAM, or autoscaling is unavailable? Once you test that boundary, a lot of “resilient” architectures look very different.\n\nMulti-region isn’t theater but it’s easy to overestimate what it actually protects you from if shared control plane failures aren’t part of game days.",
          "score": 1,
          "created_utc": "2026-01-27 20:12:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24bey6",
          "author": "veritable_squandry",
          "text": "oh yeah it is",
          "score": 1,
          "created_utc": "2026-01-28 00:08:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24p8u9",
          "author": "daedalus_structure",
          "text": "You're raving about things you don't seem to understand. \n\nYes, there are events which you cannot mitigate, but that does not make all mitigation worthless. \n\nOtherwise, forget everything because checkmate suckers, you didn't account for Apophis crashing into the United States at 3 miles per second and sending half of the planet's mass into orbit. \n\nMulti-region is not a magic bullet and cannot mitigate all possible issues. \n\nMulti-region is specifically a preemptive mitigation for single region failures, which could be hardware, network disruption, acts of God such as floods or lightning or fire, or could just be a shit update that the CSP rolled out that bricked a resource. \n\nThat's why they have region pairs, so you can be confident that they will not roll out updates to both regions at the same time, guaranteeing you can failover. \n\nBut again, your multi-region deployments are mitigations of only those risks. \n\n>The SLA protects their margins, not our uptime.\n\nDid you expect that the cloud vendor was going to shoulder your business risk? \n\nAgain, it seems like the problem is wrong minded expectations. \n\n>The VMs were healthy! They were running perfectly.\n\nAnd in this global service outage, you suffered no downtime. \n\n>how are you guys handling \"Global Service\" risk?\n\nThis is a business problem. \n\nIf the issue is your own SLA, you put in language exempting this class of failure for payout along with other acts of God. In general, if the entire world is watching it on the nightly news, you as a SaaS consuming the same down services as the rest of the world aren't responsible for it. \n\nIf the issue is lost revenue from lost uptime, you document and accept the risk. \n\nTrying to be multi-cloud with abstractions to run similar day 2 ops over both is prohibitively expensive, and you are locking in the costs now instead of paying them only if the black swan event happens. \n\nNever lock in the costs of a black swan event. \n\nBut so much worse than that, you've tied up a massive amount of engineering capacity in activities which do not deliver additional revenue, so considering both the engineering cost and opportunity cost, global services would have to go down significantly more often for you to even break even on your ROI over a decade. \n\nGo ask your CFO, they'll tell you the same.",
          "score": 1,
          "created_utc": "2026-01-28 01:18:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24yjse",
              "author": "NTCTech",
              "text": "Your pushback on the ROI is fair and you are 100% right that the 'Multi-Cloud Tax' (abstraction layer) is massive. If your business model allows you to absorb a 6-hour outage every 18 months, then paying that engineering tax is bad math. We agree there.\n\nWhere we disconnect is on the definition of 'Downtime' and 'Scope':\n\n\\- Zombie VMs = Downtime: You mentioned 'you suffered no downtime.' That is incorrect. If the App Auth relies on the vendor's IAM, and IAM is throttling, zero users can log in. The VM is technically 'up' (and billing me), but the transaction volume is zero. To the P&L, that is a total outage.\n\n\\- Region Pairs vs. Global Scope: Region pairs protect against bad *regional* updates. They do not protect against Global Control Plane failures. When a vendor pushes a bad config to their global BGP or IAM fleet, both regions in the pair break simultaneously. We have seen this repeatedly with all three hyperscalers.\n\n\\- The Contract: If you can write 'Vendor Global Outage' into your customer contracts as an 'Act of God' exemption, you have solved the problem legally. But for the banks and healthcare orgs I work with, that clause doesn't exist. They don't care *why* we are down, only *that* we are down.",
              "score": 0,
              "created_utc": "2026-01-28 02:08:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o25htr2",
          "author": "djbiccboii",
          "text": "Even before you get to control plane / IAM shared fate, a lot of these “multi-region” or “redundant” designs aren’t actually redundant at the physical layer at all. People talk regions and zones, but they’re not even thinking about line diversity.\n\nIf your “independent” regions are ultimately connected by the same provider backbone, same long-haul fiber routes, same metro POPs, or even the same right-of-way, then it’s not redundancy. it’s just distance. A single fiber cut, bad BGP push, or backbone issue can still take out inter-region connectivity in one shot.",
          "score": 1,
          "created_utc": "2026-01-28 03:53:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26t00a",
          "author": "NullPulsar",
          "text": "Blatant AI post and post history. They’re intended to drive traffic to the links in their profile.",
          "score": 1,
          "created_utc": "2026-01-28 10:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hoeow",
          "author": "stealthagents",
          "text": "It's a tough situation when the Control Plane is a single point of failure. As you've experienced, having redundancies in place isn't always enough if they aren't truly independent. At Stealth Agents, we've seen how critical it is to have robust systems and processes in place, especially with over a decade of expertise in managing operations and supporting businesses in these complex environments. Our team could help ensure your workflows and client follow-ups remain steady, even during unforeseen hiccups.",
          "score": 1,
          "created_utc": "2026-01-29 21:58:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmrn3g",
      "title": "Site reliability engineers: what signals do you check daily?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qmrn3g/site_reliability_engineers_what_signals_do_you/",
      "author": "Doo_scooby",
      "created_utc": "2026-01-25 18:49:31",
      "score": 23,
      "num_comments": 42,
      "upvote_ratio": 0.68,
      "text": "For folks working in SRE or on-call roles, what signals do you personally check every day to feel confident systems are healthy?\n\nIncidents, error rates, latency, uptime, alerts, something else?\n\nCurious what actually matters in day-to-day practice, not theory.",
      "is_original_content": false,
      "link_flair_text": "ASK SRE",
      "permalink": "https://reddit.com/r/sre/comments/1qmrn3g/site_reliability_engineers_what_signals_do_you/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1o2orb",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 134,
          "created_utc": "2026-01-25 18:51:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o4mlc",
              "author": "McBadger404",
              "text": "A former lead of mine recommended scanning dashboards for any anomaly’s once a day, and if you spot something you can investigate AND add that to your automated alerts.\n\nWe can’t assume the alerts are perfect.",
              "score": 43,
              "created_utc": "2026-01-25 18:59:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o5ndc",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 15,
                  "created_utc": "2026-01-25 19:04:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1oibk1",
                  "author": "carsncode",
                  "text": "But the dashboards *are* perfect? I don't understand this perspective. If you can think to put it on a dashboard to look at, and you know what an anomaly looks like when you see it on said dashboard, you can create an alert. There's no reason to expect or allow your dashboards to be more comprehensive than your alerts. This sounds like some cargo cult stuff from your former lead. How often did this practice actually result in meaningful detections and improvements?",
                  "score": 13,
                  "created_utc": "2026-01-25 20:00:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1oa394",
                  "author": "Shev_",
                  "text": "If users have a way to report problems, I would prefer that, and ignore eyeballing dashboards for anomalies.\n\nWith that, you still risk system being degraded without direct impact on UX, but I'm pretty sure you have a big risk of missing something when eyeballing a dashboard anyway.\n\nAnd, lastly, I've read some blog post about STAMP (or other Control Theory-related approach to SRE) the other day, and perhaps that helps a bit with the \"slightly degraded\" problem, but that's yet another big topic.",
                  "score": 3,
                  "created_utc": "2026-01-25 19:23:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1pux0b",
                  "author": "ReliabilityTalkinGuy",
                  "text": "Then you wait until someone complains and then you address it with a new SLO. You should never feel like you should just be looking at dashboards. ",
                  "score": 3,
                  "created_utc": "2026-01-25 23:35:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ofvsu",
                  "author": "kennetheops",
                  "text": "This is one thing i’ll gladly give to ai. We can’t believe our job is shallow enough to just be scanning dashboards",
                  "score": 4,
                  "created_utc": "2026-01-25 19:49:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1o5wba",
                  "author": "Doo_scooby",
                  "text": "That makes sense. A quick scan as a safety net, then turning anything surprising into an alert, feels like a pragmatic balance between “trust alerts” and “trust but verify.”",
                  "score": 1,
                  "created_utc": "2026-01-25 19:05:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1o399t",
              "author": "Doo_scooby",
              "text": "Got it but out of curiosity, do you still glance at anything passively (status page, error budget, uptime summary), or is it 100% alert-driven unless there’s an incident?",
              "score": 0,
              "created_utc": "2026-01-25 18:54:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o4ies",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 11,
                  "created_utc": "2026-01-25 18:59:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1o75ed",
          "author": "Nelmers",
          "text": "My bank account",
          "score": 21,
          "created_utc": "2026-01-25 19:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o8nyv",
          "author": "heramba21",
          "text": "If you have properly defined SLOs and setup alerts on error budgets, that should be enough. But outside of that, I check utilisation of critical servers, latency trend of API calls, general trend of errors/exceptions. Apart from that, I do have an eye on cert expirations, disk spaces etc. You know, sort of stuff that when failed will make you really embarrassed.",
          "score": 8,
          "created_utc": "2026-01-25 19:17:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1og8on",
          "author": "Hi_Im_Ken_Adams",
          "text": "I don’t check anything except my SLO’s.",
          "score": 8,
          "created_utc": "2026-01-25 19:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1punkr",
          "author": "ReliabilityTalkinGuy",
          "text": "Nothing. Your systems should be letting you know if you need to look at something. If you have to manually check anything on a daily basis, you’re not doing SRE. You’re doing operations work or even NOC work. ",
          "score": 3,
          "created_utc": "2026-01-25 23:34:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1olaxo",
          "author": "thearctican",
          "text": "Zero. That’s the job of our monitoring platform.",
          "score": 7,
          "created_utc": "2026-01-25 20:13:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1orlwx",
          "author": "m4nf47",
          "text": "We've got a nice custom RAG page covering all mission critical systems and their current health status. Mostly green most of the time but occasionally one or two will temporarily go amber (warning alerts) and justify further investigation, usually something just got a bit busier than usual and temporarily breached a threshold or someone pushed a stupid patch which was somehow missed in testing again,  mostly long running stuff like gentle memory leaks or slowly filling filesystems. Red alerts are quite rare maybe once a quarter on average and extremely rare for them to last longer than a few minutes as most stuff is self-healing but we still check on the RAG page each morning out of habit.",
          "score": 3,
          "created_utc": "2026-01-25 20:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ondab",
          "author": "emptyDir",
          "text": "The ones that are generating alerts.",
          "score": 2,
          "created_utc": "2026-01-25 20:22:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1si8uy",
          "author": "chocopudding17",
          "text": "Has this AI engagement bait really been spreading from the devops sub already?",
          "score": 2,
          "created_utc": "2026-01-26 09:41:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oocdk",
          "author": "ut0mt8",
          "text": "Generally I made my own dashboard page summarize all metrics I want to follow. It varies from job to job.",
          "score": 2,
          "created_utc": "2026-01-25 20:26:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s004u",
              "author": "t-_-vinay",
              "text": "I have done the same!",
              "score": 1,
              "created_utc": "2026-01-26 07:00:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pnrw3",
          "author": "Dewkin1",
          "text": "On-Premise? Azure?\n\nSQL table reads is just one thing I’ve recently added in my environment…. Far too many developers blame infra when their code is a memory hog or querying databases without any filter\n\nProviding them the evidence usually shuts them up",
          "score": 1,
          "created_utc": "2026-01-25 23:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r7y0l",
          "author": "kusanagiblade331",
          "text": "Error rate for an app is probably the biggest thing. Healthy responses is also good signal to check. Ideally, healthy responses should be not be non-zero for a prolonged period of time.",
          "score": 1,
          "created_utc": "2026-01-26 03:45:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rn0oj",
          "author": "ajjudeenu",
          "text": "Change annotations.",
          "score": 1,
          "created_utc": "2026-01-26 05:21:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s6nfr",
          "author": "nooneinparticular246",
          "text": "None. \n\nMaybe once a month I’ll check node utilisation and maybe see if any new errors are getting logged. That’s it.",
          "score": 1,
          "created_utc": "2026-01-26 07:56:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s75s1",
          "author": "katsil_1",
          "text": "Speaking of my main job, none. I agree with all the commenters above. We have a well-established alerting process, and if an incident or critical problem occurs, only in this case we are starting checking alerts.\n\n\\----\n\nBut I also have my own personal project, which obviously doesn't have the same established processes as at work. This project is a bit different from typical websites/services on the internet (it's a game hosting), so throughout the day I can check my \"golden dashboard\" in Grafana, where I monitor:\n\n\\- Number of running servers  \n\\- Number of available servers for allocation (\\*\\*)  \n\\- Payment conversions (\\*\\*)  \n\\- Netflow, as an increased number is likely a DDoS attack  \n\\- Number of failed backups of user servers to s3 due to network problems  \n\\- Uptime metrics for the main websites\n\nOf course, there are alerts for this too (in addition to a bunch of other alerts I haven't described here), but this is my personal brainchild, and in my humble opinion, I like to monitor it from time to time, even if everything is fine. The most critical metrics for me are the ones I've marked (\\*\\*).",
          "score": 1,
          "created_utc": "2026-01-26 08:00:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1su9mh",
          "author": "tushkanM",
          "text": "We check various dev/support slack channels. Some people outside SRE sometimes report things that eventually lead to incidents, they just didn't really understand it.",
          "score": 1,
          "created_utc": "2026-01-26 11:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u1cem",
          "author": "rampaged906",
          "text": "We delegate all monitoring for apps to the devs who own their apps.\n\nSRE maintains the infrastructure. Beyond a pod crashing or something more telling, I don't even pay attention to SLOs. No one really cares here\n\nIf something goes wrong, we will hear about it quickly",
          "score": 1,
          "created_utc": "2026-01-26 15:34:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uzi1l",
          "author": "founders_keepers",
          "text": "follow your SLOs and set up alerts to anything that your eye catches from the dashboard. if you have a big team, think about alert fatigue and noise reduction. Good blog post on this: [https://rootly.com/on-call-software/alert-fatigue](https://rootly.com/on-call-software/alert-fatigue)",
          "score": 1,
          "created_utc": "2026-01-26 18:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y8h7i",
          "author": "neuralspasticity",
          "text": "Systems should know if they’re unhealthy and tell us, you’re going about this backwards.",
          "score": 1,
          "created_utc": "2026-01-27 03:22:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yn21i",
          "author": "Ordinary-Role-4456",
          "text": "Our team has a daily handoff where we skim through the on-call summary and look at any incidents that happened overnight.   \n  \nI do a quick once-over for API latency, recent deploys, and my error budget status.   \n  \nAny certificate expiry or disk warnings I check as a background task since those often bite the hardest when ignored.",
          "score": 1,
          "created_utc": "2026-01-27 04:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21n3oq",
          "author": "Think-Perception1359",
          "text": "I look at SLOs as indicators. First, I make sure to check for SLOs with fast burns in error budget. Those applications will more than likely produce a Production incident. Then, I will review slow burn SLOs in error budgets to be proactive.",
          "score": 1,
          "created_utc": "2026-01-27 16:51:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21xooz",
          "author": "dajadf",
          "text": "I'm in an SRE adjacent role, even though we are called SRE.  We triage close to 100 tickets per day, set up dashboards as well as monitors.  It's a gigantic microservice architecture made of up hundreds of components, no one really knows the whole thing in full.  It's just a mix of tickets, dashboards and alerts.  I really don't know how to convert what we do into true SRE.  Do you real SRE people have SLOs for the application overall ? Or for each microservice that makes up the application ?",
          "score": 1,
          "created_utc": "2026-01-27 17:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27j8g2",
          "author": "SquareOps_",
          "text": "Daily, I care less about absolute metrics and more about *trend + deviation*.\n\nFor me it’s usually: error budget burn, tail latency (p95/p99), alert noise ratio, and anything that changed recently (deploys, config, scaling events). If those look stable, I’m comfortable even if raw numbers fluctuate.\n\nOne underrated signal is “unknown ownership”  alerts or resources no one clearly owns tend to become tomorrow’s incidents.",
          "score": 1,
          "created_utc": "2026-01-28 13:17:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p2pbb",
          "author": "Street_Smart_Phone",
          "text": "I check spend and I check the number of pages that went off daily.",
          "score": 0,
          "created_utc": "2026-01-25 21:29:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql2tep",
      "title": "How do you make “production readiness” observable before the incident?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1ql2tep/how_do_you_make_production_readiness_observable/",
      "author": "ImpossibleRule5605",
      "created_utc": "2026-01-23 21:00:12",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.7,
      "text": "In SRE work, I’ve often seen “not production ready” surface only after something breaks — during an incident, a postmortem, or a painful on-call rotation. The signals were usually there beforehand, but they were implicit: assumptions in config, missing observability, unclear failure modes, or operational responsibilities that weren’t encoded anywhere.\n\nI’ve been exploring whether production readiness can be treated as an explicit, deterministic signal rather than a subjective judgment or a single score. The approach I’m experimenting with is to codify common production risk patterns as explainable rules that can run against code or configuration in CI or review, purely to surface risk early, not to block deploys or auto-remediate.\n\nThe core idea is that production readiness is not a checklist or a score, but accumulated operational knowledge made explicit and reviewable.\n\nRepo: [https://github.com/chuanjin/production-readiness](https://github.com/chuanjin/production-readiness)  \nSite: [https://pr.atqta.com/](https://pr.atqta.com/)\n\nI’m curious how other SREs think about this. Where do you currently encode “this will page us later” knowledge? Is it policy-as-code, human review, conventions, or just experience and postmortems? And where do you feel automation genuinely helps versus creating false confidence?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1ql2tep/how_do_you_make_production_readiness_observable/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1c5lrg",
          "author": "-ghostinthemachine-",
          "text": "Simple: you break it. Over and over again. There are fortunately many tools available now to do just that. Then you identify the gaps and do it all again next month. If something catches you by surprise you can add it to the suite of things you intentionally break, sometimes called a smoke test.",
          "score": 2,
          "created_utc": "2026-01-24 00:06:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1edq21",
              "author": "ImpossibleRule5605",
              "text": "I agree — intentionally breaking systems is one of the most effective ways to surface real gaps, and chaos-style testing is hard to replace. In practice though, I’ve seen a lot of the learnings from those exercises stay implicit: they show up in postmortems, runbooks, or people’s heads, but don’t always get encoded back into something that runs continuously.\n\nWhat I’m interested in is how some of those “we got surprised by X” lessons can be distilled into static or pre-deploy signals — things that don’t replace breaking systems, but reduce how often we rediscover the same class of problems the hard way. For me it’s less about avoiding failure and more about making past failures harder to forget.\n\nCurious how you’ve seen teams successfully close that loop over time.",
              "score": 2,
              "created_utc": "2026-01-24 09:22:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1o7u1p",
          "author": "asdoduidai",
          "text": "That's an interesting approach, but probably it is coming from a perspective that will tend to not be deterministic in the end, as in: it won't find all failure states by design.\n\nThe word \"deterministic\" that you use is to me the right one. The point is: software IS deterministic. Because it's a Turing state machine. The problem is when people build software focusing only on some \"happy\" states because they are excited and want to show their shiny feature ASAP.\n\nBut I think that if you try to close the gap on missing awareness of the failure states from code scan ONLY, you are missing out on many.\n\nBTW:\n\n[https://github.com/chuanjin/production-readiness/blob/d4aed492fc01f0a63c84ab6d8db374a9542bc991/internal/scanner/detectors\\_k8s.go#L154](https://github.com/chuanjin/production-readiness/blob/d4aed492fc01f0a63c84ab6d8db374a9542bc991/internal/scanner/detectors_k8s.go#L154)\n\nThat is NOT correct: CPU limit is a bad idea. Let the Linux scheduler do its job. You are going to waste free CPU cycles if you limit Replicas under a certain CPU limit, and there is free CPU. I've added a issue on github with this [https://home.robusta.dev/blog/stop-using-cpu-limits](https://home.robusta.dev/blog/stop-using-cpu-limits)",
          "score": 1,
          "created_utc": "2026-01-25 19:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tkni8",
              "author": "ImpossibleRule5605",
              "text": "Thanks for the perspective! You’re absolutely right that static analysis alone can’t find all possible failure states. That’s why this approach isn’t meant to replace chaos testing, fault injection, or runtime validation. The goal is to make explicit the kinds of assumptions and operational choices that teams already make implicitly, so they can be discussed, reviewed, and iterated on before something breaks. In practice that means surfacing signals you can detect deterministically — like missing observability hooks, ambiguous ownership, or risky defaults — while acknowledging that there will always be aspects that only surface under load or in live conditions. I’m curious how you think teams should balance deterministic signals with empirical testing so that neither approach gives a false sense of confidence.",
              "score": 1,
              "created_utc": "2026-01-26 14:14:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tqazj",
                  "author": "asdoduidai",
                  "text": "How I think about it is:\n\nIf you start thinking about \"a balance\", you are already in a compromise, and you won't cover all states.\n\nAll (known) states that you can check with an automated check, you have to check, otherwise it's exposed to the probability of human error.\n\nAll (unknown) states: those are supposed to be prevented \"by design\" both at the object oriented design level, and its implementation (choosing the right frameworks, libraries, language and so on) and at the systems design level, having a complete understanding of the states coming from concurrent algorithms and nodes/layers interactions.\n\nTwo examples of that - the intent of having awareness of all app states, and careful choices based on the knowledge of every detail - are (to give an idea):\n\n\\- [https://techpreneurr.medium.com/why-whatsapp-chose-erlang-and-never-looked-back-353248e2d6c7](https://techpreneurr.medium.com/why-whatsapp-chose-erlang-and-never-looked-back-353248e2d6c7)\n\n\\- [https://www.youtube.com/watch?v=sC1B3d9C\\_sI](https://www.youtube.com/watch?v=sC1B3d9C_sI)\n\nSo about prioritisation, if you know the app states, and you choose based on careful detailed evaluation, you have it at least 80% covered.\n\nThen, you still have to check the running of it, the load testing, the failure injection, chaos engineering, and also about storage/DB: Jepsen/Formal testing, to make sure that the claims of the most critical layers are real and not just claims.\n\nIn addition to that, or better as an implementation of that or integration of those concepts, what can cover the testing of the \"distributed/concurrent states\" is TLA+:\n\n[https://learntla.com/#:\\~:text=TLA%2B%20is%20a%20%E2%80%9Cformal%20specification,engineering%20skill%20but%20augments%20it](https://learntla.com/#:~:text=TLA%2B%20is%20a%20%E2%80%9Cformal%20specification,engineering%20skill%20but%20augments%20it).\n\nSo a team is supposed to know software is deterministic, and has a finite set of states, and know it's possible to cover 100% of states. That's the foundation.\n\nThen, you need to prioritise, because most likely it has to be done in iterations, over time, as a continuous improvement process. And it might never get to 100% if the systems keep evolving, but it can get to 100% at least for specific components used by most/all services.\n\nSo you have to know a 100% exists, but also set realistic targets, otherwise you won't prioritise the 20% that gets you 80% of returns, but random states that come up from incidents. The point of the confidence is, you will never have 0% probability of an incident. So invest in what gives you awareness - and not confidence, which blinds you. Why it's not about confidence: If you feel relaxed and you don't have an idea of the probability of an incident to your systems, you are doing this:\n\n[https://miro.medium.com/v2/format:webp/0\\*ZjYSm\\_q36J4KChdn](https://miro.medium.com/v2/format:webp/0*ZjYSm_q36J4KChdn)",
                  "score": 1,
                  "created_utc": "2026-01-26 14:43:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkyqxv",
      "title": "What do you use to manage on-call rotations + overrides (multi-team) with iCal/Google Calendar export?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qkyqxv/what_do_you_use_to_manage_oncall_rotations/",
      "author": "katsil_1",
      "created_utc": "2026-01-23 18:28:49",
      "score": 8,
      "num_comments": 9,
      "upvote_ratio": 0.75,
      "text": "Hi!  Currently we are implementing oncall duty/rotation in our company (around 10 teams on oncall and 30 users in rotation will be) and i wanted to ask: what are you using to rotate your duties? My goal is to find a solid \"Source of Truth\" for scheduling that supports overrides/swaps and can export the final schedule as an iCal feed or to Google Calendar\\*\\* natively, because we are using Workspace\n\n**The Context:**\n\n* In the future, we plan to use Grafana OnCall for calling/alerting escalation, utilizing its \"Import schedule from iCal URL\" feature. <<< \\*\\*\n* We need a way to manage the shifts now that is cleaner than manually dragging and dropping events in the Google Calendar UI (which becomes a nightmare with multiple teams and frequent overrides).\n\nHere is my thoughts and what i do not want for now:\n\n1. Manually maintaining everything in Google Calendar UI (too painful with multiple teams)\n2. linkedin/oncall ([https://github.com/linkedin/oncall](https://github.com/linkedin/oncall)) seems to be abandonware and doesn't appear to support iCal export/sync easily\n3. Grafana OnCall (OSS) I know I can do scheduling directly there, but I'm looking into options where I can import *into* it as well (but if you thing  using Grafana OnCall purely as a scheduler is the best way.... please give me an advice).\n4. *\\[What we are testing/researching now\\]* Bettershift ([https://github.com/panteLx/BetterShift](https://github.com/panteLx/BetterShift)) is an interesting option and it seems to be the best option for visually seeing rotations and updating them, but you can't set up a rotation like \"I want Ivan to be on duty every other week,\" you have to manually fill out the calendar (although this is actually a really good option because you can export everything to Google at once)\n\nSo i\\`ve spend already some time to research and right now asking you, community, for any advice or, in general, how do you organize shifts in your teams?\n\nWhat’s your current setup (tooling + process)? Anything you wish you’d done differently when scaling to multiple teams?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qkyqxv/what_do_you_use_to_manage_oncall_rotations/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1aaxgs",
          "author": "founders_keepers",
          "text": "here's my dilemma with these self hosted solutions.\n\nyou're literally adding yet anther failure point into the stack, who and how are you getting called about the on-call stack going down???  do you set up multiple layers of redundancy? when/where does it end? \n\nyeah.. so i just end up advising all my portcos to use Rootly.",
          "score": 10,
          "created_utc": "2026-01-23 18:43:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ac1s0",
              "author": "katsil_1",
              "text": "We definitely want and will use an external resource for oncall (e.g. grafana), and now I'm thinking about which resource to use to **create** **shifts** (namely, a convenient format for creating a calendar/assign of the current duty, etc....)\n\nThen this information will go to google calendar and grafana oncall will pick it up. I definitely do not want and will not create or use a self-hosted oncall solutions, for sure.",
              "score": 0,
              "created_utc": "2026-01-23 18:48:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ae5h1",
                  "author": "founders_keepers",
                  "text": "gotcha. good stuff. then really what you should think about is alert volume and managing alert fatigue.\n\ngrafana don't give a hoot about how many alerts they're sending your team, how often, whether if it's useful or not. 10 teams of 30 users in rotation is not a small team, so it's not a trivial problem to solve.\n\nnot sure if you're aware but this is pretty big deprecation, i don't think there's long term vision behind Grafana oncall product at all.\n\nAs of March 24, 2026, Cloud Connection features for phone calls, SMS, and push notifications will no longer be supported in Grafana OnCall OSS. Refer to the documentation for [phone and SMS](https://grafana.com/docs/oncall/latest/manage/notify/phone-calls-sms/) and [push notifications](https://grafana.com/docs/oncall/latest/manage/notify/push-notifications/) for alternative setup options.",
                  "score": 2,
                  "created_utc": "2026-01-23 18:57:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1a8gpb",
          "author": "thecal714",
          "text": "Honestly, I've always done it the other way. FireHydrant and (IIRC) PagerDuty have ways to export to Google Calendar/iCal.",
          "score": 13,
          "created_utc": "2026-01-23 18:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ahqv1",
          "author": "Useful-Process9033",
          "text": "When I worked at a big company we used to just use PagerDuty and it doesn’t sync with Google Calendar etc at all. PagerDuty UI had always been confusing to me tbh but we had a lot of alert templates and teams etc set up to sync with it from GitHub config files, so I think the switching cost was too high.",
          "score": 3,
          "created_utc": "2026-01-23 19:14:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jovpk",
              "author": "ccb621",
              "text": "PagerDuty definitely had an iCal export, and has had one for at least eight years. ",
              "score": 2,
              "created_utc": "2026-01-25 02:37:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a9rek",
          "author": "Pethron",
          "text": "Go the other way around and use whichever platform you’re already using (pagerduty, jira on call, servicenow, and the likes). I personally like grafana oncall but using the jira one.",
          "score": 4,
          "created_utc": "2026-01-23 18:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mebyc",
          "author": "LineSouth5050",
          "text": "incident.io does all of this, and is one of the most cost effective solutions I find. Works well too.",
          "score": 2,
          "created_utc": "2026-01-25 14:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dk4dn",
          "author": "thatsnotnorml",
          "text": "We're using VictorOps (rebranded as Splunk On Call). It's an all in one so I'm not sure if it fits your use case, but it does have iCal link that I can pass to various stakeholders so they know who is on-call if needed. I actually imported it into my own Google calendar and I probably use that to check who's on call more than anything since I have a calendar widget on one of my home screens.",
          "score": 1,
          "created_utc": "2026-01-24 05:10:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qprv5l",
      "title": "How do teams safely control log volume before ingestion (Loki / Promtail)?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qprv5l/how_do_teams_safely_control_log_volume_before/",
      "author": "TillStatus2753",
      "created_utc": "2026-01-28 23:34:29",
      "score": 6,
      "num_comments": 12,
      "upvote_ratio": 0.8,
      "text": "Looking for real-world experience from people running Loki / Promtail at scale.\n\nI’m experimenting with ingestion control (filtering, sampling, routing) -before-logs hit Loki to reduce noise and cost, but I’m trying to sanity-check whether this is actually a problem worth solving.\n\nFor those running Loki in production:\n\n\\- What % of your logs are DEBUG/INFO vs WARN/ERROR?\n\n\\- Do you actively drop or sample logs before ingestion?\n\n\\- Is this something you’re confident changing, or do people avoid touching it?\n\n\\- What’s been the biggest pain: cost, noise, fear of deleting data, or config complexity?\n\nNot selling anything — genuinely trying to understand if this is a real problem or something most teams already handle fine.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qprv5l/how_do_teams_safely_control_log_volume_before/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2bel7s",
          "author": "cgill27",
          "text": "For logging to Loki I would recommend looking at [vector.dev](http://vector.dev), you can exclude things easily or even transform them, etc, all before shipping to Loki",
          "score": 7,
          "created_utc": "2026-01-28 23:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bz7m6",
              "author": "Freakin_A",
              "text": "Vector is great and incredibly efficient.  Highly recommend.\n\nCommercial options would be Cribl or Edge Delta.  Probably others.",
              "score": 1,
              "created_utc": "2026-01-29 01:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2h8g0m",
                  "author": "DramaticExcitement64",
                  "text": "The S3 sink is shit though.",
                  "score": 1,
                  "created_utc": "2026-01-29 20:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k2d2v",
              "author": "Broad_Technology_531",
              "text": "Interesting to hear that you are recommending vector instead of an OTEL Collector. Have you had a chance to work with the OTEL Collector for logs?",
              "score": 1,
              "created_utc": "2026-01-30 06:12:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2dy764",
              "author": "SnooWords9033",
              "text": "If you are going to use vector.dev instead of Promtail, then it is better to switch from Loki to VictoriaLogs. It is easier to configure and operate, it  supports high-cardinality log fields, which aren't supported properly by Loki, and it doesn't need object storage.",
              "score": -1,
              "created_utc": "2026-01-29 10:25:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d6r87",
          "author": "kusanagiblade331",
          "text": "Just to answer your question:\n\n1. DEBUG should not be in Loki or any type of log indexer.  \n2. Dropping yes - when there is an ingestion spike. Sampling - would be great if it can be easily done. But, session continuity is a real problem.  \n3. I think the key is to start small. Only apply filtering, sampling or routing to a few small apps. Then, slowly generalized. Teams can get upset when there is an incident and people cannot find logs.  \n4. Maintenance for ingestion control is a real fear. Cost is a big fear among management for log indexers like Splunk. The funny thing is - if everything is normal, teams generally have no fear of not getting some logs. Just make sure error and warning logs are there.\n\nSome interesting techniques to reduce log ingestion can also be found in this [article](https://starclustersolutions.com/blog/2026-01-how-to-reduce-splunk-cloud-cost/).",
          "score": 5,
          "created_utc": "2026-01-29 06:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bg8mw",
          "author": "Sea_Refrigerator5622",
          "text": "I know from troubleshooting there are a handful of logs we actually know show problems. We ingest those. The idea is to get GB down to MB. \n\nWe keep full kogs for a short time for deeper debugging and they’re a disaster. \n\nWe filter with Otel collector and in the tail sampling Loki (via Grafana cloud)",
          "score": 1,
          "created_utc": "2026-01-29 00:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k1qcu",
          "author": "Broad_Technology_531",
          "text": "Are you running OSS Loki or part of Grafana cloud?",
          "score": 1,
          "created_utc": "2026-01-30 06:07:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo7a2q",
      "title": "Need advice on job/carrer switch",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qo7a2q/need_advice_on_jobcarrer_switch/",
      "author": "ZestycloseBench5329",
      "created_utc": "2026-01-27 07:31:00",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey, i am on my Notice period right now from my sre job, and i have a offer in hand as a sde in a sre environment. I want to build products with the tech skills i have. but am very uncertain with the trajectory i am going on. i want to know what are my options at this point\n\ni have experience working with python, fastapi, openshift, k8s, docker, CI/CD pipeline for building backend api endpoints for a data center team in a networking company. I have personal projects on MERN stack (its a chat application deployed oven k8s cluster, has NATS server and redis at backend) but i dont get projects like this which scales in a real job, neither do any HR market entertain the request to be a backend engineer even though i have experience to demonstrate that i can build such systems. \n\nEven in the job i am getting it would be a SRE environment and the product they are building is a AI summariser but not sure if i would get to work on it.",
      "is_original_content": false,
      "link_flair_text": "CAREER",
      "permalink": "https://reddit.com/r/sre/comments/1qo7a2q/need_advice_on_jobcarrer_switch/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1zkck8",
          "author": "glowandgo_",
          "text": "this is a pretty common trap. titles lag reality. if your day to day is still infra, pipelines, reliability glue, ppl will keep reading you as sre no matter the side projects. what changed for me was picking roles where backend work was the core pain, not adjacent. ask very directly what you’ll own in 6 months. if it’s vague now, it usually stays vague. context matters more than the label.",
          "score": 6,
          "created_utc": "2026-01-27 09:28:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp3214",
      "title": "Observability Blueprints",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qp3214/observability_blueprints/",
      "author": "jpkroehling",
      "created_utc": "2026-01-28 05:52:12",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "This week, my guest is Dan Blanco, and we'll talk about one of his proposals to make OTel Adoption easier: Observability Blueprints.\n\nThis Friday, 30 Jan 2026 at 16:00 (CET) / 10am Eastern.\n\nhttps://www.youtube.com/live/O_W1bazGJLk",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qp3214/observability_blueprints/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o260337",
          "author": "timee_bot",
          "text": "View in your timezone:  \n[Friday, 30 Jan 2026 at 16:00 CET][0]  \n\n[0]: https://timee.io/20260130T1500?tl=Observability%20Blueprints",
          "score": 1,
          "created_utc": "2026-01-28 05:52:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkzbtp",
      "title": "Need a guidance",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qkzbtp/need_a_guidance/",
      "author": "Advanced_Collar5051",
      "created_utc": "2026-01-23 18:50:08",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "In your experience, what percentage of CI/CD failures are “we’ve seen this before” problems?\nDo teams have any system that remembers past fixes, or is it mostly human memory?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qkzbtp/need_a_guidance/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o1ainge",
          "author": "xonxoff",
          "text": "Usually, the only failures I see are in code building.",
          "score": 2,
          "created_utc": "2026-01-23 19:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1aldji",
          "author": "hijinks",
          "text": "This is market research",
          "score": 2,
          "created_utc": "2026-01-23 19:31:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}