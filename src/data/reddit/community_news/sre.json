{
  "metadata": {
    "last_updated": "2026-02-05 17:09:59",
    "time_filter": "week",
    "subreddit": "sre",
    "total_items": 5,
    "total_comments": 25,
    "file_size_bytes": 32419
  },
  "items": [
    {
      "id": "1qvimz3",
      "title": "How are you handling triage across multiple channels? (Slack, Email, Jira)",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qvimz3/how_are_you_handling_triage_across_multiple/",
      "author": "Coolaid2353",
      "created_utc": "2026-02-04 07:59:23",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "I‚Äôm looking at our current on-call process and realized how much time we‚Äôre losing to manual triage.\n\nThe biggest issue is when an incident hits after-hours. Usually, someone has to wake up, and they have to check if a Slack alert matches an email from a high-priority client, look up the service owner, and then decide whether to escalate it or let it wait until morning.\n\nIt feels like most of this logic is straightforward (Severity + Client Tier + Service Impact), yet we‚Äôre still using a person to do the routing.\n\nHas anyone successfully automated the \"decision layer\" between the incoming signal (Email/Slack/PagerDuty) and the actual response (Jira ticket/Escalation)? Or is the risk of an automated system mis-categorizing a P0 issue still too high to trust?\n\nAm I missing some tool, or do other people feel this pain too?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qvimz3/how_are_you_handling_triage_across_multiple/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o3i1pa3",
          "author": "xonxoff",
          "text": "If you are using PagerDuty, that is your signal. I‚Äôd stop sending alerts to anything else, you just create more noise and fatigue. If it‚Äôs important, you page on-call. That page should have a link to a runbook that should have steps to take to resolve the issue and a path of escalation. If you are unable to resolve the issue, create a slack channel for the incident and invite those who need to be in there and be sure to keep all conversations in that channel, this will help when you write up your RCA.",
          "score": 11,
          "created_utc": "2026-02-04 09:09:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j4282",
              "author": "Brief-Article5262",
              "text": "100% correct. This sounds rather like you're creating noise in different channels which simply confuses prioritization & reduces MTTA & MTTR. Focus your signals on one tool (PagerDuty or whatever you're using for your IRM) and then distribute tickets from there. Alerts should only be sent if necessary and with clear steps on how to act next.",
              "score": 1,
              "created_utc": "2026-02-04 13:58:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i1pfp",
          "author": "SudoZenWizz",
          "text": "We centralize all the channels in only on \"master channels\": Jira.\n\nAlerts from systems are automatically pushed to Jira through Opsgenie(operations), Jira is managing specific e-mail accounts for incoming tickets via e-mail and lastly users have their jira portal to raise tickets.\n\nIn terms of sistems that needs to be monitored for performance we are using for us and our clients checkmk. Based on timeperiods and notification rules, our on-call personnel receives alerts outside working hours only for services and hosts that have 24/7 support contract or the systems are businness critical. Others can wait until normal working hours.\n\nOf course, SLA is also very important. How long a client can wait an answer.\n\nBasically, key in this is having a general view in a single location and trust in the systems deployed and configured. Of course, this trust is based on many years of monitoring systems (network, servers, private and public cloud services, etc.) and we found in checkmk the flexibility to monitor and alert properly since it's begginings (more than 16 years ago).\n\n",
          "score": 2,
          "created_utc": "2026-02-04 09:09:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pmttj",
          "author": "vibe-oncall",
          "text": "We built **Vibe On-Call** specifically to address the manual toils/workflows that is involved to jump around different platforms. It‚Äôs an AI-driven pager with an orchestrator that sits across Slack, email, Jira, monitoring tools, etc., and decides *who* to page and *when* based on severity, service impact, customer tier, and past incidents.\n\nThe big thing for us was keeping everything dead simple for on-call. No new dashboard - the AI agent lives in Slack. It acks alerts, explains why it‚Äôs paging (or not), escalates if there‚Äôs no response, spins up the incident channel, and keeps Jira updated. Humans can override anytime, but most teams start by letting it handle after-hours triage and go from there.\n\nSo yeah, this pain is real, and the logic really is straightforward - it just shouldn‚Äôt require a half-awake human anymore",
          "score": 2,
          "created_utc": "2026-02-05 13:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m1vrx",
          "author": "jjneely",
          "text": "Whoa, you are using TWO people not just one.  Sounds like your pager system is your customers.",
          "score": 1,
          "created_utc": "2026-02-04 22:18:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qq4h1",
          "author": "Over-Inspector-5271",
          "text": "The core of the issue is that you're asking a person to do a job that should be handled by metadata. The solution is **data enrichment** via service labels and tags: the key is to stop treating every host the same way.\n\nInstead of forcing an operator to decide if a client is 'High Priority' in the middle of the night, you should store that information directly in Checkmk using **labels**. This allows you to build extremely granular notification rules: if the system already knows the client tier and service criticality, the notification is triggered (or suppressed) with the correct escalation level automatically.\n\nAdditionally, you can use Checkmk‚Äôs **Business Intelligence (BI)** to map dependencies. This way, if a database goes down, the system automatically understands that 'Customer Service X' is impacted. The result? You get a single high-priority notification focused on the service availability, rather than 10 separate technical alerts that someone has to manually correlate.",
          "score": 1,
          "created_utc": "2026-02-05 16:37:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr7nbo",
      "title": "Looking for a whitepaper/journeydoc for SRE transition",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qr7nbo/looking_for_a_whitepaperjourneydoc_for_sre/",
      "author": "hiveminer",
      "created_utc": "2026-01-30 14:55:33",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "So guys, in 2017, Juniper released a very nicely prepared 16 page document on the transition/journey to [NRE](https://events19.linuxfoundation.org/wp-content/uploads/2017/12/NRE-and-DevNetOps-Overview-ONS-Sept-2018.pdf)(Network Reliability Engineering).  I think it is well written.  Now, the question is, has a document like that been written for sysops?  SRE?  If now, those boasting the title of SENIOR SRE.. should consider it.  In fact, I think there are a number of parallels within that document which would apply to SRE.  We are staring at the dawn of IT second brain/digital sidekick.  That can also be incorporated, if not now, maybe for a possible version 2.",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qr7nbo/looking_for_a_whitepaperjourneydoc_for_sre/",
      "domain": "self.sre",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qudu6h",
      "title": "The requirement to deliver above all else",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qudu6h/the_requirement_to_deliver_above_all_else/",
      "author": "BoringTone2932",
      "created_utc": "2026-02-03 01:08:38",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.73,
      "text": "How do you deal with the corporate nature of the push to deliver above all else? \n\nSure, XYZ can be scripted, but the situation that caused XYZ shouldn‚Äôt exist in the first place. \n\nSure, we can move to Aurora, but we are just carrying our problems with us. \n\nRepeatedly, corporate nature drives increases to the top line, decreases to the bottom line and progress above all else. We should fix this becomes we should deprecate this in favor of that. Change creates appearance of improvement when in reality, the new servers have host files with a laundry list of hostnames because internal DNS team didn‚Äôt move fast enough, or the build pipeline has manual post-steps because we manually made changes across the environment and fixing the build pipeline isn‚Äôt prioritized. \n\nHow do you convince leadership that the small technical intricacies matter? That the small technical intricacies create long term barriers to reliability? That the steps we work around now will come back to bite us, even if they (or I) are not around anymore for it. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qudu6h/the_requirement_to_deliver_above_all_else/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o39gu61",
          "author": "calibrono",
          "text": "I haven't seen solutions other than having leadership be actually ex-engineers themselves.",
          "score": 15,
          "created_utc": "2026-02-03 01:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39qlyb",
              "author": "namenotpicked",
              "text": "It's this or non-technical leadership actually listening to the engineers and agreeing to build in tech debt work as part of regular work. Not being deprioritized or cut back. Of course I've never seen this happen but one can dream.",
              "score": 4,
              "created_utc": "2026-02-03 02:12:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o39k2ds",
          "author": "RoboErectus",
          "text": "This is easy to do when you actually log the effort going into things you only do because of tech debt. \n\nWhat‚Äôs your mttd and mttr? How long are your runbooks? How many engineers do you have on support? How fast can an engineer get a meaningful change into production, and how often does that cause a breaking side effect?\n\nThis is up to engineering leadership to frame. I‚Äôve been able to get deploys down from once a month to every day.  As a director I armed my vp with the right data and he went to bat for us.\n\nI‚Äôve also been the only ‚Äúno‚Äù on promo panels in organizations that reward the engineers that ship the most tech debt. Like, no the engineer whose response to non-idempotent non-retryable network calls was to have the on call engineer add the latest broken object id to an array so it could have a special code path forever does not get to be a staff engineer.\n\nSometimes you gotta be the voice of reason.",
          "score": 4,
          "created_utc": "2026-02-03 01:35:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39mgo7",
              "author": "BoringTone2932",
              "text": "This reminds me of 2 examples. \n\nWe had a line recently that we found from several years ago that checked the machines IP, and followed a different code path. Linked to a years old JIRA for a single on Prem client. We are fully SaaS nowadays‚Ä¶‚Ä¶ \n\nAnother one, had a 2 hour meeting recently to talk about how programming ANYTHING to route to: https://localhost/ was a horrible idea. Spent most of the call explaining to people why nobody can get an official, third-party CA generated & trusted certificate for localhost‚Ä¶ \n\nGreat times.",
              "score": 2,
              "created_utc": "2026-02-03 01:49:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3adawz",
              "author": "raisputin",
              "text": "Just got done doing the troubleshooting for a ‚Äústaff‚Äù engineer because he couldn‚Äôt read simple logs‚Ä¶ü§£ü§£ü§£",
              "score": 1,
              "created_utc": "2026-02-03 04:30:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3bhj68",
                  "author": "RoboErectus",
                  "text": "I once solved a fight between two staff engineers by showing them fucking wireshark. \n\nThe thing that one converted to a go microservice service because he thought that was going to make things faster (yeah it was faster, but not in a way that improved kpi‚Äôs and not worth a whole fucking quarter) shouldn‚Äôt have been doubling up the content type header. Oops. \n\nBut the service it was calling only crashed because the other staff engineer rewrote it in fucking elixir for equally no reason.\n\n(Duplicate http headers should not cause a crash but the behavior is undefined, so technically a crash is still in spec I guess since the legal definition of undefined behavior is any result is valid.)\n\nBoth blamed the other for months. Neither did the most basic thing ever to figure out why the fuck it was crashing.\n\nIt‚Äôs very hard to describe why I‚Äôm good at my job but this example is one of my favorites. (And I wouldn‚Äôt have allowed a rewrite in the first place because rails was still fucking awesome at the time and definitely not the problem in the first place.)",
                  "score": 2,
                  "created_utc": "2026-02-03 10:19:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o39o4m9",
          "author": "cgill27",
          "text": "Businesses will usually always respond to matters of money, work with teams to come up with what downtime costs the company and present it.",
          "score": 2,
          "created_utc": "2026-02-03 01:58:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3h5kc7",
          "author": "ethereonx",
          "text": "Problem is middle management and non technical leadership. They over promise something even though they have no clue what it means and then they just want us engineers to do the magic. Of course we can do a script here and there and a quick and ugly proof of concept solutions. But no one seems to care that this is not sustainable and not maintainable.",
          "score": 1,
          "created_utc": "2026-02-04 04:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jvzzz",
          "author": "neuralspasticity",
          "text": "Your error budget tells you if you can release new feature and changes in general. \n\nIf your SLOs are being met any technical debt in your backlog suggests it‚Äôs not causing an impact which requires fixes, yet it can‚Äôt tell you if that‚Äôs efficient or cost effective.  \n\nAt some point you‚Äôll want to examine that backlog and how to improve efficiency and reduce costs by replacing underlying infrastructure in favor of ‚Äúlower costs‚Äù that maintain the same reliability or improve it. Theses costs aren‚Äôt just your AWS bill yet also include reductions in operations effort, reduction of toil, right-sized solutions, less complex technology, ‚Ä¶ \n\nYou‚Äôll make those changes to improve business and operations - not reliability goals which you‚Äôll maintain or improve \n\nThink about it as ‚Äúhow can I produce my widgets better‚Äù now that you‚Äôre making them. A startups goals were to prove widgets could be made and people wanted to buy them. Maturity developed and customers wanted the widgets reliably and that‚Äôs been delivered. Now how can the business do all this cheaper and easier and improve costs and be more profitable. That normally means examining how to make the widget cheaper. \n\nLead along these arguments.",
          "score": 1,
          "created_utc": "2026-02-04 16:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l0fr5",
          "author": "3sc2002",
          "text": "Put it in an \"I told you so\" file and use it later?\n\n</snark>\n\nBut in reality . . . you have to find a way to put it into \"their language\".  You have to do some work on \"the cost\" and SHOW them why it needs to be done this way.\n\nMost \"refactoring\" arguments are emotional / gut feeling . . . \"because we need to clean it up\", not \"We need to refactor this because 70% of our changes are within this monolith, and 50% of our outages are also due to this.  Each outage has an opportunity cost of X (cost of not doing what you are supposed to).  We expect the refactor to cost Y and will return Z number of hours to the team\"",
          "score": 1,
          "created_utc": "2026-02-04 19:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ab8p0",
          "author": "Specialist_Cow6468",
          "text": "Nothing is more get truly going to get better until it‚Äôs legal to play the most dangerous game with MBAs",
          "score": 0,
          "created_utc": "2026-02-03 04:16:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qra6x4",
      "title": "What are some useful things you can do with telemetry data outside of incident response?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qra6x4/what_are_some_useful_things_you_can_do_with/",
      "author": "Useful-Process9033",
      "created_utc": "2026-01-30 16:28:14",
      "score": 5,
      "num_comments": 20,
      "upvote_ratio": 0.73,
      "text": "In my previous role I pretty much only look at the logs/ metrics when I get paged. Or only during weekly reviews checking the dashboards and making sure all our services are in a good state. I suppose if you've got to a good state and incidents/ alerts are rare, when would you ever want to look at your logs/ metrics/ traces, and where else they'd be useful outside of incident response?",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qra6x4/what_are_some_useful_things_you_can_do_with/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2mo4as",
          "author": "AmazingHand9603",
          "text": "You can dive into telemetry to spot unexpected patterns or usage you didn‚Äôt anticipate. For example, maybe a particular endpoint is getting hammered every Monday morning and you‚Äôd never know unless you looked. Or maybe an old feature is still way more popular than you thought. This kind of info can help with planning, prioritizing, and making better product decisions. I‚Äôve found a few minor issues this way before they ever became incidents, just by poking around out of curiosity.",
          "score": 9,
          "created_utc": "2026-01-30 16:36:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mv83h",
              "author": "Useful-Process9033",
              "text": "do you set a dedicated time daily/ weekly to review telemetry this way? or you look when the team is dealing with planning etc.",
              "score": 2,
              "created_utc": "2026-01-30 17:08:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mq5n7",
          "author": "bittrance",
          "text": "As a platform engineer, I often look at service metrics to understand how services behave in production. Do we have noisy neighbor problems? Do teams understand how to scale their services? Do logs flow properly? I think of myself as a game keeper or gardener walking the grounds.",
          "score": 6,
          "created_utc": "2026-01-30 16:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mvrwh",
              "author": "Useful-Process9033",
              "text": "how often do you have to review these and how often do you find something worth acting on? I suppose if you discover a noisy neighbor problem/ scaling problem you'd need to file a ticket of some sort and ping the team.",
              "score": 1,
              "created_utc": "2026-01-30 17:10:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ueca7",
                  "author": "bittrance",
                  "text": "We have a lot of services and they are a little too different to meaningfully aggregate metrics across them, so I look almost daily at metrics of some sort, but not so often at any particular metric.",
                  "score": 2,
                  "created_utc": "2026-01-31 19:47:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mwrtl",
              "author": "abuhd",
              "text": "There's a sweet newish cisco tool for understanding services, with visuals! It's not free but I helped do some basic ux testing for it. It's called Cisco Thousandeyes.",
              "score": 0,
              "created_utc": "2026-01-30 17:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2u22pa",
                  "author": "redipin",
                  "text": "Haha, this is hilarious‚Ä¶ it‚Äôs only new with regards to being part of Cisco‚Äôs offerings. We had started using Thousandeyes over a decade ago in my org, though we dropped it before Cisco purchased it.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:48:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mo5h7",
          "author": "s5n_n5n",
          "text": "You can use telemetry for a lot of things:\n\n- optimization. So looking for things to improve¬†\n- prevent incidents, finding what might break next¬†\n- there are some auto scalers that work of tracing or metrics¬†\n- Any kind of business analytics of course.¬†",
          "score": 3,
          "created_utc": "2026-01-30 16:36:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mvgdj",
              "author": "Useful-Process9033",
              "text": "for optimization & preventing incidents, how do you go about it?",
              "score": 2,
              "created_utc": "2026-01-30 17:09:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mybtm",
                  "author": "s5n_n5n",
                  "text": "Optimization: tracing can be very helpful here since you can find the critical paths and what consumes the most time, so when you want to trim your overall response time, you can focus on the right things. Or, what you can do is track which queries are called how often in production with what duration and focus on those to make things better.¬†\n\nPrevent incidents is of course somehow harder and sits somewhere between optimization and incidence response. You need to find what is slowly moving towards a breaking point eg with anomaly detection¬†",
                  "score": 2,
                  "created_utc": "2026-01-30 17:22:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2nk5c9",
          "author": "robscomputer",
          "text": "Metrics in long term can show you how the service or component is handling target SLO's. It's a good point to show this during postmortems when someone says your service is not stable, the SLO metric will display the true story. It is tricky to get everyone to agree upon what the SLO should measure tho. :)",
          "score": 2,
          "created_utc": "2026-01-30 18:57:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mumt8",
          "author": "abuhd",
          "text": "I manually dig through logs all day. If I find something of interest, I simply add it to my log pipeline. I keep the data temporarily for 7 days to see how much storage gets used. Then I figure out if the cost of the data is worth it to increase it to say 30 days. Then every 2-3 months, I revisit my pipeline and remove what isn't relevant. It's a never ending cycle of reading latest news, then figuring out what's important to YOU.",
          "score": 1,
          "created_utc": "2026-01-30 17:05:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2muze6",
              "author": "Useful-Process9033",
              "text": "interesting. how do you know if the data is worth the cost of 30 days storage? how do you know its value?",
              "score": 2,
              "created_utc": "2026-01-30 17:07:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mxi5e",
                  "author": "abuhd",
                  "text": "The value is what's important to me at the time. I think what I was trying to say in a roundabout way, was that, this process you're looking to find doesn't exist because it evolves daily.",
                  "score": 1,
                  "created_utc": "2026-01-30 17:18:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ot5ud",
          "author": "fubo",
          "text": "Growth modeling and budgeting. Performance & efficiency. Looking for \"weird stuff\".\n\n\"Hey, why does this cache hit rate take a huge narrow dip at the beginning of every hour? Oh look, the cache entry expiration time is one hour, there's a single entry that accounts for 50% of all queries, and that particular entry takes 500msec to compute on a cache miss. So when that entry expires, 50% of all queries are cache misses for half a second. If we precompute that one entry and stuff it into the cache before it expires, we can get rid of that weird traffic spike on the backends. More generally, we should do that for the top N queries anyway; there's no point in ever having those be a cache miss.\"",
          "score": 1,
          "created_utc": "2026-01-30 22:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r2lir",
          "author": "c0Re69",
          "text": "Making the connection to customers in form of SLOs is the first important step, and the next one would be connecting that to business metrics/sales, which unlocks some nice leverage and brings you closer to the money.",
          "score": 1,
          "created_utc": "2026-01-31 07:03:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l1wgo",
          "author": "3sc2002",
          "text": "Correlate it with \"code changes\" . . . see if there are any \"hiccups\" based upon deployments.  For example: look for increasing response times when X is deployed (AKA, its getting longer, still w/in SLA/SLOs but trending that way).\n\nCorrelation != causation, but it does allow for a more proactive view.\n\nUnfortunately, i don't know of any tools out there that make it easy.  You could probably do it with an ELK stack tho . . . ",
          "score": 1,
          "created_utc": "2026-02-04 19:26:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvwbgf",
      "title": "At what point does reasonable assurance turn into busywork?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qvwbgf/at_what_point_does_reasonable_assurance_turn_into/",
      "author": "Salt_Slip_7732",
      "created_utc": "2026-02-04 18:20:11",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.71,
      "text": "We‚Äôre not trying to dodge audits but some requests feel more about formatting than risk.  \n Same control, same outcome just asked three different ways across customers and frameworks.\n\nWe keep answering honestly but the overhead keeps growing.\n\nHow do you decide when evidence is enough?",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qvwbgf/at_what_point_does_reasonable_assurance_turn_into/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o3kvyhc",
          "author": "FunnyAd6792",
          "text": "Agreed, we centralized the explanations alongside the evidence. Delve helped us capture context once so auditors didn‚Äôt need live walkthroughs every cycle.",
          "score": 6,
          "created_utc": "2026-02-04 18:59:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kobsw",
          "author": "Fantastic-Opening-57",
          "text": "This is a documentation problem not a control problem",
          "score": 5,
          "created_utc": "2026-02-04 18:25:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kxd1g",
          "author": "fubo",
          "text": "Who is being assured?",
          "score": 1,
          "created_utc": "2026-02-04 19:05:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l2fcw",
              "author": "Salt_Slip_7732",
              "text": "Mostly customers and their risk teams and auditors.",
              "score": 1,
              "created_utc": "2026-02-04 19:28:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3le99c",
          "author": "EventHorizon1997",
          "text": "If it‚Äôs a consistent problem with regular formats, then it‚Äôs time to automate the documentation. Audits are important, formatting is trivial as long as the information is in tact.\n\nHave you tracked the hours you and your team address spending on it? Find where in the price you can reduced the busy work and m make it an easy project win.",
          "score": 1,
          "created_utc": "2026-02-04 20:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oappe",
          "author": "DandyPandy",
          "text": "We just paid Vanta for this. They will tie evidence to controls for multiple frameworks. You pay per framework, but it does a good job of integrating with stuff to gather a fair amount of evidence, as well as having the option to handle customer questionnaires. Drata does the same, but I felt like Vanta had better tooling and docs. Drata was going to be slightly cheaper, but not enough to justify the extra headache I was running into with just setting up a demo.",
          "score": 1,
          "created_utc": "2026-02-05 06:27:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}