{
  "metadata": {
    "last_updated": "2026-02-04 17:09:57",
    "time_filter": "week",
    "subreddit": "sre",
    "total_items": 5,
    "total_comments": 23,
    "file_size_bytes": 30025
  },
  "items": [
    {
      "id": "1qprv5l",
      "title": "How do teams safely control log volume before ingestion (Loki / Promtail)?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qprv5l/how_do_teams_safely_control_log_volume_before/",
      "author": "TillStatus2753",
      "created_utc": "2026-01-28 23:34:29",
      "score": 8,
      "num_comments": 14,
      "upvote_ratio": 0.9,
      "text": "Looking for real-world experience from people running Loki / Promtail at scale.\n\nI‚Äôm experimenting with ingestion control (filtering, sampling, routing) -before-logs hit Loki to reduce noise and cost, but I‚Äôm trying to sanity-check whether this is actually a problem worth solving.\n\nFor those running Loki in production:\n\n\\- What % of your logs are DEBUG/INFO vs WARN/ERROR?\n\n\\- Do you actively drop or sample logs before ingestion?\n\n\\- Is this something you‚Äôre confident changing, or do people avoid touching it?\n\n\\- What‚Äôs been the biggest pain: cost, noise, fear of deleting data, or config complexity?\n\nNot selling anything ‚Äî genuinely trying to understand if this is a real problem or something most teams already handle fine.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qprv5l/how_do_teams_safely_control_log_volume_before/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2bel7s",
          "author": "cgill27",
          "text": "For logging to Loki I would recommend looking at [vector.dev](http://vector.dev), you can exclude things easily or even transform them, etc, all before shipping to Loki",
          "score": 7,
          "created_utc": "2026-01-28 23:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bz7m6",
              "author": "Freakin_A",
              "text": "Vector is great and incredibly efficient.  Highly recommend.\n\nCommercial options would be Cribl or Edge Delta.  Probably others.",
              "score": 1,
              "created_utc": "2026-01-29 01:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2h8g0m",
                  "author": "DramaticExcitement64",
                  "text": "The S3 sink is shit though.",
                  "score": 1,
                  "created_utc": "2026-01-29 20:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k2d2v",
              "author": "Broad_Technology_531",
              "text": "Interesting to hear that you are recommending vector instead of an OTEL Collector. Have you had a chance to work with the OTEL Collector for logs?",
              "score": 1,
              "created_utc": "2026-01-30 06:12:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2dy764",
              "author": "SnooWords9033",
              "text": "If you are going to use vector.dev instead of Promtail, then it is better to switch from Loki to VictoriaLogs. It is easier to configure and operate, it  supports high-cardinality log fields, which aren't supported properly by Loki, and it doesn't need object storage.",
              "score": -1,
              "created_utc": "2026-01-29 10:25:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d6r87",
          "author": "kusanagiblade331",
          "text": "Just to answer your question:\n\n1. DEBUG should not be in Loki or any type of log indexer.  \n2. Dropping yes - when there is an ingestion spike. Sampling - would be great if it can be easily done. But, session continuity is a real problem.  \n3. I think the key is to start small. Only apply filtering, sampling or routing to a few small apps. Then, slowly generalized. Teams can get upset when there is an incident and people cannot find logs.  \n4. Maintenance for ingestion control is a real fear. Cost is a big fear among management for log indexers like Splunk. The funny thing is - if everything is normal, teams generally have no fear of not getting some logs. Just make sure error and warning logs are there.\n\nSome interesting techniques to reduce log ingestion can also be found in this [article](https://starclustersolutions.com/blog/2026-01-how-to-reduce-splunk-cloud-cost/).",
          "score": 5,
          "created_utc": "2026-01-29 06:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bg8mw",
          "author": "Sea_Refrigerator5622",
          "text": "I know from troubleshooting there are a handful of logs we actually know show problems. We ingest those. The idea is to get GB down to MB. \n\nWe keep full kogs for a short time for deeper debugging and they‚Äôre a disaster. \n\nWe filter with Otel collector and in the tail sampling Loki (via Grafana cloud)",
          "score": 1,
          "created_utc": "2026-01-29 00:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k1qcu",
          "author": "Broad_Technology_531",
          "text": "Are you running OSS Loki or part of Grafana cloud?",
          "score": 1,
          "created_utc": "2026-01-30 06:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kx8ll",
              "author": "TillStatus2753",
              "text": "Both. I‚Äôve been experimenting with OSS Loki locally and also talking to teams on Grafana Cloud. The questions around safely reducing logs seem to show up in both.",
              "score": 1,
              "created_utc": "2026-01-30 10:42:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3go15c",
                  "author": "Broad_Technology_531",
                  "text": "Well in Grafana cloud you have adaptive telemetry which really solves the problem of high volume data. Take a look at it. Pretty cool\n\n[https://grafana.com/docs/grafana-cloud/adaptive-telemetry/](https://grafana.com/docs/grafana-cloud/adaptive-telemetry/)",
                  "score": 1,
                  "created_utc": "2026-02-04 02:52:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qra6x4",
      "title": "What are some useful things you can do with telemetry data outside of incident response?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qra6x4/what_are_some_useful_things_you_can_do_with/",
      "author": "Useful-Process9033",
      "created_utc": "2026-01-30 16:28:14",
      "score": 6,
      "num_comments": 19,
      "upvote_ratio": 0.8,
      "text": "In my previous role I pretty much only look at the logs/ metrics when I get paged. Or only during weekly reviews checking the dashboards and making sure all our services are in a good state. I suppose if you've got to a good state and incidents/ alerts are rare, when would you ever want to look at your logs/ metrics/ traces, and where else they'd be useful outside of incident response?",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qra6x4/what_are_some_useful_things_you_can_do_with/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2mo4as",
          "author": "AmazingHand9603",
          "text": "You can dive into telemetry to spot unexpected patterns or usage you didn‚Äôt anticipate. For example, maybe a particular endpoint is getting hammered every Monday morning and you‚Äôd never know unless you looked. Or maybe an old feature is still way more popular than you thought. This kind of info can help with planning, prioritizing, and making better product decisions. I‚Äôve found a few minor issues this way before they ever became incidents, just by poking around out of curiosity.",
          "score": 10,
          "created_utc": "2026-01-30 16:36:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mv83h",
              "author": "Useful-Process9033",
              "text": "do you set a dedicated time daily/ weekly to review telemetry this way? or you look when the team is dealing with planning etc.",
              "score": 2,
              "created_utc": "2026-01-30 17:08:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mq5n7",
          "author": "bittrance",
          "text": "As a platform engineer, I often look at service metrics to understand how services behave in production. Do we have noisy neighbor problems? Do teams understand how to scale their services? Do logs flow properly? I think of myself as a game keeper or gardener walking the grounds.",
          "score": 7,
          "created_utc": "2026-01-30 16:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mvrwh",
              "author": "Useful-Process9033",
              "text": "how often do you have to review these and how often do you find something worth acting on? I suppose if you discover a noisy neighbor problem/ scaling problem you'd need to file a ticket of some sort and ping the team.",
              "score": 1,
              "created_utc": "2026-01-30 17:10:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ueca7",
                  "author": "bittrance",
                  "text": "We have a lot of services and they are a little too different to meaningfully aggregate metrics across them, so I look almost daily at metrics of some sort, but not so often at any particular metric.",
                  "score": 2,
                  "created_utc": "2026-01-31 19:47:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mwrtl",
              "author": "abuhd",
              "text": "There's a sweet newish cisco tool for understanding services, with visuals! It's not free but I helped do some basic ux testing for it. It's called Cisco Thousandeyes.",
              "score": 0,
              "created_utc": "2026-01-30 17:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2u22pa",
                  "author": "redipin",
                  "text": "Haha, this is hilarious‚Ä¶ it‚Äôs only new with regards to being part of Cisco‚Äôs offerings. We had started using Thousandeyes over a decade ago in my org, though we dropped it before Cisco purchased it.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:48:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mo5h7",
          "author": "s5n_n5n",
          "text": "You can use telemetry for a lot of things:\n\n- optimization. So looking for things to improve¬†\n- prevent incidents, finding what might break next¬†\n- there are some auto scalers that work of tracing or metrics¬†\n- Any kind of business analytics of course.¬†",
          "score": 3,
          "created_utc": "2026-01-30 16:36:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mvgdj",
              "author": "Useful-Process9033",
              "text": "for optimization & preventing incidents, how do you go about it?",
              "score": 2,
              "created_utc": "2026-01-30 17:09:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mybtm",
                  "author": "s5n_n5n",
                  "text": "Optimization: tracing can be very helpful here since you can find the critical paths and what consumes the most time, so when you want to trim your overall response time, you can focus on the right things. Or, what you can do is track which queries are called how often in production with what duration and focus on those to make things better.¬†\n\nPrevent incidents is of course somehow harder and sits somewhere between optimization and incidence response. You need to find what is slowly moving towards a breaking point eg with anomaly detection¬†",
                  "score": 2,
                  "created_utc": "2026-01-30 17:22:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2nk5c9",
          "author": "robscomputer",
          "text": "Metrics in long term can show you how the service or component is handling target SLO's. It's a good point to show this during postmortems when someone says your service is not stable, the SLO metric will display the true story. It is tricky to get everyone to agree upon what the SLO should measure tho. :)",
          "score": 2,
          "created_utc": "2026-01-30 18:57:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mumt8",
          "author": "abuhd",
          "text": "I manually dig through logs all day. If I find something of interest, I simply add it to my log pipeline. I keep the data temporarily for 7 days to see how much storage gets used. Then I figure out if the cost of the data is worth it to increase it to say 30 days. Then every 2-3 months, I revisit my pipeline and remove what isn't relevant. It's a never ending cycle of reading latest news, then figuring out what's important to YOU.",
          "score": 1,
          "created_utc": "2026-01-30 17:05:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2muze6",
              "author": "Useful-Process9033",
              "text": "interesting. how do you know if the data is worth the cost of 30 days storage? how do you know its value?",
              "score": 2,
              "created_utc": "2026-01-30 17:07:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mxi5e",
                  "author": "abuhd",
                  "text": "The value is what's important to me at the time. I think what I was trying to say in a roundabout way, was that, this process you're looking to find doesn't exist because it evolves daily.",
                  "score": 1,
                  "created_utc": "2026-01-30 17:18:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ot5ud",
          "author": "fubo",
          "text": "Growth modeling and budgeting. Performance & efficiency. Looking for \"weird stuff\".\n\n\"Hey, why does this cache hit rate take a huge narrow dip at the beginning of every hour? Oh look, the cache entry expiration time is one hour, there's a single entry that accounts for 50% of all queries, and that particular entry takes 500msec to compute on a cache miss. So when that entry expires, 50% of all queries are cache misses for half a second. If we precompute that one entry and stuff it into the cache before it expires, we can get rid of that weird traffic spike on the backends. More generally, we should do that for the top N queries anyway; there's no point in ever having those be a cache miss.\"",
          "score": 1,
          "created_utc": "2026-01-30 22:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r2lir",
          "author": "c0Re69",
          "text": "Making the connection to customers in form of SLOs is the first important step, and the next one would be connecting that to business metrics/sales, which unlocks some nice leverage and brings you closer to the money.",
          "score": 1,
          "created_utc": "2026-01-31 07:03:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqah7q",
      "title": "Any good tools for Kubernetes access control?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qqah7q/any_good_tools_for_kubernetes_access_control/",
      "author": "SidLais351",
      "created_utc": "2026-01-29 14:46:16",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 0.87,
      "text": "managing access to multiple clusters with different environments and teams. We want tighter control over kubectl access, auditability, and clean offboarding. Looking for tools or patterns that have worked well in real setups. \n\ncommunity input would really helpful",
      "is_original_content": false,
      "link_flair_text": "HELP",
      "permalink": "https://reddit.com/r/sre/comments/1qqah7q/any_good_tools_for_kubernetes_access_control/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o2f3s7c",
          "author": "JoshSmeda",
          "text": "If you have money, Teleport PAM.",
          "score": 3,
          "created_utc": "2026-01-29 14:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fbbmc",
          "author": "granviaje",
          "text": "Tailscale and Tailscale ACL are quite sweet¬†",
          "score": 2,
          "created_utc": "2026-01-29 15:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f4iyy",
          "author": "walt_dinio",
          "text": "following because we're also running into this problem as we continue to increase our k8s adoption. only where it makes sense. ",
          "score": 1,
          "created_utc": "2026-01-29 14:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fhplw",
          "author": "kckrish98",
          "text": "we looked at this problem from the perspective of making Kubernetes access consistent with other infrastructure access. we use Teleport to manage RBAC for SSH and Kubernetes through our identity provider, so user access is tied to roles instead of static kubeconfigs or separate credential stores. It gave us a more consistent permission model across clusters and host access",
          "score": 1,
          "created_utc": "2026-01-29 15:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mlcp9",
          "author": "SadFaceSmith",
          "text": "Been loving the Tailscale K8s Operator",
          "score": 1,
          "created_utc": "2026-01-30 16:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36wanb",
          "author": "RunHoliday9384",
          "text": "I think Portainer is worth a look",
          "score": 1,
          "created_utc": "2026-02-02 17:41:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qudu6h",
      "title": "The requirement to deliver above all else",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qudu6h/the_requirement_to_deliver_above_all_else/",
      "author": "BoringTone2932",
      "created_utc": "2026-02-03 01:08:38",
      "score": 5,
      "num_comments": 10,
      "upvote_ratio": 0.73,
      "text": "How do you deal with the corporate nature of the push to deliver above all else? \n\nSure, XYZ can be scripted, but the situation that caused XYZ shouldn‚Äôt exist in the first place. \n\nSure, we can move to Aurora, but we are just carrying our problems with us. \n\nRepeatedly, corporate nature drives increases to the top line, decreases to the bottom line and progress above all else. We should fix this becomes we should deprecate this in favor of that. Change creates appearance of improvement when in reality, the new servers have host files with a laundry list of hostnames because internal DNS team didn‚Äôt move fast enough, or the build pipeline has manual post-steps because we manually made changes across the environment and fixing the build pipeline isn‚Äôt prioritized. \n\nHow do you convince leadership that the small technical intricacies matter? That the small technical intricacies create long term barriers to reliability? That the steps we work around now will come back to bite us, even if they (or I) are not around anymore for it. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qudu6h/the_requirement_to_deliver_above_all_else/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o39gu61",
          "author": "calibrono",
          "text": "I haven't seen solutions other than having leadership be actually ex-engineers themselves.",
          "score": 16,
          "created_utc": "2026-02-03 01:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39qlyb",
              "author": "namenotpicked",
              "text": "It's this or non-technical leadership actually listening to the engineers and agreeing to build in tech debt work as part of regular work. Not being deprioritized or cut back. Of course I've never seen this happen but one can dream.",
              "score": 5,
              "created_utc": "2026-02-03 02:12:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o39k2ds",
          "author": "RoboErectus",
          "text": "This is easy to do when you actually log the effort going into things you only do because of tech debt. \n\nWhat‚Äôs your mttd and mttr? How long are your runbooks? How many engineers do you have on support? How fast can an engineer get a meaningful change into production, and how often does that cause a breaking side effect?\n\nThis is up to engineering leadership to frame. I‚Äôve been able to get deploys down from once a month to every day.  As a director I armed my vp with the right data and he went to bat for us.\n\nI‚Äôve also been the only ‚Äúno‚Äù on promo panels in organizations that reward the engineers that ship the most tech debt. Like, no the engineer whose response to non-idempotent non-retryable network calls was to have the on call engineer add the latest broken object id to an array so it could have a special code path forever does not get to be a staff engineer.\n\nSometimes you gotta be the voice of reason.",
          "score": 4,
          "created_utc": "2026-02-03 01:35:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39mgo7",
              "author": "BoringTone2932",
              "text": "This reminds me of 2 examples. \n\nWe had a line recently that we found from several years ago that checked the machines IP, and followed a different code path. Linked to a years old JIRA for a single on Prem client. We are fully SaaS nowadays‚Ä¶‚Ä¶ \n\nAnother one, had a 2 hour meeting recently to talk about how programming ANYTHING to route to: https://localhost/ was a horrible idea. Spent most of the call explaining to people why nobody can get an official, third-party CA generated & trusted certificate for localhost‚Ä¶ \n\nGreat times.",
              "score": 2,
              "created_utc": "2026-02-03 01:49:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3adawz",
              "author": "raisputin",
              "text": "Just got done doing the troubleshooting for a ‚Äústaff‚Äù engineer because he couldn‚Äôt read simple logs‚Ä¶ü§£ü§£ü§£",
              "score": 1,
              "created_utc": "2026-02-03 04:30:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3bhj68",
                  "author": "RoboErectus",
                  "text": "I once solved a fight between two staff engineers by showing them fucking wireshark. \n\nThe thing that one converted to a go microservice service because he thought that was going to make things faster (yeah it was faster, but not in a way that improved kpi‚Äôs and not worth a whole fucking quarter) shouldn‚Äôt have been doubling up the content type header. Oops. \n\nBut the service it was calling only crashed because the other staff engineer rewrote it in fucking elixir for equally no reason.\n\n(Duplicate http headers should not cause a crash but the behavior is undefined, so technically a crash is still in spec I guess since the legal definition of undefined behavior is any result is valid.)\n\nBoth blamed the other for months. Neither did the most basic thing ever to figure out why the fuck it was crashing.\n\nIt‚Äôs very hard to describe why I‚Äôm good at my job but this example is one of my favorites. (And I wouldn‚Äôt have allowed a rewrite in the first place because rails was still fucking awesome at the time and definitely not the problem in the first place.)",
                  "score": 2,
                  "created_utc": "2026-02-03 10:19:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o39o4m9",
          "author": "cgill27",
          "text": "Businesses will usually always respond to matters of money, work with teams to come up with what downtime costs the company and present it.",
          "score": 2,
          "created_utc": "2026-02-03 01:58:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3h5kc7",
          "author": "ethereonx",
          "text": "Problem is middle management and non technical leadership. They over promise something even though they have no clue what it means and then they just want us engineers to do the magic. Of course we can do a script here and there and a quick and ugly proof of concept solutions. But no one seems to care that this is not sustainable and not maintainable.",
          "score": 1,
          "created_utc": "2026-02-04 04:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jvzzz",
          "author": "neuralspasticity",
          "text": "Your error budget tells you if you can release new feature and changes in general. \n\nIf your SLOs are being met any technical debt in your backlog suggests it‚Äôs not causing an impact which requires fixes, yet it can‚Äôt tell you if that‚Äôs efficient or cost effective.  \n\nAt some point you‚Äôll want to examine that backlog and how to improve efficiency and reduce costs by replacing underlying infrastructure in favor of ‚Äúlower costs‚Äù that maintain the same reliability or improve it. Theses costs aren‚Äôt just your AWS bill yet also include reductions in operations effort, reduction of toil, right-sized solutions, less complex technology, ‚Ä¶ \n\nYou‚Äôll make those changes to improve business and operations - not reliability goals which you‚Äôll maintain or improve \n\nThink about it as ‚Äúhow can I produce my widgets better‚Äù now that you‚Äôre making them. A startups goals were to prove widgets could be made and people wanted to buy them. Maturity developed and customers wanted the widgets reliably and that‚Äôs been delivered. Now how can the business do all this cheaper and easier and improve costs and be more profitable. That normally means examining how to make the widget cheaper. \n\nLead along these arguments.",
          "score": 1,
          "created_utc": "2026-02-04 16:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ab8p0",
          "author": "Specialist_Cow6468",
          "text": "Nothing is more get truly going to get better until it‚Äôs legal to play the most dangerous game with MBAs",
          "score": 0,
          "created_utc": "2026-02-03 04:16:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr7nbo",
      "title": "Looking for a whitepaper/journeydoc for SRE transition",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qr7nbo/looking_for_a_whitepaperjourneydoc_for_sre/",
      "author": "hiveminer",
      "created_utc": "2026-01-30 14:55:33",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.78,
      "text": "So guys, in 2017, Juniper released a very nicely prepared 16 page document on the transition/journey to [NRE](https://events19.linuxfoundation.org/wp-content/uploads/2017/12/NRE-and-DevNetOps-Overview-ONS-Sept-2018.pdf)(Network Reliability Engineering).  I think it is well written.  Now, the question is, has a document like that been written for sysops?  SRE?  If now, those boasting the title of SENIOR SRE.. should consider it.  In fact, I think there are a number of parallels within that document which would apply to SRE.  We are staring at the dawn of IT second brain/digital sidekick.  That can also be incorporated, if not now, maybe for a possible version 2.",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qr7nbo/looking_for_a_whitepaperjourneydoc_for_sre/",
      "domain": "self.sre",
      "is_self": true,
      "comments": []
    }
  ]
}