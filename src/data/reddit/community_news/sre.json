{
  "metadata": {
    "last_updated": "2026-01-21 17:20:00",
    "time_filter": "week",
    "subreddit": "sre",
    "total_items": 7,
    "total_comments": 49,
    "file_size_bytes": 61194
  },
  "items": [
    {
      "id": "1qfhnox",
      "title": "Datadog pricing aside, how good is it during real incidents",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfhnox/datadog_pricing_aside_how_good_is_it_during_real/",
      "author": "HotelBrilliant2508",
      "created_utc": "2026-01-17 16:29:29",
      "score": 76,
      "num_comments": 25,
      "upvote_ratio": 0.96,
      "text": " Considering Datadog setting aside the pricing debate for a second - how does it actually perform when things are on fire?\n\nIs the correlation between metrics and traces actually useful?\n\nWant to hear from people who've used it during actual incidents. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfhnox/datadog_pricing_aside_how_good_is_it_during_real/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o055utj",
          "author": "rolexboxers",
          "text": "The real question is does your team actually know how to use it under pressure? That matters way more than which specific platform you're on.",
          "score": 32,
          "created_utc": "2026-01-17 17:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bcpvm",
          "author": "AccountEngineer",
          "text": "Used a few different ones at different companies. They all have strengths and weaknesses. None of them are going to magically solve incidents for you if that's what you're asking.\n\nWhat specific use case are you trying to optimize for?",
          "score": 29,
          "created_utc": "2026-01-18 16:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04v7h2",
          "author": "Mmmm618",
          "text": "Yeah, it holds up. The correlation stuff isn't bullshit - you can actually track an issue from alert to root cause without wanting to throw your laptop.",
          "score": 17,
          "created_utc": "2026-01-17 17:07:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06miro",
              "author": "Mangudai_11",
              "text": "And what about predicting events? Before they happen and before I throw away my laptop xD",
              "score": 1,
              "created_utc": "2026-01-17 22:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o07or8f",
          "author": "Grandpabart",
          "text": "If incident response is your actual priority, Datadog will tell you something is broken, but you're going to need an action layer (e.g. an IDP like Port) to make sure the right owners of the problem are on it and everyone knows their role/what to do/how.",
          "score": 14,
          "created_utc": "2026-01-18 01:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04sxzs",
          "author": "engineered_academic",
          "text": "Datadog is probably the best tool but like any tool you have to configure it properly and be familiar with it so that you can wield it during incidents. Doing chaos engineering experiments to test your runbooks and exercise those incident muscles is important. It definitely isn't fire and forget, you need to have a proper strategy and vision around logs and traces as well as controlling and justifying costs.",
          "score": 52,
          "created_utc": "2026-01-17 16:57:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o054l4w",
              "author": "hijinks",
              "text": "i 2nd this.. i run a consulting company that specializes in o11y and helping people with spend.\n\nIf DD was half the price then most would be using it. In all my time I've never heard a company say we hate DD the service. It's always we hate DD pricing.",
              "score": 21,
              "created_utc": "2026-01-17 17:51:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08ko8z",
                  "author": "tcpWalker",
                  "text": "I feel like I have heard about companies being talked into it by management firms and then having the agents cause incidents. But I am not sure why and was not running those incidents. I suppose like with any new major service, do your PoCs and manage your risk as you gain familiarity with the project.  \n  \n(And the ordinary tradeoffs about building in-house vs outsourcing to a vendor apply)",
                  "score": 1,
                  "created_utc": "2026-01-18 04:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04zeep",
              "author": "stikko",
              "text": "100% this. Don’t expect to be able to spew a bunch of garbage in and get magic incident resolutions back out.  Strategy from SRE and discipline from the app teams are key.",
              "score": 4,
              "created_utc": "2026-01-17 17:27:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o050u6x",
              "author": "Fattswindstorm",
              "text": "Yes it’s a great tool. Just be mindful when you are about to start a trial, they are gonna be up your ass to get agents installed so be prepared.  Have the proper doors open, and a strategy to get the agents installed.  \n\n- Have a basic tagging scheme ready.  Like define your services, applications, environments, etc. \n- Know what logs you want ingested, and determine which ones are important and which ones are noise.  You’ll want to filter out the noise fore ingesting the logs. \n\nWe enabled DataDog the same time we were doing a lift an shift into Aws and both landed on my lap.  It was very chaotic and hard trying to figure all this out during the trial period. So if you have the room, give an engineer or two the room to set this up with some sort of framework ready for guiding the implementation",
              "score": 2,
              "created_utc": "2026-01-17 17:34:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06vqeu",
              "author": "Hebrewhammer8d8",
              "text": "It is fire and forget if the lead engineer decides to leave and take another job with little documentation, and management expects that we can do what lead engineer did.",
              "score": 1,
              "created_utc": "2026-01-17 23:01:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o059lgn",
          "author": "jimmyjohns69420xl",
          "text": "at the user layer it’s fantastic. great UI and tools.\n\nat the admin layer its ok, but you’ll spend more time than you expect pulling your hair out over undocumented agent configs and such. you’ll also likely end up doing annoying cost control exercises.\n\nI think the best argument against DD today is that with LLMs, the UI layer is becoming less important, and DD’s pricing model disincentivizes sending complete and high-cardinality data, which is what LLMs need to behave well. it’s still early on that front but something to consider.",
          "score": 6,
          "created_utc": "2026-01-17 18:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04nihz",
          "author": "whitechapel8733",
          "text": "The best IMO.",
          "score": 10,
          "created_utc": "2026-01-17 16:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04u6rl",
          "author": "theubster",
          "text": "Its better than most at anomaly detection. But, I don't trust any software enough to have thay kind of thing be the core of our detection or root cause discovery. \n\nHonestly, its dashboards are what sets it apart. The widget options & display are second to none. Plus, the sheer number of integrations, allowing it to be the one main hub of information about what's happening in our platform. \n\nUltimately, incident recovery comes down to platform knowledge and timely detection. Both of which can work with any tool. Why I like datadog for incidents is that their incident tooling make data capture easy, and gives fantastic visibility into incident trends over time.",
          "score": 1,
          "created_utc": "2026-01-17 17:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0581xs",
          "author": "IS300FANATIC",
          "text": "In my experience- Datadog has been an amazing observability tool. \n\nThe event correlation was a big deal and difference maker when reviewing their stack against others like new relic, app lnsights, signalfx and even dynatrace.  Dynatrace was actually a pretty close second imo but where they lost me is they failed to remove unused or empty data selectors from selection scope. For example - if im an Azure shop - i dont want to have to scroll past empty non-used AWS metrics and features just to find data I am ingesting and needing to filter down to. (Atleast 3ish years ago it was this way, not sure about now) + a few other small things. \nAnyway - back to the question: that quick data isolation moment in time saves time when needing to jump in quick if you didnt do the firedrill work to make those correlations or build the visibility before hand\n\nOne other piece that im quite impressed with Datadog tooling before and during an issue is how light their continuous profiler is.  Super cheap on resources and just seems to work in most cases - making it more comfortable to turn it on, let it ride in a close lower and curve some of that production inefficiency or spotting edge case bugs in production based on conditions that may not have been possible to reproduce in a lower (vendors didnt offer sandbox or test envs so production is your testing bed for feature with live traffic, not just mock data)\n\nI'm not a super huge fan of their RUM though and their syntetics could be a little more flexible. - that product suite just feels a bit clunky imo.\n\nLastly - npm has been a life saver in a k8s environment- it builds huge context into what's talking to what and can filter down pretty nicely to isolate traffic patterns so let's say \"podx is in imagepullbackoff\" - why? Well typically first thing you think is it not being able to pull its image from whatever container registry. Well I've actually caught an issue in scenarios like this where the pod was trying to hit the ACR at its public endpoint and not its private connection endpoint configure - forcing us to open up to public with restrictions to satisfy the incident, open ticket with msft and better understand the conditions which turned out to be msft side issue. - without npm in our pocket, looking at DNS queries and outbound connection attempts would have been a lot tough/slower.\n\nOthers may disagree with a few points here but they have all helped myself and my teams in their journey for reliability and they seem to win the race mostly every time. \nThey are pricey a f though nonetheless lol",
          "score": 2,
          "created_utc": "2026-01-17 18:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05ffbk",
          "author": "Ecestu",
          "text": "We've found that simpler is usually better during incidents. Too many features and fancy visualizations just add confusion when you're stressed and need to act fast.",
          "score": 1,
          "created_utc": "2026-01-17 18:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08o8y5",
          "author": "inferix",
          "text": "Im mainly into Dynatrace and wondered how it compares to Datadog?",
          "score": 1,
          "created_utc": "2026-01-18 05:00:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bdvf0",
          "author": "Relative_Taro_1384",
          "text": "Been on Datadog for like 2 years. It's fine. Does what it needs to do most of the time. The APM stuff is useful if your services are instrumented well. Ours aren't in some places so YMMV.",
          "score": 1,
          "created_utc": "2026-01-18 16:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dd4fy",
          "author": "AxiomOfLife",
          "text": "I prefer grafana but you gotta fiddle with it more, datadog seems more user friendly",
          "score": 1,
          "created_utc": "2026-01-18 22:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0f1p8n",
          "author": "safetytrick",
          "text": "I've used Datadog at two different places and I've never loved it. I prefer Prometheus and Grafana.\n\nFor logs Splunk is amazing but pricey. Graylog is okay. Elk looks nice but I don't have deep experience with it.",
          "score": 1,
          "created_utc": "2026-01-19 03:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g97hn",
          "author": "samsuthar",
          "text": "100%, datadog is the best tool out there to get metrics and trace correlations during incidents. But I would suggest trying 2 to 3 tools and see which works best, especially if you check whether they give predictions before an incident happens based on predictive analysis.",
          "score": 1,
          "created_utc": "2026-01-19 09:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06r2wg",
          "author": "badaccount99",
          "text": "We just going to ignore their sales team?  Their product might be the best in the world, and I've got a huge budget I'm spending with New Relic right now, but their sales guys.... sheesh.  No thanks.\n\nNew Relic is pretty decent though.\n\nEdit:  Downvoted maybe from their sales team.  They might have the greatest product of all time, but I had them calling my work number every other day, then they started calling my personal cell.  Seriously, I've got like $200k to spend on monitoring, but you blow up my personal phone you're never getting our business, and I'm talking crap about you to my parent company too which is worth way more than I can spend.",
          "score": 0,
          "created_utc": "2026-01-17 22:38:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05gy4r",
          "author": "NeuralNexus",
          "text": "Datadog is a great tool (the best on the market in many areas, competitive and best in ecosystem in others) but it's not really worth the price at scale. \n\nYou should really consider open telemetry and what features you need and how that pipeline can look. You can get a lot of serviceability and observability with open source tooling now. You do not need every single bell and whistle. \n\nDatadog is a great 'quick fix' kind of solution that works pretty well with the defaults. But I'd really recommend you think about otel and how you can build the kind of observation stack your app needs for incident management.",
          "score": 1,
          "created_utc": "2026-01-17 18:48:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0523vy",
          "author": "danukefl2",
          "text": "Being a place with mostly vendor provided software, it's not great on that side but works fine for metrics and logs. If you have OTEL and the visibility it does work much better. The trick is not throwing too much crap into it so it can actually work, plus that can help keep prices down too.",
          "score": 1,
          "created_utc": "2026-01-17 17:40:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi2ian",
      "title": "SRE: Past, Present, and Future - what changed and where is it going?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qi2ian/sre_past_present_and_future_what_changed_and/",
      "author": "Pippa_the_second",
      "created_utc": "2026-01-20 14:27:29",
      "score": 35,
      "num_comments": 24,
      "upvote_ratio": 0.86,
      "text": "In the 2010s, SRE was a hot field. Companies wanted SREs and many were even willing to pay a premium, relative to their SWE counterparts. Which made sense considering the on-call and after hours work.\n\nIt stopped being a hot field after a few years. I cannot pinpoint an actual event to cause this, but with the rise of AWS and Kubernetes, my sense is that SRE was not as critical as before.\n\nThe overall brand also faced dilution. To some, SRE was a SWE who could not code. This was reflected in hiring. In one FAANG, I remember there was a brouhaha when a SRE recruiter asked his SWE counterparts to send him candidates who performed strongly but did not pass the coding bar. The SREs were livid. I hope I am not doxxing myself now.\n\nAs we come to the recent few years, there was a trend towards Platform Engineers. To me, they were SREs at the core. Now that trend feels like it is disappearing. I see fewer discussions about Platform Engineers AND SREs.\n\nAs I look to the future, I sense that SRE has been stripped out of so many core functions that it has lost its meaning. SRE means so little that other vendors now sell AI SRE and companies are willing to try it out. You do not hear about companies selling AI SWE even though Claude can write code.\n\nWhat do you think the future holds for SRE?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qi2ian/sre_past_present_and_future_what_changed_and/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o0omqgs",
          "author": "Electronic-Work-311",
          "text": "I’ve been working as an SRE for the past 10 years, and I’ve come to realize that the title itself can be quite ambiguous, often varying significantly from one company to another. Some organizations even brand traditional Application or Production Support roles as “SRE” to make the position sound more appealing. \nIn my case, I primarily work with AWS, Terraform, architecture design, GitOps, Kubernetes, and some Python - building and operating infrastructure platforms. I developed internal workflows integrating Amazon Bedrock (AI) with the monitoring alerts. This has made me reflect on how I should actually define my role: Platform Engineer, DevOps Engineer, SRE, or Cloud Engineer.\nUltimately, though, I’ve learned that titles matter far less than the underlying skill set. Across companies, there’s a common core of skills that companies look for, and that’s what truly makes the difference. Again SWE are not SREs and vice versa, so the expectations shouldn't be same at all.",
          "score": 12,
          "created_utc": "2026-01-20 15:48:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oydag",
              "author": "hawtdawtz",
              "text": "While I agree this is the case for some companies, once you hit “big-tech” almost every SRE is a previous SWE or has substantial SWE skill sets. I feel like what you describe is a bit more platformy and devops like, which most bigger companies will have dedicated teams for.\n\nSincerely,\n\nA guy who’s done both and seen lots of flavors of SRE",
              "score": 5,
              "created_utc": "2026-01-20 16:41:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tci6v",
              "author": "Useful-Process9033",
              "text": "I feel like the tooling layer has evolved but the underlying core skill sets hold true.\n\nSounds like you built an ai sre for your team internally, curious how that played out.\n\nBtw how did you find aws bedrock? I was at aws reinvent last year, went to a iHeartMedia talk, they also used it to develop their internal ai sre.",
              "score": 1,
              "created_utc": "2026-01-21 06:58:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0od8rt",
          "author": "Remote_Succotash",
          "text": "Out of curiosity, who sells \"AI SRE\"?",
          "score": 8,
          "created_utc": "2026-01-20 15:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ogtvk",
              "author": "Useful-Process9033",
              "text": "There’s plenty discussion on them in this sub and there’s like 100s of companies building them.\n\nThey more or less do the same thing, connecting to your logs, metrics, runbooks etc and tells you root cause when alerts fire/ draft postmortems, etc.",
              "score": 6,
              "created_utc": "2026-01-20 15:19:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0oyc77",
                  "author": "ambitiousGuru",
                  "text": "These are cool and all but if you don’t know what it is saying(assuming that it is correct) do you expect a SWE to be able to solve it? Especially if it is infra related. \n\nAI is good at gathering data points for you like these SRE agents but that’s one part of the job. \n\nIMO the SWE are going to hurt more because a business major can now come do their job for them. SWE at the core was architecting code and being in the weeds of the code. If now all you’re doing is prompting a LLM “create a function that does X”, how hard is the job really? Then it’s made a huge PR that a human cannot mentally take on so you need an agent to PR review. Seriously what is your job lol?\n\nUnless something that I haven’t come across yet that can view the full context(doesn’t exceed the context window) of the environment and you give it write access to do it then we will be the last to go. Even then do you trust a indeterministic solution to replace a human? Time will tell",
                  "score": 1,
                  "created_utc": "2026-01-20 16:41:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0p74sw",
              "author": "blitzkrieg4",
              "text": "Incident.io",
              "score": 4,
              "created_utc": "2026-01-20 17:22:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0pl5cs",
              "author": "emery-glottis",
              "text": "[Resolve.ai](http://Resolve.ai), [Rootly.com](http://Rootly.com), DataDog Bit AI, [incident.io](http://incident.io), [Traversal.com](http://Traversal.com), [Cleric.ai](http://Cleric.ai), [Neubird.ai](http://Neubird.ai) there's got to be 30 or more advertising AI SRE at this point. Also, [https://www.reddit.com/r/devops/comments/1m4egqq/a\\_growing\\_wave\\_of\\_ai\\_sre\\_tools\\_are\\_they/](https://www.reddit.com/r/devops/comments/1m4egqq/a_growing_wave_of_ai_sre_tools_are_they/) come up but even that list isn't complete at the time and has grown since. Resolve is dedicated and quite hype at the moment, incident has shown capability live on stage, i've spoken 3 different people who use Rootly say good things. I would advise, like any other tool, you find a baseline test and compare the top choices. If you're a DataDog shop that's prob an easy test for you.",
              "score": 2,
              "created_utc": "2026-01-20 18:26:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0puun7",
                  "author": "Disastrous-Glass-916",
                  "text": "And [Anyshift.io](http://Anyshift.io) :)",
                  "score": 1,
                  "created_utc": "2026-01-20 19:09:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tbz1l",
                  "author": "Useful-Process9033",
                  "text": "And incidentfox! https://github.com/incidentfox/incidentfox/",
                  "score": 1,
                  "created_utc": "2026-01-21 06:53:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0omf6z",
              "author": "neeltom92",
              "text": "built an opensource one here : [https://github.com/neeltom92/sre-copilot](https://github.com/neeltom92/sre-copilot)  \n  \ninspired by : [https://clickhouse.com/blog/llm-observability-challenge](https://clickhouse.com/blog/llm-observability-challenge)",
              "score": 1,
              "created_utc": "2026-01-20 15:46:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0rj21w",
              "author": "NefariousnessOk5165",
              "text": "Ai Sre agents only helps you remove L1 , L2 layer … Sre in itself is quite huge !",
              "score": 1,
              "created_utc": "2026-01-21 00:02:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0st4a4",
              "author": "Ok_Housing7981",
              "text": "Ever heard of the company called “Harness”\n, they are taking a lot of funding and their entire goal is to replace Testing and devops/sre roles with AI",
              "score": 1,
              "created_utc": "2026-01-21 04:30:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ofy24",
              "author": "Pippa_the_second",
              "text": "You can find plenty through a google search. This question will attract sales reps like flies to honey.",
              "score": -1,
              "created_utc": "2026-01-20 15:15:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0owu21",
          "author": "red_flock",
          "text": "Anybody can code, but not everyone can stare at a million line code base and solve a performance issue at 3am. Is that SRE? I dont know, but that's what I think an ideal \"real\" SRE ought to be. But then again, is this a senior dev or an SRE?\n\nMeanwhile, there are SREs who code, but they dont touch the business code. Their coding is restricted to their tooling or Infrastructure. Is this SRE? Or Platform Engineers?\n\nThen there is people like me. I can write a little python, read a little bit of other people's code, but I mostly dont have to code, because there is always enough things on fire for me to put out. Most of you will say this is not SRE work, but even though I cannot check in a bug fix at 3am, I most certainly can get stuff back to a working state by hook or crook. If I am not an SRE, what am I?",
          "score": 8,
          "created_utc": "2026-01-20 16:34:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ohnru",
          "author": "Useful-Process9033",
          "text": "Honestly I think the established engineers working in a company for a long time will never get replaced. I worked at a consumer big tech company and we had a team of people who’ve worked at the company for 10+ years that’s knows the infra inside and out. The tribal knowledge they have is crazy and all serious incidents/ outages get routed to them.",
          "score": 4,
          "created_utc": "2026-01-20 15:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0q7ck2",
              "author": "interrupt_hdlr",
              "text": "exactly. people saying AI will replace SREs completely in X years have no idea about what running a complex environment is like. \n\nAI can do a lot of things but it currently doesn't have all the information/signals to decide and if it did, 1M context windows would not even begin to scratch the surface. You need a dozen subagents , all sorts of MCPs and integrations... and you will still miss a lot of necessary context.",
              "score": 1,
              "created_utc": "2026-01-20 20:07:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ta3is",
                  "author": "Useful-Process9033",
                  "text": "Haha I’m taking a stab at this with the dozen sub agents + tons of MCP approach you mentioned.\n\nI made another post in this sub about the open source ai sre agent that I’m building. Didn’t seem to get much attention yet.\n\nBut yea I think one of the hardest problem to solve is you need to have all these integrations since all the context is spread everywhere across different tools & every environment is different. I’m just sucking it up and building all these integrations one by one for now.\n\nThe context window problem is still very much true. I think they can be cracked with smart enough engineering.\n\nThough at the end of the day I’d think of AI as copilot and can’t be trusted to do write actions without human approval, since you still need human accountability in incident management.",
                  "score": 1,
                  "created_utc": "2026-01-21 06:37:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0qf6v1",
          "author": "HiddenWithChrist",
          "text": "When some companies say SRE, they really mean Unicorn. At other Orgs, people are given the SRE title for their role, but in reality only do one particular thing (e.g. managing infra) and rely more heavily on Senior Engineering roles for more complex issues. At my workplace I'm whatever the Org needs me to be and wear the hats of SWE - identifying and troubleshooting bugs, Network Engineer- setting up VLANs, routing, Internal/External DNS, etc., DevOps- ci/cd pipeline creation/troubleshooting, K8s deployments, Terraform, Ansible, Argo, etc., Cloud Engineer, Platform Engineer- infrastructure deployment management and observability, Systems Performance, InfoSec, the list goes on and on. It's been great for me, since I enjoy challenging tasks and learning. SRE has kind of always been a vague title, IMO- same as \"DevOps\".",
          "score": 4,
          "created_utc": "2026-01-20 20:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0p1hoq",
          "author": "one-alexander",
          "text": "I like your discussion, but there are of course AI SWE in the market.",
          "score": 0,
          "created_utc": "2026-01-20 16:56:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0std7d",
              "author": "Ok_Housing7981",
              "text": "Tell me one company , which has its business only on AI SWE , there is no company like that\n\nBut as far as testing and sre/ devops is concerned , search about “Harness” , you will get to know",
              "score": 1,
              "created_utc": "2026-01-21 04:31:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qicfdh",
      "title": "RCA: Why our H100 training cluster ran at 35% efficiency (and why \"Multi-AZ\" was the root cause)",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qicfdh/rca_why_our_h100_training_cluster_ran_at_35/",
      "author": "NTCTech",
      "created_utc": "2026-01-20 20:25:43",
      "score": 25,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Hey everyone,\n\nI wanted to share a painful lesson we learned recently while architecting a distributed training environment for a client. I figure some of you might be dealing with similar \"AI infrastructure\" requests landing on your ops boards.\n\nThe Incident: We finally secured a reservation for a cluster of H100s after a massive wait. The Ops team (us) did what we always do for critical web apps: we spread the compute across three Availability Zones (AZs) for maximum redundancy.\n\nThe Failure Mode: Training efficiency tanked. We were seeing massive idle times on the GPUs. After digging through the logs and network telemetry, we realized we were treating AI training like a stateless microservice. It’s not.\n\nIt turns out that in distributed training (using NCCL collectives), the cluster is only as fast as the slowest packet. Spanning AZs introduced a \\~2ms latency floor. For a web app, 2ms is invisible. For gradient synchronization, it was a disaster. It caused \"Straggler GPUs\" basically, 127 GPUs were sitting idle burning power while waiting for the 128th GPU to receive a packet across that cross-AZ link.\n\nThe Fix (and the headache):\n\n1. Physics > Availability: We had to violate our standard \"survivability\" protocols and condense the cluster into a single placement group to get the interconnect latency down to microseconds.\n2. The \"Egress Trap\": We looked at moving to a Neocloud (like CoreWeave) to save on compute, but the SRE team modeled the egress costs of moving the checkpoints back to our S3 lake. It wiped out the savings. We ended up building a \"Just-in-Time\" hydration script to move only active shards to local NVMe, rather than mirroring the whole lake.\n\nThe Takeaway for SREs: If your leadership is pushing for \"AI Cloud,\" stop looking at CPU/RAM metrics. Look at Jitter and East-West throughput. The bottleneck has shifted from \"can we get the chips?\" to \"can we feed them fast enough?\"\n\nI wrote up a deeper dive on the architecture (specifically the \"Hub and Spoke\" data pattern we used to fix the gravity issue) if anyone is interested in the diagrams:\n\n[https://www.rack2cloud.com/designing-ai-cloud-architectures-2026-gpu-neoclouds/](https://www.rack2cloud.com/designing-ai-cloud-architectures-2026-gpu-neoclouds/)\n\n*Has anyone else had to explain to management why \"High Availability\" architecture is actually bad for LLM training performance?*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qicfdh/rca_why_our_h100_training_cluster_ran_at_35/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o0qj456",
          "author": "maccam94",
          "text": "Neoclouds mostly make sense if you run whole training jobs there, then send your completed runs back to your cloud storage. AI workloads must maximize data locality and minimize egress.",
          "score": 6,
          "created_utc": "2026-01-20 21:02:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qtqtc",
              "author": "NTCTech",
              "text": "You are absolutely correct....the math only works if the compute savings significantly outweigh that initial data ingress and the final model egress.\n\nWhere we ran into trouble and why we ended up building that \"Just-in-Time\" hydration script was dealing with datasets in the multi-petabyte range. Moving the \"whole\" dataset over to the Neocloud before starting training was taking days and killing our iteration speed.\n\nWe've found that for the really big jobs, we have to stop treating data as a monolith and start streaming only the active shards to local NVMe. \"Maximize data locality\" is definitely the golden rule of '26.",
              "score": 2,
              "created_utc": "2026-01-20 21:50:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0r48cq",
                  "author": "Stephonovich",
                  "text": "> Maximize data locality\n\nSome of us have been screaming this for years, and were called old-fashioned, anti-progress, etc.\n\n2 msec is only invisible in a web app if it’s only happening a few times. When you have to take that hit on every transit, it adds up. This is the part that the optimists fail to understand: *most* web apps are not well-written, do not have performance in mind, and use a million abstractions to hide the fact that what is happening under the hood is very complex (often unnecessarily so, but I digress). So even if you built a well-designed schema, your ORM is going to murder performance by doing things like lazy-loading, rewriting JOINs to do separate SELECTs, etc. And when that happens, the 2 msec starts to add up very, very quickly.",
                  "score": 2,
                  "created_utc": "2026-01-20 22:42:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0sym6x",
          "author": "Ordinary-Role-4456",
          "text": "Funny how everything we learned from classic app deployments kind of falls apart with ML clusters. Leaders are always asking about redundancy but very few grok that distributed training is its own beast.   \n  \nAt the end of the day, feeding GPUs fast enough is more important than surviving a zone failure. It took a few wasted days of watching the cluster idle to hammer that lesson in here.",
          "score": 2,
          "created_utc": "2026-01-21 05:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzjmp",
              "author": "NTCTech",
              "text": "That \"wasted days watching the cluster idle\" hits home hard. It’s the most expensive screensaver in the world.\n\nYou nailed the core conflict here. The hardest part of this transition isn't the technology; it's de-programming leadership (and frankly, ourselves) from fifteen years of \"always-on,\" \"design for failure\" web architecture dogma.\n\nWe have to keep reminding them: Distributed training isn't a microservice; it's High-Performance Computing (HPC) with better marketing. If a node dies, you don't want graceful degradation; you want to crash fast, roll back to the last checkpoint, and restart with full bandwidth. Trying to survive a zone failure by sacrificing 40% performance is bad math.",
              "score": 1,
              "created_utc": "2026-01-21 10:34:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0t9hlj",
          "author": "granviaje",
          "text": "You forget one important thing: coreweave is muuuuuuuuch better at getting working hardware to you and handling gpu failures than AWS and Azure. \n\n But yes,  ML training is HPC not some web service. There are very different requirements. ",
          "score": 1,
          "created_utc": "2026-01-21 06:32:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u1f0e",
              "author": "NTCTech",
              "text": "That is an incredibly valid point and a fair critique.\n\nWe are definitely seeing the hyperscalers struggle with the physical reality of running these dense H100 racks at scale. The GPU failure rates on standard AWS P5 instances have been... higher than we'd like. The specialists like CoreWeave do seem to have better physical ops for keeping the silicon running right now because that's *all* they do.\n\nIt always comes down to that trade-off: Do you want better raw hardware reliability (Neoclouds), or do you want the comfort of your existing IAM/Security perimeter and data gravity (Hyperscalers)?\n\nAnd totally agree with your summary: It’s just HPC hidden behind a cloud API. We forget that at our peril.",
              "score": 1,
              "created_utc": "2026-01-21 10:50:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfc42f",
      "title": "What usually causes observability cost spikes in your setup?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfc42f/what_usually_causes_observability_cost_spikes_in/",
      "author": "jopsguy",
      "created_utc": "2026-01-17 12:34:45",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 0.79,
      "text": "We’ve seen a few cases where observability cost suddenly jumps without an obvious infra change.\n\nIn hindsight, it’s usually one of:\n\n* a new high-cardinality label\n* log level changes\n* sampling changes that weren’t coordinated\n\nFor people running OpenTelemetry in production:\n\n1. how do you detect these issues early?\n2. do you have any ownership model for telemetry cost?\n\nInterested in real-world approaches, not vendor answers.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfc42f/what_usually_causes_observability_cost_spikes_in/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o03krml",
          "author": "hawtdawtz",
          "text": "You monitor it with more observability and alerts.",
          "score": 10,
          "created_utc": "2026-01-17 13:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03mc95",
          "author": "Sea_Refrigerator5622",
          "text": "Definitely high cardinality labels. Monitoring and aggressive filtering are the only way. Tell teams you will filter first and ask questions later.",
          "score": 5,
          "created_utc": "2026-01-17 13:18:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03prk5",
          "author": "jjneely",
          "text": "I build dashboards around ownership and link the volume of telemetry from a service to how much that costs from the vendors.  This usually gets the team and the manager's attention.",
          "score": 3,
          "created_utc": "2026-01-17 13:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ap1pq",
          "author": "kusanagiblade331",
          "text": "One of my experience with cost spike was due to Splunk. High log ingestion spike event can be quite random. Usually, it can be due new logs being added, new features being added or some special incident events related.\n\nAs for some of the techniques that I have used in the past to control log ingestion spike, you can find them [here](https://www.linkedin.com/pulse/how-reduce-splunk-cloud-cost-without-losing-yao-hong-kok-zueac/?trackingId=Xi4K2gDCQzKrwI1tQZGzSA%3D%3D).",
          "score": 3,
          "created_utc": "2026-01-18 14:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g8q80",
          "author": "samsuthar",
          "text": "I think there are multiple ways to control costs.   \n1. Look at one week pattern and see which host or services affecting   \n2. Have ingestion control in place   \n3. Set up alerts with a threshold so you can get notified.",
          "score": 2,
          "created_utc": "2026-01-19 09:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06r113",
          "author": "foobarstrap",
          "text": "top issue: we do a node rollout in our Kubernetes cluster: replacing all the nodes, moving all pods around.",
          "score": 1,
          "created_utc": "2026-01-17 22:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dk972",
          "author": "maxfields2000",
          "text": "We have monitors on our collectors as well as on ingest (we use a vendor, but are converting the pipeline to OpenTel).\n\nThat said, our collectors block unknown/unapproved labels/tags on metrics in production and all tags/labels/new metrics that go to production go through a cardinality review, so it's rare we are \"surprised\" by a new tag/label.  We can be surprised by a sudden uptick in unique values used on already approved tags.  (side note, in QA/test environments tags/labels are essentially unique value rate limited so they can be actively developed without approval).\n\nSo our causes tend to be new services launching using existing tags/labels that increase cardinality.  Sudden spikes on logs for services that don't use sampling (we have a few.. it's usually actually loadtests, not production, that have gone sideways/spiraled out of control).\n\nWe do use some host based vendor monitoring as well, and someone accidentally deploying/activating hosts for monitoring can be another cause.\n\nIn all cases we have monitors/alerts that catch unexpected rate changes within minutes to hours depending on the type of cost and within 24/48 hours we've worked out containment. Our team has authority to immediately block/disable any high cost monitoring that lacks approval or we can't find the owner of as well. \n\nBy catching all cost issues within 12-24 hours we rarely have a surprise bill/cost.",
          "score": 1,
          "created_utc": "2026-01-18 23:04:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ouk7x",
          "author": "bourgeoisie_whacker",
          "text": "Our costs don't change(much) because we self host everything. I set a limit on scrapes and traces. Most we need to do is to throw more memory/cpu/storage at it.",
          "score": 1,
          "created_utc": "2026-01-20 16:24:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdfauz",
      "title": "I need to vent about process",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qdfauz/i_need_to_vent_about_process/",
      "author": "raslan81",
      "created_utc": "2026-01-15 09:40:40",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 0.63,
      "text": "Let's moan about process.  \n  \nProcess in tech feels like an onion. As products mature, more and more layers get added, usually after incidents or post mortems. Each layer is meant to make things safer, but we almost never measure what that extra process actually costs.\n\nWhen a post mortem leads to a new process, what we are really doing is slowing everyone down a little bit more. We do not track the impact on developer frustration, speed of execution, or the people who quietly leave because getting anything done has become painful.\n\nIf you hire good people, you should be able to accept that some things will go wrong and move on, rather than trying to process every failure out of existence. Most companies only reward the people who add process, because it looks responsible and is easy to defend. The people who remove process take the risk, and if anything goes wrong they get the blame, even if the team delivers faster and with fewer people afterwards.\n\nThat imbalance is why process only ever seems to grow, and why innovation slowly gets squeezed out.\n\n\n\n**Note:** thank you to Chatgpt for summarising my thoughts so eloquently \n\nEx SRE, now a Product Manager in tech.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qdfauz/i_need_to_vent_about_process/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "nzr3bj1",
          "author": "pdp10",
          "text": "> more and more layers get added, usually after incidents or post mortems. Each layer is meant to make things safer, but we almost never measure what that extra process actually costs.\n\nExactly correct.\n\n> Most companies only reward the people who add process, because it looks responsible and is easy to defend.\n\nAlso exactly correct.\n\nAlthough it seems a bit cliche for SRE, I do have something of an answer for you: writing code. When we've added automation to a process, it's usually been successful in the long run. Whereas when we add manual human steps, it's most often been a failure in the long run.\n\nInterestingly, in a well-lubricated modern team, it usually only requires one Individual Contributor to add automation, whereas it typically requires a higher authority to assign additional process to humans.\n\nI'm happily surprised that ChatGPT produced such an eloquent result.",
          "score": 12,
          "created_utc": "2026-01-15 16:26:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hefeo",
              "author": "FostWare",
              "text": "More layers and more process should usually mean additional preflight checks and clearer documentation until four confluence migrations and a bunch of staff turnover later, and you now can’t remember why the test is there but it damn well runs right every time.",
              "score": 3,
              "created_utc": "2026-01-19 14:44:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrue9p",
          "author": "daedalus_structure",
          "text": "Customers expect their data to be secure, the SLAs provided to them to be fulfilled, and investors expect that the money they are spending on engineering hours, the most expensive part of making software, isn't wasted. \n\nWhen you just depend on \"good people will do the right thing\", you get as many different ideas of what that right thing is as you have people. \n\nDoing things 50 different right ways is more damaging to an engineering organization than establishing a process that 45 will follow with professional attention and 5 will buck, because dealing with non-compliance is straightforward because expectations have been clearly set instead of \"I dunno, do what you want\". \n\nChildren always want to run with scissors and sometimes adults in the room need to tell them no. We completely understand that you want to cut fast. But Jimothy over there has one eye, and so we're not doing that anymore.",
          "score": 7,
          "created_utc": "2026-01-15 18:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00ma6a",
          "author": "WheredTheSquirrelGo",
          "text": "I hear you on the pain point, but 98% of people that are authorized to implement a process have no actual experience of good process design.\n\nGood process design eliminates waste and reduces output variation. Good process design champions enablement in a manner that is mindful of innovation. It’s an optimization model balancing the constraints to get the most value.\n\nRyan f9 does an excellent job explaining this in relation to Hondas manufacturing story. https://youtu.be/0LfbsW-5tAk?si=A72vlZWIBBQ4eMFt\n\nDon’t blame process for your pain, blame the human that doesnt understand process design principles. But ultimately participate in enabling good process design, be the change.",
          "score": 3,
          "created_utc": "2026-01-16 23:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqd8l4",
          "author": "the_packrat",
          "text": "Process is almost always the wrong way to make things safer as well. Where folks are adding process, it's usually because really improving things can't be conceived of by non-technical people, or because it's not considered worth the cost, so process is the plan B.\n\nIt has all the costs you mention, but typically is either neutral or more commonly actually makes things worse.",
          "score": -2,
          "created_utc": "2026-01-15 14:23:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfof5o",
      "title": "How many meetings / ad-hoc calls do you have per week in your role?",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qfof5o/how_many_meetings_adhoc_calls_do_you_have_per/",
      "author": "Ok_Discipline3753",
      "created_utc": "2026-01-17 20:48:55",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.7,
      "text": "I’m trying to get a realistic picture of what the day-to-day looks like. I’m mostly interested in:\n\n1. number of scheduled meetings per week\n2. how often you get ad-hoc calls or “can you jump on a call now?” interruptions\n3. how often you have to explain your work to non-technical stakeholders?\n4. how often you lose half a day due to meetings / interruptions\n\nhow many hours per week are spent in meetings or calls?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/sre/comments/1qfof5o/how_many_meetings_adhoc_calls_do_you_have_per/",
      "domain": "self.sre",
      "is_self": true,
      "comments": [
        {
          "id": "o066r6y",
          "author": "interrupt_hdlr",
          "text": "big tech numbers:  \n  \n1. about 6-7 scheduled meetings per week  \n2. every other day  \n3. rarely  \n4. 2-3 days per week",
          "score": 3,
          "created_utc": "2026-01-17 20:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06lf50",
          "author": "hawtdawtz",
          "text": "1. On average 6-7 outside of standups like the other guy said. I’m in two standups which adds another ~3 hours a week\n2. Roughly 1-3 times a week\n3. Varies, idk 0-3 times a week \n4. 30% of the time",
          "score": 2,
          "created_utc": "2026-01-17 22:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06ym0p",
          "author": "Escatotdf",
          "text": "1. 6-7 a week\n2. 2-3 a week\n3. About once a week\n4. 2-3 times a week\n\n8-11 hours of meetings total per week according to my calendar. As a senior SRE",
          "score": 2,
          "created_utc": "2026-01-17 23:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06a5i8",
          "author": "Ecstatic-Minimum-252",
          "text": "30 minutes meeting once a week",
          "score": 1,
          "created_utc": "2026-01-17 21:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08pz5b",
          "author": "SomeGuyNamedPaul",
          "text": "1. about 7, including daily stand-ups and I'm absolutely including them because they suck down 90 minutes and parts of my soul.  It's so terrible.\n\n2. very infrequently.  It's basically only a text chat with a co-worker extends past 4 exchanges \n\n3. Never\n\n4. Infrequently, but when it does happen my morale is so crushed that the rest of the day is wildly unproductive anyway.",
          "score": 1,
          "created_utc": "2026-01-18 05:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b9uik",
          "author": "Sufficient-Bad-7037",
          "text": "1. Once in a week\n2. Very infrequently \n3. Rarely\n4. Mostly async slack interruptions",
          "score": 1,
          "created_utc": "2026-01-18 16:25:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bq62a",
          "author": "Fearless_Meat_1655",
          "text": "1. 15-17 a week. In my previous org we had 20 in a week \n2. Ad hoc is 2-3 and lasts few mins ",
          "score": 1,
          "created_utc": "2026-01-18 17:43:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0d81w6",
          "author": "raisputin",
          "text": "Too often",
          "score": 1,
          "created_utc": "2026-01-18 22:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ekf8z",
          "author": "FullStackDestroyer",
          "text": "1.\t⁠number of scheduled meetings per week\n> 10-15 hrs\n\n2.\t⁠how often you get ad-hoc calls or “can you jump on a call now?” interruptions\n> 2-3x per day\n\n3.\t⁠how often you have to explain your work to non-technical stakeholders?\n> every single day.  Seriously.\n\n4.\t⁠how often you lose half a day due to meetings / interruptions\n> 2-3x per week.  There are ways to improve, which I’m working on, but change is hard.  Get a helmet. \n\nCreds: lead SRE/release at a public company, lots of AI dev, hundreds of services, dozens of external dependencies, software fails SO regularly, but we make it work…well.",
          "score": 1,
          "created_utc": "2026-01-19 02:19:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lpl7z",
          "author": "FreshEmd",
          "text": "I run two teams while also being hands on IC work, and I probably have on average 6-8 meetings per day. Ad-hoc calls would be difficult to count, but it's often.  These are not just standard meetings, they are often specific to a topic for solution design. All my hands on work happens after hours for 15-30 min I get between meetings. Which means if it wasn't for AI tools I don't think I'd be able to get anything meaningful done. Don't be me.",
          "score": 1,
          "created_utc": "2026-01-20 03:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi0m7n",
      "title": "Drafted a \"Ring 0\" safety checklist for kernel/sidecar deployments (Post-CrowdStrike)",
      "subreddit": "sre",
      "url": "https://www.reddit.com/r/sre/comments/1qi0m7n/drafted_a_ring_0_safety_checklist_for/",
      "author": "Neat_Economics_3991",
      "created_utc": "2026-01-20 13:06:44",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.83,
      "text": "Hey all,\n\nBeen digging into the mechanics of the CrowdStrike outage recently and wanted to codify a strict \"Ring 0\" protocol for high-risk deployments. Basically trying to map out the hard gates that should exist before anything touches the kernel or root.\n\nThe goal is to catch the specific types of logic errors (like the null pointer in the channel file) that static analysis often misses.\n\nHere is the current working draft:\n\n* Build Artifact (Static Gates)\n   * Strict Schema Versioning: Config versions must match binary schema exactly. No \"forward compatibility\" guesses allowed.\n   * No Implicit Defaults: Ban null fallbacks for critical params. Everything must be explicit.\n   * Wildcard Sanitization: Grep for \\* in input validation logic.\n   * Deterministic Builds: SHA-256 has to match across independent build environments.\n* The Validator (Dynamic Gates)\n   * Negative Fuzzing: Inject garbage/malformed data. Success = graceful failure, not just \"error logged.\"\n   * Bounds Check: Explicit Array.Length checks before every memory access.\n   * Boot Loop Sim: Force reboot the VM 5x. Verify it actually comes back online.\n* Rollout Topology\n   * Ring 0 (Internal): 24h bake time.\n   * Ring 1 (Canary): 1% External. 48h bake time.\n   * Circuit Breaker: Auto-kill deployment if failure rate > 0.1%.\n* 4. Disaster Recovery\n   * Kill Switch: Non-cloud mechanism to revert changes (Safe Mode/Last Known Good).\n   * Key Availability: BitLocker keys accessible via API for recovery scripts.\n\nI threw the markdown file on GitHub if anyone wants to fork it or PR better checks: [https://github.com/systemdesignautopsy/system-resilience-protocols/blob/main/protocols/ring-0-deployment.md](https://github.com/systemdesignautopsy/system-resilience-protocols/blob/main/protocols/ring-0-deployment.md)\n\nI also recorded a breakdown of the specific failure path if you prefer visuals: [https://www.youtube.com/watch?v=D95UYR7Oo3Y](https://www.youtube.com/watch?v=D95UYR7Oo3Y)\n\nCurious what other \"hard gates\" you folks rely on for driver updates?",
      "is_original_content": false,
      "link_flair_text": "DISCUSSION",
      "permalink": "https://reddit.com/r/sre/comments/1qi0m7n/drafted_a_ring_0_safety_checklist_for/",
      "domain": "self.sre",
      "is_self": true,
      "comments": []
    }
  ]
}