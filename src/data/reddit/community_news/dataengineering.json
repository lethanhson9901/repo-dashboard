{
  "metadata": {
    "last_updated": "2026-01-07 08:56:10",
    "time_filter": "week",
    "subreddit": "dataengineering",
    "total_items": 34,
    "total_comments": 567,
    "file_size_bytes": 570489
  },
  "items": [
    {
      "id": "1q034du",
      "title": "Senior Data Engineer Experience (2025)",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q034du/senior_data_engineer_experience_2025/",
      "author": "ElegantShip5659",
      "created_utc": "2025-12-31 03:55:05",
      "score": 773,
      "num_comments": 104,
      "upvote_ratio": 0.99,
      "text": "I recently went through several loops for Senior Data Engineer roles in 2025 and wanted to share what the process actually looked like. Job descriptions often don‚Äôt reflect reality, so hopefully this helps others.\n\nI applied to 100+ companies, had many recruiter / phone screens, and advanced to full loops at the companies listed below.\n\n# Background\n\n* Experience: 10 years (4 years consulting + 6 years full time in a product company)\n* Stack: Python, SQL, Spark, Airflow, dbt, cloud data platforms (AWS primarily)\n* Applied to mid large tech companies (not FAANG-only)\n\n# Companies Where I Attended Full Loops\n\n* Meta\n* DoorDash\n* Microsoft\n* Netflix\n* Apple\n* NVIDIA\n* Upstart\n* Asana\n* Salesforce\n* Rivian\n* Thumbtack\n* Block\n* Amazon\n* Databricks\n\n# Offers Received : SF Bay Area\n\n* **DoorDash** \\-¬† Offer not tied to a specific team (**ACCEPTED**)\n* **Apple** \\- Apple Media Products team\n* **Microsoft** \\- Copilot team\n* **Rivian** \\- Core Data Engineering team\n* **Salesforce** \\- Agentic Analytics team\n* **Databricks** \\- GTM Strategy & Ops team\n\n# Preparation & Resources\n\n1. **SQL & Python**\n   * Practiced complex joins, window functions, and edge cases\n   * Handling messy inputs primarily json or csv inputs.\n   * Data Structures manipulation\n   * Resources: stratascratch & leetcode\n2. **Data Modeling**\n   * Practiced designing and reasoning about fact/dimension tables, star/snowflake schemas.\n   * Used AI to research each company‚Äôs business metrics and typical data models, so I could tie Data Model solutions to real-world business problems.\n   * Focused on explaining trade-offs clearly and thinking about analytics context.\n   * Resources: AI tools for company-specific learning\n3. **Data System Design**\n   * Practiced designing pipelines for batch vs streaming workloads.\n   * Studied trade-offs between Spark, Flink, warehouses, and lakehouse architectures.\n   * Paid close attention to observability, data quality, SLAs, and cost efficiency.\n   * Resources: *Designing Data-Intensive Applications* by Martin Kleppmann, *Streaming Systems* by Tyler Akidau, YouTube tutorials and deep dives for each data topic.\n4. **Behavioral**\n   * Practiced telling stories of ownership, mentorship, and technical judgment.\n   * Prepared examples of handling stakeholder disagreements and influencing teams without authority.\n   * Wrote down multiple stories from past experiences to reuse across questions.\n   * Practiced delivering them clearly and concisely, focusing on impact and reasoning.\n   * Resources: STAR method for structured answers, mocks with partner(who is a DE too), journaling past projects and decisions for story collection, reflecting on lessons learned and challenges.\n\n**Note:** Competition was extremely tough, so I had to move quickly and prepare heavily. My goal in sharing this is to help others who are preparing for senior data engineering roles.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q034du/senior_data_engineer_experience_2025/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwuuw6d",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-31 03:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuwh9h",
          "author": "smartdarts123",
          "text": "Did most of those places put you through the standard leetcode style coding screens?",
          "score": 56,
          "created_utc": "2025-12-31 04:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuww26",
              "author": "ElegantShip5659",
              "text": "NVIDIA, Block and Netflix were typical LC. The rest were mostly Data Structure Manipulation, cleaning up messy JSON and deriving few aggregations. And typical SQL style Q's",
              "score": 89,
              "created_utc": "2025-12-31 04:07:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuy3h8",
                  "author": "smartdarts123",
                  "text": "That's cool, thanks for sharing your experience. Did you do any specific prep for coding screens or did you find that your existing experience was sufficient to feel your way through the problems?\n\nFor example, I'm pretty sure I'd breeze through json parsing, data manipulations, etc, but for things that are more leetcode style, I need to study.",
                  "score": 16,
                  "created_utc": "2025-12-31 04:15:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuwwfv",
              "author": "Odd_Strength_9566",
              "text": "+1",
              "score": 0,
              "created_utc": "2025-12-31 04:08:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuz1ca",
          "author": "discussitgal",
          "text": "Can you share some examples of data structure manipulations? Was it basic array dicts and pandas?",
          "score": 29,
          "created_utc": "2025-12-31 04:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuzp7n",
              "author": "ElegantShip5659",
              "text": "Yes. Mostly arrays and dicts.",
              "score": 28,
              "created_utc": "2025-12-31 04:26:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwtox8",
                  "author": "Seven_Minute_Abs_",
                  "text": "Can you give an example of?",
                  "score": 3,
                  "created_utc": "2025-12-31 13:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuxy2o",
          "author": "on_the_mark_data",
          "text": "Those are all great offers! Beyond TC, is there a reason why you chose Doordash over others?",
          "score": 22,
          "created_utc": "2025-12-31 04:14:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuyz6o",
              "author": "ElegantShip5659",
              "text": "TC definitely played a role, to be honest, but what really drew me to the team I got matched to were the growth opportunities and the kind of projects and work the team was doing. It just felt like the right fit for me.",
              "score": 25,
              "created_utc": "2025-12-31 04:21:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvbgy7",
          "author": "Sad-Tomato3450",
          "text": "Impressive. For the above listed companies did you apply directly or via networking. Wanted to understand the schematics given the market is overwhelmingly saturated with more applicants than there are openings.\nCongratulations for the new beginnings !!!",
          "score": 17,
          "created_utc": "2025-12-31 05:49:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvmnqj",
              "author": "ElegantShip5659",
              "text": "It‚Äôs a mix of direct applications and networking via linkedin. For me direct applications worked for the most part.",
              "score": 12,
              "created_utc": "2025-12-31 07:22:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvnzv9",
                  "author": "Sad-Tomato3450",
                  "text": "Nice to hear that direct application is still working wonders. Let me know if you will be open to do a resume overview for me. I am trying to get an opinion for someone who is battle tested :)",
                  "score": 9,
                  "created_utc": "2025-12-31 07:34:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxqnziz",
                  "author": "ratzz505",
                  "text": "don‚Äôt want to tag on but, would be open to DM me as well to go over some resume help ? I have success recruiter reaching out on linkedIn but I direct application never works. Happy to look if you have post on resume tips in any public forum.",
                  "score": 1,
                  "created_utc": "2026-01-05 01:55:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv0v6c",
          "author": "MichelangeloJordan",
          "text": "Congrats OP! New year, new job. Hope both are good!",
          "score": 15,
          "created_utc": "2025-12-31 04:34:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv1jf2",
              "author": "ElegantShip5659",
              "text": "Thank you",
              "score": 3,
              "created_utc": "2025-12-31 04:38:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvi3id",
          "author": "codemega",
          "text": "I interviewed with DoorDash a couple years ago and I struggled with two rounds:\n\nData modeling - they gave me some weird metric that I needed to build tables for. It was difficult to even understand what the metric was. I was a bit lost on it.\n\nSystem design - the interviewer asked me to design a url shortener. This is a classic SWE sys design question but I had little idea of how to do it as a data engineer.\n\nAnyway, interviews can vary a lot depending on who you get matched up with. Congrats for getting through.",
          "score": 8,
          "created_utc": "2025-12-31 06:42:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvojc7",
              "author": "ElegantShip5659",
              "text": "Agreed, my DoorDash interview was generic. I got matched with a team after clearing the interview. I guess things may have changed or you may have interviewed for a team with a specific need?",
              "score": 6,
              "created_utc": "2025-12-31 07:39:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww8eu6",
                  "author": "R0kies",
                  "text": "Good stuff this post buddy! Regarding data modeling, you mentioned it revolved around metrics, as commenter mentioned it too, it's seems usual. \n\nHow does it go around? They give you facts with 100 columns and 20 dimensions and you are supposed to pick what columns are needed and in what relationship? \n\nWas it mostly OLAP or OLTP too?",
                  "score": 2,
                  "created_utc": "2025-12-31 10:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv2mww",
          "author": "wiseyetbakchod",
          "text": "This helps, thanks a lot.",
          "score": 6,
          "created_utc": "2025-12-31 04:45:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv32gt",
          "author": "Pristine-Trainer7109",
          "text": "Congrats on your offers! Can you share some of the data modeling questions?",
          "score": 7,
          "created_utc": "2025-12-31 04:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv3t1u",
              "author": "ElegantShip5659",
              "text": "Almost all companies came up with a typical \"design a data model for a xxxx( for ex: music streaming service)\"  Focus was primarily on agreeing upon metrics with the interviewer, defining core objects, building the facts and dimensions, writing SQL at the end to achieve those metrics. Sometimes had to draw a visual to represent the metric. A few companies started with providing list of metrics directly and had to work around the model accordingly.",
              "score": 26,
              "created_utc": "2025-12-31 04:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwv5vva",
                  "author": "sureveS_Snape",
                  "text": "Congrats on your offer(s). Do you have any resource recommendations for data modeling?",
                  "score": 2,
                  "created_utc": "2025-12-31 05:08:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv96lq",
          "author": "leonseled",
          "text": "Any other resources for streaming workloads? I‚Äôm a mid-level engineer looking to upskill in 2026. One of my goals is adding streaming proficiency to my toolbelt.",
          "score": 4,
          "created_utc": "2025-12-31 05:32:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvn417",
              "author": "ElegantShip5659",
              "text": "Tech blogs, youtube videos. I focussed on Kafka and Flink architecture. Zach Wilson and Darshil Parmar on Youtube",
              "score": 7,
              "created_utc": "2025-12-31 07:26:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv5cm0",
          "author": "cmcclu5",
          "text": "Excellent insights. I don‚Äôt know how you‚Äôre getting all these interviews, though. I have a decade+ in the same tech stacks plus others and can‚Äôt even get a returned phone call. Enjoy that new job and paycheck! Good New Years‚Äô present!",
          "score": 5,
          "created_utc": "2025-12-31 05:04:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnmyc",
              "author": "ElegantShip5659",
              "text": "I completely agree, the market has been brutal even for very strong profiles. Honestly, i think i was just lucky.",
              "score": 5,
              "created_utc": "2025-12-31 07:31:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwy3q51",
          "author": "Foreign_Yam3729",
          "text": "Thanks for sharing !! Any suggested tutorials/links for the below things:\n1. System design round ( esp for data)\n2. Data modelling apart from Kimball/chatgpt as mentioned earlier",
          "score": 3,
          "created_utc": "2025-12-31 17:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv0du5",
          "author": "Garcon_sauvage",
          "text": "TC?",
          "score": 2,
          "created_utc": "2025-12-31 04:30:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv1tq5",
              "author": "ElegantShip5659",
              "text": "DM'd",
              "score": 3,
              "created_utc": "2025-12-31 04:40:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwytmhk",
                  "author": "x1084",
                  "text": "I'm curious as well, if you don't mind sharing. I'm also in a HCOL area, albeit not Bay Area level.",
                  "score": 1,
                  "created_utc": "2025-12-31 19:48:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxj3x1x",
                  "author": "azncuttie",
                  "text": "OP, can you also DM me about TC? Thank you.",
                  "score": 1,
                  "created_utc": "2026-01-03 23:39:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwv1jky",
              "author": "Funny-Message-9282",
              "text": "Total compensation. It includes your base salary + Bonus + RSUs (Restricted Stock Units)",
              "score": 0,
              "created_utc": "2025-12-31 04:38:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv1cze",
          "author": "DRUKSTOP",
          "text": "Ive interviewer at Meta, DD, Stripe, TikTok, and Amazon over the last 2 years and they all dus typical Leetcode. So interesting to see some may have changed.",
          "score": 2,
          "created_utc": "2025-12-31 04:37:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv1t3d",
              "author": "ElegantShip5659",
              "text": "Meta and DD have the same format from years I suppose. The others are mostly team dependent in my exp. Amazon has made some changes - they now do a 75 min 1st round covering coding, data modeling, system design and LP all in 75 min. And then a 4-5 team loop",
              "score": 5,
              "created_utc": "2025-12-31 04:40:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwuyml",
                  "author": "DRUKSTOP",
                  "text": "So meta was 5 python + 5 SQL leetcode, all in 50 minutes?",
                  "score": 2,
                  "created_utc": "2025-12-31 13:42:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwv28le",
              "author": "vuachoikham167",
              "text": "Just curious, do those companies ever ask you hard Leetcode Qs or mostly easy-medium?",
              "score": 3,
              "created_utc": "2025-12-31 04:43:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv2pen",
                  "author": "ElegantShip5659",
                  "text": "For data engineering, I wouldn‚Äôt expect anything beyond medium, though Netflix and NVIDIA asked me hard questions.",
                  "score": 10,
                  "created_utc": "2025-12-31 04:46:25",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv91k6",
          "author": "Pranu12",
          "text": "OP, Congratulations on your new offers. Really impressed with the amount of hardwork that went on during your preparation. I'm a 4 year experienced person who really wants to secure a job in a product based company. Could you please guide me PLEASEEE!!",
          "score": 2,
          "created_utc": "2025-12-31 05:31:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo05p",
              "author": "ElegantShip5659",
              "text": "Sure i'll DM",
              "score": 2,
              "created_utc": "2025-12-31 07:34:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvdki2",
          "author": "verus54",
          "text": "Any recommendations on how to get through on the resume screen? I seem to be getting auto rejected by bigger companies and I get plenty of interviews at smaller companies. 3YOE consulting + 1YOE on product.",
          "score": 2,
          "created_utc": "2025-12-31 06:05:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnduy",
              "author": "ElegantShip5659",
              "text": "Only thing i did outside of normal was optimize my resume to include keywords to match ATS systems. I used Jobscan and ChatGpt",
              "score": 3,
              "created_utc": "2025-12-31 07:28:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvgwdf",
          "author": "IcyRashid",
          "text": "Congratulations OP. This is a constructive post. Did any of those companies ask coding questions using Spark, or were they only plain SQL and Python questions?",
          "score": 2,
          "created_utc": "2025-12-31 06:32:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvoeyx",
              "author": "ElegantShip5659",
              "text": "Mostly plain SQL and Python. Upstart and Salesforce did specifically ask to write code using PySpark.",
              "score": 3,
              "created_utc": "2025-12-31 07:38:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvhik7",
          "author": "LelouchYagami_",
          "text": "Really cool of you for sharing.",
          "score": 2,
          "created_utc": "2025-12-31 06:37:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwviv85",
          "author": "XcytekOfficial",
          "text": "I appreciate this entry. Really helpful.",
          "score": 2,
          "created_utc": "2025-12-31 06:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvo7yf",
          "author": "RDTIZFUN",
          "text": "Congrats and thanks for the post (would be cool if you could add a rough estimate +/- $10k-$20k of actual TC offers from those companies and whether they were remote or hybrid/onsite, thanks).",
          "score": 2,
          "created_utc": "2025-12-31 07:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvspyz",
          "author": "Alternative-Guava392",
          "text": "Legend run of interviews! Congratulations and thanks for the notes.",
          "score": 2,
          "created_utc": "2025-12-31 08:18:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxlcqd",
          "author": "PipelineInTheRain",
          "text": "Congrats on receiving/accepting the offer! Out of curiosity, how much do you feel your experience with AWS played a part in your interviews?",
          "score": 2,
          "created_utc": "2025-12-31 16:05:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxvbej",
          "author": "MassyKezzoul",
          "text": "Thanks for sharing, this help a lot. Can you share an estimate of the TC offers of those companies ?",
          "score": 2,
          "created_utc": "2025-12-31 16:55:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwykwck",
          "author": "Pucci800",
          "text": "Congratulations! This is really impressive and helps a ton.",
          "score": 2,
          "created_utc": "2025-12-31 19:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv88p3",
          "author": "Deiice",
          "text": "Which ressource(s) helped you the most to prepare for these?",
          "score": 2,
          "created_utc": "2025-12-31 05:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvqz8m",
              "author": "ElegantShip5659",
              "text": "ChatGPT and Youtube for Product Sense, Design and Data Modeling. LC and Stratascratch for Coding.",
              "score": 3,
              "created_utc": "2025-12-31 08:02:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvkg1f",
          "author": "Impossible-Appeal660",
          "text": "OP, What's the TC offered (if you dont mind sharing)?",
          "score": 2,
          "created_utc": "2025-12-31 07:02:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvolb9",
              "author": "ElegantShip5659",
              "text": "TC is 430 after final negotiations.",
              "score": 15,
              "created_utc": "2025-12-31 07:40:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxiimfo",
                  "author": "escarbadiente",
                  "text": "how do you negotiate? they set a price and you tell them hey i need X, or do you just let them propose and accept? how do you explain what you're worth?",
                  "score": 1,
                  "created_utc": "2026-01-03 21:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv33yq",
          "author": "Realistic_Ad_5409",
          "text": "Anyone has experience with Capitalone powerday for lead data engineer role they can share?",
          "score": 1,
          "created_utc": "2025-12-31 04:49:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvg5rh",
          "author": "[deleted]",
          "text": "Hey just a doubt \nI‚Äôm a junior data analyst trying to pivot into Data Engineering. Do u have any advice for me?\nOr is it very difficult to switch from here ?",
          "score": 1,
          "created_utc": "2025-12-31 06:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo5ls",
              "author": "ElegantShip5659",
              "text": "Industry now has a new role Data Analytics Engineer which is very close to Data Analyst. Have you had a chance to explore those roles?",
              "score": 3,
              "created_utc": "2025-12-31 07:35:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvolrp",
                  "author": "[deleted]",
                  "text": "I‚Äôve heard about analytics engineer but haven‚Äôt seen a lot of openings the thing is tbh I don‚Äôt like being an analyst I like the building thinking of systems kinda thing. I had to take this cus this was the only role I was close to getting at the time so looking to switch into something more engineering oriented",
                  "score": 1,
                  "created_utc": "2025-12-31 07:40:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvgq7s",
          "author": "ChainEnvironmental58",
          "text": "Congrats! Any specific resource or platform you recommend or any guide for Python preparation specifically for DEs? thanks",
          "score": 1,
          "created_utc": "2025-12-31 06:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvoaf4",
              "author": "ElegantShip5659",
              "text": "I have hands on python experience in my job, so outside of that I practiced LC Easy and Medium.",
              "score": 2,
              "created_utc": "2025-12-31 07:37:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvtzmd",
          "author": "BusinessRoyal9160",
          "text": "Thanks for the detailed information.\n\nI am on the same boat as you and I am struggling to find good resources for Data System Design. Could you please share the resources e.g. YouTube playlists which you followed?",
          "score": 1,
          "created_utc": "2025-12-31 08:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvywyi",
          "author": "xorgeek",
          "text": "Congratulations on all ur offers.\n\nCould share some concrete resources of data system design",
          "score": 1,
          "created_utc": "2025-12-31 09:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvz0je",
          "author": "FuckTheStateofOhio",
          "text": "Just out of curiosity, what did your experience in consulting look like? Were you client facing? Was your firm primarily focused on data projects? Also feel free to DM me if you don't feel like disclosing this info publicly.",
          "score": 1,
          "created_utc": "2025-12-31 09:18:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvz2dt",
          "author": "ReginaldLlama3",
          "text": "Congrats. Happy New Year indeed",
          "score": 1,
          "created_utc": "2025-12-31 09:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww81jt",
          "author": "Constant_Vegetable13",
          "text": "Hey Congartulations! What was your TC? Can you please DM me? Thanks.",
          "score": 1,
          "created_utc": "2025-12-31 10:43:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwmaoh",
          "author": "soh219",
          "text": "Congrats, how did you structure your resume, I‚Äôve been consulting for the last 4 yrs and I‚Äôm about to enter the job market",
          "score": 1,
          "created_utc": "2025-12-31 12:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwvqg3",
          "author": "ForPosterS",
          "text": "Congrats! Did you get any questions around cloud/AWS/data services in your interviews or was it largely around Python, SQL and data modeling? Also, in Python topics would you recommend looking into python specifically for someone who is preparing for interviews? Should the focus be more on data manipulation like arrays, dicts, pyspark or object oriented programming?\n\nAlso, did any place expect data bricks?",
          "score": 1,
          "created_utc": "2025-12-31 13:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwy8n0",
          "author": "m98789",
          "text": "What is your education background?",
          "score": 1,
          "created_utc": "2025-12-31 14:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx0jqz",
          "author": "CometChaserStarGazer",
          "text": "Congratulations OP! Could you share some prep materials for System Design? I recently interviewed at 3 companies from your list and I feel like the system design round was my weakest.",
          "score": 1,
          "created_utc": "2025-12-31 14:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx31cj",
          "author": "mindwrapper13",
          "text": "Wow this is amazing! Congrats! How much time did it take to prepare all this? What about topics like Spark etc ? Do you already know them from experience?",
          "score": 1,
          "created_utc": "2025-12-31 14:30:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx8u7w",
          "author": "halfrightface",
          "text": "nice notes and congrats. i've been doing SDE rounds lately and this is pretty similar to my experiences as well. were you going from SDE to SDE and looking for a pay bump or from DE to SDE? also did you apply to any analytics engineering roles?",
          "score": 1,
          "created_utc": "2025-12-31 15:02:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx9n68",
          "author": "_Marwan02",
          "text": "Hello,\n\nDid you have any DSA rounds? If so, how difficult were they?\nFor the SQL test, was it more LeetCode-style questions or real-world scenarios?",
          "score": 1,
          "created_utc": "2025-12-31 15:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0azs7",
          "author": "noobcoder17",
          "text": "This is very helpful OP. Happy new year and wishing you the best in your new role.   \nAre you on a work VISA or citizen? Asking to gauge the market for people on a work visa.",
          "score": 1,
          "created_utc": "2026-01-01 00:53:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0cc0v",
          "author": "siav8",
          "text": "TC?",
          "score": 1,
          "created_utc": "2026-01-01 01:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0wso7",
          "author": "solo_stooper",
          "text": "Wow. Nice! Congrats!¬†Were your 4 years of consulting were about tech consulting? I have a somewhat similar background with 3+ years in civil engineering consulting before switching to tech. Also no CS bachelor‚Äôs but a masters in analytics.¬†",
          "score": 1,
          "created_utc": "2026-01-01 03:17:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0x0t3",
          "author": "solo_stooper",
          "text": "Curious about your examples¬†of handling stakeholder disagreements and influencing teams without authority. Genuinely interested even for my personal development!",
          "score": 1,
          "created_utc": "2026-01-01 03:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1hg4l",
          "author": "GMUsername",
          "text": "Hey! Appreciate the post and the resources you used to study. Just wondering if the offers you received were based on location? Any offers remote? Not looking to move from my current location",
          "score": 1,
          "created_utc": "2026-01-01 05:55:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx26kag",
          "author": "Tall_Working_2146",
          "text": "Man congratulations on your new job, would you be free in mentoring an engineering student graduating in 1.5 years? I would like to spend it working on specializing in data/cloud although I have a roadmap for it being under the guidance of an expert sure is rewarding, won't take much of your time promise and I promise to be as teachable as clay.",
          "score": 1,
          "created_utc": "2026-01-01 10:09:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4xbyv",
          "author": "Happy_guy_1980",
          "text": "How much were these places paying for Sr Data Engineers?",
          "score": 1,
          "created_utc": "2026-01-01 20:35:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx65nsc",
          "author": "No_Cat_8466",
          "text": "First of all, congratulations on your offers! Thanks so much for sharing this it's an absolute gold mine. I've been trying to figure out a plan to pivot for the 2026 hiring cycle (if there even is one), so this is super helpful.\n\nReally glad to hear that direct applications are still effective! Would you consider doing a post about your resume and how you approached it? I think a lot of people would find that valuable.",
          "score": 1,
          "created_utc": "2026-01-02 00:33:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6r1qm",
          "author": "kushagraketo21",
          "text": "Hey thats a very elaborate post, puts things into perspective for someone preparing for a similar role.",
          "score": 1,
          "created_utc": "2026-01-02 02:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc4sds",
          "author": "EdwardMitchell",
          "text": "What AI experience do you have? How did you get these two offers?\n\n‚Å†Microsoft - Copilot team\n‚Å†Salesforce - Agentic Analytics team",
          "score": 1,
          "created_utc": "2026-01-02 22:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdz331",
          "author": "thowawaywookie",
          "text": "Are you in the United States or somewhere else?",
          "score": 1,
          "created_utc": "2026-01-03 05:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxedw4v",
          "author": "krmehul-tech-7564",
          "text": "Thanks for sharing.",
          "score": 1,
          "created_utc": "2026-01-03 07:10:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxn2g0a",
          "author": "AnyConcentrate7050",
          "text": "Congratulations, could you share more resources that can help for an aspiring DA to DE",
          "score": 1,
          "created_utc": "2026-01-04 15:44:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxn49h6",
          "author": "AnyConcentrate7050",
          "text": "What‚Äôs your advice for someone transitioning from Data Analysis to Engineering, resources for upskilling and learning road map",
          "score": 1,
          "created_utc": "2026-01-04 15:52:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxni8fp",
          "author": "NewCut7254",
          "text": "Do you have any tips or recommendations for someone who wants to transition from data analytics to data engineering?",
          "score": 1,
          "created_utc": "2026-01-04 16:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxskcqv",
          "author": "Humble-Air3352",
          "text": "Congratulations üéâ¬†\nCould you please also share how much time you invested in your preparation and how long these companies took to complete all the interview rounds?\n\n\nAdditionally, which months of the year are generally considered the best for a job search?",
          "score": 1,
          "created_utc": "2026-01-05 10:03:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvhd7a",
          "author": "next-strangr",
          "text": "thanks , this is helpful. Please check dm",
          "score": 1,
          "created_utc": "2025-12-31 06:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwya1p3",
          "author": "Available_Fig_1157",
          "text": "How many experiences do you have ?",
          "score": 0,
          "created_utc": "2025-12-31 18:07:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwyar8n",
          "author": "TheITGuy93",
          "text": "+1",
          "score": 0,
          "created_utc": "2025-12-31 18:11:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1iezn",
      "title": "Can we do actual data engineering?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q1iezn/can_we_do_actual_data_engineering/",
      "author": "marketlurker",
      "created_utc": "2026-01-01 23:22:22",
      "score": 184,
      "num_comments": 68,
      "upvote_ratio": 0.84,
      "text": "Is there any way to get this subreddit back to actual data engineering? The vast majority of posts here are how do I use <fill in the blank> tool or compare <tool1> to <tool2>. If you are worried about how a given tool works, you aren't doing data engineering. Engineering is so much more and tools are near the bottom of the list of things you need to worry about. \n\n<rant>The one thing this subreddit does tell me is that the Databricks marketing has earned their yearend bonus. The number of people using the name medallion architecture and the associated colors is off the hook. These design patterns have been used and well documented for over 30 years. Giving them a new name and a Databricks coat of paint doesn't change that. It does however cause confusion because there are people out there that think this is new.</rant>",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q1iezn/can_we_do_actual_data_engineering/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx5tvyt",
          "author": "rycolos",
          "text": "I'll take someone asking how to do an scd 2 snapshot in dbt a million times over some doofus sharing his AI-written linkedin or substack hype shitpost for \"conversation\"",
          "score": 231,
          "created_utc": "2026-01-01 23:28:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5ug0l",
              "author": "SirGreybush",
              "text": "Upvotey for doofus use. Happy Festivus.",
              "score": 23,
              "created_utc": "2026-01-01 23:31:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx5wymg",
              "author": "5pitt4",
              "text": "I'm new, where can i learn about scd 2 (and I'm assuming there are other types like1 or 3)?",
              "score": 8,
              "created_utc": "2026-01-01 23:45:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx67twz",
                  "author": "TheOneWhoSendsLetter",
                  "text": "Kimball's Data Warehouse Toolkit",
                  "score": 32,
                  "created_utc": "2026-01-02 00:46:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx84gvz",
                  "author": "daguito81",
                  "text": "SCD is Slowly Changing Dimesions. And it‚Äôs basically type 1 and 2\n\nSimplifying a bit here, before the NF police comes. Imagine you have a table for customers, in that table you have an address. I have many transactions every day but ‚Äúa new customer‚Äù or change a customer is not something that happens all the time. So that table changes ‚ÄúSlowly‚Äù   That‚Äôs the SCD. It‚Äôs a fancy way to say ‚ÄúA dimensional table that changes‚Äù \n\nNow type 1 vs type 2 basically determines what you do when something changes. \n\nLet‚Äôs say you changed your address. I can just go and update the customer table with your id and your new address and call it a day. That‚Äôs a Type 1. \n\nHowever let‚Äôs say you need to retain all the addresses you‚Äôve had historically. So you create some new metadata columns. A ‚Äúcurrent‚Äù column, and a couple of date columns. \n\nSo now when you changed your address I basically copy your line changed the old one ‚Äúcurrent‚Äù to false than when th change happens. And the new one  set current to true. \n\nSo now if I want your address infilter by your id and tha current is True. If I want your old ones. Filter by current is False and the date columns will order them. \n\nDoing that ‚Äúbasically not replacing the line but instead creating a new one for every change ‚Äú makes it a Type 2 \n\nSo SCD1 and SCD2 are basically those\n\nEdit: Just realized that I put 1 and 3 at the beginning fixed to 1 and 2",
                  "score": 10,
                  "created_utc": "2026-01-02 08:59:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6m7nf",
                  "author": "marketlurker",
                  "text": "It isn't just slowly changing dimensions you need to understand. Look at all the types of SCDs.\n\nThere are different types of tables, like associative, when to use them and when not to. I'm looking at you temp tables (vs CTEs).\n\nThe different types of normalization and why no one outside of an educational environment using anything beyond 3NF. Has anyone used Boyce-Codd in real life? Understand why so many crappy tools out there think the world begins and ends with 1NF (lists).\n\nUnderstand the different types of joins and when to use them.\n\nThere are so many useful things to understand in DE. I am starting to use someone using the phrase \"medallion architecture\" or \"gold layer\" as a canary in the coal mine for a relative newbie in the discipline.",
                  "score": 7,
                  "created_utc": "2026-01-02 02:13:26",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx7mqke",
                  "author": "its_PlZZA_time",
                  "text": "The Wikipedia article is actually pretty good\n\nhttps://en.wikipedia.org/wiki/Slowly_changing_dimension",
                  "score": 2,
                  "created_utc": "2026-01-02 06:18:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx8zfcw",
                  "author": "Thinker_Assignment",
                  "text": "wikipedia, google. We host some colab code demos that come up when you google.\n\ndid you know why we call it slowly? because if it changes faster than our load interval (like daily) then we do not capture the change - so it only works with \"slowly\" changing dimensions\n\nSCD2 is the \"main\" you use for historisation and all the rest from 2+ are just derivatives for different use cases",
                  "score": 1,
                  "created_utc": "2026-01-02 13:22:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6u95m",
                  "author": "SRMPDX",
                  "text": "Claude. Ok I'm only slightly kidding.",
                  "score": 1,
                  "created_utc": "2026-01-02 03:02:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5xaqz",
                  "author": "beta_ketone",
                  "text": "Google, wikipedia...",
                  "score": -1,
                  "created_utc": "2026-01-01 23:47:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5wpx1",
          "author": "SmallAd3697",
          "text": "There are so many different types of folks using big data tools.  Many aren't even coders, aside from using a minimal amount of notebook-hosted python and SQL.\n\nI think it is important to realize that many folks build solutions by connecting a bunch of software components together using configuration. They focus on these third-party components and show little interest in underlying software engineering concepts. Some may not even know the implication when choosing between rowstore and columnstore, or between one language/runtime or another.\n\nSometimes this just seems like a community of chefs who talk about microwave dinners, and they focus primarily on the type of microwave that cooks the fastest. I find that data engineers can become quickly stunted, as compared to other types of software engineers.",
          "score": 61,
          "created_utc": "2026-01-01 23:44:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9w62y",
              "author": "Little_Kitty",
              "text": "> a community of chefs who talk about microwave dinners\n\nBrutal but funny XD",
              "score": 13,
              "created_utc": "2026-01-02 16:18:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5u1gs",
          "author": "ThroughTheWire",
          "text": "unfortunately most people nowadays aren't doing \"actual\" data engineering anymore. that's probably like 10-20 percent of people with the title",
          "score": 73,
          "created_utc": "2026-01-01 23:29:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx849wp",
              "author": "Feisty_Following9720",
              "text": "I was saying that 10 years ago. Its gotta be lower now.",
              "score": 6,
              "created_utc": "2026-01-02 08:58:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx9ux4s",
              "author": "Little_Kitty",
              "text": "I feel a lot of the people here are actually just configuring some json held inside a python script to change filenames / paths.  Very little thinking about memory usage, spilling to disk, cluster complexities, optimal data types, incrementalisation etc.",
              "score": 3,
              "created_utc": "2026-01-02 16:12:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5xrfs",
          "author": "Automatic_Red",
          "text": "I'm just happy the Excel formatting questions are fairly minimal.",
          "score": 14,
          "created_utc": "2026-01-01 23:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5ugma",
          "author": "peterxsyd",
          "text": "Dude -  I think that - today - people think that's what data engineering is. And sadly it's what it's become - a funnel for overpriced DataBricks and Microsoft. And all the consultants with partnerships with those guys recommend them - and the CIO's accept the recommendations and it's all a big fugaze.\n\nData modelling and engineering actual information about what is going on in a business to show it which is so much more important - I actually genuinely don't think people focus there now. And those companies don't care at all - because - people are paying 10K a month (or more) for tables when they have 5 million rows, still hitting spark java jvm out of memory errors and a postgres database would literally solve their problems way easier and more effectively.\n\nI'm with ya.",
          "score": 41,
          "created_utc": "2026-01-01 23:31:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7tqq4",
              "author": "Ddog78",
              "text": "It's a bit sad to be honest.\n\nI was curious so I applied for a few positions. Nearly all the interviewers were focused on either tech stack or weird architectural questions that clearly had one right answer.",
              "score": 2,
              "created_utc": "2026-01-02 07:18:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx8givh",
              "author": "girlgonevegan",
              "text": "As someone who has to work in a Marketing Automation Platform at the enterprise level dealing with high volumes of data, I appreciate this insight. I‚Äôve never held a DE title as it‚Äôs not even formally my background, but the work I do feels like engineering and often feels more complex than what seems to get the most funding and resources. It‚Äôs hard for me to wrap my head around. It‚Äôs as if decision makers cannot differentiate between production databases and warehouses. Bill Inmon wrote about the ‚Äú[Data Warehouse Blues](https://www.linkedin.com/posts/billinmon_activity-7412163904841109504-58UF?utm_source=share&utm_medium=member_ios&rcm=ACoAAAYK0cMB7NggJ0_c4Cnsb2WcD4klavNLF8E),‚Äù and I finally feel like I‚Äôm not crazy.",
              "score": 1,
              "created_utc": "2026-01-02 10:53:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8h12q",
                  "author": "girlgonevegan",
                  "text": "What‚Äôs astonishing to me is that I have been apart of companies where the internal IT department will admit they are doing these implementations KNOWING they will not produce the desired outcome. It‚Äôs as if they themselves are just going through the motions because this is their ‚Äúbullshit job,‚Äù and they blame the vendors because they don‚Äôt think any of it actually works (because they‚Äôve never seen it since they don‚Äôt have the skillset/understanding of application data).",
                  "score": 1,
                  "created_utc": "2026-01-02 10:57:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6j4ys",
          "author": "riv3rtrip",
          "text": "There isn't anything to talk about. Do you want me to post my SQL queries or something",
          "score": 21,
          "created_utc": "2026-01-02 01:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx78kge",
              "author": "ExOsc2",
              "text": "There's always\n\n    SELECT JEFF\n    FROM JEFF\n    WHERE JEFF\n    GROUP BY JEFF\n    HAVING JEFF\n    QUALIFY JEFF OVER (PARTITION BY JEFF ORDER BY JEFF) JEFF",
              "score": 19,
              "created_utc": "2026-01-02 04:34:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7w5bd",
                  "author": "TheOneWhoSendsLetter",
                  "text": "    ORDER BY JEFF DESC\n    LIMIT JEFF\n    OFFSET JEFF ROWS",
                  "score": 7,
                  "created_utc": "2026-01-02 07:41:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7qyiw",
                  "author": "WhipsAndMarkovChains",
                  "text": "MY NAME JEFF",
                  "score": 3,
                  "created_utc": "2026-01-02 06:53:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx90o7z",
              "author": "Thinker_Assignment",
              "text": "as a dev and vendor i think tool discussion is important because without it, we have no quality, just sales pitches. Of course, dev tool discussion, not what we do here bc here we don't discuss nuance, we just parrot commonly accepted points.\n\nBc devtools are not general b2c consumer goods, paid ads don't work (CR is tiny, ads are expensive) so it's either devtool vendors try to reach communities via devrels, or they stop trying and instead of building good tools they build a salesforce and sell shit to your manager which quickly becomes your problem.\n\nThen you can come on here and complain about how you're hating your job and developing no future-proof skills while asking what is SCD2 because you never developed the ability to help yourself and a question you could learn the answer to in 5min suddenly becomes your personality.\n\nBut the real problem is different: There's very little love for the craft out there. Producing useful content takes time and thinking that the vast majority will not invest unless they get something out of it - such as marketing.\n\nI would love to read interesting content but frankly it's mostly vendors and consultants that take the time to express.",
              "score": 3,
              "created_utc": "2026-01-02 13:30:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7xcj1",
          "author": "VEMODMASKINEN",
          "text": "Actual data engineering is building data platforms and tools. Like what Google did in the early 2000's with MapReduce and Yahoo with Hadoop.¬†\n\n\nNo one here does that that kind of software engineering focused on data.\n\n\nMost people here just do ETL. In other words they're what we used to call ETL Devs. OP included.¬†",
          "score": 9,
          "created_utc": "2026-01-02 07:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx643bw",
          "author": "NoleMercy05",
          "text": "Did you pass the Data Engineering PE exam?\n\nIt's just a made up Engineering title with no real definition.",
          "score": 12,
          "created_utc": "2026-01-02 00:25:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5vn83",
          "author": "wingman_anytime",
          "text": "At my company, ‚ÄúData engineering‚Äù has turned into a slop bucket of people who don‚Äôt have the rigor to engineer robust solutions, but had some data science background or some SQL or Python experience, so they throw them into our Data org. \n\nHaving been working on data warehouses and ETL pipelines since 2001, I just shake my head and keep on doing my job.",
          "score": 18,
          "created_utc": "2026-01-01 23:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9l79g",
              "author": "Busy-Ad1968",
              "text": "My colleagues once needed access to a computing cluster to calculate a result based on a table with several 10 million rows. Their code took 6 hours to calculate the result, and they said optimization wouldn't help. They requested a server with a GPU, wrote requests to a neighboring department, and went to meetings about it... well, you get the idea... In the end, after I optimized their code, the processing took less than a minute on a regular laptop. I was surprised that they tried to hush it up as quickly as possible and were very unhappy, because they had big plans for cooperation with the neighboring department, and it even benefited them that their code was running so slowly.",
              "score": 2,
              "created_utc": "2026-01-02 15:25:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9xf48",
                  "author": "Little_Kitty",
                  "text": "Similar experience here - the 'experts' wrote some python which bogged down a whole cluster for hours and couldn't handle more than about 100k entities, so I rewrote it to run in browser in javascript in about a minute with 1M+ and a frontend visualisation.\n\nData structures and big O are a thing, especially with graph like sparse data.",
                  "score": 2,
                  "created_utc": "2026-01-02 16:24:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6i9ro",
          "author": "StewieGriffin26",
          "text": "Just you wait, Databricks just recently rolled out.... stored procedures. Lol",
          "score": 6,
          "created_utc": "2026-01-02 01:49:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6uo6q",
          "author": "SRMPDX",
          "text": "Sure, go for it. I'll read it and maybe contribute. What's stopping you?",
          "score": 5,
          "created_utc": "2026-01-02 03:04:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5uhlu",
          "author": "shockjaw",
          "text": "You gotta point. There‚Äôs more to this discipline than just arguing over which company we should pay money to.",
          "score": 8,
          "created_utc": "2026-01-01 23:31:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx70gim",
          "author": "dubh31241",
          "text": "While we're on this topic. Can the majority of the current state of data engineering workflows be solved by running containerized sparks jobs in k8, storing data in some kind of columnar format and using a catalog system?\n\n I am DevOps/Infrastructure engineer coming into the data engineering space. I feel like Databricks, Snowflake and AWS's clusterfuck of services are doing relatively the same thing and charging a shit ton for this. I get the extended data governance part, but thats just rules and business logic. What am I missing?",
          "score": 3,
          "created_utc": "2026-01-02 03:41:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx783z3",
              "author": "PrestigiousAnt3766",
              "text": "Yeah, that basically describes databricks.¬†\n\n\nYou pay for convenience and not having to manage infra.",
              "score": 2,
              "created_utc": "2026-01-02 04:31:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6aogd",
          "author": "CallAnAmbulancee",
          "text": "i am new into data engineering field and i am just keep hearing influencers¬†and people in youtube just keep talking in the tools and this stuff, so how to truly learn data engineering and it's real concepts?",
          "score": 5,
          "created_utc": "2026-01-02 01:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6xa7r",
              "author": "pekingducksoup",
              "text": "I'm probably going to get some hate on this, but I find that asking LLM questions helps.¬†\nStart with high level i.e. \"please explain the fundamentals of data engineering\"\nAsk for references to books, articles, websites etc.¬†\nDrill down on each of the topics, drill down further for patterns etc.¬†\n\n\nI had a skim through the fundamentals of data engineering by Reis and Housley, that seems like a pretty good place to start.\n\n\nModelling is also pretty important for DE work, Kimbell is good for understanding concepts of data delivery. Even if you don't use star schema.\nInmon books are also pretty good from what I understand, I haven't read them though.\nThere are a others. Data Vault is pretty interesting as well, but I don't like implementing it personally.",
              "score": 10,
              "created_utc": "2026-01-02 03:21:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxb6v2a",
                  "author": "peterxsyd",
                  "text": "Sorry yes of course. But it is one part of the picture. The focus is then going to be on tools and techniques instead of the actual issue - the business problem being solved or the value that said data infrastructure is delivering (or not delivering). AI is an excellent learning tool - but the mentor part is what I meant doesn't come from a book or chatbot.",
                  "score": 1,
                  "created_utc": "2026-01-02 19:56:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx6hhuv",
              "author": "peterxsyd",
              "text": "Avoid listening to influencer shit shows go read a valid book on it and ideally work and get mentored by someone who knows what they are doing.",
              "score": 3,
              "created_utc": "2026-01-02 01:44:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx6u0ex",
              "author": "marketlurker",
              "text": "Check out the comment [here](https://www.reddit.com/r/dataengineering/comments/1q1iezn/comment/nx6m7nf/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).",
              "score": 2,
              "created_utc": "2026-01-02 03:00:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8jo89",
          "author": "mailed",
          "text": "Most of us are/were serving analytics and machine learning use cases. Analytics solutions have been created using high level abstractions built by software engineers for decades. It was GUI driven tools for a long time and now it's these SaaS environments like Databricks or Fabric or Snowflake. This won't ever change. It's why a majority of data engineers still can't or won't write code outside of SQL and don't need to.  \n  \nIf you want to do what you think is \"actual\" data engineering, go join a company as a software engineer on projects that handle tons of live data. A lot of transactional systems are handling more volume than your average analytics solution. It's what DDIA was written about (and why I don't recommend data engineers reading it a priority - because of the above paragraph).  \n  \nI personally have left the field for somewhat similar reasons (alongside having one too many data engineering teams tell me I was more of a dev/devops/infrastructure guy).",
          "score": 2,
          "created_utc": "2026-01-02 11:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5uhdm",
          "author": "Winter-Statement7322",
          "text": "‚ÄúThe vast majority of posts here are how do I use <fill in the blank> tool or compare <tool1> to <tool2>. If you are worried about how a given tool works, you aren't doing data engineering.‚Äù\n\nSome people are coming from other technologies and are legitimately interested in how a technology they‚Äôve previously used compares to one this are going to/will have to use",
          "score": 5,
          "created_utc": "2026-01-01 23:31:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6tvcw",
              "author": "marketlurker",
              "text": "That is a fair use case, but it has little to do with data engineering. Product comparison is just one small area in this discipline. So many other things can have a much larger impact on your system.",
              "score": -1,
              "created_utc": "2026-01-02 02:59:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7dsm5",
          "author": "ithinkiboughtadingo",
          "text": "No we're just going to keep asking how to switch from [insert whatever career path here] into DE over and over until the heat death of the universe",
          "score": 1,
          "created_utc": "2026-01-02 05:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx88ru4",
          "author": "one-step-back-04",
          "text": "Hard agree on the spirit of this. Even in reality bhaaai, most real data engineering work I see has very little to do with debating tools and a lot to do with **constraints**. Bad source data, unclear ownership, changing business logic, SLAs no one documented, and pipelines that need to survive people, not just demos.\n\nTools come and go. For me, the actual work is deciding what deserves to be modeled, what can be wrong, what must never be wrong, and how fast the business needs to know. That‚Äôs the part that rarely gets discussed.\n\nAlso yes to the medallion point. Renaming long-standing patterns isn‚Äôt innovation, it‚Äôs branding. Useful shorthand maybe, but it becomes a problem when people think the diagram is the architecture.\n\nWould love to see more posts on failure modes, trade-offs, and ‚Äúthis looked clean on paper but blew up in prod.‚Äù That‚Äôs where the engineering actually lives to me.",
          "score": 1,
          "created_utc": "2026-01-02 09:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd3qm2",
          "author": "KrisPWales",
          "text": "By \"do actual data engineering\", do you mean how it was done twenty years ago? There are a lot of low effort posts, but data engineering has evolved whether you like kenit or not, or even consider it engineering at all. Just look at all the job postings. It's all a bit of python, SQL, cloud X and tools a, b and c. Of course that's what this forum was going to become. There were probably complaints when SSIS questions starting appearing on forums back in the day.",
          "score": 1,
          "created_utc": "2026-01-03 01:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdf2yy",
              "author": "marketlurker",
              "text": "Data engineering is an extremely mature practice. It doesn't change very much and hasn't in over 20 years. I haven't seen much that would be considered innovative in a very long time.",
              "score": 1,
              "created_utc": "2026-01-03 03:04:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxf0x1g",
                  "author": "KrisPWales",
                  "text": "Then what do you want this sub to be about? The fundamentals that haven't changed in decades?",
                  "score": 1,
                  "created_utc": "2026-01-03 10:28:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx62h3n",
          "author": "Silent_Calendar_4796",
          "text": "Data engineering is dead, AI replaced it",
          "score": -19,
          "created_utc": "2026-01-02 00:16:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx66wx3",
              "author": "Lucade2210",
              "text": "You are the problem. And also apparently not a data engineer.",
              "score": 12,
              "created_utc": "2026-01-02 00:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx83vux",
                  "author": "Silent_Calendar_4796",
                  "text": "Coping hard I see",
                  "score": -7,
                  "created_utc": "2026-01-02 08:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q48kzj",
      "title": "Small Group of Data Engineering Learners",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q48kzj/small_group_of_data_engineering_learners/",
      "author": "lil_faucet",
      "created_utc": "2026-01-05 01:52:37",
      "score": 164,
      "num_comments": 104,
      "upvote_ratio": 0.96,
      "text": "**Hey everyone!**\n\nI realized I could really use more DE coworkers / people to nerd out with. I‚Äôd love to start a casual weekly call where we can talk data engineering, swap stories, and learn from each other.\n\nOver time, if there‚Äôs interest, this could turn into things like a textbook or whitepaper club, light presentations, or deeper dives into topics people care about. Totally flexible.\n\n**What you‚Äôd get out of it:**\n\n* Hearing how other people think about DE problems\n* Learning stuff that doesn‚Äôt always come up in day-to-day work\n* Getting exposure to different career paths and ways of working\n* Practical ideas you can actually use\n\n**Some topics I‚Äôm especially interested in:**\n\n* Performance and scaling\n* Systems thinking\n* Data platforms and infrastructure\n* FinOps / cost awareness\n* Reliability, observability, and ops\n* Architecture tradeoffs (build vs buy, etc.)\n* How data stacks evolve as companies grow\n\nThis is mainly for early-to-mid career folks, but anyone curious is welcome. If this sounds interesting, reach out and we‚Äôll see what happens.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q48kzj/small_group_of_data_engineering_learners/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxqnjau",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-05 01:52:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr0lyf",
          "author": "AnalyticsDepot--CEO",
          "text": "And if you need jobs, holla",
          "score": 21,
          "created_utc": "2026-01-05 03:02:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrpljm",
              "author": "Can-I-leave-Please",
              "text": "Hello, can I find some help?\nI've been looking for a while now and can't seem to find anything entry level",
              "score": -2,
              "created_utc": "2026-01-05 05:32:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvupz6",
                  "author": "Reach_Reclaimer",
                  "text": "Join as a data analyst, transition to data scientist/engineer",
                  "score": 4,
                  "created_utc": "2026-01-05 20:49:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxqnm2m",
          "author": "lil_faucet",
          "text": "[https://discord.gg/vPwCuvZsM8](https://discord.gg/vPwCuvZsM8)",
          "score": 13,
          "created_utc": "2026-01-05 01:53:02",
          "is_submitter": true,
          "replies": [
            {
              "id": "nxs1drd",
              "author": "uncertainschrodinger",
              "text": "I'd love to join as well but I live in Turkey and discord is banned here - is there another way to join?",
              "score": 3,
              "created_utc": "2026-01-05 07:06:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxsh1nx",
                  "author": "HellaSwellaFella",
                  "text": "Use a vpn",
                  "score": 5,
                  "created_utc": "2026-01-05 09:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvyd99",
              "author": "MathmoKiwi",
              "text": "Joined :)",
              "score": 1,
              "created_utc": "2026-01-05 21:06:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxqqqtn",
          "author": "Rough_Act_1005",
          "text": "Sounds Interesting to discuss",
          "score": 5,
          "created_utc": "2026-01-05 02:09:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqxcx6",
          "author": "Luffy_Uzumaki09",
          "text": "Not Data Engineer actually, but aspiring to be, would like to join as well.",
          "score": 6,
          "created_utc": "2026-01-05 02:44:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqsd34",
          "author": "Distinct-deel",
          "text": "I would like to join",
          "score": 3,
          "created_utc": "2026-01-05 02:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrr06t",
              "author": "Accomplished-Ear1126",
              "text": "+1",
              "score": 2,
              "created_utc": "2026-01-05 05:42:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxtplbj",
          "author": "crossmirage",
          "text": "Have you considered joining an existing community? You can search for Practical Data Community Discord, which is run by Joe Reis.\n\n\nIf you have your reasons for starting a new group, that's fair, but otherwise plugging into an existing, active community might be an easier way to get what you're looking for.",
          "score": 7,
          "created_utc": "2026-01-05 14:50:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs0ua3",
          "author": "Automatic-Ad5179",
          "text": "I‚Äôm interested",
          "score": 2,
          "created_utc": "2026-01-05 07:01:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqyffy",
          "author": "Moist_Sandwich_7802",
          "text": "I would be happy to join, i bring pyspark, databricks and some palantir knowledge to the the table.",
          "score": 4,
          "created_utc": "2026-01-05 02:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqt2jt",
          "author": "Jo_Swayze",
          "text": "I‚Äôm interested.",
          "score": 1,
          "created_utc": "2026-01-05 02:21:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqt3d1",
          "author": "rtripat",
          "text": "Count me in!",
          "score": 1,
          "created_utc": "2026-01-05 02:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqui57",
          "author": "69odysseus",
          "text": "I thought it was posted earlier, was it deleted? I'm data modeler, count me in.",
          "score": 1,
          "created_utc": "2026-01-05 02:29:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqulb6",
          "author": "baddhambhaskar1",
          "text": "Hey! I would like to join",
          "score": 1,
          "created_utc": "2026-01-05 02:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqulxn",
          "author": "droptablesnotbombs",
          "text": "I‚Äôd be down",
          "score": 1,
          "created_utc": "2026-01-05 02:30:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxquvvj",
          "author": "SubstantialNotice542",
          "text": "Me too",
          "score": 1,
          "created_utc": "2026-01-05 02:31:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqvj3u",
          "author": "AgitatedClub883",
          "text": "Sure I‚Äôll bite. Color me curious! I‚Äôm down",
          "score": 1,
          "created_utc": "2026-01-05 02:35:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqwwuk",
          "author": "ImmediateGuarantee27",
          "text": "I would like to be a part of this!",
          "score": 1,
          "created_utc": "2026-01-05 02:42:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqxhva",
          "author": "Lost-in-lyf",
          "text": "Interested, please add me",
          "score": 1,
          "created_utc": "2026-01-05 02:45:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqy880",
          "author": "RaceElectronic4525",
          "text": "Would like to join",
          "score": 1,
          "created_utc": "2026-01-05 02:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqyhzy",
          "author": "ComfortableHabit5",
          "text": "would like to join",
          "score": 1,
          "created_utc": "2026-01-05 02:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqysy6",
          "author": "fmoralesh",
          "text": "Include me!",
          "score": 1,
          "created_utc": "2026-01-05 02:52:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqyyf2",
          "author": "mudkip_thiss",
          "text": "I‚Äôm interested!",
          "score": 1,
          "created_utc": "2026-01-05 02:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqz3w6",
          "author": "Big_Pearr",
          "text": "I‚Äôm interested!",
          "score": 1,
          "created_utc": "2026-01-05 02:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqzuon",
          "author": "Ashish25sahu",
          "text": "I would Join",
          "score": 1,
          "created_utc": "2026-01-05 02:58:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqzv9c",
          "author": "professional_junkie",
          "text": "I‚Äôm down",
          "score": 1,
          "created_utc": "2026-01-05 02:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr066e",
          "author": "papuceb",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 02:59:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr0dek",
          "author": "Informal-Tip-1109",
          "text": "Count me in",
          "score": 1,
          "created_utc": "2026-01-05 03:01:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr0e80",
          "author": "kimutin",
          "text": "Would like to join. Dm me",
          "score": 1,
          "created_utc": "2026-01-05 03:01:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr0m0g",
          "author": "Sea_Application7426",
          "text": "I‚Äôm interested as well",
          "score": 1,
          "created_utc": "2026-01-05 03:02:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr0m8o",
          "author": "luxizer",
          "text": "I Would like to join!",
          "score": 1,
          "created_utc": "2026-01-05 03:02:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr0t37",
          "author": "bigknocker12",
          "text": "Yaü´°",
          "score": 1,
          "created_utc": "2026-01-05 03:03:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr0z9x",
          "author": "kimutin",
          "text": "Add me",
          "score": 1,
          "created_utc": "2026-01-05 03:04:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr2pzd",
          "author": "Broad_Trash_2758",
          "text": "Happy to join as well :)",
          "score": 1,
          "created_utc": "2026-01-05 03:14:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr2xm3",
          "author": "subsetdht",
          "text": "I'm late career, early data eng. Would love to join in!",
          "score": 1,
          "created_utc": "2026-01-05 03:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr35jx",
          "author": "Escushiboy",
          "text": "Would love to join",
          "score": 1,
          "created_utc": "2026-01-05 03:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr3ykt",
          "author": "zacpar546",
          "text": "interested",
          "score": 1,
          "created_utc": "2026-01-05 03:21:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr513e",
          "author": "Last_Commission_6718",
          "text": "I‚Äôm in",
          "score": 1,
          "created_utc": "2026-01-05 03:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr55v9",
          "author": "SwedishFishSticks",
          "text": "I‚Äôd be interested as well",
          "score": 1,
          "created_utc": "2026-01-05 03:27:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr5puy",
          "author": "randomName77777777",
          "text": "I'm interested",
          "score": 1,
          "created_utc": "2026-01-05 03:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr67zz",
          "author": "halpmeowtbruv",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 03:33:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr6ivo",
          "author": "perdus17",
          "text": "I would like to join",
          "score": 1,
          "created_utc": "2026-01-05 03:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr6lie",
          "author": "perdus17",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 03:35:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr6wr4",
          "author": "Itzombielikethat",
          "text": "Im in!",
          "score": 1,
          "created_utc": "2026-01-05 03:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr7d50",
          "author": "chicalette",
          "text": "following this post! super interested",
          "score": 1,
          "created_utc": "2026-01-05 03:40:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr7klb",
          "author": "TechieTrekker",
          "text": "Count me in!",
          "score": 1,
          "created_utc": "2026-01-05 03:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr873x",
          "author": "Responsible_Mix_6139",
          "text": "I'm interested",
          "score": 1,
          "created_utc": "2026-01-05 03:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr89k7",
          "author": "peelemme",
          "text": "I am interested!\n\npyspark, palantir",
          "score": 1,
          "created_utc": "2026-01-05 03:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr97ip",
          "author": "Historical-Chart-405",
          "text": "Let‚Äôs do this!",
          "score": 1,
          "created_utc": "2026-01-05 03:50:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr9ekq",
          "author": "Spiritual_Gangsta22",
          "text": "Count me in too!",
          "score": 1,
          "created_utc": "2026-01-05 03:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr9pqo",
          "author": "caffeinatedSoul89",
          "text": "I‚Äôd be down",
          "score": 1,
          "created_utc": "2026-01-05 03:53:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrchwu",
          "author": "Early_Prize_5001",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 04:10:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrcmgr",
          "author": "tibialispain",
          "text": "Interested!",
          "score": 1,
          "created_utc": "2026-01-05 04:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrcx2s",
          "author": "Snoo-53366",
          "text": "Hi. I'd be interested in joining.  I was awarded a data analytics coding fellowship (I'm pivoting over from IT) and currently learning SQL and Python trying to get a head start before the program begins. \n\nThis sounds like a really good opportunity for learning how data engineers work on problems in the real world.",
          "score": 1,
          "created_utc": "2026-01-05 04:12:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrferm",
          "author": "Afraid-Sound5502",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 04:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrl9b6",
          "author": "Samarth978",
          "text": "I am in",
          "score": 1,
          "created_utc": "2026-01-05 05:02:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrm2u7",
          "author": "Rare_Decision276",
          "text": "I‚Äôm in bro",
          "score": 1,
          "created_utc": "2026-01-05 05:07:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxroy26",
          "author": "First_Adeptness_2461",
          "text": "I‚Äôm interested",
          "score": 1,
          "created_utc": "2026-01-05 05:27:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrozdq",
          "author": "[deleted]",
          "text": "I‚Äôm interested",
          "score": 1,
          "created_utc": "2026-01-05 05:27:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrsip5",
          "author": "real_madrid_100",
          "text": "I am in.",
          "score": 1,
          "created_utc": "2026-01-05 05:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrts4q",
          "author": "draezo",
          "text": "Here for it",
          "score": 1,
          "created_utc": "2026-01-05 06:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxru9wv",
          "author": "im_a_lost_child",
          "text": "i‚Äôm also interested! not in the field yet but i have some friends and im interested!",
          "score": 1,
          "created_utc": "2026-01-05 06:07:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrwgnv",
          "author": "Alternative_Day155",
          "text": "Please DM me. I am doing DE course and intrested in projects",
          "score": 1,
          "created_utc": "2026-01-05 06:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs3qms",
          "author": "ajikeyo",
          "text": "I‚Äôm interested",
          "score": 1,
          "created_utc": "2026-01-05 07:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs3yht",
          "author": "Fearless-You4021",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 07:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs41yo",
          "author": "Powerful-Year9570",
          "text": "Also interested!",
          "score": 1,
          "created_utc": "2026-01-05 07:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs6xml",
          "author": "ravi1912",
          "text": "I'm intersted.",
          "score": 1,
          "created_utc": "2026-01-05 07:56:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsapr0",
          "author": "Glad_Appearance_8190",
          "text": "hey this sounds super cool.., ive joined a few casual data calls before and honestly just hearing how other people approach problems opens your brain in weird ways even small things like noticing patterns in logs or how teams handle edge cases sticks with you also talking about reliability observability and ops is huge because thats where most scripts silently fail imo would def join a weekly call like this if i could...",
          "score": 1,
          "created_utc": "2026-01-05 08:31:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsbdl4",
          "author": "unhampered_by_pants",
          "text": "I'm down",
          "score": 1,
          "created_utc": "2026-01-05 08:38:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsep3t",
          "author": "Geeky-Info",
          "text": "Count me in",
          "score": 1,
          "created_utc": "2026-01-05 09:09:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsh79j",
          "author": "Dangerous-Bat8238",
          "text": "Would like too",
          "score": 1,
          "created_utc": "2026-01-05 09:33:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsibm3",
          "author": "Adept_Bridge_8811",
          "text": "Interested, trying to switch towards data engineering",
          "score": 1,
          "created_utc": "2026-01-05 09:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsm1rm",
          "author": "kw2006",
          "text": "Trying to pick up dbt and still deciding between dagster or airflow. Would like to join. My career is not a data engineer though.",
          "score": 1,
          "created_utc": "2026-01-05 10:18:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsr5dc",
          "author": "Psychological_Fix864",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-01-05 11:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsuppg",
          "author": "sarthak009",
          "text": "Interested.",
          "score": 1,
          "created_utc": "2026-01-05 11:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsyw1q",
          "author": "Akash_Rajvanshi",
          "text": "Interested!",
          "score": 1,
          "created_utc": "2026-01-05 12:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxt8yv7",
          "author": "RandomAccount0799",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 13:16:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtklkh",
          "author": "ptyws",
          "text": "I tried to create a book/article club in the subreddit's discord but very few people were interested.\nMy suggestion was voting for an article and we'd get together every 2 or 4 weeks to discuss it.",
          "score": 1,
          "created_utc": "2026-01-05 14:24:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtlkkm",
          "author": "Nomadfocus",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-05 14:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtm06x",
          "author": "Dazzling-Cry-4688",
          "text": "I'm down",
          "score": 1,
          "created_utc": "2026-01-05 14:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxttw9z",
          "author": "expialadocious2010",
          "text": "Would like to join. Thanks",
          "score": 1,
          "created_utc": "2026-01-05 15:13:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuezxw",
          "author": "billytimmy123",
          "text": "I‚Äôm in. In EST time zone",
          "score": 1,
          "created_utc": "2026-01-05 16:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxugher",
          "author": "Lynxkub",
          "text": "I‚Äôm interested in joining, thanks!",
          "score": 1,
          "created_utc": "2026-01-05 16:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxumxft",
          "author": "Firefox1950",
          "text": "Would join, current DE in government.",
          "score": 1,
          "created_utc": "2026-01-05 17:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvwozb",
          "author": "HanDw",
          "text": "I'm interested.",
          "score": 1,
          "created_utc": "2026-01-05 20:58:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwpzxj",
          "author": "KantLiquor",
          "text": "I am a hiring executive. Happy to share my view.",
          "score": 1,
          "created_utc": "2026-01-05 23:19:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxcx3a",
          "author": "Witty-Ninja-8403",
          "text": "count me in",
          "score": 1,
          "created_utc": "2026-01-06 01:20:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxm7jk",
          "author": "Fancy_Cheesecake_533",
          "text": "I‚Äôm interested!",
          "score": 1,
          "created_utc": "2026-01-06 02:10:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxym3dl",
          "author": "Lazy_Valuable7799",
          "text": "I might be leaving my current ORG and they will look for a replacement. Its a major credit card payment  firm and the pay is good. DM me with the resume + work auth in the US. Position will be mostly based in SF.",
          "score": 1,
          "created_utc": "2026-01-06 05:52:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzd0gl",
          "author": "geoheil",
          "text": "also - always interested",
          "score": 1,
          "created_utc": "2026-01-06 09:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzpj8s",
          "author": "Aggravating_Debt231",
          "text": "Awesome thought , joined",
          "score": 1,
          "created_utc": "2026-01-06 11:44:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny50g9r",
          "author": "Fresh-Paramedic-5599",
          "text": "Joined",
          "score": 1,
          "created_utc": "2026-01-07 03:51:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5n596",
          "author": "rahuldaida",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-07 06:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny60qpb",
          "author": "Key_Toe_43",
          "text": "+1 interested in joining",
          "score": 1,
          "created_utc": "2026-01-07 08:28:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1174f",
      "title": "The Data warehouse blues by Inmon, do you think he's right about Databricks & Snowflake?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q1174f/the_data_warehouse_blues_by_inmon_do_you_think/",
      "author": "Tall_Working_2146",
      "created_utc": "2026-01-01 10:26:23",
      "score": 110,
      "num_comments": 151,
      "upvote_ratio": 0.98,
      "text": "Bill Inmon posted on substack saying that Data-warehousing got lost in the modern data technology.\n\nIn a way that companies are now mistakenly confusing storage for centralization and ingestion for integration. Although I agree with the spirit of his text, he does take a swing at Databrick&Snowflake, as a student I didn't have the chance to experiment with these plateforms yet so I want to know what experts here think.\n\nLink to the post : [https://www.linkedin.com/pulse/data-warehouse-blues-bill-inmon-sokkc/](https://www.linkedin.com/pulse/data-warehouse-blues-bill-inmon-sokkc/)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q1174f/the_data_warehouse_blues_by_inmon_do_you_think/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx2hiqn",
          "author": "Peanut_-_Power",
          "text": "I think Bill is right about the symptoms; however, I think he misidentifies the cause.\n\nModern data architecture has shifted; not because data warehousing is dead, but because it is no longer the only thing organisations need. Today‚Äôs platforms have to support analytics, data science, and increasingly AI workloads alongside traditional BI.\n\nThe criticism of Databricks and Snowflake feels a little unfair; they are not trying to replace data warehousing fundamentals, they are trying to support multiple workloads. Both platforms can absolutely deliver a well-designed data warehouse if the right discipline is applied.\n\nIn my experience, the real issue is people rather than platforms; there is a strong tendency to chase modern tools and certifications while neglecting core concepts such as data modelling and integration. I regularly see engineers openly say they have no interest in modelling, which I would argue is foundational to being effective in this space.\n\nSo I agree with the spirit of the post; that we have lost sight of fundamentals. I do not think modern platforms are the culprit; they simply expose gaps in skills and architectural thinking that were always there.",
          "score": 145,
          "created_utc": "2026-01-01 12:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2vjgz",
              "author": "Nekobul",
              "text": "Up until recently both of these platforms didn't have any integration tools. They relied on third-parties like Fivetran and dbt to get the data somewhat into shape. But at a huge cost because the columnar databases are not exactly designed for integration work. And then they slapped the keyword \"modern\" to mask the issues and make everyone assume all previous solutions are outdated or not relevant. I believe that is the main critique by Mr.Inmon and I totally agree with him. People were sold a lie and both of these companies willingly participated in that lie.",
              "score": 23,
              "created_utc": "2026-01-01 13:58:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx32kar",
                  "author": "EarthGoddessDude",
                  "text": "Omg. You wrote an entire paragraph without mentioning SSIS once. A new year indeed! üéâ",
                  "score": 25,
                  "created_utc": "2026-01-01 14:45:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx44uax",
                  "author": "TheThoccnessMonster",
                  "text": "You wrote that entire paragraph without acknowledging that the people that build Databricks built Spark and that very much IS the new modern. \n\nWe migrated  latency sensitive workload off Hadoop/MapR that treated the warehouse cluster as basically an API, wholesale, over to S3 + SQL Warehouses. Something originally we thought would not be possible. I admit that much of the fundamentals of the old ways don‚Äôt have perfect facsimiles in the new world but doing Spark on the old cluster just for batch jobs vs.  now using it for everything (technically) is possible.",
                  "score": 7,
                  "created_utc": "2026-01-01 18:13:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5ry3s",
                  "author": "GreyHairedDWGuy",
                  "text": "I don't think they were sold a lie per se.  I agree with what you say about these vendors leaving integration to other tools/vendors but customers need to be smart enough to understand this and read between the lines and fill in the gaps. \n\nSo many people want a simple 'one size fits all' solution and want it so bad that they overlook needing to think for themselves.",
                  "score": 3,
                  "created_utc": "2026-01-01 23:17:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx46b4w",
                  "author": "BarfingOnMyFace",
                  "text": "Same",
                  "score": 0,
                  "created_utc": "2026-01-01 18:20:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx5rb6b",
              "author": "GreyHairedDWGuy",
              "text": "I also thought it was a bit misplaced to call out Databricks and Snowflake in such a way.  They are merely specialized infrastructure platforms that support the goals of enterprise analytics (and data warehousing...however someone wants to define it).  The main issue is 'people' as you say. This is partly (maybe mostly) caused by consulting companies selling a dream and quick wins when nothing about integrating organizational data is not generally a simple task.  I've seen it time an time again.   There is also a part of this that is due to education.  Many current practitioners also want quick wins, have not really done this type of work before and end up learning the wrong lessons.  I'm a decade younger than Bill and Ralph but have been doing this sort of work for 25+ years.  I see many consultants sell this this a a technology project when it is only partly this.\n\nI think a part of this started with 'schema on read', 'agile (get er done quickly and fail fast) and the era around early 2000's when companies just wanted dashboards (the tip of the iceberg) and didn't care about how that happened.\n\napologies...I'm ranting now :)",
              "score": 4,
              "created_utc": "2026-01-01 23:13:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2mud6",
              "author": "Tall_Working_2146",
              "text": "From what I read online, it feels that the core difference between Databricks and Snowflake is that Databricks is more of multiple Workloads Plateform and Snowflake is kind of closer to Datawarehousing (ACID transactions etc)",
              "score": 1,
              "created_utc": "2026-01-01 12:51:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2owmo",
                  "author": "NW1969",
                  "text": "Possibly true about Snowflake a few years ago, not the case now",
                  "score": 9,
                  "created_utc": "2026-01-01 13:08:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2oa4i",
          "author": "Kardinals",
          "text": "I think the core issue here is not Databricks or Snowflake. I mostly agree with the other commentator who said the cause is misidentified. In data analytics and engineering the real problems are and always have been people and processes. Technology only enables them. What actually got us here is weak data management and governance.\n\nThat said, I also partly agree with the criticism. Modern data stack tooling has reached a point where many organizations think that throwing Databricks or Snowflake at every data problem and calling it done is enough. This is especially true in organizations where IT (which should not be the owner of data in the first place) is expected to ‚Äúfix‚Äù data issues. They look for technological solutions to what are, in reality, problems of processes, roles, ownership, and culture. In that sense, the frustration expressed by Inmon is very much understandable.\n\nSo the concept of the data warehouse did not fail. Technology simply made it easier to avoid the hard work. Vendors and consultants sold a convenient illusion. Integration and enterprise data are not a tooling problem, they are a design choice and an organizational commitment, and a governance problem. Expecting a platform to solve that is exactly the mistake that keeps repeating itself. Blaming modern platforms for the decline of data warehousing feels like blaming a database for poor data modeling. The real issue is that many organizations never built enterprise data capabilities in the first place.",
          "score": 41,
          "created_utc": "2026-01-01 13:03:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx46qzw",
              "author": "slowboater",
              "text": "You go from disagreeing (on what u would argue are semantics of sorts) in your first paragraph to totally agreeing in the last. Its a fine distinction, but i think from a big picture tells the same story. Its not just snowflake and databricks, all of the big 3 cloud providers are guilty too. The core problem/cause that links both of your nuanced agree/disagreee statements (and which you and bill acknowledge) is that these service providers willingly sell the lie that proper storage forms are not needed. Which is a mistruth at best and straight up deception at worst. Ive seen this happen several times, often because the people being sold/(lied) to are high level csuites with not experience in data modeling and as that sign off tumbles down the ladder til it gets in front of a skilled DE, no one above them believes (or has the necessary understanding/capacity to) that these older models are necessary anymore (and then gets frustrated when their apps arent fast enough and think the DE is judt picking a bone/lazy about their implementation blaming it on this foreign/alien concept of a proper warehouse structure, that they must so clearly have because its just a digital object where all their disorganized crap goes, and thats what the provider's solution architect said). Idk if some of these execs are really that naive that a 'solutions architect' is just a salesman, theyre slightly aware/embarassed enough of their lack of understanding of the concept to feel they cant admit it and play along in the room, or if they really just get caught up in the sway of wishful thinking (i.e. AI will solve all!!!). I tend to lean towards the last case here and as someone commented on the linkedin post (not on *substack* yet?) It feels a function/product of the quarterly/yearly reporting structure of corporate america.  I sure as hell know my last company did NOT like my diagnosis that 15 years of unstructured (worse, fragmented unstructure) raw data with no underlying, bought in, agreed upon process documentation would take 2 years minimum for me as a one man team magician to set up a modern architecture that could merge with SAP and make something clean enough to run near-live visualizations from. When i came in they had excel sheets everywhere with fragile AF VBA scripts linking across a whole plant to a shared drive... was literally told by one the the heads of IT to just use fabric to directly ingest these sheets like itd be that easy... On a level its almost a tradgedy of the commons in regards to our collective understanding of these concepts battling with our collective understanding to \"look\" busy via measured outputs at an \"acceptable\" pace from the glass office in the sky and therefore no EDW is getting the time or attention it needs for the past 5 years.",
              "score": 7,
              "created_utc": "2026-01-01 18:22:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx91yql",
                  "author": "Nekobul",
                  "text": "Thank you for the thoughtful comment! You have described the state of the data warehousing market very well.",
                  "score": 2,
                  "created_utc": "2026-01-02 13:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3g1ei",
              "author": "KWillets",
              "text": "But Snowflake will buy me lunch if I do that.",
              "score": 3,
              "created_utc": "2026-01-01 16:04:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4707c",
                  "author": "slowboater",
                  "text": "ü§£ no so legitimately this tho... did discover there were kickback arrangements for the highest IT directors at my last place...",
                  "score": 3,
                  "created_utc": "2026-01-01 18:23:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx45feq",
              "author": "TheThoccnessMonster",
              "text": "Right fucking here, the actual answer.",
              "score": 3,
              "created_utc": "2026-01-01 18:15:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx5sf68",
              "author": "GreyHairedDWGuy",
              "text": "right on the money",
              "score": 2,
              "created_utc": "2026-01-01 23:19:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2w9ii",
              "author": "Nekobul",
              "text": "Integration is a tooling problem. Both platforms didn't provide basic capabilities, relying on third-party vendors and custom code to get the job done in mostly hacky, non-systematic and non-reusable way.",
              "score": -1,
              "created_utc": "2026-01-01 14:03:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2imrv",
          "author": "JonPX",
          "text": "The issue has always been the same, companies can't make modeling easy so they just sell it as not necessary¬†",
          "score": 24,
          "created_utc": "2026-01-01 12:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2kkkt",
              "author": "PrestigiousAnt3766",
              "text": "Its not easy. Its an undervalued skill.",
              "score": 21,
              "created_utc": "2026-01-01 12:31:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx421vt",
                  "author": "jamjam125",
                  "text": ">It‚Äôs not easy.It‚Äôss an undervalued skill.\n>\nGenuinely curious why is data modeling an undervalued skill?",
                  "score": 1,
                  "created_utc": "2026-01-01 17:59:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2vpcr",
              "author": "Nekobul",
              "text": "Modeling is the actual work and it cannot be automated with finger snapping.",
              "score": 8,
              "created_utc": "2026-01-01 13:59:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3x0za",
                  "author": "One-Employment3759",
                  "text": "Modelling also can't be solved by subscribing to some prescriptive modelling technique.\n\n\nWhich is why I think Inmon and Kimball are fluff and it's ultimately about understanding the business needs and the data domain.",
                  "score": 6,
                  "created_utc": "2026-01-01 17:33:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3fysd",
              "author": "MyNameDebbie",
              "text": "Getting into this space some as an experienced dev. Traditionally when I thought of data warehousing data modeling(star schemas) was necessary as massive compute wasn‚Äôt ubiquitous. Now you can throw tons of compute at it (more $$ for databricks/snowflake). \n\nMy question is with modern object store what data modeling is useful? Any resources available for this?",
              "score": -2,
              "created_utc": "2026-01-01 16:03:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3hlts",
                  "author": "JonPX",
                  "text": "Imagine if you went to IKEA, you wanted to buy something, and they gave you 50 locations of where to find every individual item you need to build the thing you want? If the warehouse picker is fast enough, that is just as good as everything in a prepackaged packet with all the items you need?\n\nData modeling is useful because it brings together all linked data in a format that is perfect for its intended usage. If that is dimensional or relational or DataVault or a mix of them, that doesn't matter.\n\nOr as a development equivalent, you don't write everything in a single routine that does everything, right? You make different classes, sub-routines etc.",
                  "score": 6,
                  "created_utc": "2026-01-01 16:12:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx31xe1",
          "author": "Firm-Yogurtcloset528",
          "text": "In my experience I see big companies having dedicated teams doing data governance and even data modeling that are totally separated from the business teams and not knowledgeable about data warehousing, who are suppose to take control of the whole modern lake house concept with enormous amount of money spend and the ones owning budgets clueless what is money well spend or not. They get handed awards from Data ricks and Snowflake for being amazingly innovative  thus buying into middle management who passes on the BS to upper management like it is the best thing since sliced bread.",
          "score": 5,
          "created_utc": "2026-01-01 14:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2g53w",
          "author": "West_Good_5961",
          "text": "![gif](giphy|fqtyYcXoDV0X6ss8Mf)",
          "score": 21,
          "created_utc": "2026-01-01 11:49:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2kivs",
              "author": "ProfessorNoPuede",
              "text": "That's the right meme on several levels. Good start of the Year.",
              "score": 4,
              "created_utc": "2026-01-01 12:31:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2mvzy",
              "author": "Tall_Working_2146",
              "text": "![gif](giphy|IPG1x2rrFiWB527EVg)",
              "score": 1,
              "created_utc": "2026-01-01 12:52:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2vuut",
          "author": "Yonko74",
          "text": "I think he‚Äôs bang on the money. Not necessarily with singling out Databricks and Snowflake, but the general principle is spot on.\n\nWe‚Äôre not talking about the relatively few large-scale organisations here, but the bulk of small to mid tier companies who suddenly discovered they desperately needed ‚Äòdata‚Äô solutions, and were sold the dream by snake oil salesmen.\n\nFor too long these organisations were happy to  see ‚Äòdata‚Äô as an isolated function that they could chuck cheap engineering labour on top of a plethora of ever-changing tech stacks (that all do the same fundamental thing)\n\nNow though the chickens are coming home to roost and the AI boom is flagging how such actions create inconsistency, miss governance, wrecks quality and builds layer upon layer of technical debt.\n\nThe sooner we get back to viewing data as an asset rather than a product, the better imo.",
          "score": 3,
          "created_utc": "2026-01-01 14:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2pq4v",
          "author": "totalsports1",
          "text": "It is a fact that most companies don't follow any sort of warehousing principles. But the fact is whatever that is being done across orgs gets stuff done. Reporting/BI is a cost centre in most companies, ultimately they're measured by how well they serve business. But this haphazard approach is a problem when the going gets tough. Suddenly everyone is worried about cost and eyes fall on the BI team with so many data analysts. No org is going to prioritize cost/efficiency over time to market while building the team from grounds up.",
          "score": 3,
          "created_utc": "2026-01-01 13:15:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2x4l7",
              "author": "Nekobul",
              "text": "They are not following principles because most organizations were sold the myth these platforms will do the job for them at very low cost. It was a rush to the bottom and now comes the time to pay the price.",
              "score": 1,
              "created_utc": "2026-01-01 14:09:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4kx28",
          "author": "blobbleblab",
          "text": "What's he smoking, both platforms can and do (I have built them) deliver data warehouses if that's what's needed. It's all about what the customer needs at the other end. If they don't need a data warehouse though, you don't make one as it's a lot of design effort.",
          "score": 3,
          "created_utc": "2026-01-01 19:32:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6i386",
          "author": "peterxsyd",
          "text": "It is 100% true. I have seen companies ingest data and have an integration per end user request or report, as opposed to a data warehouse actually modelling the business from the multiple upstream source systems. That is the whole point of data engineering.",
          "score": 3,
          "created_utc": "2026-01-02 01:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx939yf",
              "author": "Nekobul",
              "text": "Absolutely correct.",
              "score": 2,
              "created_utc": "2026-01-02 13:46:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx78ilg",
          "author": "kthejoker",
          "text": "Old man yells at cloud (data platforms)\n\nIn what world does Snowflake or Databricks *not* want to be the core integration point for all systems upstream and downstream?\n\nBeing the sticky compute in the middle of all of your source systems and your valuable use cases is literally where all the money is.\n\nThis is literally just ranting about something he wishes were true but is not - that it's the technology's fault.\n\nWhen the actual issue always has been people who buy the technology hoping it lets them avoid the hard work of translating their business processes into data models, insights, and solutions ...\n\nDiscovering it does not ..\n\nand then blaming the technology for not eliminating the hard work (oh well! On to the next technology....)\n\nDisclaimer: I work at Databricks",
          "score": 3,
          "created_utc": "2026-01-02 04:34:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx94kgs",
              "author": "Nekobul",
              "text": "\\* Your company doesn't have any unique technology that is not available to other major players in the market. Your crown-jewel Photon native execution Spark engine is now replicated by both Microsoft and Snowflake. Soon, there will be completely free OSS replacement on the market, too.  \n\\* Your company promoted the medallion methodology which is ridiculous.  \n\\* Your company didn't provide proper ETL tooling for many years, pushing the ELT as the quick but terrible workaround.  \n\\* Your company doesn't provide Databricks for installation and running on-premises. Therefore, all the testing and development and optimization work has to be paid by the minute in the cloud.  \n\\* Your distributed platform is not needed by the vast majority of the market. Most data work can be done on a single machine using SQL Server or DuckDB.",
              "score": -1,
              "created_utc": "2026-01-02 13:54:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxabeu3",
                  "author": "kthejoker",
                  "text": "Seems like our platform is needed by 25,000 customers to the tune of $5 billion a year?\n\nBut go on",
                  "score": 1,
                  "created_utc": "2026-01-02 17:29:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2k3r0",
          "author": "PrestigiousAnt3766",
          "text": "I think its mostly about the trade-off of having the luxury to model vs getting the data out there quicky.\n\n\nToday everyone wants data to do whatever ai, bi, experiments. Requirements change rapidly. You see a push to model as late as you can get away with.¬†\n\n\nStrong emphasis on modeling within an org slows everything down. Not many people can model, and shared data models are difficult to design and take time.\n\n\nSo, I think having multiple / decentralized models are the way for now.",
          "score": 9,
          "created_utc": "2026-01-01 12:27:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3d8zi",
              "author": "Separate_Newt7313",
              "text": "@PrestigiousAnt3766 your comment is giving me heartburn.\n\nA data model explains how a business' data fits together and what it means, so people can use it consistently and correctly.\n\nData modeling is largely detective work. It's hitting the streets, talking to the people who really know how the business works, and why the data look the way they do. \n\nSample conversation: \n\"Is this sales line item for a single product or for the entire transaction? Oh it's a roll-up for all transactions in the entire month? Whoops! Glad I asked!!! Where can I get the data for each line item?\"\n\nHow the _hell_ are you going to be piping raw data into a dashboard, LLM, or an ML model, and expect anything other than garbage to come out? Do you put crude oil directly into your car, too‚ÄΩ\n\nAt the end of the day, the main reason why data science is hard is because data modeling is hard, not because using PyTorch is hard.",
              "score": 16,
              "created_utc": "2026-01-01 15:48:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3xp95",
                  "author": "dbrownems",
                  "text": "But the business context required for modeling is one of the main reasons you need multiple/decentralized models.",
                  "score": 1,
                  "created_utc": "2026-01-01 17:37:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3ljpo",
                  "author": "PrestigiousAnt3766",
                  "text": "Have you ever worked for ML or datascientists? They want to access raw and unmodeled data. Thats why in medallion structure you have a bronze layer.\n\nFor BI there is value in modeling but in all companies I have been to as a consultant the stories I hear are an overwhelmed central modeling team and business tired of waiting for their changes.",
                  "score": -1,
                  "created_utc": "2026-01-01 16:33:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2vup3",
              "author": "Nekobul",
              "text": "When you get different numbers from different models what do you do?",
              "score": 6,
              "created_utc": "2026-01-01 14:00:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx30x3e",
                  "author": "PrestigiousAnt3766",
                  "text": "Accept it.",
                  "score": -2,
                  "created_utc": "2026-01-01 14:35:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2x50h",
              "author": "Tall_Working_2146",
              "text": "but is modeling a luxury really? I thought that was the backbone of every useful analytical system, OLAP, semantic models on powerBI, isn't it the entire point to have well designed- single source of truth the way to go ?",
              "score": 6,
              "created_utc": "2026-01-01 14:09:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2z47j",
                  "author": "Nekobul",
                  "text": "Exactly! That is what Mr.Inmon has always said:\n\n\\* Single Source of Truth!  \n\\* Single Source of Truth!  \n\\* Single Source of Truth!",
                  "score": 1,
                  "created_utc": "2026-01-01 14:22:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx302rp",
                  "author": "PrestigiousAnt3766",
                  "text": "Datascience, apis, integrations, flat tables dont necessarily need a dimensional model. If you do realtime, modelling is prohibitively slow.\n\nIf you have regulatory requirements than yes, you need to model.",
                  "score": -1,
                  "created_utc": "2026-01-01 14:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2ld3t",
          "author": "ProfessorNoPuede",
          "text": "The criticism of databricks and snowflake is a miss. It's not about the tool, so why attack those? They're quite good at what they do, especially compared to what came before.\n\nSecondly, no reflection here? None? Perhaps there is a reason the enterprise data warehouse always failed? A better reason than \"they don't understand it\". Organisations are able to grasp very complex concepts and execute on them if the urgency and value are there.\n\nData Integration is apparently hard. Well shucks. Why is it hard and why is it not perceived as valuable enough to solve relative to the complexity?",
          "score": 6,
          "created_utc": "2026-01-01 12:39:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2ww9m",
              "author": "Nekobul",
              "text": "Of course it is about the tool. These platforms are built on columnar databases and those technologies are not suitable for integration. However, the vendors have pushed hacky, dumb solutions to somehow make the integration work. Yet when the underlying technology is not suitable it also makes the processing highly inefficient and expensive.",
              "score": 0,
              "created_utc": "2026-01-01 14:07:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2x91j",
                  "author": "ProfessorNoPuede",
                  "text": "I'm not sure why columnar databases don't allow you to join or process in compute? I see no reason why columnar (be it in parquet, SQL server or snowflake) wouldn't be suited for data integration? It's not the best for low latency random access joins, but that's about it.",
                  "score": 3,
                  "created_utc": "2026-01-01 14:10:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2wnba",
          "author": "GachaJay",
          "text": "To be fair, Databricks is actively pitching it wants to be utilized in operational workloads as well. Same with Fabric. They want to remove the need for separate SQL warehouses to capture the whole market.",
          "score": 2,
          "created_utc": "2026-01-01 14:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2zk4c",
              "author": "Nekobul",
              "text": "I'm waiting for Databricks to start building pig farms. They can use the generated waste to increase the amount of generated BS because it is not enough right now.",
              "score": 5,
              "created_utc": "2026-01-01 14:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2zogk",
                  "author": "GachaJay",
                  "text": "If it‚Äôll make that IPO the right number, they‚Äôll do just about anything",
                  "score": 3,
                  "created_utc": "2026-01-01 14:26:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx344dj",
          "author": "I_Am_Robotic",
          "text": "Ok I‚Äôm a newbie - I‚Äôm not completely following the argument. Can someone help me? \n\nIs he saying people are just dumping data into the data lake and not actually making sense of it so it‚Äôs useful to the business? Isn‚Äôt the whole concept of medallion architecture in dbx exactly about it being useful by the time it gets to gold? And aren‚Äôt both dbx and snowflake largely intended for non-transactional purposes?\n\nIf so, how‚Äôs any of this the fault of snowflake or dbx?",
          "score": 2,
          "created_utc": "2026-01-01 14:55:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3wh7l",
              "author": "Nekobul",
              "text": "That's precisely what these platforms have encouraged and what tools like Fivetran have assisted. Dump the data and then people downstream will grab whatever they want and model it and use it. The problem is the people downstream rarely have understanding what is the data context and semantic. The outcomes can be totally different from one analyst to another.\n\nPeople say these platforms are not responsible for the situation. However, the vendors pushing these platforms are the ones who have encouraged such approach for years because it is highly profitable for them. Instead of doing the computation and modeling once, you now have proliferation of models who are trying to do similar stuff. Some people have said in the comments you should accept that and move on because that is the price you have to pay for velocity and agility. I'm not sure what kind of velocity they are talking about if you don't know whether the data you are dealing is garbage. Garbage is garbage.",
              "score": 3,
              "created_utc": "2026-01-01 17:31:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5o0g6",
          "author": "Livelife123123",
          "text": "They are just tools. The last thing you want is a tool that does everything half assed.\n\nA real life warehouse isn't useful if anyone can dump anything anywhere inside. It stops becoming a warehouse and looks more like a dump.",
          "score": 2,
          "created_utc": "2026-01-01 22:55:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2vu4m",
          "author": "VarietyOk7120",
          "text": "Databricks said they were gonna replace the Data Warehouse with the Lake House. They said that SQL was outdated. \n\nWhen that didn't happen, they then pivoted to releasing Databricks SQL. So he's right there. \n\nI don't think you can knock Snowflake as much however. He seems to be saying they haven't focused as much on integration. You can use a range of third party integration tools with platforms like Snowflake, Fabric and Databricks SQL. \n\nThe main thing is, whatever platform you choose, make sure you understand DW modeling and design fundamentals",
          "score": 2,
          "created_utc": "2026-01-01 14:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx304ka",
              "author": "Nekobul",
              "text": "They have not focused on the integration work at all and it is plainly evident. However, they have willingly sold the lie you can do integration with their platforms, conveniently avoiding the fact their systems are not suitable for that. These platforms have caused huge damage that people are yet to experience.",
              "score": -1,
              "created_utc": "2026-01-01 14:29:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5lxdu",
                  "author": "VarietyOk7120",
                  "text": "Well, you're right, I have seen Databricks implementations (Lake house) that have done huge damage , where the customer is left with something barely usable, having spent a lot on consulting fees etc. The one customer is now building an old style Data Warehouse on a regular database platform just to get reporting back to where it was",
                  "score": 3,
                  "created_utc": "2026-01-01 22:43:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4cwcz",
          "author": "gigatexalBerlin",
          "text": "The company I work for has been around for almost 15 years, been employing people, survived the pandemic, been making moves etc etc and there's not a single fact or dimension table anywhere.\n\nBest I can describe is a bunch of degenerate dim fact tables with no ERD to think of... it's all just in the senior analysts heads.\n\nI keep championing more structure and since we're a Snowflake shop I think we'll get it because some of the tools we are looking to use rely on a semantic layer which will require analytics to bake into a standard-ish format the relationships between tables and columns and id the primary keys and foreign keys etc....\n\nI hope we can then at least get to data marts that are their own star/snowflake schema'd pieces of art that are sane and easily understandable... I can dream though.",
          "score": 1,
          "created_utc": "2026-01-01 18:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5u8k2",
          "author": "BlueMercedes1970",
          "text": "I don‚Äôt know what he is talking about. ETL has always been the hardest part of data warehousing and these platforms provide those tools so where is the issue ?",
          "score": 1,
          "created_utc": "2026-01-01 23:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx933ki",
              "author": "Nekobul",
              "text": "These platforms didn't provide any ETL tools for many years. These vendors promoted a workaround called \"ELT\" as alternative to a proper ETL platform at a huge extra cost for the customers. They have only recently started to introduce ETL tooling after they reached the point where the ELT hack is no longer sustainable.",
              "score": 1,
              "created_utc": "2026-01-02 13:45:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxcqedm",
                  "author": "BlueMercedes1970",
                  "text": "So you think using SSIS with Snowflake would be more optimal? So if I want to perform a delta and load only rows that don‚Äôt exist in the fact table I should pull billions of rows of data into memory in SSIS to do that? And you think that is faster and cheaper than using the database?\nI don‚Äôt think you know what you are talking about",
                  "score": 1,
                  "created_utc": "2026-01-03 00:41:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8kfqf",
          "author": "codek1",
          "text": "This does seem to have really kicked off! Joe Reiss jumped onto it too.",
          "score": 1,
          "created_utc": "2026-01-02 11:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8znm0",
          "author": "rodrig_abt",
          "text": "I've not yet met a single company without some garbage data and/or pipeline. Whatever the reason (people, processes, tool mess, politics, or all of them), the reality is that you do not raise at the level of your business priorities but fall to the level of your systems...and if your systems are \"bad,\" bad things happen. Data has always been complicated (even with the term \"data\" itself), but no matter how complicated, you can always trace a predictable path to achieve something. Data warehousing is a great example: it started as a way to solve reporting and analytics problems. Regardless of industry, size, or type of data, the data-warehousing modeling approach can provide a predictable path to achieve reporting and analytics goals, no matter how complicated. You didn't need many tools: just extraction, transformation, and loading. And yes, the devil is in the details, you're right, but that's precisely where things get complex when you start adding too many moving parts (tools) that create complexity. Complexity is always bad. Always. Remove complexity first, then work out complications.",
          "score": 1,
          "created_utc": "2026-01-02 13:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9y2ag",
          "author": "alex_korr",
          "text": "Inmon got big in the days when the star schema was the only way to make analytics work, your main dataset was customer orders and the GL, ETL 4x a day was considered to cuttingedge, etc. Nowadays, it's simply not the case.",
          "score": 1,
          "created_utc": "2026-01-02 16:27:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdje8h",
          "author": "-miked",
          "text": "What does he mean by \"integration of data\"?",
          "score": 1,
          "created_utc": "2026-01-03 03:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2uegp",
          "author": "Nekobul",
          "text": "Using columnar, non-volatile databases for data integration is expensive, highly inefficient, with high latency and frankly dumb. However, both of these major players have sold that hack/concept to the masses with great success. And then they called it \"modern\". What a joke.\n\nWake up, people!",
          "score": -1,
          "created_utc": "2026-01-01 13:50:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5tptx",
              "author": "BlueMercedes1970",
              "text": "You aren‚Äôt making sense. What integration are you talking about and for what purpose? If you are building a data warehouse or data lake then both of these platforms are perfectly valid.",
              "score": 2,
              "created_utc": "2026-01-01 23:27:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5uo82",
                  "author": "Nekobul",
                  "text": "How do you \"massage\" the data to make it usable?",
                  "score": 1,
                  "created_utc": "2026-01-01 23:32:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2nl4g",
          "author": "sleeper_must_awaken",
          "text": "Unhinged article without root cause analysis. Many leaders figured out decades ago that enterprises need information (or data) management and governance. Without these, all you have are tools, but no hands, no guidelines, no direction, no accountability, no improvement processes‚Ä¶\n\nYou‚Äôre misguided if you believe you can use cheap consultancies to enable and strategically leverage a core asset of your organization: data.",
          "score": -2,
          "created_utc": "2026-01-01 12:58:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2wj50",
              "author": "Nekobul",
              "text": "These platforms didn't provide any integration tools. Everyone was left on their own to come up with a way to shape the data. That is major issue that these vendors only recently acknowledged/understood exists.",
              "score": 0,
              "created_utc": "2026-01-01 14:05:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx6jzql",
                  "author": "BlueMercedes1970",
                  "text": "What are you talking about? They have Spark, SQL and Python.",
                  "score": 1,
                  "created_utc": "2026-01-02 01:59:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyecpd",
      "title": "Mid Senior Data Engineer struggling in this job market. Looking for honest advice.",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pyecpd/mid_senior_data_engineer_struggling_in_this_job/",
      "author": "This_Bird6184",
      "created_utc": "2025-12-29 05:58:33",
      "score": 110,
      "num_comments": 59,
      "upvote_ratio": 0.9,
      "text": "Hey everyone,\n\nI wanted to share my situation and get some honest perspective from this community.\n\nI‚Äôm a data engineer with 5 years of hands-on experience building and maintaining production pipelines. Most of my work has been around Spark (batch + streaming), Kafka, Airflow, cloud platforms (AWS and GCP), and large-scale data systems used by real business teams. I‚Äôve worked on real-time event processing, data migrations, and high-volume pipelines, not just toy projects.\n\nDespite that, the current job hunt has been brutal.\n\nI‚Äôve been applying consistently for months. I do get callbacks, recruiter screens, and even technical rounds. But I keep getting rejected late in the process or after hiring manager rounds. Sometimes the feedback is vague. Sometimes there‚Äôs no feedback at all. Roles get paused. Headcount disappears. Or they suddenly want an exact internal tech match even though the JD said otherwise.\n\nWhat‚Äôs making this harder is the pressure outside work. I‚Äôm managing rent, education costs, and visa timelines, so the uncertainty is mentally exhausting. I know I‚Äôm capable, I know I‚Äôve delivered in real production environments, but this market makes you question everything.\n\nI‚Äôm trying to understand a few things:\n\n\t‚Ä¢\tIs this level of rejection normal right now even for experienced data engineers?\n\n\t‚Ä¢\tAre companies strongly preferring very narrow stack matches over fundamentals?\n\n\t‚Ä¢\tIs the market simply oversaturated, or am I missing something obvious in how I‚Äôm interviewing or positioning myself?\n\n\t‚Ä¢\tFor those who recently landed roles, what actually made the difference?\n\nI‚Äôm not looking for sympathy. I genuinely want to improve and adapt. If the answer is ‚Äúwait it out,‚Äù I can accept that. If the answer is ‚Äúyour approach is wrong,‚Äù I want to fix it.\n\nAppreciate any real advice, especially from people actively hiring or who recently went through the same thing.\n\nThanks for reading.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pyecpd/mid_senior_data_engineer_struggling_in_this_job/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwiz4vz",
          "author": "ThroughTheWire",
          "text": "you definitely buried the lede with the visa timelines bit being buried in the post.\n\ndealing with visas is a huge headache for companies rn, especially in the USA. not sure where you are from or if this is about the job market in America specifically but in general people are really avoiding making visa hires everywhere rn",
          "score": 71,
          "created_utc": "2025-12-29 11:02:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj5irf",
              "author": "financialthrowaw2020",
              "text": " Yep. It's also just logical: there are many unemployed people who are citizens. The jobs should go to them before anyone else.",
              "score": 36,
              "created_utc": "2025-12-29 11:56:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwmwync",
              "author": "reviverevival",
              "text": "There's also a compliance factor, where I work we have government (state and federal) clients and it's possible that with changes coming down the pipe that non-us citizens won't even be allowed to look at the relevant data. They are a minority of our customers, and the restrictions aren't even set in stone yet, but it doesn't make sense for us to take the risk and hire someone who can't work with the data from _all_ our customers if there's any other option. (Not to even mention the visa uncertainties)",
              "score": 4,
              "created_utc": "2025-12-29 23:40:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi6w74",
          "author": "vikster1",
          "text": "if you are us based and have a visa, maybe that's a reason too? maybe some companies don't want the extra layer of complexity given the current administration and their \"handling\" of humans that could maybe not be born in the us.",
          "score": 57,
          "created_utc": "2025-12-29 06:42:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiepmw",
              "author": "JohnPaulDavyJones",
              "text": "One big issue adjacent to that is that a lot of data teams just can‚Äôt sponsor a visa-holder, and they‚Äôll make that clear in the job posting, but a boatload of folks who need visas still apply in the hopes that they‚Äôll get hired and then, a few months in, spring the ‚ÄúHey, I need visa support‚Äù message.\n\nMy old firm had that exact thing happen to them, and it burnt them so hard that so they just outright started filtering any resumes for folks who came from overseas and did a grad degree here, graduating less than a year prior to application. Ironically, one of the only folks who got through and got hired explicitly put that he was a naturalized US citizen on his resume.\n\nThat single filter removed probably 75% to 80% of our 1,000+ applicants per position, every time. Probably caught plenty of eligible folks with that broad of a net.",
              "score": 27,
              "created_utc": "2025-12-29 07:51:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwj5v24",
                  "author": "financialthrowaw2020",
                  "text": "Yep. Sponsorship doesn't work in this economy where there are plenty of good engineers in the labor pool. Visas are for when you can't find Americans to do the work. Plenty of Americans available right now.",
                  "score": 12,
                  "created_utc": "2025-12-29 11:59:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwifi2p",
                  "author": "vikster1",
                  "text": "agreed. same in germany. the amount of unwanted applications from India is astounding. no one wants to deal with that.",
                  "score": 25,
                  "created_utc": "2025-12-29 07:59:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwjt89x",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 2,
                  "created_utc": "2025-12-29 14:33:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwig5v1",
          "author": "RunnyYolkEgg",
          "text": "Technical people tend to be very‚Ä¶specific when it comes to soft skills. I‚Äôm not saying that this is your case but how good do you think you are when it comes to that? \n\nIn my experience, I got jobs that I‚Äôm not even qualified for just because they liked me. A recurrent comment I hear around peers who are interviewing candidates is that, because of the amount of experience required for the position, they know you will learn whatever they throw at you so they focus on getting someone who clicks with them. Good communication, good presentation, knows how to move the conversation without making it awkward, shows ownership.\n\nNot an awkward guy in a hoodie asking for full remote position and just tanking questions in the interviews. Unfortunately, extroverted people get rewarded.\n\nJust trying to help and give you my experience. Good luck man!",
          "score": 26,
          "created_utc": "2025-12-29 08:05:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlc3hz",
              "author": "lzwzli",
              "text": "This.",
              "score": 3,
              "created_utc": "2025-12-29 18:56:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi5778",
          "author": "AZjackgrows",
          "text": "the job market is brutal right now and most companies are hiring internal whenever they can to retain talent. orgs are treading water seeing if the other shoe drops on this economic reset. \n\nfor what it‚Äôs worth, try to highlight business impact just as much as your technical skills. the missing piece that any org is looking for is someone who can work with the business to help identify, communicate and develop requirements that lead to value delivery. there‚Äôs still a need for those who can run a transformation vs managing break-fix. \n\nif you want to go the ai buzz route, talk about how your work can lay foundation for ai and clean up data to enable agentic capabilities. \n\ni‚Äôd also speak to governance. it demonstrates that you don‚Äôt just build architecture that can‚Äôt be managed as systems evolve. we‚Äôre all stuck in legacy data environments that look like spaghetti on the back end. lots of us on the business side (who know our way around data) are getting really tired of hearing that simple requests will take months or can‚Äôt be done due to capacity constraints. \n\nmy company is buying all these things rather than building them right now bc we don‚Äôt have the internal staff with these future-forward skillsets‚Ä¶ i‚Äôd much rather be building them in house but we just don‚Äôt have the people who can do it.",
          "score": 11,
          "created_utc": "2025-12-29 06:28:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi5ell",
              "author": "AZjackgrows",
              "text": "and think of looking at larger fortune 500 companies that have core business in supply chain, retail or finance vs technology firms.",
              "score": 2,
              "created_utc": "2025-12-29 06:30:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj5af7",
          "author": "financialthrowaw2020",
          "text": "It's the visa. We're not hiring non-citizens. The market is already tough on citizens right now, so visa holders don't even get a first look. There are already too many talented engineers needing jobs and that's how it's supposed to work: the existing jobs are supposed to go to the citizens of this country before anyone else.",
          "score": 26,
          "created_utc": "2025-12-29 11:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi2f4l",
          "author": "No_Lifeguard_64",
          "text": "At 5 years of experience I wouldn't call you senior at all. You would barely be mid level at my company.\n\nWithout seeing your resume or anything the most actionable advice I can give you is network and look in unconventional spaces. Most jobs are gotten through connections and that is especially true in today's AI landscape where both company and applicant are using AI. Companies using AI to screen resumes and applicants using AI resumes. Its just bots talking to each other. That being said, almost every job I have gotten I have applied into the void and gotten lucky.\n\n As far as unconventional spaces, more people than you think need a data engineer. Its not just Fortune 500 tech companies. Medicine needs data engineers, agriculture needs data engineers, non-profits especially need data engineers.",
          "score": 54,
          "created_utc": "2025-12-29 06:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj9jxr",
              "author": "value_here",
              "text": "I've got about 3.5 years xp doing data engineering tasks as my main workload and I just took a new job, and they are insisting on giving me Senior DE as my title. Every company is different about titles.",
              "score": 15,
              "created_utc": "2025-12-29 12:28:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi3sog",
              "author": "This_Bird6184",
              "text": "That‚Äôs fair, and I appreciate the honest perspective.\n\nTitles definitely vary a lot by company. I‚Äôm not particularly attached to the ‚Äúsenior‚Äù label itself. What I care more about is the scope of work. In my previous roles, I‚Äôve owned production pipelines end to end, worked on large-scale Spark and streaming systems, and supported business-critical use cases. Some companies call that senior, others call it mid-level, and I‚Äôm okay with that distinction.\n\nI fully agree with you on networking and the AI screening problem. It really does feel like bots talking to bots right now, which makes the process feel detached from actual engineering ability. I‚Äôve landed roles both through cold applications and through luck, so your point about ‚Äúapplying into the void‚Äù resonates.\n\nThe unconventional spaces advice is solid. I‚Äôve started expanding beyond traditional tech companies and am actively looking into healthcare, education, logistics, and other domains where data engineering is needed but hiring is less automated. That shift already feels more promising than chasing only big-name tech roles.\n\nIf you don‚Äôt mind sharing, I‚Äôd be curious how you‚Äôve approached networking into those spaces. Did you focus on referrals, direct outreach to managers, or something else that worked consistently for you?",
              "score": 12,
              "created_utc": "2025-12-29 06:16:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwi9315",
                  "author": "lemonfunction",
                  "text": "can you try framing your experience around business needs/requirements? recruiters and hiring managers only glance at tech experience, but a good resume with business wins will make your resume stand out a ton.\n\nassuming you're applying in the usa, sponsorship might be adding to the turbulence you're experiencing",
                  "score": 12,
                  "created_utc": "2025-12-29 07:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwja74t",
              "author": "0MEGALUL-",
              "text": "Seniority is determent by responsibility, ownership and output, not by amount of years experience. \n\nYears of experience gives you more room to grow in those 3, but it‚Äôs not where the value is. It‚Äôs just something employers say to negotiate salary down. The value is in responsibility, ownership and output.\n\nNot dying isn‚Äôt really an accomplishment, is it? Neither a guarantee to be more skilled than someone with less experience.",
              "score": 7,
              "created_utc": "2025-12-29 12:33:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwk0kqs",
                  "author": "No_Lifeguard_64",
                  "text": "I am not giving complete product ownership to someone with only 5 years of experience but you do you.",
                  "score": 0,
                  "created_utc": "2025-12-29 15:12:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj2m03",
              "author": "GuhProdigy",
              "text": "\nThe title of data engineer has been mainstream for 10 maybe 12 years. saying 5 years isn‚Äôt senior, when in a lot of orgs you have principal positions, manager positions, and director positions in data engineering is overtly obtuse. Reminds me of the meme of company‚Äôs asking for more experience using a technology than the technology has even existed for. Stop parroting HR talking points that are meant to rationalize why they didn‚Äôt give you a promotion. \n\nWe all know YOE doesn‚Äôt equal technical prowess. I‚Äôm having too teach a 50 year old with over a decade in data git.",
              "score": 14,
              "created_utc": "2025-12-29 11:32:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjzuex",
                  "author": "SELECTaerial",
                  "text": "The title of data engineer has been mainstream for more like 3-5yrs‚Ä¶where are you getting this 10-12? The term didn‚Äôt even exist 12-15yrs ago\n\n10-12yrs ago there were software engineers or ETL developers or database developers‚Ä¶not data engineers",
                  "score": 2,
                  "created_utc": "2025-12-29 15:08:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwl8wyo",
                  "author": "gjionergqwebrlkbjg",
                  "text": "I'm sorry, are you under impression that people who were dealing with data modelling and other aspects like that before the title came to be suddenly disappeared? Who do you think was working on data warehouses, hadoop and whatnot?",
                  "score": 1,
                  "created_utc": "2025-12-29 18:42:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkbp71",
              "author": "Natural-Intelligence",
              "text": "What about a person with 10 YOE in SWE but 2 YOE in data engineering? What about the 3 YOE data engineer who passionate about the field and is much more productive than your 10 YOE data engineers?\n\nGate keeping with seniority is an excellent way to get rid of your high performers. Unfortunately, the pay is tied to the title.",
              "score": 4,
              "created_utc": "2025-12-29 16:06:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkfn35",
                  "author": "No_Lifeguard_64",
                  "text": "You're trying to be pedantic for no reason. Sorry my opinion hit you in the feelings but in this instance, all I have to go on is the stated years of experience and on average, I wouldn't trust someone with 5 years of experience with product ownership.",
                  "score": 3,
                  "created_utc": "2025-12-29 16:25:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjzgjc",
              "author": "SELECTaerial",
              "text": "I had the same initial thought. 5yrs isn‚Äôt senior anything imo",
              "score": 4,
              "created_utc": "2025-12-29 15:06:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwidndg",
          "author": "rajekum512",
          "text": "Sorry this might be the true case. There was a senior DE role opened in my company and they are open to remote  roles. But the position got backfilled by internal transfer who is half DE only but very strong domain experience. Second important issue is visa problem..We are ok with stable resource who doesn't bring heavy baggage of unstability",
          "score": 6,
          "created_utc": "2025-12-29 07:42:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj55qm",
          "author": "kali-jag",
          "text": "Where are you based out off?\n\nAlso from my expirience, December to feberuary is very dry period for interviews...",
          "score": 4,
          "created_utc": "2025-12-29 11:53:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjgk20",
          "author": "TheCamerlengo",
          "text": "Here is a story‚Ä¶my company recently posted an ad for a data engineer (mid to senior level). Within 2 days we were overwhelmed with candidates, easily over 300. We explicitly said no sponsorship in the ad. What did we get?\n\n98% Indian candidates that were in some sort of work visa limbo. Most of the students with student work visas that were expiring within the next couple years, some h1-b or married to h1-b candidates. \n\nThere were a few green card holders and 2 US born candidates that were extremely unqualified for the role. \n\nAlso every resume seemed to come from the same template and pretty much looked the same with different schools, companies, starting dates but mostly the same skills and general experience. It was very difficult to go thru the stack of candidates because very few stood out as different. \n\nI think this market is broken. Qualified candidates have trouble getting thru the noise and it is flooded with foreign workers. My guess is that most employees are staying out of the market for now and waiting for conditions to improve. Those looking for work are those that were downsized or student workers looking to land someplace to start an h1-b process. \n\nMy analysis may be incorrect but this is what we experienced posting on LinkedIn and indeed.",
          "score": 6,
          "created_utc": "2025-12-29 13:18:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiswsz",
          "author": "omonrise",
          "text": "do you also talk to recruiters using chatgpt like you do to us? could be the reason üòú",
          "score": 5,
          "created_utc": "2025-12-29 10:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigsl5",
          "author": "btown75",
          "text": "Hiring is a pain.  Thousands of submissions per job posting, resumes all look the same since they are written by ChatGPT, lying on application knockout questions, using AI to answer questions DURING the interview (sometimes obvious when they do this), and now I‚Äôm hearing from others they caught people using AI deepfakes on behalf of others.  Sponsoring has been too complicated for a while now, and am still getting plenty of applicants that don‚Äôt need it.  With modern tools maturing, lakehouse best practices solidifying, and AI automation, I just don‚Äôt need as big of a team as I used to either.  \n\nSorry, friend, I know that doesn‚Äôt help you.  Just rant from my side on the state of things.  Good luck to you.  I‚Äôve been in similar situations when I was younger, and probably will again. Just don‚Äôt give up, keep trying and don‚Äôt be afraid to reinvent yourself if needed.",
          "score": 2,
          "created_utc": "2025-12-29 08:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwirqh9",
          "author": "testEphod",
          "text": "The vague feedback is also a way to avoid lawsuits, therefore it is done in an ambiguous way.",
          "score": 2,
          "created_utc": "2025-12-29 09:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj5d42",
          "author": "kali-jag",
          "text": "Alzo if you in us on h1b or other, that very well would be a reason.\n\nMy brother in law is working for half his usual salary(he comes with 18 years of expirience).\n\n Some of my friend who are good are struggling with some rejection reason very often late in tge interview process..",
          "score": 2,
          "created_utc": "2025-12-29 11:55:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjzb32",
          "author": "SELECTaerial",
          "text": "TIL 5yrs experience is considered mid-senior.",
          "score": 2,
          "created_utc": "2025-12-29 15:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu9x8u",
          "author": "TheSchlapper",
          "text": "If you are in a decent sized city and willing to do hybrid or on-site, then I haven‚Äôt heard of a better time to be in data.\n\nIf you are only applying to remote jobs then the market is pretty much in the air and I wish you the best of luck.\n\nEdit: not a good time to be a visa holder in really any capacity tbh unless you are top 1% in skills or something",
          "score": 2,
          "created_utc": "2025-12-31 01:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwicqbc",
          "author": "AlynaRoe",
          "text": "which country?",
          "score": 2,
          "created_utc": "2025-12-29 07:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiwu39",
          "author": "peterxsyd",
          "text": "Focus on escaping the noise and the pile:\n\n\\- reach out to recruiters with genuine but concise and respectful messages, where you are adding value by letting them know you are available have xyz skills and can start y\n\n\\- focus heaps of time on having a ready to go portfolio of exactly what you can do so they immediately see that, your future employer sees that and they go they can do that here now no risk no trouble I get that if I pay for you\n\n\\- make sure your Linked in is presented well, that your experience shows a clear line of growth and breadth of experience\n\n\\- tailor your resume for every job you apply for. Also, remember, even before you reach the hiring manager - the gates you need to get passed is ATS and the recruiter. Make sure you do\n\n\\- message the recruiter directly (and politely) after you apply so they know you are interested and so you are memorable. They don‚Äôt like or call that there are 200 people applying for a position. Literally, it‚Äôs just annoying for them. Go around the pile.\n\n\\- make sure your communication skills are polished, and that you don‚Äôt waffle, and are really positive about your experience, skills, and team work.\n\nThis will help I promise.",
          "score": 2,
          "created_utc": "2025-12-29 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj70a5",
          "author": "NoleMercy05",
          "text": "5 yrs XP is not 'mid-senior'. Lol.",
          "score": 2,
          "created_utc": "2025-12-29 12:08:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwijnda",
          "author": "Financial_Anything43",
          "text": "Which industries have you targeted?",
          "score": 1,
          "created_utc": "2025-12-29 08:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjw327",
          "author": "goldenfoxinthewild",
          "text": "Hijacking this post to ask - would the situation change if one already has a spousal visa in the US?",
          "score": 1,
          "created_utc": "2025-12-29 14:49:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk1agh",
          "author": "lmp515k",
          "text": "No point in hiring recently graduated masters right now, most of the resumes are fabrications and frankly they are just not very good and AI is nipping at their heels.  Also, NOBODY CARES about your certifications.",
          "score": 1,
          "created_utc": "2025-12-29 15:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl6dm2",
          "author": "lzwzli",
          "text": "If you're in a situation where you are heading into a visa expiration, you probably need to be peepared to head home. The uncertainty around visas is making every company pause all hiring that require a visa. \n\nDo you have a job now or you just graduated?",
          "score": 1,
          "created_utc": "2025-12-29 18:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm0myx",
          "author": "Lucade2210",
          "text": "5 year senior? Lol",
          "score": 1,
          "created_utc": "2025-12-29 20:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnthca",
          "author": "apono4life",
          "text": "I‚Äôm sorry you are struggling, because you are in a visa it may be difficult‚Ä¶ also you may want to look at a lower title level. 5 years isn‚Äôt a ton of experience at this point.",
          "score": 1,
          "created_utc": "2025-12-30 02:38:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqjr9a",
          "author": "Great_Assistant_7854",
          "text": "A lot of people seem to be struggling too right now with the tech industry job market",
          "score": 1,
          "created_utc": "2025-12-30 14:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuqsam",
          "author": "SeaworthinessDue3355",
          "text": "The visa thing is a big deal in the US. I work for a company that has lots of mid level positions open but we aren‚Äôt going to sponsor.",
          "score": 1,
          "created_utc": "2025-12-31 03:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx06ddh",
          "author": "No_Resolution8717",
          "text": "What is your CTC if I may ask with 5YOE?",
          "score": 1,
          "created_utc": "2026-01-01 00:25:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk3cb8",
          "author": "Brilliant-Gur9384",
          "text": "My unpopular take is the demand's not coming back. I added clients this year with AI tooling I've built that eliminates the need for the entire data stack. This is growing (replacing entire tech teams). I get that this comment will be viewed as unpopular here, but I'm trying to help you zoom out here. \n\nWhat do you think it looks like in 5 years?\n\nEven for people employed, where do you think this industry is heading?\n\nIf you're smart, think ahead and take action now.",
          "score": 1,
          "created_utc": "2025-12-29 15:26:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjcei0",
          "author": "dasnoob",
          "text": "5 years is not much experience. That's your issue. Title inflation is real at a lot of companies.",
          "score": 0,
          "created_utc": "2025-12-29 12:49:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4zzac",
      "title": "What actually differentiates candidates who pass data engineering interviews vs those who get rejected?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q4zzac/what_actually_differentiates_candidates_who_pass/",
      "author": "Murky-Equivalent-719",
      "created_utc": "2026-01-05 22:16:45",
      "score": 90,
      "num_comments": 41,
      "upvote_ratio": 0.93,
      "text": "Hey everyone,  \nI‚Äôm currently looking for a data engineering role and I‚Äôve always been curious about what really separates people who make it into Google (or similar big tech) from those who don‚Äôt. Not talking about fancy schools or prestige, just real, practical differences. From your experience, what do strong candidates consistently do better, and what are the most common gaps you see? I‚Äôd really appreciate any honest, experience-based insights. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q4zzac/what_actually_differentiates_candidates_who_pass/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxwfm6t",
          "author": "69odysseus",
          "text": "DE requires multitude of skills, out of which many, and most companies test primarily on SQL, Data Modeling and Distributed systems (Databricks and snowflake). FAANG test right out of the bat on SQL and DSA's, just to weed out candidates. If they go past that, then same concepts are tested at much advance levels.¬†\n\nAll these only come with experience. One of the trickier questions always asked is about the trade-offs for various roles. They're testing to see if a person can think of other ways than just one way. Questions on cost vs speed of data is also asked quite a bit.¬†",
          "score": 83,
          "created_utc": "2026-01-05 22:27:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwwk77",
          "author": "ElCapitanMiCapitan",
          "text": "Most data engineers I interview are horrible communicators. In my company, poor English is an automatic disqualifier. We need to be able to put you in front of business users, and to do that we need to trust that you can speak coherently and answer questions directly without rambling for 20 minutes. These communication requirements disqualify about 90% of people I interview. After that it comes down to skill and personality fit. I will say even now, good candidates are incredibly rare, most candidates are career consultants who overstate their capabilities in every way imaginable.",
          "score": 57,
          "created_utc": "2026-01-05 23:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxmez8",
              "author": "CarefulCoderX",
              "text": "Ive been working with a lot of nearshore and offshore/H1B folks and this has been a huge issue.\n\n\nI'm not necessarily blaming them, knowing 2 languages that well is very difficult and impressive.¬†\n\n\nHowever, there has been a lot time wasted when there's a misunderstanding as to what's to be implemented.\n\n\nThis is especially bad with misunderstandings between the client and our team.",
              "score": 16,
              "created_utc": "2026-01-06 02:11:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz965x",
                  "author": "speedisntfree",
                  "text": "We get a lot of these because we don't pay the market rate. The interviews have been pretty harrowing: even asking a basic question, I get a monologue of buzzwords flung at me with answers to 5 questions I didn't ask.\n\nIt is a massive problem because in a lot of DE, they won't have a software business analyst doing the requirements for them and they will need to do it.",
                  "score": 10,
                  "created_utc": "2026-01-06 09:19:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwf55j",
          "author": "mweirath",
          "text": "I do a lot of hiring but not at big tech. That said for me a lack of practical hands on experience and the ability to translate that experience into new areas is the reason I reject most candidates. \n\nI want to know you have experience and can apply that to new problems you haven‚Äôt seen before.",
          "score": 46,
          "created_utc": "2026-01-05 22:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwgcno",
              "author": "Murky-Equivalent-719",
              "text": "Thanks for your reply. I believe most of us have that kind of ability, how can you evaluate it through the resume? I really appreciate your answer",
              "score": 13,
              "created_utc": "2026-01-05 22:30:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwsc1a",
                  "author": "mweirath",
                  "text": "This is more an interview topic vs. a resume.  Like other people have mentioned - problem solving.  Be able to explain a problem.  How did you come up with the solution?  Did you have some experience in the past you were able to reuse?  How did you track down the issue?\n\nI can‚Äôt tell you how many people told me ‚Äúmy coworker/lead told me what to do‚Äù or ‚ÄúI found an answer on Google‚Äù - if I try to dig in ‚ÄúWhat did you learn from this?‚Äù ‚ÄúWhat are you going to do differently from now on?‚Äù ‚ÄúHow are you going to keep this from happening in the future?‚Äù And I get blank stares or nothing answer the interview is over.",
                  "score": 9,
                  "created_utc": "2026-01-05 23:31:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwht1e",
              "author": "jfrazierjr",
              "text": "Define practical hands on experience?   Perhaps its the job title thats throwing me off.\n\n\nFor example, i I built mu first SOAP client and server roughly 17 years ago and have built 3 dozen APIs since then many in both directions.    In addition 17 years working with read/write xml and flat files(sftp)\n\nMssql queries 25 years ago in support and 17 years of insert/update/delete with a few new tables I created for new functionality about 10 years ago.\n\nMore recently, used airbyte to pull data from a mssql db using CDC followed by DBT transforms into a different schema.\n\nWould that combined make me a Data Engineer?  If not what's missing(i might have that too)?",
              "score": 5,
              "created_utc": "2026-01-05 22:37:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz6wxq",
                  "author": "Budget-Minimum6040",
                  "text": "You usually need:\n\n* Data modelling (Star Schema is the classic one + Snowflake, Data Vault maybe)\n* very good SQL skills\n* dbt core\n* Python (core + async, pySpark, polars)\n* Cloud experience (Databricks, BigQuery, Snowflake, Fabric)\n* IaC (Docker/OCI compliant container, Terraform)\n\nAside from the first part (never got asked data modelling questions in Germany so far, no idea why) everything else was asked in the interview, usually at a superficial level.",
                  "score": 4,
                  "created_utc": "2026-01-06 08:57:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwiwlw",
          "author": "JonPX",
          "text": "Can you string together two coherent sentences on what you did? Like explain what you did, what your thought process was, how you came to certain conclusions. Because if you can't, you are either lying about what you did, your skills aren't up to what you claim, or you will simply not be able to properly communicate with other team members. \n\nI have seen a lot of candidates where five minutes into the interview I already know I'm not moving ahead with them, even if their resume sounds perfect.",
          "score": 30,
          "created_utc": "2026-01-05 22:43:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwjzjj",
              "author": "atrifleamused",
              "text": "I like picking the candidate up from reception. During the lift journey I learn a lot about them and how they are able to interact. You at least know if you like them and if the rest of the team will.",
              "score": 7,
              "created_utc": "2026-01-05 22:48:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxyx8s",
                  "author": "artozaurus",
                  "text": "What year was this written in? 2018?\nAre in person interviews still a thing?\nEven for hybrid on in office roles, people are required to come to the office for an interview?",
                  "score": 7,
                  "created_utc": "2026-01-06 03:20:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxzwlon",
                  "author": "Ok-Obligation-7998",
                  "text": "Why do you care so much about liking them? They aren‚Äôt going to become your friends. And they most probably won‚Äôt care if you dropped dead the next day.",
                  "score": -2,
                  "created_utc": "2026-01-06 12:36:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwk5ft",
          "author": "Firm_Bit",
          "text": "It‚Äôs not a mystery. Be the highest performing candidate that they can afford. This is a combination of technical skills and experience and personality. And the interviewers weigh each aspect however they wish. \n\nYou could perform exactly the same and get different outcomes depending on who the other candidates are and who the interviewers are.",
          "score": 10,
          "created_utc": "2026-01-05 22:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyrrv2",
          "author": "compubomb",
          "text": "I have ADHD, what got me through my last interviews was focusing on my STAR method. Keeping my story sequential, not a Tarantino movie. I have to practice keeping things ordered or I get out of sequence quickly.",
          "score": 9,
          "created_utc": "2026-01-06 06:38:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwf7wc",
          "author": "umognog",
          "text": "Independent problem solving.\n\nI can interview dozens, hundreds of people and get pretty much the same interview out of everyone.\n\nWhat im looking for, is the person that gets that look in their eye, the passion, the enjoyment, the fun of doing it because its what they enjoy. The challenge and solving it, or knowing when to call it.\n\nMotivated, self driven to do better because that is who they are not because its what they are paid to do.\n\nI love the fact that I get paid to do my hobby.",
          "score": 9,
          "created_utc": "2026-01-05 22:25:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwrpdw",
              "author": "AnonymousTAB",
              "text": "This is me. I struggle in other areas of the process (big panic during technical interviews) but I keep making it to second and third rounds because apparently everyone I talk to can tell I‚Äôm passionate and very driven.",
              "score": 3,
              "created_utc": "2026-01-05 23:28:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxpb5k",
          "author": "amtobin33",
          "text": "People Skills",
          "score": 3,
          "created_utc": "2026-01-06 02:27:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzx0zm",
          "author": "dataflow_mapper",
          "text": "From what I have seen, the people who pass usually think in systems, not just tools. They can explain tradeoffs clearly, like why they would choose one storage or processing pattern over another given scale, cost, or failure modes. Strong candidates also communicate their thinking out loud and course correct when they realize an assumption is wrong. A common gap is treating pipelines like coding exercises instead of production systems with monitoring, data quality, and consumers. Big tech interviews care less about memorized syntax and more about whether you understand how data actually behaves over time. The best answers usually sound like someone who has been burned before and learned from it.",
          "score": 3,
          "created_utc": "2026-01-06 12:39:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0yvpz",
              "author": "Thlvg",
              "text": "Last sentence is the most spot on I've seen so far. I don't care how you optimize a Spark job. I want to know about that time you understood you got it wrong, why, how you solved it, what you learned from it and what you do for it not to happen again. The proverbial drop table in production: what happened, what did you learn, how will you make it not happen again ?",
              "score": 1,
              "created_utc": "2026-01-06 16:01:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwiqj3",
          "author": "codemega",
          "text": "The most common gap is not knowing how to code. Good candidates are pretty good at everything - python, SQL, data pipeline design, system design, explanations of past projects, Spark, data modeling, etc. You don't have to be an expert at all of these, but you should be pretty good in all of them and very good in at least some of them.",
          "score": 7,
          "created_utc": "2026-01-05 22:42:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxftmh",
          "author": "mike8675309",
          "text": "For me, the guys who get through me, the Hiring Manager, are the ones who can clearly communicate their knowledge when asked specific questions.  They need to be able to take a data engineering problem and describe the solution they would create using whatever knowledge they have.  Whatever tool set they know.  \n And they have to be able to understand a business perspective and their impact on the business.    \nDepending on the level the expectations will be adjusted.",
          "score": 4,
          "created_utc": "2026-01-06 01:35:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxoxfo",
          "author": "MayaKirkby_",
          "text": "It‚Äôs mostly solid basics plus how you think, not magic. People who pass can write clean SQL, design a sensible pipeline for a clear use case, and talk through trade-offs out loud, they clarify requirements, state assumptions, start simple, then refine. People who struggle often lean on buzzwords or tools without tying it to a coherent system or real examples from their past work.",
          "score": 2,
          "created_utc": "2026-01-06 02:25:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxu5mv",
          "author": "DenselyRanked",
          "text": "I haven't checked in a while, but I don't think Google has a traditional in-house \"Data Engineer\" role. They have \"Cloud Data Engineer\" roles under Professional Services, which is more sales / support / migration work for GCP clients. But if it is available, then this is relevant.\n\nPreparing for a FAANG tier/Big N interview is different than other companies because most of them are standardized, except Apple/Netflix which are team dependent. The other tiers can be a completely random process. Generally speaking, you will need to brush up on SQL, DSA, data modeling, and something company specific. Check this subreddit, glassdoor, blind, google, or chatgpt/AI for interview tips and study guide for the specific company.\n\nWhat they are looking for also varies by company, but I can say that you will have a much higher rate of success if you take the time to research. It doesn't take much to fail and you will need some luck to avoid bad interviewers.",
          "score": 2,
          "created_utc": "2026-01-06 02:53:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy9c20",
          "author": "ExistentialFajitas",
          "text": "Listing things on your resume that you can‚Äôt speak to. I don‚Äôt actually care if you have experience with the entire universe of DE tooling and problem sets, but if you list something I will ask. If you can‚Äôt speak on the topic, it‚Äôs a large negative mark. Do not list what you aren‚Äôt confident in.",
          "score": 2,
          "created_utc": "2026-01-06 04:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy3tym",
          "author": "addictzz",
          "text": "Based on my observation:  \n\\- **Learning how to learn**: The ability to quickly learn something new in structured way.  \n\\- **Problem-solving**: Breaking down complex problems into bits of clear, executable task. This is also about applying your acquired skills to solve problems or to boost the business.  \n\\- **Communication**: You will have to be able to communicate the problem and *potentially* the solution cross-BUs.\n\nSeemingly simple but turns out having these 3 aspects is a rarity among candidates. And coming from fancy, prestigious background is not always a clear sign that you are gonna have these.",
          "score": 1,
          "created_utc": "2026-01-06 03:49:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyo5s4",
          "author": "Unhappy_Commercial_7",
          "text": "Bar raiser at big tech :\n\nDepends on the leveling guidelines however largely speaking: \n\nthe phone screens are to assess basic skills (i know leetcode had a bad reputation, but there really is no good answer on how else to do it, AI seems to be quickly making way here and it might be a part of this in near future who knows)\n\n\nCoding rounds on the onsite probe deeper for the role and most times are with folks on the same team, they want to see your problem solving skills under pressure, no one wants a new team mate who can‚Äôt carry his own weight- outcomes generally binary from these rounds\n\n\nSystems designs really weighs in the most from leveling perspective - a sr or staff can quickly drive discussion, showcase what areas to dive deep into and what to leave out, trade offs in design, some knowledge sprinkled in from past experience shows up - you might end up learning something new that you didn‚Äôt think of either \n\n\nThe next that counts most is usually behavioral rounds - these showcase scope and maturity from past experience, can‚Äôt count no of times this has led to down leveling simply because the candidate did not prepare for these",
          "score": 1,
          "created_utc": "2026-01-06 06:08:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny17n6b",
          "author": "wqrahd",
          "text": "Those who can explain what they did, how they did, and why they did. \nIf they can explain all these clearly, it stands out.",
          "score": 1,
          "created_utc": "2026-01-06 16:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1pg7h",
          "author": "JJ3qnkpK",
          "text": "You need to understand and frame your technical work in the context of business problems. What business problem did you encounter? How did you use both business and data engineering expertise to design, propose, modify, and execute a solution? How did you ensure your proposed solution *actually* addressed the business problem?\n\nI'm consulting with a company that used a large vendor to design their data solutions. None of those original solutions actually solved the business problems at hand, despite the fact that they handled all of the data, did transformations, designed and scheduled pipelines, and all of the other data engineering fun stuff.\n\nToo many technical and data folk will specify and build something very technical and impressive looking just enough to get greenlit by managers. Then they go and build said thing and deliver the equivalent to a car whose wheels are on the roof. They met all the expectations: an engine, uses gasoline, produces torque through wheels, has seats, etc., but ultimately built something useless to the business.\n\nReviewing that previous group's work, I've found that their stakeholder presentations are full of technical diagrams outlining the tools and technologies they will use, and how cool said tools are, but overall they contain little about how they will help. Using cars as an analogy (again), it's as if one advertised how great the metallurgy is in the car, what electrical connectors it uses, etc.; that kind of detail is ultimately important, the engineers are interested in it and stakeholders are impressed by it, but what *actually* matters is how the car works. Is the car safe, can it accelerate well, does it have good enough mileage, can it accommodate everybody in it, does it do what you need, and can you explain why and how it achieves all of this?\n\nHow do *you* avoid doing the above and ensure your work fulfills business problems? Can you work both domains simultaneously: both understanding the business *and* having the technical expertise to construct data solutions that solve the problem? Can you show your technical expertise while explaining the business problem, and then show your understanding of the business problem while discussing your technical expertise? Can you keep all of this together in a cohesive story?\n\nLastly, can you demonstrate the ability to learn and adapt? Can you learn new business environments/problems and then describe how you would approach finding a technical solution? Can you show that you're not a one-trick pony and have found proper solutions to things? The way my manager framed it: can you be left alone with a client such that your manager can sleep easy, knowing you will properly present and harvest \n\nBeing able to speak both \"languages\", business and technical, puts you far ahead of other candidates. In interviews, I often find myself interviewing the interviewer, asking about the characteristics of the project, discussing their current solutions, and where they're trying to go. Then from there, I describe technical solutions, the needed labor/resources, and how the outcomes of said solutions will solve their business problems. I interweave past experience into this to demonstrate familiarity and competency. Also, don't be a know it all - when you encounter something you don't know, say so and provide your approach to developing understanding - people can tell when you're bullshitting, and dishonesty is always a major liability.\n\nAnyway, that's my two (hundred) cents.",
          "score": 1,
          "created_utc": "2026-01-06 18:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5x0z1",
          "author": "its_PlZZA_time",
          "text": "Couple of trends I have seen:\n\n1) A lot of DE's do not have data modeling skills. They know how to optimize pipelines and save money, but can't even really talk about why the data they are processing is useful, let alone how they would factor in actual business rules when building data marts.\n\n2) Can't go in depth on projects they've worked on. When I ask about projects I'm looking for:\n\n- What the project was\n- Why you were doing it\n- What was the challenge that came up\n- If the challenge was a bug, how did you go about diagnosing it\n- What were some things you did to try to fix it\n- what actually worked.\n\nI basically give people this rubric when I ask the question (as in, I list all the bullet points above). And a lot of answers still don't really cover them.\n\n3) Can't justify decisions or talk about trade-offs. This is for systems design interviews, but a lot of people will be able to talk about scaling and jump to something very large without considering the actual scale of the problem presented. Also, their justifications of tech are \"x scales better\" but they can't describe why.",
          "score": 1,
          "created_utc": "2026-01-07 07:54:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz7ckx",
          "author": "NoleMercy05",
          "text": "Eye color",
          "score": 0,
          "created_utc": "2026-01-06 09:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz7vl3",
          "author": "dev_lvl80",
          "text": "Being friend of VP",
          "score": 0,
          "created_utc": "2026-01-06 09:06:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q37mpm",
      "title": "Again - Take home assignment",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q37mpm/again_take_home_assignment/",
      "author": "Limp-Complaint5817",
      "created_utc": "2026-01-03 22:05:26",
      "score": 84,
      "num_comments": 34,
      "upvote_ratio": 0.95,
      "text": "I am a senior engineer, and although this has been discussed before, I experienced it again recently. I was asked to prepare a presentation for a panel with only two days‚Äô notice. I spent the weekend preparing the slides, attended the final meeting, and presented to six people. The presentation went very well. However, a month later, I was informed by the recruiter that the hiring process had been paused. After that experience, I decided not to accept take-home assignments again.\n\nUnfortunately, I made the same mistake again recently. After a phone screening with fairly basic questions, I was given a take-home assignment. It was described as a prototype, expected to take only a few hours, with up to a week to complete. They also said it didn‚Äôt need to be fully finished, as long as I explained what I would do with more time.\n\nI was genuinely interested in the company, so I spent two full days working on it and submitted what I had. The feedback came back saying it wasn‚Äôt at the level they expected and that more work was needed, so they decided not to move forward. From the comments, it was clearly not a ‚Äúfew hours‚Äù task, it was closer to a full week of work and would require paid cloud resources.  \n\nWhat is your opinion?\n\n\n\n",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q37mpm/again_take_home_assignment/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxilkzv",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-03 22:05:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxim67o",
          "author": "69odysseus",
          "text": "I fucking hate those kinds of interviews as it takes up a lot of our personal time and without pay. I'd reject these companies in full pledge and find something else.¬†",
          "score": 91,
          "created_utc": "2026-01-03 22:08:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiwgib",
              "author": "Limp-Complaint5817",
              "text": "I learnt the lesson in hard way.",
              "score": 8,
              "created_utc": "2026-01-03 23:00:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxir7od",
          "author": "ManonMacru",
          "text": "I have a kid and a full time job. When an interviewer tells me about a take-home assignment I will tell them that I will spend maximum 2h on it and can explain how I would tackle the rest during the panel.\n\nIf it does not work out given 2h, I tell them that I believe they value candidates having free time over candidates having skills, and that unfortunately I do not have free time as I am already employed.\n\nYes, sure I sound arrogant, but since I have adopted that strategy it's less hassle for me, and I make a few recruiters think long and hard about their process.",
          "score": 80,
          "created_utc": "2026-01-03 22:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlgv1k",
              "author": "MadT3acher",
              "text": "I‚Äôm on the same boat as you and do the same.\n\nYou are a parent and have other stuff to do than spending time for recruiters. Personally I almost nope out as soon as I hear ‚Äútake home assignement‚Äù.\n\nAnyway, these tend to disappear because of the LLMs. Or so I hear from junior on our team.",
              "score": 6,
              "created_utc": "2026-01-04 08:52:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxjc3jr",
          "author": "makesufeelgood",
          "text": "I would never do a take home assignment.  And if you and others keep acquescing they will continue to be encouraged to give them.  Let them see a pattern of seemingly high caliber candidates drop out of the running the second the take home assignment hits the table.",
          "score": 17,
          "created_utc": "2026-01-04 00:22:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiv4d4",
          "author": "Backoutside1",
          "text": "Ya I‚Äôll forever deny take home assignments, let‚Äôs do a white board style instead, I don‚Äôt do free labor.",
          "score": 10,
          "created_utc": "2026-01-03 22:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlb11o",
          "author": "CertainShop8289",
          "text": "Take home is a waste of time for candidate quality now anyway. \n\nI much prefer a live assessment, give a candidate a problem, some time to think and design. Then given the AI tool of choice build a prototype explaining design decisions along the way.\n\nCould be the databricks assistant, ChatGPT, Claude code, cursor‚Ä¶ whatever.\n\nHow they think through problems and work alongside AI are better indicators of modern performance",
          "score": 9,
          "created_utc": "2026-01-04 07:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiqmyj",
          "author": "Character-Education3",
          "text": "My opinion is if companies want a take home assignment they pay you a contract rate for x hours. Those hours will allow them to set an expectation for the workload, compensate candidates fairly for their time, and hopefully cause hiring managers to be more selective and not waste theirs and others time.\n\nI know there will be some wanna be founders and hr types who hate this take. But it is fair for everyone",
          "score": 18,
          "created_utc": "2026-01-03 22:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxte5u1",
              "author": "McNoxey",
              "text": "This is such an insane take lmao",
              "score": 1,
              "created_utc": "2026-01-05 13:47:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxiycm3",
          "author": "Toastbuns",
          "text": "I've been on both sides of the table, and it's one of the reasons why when I hire, I don't do take home assignments either‚Äîtotal waste of everyone's time.",
          "score": 6,
          "created_utc": "2026-01-03 23:10:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj3i4p",
          "author": "Ok-Working3200",
          "text": "This is why I OE, Obviously, this is not full proof.  It's sucks how you were treated.  Remember these time when you do get another job and people asking for late nights and other bs.",
          "score": 4,
          "created_utc": "2026-01-03 23:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj8aoo",
          "author": "adgjl12",
          "text": "I don‚Äôt do take homes anymore. I used to before the job market got all crazy and it used to give me a high chance of passing onto the next step or offer. Now it‚Äôs just not worth the time",
          "score": 3,
          "created_utc": "2026-01-04 00:02:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl38ax",
          "author": "solo_stooper",
          "text": "Better than leetcode hard. I‚Äôve spent hundreds of hours of leetcode and still messed it up.",
          "score": 4,
          "created_utc": "2026-01-04 06:51:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqkxxu",
          "author": "goblueioe42",
          "text": "I would prefer take home over live coding personally. But now all I get are these timed take homes that record your eye movement to prevent cheating. Those seem to be the worst.",
          "score": 5,
          "created_utc": "2026-01-05 01:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxku314",
          "author": "A-Walker",
          "text": "All these people saying they refuse to do a take home assignment. Having recently been made redundant and having to find a new job, literally every company I interviewed at asked me to do a take home assignment, if I rejected that I just wouldn‚Äôt have a job.",
          "score": 8,
          "created_utc": "2026-01-04 05:40:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjovcq",
          "author": "rampagenguyen",
          "text": "Instance pass for me if it‚Äôs a requirement for an interview.",
          "score": 3,
          "created_utc": "2026-01-04 01:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkoij8",
          "author": "efermi",
          "text": "It‚Äôs bullshit, but the market dictates what companies can do. Unless everyone says no these companies don‚Äôt get signal that they can‚Äôt use them.",
          "score": 3,
          "created_utc": "2026-01-04 05:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlozss",
              "author": "volkoin",
              "text": "collective action problem",
              "score": 1,
              "created_utc": "2026-01-04 10:06:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxlyil2",
          "author": "CaptainDawah",
          "text": "Just tell them your hourly rate and send them an invoice after\n\nAnd if they refuse after send them to collections üíÄ",
          "score": 3,
          "created_utc": "2026-01-04 11:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlgei6",
          "author": "Ulfrauga",
          "text": "I didn't know this was \"a thing\".  That said, I've been at my place since I joined as a junior, and there was not even a coding test, much less a take-home assignment.  \n\n\nI was curious if this was just something that was unusual in my country.  I've not come across it, or heard about it, but I wouldn't be too surprised if it happened at larger companies, particularly those who are international.  I found mention of it in brief employment law information:\n\n*\"If you are asking an¬†employee¬†for a pre-employment trial, make it clear that performance of any tasks is a part of the interview process, and that the assessment is not paid or rewarded.*\n\n*...*\n\n*For example, it is acceptable to ask a barista to make 2 or 3 coffees, or a waitress to service a table, but working a whole shift, or even a few hours, would likely be considered employment.\"*\n\n  \nI get the need to evaluate a candidate.  But pushing for several hours worth of work does not seem reasonable or ethical - especially to then decline the applicant afterwards.  Opens up the whole idea of \"free labour\".",
          "score": 2,
          "created_utc": "2026-01-04 08:48:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlh7kk",
              "author": "Limp-Complaint5817",
              "text": "They describe the take home assignment as \"typically should not take more than few hours and you can complete it in one week\". To avoid any legal issue. Y",
              "score": 1,
              "created_utc": "2026-01-04 08:55:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxlnqsq",
          "author": "volkoin",
          "text": "it means you should not be interested in that idiots anymore",
          "score": 2,
          "created_utc": "2026-01-04 09:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlt2pw",
          "author": "chatsgpt",
          "text": "One take home assignment for a company I interviewed was actually client work I found out after getting hired. I should have asked to be paid for the work during the interview but was naive and new.",
          "score": 2,
          "created_utc": "2026-01-04 10:43:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs6wbz",
          "author": "BuildingViz",
          "text": "Same experience a few months back. Was told to take 4-5 hours MAX on the assignment, spent maybe 6-8 total, but gave them multiple solutions to the core assignment to demonstrate different approaches. Was told that my solution \"demonstrated a strong understanding of the problem and was communicated clearly\", but that \"some implementation details raised concerns about alignment with best practices\".\n\nLike, yeah, I cranked out multiple solutions in 6 hours, including roping in GCP resources and implementations, plus testing and metrics to show the differences in the approaches, including cost estimates. My focus was the getting multiple functional solutions (plus testing and metrics) to demonstrate the different approaches, not making sure my take-home assignment code, written on a short deadline, was production-ready code.",
          "score": 2,
          "created_utc": "2026-01-05 07:56:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiqjrn",
          "author": "higeorge13",
          "text": "Classic¬†",
          "score": 2,
          "created_utc": "2026-01-03 22:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlqn5t",
          "author": "basis_16",
          "text": "Was this an interview with an Indian company?",
          "score": 1,
          "created_utc": "2026-01-04 10:21:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxluig1",
              "author": "Limp-Complaint5817",
              "text": "No, UK based one.",
              "score": 1,
              "created_utc": "2026-01-04 10:55:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxno4jl",
          "author": "RepresentativeGear88",
          "text": "These take home assignments feel like an underhanded way of getting free consulting work.",
          "score": 1,
          "created_utc": "2026-01-04 17:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxse6lx",
          "author": "sib_n",
          "text": "I have mixed feelings about this because I had a good and bad experience with this.  \n\nI did once, for the first time, and it landed me the best job I have ever had (better company, salary, benefits, team, tech stack, and code quality).  \n\nThe second time, they asked me to complete some OOP code using tests as a reference in a language that I didn't know and that is completely irrelevant to DE. So it took me a lot of time to produce something that I consider representative of a senior's work. Review went well. Then I get a \"decided to proceed with other candidates who more closely match their requirements\", with no further feedback despite asking. So, a massive waste of time and a demonstration of disrespect for candidates.\n\nSo my opinion would be, only accept it if the position looks 100% worth it (like my first case) and you had some good prior *human* contact that confirmed that you were a good match.\nDo not waste your time for some semi-researched \"fast apply\" offer with a 50% match of your expectations.",
          "score": 1,
          "created_utc": "2026-01-05 09:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwdcv8",
              "author": "Limp-Complaint5817",
              "text": "The issue is that, they tell you it is only few hours assignment then questions you of a project that needs 1 week of work, even it costs money.",
              "score": 1,
              "created_utc": "2026-01-05 22:15:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxx90jz",
                  "author": "sib_n",
                  "text": "Definitely do not spend any money specifically for a recruitment process.",
                  "score": 1,
                  "created_utc": "2026-01-06 00:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxylfmt",
          "author": "[deleted]",
          "text": "I would do a LOT of research on an organization before I would put much time into providing any technical solutions to problems provided during hiring process, with one BIG exception.  That exception is in-person interviews and technical discussions with whiteboarding or even (limited) example coding at the interview.  \n\nIn-person live interviews show that the company is willing to invest the time of their \\_own\\_ personnel hours to match the time they are asking you to invest.",
          "score": 1,
          "created_utc": "2026-01-06 05:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk1g1e",
          "author": "eeshann72",
          "text": "New way of getting the work done for free. Indians are smart",
          "score": 1,
          "created_utc": "2026-01-04 02:41:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyr86l",
      "title": "One Tool/Skill other than SQL and Python for 2026",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pyr86l/one_toolskill_other_than_sql_and_python_for_2026/",
      "author": "crushed_peppe",
      "created_utc": "2025-12-29 16:48:13",
      "score": 60,
      "num_comments": 23,
      "upvote_ratio": 0.93,
      "text": "If you had to learn one tool or platform beyond SQL and Python to future-proof your career in 2026, what would it be?\n\nI‚Äôm a Senior Database Engineer with 15+ years of experience, primarily in T-SQL (‚âà90%) with some C#/.NET. My most recent role was as a Database Engineering Manager, but following a layoff I‚Äôve returned to an individual contributor role.\n\nI‚Äôm noticing a shrinking market for pure SQL-centric roles and want to intentionally transition into a Data Engineering position. Given a 6-month learning window, what single technology or platform would provide the highest ROI and best position me for senior-level data engineering roles?\n\nEdit: Thank you for all your responses. I asked ChatGPT and this is what it thinks I should do, please feel free to critic:\n\nGiven your background and where the market is heading in 2026, **if I had to pick exactly one tool/skill beyond SQL and Python**, it would be:\n\n# Apache Spark (with a cloud-managed flavor like Databricks)\n\nNot Airflow. Not Power BI. Not another programming language. **Spark.**",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pyr86l/one_toolskill_other_than_sql_and_python_for_2026/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwkkgoe",
          "author": "AutoModerator",
          "text": "Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-29 16:48:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwknvbf",
          "author": "Thanael124",
          "text": "Cloud stack, IaC and CI/CD.",
          "score": 72,
          "created_utc": "2025-12-29 17:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkq3tk",
              "author": "Beautiful-Hotel-3094",
              "text": "Correct answer. Basically: everything",
              "score": 22,
              "created_utc": "2025-12-29 17:14:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwuxbq7",
              "author": "JBalloonist",
              "text": "Yep, this is my answer. Learned IaC and CI/CD mostly in the last year at a previous role and made my new role so much easier.",
              "score": 2,
              "created_utc": "2025-12-31 04:10:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwl05iz",
          "author": "mrbartuss",
          "text": "people skills",
          "score": 26,
          "created_utc": "2025-12-29 18:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl8rp7",
              "author": "PaulSandwich",
              "text": "Unironically, yes.   \n   \nEqually/more important, business skills.   \nAI is weakest at understanding business nuance and where the operational value in data is. Writing and optimizing basic pipelines is trivial these days.    \n   \nTurning data into KPIs that make or save your employer money will never go out of style and, thankfully, it's where AI fails the hardest.",
              "score": 6,
              "created_utc": "2025-12-29 18:41:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqc3uy",
          "author": "analyticspitfalls",
          "text": "I am going old school. Data modeling. Both Kimball and Inmon methods.  \n\nIf you can be a part of building Stupidly simple data for you and others to use. You will be a ninja!!!\n\nI would bet less than 5% of the people I have worked with know this dark art - and those that do are dangerous.",
          "score": 22,
          "created_utc": "2025-12-30 14:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr0kpi",
              "author": "bugtank",
              "text": "This is the way",
              "score": 3,
              "created_utc": "2025-12-30 16:08:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwklcp2",
          "author": "MikeDoesEverything",
          "text": ">I‚Äôm noticing a shrinking market for pure SQL-centric roles and want to intentionally transition into a Data Engineering position\n\nIn my opinion, it has been like that since I became a DE which was around 5 years ago.\n\n>Given a 6-month learning window, what single technology or platform would provide the highest ROI and best position me for senior-level data engineering roles?\n\nYou'd fit well within an organisation which is Microsoft leaning.  Pick up the Azure stack as you go as your skills slot directly into it.\n\nI have worked with a lot of people who have your exact stack and they all have what you have which is a lot of T-SQL and not a huge amount of general purpose programming.  This is both good and bad.  Good because if you applied to a Microsoft heavy company, you're probably going to get in.  Bad because it means everybody on your team is likely to be like you i.e. not a huge pool of skills to learn from.  If that isn't a problem for you, then that's cool.  The job advertised is also likely to be on prem only.  Again, depends what you want to do on a daily basis as there are going to be some on-prem only jobs with your stack which pay really well.\n\nIf you want to get away from a Microsoft stack, brush up on DE and cloud fundamentals as well as pick up Python.",
          "score": 13,
          "created_utc": "2025-12-29 16:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkz91v",
              "author": "aisakee",
              "text": "This, the azure stack is a no-brainer if you want to land a job quickly. Most big companies (no startups nor tech companies) use the Microsoft package so they by default go to Azure for everything.",
              "score": 3,
              "created_utc": "2025-12-29 17:57:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwku2x8",
          "author": "beiendbjsi788bkbejd",
          "text": "Docker",
          "score": 6,
          "created_utc": "2025-12-29 17:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkkg83",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 3,
          "created_utc": "2025-12-29 16:48:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu8rxn",
          "author": "dev_lvl80",
          "text": "I was purely TSQL dev/dba decade ago. Totally understand what are you trying to do.\n\nMy 2c on core skills in demand\n\n\\- SQL (absolutely must wit deep level of expertise in optimization)\n\n\\- Python (leet code medium level)\n\n\\- Databricks + PySpark or Snowflake\n\n\\- DBT/Airflow \n\n\\- Data Modeling (surprise)\n\n\\- A bit AI/ML\n\nPS in you case Databricks/Pyspark looks about right",
          "score": 2,
          "created_utc": "2025-12-31 01:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkwq4z",
          "author": "ScholarlyInvestor",
          "text": "I‚Äôd recommend reading up on Fundamentals of Data Engineering itself. Maybe start with the book by Reis/Housley. There are many tools in the marketplace but betting on Databricks or Snowflake besides boning up on cloud stack will help.",
          "score": 2,
          "created_utc": "2025-12-29 17:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl15b1",
          "author": "exjackly",
          "text": "Future proof is going to require cloud.  Pick a stack - AWS is the leader, but I've been seeing more companies choose Azure (good with your T-SQL background) and GCP.\n\nStart with the portions that match your experience (the databases and cloud storage) and start working your way out to the data transformers, pipelines/orchestrators, CI/CD.  \n\nIaC isn't something to ignore, but it shouldn't be a priority until you are comfortable with the rest unless you wind up at a mid-small place that doesn't have dedicated infra roles.",
          "score": 2,
          "created_utc": "2025-12-29 18:06:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpgcgg",
          "author": "jjopm",
          "text": "SQL and Python",
          "score": 2,
          "created_utc": "2025-12-30 10:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpr5w7",
          "author": "asevans48",
          "text": "Start with the cloud, data streaming and automation, and basic data quality tools. Your stack sounds legacy. Ive written 1 ssis package with a c# program in 4 years even with sql server. Get some cloud certs to start. Also, data governance is becoming the hot topic. I am pushing hard to avoid the literal budget destruction of microsoft fabric with open source tools. Fabric-like patterns and AI tools will be prevalent. Autonomous systems might be a thing. A fabric cert is probably a good idea. CDC is still a good skill to have. Personally leaning into AI and ml education and certs. We are close to self-service data analytics. In saas land api automation is key. Less and less folks offer free or cheap.database replication.",
          "score": 1,
          "created_utc": "2025-12-30 11:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqxjrq",
          "author": "Immediate-Pair-4290",
          "text": "If AI is supposedly going to replace coders then you need to be able to turn business problems into solutions. No need to learn another language. Tools like DuckDB make Spark obsolete for the majority of companies. It‚Äôs not really that different from SQL anyway other than the storage formats.",
          "score": 1,
          "created_utc": "2025-12-30 15:53:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpagmo",
              "author": "Ok-Butterscotch6249",
              "text": "Good call out regarding AI and needing to be able to detect and decode business problems into requirements. However I don‚Äôt follow how DuckDB will replace Spark? Can you elaborate please?",
              "score": 1,
              "created_utc": "2026-01-04 21:48:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxpgavi",
                  "author": "Immediate-Pair-4290",
                  "text": "Because the majority of companies don‚Äôt have big data. Spark is overkill for 95% of businesses leading to overpriced costs and slower performance for analytics. Most are too infatuated with the features of Databricks to realize its compute is a poor fit. DuckDB is helping to expose this.",
                  "score": 1,
                  "created_utc": "2026-01-04 22:15:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwku43q",
          "author": "BrupieD",
          "text": "There will always be a market for what I think of as \"last mile\" tools. That is, tools that transform raw data into the consumed form. If management/end users in your company likes Excel, maybe VBA is still a good choice. If people want reports and dashboards, PowerBi might be it.",
          "score": -6,
          "created_utc": "2025-12-29 17:34:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqd9j7",
              "author": "Budget-Minimum6040",
              "text": "> maybe VBA is still a good choice\n\nYou can't be serious.\n\nHe/she wants to go from DBA to DE. VBA is a piece of shit you won't touch with both careers.",
              "score": 9,
              "created_utc": "2025-12-30 14:08:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q04miu",
      "title": "Fellow DEs ‚Äî what's your go-to database client these days?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q04miu/fellow_des_whats_your_goto_database_client_these/",
      "author": "SainyTK",
      "created_utc": "2025-12-31 05:10:47",
      "score": 60,
      "num_comments": 64,
      "upvote_ratio": 0.89,
      "text": "Been using DBeaver for years. It gets the job done, but the UI feels dated and it can get sluggish with larger schemas. Tried DataGrip (too heavy for quick tasks), TablePlus (solid but limited free tier), Beekeeper Studio (nice but missing some features I need).\n\nWhat's everyone else using? Specifically interested in:\n\n* Fast schema exploration\n* Good autocomplete that actually understands context\n* Multi-database support (Postgres, MySQL, occasionally BigQuery)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q04miu/fellow_des_whats_your_goto_database_client_these/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwv7lsq",
          "author": "PatientlyAnxiously",
          "text": "DBeaver is my go to for multi database support. The only others I use are specific to one system: SSMS for MS SQL Server and Snowsight web UI for Snowflake.",
          "score": 66,
          "created_utc": "2025-12-31 05:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwzryh",
              "author": "SmallAd3697",
              "text": "Yes, for best experience use the client provided by vendor.  No generic client will give a better experience than one that is tailored for a particular database engine.  Ssms is tailored to SQL and azure SQL and has lots of auth mechanisms for connecting to databases (as one simple example).\n\nWondering about the OP question itself.  I think certain DE's don't want to invest in learning multiple tools.  Or they have hate for a vendor (msft) and use that vendor's tools as little as possible.  In that case you are making a deliberate compromise, and that is a totally acceptable path as well.",
              "score": 6,
              "created_utc": "2025-12-31 14:11:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv6tgv",
          "author": "vikster1",
          "text": "dbeaver works. dbeaver is free. dbeaver does not try to shove ai shit down my throat. as far as I'm concerned, dbeaver is the peak of software in 2025 and most likely in 2026 as well but maybe something big changes in the next hours. idk. I'm not an oracle.",
          "score": 107,
          "created_utc": "2025-12-31 05:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvghrv",
              "author": "SRMPDX",
              "text": "You may not be an oracle, but you are a sequel server.",
              "score": 19,
              "created_utc": "2025-12-31 06:29:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwydgnc",
                  "author": "AlGoreRnB",
                  "text": "I prefer the og server tbh. I find the sequel to be shallow and pedantic.",
                  "score": 1,
                  "created_utc": "2025-12-31 18:24:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwv9st7",
              "author": "ZirePhiinix",
              "text": "Oracle DB is pretty mid. It's not really bad, but god damn I hate that I can't add a column to a `select *` without giving my table an alias.",
              "score": 3,
              "created_utc": "2025-12-31 05:36:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwwn3j5",
              "author": "Budget-Minimum6040",
              "text": "DBeaver is awesome but it only supports ODBC/JDBC which means anything else (like BigQuery dryrun giving totalBytesProcessed information on how much data = $$$ your query will cost before you send it) is not possible.\n\nSee this 6 year old issue which is still open: https://github.com/dbeaver/dbeaver/issues/4907\n\nSo DBeaver is a bad choice for DBs where you pay per queried data and developing in the browser is pure shitshow. I haven't found a solution for a proper SQL IDE that supports such cloud DBs so far ...",
              "score": 1,
              "created_utc": "2025-12-31 12:50:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx41mpi",
                  "author": "querylabio",
                  "text": "Have you tried Querylab.io?\n\nFull disclosure: I‚Äôm the founder.\n\nWe‚Äôre building a BigQuery-native IDE (no JDBC/ODBC). Beyond a clean, modern UI, we already support:\n\n* dry-run with per-query limits and daily / weekly / monthly budgets\n* per-CTE cost breakdown\n* partial execution & cost estimation for selected CTEs\n* TABLESAMPLE dev mode for cheap iteration\n* on-demand vs reservation cost comparison\n* proper handling of nested & repeated fields\n* diagnostics for expensive patterns like SELECT \\*\n* full Pipe Syntax support\n* and many more\n\nHappy to get feedback!",
                  "score": 2,
                  "created_utc": "2026-01-01 17:57:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwr975",
              "author": "Ok-Improvement9172",
              "text": "No vi mode though",
              "score": 1,
              "created_utc": "2025-12-31 13:19:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzv3eb",
                  "author": "Daemoncoder",
                  "text": "In Dbeaver? - Use Vrapper.",
                  "score": 1,
                  "created_utc": "2025-12-31 23:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvd2w6",
          "author": "addictzz",
          "text": "DBeaver. It is ugly, I don't like the interface. But it works everytime and it is free. No ads or request to upgrade to Pro version disrupting my workflow.",
          "score": 33,
          "created_utc": "2025-12-31 06:01:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnpj7",
              "author": "SainyTK",
              "text": "What databases do you connect using DBeaver?",
              "score": 2,
              "created_utc": "2025-12-31 07:31:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvp6z0",
                  "author": "addictzz",
                  "text": "Mysql, postgres, databricks",
                  "score": 2,
                  "created_utc": "2025-12-31 07:45:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv9dk4",
          "author": "SirGreybush",
          "text": "Visual Studio Code",
          "score": 42,
          "created_utc": "2025-12-31 05:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxdsmm",
              "author": "The_Wanderer33",
              "text": "Interesting tell me more‚Ä¶",
              "score": 3,
              "created_utc": "2025-12-31 15:28:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxmhlj",
                  "author": "ask-the-six",
                  "text": "There‚Äôs extensions for basically any database. Really convenient to make a devcontainer with all the tools installed needed per project. One example: \n\nhttps://marketplace.visualstudio.com/items?itemName=mtxr.sqltools",
                  "score": 6,
                  "created_utc": "2025-12-31 16:11:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwy10v9",
                  "author": "SirGreybush",
                  "text": "It‚Äôs made by Microsoft but is open source and has add-ons for everything, database types, Snowflake, Python, etc.",
                  "score": 1,
                  "created_utc": "2025-12-31 17:23:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvbc84",
          "author": "Few_Noise2632",
          "text": "datagrip. i have all products pack and it is pretty cheap together with all the other stuff from jetbrains (total is 180$ after 3 years of sub)\n\ndbeaver is good enough for some people but i can't afford to spend my eyes resource on that ugliness",
          "score": 38,
          "created_utc": "2025-12-31 05:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvg79f",
              "author": "BeardedYeti_",
              "text": "This should be the only answer.",
              "score": 9,
              "created_utc": "2025-12-31 06:26:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvhfgz",
                  "author": "randomName77777777",
                  "text": "Yeah, datagrip all the way. \n\nI now spend most of my days on databricks, but only because I haven't found a good way to connect it to datagrip. But for all my other data sources I use datagrip - azureSQL, big query, redshift, postgres.",
                  "score": 5,
                  "created_utc": "2025-12-31 06:36:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwzrlgk",
              "author": "scallion_2",
              "text": "You can set up git projects in DataGrip too. I work with multiple SQL repos so this is a huge benefit imo.",
              "score": 2,
              "created_utc": "2025-12-31 22:55:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvrcap",
          "author": "MichelangeloJordan",
          "text": "This is my installation of DBeaver. There are many like it, but this one is mine.",
          "score": 8,
          "created_utc": "2025-12-31 08:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvduzh",
          "author": "blueadept_11",
          "text": "Dbvisualizer for 15 years now",
          "score": 6,
          "created_utc": "2025-12-31 06:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo324",
              "author": "SainyTK",
              "text": "Interesting choice. Very solid and good looking UI. $229 one-time purchase.",
              "score": 3,
              "created_utc": "2025-12-31 07:35:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwxibfl",
                  "author": "DiabolicallyRandom",
                  "text": "Their support is excellent, very responsive, and they add features based on customer request regularly, fix bugs, etc.\n\nI used it for several years at my last gig (at my urging, the company bought licenses for our team), and they were always very helpful.\n\nThe only \"drawback\" is being Java/JDBC based, but that gives you the entire world of possible database drivers, so more of a boon IMO.",
                  "score": 4,
                  "created_utc": "2025-12-31 15:50:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwx9yxg",
              "author": "Askew_2016",
              "text": "I loved DBVisualizer but my company decommissioned it so I‚Äôm using DBBeaver now",
              "score": 3,
              "created_utc": "2025-12-31 15:08:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvom4x",
          "author": "RemcoE33",
          "text": "Look at Beekeeper",
          "score": 6,
          "created_utc": "2025-12-31 07:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww7ok1",
              "author": "firebypeace",
              "text": "I love working with Beekeeper. Paying for it helps as I like using it for Duckdb things. I'd recommend it",
              "score": 1,
              "created_utc": "2025-12-31 10:40:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww7xm7",
                  "author": "RemcoE33",
                  "text": "Yeah I pay as well. Love the project, the speed of development and the amount of db's it supports. I do use it a lot with duckdb, SQLite and Bigquery.",
                  "score": 1,
                  "created_utc": "2025-12-31 10:42:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvmky4",
          "author": "dataflow_mapper",
          "text": "I still see a lot of people settle back on DBeaver despite the complaints, mostly because it is the least bad all around option. The UI is clunky, but the schema explorer and cross database support are hard to beat once you tune it a bit.\n\nWhat has helped me more than switching clients is changing how I use them. Smaller result set limits by default, fewer auto refreshes, and leaning on the SQL editor instead of clicking around the tree constantly. That alone fixes most of the sluggish feeling.\n\nI have not found a single tool that nails fast exploration, smart autocomplete, and wide database support without tradeoffs. Most teams I know end up with one main client and a lighter secondary one for quick checks, rather than trying to force one tool to do everything.",
          "score": 4,
          "created_utc": "2025-12-31 07:21:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvxzt5",
          "author": "m915",
          "text": "Usually VS code extensions",
          "score": 4,
          "created_utc": "2025-12-31 09:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwa1t8",
          "author": "LargeSale8354",
          "text": "I liked Aquafold DataStudio. It was hell getting management to pay for licenses. \nDBeaver is OK.\nBasically,  the choice is \"What free development IDE\" will management allow?\" Not, \"What IDE allows our staff to be most productive?\"",
          "score": 3,
          "created_utc": "2025-12-31 11:02:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx140f",
              "author": "NoResolution4706",
              "text": "Using this also, my whole team is. Really does everything I need from it.",
              "score": 2,
              "created_utc": "2025-12-31 14:19:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwxcht",
          "author": "IckyNicky67",
          "text": "I‚Äôm surprised no one‚Äôs mentioned PyCharm yet. It has a great interface and it makes it so easy to switch from SQL/databases to Python (or whatever programming languages you tend to use besides of SQL)\n\nEDIT: Forgot to add that it has all three of your requirements",
          "score": 3,
          "created_utc": "2025-12-31 13:57:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxw0r7",
          "author": "AcanthisittaMobile72",
          "text": "pgAdmin or vscode/vscodium extension",
          "score": 3,
          "created_utc": "2025-12-31 16:58:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwybrw5",
          "author": "dirks74",
          "text": "Navicat Premium",
          "score": 3,
          "created_utc": "2025-12-31 18:16:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxj5p0e",
              "author": "bjust-a-girl",
              "text": "I scrolled to find this response!",
              "score": 2,
              "created_utc": "2026-01-03 23:48:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvc0qm",
          "author": "thickmartian",
          "text": "Yeah I'm using TablePlus.\n\nHappy with it. It does enough. I get most of the info (schema etc ...) I need from SQL queries anyways.\n\nAt least it's relatively pleasing on the eye...",
          "score": 3,
          "created_utc": "2025-12-31 05:53:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnrdq",
              "author": "SainyTK",
              "text": "Do you pay for TablePlus or just use a free version of it?",
              "score": 1,
              "created_utc": "2025-12-31 07:32:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvuodg",
          "author": "james2441139",
          "text": "Data architect here, but I do a fair bit of pipeline engineering as well. We are a fully MS shop, so primary setup is Synapse and MS Fabric. I have been using DB Schema Pro, and found it really useful for data modeling, exploration, design, documentation. It connects to all major databases, has fast schema exploration. Doesn't have autocomplete in the sense of something like Intellisense, but that is not important for me.\n\nTons of tools out there, even VSCode has quite a few extensions. I settled on this for now, and focusing my productivity on actual data modeling rather than tools.",
          "score": 5,
          "created_utc": "2025-12-31 08:37:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvuadv",
          "author": "Sad_Cell_7891",
          "text": "try OmniDB",
          "score": 2,
          "created_utc": "2025-12-31 08:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvxey2",
          "author": "bjatz",
          "text": "NiFi ExecuteSQL processor",
          "score": 2,
          "created_utc": "2025-12-31 09:03:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvyrff",
          "author": "Alternative-Guava392",
          "text": "Dbeaver",
          "score": 2,
          "created_utc": "2025-12-31 09:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx0nrx",
          "author": "k00_x",
          "text": "Atom. It's legacy and out of date but was great. Microsoft nerfed it so it didn't compete with vscode when they acquired GitHub.",
          "score": 2,
          "created_utc": "2025-12-31 14:16:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx2eyx",
          "author": "s-to-the-am",
          "text": "Datagrip",
          "score": 2,
          "created_utc": "2025-12-31 14:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxjst7",
          "author": "Awkward_Tick0",
          "text": "Does nobody use ssms anymore‚Ä¶?",
          "score": 2,
          "created_utc": "2025-12-31 15:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzdeas",
              "author": "Cupakov",
              "text": "I do but not by choice¬†",
              "score": 1,
              "created_utc": "2025-12-31 21:35:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxkub0",
          "author": "5pitt4",
          "text": "Datagrip community version",
          "score": 2,
          "created_utc": "2025-12-31 16:03:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy2s1v",
          "author": "IAmBeary",
          "text": "im going to get laughed at but Im using mysqlworkbench \n\nmy favorite feature is when it crashes\n\nluckily we are transitioning towards blob storage for a datalake so the app collects dust most of the time. I already have my profiles set up so there's a cost to switching clients",
          "score": 2,
          "created_utc": "2025-12-31 17:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0k85k",
          "author": "jayzfanacc",
          "text": "I use Azure Data Studio because I like that it feels like VS Code. I will switch to VS Code when Azure Data Studio gets deprecated.",
          "score": 2,
          "created_utc": "2026-01-01 01:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx130zd",
          "author": "Suspicious_East591",
          "text": "Datagrip with license some companies offer us license to use that but I see other teammates who prefer using dbeaver",
          "score": 2,
          "created_utc": "2026-01-01 03:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvkf8c",
          "author": "West_Good_5961",
          "text": "Really depends on the dbms. Currently using VScode extensions for everything because it‚Äôs the only application we‚Äôre allowed to install.\n\nDb Forge is very good and probably meets your requirements.",
          "score": 2,
          "created_utc": "2025-12-31 07:02:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvmups",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2025-12-31 07:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvojnx",
              "author": "SainyTK",
              "text": "For those who come across and aren't happy with DBeaver like me, you may consider trying https://sheeta.ai.\n\nFor transparency, I'm the builder of it. I know that \"AI\" is something prohibited here, but 90% of this app is not about AI. It's just another SQL client with cleaner UI with fully functional good features inspired by best tools we all know. All non-AI features are completely free.\n\nSo, please feel free to give it a chance and do let me know your thoughts.",
              "score": -20,
              "created_utc": "2025-12-31 07:39:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwjxnj",
                  "author": "soluto_",
                  "text": "Ruined an otherwise good thread.",
                  "score": 3,
                  "created_utc": "2025-12-31 12:26:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvjyfx",
          "author": "burningburnerbern",
          "text": "You‚Äôre gonna hate me but I just use the web UI",
          "score": 0,
          "created_utc": "2025-12-31 06:58:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyen4a",
      "title": "Kafka - how is it typically implemented ?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pyen4a/kafka_how_is_it_typically_implemented/",
      "author": "AdFormal9428",
      "created_utc": "2025-12-29 06:13:47",
      "score": 55,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "Hi all,\n\nI want to understand how Kafka is typically implemented in a mid sized company and also in large organisations.\n\nStreaming is available in Snowflake as a Streams and Pipes (if I am not mistaken) and presume other platforms such as AWS (Kinesis) Databricks provide their own version of streaming data ingestion for Data Engineers.\n\nSo what does it mean to learn Kafka ? Is it implemented separately outside of the tools provided by the large scale platforms (such as Snowflake, AWS, Databricks) and if so how is it done ?\n\nAsking because I see Joh descriptions explicitly mention Kafka as a experience requirement while also mentioning Snowflake as required experience . What exactly are they looking at and how is it different to know Snowflake streams and separately Kafka.\n\nIf Kafka is deployed separately to Snowflake / AWS / Databricks, how is it done? I have seen even large organisations put this as a requirement.\n\nTrying to understand what exactly to learn in Kafka, because there are so many courses and implementations - so what is a typical requirement in a mid to large organization ?\n\n  \n\\*Edit\\* - to clarify - I have asked about streaming, but I meant to also add Snowpipe. ",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pyen4a/kafka_how_is_it_typically_implemented/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwi3esg",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-29 06:13:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi4bsj",
          "author": "Abdul_DataOps",
          "text": "Think of Kafka as the Central Nervous System of the enterprise while Snowflake/Databricks are the Brain (Storage/Compute).\n\n1. Is it deployed separately?\nYs. In 99% of large organizations Kafka is a completely separate standalone cluster. It usually lives in its own VPC on AWS (MSK) or is managed via Confluent Cloud. It sits upstream from your data warehouse.\n\n2. Why use Kafka if Snowflake has Streams?\nBecause Snowflake Streams are proprietary. If u use Snowflake Streams you are locked into Snowflake. But a large enterprise has 50 other systems that need that data (Microservices, Real-time Dashboards, ML Models, Fraud Detection). None of those can read from a Snowflake Stream efficiently in real-time.\n\n3. Typical Implementation Pattern\nSource (App/DB) -> [Kafka Producer] -> Kafka Topic -> [Kafka Connect / Snowpipe] -> Snowflake\n\nu learn Kafka to handle the transport layer. \nu learn Snowflake Streams to handle the ingestion/transformation layer once the data lands. They are complementary not competitive.",
          "score": 97,
          "created_utc": "2025-12-29 06:21:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiea66",
              "author": "poinT92",
              "text": "Solid answer",
              "score": 9,
              "created_utc": "2025-12-29 07:47:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwmbywp",
                  "author": "CulturMultur",
                  "text": "Kafka and Snowflake Streams are completely different technologies with different semantics and reasoning.",
                  "score": 3,
                  "created_utc": "2025-12-29 21:50:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwmlrlu",
              "author": "aisakee",
              "text": "Are all orgs required to use stream jobs? I mean, many JDs contains Kafka as a req but since I started as DE I've never had to use streams.. which cases are optimal for this technology if you're not a FAANG?",
              "score": 2,
              "created_utc": "2025-12-29 22:39:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj8yrc",
          "author": "addictzz",
          "text": "Kafka is basically a big buffer for your immense influx stream of data so that your data consumers do not get overwhelmed.\n\nWhen it is listed as required skills, it is either requiring you to manage and fine-tune Kafka clusters (if the company is not using a managed service Kafka already), develop Kafka connectors/sinks/sources, or require you to develop Kafka consumers.",
          "score": 9,
          "created_utc": "2025-12-29 12:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwie29u",
          "author": "robverk",
          "text": "Kafka is used to decouple your processing. Ingest just reads and pushes messages in. Now 5 processes that need to read raw source data can read that topic. One of them can clean, normalize, parse and push into the next topic. Etc\n\nYou van build high performant but very flexible chains of processing while each step can be a simple unit of: input - process - output. If you need more performance for processing a topic you can just add more consumers/producers so it fits well within distributed processing frameworks.",
          "score": 4,
          "created_utc": "2025-12-29 07:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjmoh3",
          "author": "jduran9987",
          "text": "In most cases you aren‚Äôt touching a Kafka cluster as a data engineer. You typically have SWE or platform folks managing everything. I would just focus on ingesting events from a topic either by sending them to S3 or writing a consumer. At some point, those ingested events are stored in a Snowflake table.",
          "score": 4,
          "created_utc": "2025-12-29 13:55:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjs8a8",
              "author": "AdFormal9428",
              "text": "Thank you. What technologies do I need to learn to be able to \"sending them to S3 or writing a consumer\" ?",
              "score": 1,
              "created_utc": "2025-12-29 14:27:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuxvzs",
          "author": "Front-Ambition1110",
          "text": "Kafka is a pub/sub system, for decoupling producers and consumers, with a very robust ecosystem (Kafka Connect, KSQL, etc). Data streaming/replication/CDC is just one use of Kafka. I imagine most Kafka uses are not oriented to data engineering, but more publisher-broker-consumer backend apps.\n\n\"Kafka experience requirement\" is I guess more about using, configuring, and maintaining a clustered pub/sub system. It's worth learning (partitions, consumer groups, replicas, retention and cleanup policy, etc), but DON'T rush to implement it in prod! This thing is a beast",
          "score": 2,
          "created_utc": "2025-12-31 04:14:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi7noz",
          "author": "AdFormal9428",
          "text": "Thank you u/Abdul_DataOps\n\nAre there any tools or platforms that help with implementation of Kafka ? For analogy, DBT provides a way to write SQL transformations, Databricks allows easier implementation of Spark clusters etc. - similarly are there tools for Kafka or is it pure open source implementation? If tools exist - what are the tools typically used? You have mentioned MSK - I assume this is such a tool ?\n\nAlso, in what way is a Data Engineer implements Kafka? Because typically when I say Data Engineer I think of Analytics / Data Provisioning for ML etc. - essential for the data platform - and for this do Snow pipes and other such platform tools help?\n\nDo Data Engineers also build Kafka pipelines for consumption by other software applications such as microservice ? Or do the Software Devs do it themselves?",
          "score": 2,
          "created_utc": "2025-12-29 06:49:24",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwjkdq8",
              "author": "userousnameous",
              "text": "Kafka at scale gets complex. It wouldn't be data engineering, it would likely be a competent software engineering team, followed by ongoing adding of services ( integration with company auth, management and alerting capabilities, scaling).  You could reduce the burden somewhat with a AWS/GCP/Azure offering, but someone is going to have to be involved and cognizant of that system, upkeep and cost management.",
              "score": 2,
              "created_utc": "2025-12-29 13:42:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwk3gwg",
                  "author": "AdFormal9428",
                  "text": "Thank you. When I watch YouTube videos, Data Engineers specify Kafka as a technology to learn. (videos listing tech. to learn and not getting in depth).\n\nI wonder what they mean when they say Kafka. Like what programming language + specific library to learn as a Data Engineer. What tech tools to learn specifically focus for it.",
                  "score": 2,
                  "created_utc": "2025-12-29 15:26:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpg9pr",
              "author": "Exciting_Tackle4482",
              "text": "Tools like [Lenses.io](http://Lenses.io) (note: I work for them) will help simplify operating Kafka as as a data/software engineer.  They offer an operating plane for your Kafka estate.  Including to do data exploration, integration & transformation.  \n\nTo answer your last question, the idea is to make the end user (ie. software engineer in your case) as autonomous to build data pipelines in Kafka with the likes of Lenses.  Check out: [https://www.youtube.com/watch?v=Z4yeQFyZ75Y&t=131s](https://www.youtube.com/watch?v=Z4yeQFyZ75Y&t=131s)",
              "score": 1,
              "created_utc": "2025-12-30 10:00:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlf8lj",
          "author": "No_Song_4222",
          "text": "Meaning to learn Kafka can go from everything to everything.  However irrespective of your level of choice of depth the following is my opinion : \n\n1. Why it was built ? How is it different than a typical pub/sub. \n\n2. Topics, partitions, producers and consumer. Typical things like how exactly once delivery, ordering etc just know them it is good. \n\nHow deep ? On most occasions as a DE you just consume the data and dump everything in a storage layer ( the single source of truth).   \n\n\nif your job descriptions asks you to setup a cluster, manage, work at a  source level, fine tuning, making changes and coding in Java etc. You need a lot more depth in  distributed systems and understanding in Java and Scala to debug performance issues and stuff.",
          "score": 1,
          "created_utc": "2025-12-29 19:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpfmr8",
          "author": "eccentric2488",
          "text": "There is a difference between open source frameworks and managed services for these open source frameworks.",
          "score": 1,
          "created_utc": "2025-12-30 09:54:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiy6vl",
          "author": "RangePsychological41",
          "text": "If someone asked me that in real life I‚Äôd immediately ask them ‚Äúwhat is Kafka?‚Äù Because, in all likelihood they don‚Äôt know. Which makes the point of question‚Ä¶ questionable.",
          "score": 0,
          "created_utc": "2025-12-29 10:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi53z2",
          "author": "West_Good_5961",
          "text": "Unnecessary",
          "score": -6,
          "created_utc": "2025-12-29 06:27:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwihemd",
              "author": "Doto_bird",
              "text": "You can't say that. In fact you cant say that about most tech out there. Remember all things were built to solve a specific problem. Maybe that tech doesn't generalize as well to other problems, but it doesn't mean there isn't some niche use case it will work really well with.\n\nIn terms of Kafka, very few services scale and integrate as well with things like Flink so that you can handle insane volumes while maintaining consistent throughput. Sure, there are managed cloud solutions thst can probably do the same, but you'll be surprised how many enterprises still have massive on-prem clusters that they need to use until EOL to get value of of their investment.\n\nKafka is, however, unnecessary if you're running a small 10tps streaming service. There are easier ways to handle that.",
              "score": 2,
              "created_utc": "2025-12-29 08:16:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q13vjk",
      "title": "Best certificates nowadays for Data Engineers?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q13vjk/best_certificates_nowadays_for_data_engineers/",
      "author": "Irachar",
      "created_utc": "2026-01-01 13:10:46",
      "score": 51,
      "num_comments": 23,
      "upvote_ratio": 0.89,
      "text": "What are the best certificates to earn this 2026 as a FREELANCE DE?\n\n  \nI assume from AWS and Azure for sure.\n\n\\*Azure has the DP-700 (Fabric Data Engineer) as a new standard?\n\n  \nWhat about the rest? Databricks, dbt, snowflake, something in LLM maybe?\n\n",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q13vjk/best_certificates_nowadays_for_data_engineers/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx2q1oo",
          "author": "Wingedchestnut",
          "text": "Any Databricks/ Snowflake, Azure/AWS certification your company or client wants you to have.",
          "score": 53,
          "created_utc": "2026-01-01 13:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2qk1c",
              "author": "Irachar",
              "text": "Well I‚Äôm freelance, so my clients are basically all the market possible.",
              "score": 8,
              "created_utc": "2026-01-01 13:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3ifw1",
                  "author": "mrbartuss",
                  "text": "Then you answered your question, didn't you?",
                  "score": 16,
                  "created_utc": "2026-01-01 16:16:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4l547",
                  "author": "protonchase",
                  "text": "How do you find work as freelance?",
                  "score": 3,
                  "created_utc": "2026-01-01 19:33:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2qwj9",
          "author": "SnooGiraffes7113",
          "text": "If there are certain companies you want to work for then find out what tools they use and get those certs. If not, then any big 3 cloud cert would be good. Other than those, there is high demand for dbt, snowflake and databricks. You can try to pick those certs. Those should give you a base and also show that you understand cloud, can use databases and transform data. Finally, python and SQL are the gold standard for languages to know for DEs.",
          "score": 11,
          "created_utc": "2026-01-01 13:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgby6l",
              "author": "NIKINIKITA",
              "text": "Which are the big 3?",
              "score": 1,
              "created_utc": "2026-01-03 15:38:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgoby8",
                  "author": "SnooGiraffes7113",
                  "text": "AWS, Azure, GCP",
                  "score": 2,
                  "created_utc": "2026-01-03 16:38:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx3ffx3",
          "author": "swapripper",
          "text": "I was actually thinking about this exact thing since it‚Äôs New Year‚Äôs resolution time and all that, lol.\n\nI have a few DE-specific certs. For data engineers in general, the big ones are AWS/Snowflake/Databricks/Azure/GCP/dbt - depending on your tech stack and/or location.\n\nAs we move toward agentic workflows, I feel like streaming and real-time knowledge will become increasingly valuable. Worth checking if there are dedicated certs from major players like Apache Kafka.\n\nAlong similar lines, as AI code automation/GPU workloads grow, understanding infra deployments/ sandboxed environments will become critical. So it might be worth looking into Terraform/K8s/Nvidia certs depending on your setup.\n\nCurious to hear how others feel about this.",
          "score": 7,
          "created_utc": "2026-01-01 16:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx57bvz",
          "author": "Ill-Strawberry-3585",
          "text": "Consider the answer might be none. It really depends on what types of clients you‚Äôre targeting. As someone who‚Äôs both hired freelancers and worked as a freelancer in the startup space, certifications are basically meaningless. Your time would be relatively better spent building out a portfolio that shows you can deliver real-world value with these tools.",
          "score": 6,
          "created_utc": "2026-01-01 21:27:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9mnph",
              "author": "elraba",
              "text": "I suppose that certs are neither valued nor respected in the startup world, but they might be in more traditional and highly regulated sectors such as finance, telcos, public admin, big consulting... All depends on the type of clients you are targeting.",
              "score": 3,
              "created_utc": "2026-01-02 15:32:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx35v2i",
          "author": "PrestigiousAnt3766",
          "text": "I have (amongst others)\n¬†- az 104 (if you do infra)\n¬†- databricks DE professional (if you do DE in DBR).\n\n\nNow DP203 is retired I feel ms doesnt have a good DE certification anymore.¬†\n\n\nDp600/700 I feel were mostly useful if you go fabric (which I dont).",
          "score": 1,
          "created_utc": "2026-01-01 15:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx46e13",
          "author": "Usurper__",
          "text": "Just get the pro certs",
          "score": 1,
          "created_utc": "2026-01-01 18:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pvfe",
          "author": "Dylan_SmithAve",
          "text": "I'm mostly familiar with the AWS certifications. The data analytics/data engineering certs have been updated recently. Udemy courses and practice tests have proven to be super helpful in the past, so I always recommend going that route.\n\nCloud Practitioner and AI Practitioner would be solid to go for first. Then, you can work up towards the Generative AI Developer - Professional cert. That one is still in beta, so the available courses should improve over the next 6 months. Unfortunately the Data Analytics Specialty certification that I have is no longer available.\n\nThe Data Engineer - Associate exam could also be a good one to take if you need want another test before jumping into a professional level certification!",
          "score": 1,
          "created_utc": "2026-01-01 19:56:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5wkce",
              "author": "SupoSxx",
              "text": "What do you think about Solutions Associate and Professional for a Data Engineer?",
              "score": 1,
              "created_utc": "2026-01-01 23:43:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6qmib",
          "author": "eeshann72",
          "text": "Power bi",
          "score": 1,
          "created_utc": "2026-01-02 02:40:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxoe7mu",
          "author": "LongIslandIceTeas",
          "text": "Any update on which one u went for op?",
          "score": 1,
          "created_utc": "2026-01-04 19:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc9q7u",
          "author": "Ambitious_Mixture479",
          "text": "I THINK DATA BRAKE CERTIFICATION THEN THE SNOWFLAKE CERTIFICATION FOLLOWED BY SOME AILLM RELATED CERTIFICATIONS MIGHT HELP THANKS",
          "score": 1,
          "created_utc": "2026-01-02 23:09:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py48h7",
      "title": "How do you explore a large database you didn‚Äôt design (no docs, hundreds of tables)?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1py48h7/how_do_you_explore_a_large_database_you_didnt/",
      "author": "Technical_Safety4503",
      "created_utc": "2025-12-28 22:16:39",
      "score": 51,
      "num_comments": 48,
      "upvote_ratio": 0.88,
      "text": "I often have to make sense of **large databases with little or no documentation**.  \nI didn‚Äôt find a tool that really helps me explore them step by step ‚Äî figuring out **which tables matter** and **how they connect** in order to answer actual questions.\n\nSo I put together a small prototype to visually explore database schemas:\n\n* load a schema and get an interactive ERD\n* search across **table and column names**\n* select a few tables and automatically **reveal how they‚Äôre connected**\n\n  \n**GIF below (AirportDB example)**\n\nhttps://i.redd.it/yklm55oeq0ag1.gif\n\nBefore building this further, I‚Äôm curious:\n\n* **Do you run into this problem as well?** If so, what‚Äôs the most frustrating part for you?\n* **How do you currently explore unfamiliar databases?** Am I missing an existing tool that already does this well?\n\nHappy to learn from others ‚Äî I‚Äôm doing this as a starter / hobby project and mainly trying to validate the idea.\n\nPS: this is my first reddit post, be gentle :)",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1py48h7/how_do_you_explore_a_large_database_you_didnt/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwfug5n",
          "author": "AutoModerator",
          "text": "You can find our open-source project showcase here: https://dataengineering.wiki/Community/Projects\n\nIf you would like your project to be featured, submit it here: https://airtable.com/appDgaRSGl09yvjFj/pagmImKixEISPcGQz/form\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-28 22:16:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfwqqu",
          "author": "git0ffmylawnm8",
          "text": "Start exploring critical dashboards and reports to understand what metrics are delivered and the logic used. Then start branching out to different tables and understand how they're built out",
          "score": 70,
          "created_utc": "2025-12-28 22:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh20yl",
              "author": "West_Good_5961",
              "text": "Second this. I pick a dashboard and work backwards, looking at dependent tables.",
              "score": 8,
              "created_utc": "2025-12-29 02:13:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwk9c4m",
                  "author": "El_Kikko",
                  "text": "There are people who don't do this?",
                  "score": 1,
                  "created_utc": "2025-12-29 15:55:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwlff6i",
              "author": "Mo_Steins_Ghost",
              "text": "This, and/or as a proof try to replicate all the key metrics... systematically starting from tying out grand totals to breaking it down and tying out cohorts, segments, etc.\n\nNow, mind you, this is mostly for cursory exploration BUT you should really be talking to database and product owners in an org to understand all the idiosyncrasies.  Our acquisition of another company became an eight month project of mapping out multiple source systems and not just existing databases.\n\nIt gets even more critical if you are serving executive level dashboards and financial data that become adopted as \"gold standard\".  Arguably, if you are a business analyst, you should not be touching financial data... you don't know revenue recognition rules.  But, there are companies that task non FP&A people with FP&A like responsibilities all the time and it's insane.",
              "score": 1,
              "created_utc": "2025-12-29 19:12:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwg1695",
              "author": "Technical_Safety4503",
              "text": "Seems logical: learning from what already exists from a high value business perspective. \nDo you use SQL, DBeaver, PGadmin, ... or others to do the exploration or? Or is it the database/ecosystem that dictates the tool you use?",
              "score": -3,
              "created_utc": "2025-12-28 22:51:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwg29qv",
                  "author": "git0ffmylawnm8",
                  "text": "I raw dog the SQL scripts",
                  "score": 33,
                  "created_utc": "2025-12-28 22:57:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgucgg",
          "author": "No_Flounder_1155",
          "text": "truncate every table and see what parts of the business complain.",
          "score": 62,
          "created_utc": "2025-12-29 01:29:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgvysw",
              "author": "codykonior",
              "text": "Shit on the keyboard to establish dominance.",
              "score": 17,
              "created_utc": "2025-12-29 01:38:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhre3u",
              "author": "RBeck",
              "text": "That's crazy, just rename one table at a time and see what barfs.",
              "score": 8,
              "created_utc": "2025-12-29 04:45:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwmgs2e",
                  "author": "No_Flounder_1155",
                  "text": "no, just gaslight them and tell them it must be a failure with __their__ work.",
                  "score": 3,
                  "created_utc": "2025-12-29 22:14:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwimluo",
              "author": "VipeholmsCola",
              "text": "Can you hear them yell from different locations as you go?",
              "score": 2,
              "created_utc": "2025-12-29 09:05:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfzmeu",
          "author": "ColdStorage256",
          "text": "I need this.\n\n\n\n\nI have been asked to work with a db with no docs, hundreds of tables, and the BI team is useless and won't let me know what they're doing to produce reports.\n\n\n\n\nI work in enterprise though so my question is, how does this tool work? Is it open source and self hosted? Is it totally local server?¬†",
          "score": 10,
          "created_utc": "2025-12-28 22:43:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi5dvo",
              "author": "chock-a-block",
              "text": "Dbeaver.¬†",
              "score": 2,
              "created_utc": "2025-12-29 06:30:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwg4zs9",
              "author": "Technical_Safety4503",
              "text": "That situation is exactly what triggered this project.  \nRight now it‚Äôs a very early prototype, not a product yet.\n\n**How it works today:**\n\n* Runs fully **locally** (SQLAlchemy for schema extraction, FastAPI + WebSockets for serving, JavaScript/Cytoscape for visualization), all packaged in Docker\n* You point it at a database ‚Üí it extracts tables, columns, and primary/foreign keys\n* You can then **search across tables and columns** and visually reveal how selected tables connect\n* No cloud, no external services ‚Äî **no data leaves your environment**\n\nI built this specifically with **enterprise constraints** in mind: no documentation, siloed BI teams, restricted access.\n\nAt the moment it‚Äôs:\n\n* not packaged\n* not polished\n* not open-source yet\n\nI wanted to validate first that others actually run into this problem before investing more time.\n\nIf this is something you‚Äôd realistically use:\n\n* **What database(s) are you on?**\n* **Would** ***read-only + local/self-hosted*** **be a hard requirement for you?**",
              "score": 2,
              "created_utc": "2025-12-28 23:11:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwg7tbs",
                  "author": "ColdStorage256",
                  "text": "I work in a regulated industry. Fully local would be hard requirement whether I get permission to use a tool, or whether there was an enterprise license involved.\n\n\n\n\nOne issue I have in particular is that keys have different column names across different tables. Is there any way to deal with that?¬†\n\n\n\n\nIt's an absolute joke for me trying to put data together haha",
                  "score": 3,
                  "created_utc": "2025-12-28 23:27:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhnmjo",
                  "author": "Chobo1972",
                  "text": "If and when you are ready to share count me in.  Currently in an ancient SSRS environment within a 30 year old company.  So many tables (most old and broken) and so little time or documentation to know what is still usable.",
                  "score": 1,
                  "created_utc": "2025-12-29 04:21:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgd5ah",
          "author": "Striking_Meringue328",
          "text": "I generally start with the query logs",
          "score": 10,
          "created_utc": "2025-12-28 23:55:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgdmu8",
              "author": "Technical_Safety4503",
              "text": "ooeeh, that one I didn't see coming, good one! So you scan through to see which tables/columns get querried the most frequent and start your exploration from there?",
              "score": 2,
              "created_utc": "2025-12-28 23:58:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwiyf2b",
                  "author": "Thanael124",
                  "text": "You also see how the tables are joined in queries.",
                  "score": 2,
                  "created_utc": "2025-12-29 10:55:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgu30v",
              "author": "AZjackgrows",
              "text": "And the reverse of this- how often tables are being updated by their sources.",
              "score": 1,
              "created_utc": "2025-12-29 01:27:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh94dq",
          "author": "streetrider_sydney",
          "text": "Dive into system views to unravel which routines and views are most utilised. Take top 10 and explore the tables referred in them. Once you are done with them, explore next 10 and so forth. Also, check for table prefixes or nomenclature standards to see if there are types of tables - such as reference, lookup and user tables.",
          "score": 3,
          "created_utc": "2025-12-29 02:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh7k82",
          "author": "idodatamodels",
          "text": "Yes, Erwin will do this. Did you ever look at that tool?",
          "score": 5,
          "created_utc": "2025-12-29 02:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgrjut",
          "author": "robberviet",
          "text": "Starts from both start (data sources), and end (end users needs). Just focus on what is needed first for quick iterations.",
          "score": 2,
          "created_utc": "2025-12-29 01:13:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhzc05",
          "author": "mweirath",
          "text": "I use red gate‚Äôs sql toolkit. Their SQL Doc tool does a lot of this for most DBs.",
          "score": 2,
          "created_utc": "2025-12-29 05:41:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi5g4h",
          "author": "chock-a-block",
          "text": "GitHub link?",
          "score": 2,
          "created_utc": "2025-12-29 06:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwifnug",
          "author": "sjcuthbertson",
          "text": "Generally when I have this problem, there are no foreign keys declared either, so figuring out the relationships is more manual. Looks like your tool wouldn't help much with no fks?",
          "score": 2,
          "created_utc": "2025-12-29 08:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnchpa",
              "author": "Technical_Safety4503",
              "text": "You‚Äôre right ‚Äî in its current form, it wouldn‚Äôt help much without declared relationships.  \nAutomated relationship detection is something I‚Äôm considering, but it‚Äôs non-trivial and easy to get wrong at scale.  \nFor now I‚Äôm validating the FK-based workflow first before expanding into heuristics.",
              "score": 1,
              "created_utc": "2025-12-30 01:04:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwp654w",
                  "author": "sjcuthbertson",
                  "text": ">Automated relationship detection\n\nYeah I probably wouldn't bother to be honest. There's no way you'd ever be able to work out programmatically that creator_id joins to resource_id, without also generating a lot of false positive relationships. I can't imagine LLMs being any better than traditional approaches at this, either.",
                  "score": 1,
                  "created_utc": "2025-12-30 08:25:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwlcfku",
          "author": "justanothersnek",
          "text": "For me, the methodical way: find SMEs from both the business side and technical side/DB team.¬† Get them all in a room and just hammer out the details.¬† Did they have wikis?¬† Confluence pages?¬† Etc.\n\n\n\n\nIf you have no such SMEs...youre F'ed.¬† Slightly kidding, but yeah then you can resort to some tools within the database system or external tools.¬† But even still, you'll maybe run into some weird esoteric business logic that is randomly there like with CASE WHEN statements.¬† So you can reverse engineer all you want, something can still bite you in the ass and still be F'ed.",
          "score": 2,
          "created_utc": "2025-12-29 18:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgurgz",
          "author": "cmcclu5",
          "text": "[Here‚Äôs](https://horizonanalytic.com/landing/packages/horizon-schema) something that does something similar. Loads up all the tables, visually shows relationships between tables using established keys, and lets you inspect column data types.",
          "score": 1,
          "created_utc": "2025-12-29 01:31:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhbi1f",
          "author": "AcanthaceaeOpposite",
          "text": "/RemindMe 2 weeks",
          "score": 1,
          "created_utc": "2025-12-29 03:07:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhdd2d",
          "author": "Sudden_Beginning_597",
          "text": "there are some open source auto eda tools like rath, that can automatelly discover data views/ charts with potential insights for you, useful for those large dataset which you have no idea where to start, but it consumer huge amount of resources for computation.",
          "score": 1,
          "created_utc": "2025-12-29 03:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjgff6",
          "author": "SomeDayIWi11",
          "text": "Commenting so that I can come back later",
          "score": 1,
          "created_utc": "2025-12-29 13:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwncmdf",
              "author": "Technical_Safety4503",
              "text": "I‚Äôll give an update after doing some tweaks",
              "score": 1,
              "created_utc": "2025-12-30 01:05:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwk5pjm",
          "author": "empireofadhd",
          "text": "Find transaction tables and rebuild the model from there. Eg entries, purchases etc. They are the spinal chord of databases and the resin they exist most of the time.",
          "score": 1,
          "created_utc": "2025-12-29 15:37:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q05yz4",
      "title": "Snowflake or Databricks in terms of DE career",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q05yz4/snowflake_or_databricks_in_terms_of_de_career/",
      "author": "DryYesterday8000",
      "created_utc": "2025-12-31 06:23:05",
      "score": 49,
      "num_comments": 36,
      "upvote_ratio": 0.95,
      "text": "I am currently a Senior DE with 5+ years of experience working in Snowflake/Python/Airflow. In terms of career growth and prospects, does it make sense to continue building expertise in  Snowflake with all the new AI features they are releasing or invest time to learn databricks?\n\nCurrent employer is primarily a Snowflake shop. Although can get an opportunity to work on some one off projects in Databricks.\n\nLooking to get some inputs on what will be a good choice for career in the long run. ",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q05yz4/snowflake_or_databricks_in_terms_of_de_career/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwvgzg2",
          "author": "addictzz",
          "text": "I think ultimately they are just tools. You shouldnt have problems learning both. I feel both platform is equally competitive and evolving to be future proof.",
          "score": 50,
          "created_utc": "2025-12-31 06:33:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo221",
              "author": "PrestigiousAnt3766",
              "text": "Agree 100%.\n\n\nThat said, i exclusively work with databricks but as long as it isnt fabric you should be fine.",
              "score": 19,
              "created_utc": "2025-12-31 07:34:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvp1sx",
                  "author": "addictzz",
                  "text": "The same as I. \n\nI don't want to pick on any platform but I must say compared to DB or Snow, Fabric may not be there yet in terms of maturity.",
                  "score": 2,
                  "created_utc": "2025-12-31 07:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwn2n4",
              "author": "toem033",
              "text": "This is just a half-baked truth. You shouldn't have problems learning both but companies when hiring only look for candidates with deep expertise already. My company, which is deep in Databricks, unflinchingly prefer those who have experience with the platform.",
              "score": 5,
              "created_utc": "2025-12-31 12:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwpkhe",
                  "author": "addictzz",
                  "text": "As an employer, I too would prefer somebody who is deep in a platform I use day to day. But technical skill is not the only factor in hiring and you dont always get a good pool of people highly adept in particular platform. \n\nBut then among these 2 platforms, who can say for sure which one will have the most market share in the future? Snowflake has gone public, but Databricks is gaining a lot of traction lately.",
                  "score": 7,
                  "created_utc": "2025-12-31 13:08:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvx647",
          "author": "NeedleworkerIcy4293",
          "text": "Build the foundations in the end if foundations are solid you should be able to pick up anything",
          "score": 8,
          "created_utc": "2025-12-31 09:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0tark",
          "author": "Tushar4fun",
          "text": "Go and work on basics of spark and how bigdata works.\n\nDBX & ‚ùÑÔ∏è are just platform to work upon. Get a hands on for the interview.\n\nI‚Äôm into DE since 2013 and mostly worked on pipelines using python/sql and later on moved to python/sql/bigdata. Of course, there were other tools like airflow, kubernetes, etc were there but I never worked on DBX.\n\nRight now, I‚Äôm working on DBX in my new organisation and they hired me for my DE knowledge.\n\nI‚Äôve seen people know DBX but they dont have any idea how to structure a project.\n\nFor example - \n\nusing notebooks in prod - there are many cons of using them on prod like no modularisation, difficulty in code review,etc\n\nWriting everything in a single script - no use of DRY coding.\n\nMaybe I know these things because I‚Äôve worked on end to end architecture including building of API services too.\n\nWork on end to end.",
          "score": 3,
          "created_utc": "2026-01-01 02:53:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1mxip",
              "author": "idiotlog",
              "text": "How is it that a notebook cannot be modular? They accept parameters, and can invoke libraries?",
              "score": 1,
              "created_utc": "2026-01-01 06:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1w4qp",
                  "author": "Tushar4fun",
                  "text": "You just cannot make wheel out of bunch of notebooks.\n\nPlus, %run to include a notebook with the whole path in each cell is not pythonic.\n\nApart from this, if you are printing outputs on notebooks there is a huge possibility that it will produce different output on different environments.\n\nWhen you commit it to repo, even though code is same in the cells it will show diffs.\n\nNotebooks are best for data analysis, EDA and for Data scientists. But it‚Äôs simply not pythonic.",
                  "score": 1,
                  "created_utc": "2026-01-01 08:17:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1pg67",
          "author": "dataflow_mapper",
          "text": "At your level it is less about picking a winner and more about avoiding being boxed in. Snowflake expertise will stay valuable, especially if you lean into data modeling, cost control, and platform architecture rather than just writing SQL. The AI features are interesting, but they are not a moat by themselves yet.\n\nDatabricks is worth learning enough to be fluent. Not because you need to switch stacks tomorrow, but because it stretches different muscles around Spark, distributed compute, and more engineering heavy pipelines. Even a few real projects is usually enough to make your profile read as ‚Äúplatform agnostic‚Äù instead of ‚ÄúSnowflake only.‚Äù\n\nIf your employer gives you legit Databricks work, take it. You do not need to abandon Snowflake. The strongest DE profiles right now can explain tradeoffs between the two and have scars from both.",
          "score": 3,
          "created_utc": "2026-01-01 07:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxesxjz",
          "author": "valentin-orlovs2c99",
          "text": "Both Snowflake and Databricks are safe bets for the foreseeable future, but they‚Äôre carving out slightly different niches in the data landscape. Snowflake's sweet spot is still managed warehousing and SQL analytics, but their new AI/ML features are definitely pushing toward a more versatile platform. Databricks, meanwhile, was built for big data and machine learning from the start, so if you want to lean deeper into data science, streaming, or large-scale ETL, it‚Äôs a strong addition to your toolbelt.\n\nFrom a career perspective, deep expertise in either is valuable, but having practical, hands-on experience in both will make you stand out. If you can get some Databricks exposure‚Äîeven in one-off projects‚ÄîI‚Äôd take it. At the very least, you‚Äôll be able to talk intelligently about both in interviews, and more companies are running hybrid stacks.\n\nLong story short: Don‚Äôt abandon Snowflake just because Databricks is cool now, but add some Databricks to your arsenal if you can. That cross-platform knowledge is gold in this market.",
          "score": 3,
          "created_utc": "2026-01-03 09:20:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvqy58",
          "author": "afahrholz",
          "text": "both snowflake and databricks are solid paths and worth knowing, snowflake for warehousing sql workflows and databricks for heavy data pipelines ml learning both over time seems like a good long term play",
          "score": 7,
          "created_utc": "2025-12-31 08:01:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0m6em",
          "author": "crevicepounder3000",
          "text": "Databricks for sure and I say that as someone with 4+ years of Snowflake experience. The job market wants Databricks and Spark. Rightly or wrong, Snowflake is seen as too expensive relative to Databricks and people are increasingly focused on cost",
          "score": 5,
          "created_utc": "2026-01-01 02:06:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy486v",
          "author": "imcguyver",
          "text": "If you look at the history of snowflake & databricks, snowflake leans towards BI, databricks leans towards DS. But both 'databases' are now platforms with so many new features added over the years that they arguably look the same. Using one or the other then comes down to personal preference.",
          "score": 3,
          "created_utc": "2025-12-31 17:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzdmj7",
          "author": "BoringGuy0108",
          "text": "Be an expert in whichever your company uses, but know the pros and cons of each. Get databricks exposure if you can at your company, but I wouldn't be too concerned if you don't get the opportunity.",
          "score": 3,
          "created_utc": "2025-12-31 21:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzr1pt",
          "author": "goblueioe42",
          "text": "Either is fine. I am on the GCP but also have snowflake experience. I‚Äôm sure knowing any of these well is the best part",
          "score": 2,
          "created_utc": "2025-12-31 22:52:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0ou62",
          "author": "vfdfnfgmfvsege",
          "text": "doesn't matter. I've run the full gamut of tools in the data space and if I need to learn something new I just pick it up.",
          "score": 1,
          "created_utc": "2026-01-01 02:23:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw8uzb",
          "author": "dkallday88",
          "text": "Databricks employee here. \n\nDatabricks is at a run rate of $5B per year and growing at 55% YoY\n\nSnowflake about the same revenue and growing 29% YoY.\n\nThere will be more demand for Databricks in the future.  \n\nI talk to companies often who are migrating from Snowflake to Databricks for a number of reasons pretty regularly.  \n\nIf you are tied into the DE / Analytics community -- it is becoming pretty widely known that Databricks is the most complete Data Platform on the market right now",
          "score": 1,
          "created_utc": "2026-01-05 21:54:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvih38",
          "author": "Born-Pirate1349",
          "text": "I think Databricks, firstly it is the complete lakehouse engine. The current market trend is towards the lakehouse rather than the warehouse due to vendor lock-in in terms of data. Databricks is a very good learning platform for any DE, either in their early stages or during the exploration phase. \n\nWe all know that databricks is built on top of spark engine, which is the most basic tool for any DE should be known. So when you learn databricks you will learn spark as well and all its features. All the features like open table formats like delta , catalog layer like unity and its governance capabilities and all the streaming tools. Hence this is the best platform for any DE to learn more about DE.",
          "score": -6,
          "created_utc": "2025-12-31 06:45:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzulxj",
              "author": "Infinite_Bug_8063",
              "text": "I don‚Äôt know why you are getting downvoted, but what you said is true. But I feel like Microsoft Fabric is the new trend.",
              "score": 3,
              "created_utc": "2025-12-31 23:14:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx6mmrb",
                  "author": "Born-Pirate1349",
                  "text": "Even I agree that fabric is becoming a new trend. I think people are more obsessed with snowflake because of their better performance and features but people tend to forget about the cost. For any DE I feel like databricks is a very good option in terms of basics and even fabric is good.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:16:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxzmn4",
              "author": "verus54",
              "text": "I‚Äôve been noticing the opposite trend. I see so many more snowflake jobs than data bricks. I‚Äôve never used snowflake, but I have used databricks. I thought snowflake was cloud agnostic, making it not vendor-locked?",
              "score": 4,
              "created_utc": "2025-12-31 17:16:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx03gfv",
                  "author": "mike-manley",
                  "text": "Snowflake is cloud agnostic... its provisioned on any of the Big Three and a region of your choosing.",
                  "score": 1,
                  "created_utc": "2026-01-01 00:07:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwyx4gm",
          "author": "Rare_Decision276",
          "text": "Databricks bro because it‚Äôs a lakehouse platform and most widely used application. Snowflake is a data warehouse.",
          "score": -7,
          "created_utc": "2025-12-31 20:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx03jdw",
              "author": "mike-manley",
              "text": "The lines are way more blurred now.",
              "score": 2,
              "created_utc": "2026-01-01 00:08:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwznwxx",
          "author": "Nekobul",
          "text": "Both of these platforms don't support on-premises. For that major reason, neither of these two are good options.",
          "score": -4,
          "created_utc": "2025-12-31 22:34:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx03lpp",
              "author": "mike-manley",
              "text": "Bro, what?",
              "score": 3,
              "created_utc": "2026-01-01 00:08:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx08wch",
                  "author": "Nekobul",
                  "text": "What is not clear?",
                  "score": -2,
                  "created_utc": "2026-01-01 00:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvn8vy",
          "author": "latent_signalcraft",
          "text": "this is a good example of where automation shines on throughput but still hides some risk. i have seen similar setups work well until quality compliance or subtle factual drift starts compounding across dozens of posts. the real leverage usually comes when there is a clear review or sampling loop so humans periodically validate outputs instead of trusting volume alone. without that scaling just amplifies small errors very quickly.",
          "score": -6,
          "created_utc": "2025-12-31 07:27:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q22shj",
      "title": "What does an ideal data modeling practice look like? Especially with an ML focus.",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q22shj/what_does_an_ideal_data_modeling_practice_look/",
      "author": "Capable_Mastodon_867",
      "created_utc": "2026-01-02 16:17:12",
      "score": 48,
      "num_comments": 25,
      "upvote_ratio": 0.91,
      "text": "I was reading through Kimballs warehouse toolkit, and it gives this beautiful picture of a central collection of conformed dimensional models that represent the company as a whole. I love it, but it also feels so central that I can't imagine a modern ML practice surviving with it.\n\nI'm a data scientist, and when I think about a question like \"how could I incorporate the weather into my forecast?\" my gut is to schedule daily api requests and dump those as tables in some warehouse, followed by pushing a change to a dbt project to model the weather measurements with the rest of my features. \n\nThe idea of needing to connect with a central team of architects to make sure we 'conform along the dimensional warehouse bus' just so I can study the weather feels ridiculous. Dataset curation and feature engineering would likely just die. On the flip side, once the platform needs to display both the dataset and the inferences to the client as a finished product, then of course the model would have to get conformed with the other data and be secure in production.\n\nOn the other end of the extreme from Kimballs central design, I've seen mentions of companies opening up dbt models for all analysts to push using the staged datasets as sources. This looks like an equally big nightmare, with a hundred under-skilled math people pushing thousands of expensive models, many of which would achieve relatively the same thing with minor differences and numerous unchecked data quality problems, different interpretations of data, confusion on different representations from the different datasets, I can't imagine this being a good idea.\n\nIn the middle, I've heard people mention the Mesh design of having different groups manages their warehouses. So analytics could set up its own warehouse for building ML features and a maybe a central team helps coordinate the different teams data models to be coherent. One difficulty that comes to mind is if a healthy fact table in one teams warehouse is desired for modeling and analysis by another team, spinning up a job to extract and load a  healthy model from one warehouse to another is silly, and it also makes one groups operation quietly dependent on the other groups maintenance of that table.\n\nThere seems to be a tug-of-war on the spectrum between agility and coherent governance. I truly don't know what the ideal state should look like for a company. To some extent, it could even be company specific. If you're too small to have a central data platform team, then could you even conceive of Kimballs design? I would really love to hear thoughts and experiences.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q22shj/what_does_an_ideal_data_modeling_practice_look/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx9vyw5",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-02 16:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxan147",
          "author": "Arnechos",
          "text": "For ML you should pretty much have a feature store that DS/ML team owns - not the data engineers. Offline storage can be pretty much anything starting from parquet files to star schema in a database, online storage like redis. You can read blog posts on hopsworks on FTI architecture, personally I never used their feature store only AWS but it's my go to approach to ML/DS/AI architecture",
          "score": 15,
          "created_utc": "2026-01-02 18:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxejsnz",
              "author": "jpdowlin",
              "text": "At this point I will not just plug my O'Reilly book which is all about this, but provide a happy new year's code for downloading the PDF for it. \n\n[https://www.hopsworks.ai/lp/full-book-oreilly-building-machine-learning-systems-with-a-feature-store](https://www.hopsworks.ai/lp/full-book-oreilly-building-machine-learning-systems-with-a-feature-store)  \nPromo Code \"jim\"\n\nIt covers everything data engineering related ML most teams need in 2026.",
              "score": 13,
              "created_utc": "2026-01-03 08:00:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxc8bha",
              "author": "Capable_Mastodon_867",
              "text": "This does seem like a good place to bring up feature stores. Since you're working with one, I wanna get your thoughts (I haven't used one in production myself).\n\nMy current mental model for a feature store puts it as more like an operational store, in spite of it holding large analytical datasets. I wouldn't want BI analysts, clients, or the platform to be able to see the data in there directly, but then if I ingest raw datasets directly into it I can't present them the core datasets that drive the inference. To present the core data would mean I would have to also copy those raw datasets into the warehouse as well, and retransorm? Or should I just extract the ML feature sets from the feature store into the warehouse? It feels like I should stage the data in the warehouse, do some initial structuring, then join and pull them into a big table in the feature store from there, followed by deriving the more specialized ML features.\n\nI also don't know if it's necessary to land my inferences back in a feature store for analysis, if I'm not going to use those as a feature as well? If my core datasets are in the warehouse, I can dump my inferences as a dataset, present models from them, and dashboard/present them from there. \n\nGiven your experience with a feature store, is my thought process off here?",
              "score": 3,
              "created_utc": "2026-01-02 23:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxf6n6v",
                  "author": "Arnechos",
                  "text": "To answer your questions - I'll assume you have a lakehouse.\n\nFeature store is owned by DS/ML which might have some DE's inside (not your DE-platform guys but dedicated). Your lakehouse is their source system - they handle ingestions/transformations (like pulling data from external API's). They should dump predictions into the bronze layer or expose predictions via api so you can pull the data to the bronze layer. That way you can extend your pipeline to serve predictions in the presentation (gold) layer without ML/DS team write-level access to gold layer.\n\nIt's like the data mesh architecture, where ML output is a data product which is their responsibility and you have clear data contract\n\nu/jpdowlin already posted a link to his book which is really great (I've read it over Christmas) I recommend it.",
                  "score": 2,
                  "created_utc": "2026-01-03 11:17:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbbd8v",
          "author": "exjackly",
          "text": " (slightly rephrased) Absolutely there is a tug-of-war between distributed and centralized governance.  And the right balance does change with the company and industry.\n\nBut you are never too small to incorporate elements from Kimball into design and models.  Even if you only have a single DE, they can use Kimball to guide the modelling that they do.  And, depending on the needs of the organization, it isn't critical to make the model gigantic and intricate. \n\nLike most things, limit it to what you are working on and the level you need it at.\n\nSome specifics - Mesh is very much a decentralized approach, though the specifics vary quite a bit based on who you talk to.  But in a Mesh environment, pulling a healthy fact table across is what you want to have happen.  It allows that cross-group information flow, while minimally impacting either group.  Yes, it is a dependency, but point to point isn't bad as long as there aren't too many; at which point you start looking at a central repository for these cross-group objects in place of point-to-point.\n\n  \ndbt for all, I agree is not a good ideal and fortunately not one I've come across in the wild.",
          "score": 5,
          "created_utc": "2026-01-02 20:18:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxcj5ea",
              "author": "Capable_Mastodon_867",
              "text": "Not sure why I didn't think of still keeping a central warehouse while having a mesh. Seems pretty simple now that you say it, lol. \n\nThanks for letting me know that you've seen a healthy pattern of teams moving facts from one warehouse to another in a mesh. I had discounted the idea. It still feels odd, but maybe it's done between the DE teams behind the scenes so members of one department don't need credentials for the warehouse of another department? That's starting to make sense, I'll have to think it over more, I should probably read more on the mesh concept.",
              "score": 1,
              "created_utc": "2026-01-03 00:01:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxb08vw",
          "author": "RunOrdinary8000",
          "text": "I would not treat ML that special.\nIt's output is just another source.\n\nSo what I would do is to have a low level model to unify all data I receive. The best approach is IMHO data vault.\nThen per use case you diverge from there and pan out a data Mart, feature schema where you find the data the use case wants. This could be a Kimball Star model but does not have to. Could be a flat table (IMHO most common)\n\nData scientist I would explain the data vault concept so they can search the data for themself, only require that they have something that describes for them what they need.\nI need that for the data lineage. Because I need to know if something is broken in the data to inform.\n\nI hope that helps as a rough overview.",
          "score": 6,
          "created_utc": "2026-01-02 19:24:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxc65hk",
              "author": "Capable_Mastodon_867",
              "text": "This sounds pretty functional, but my understanding of data vault is low, so I got some questions. If I recall, I thought data vault was an append only structure that broke the datasets into keys, relationships, and observations? If that's right, it feels very close to raw, and I wonder if many data scientists would struggle trying to apply definitions to source system keys that conflict with the way the DE team presents it in the kimball style models they present.\n\nI do recall something about a raw vault vs a business vault, so maybe that's what fixes it? Maybe if the data scientist was the one bringing this new dataset they could pull from raw vault and for datasets they're not familiar with they pull from business? Im out of my depth here, how would you want to protect the company from breaking into different definitions of the same datasets?",
              "score": 2,
              "created_utc": "2026-01-02 22:50:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxf0iba",
                  "author": "RunOrdinary8000",
                  "text": "I would only use the Kimball Star or snowflake model if the consumer of the data wants to report something. I would not use it for ML or other stuff. For Product website flat tables are king.\n\nI am not sure what you mean by breaking into different definitions. That sounds wired.  Let's make an example.\nYou have 2 tables.\nThe one is about cars, the other one monthly sold cars.\nTable 1 consists of car name, engine type, horsepower, wheels, extras, comment, up.\nThe other table is frame number, car type, sold amount, discount, comment.\n\nNow the keys in your Modell is car name in table 1 and frame number in table 2. If you are lucky you could build a relationship by using frame number and car type if car type is equal to car name. If not, you might need some solution to match both.\nIf you do not have that much luck you need additional data. Many you have a lookup for frame number where you get a better name. Or you use a trained model to make the match from table1.car name to table2.Car type. And that service is again an own source into the model.\n(If we distinguish between business vault and raw vault, the service generated table is in the raw vault and the abstracted relationship would be something on the business vault. But these helps to give you a orientation where the data comes from. Is it derived or external input)\n\nThe approach is that basic, you would not have a different approach.\n\nI had once the case in a finance world, that I had been asked to extract all defaulted accounts. There was a field default, if true it was defaulted.\nI did use that as a filter, send the query back, and I got them the answer that is not what the requested ment he was looking for default per basel3 standard and the field reflected only the institute internal definition.\nYou mean confusion like this?\nThat is data governance. You need to know what field expresses what. And you can have multiple flavors of one and the same thing. It is a good thing to have a clear name in for that. I would create a business board which decides on the naming. Because BI is always business driven.",
                  "score": 1,
                  "created_utc": "2026-01-03 10:25:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbuxg8",
          "author": "GreyHairedDWGuy",
          "text": "The ideal state is what works in your org.  There is no single 'best' way to do this work.  There are general design paradigms you can use as a rough guide but again, fit depends on your unique circumstances.",
          "score": 2,
          "created_utc": "2026-01-02 21:53:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc14xp",
          "author": "otto_0805",
          "text": "Remindme! 2 days",
          "score": 2,
          "created_utc": "2026-01-02 22:24:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxc1bz7",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 2 days on [**2026-01-04 22:24:52 UTC**](http://www.wolframalpha.com/input/?i=2026-01-04%2022:24:52%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/1q22shj/what_does_an_ideal_data_modeling_practice_look/nxc14xp/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2F1q22shj%2Fwhat_does_an_ideal_data_modeling_practice_look%2Fnxc14xp%2F%5D%0A%0ARemindMe%21%202026-01-04%2022%3A24%3A52%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q22shj)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-02 22:25:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxexr9q",
          "author": "jpdowlin",
          "text": "On your specific point about weather data, in my O'Reilly book I cover air quality prediction in Chapter 3. You generate the code to download the historical weather and air quality data (backfilling) into feature groups (tables) and write an incremental pipeline to download weather forecasts, weather observations, and air quality observations. \n\nThe 1st challenge you will encounter is how to perform a temporal join to join air quality observations with weather observations. This requires an ASOF LEFT JOIN between the tables. Only a few data warehouses support it it (and Hopsworks feature store). You want that data back in Python (e.g., Pandas DataFrames) to train the model. Hopsworks provides that data via Arrow from the Lakehouse tables. If you go via JDBC/ODBC to your data warehouse, performance will be vile - Arrow (ADBC or ArrowFlight) is best for Python clients.\n\nMy book (referenced in the thread) covers these fundamental skills of backfilling vs incremental pipelines and data models - star schema, snowflake  schema data model and OBT (don't use this for ML!).",
          "score": 2,
          "created_utc": "2026-01-03 10:02:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa7v6v",
          "author": "SirGreybush",
          "text": "ML usually uses the lowest level distinct data. So either bronze if bronze has 100% of source (usually they never do) or you build into staging an extra layer for de duplicating row data with hashing, from vetted sources. \n\nIf data from higher levels is required, either a view or make some new tables in silver for the ML data process. Make sure it is repeatable at different times or days with the same filter. \n\nThen what the ML spits out, that is information, store that in Dim/Fact gold layer. \n\nLike a Customer Rating. The fact is the rating calculated by the ML based on history available at the time the CR was calculated. The Dims would be DimCustomer, DimCustomerRating, DimDates, FKs into a new FactCustomerRating table. \n\nSo the ML the Python code reads the datawarehouse repeatable data, and sends into staging the output, that then gets processed. \n\nUnless you build APIs and have the ML process running continuously so that a new customer is processed in near real-time.",
          "score": 3,
          "created_utc": "2026-01-02 17:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa9irn",
              "author": "SirGreybush",
              "text": "The reason for repeatable data is that the people behind the ML can change the math model and want to reprocess with the same data, compare two different results. \n\nDon‚Äôt count on them saving the datasets obtained, as the DE that is your responsibility to have it available. \n\nIn some situations you might need to export as tables the tuples they need into a new database.\n\nI used a BigInt to store Datetime to the millisecond as a key for this data, so basically a snapshot date time.",
              "score": 3,
              "created_utc": "2026-01-02 17:20:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxeo2yy",
                  "author": "jpdowlin",
                  "text": "When you provide Data Scientists Python APIs for computing features and easily saving backfill-incremental data, then you can trust them to manage their data for AI. From my experience, the data scientist job of old is pretty much gone in most industries. Now, there are mostly ML engineers left who have to take ownership of the data.",
                  "score": 5,
                  "created_utc": "2026-01-03 08:38:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxceovi",
                  "author": "Capable_Mastodon_867",
                  "text": "Interesting, you're mentioning the medallion language, probably working with lakehousing then? Medallion always felt vague so could you clear the stages up for me? I had personally just mapped the idea of bronze->silver->gold  to staging->transforming->presentation, but you just mentioned both staging and bronze as separate things, so I'm clearly off here.\n\nEither way, what you're saying is we should look at ML as part of the transformation steps before presentation? That's definitely a different picture, as I was picturing ML as taking the presented fact tables and producing a new inference based dataset from it that itself goes back into staging and transformed into presentation in a new data model. \n\nThis picture makes sense with my work in forecasting, where I generally want a rolling snapshot fact table to already be prepared, and the forecasts can get dumped as jsons or parquet by the ML pipeline, and ingested and transformed into new facts from there. With this, clients can see the same snapshot dataset we trained on as well as the forecast itself. \n\nHow would you ideally want this forecasting example to fit into data modeling the way you did with your customer rating example?",
                  "score": 1,
                  "created_utc": "2026-01-02 23:37:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjek38",
          "author": "tecedu",
          "text": "OP so multiple things\n\nThe struggle that you are seeing is just due to you not having permission, you should atleast have it in dev and preprod. So depends on what changes you are asking for? \n\nImo, people or teams have a single POC with elevated permissions, who can create tables and modify, they do all of liasing. They just report on what people are soint so people across the business if they come with a similar usecase, they can get access easily and clone it or get read access to it. \n\nMy team was a small team seperated from our centralised team, we started with our own different jank. Never really did kimball properly and honestly still won‚Äôt do it with the amount the storage and compute exists today, in a data warehouse if in parquet you can get parquet pushdowns to avoid the issue. We are however very knowledge about our data and its a mess for 3rd parties.\n\nOur centralised team follows it so anytime we contribute something to them we follow their rules.",
          "score": 1,
          "created_utc": "2026-01-04 00:35:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxn8t17",
          "author": "mattiasthalen",
          "text": "I think this is where the Analytical Data Storage System shines. You have three layers:\n\nData according to system (das)\n‚Ä¶business (dab)\n‚Ä¶requirements (dar)\n\nDAB is where you integrate so all sources plays nice. The idea is to model how the business sees the data. I like HOOK, but Data Vault is nice too.\n\nDAR is what the consumer need: star schema, unified Star schema, OBT, feature tables.",
          "score": 1,
          "created_utc": "2026-01-04 16:14:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa3u8q",
          "author": "nonamenomonet",
          "text": "Following",
          "score": -3,
          "created_utc": "2026-01-02 16:53:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxu6ts",
      "title": "Are we too deep into Snowflake?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pxu6ts/are_we_too_deep_into_snowflake/",
      "author": "stuckplayingLoL",
      "created_utc": "2025-12-28 15:36:35",
      "score": 47,
      "num_comments": 35,
      "upvote_ratio": 0.96,
      "text": "My team uses Snowflake for majority of transformations and prepping data for our customers to use. We sort of have a medallion architecture going that is solely within Snowflake. I wonder if we are too vested into Snowflake and would like to understand pros/cons from the community. The majority of the processing and transformations are done in Snowflake. I anticipate we deal with 5TB of data when we add up all the raw sources we pull today.\n\nQuick overview of inputs/outputs:\n\nEL with minor transformations like appending a timestamp or converting from csv to json. This is done with AWS Fargate running a batch job daily and pulling from the raw sources. Data is written to raw tables within a schema in Snowflake, dedicated to be the 'stage'. But we aren't using internal or external stages.\n\nWhen it hits the raw tables, we call it Bronze. We use Snowflake streams and tasks to ingest and process data into Silver tables. Task has logic to do transformations.\n\nFrom there, we generate Snowflake views scoped to our customers. Generally views are created to meet usecases or limit the access.\n\nMajority of our customers are BI users that use either tableau or power bi. We have some app teams that pull from us but not as common as BI teams.\n\nI have seen teams not use any snowflake features and just handle all transformations outside of snowflake. But idk if I can truly do a medallion architecture model if not all stages of data sit in Snowflake. \n\nCost is probably an obvious concern. Wonder if alternatives will generate more savings.\n\nThanks in advance and curious to see responses.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pxu6ts/are_we_too_deep_into_snowflake/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwdm926",
          "author": "mamaBiskothu",
          "text": "This is no different from choosing between on prem and aws. Unless youre dealing with petabytes of data and millions in snowflake bills, it makes sense to abstract away the infrastructure part of data engineering to snowflake. Your DE team is likely half or less the size it woild need to be if you leave snowflake.",
          "score": 46,
          "created_utc": "2025-12-28 15:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweyw25",
              "author": "stuckplayingLoL",
              "text": "I think you summed it up pretty well for us. My team is not very experienced with the infrastructure aspect of AWS and really leans on 1 engineer to keep the infrastructure afloat. Thanks for raising that point.",
              "score": 7,
              "created_utc": "2025-12-28 19:41:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdovon",
          "author": "Pittypuppyparty",
          "text": "There‚Äôs a trade off. Can you save money refactoring pipelines away from snowflake? Sure. But you could probably just refactor in snowflake and save a ton of money. I think people underestimate how much snowflake does under the hood to make things work seamlessly and quickly. In my experience the added overhead of other services wasn‚Äôt worth the savings and refactoring on snowflake gave the best bang for my buck.",
          "score": 11,
          "created_utc": "2025-12-28 15:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdmxhf",
          "author": "konwiddak",
          "text": "My rule for software value is - If Snowflake pricing doubled overnight, would it be a showstopper? If it generates enough business value that you'd carry on (perhaps putting in place a migration plan) - then it's probably fine as a choice. If you're so cost sensitive that a small rise in costs would cause problems, then it's probably not the right choice.\n\nGenerally being heavily invested into Snowflake doesn't concern me too much. Most of the transformation logic is SQL. Unlike say python, or a lot of proprietary solutions SQL code written today will still be good in 20 years time. There aren't any libraries to end up unmaintained. If you *really* need to you can port to another database technology and most of your transformations will port over perfectly. You might need to spin up a solution for tasks, and change your ingestion methodology, but all that business logic will be fine still.\n\nUnless you go completely open source, and want to take on board the substantial overhead with doing that, I'd say it's pretty good value. Vendor lock in mainly comes from the fact that it's really good, rather than vendor lock in from being proprietary. Very little of it is highly proprietary unlike a lot of other ETL methodologies.\n\nWhy aren't you at least using internal stages? There's far less overhead in using COPY INTO vs INSERT unless you're inserting a trivial amount of data. External stages and pipes even less overhead and cost.",
          "score": 16,
          "created_utc": "2025-12-28 15:49:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwexosb",
              "author": "stuckplayingLoL",
              "text": "I feel like cost isn't a concern yet but at the rate that we are going, we could be scaling to higher usage and thus the conversation with cost could come up.\n\nWe are not using internal stages only because previous engineers on the team resorted to using Python write_pandas and prayed that the auto generated tables did not cause issues down the road. It's absolutely tech debt due to us running into type issues. It will be something that I will look into though, thanks!",
              "score": 1,
              "created_utc": "2025-12-28 19:36:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfv2vx",
                  "author": "kudika",
                  "text": "`write_pandas()` uses temporary internal stages btw",
                  "score": 3,
                  "created_utc": "2025-12-28 22:19:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhqlgp",
                  "author": "Choperello",
                  "text": "95% likelyhood you can cut your snowflake bill just by optimizing how you‚Äôre using it.",
                  "score": 2,
                  "created_utc": "2025-12-29 04:40:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwe47m3",
          "author": "goblueioe42",
          "text": "I have interviewed with some snowflake teams that found my non snowflake experience not helpful or they only zeroed in on snowflake and exclude perfectly good non snowflake experience. I think you are in too deep if someone without recent ( let‚Äôs say last year) snowflake experience can‚Äôt join the team easily. That‚Äôs the worry is if you exclude the talent pool too much. It doesn‚Äôt make a fun interview if the interviewers only know snowflake and no other way to approach a problem.",
          "score": 5,
          "created_utc": "2025-12-28 17:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf1a1j",
              "author": "stuckplayingLoL",
              "text": "Good perspective. I do feel like most of the complex portion of the code is within the Snowflake tasks, but the general pattern from ingesting raw data to making customer ready datasets is consistent. I don't think junior engineers could take a look at the overall architecture and understand how their day to day work fits in the model without some mentorship. But I assume that's just how it goes with data engineering.",
              "score": 2,
              "created_utc": "2025-12-28 19:53:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwf7apx",
                  "author": "goblueioe42",
                  "text": "That‚Äôs fair. As long as you can say yes I would use airflow or tasks. Or we could use snow pipe or flink or spark streaming. I see what you mean. The only worry is lockout of great candidates. I think that perspective is fair",
                  "score": 2,
                  "created_utc": "2025-12-28 20:22:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgt6pu",
          "author": "sib_n",
          "text": "Having most of your transformations coded as SQL is a good thing. SQL is the most stable tech in data for the past 30 years, so you should be able to easily port it to another SQL engine in the future if needed.  \nI think your stronger dependency, here, is using Snowflake as an orchestrator. You could move this to something open source like Airflow, Dagster or Prefect. But if it's currently working well for you and your budget is in control, don't change it just for the sake of it, wait for a rational reason.",
          "score": 6,
          "created_utc": "2025-12-29 01:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe84zq",
          "author": "Hofi2010",
          "text": "So you can call you architecture medallion even if the bronze, silver and gold outside your data warehouse. \n\nA lot of teams migrating transformation outside of snowflake or redshift or GBC. We use iceberg tables stored in s3 and use Fargate or EC2 to transform the data. Then we map the gold layer into redshift. You can do the same with snowflake. For performance probably better to ingest gold layer into snowflake.\n\nFor transformation we use duckdb on an EC2 with dbt. If you already use DBT this would be an easy shift.\n\nHere a medium article where I describe the basic principle \nhttps://medium.com/@klaushofenbitzer/save-up-to-90-on-your-data-warehouse-lakehouse-with-an-in-process-database-duckdb-63892e76676e\n\nThis architecture is as fast or even faster than snowflake for about 10% of the cost",
          "score": 3,
          "created_utc": "2025-12-28 17:35:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf1rm3",
              "author": "stuckplayingLoL",
              "text": "Thanks for this thought. I did see that some teams at my company were shifting to iceberg tables and using Airflow with AWS but wasn't sure how mature their processes were. I'll look into this topic and see how it changes things, because it sounds like a huge architectural shift in the long run.",
              "score": 1,
              "created_utc": "2025-12-28 19:55:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfi34z",
                  "author": "Hofi2010",
                  "text": "Depends on if you are optimizing for cost or ease of use.",
                  "score": 2,
                  "created_utc": "2025-12-28 21:15:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwe4so1",
          "author": "chock-a-block",
          "text": "Not according to c-level who have to listen to the whims of the BOD.¬†\n\nThe c-level pissing contents about tech are pretty comical.¬†",
          "score": 2,
          "created_utc": "2025-12-28 17:19:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwewuck",
              "author": "M4A1SD__",
              "text": "BOD?",
              "score": 1,
              "created_utc": "2025-12-28 19:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwf05rg",
                  "author": "GreyHairedDWGuy",
                  "text": "board of directors (I think)",
                  "score": 3,
                  "created_utc": "2025-12-28 19:48:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf9hn6",
          "author": "Think-Trouble623",
          "text": "It seems to me that you‚Äôre facing some of the same questions I have about moving from Azure SQL to Snowflake. Things are going well, pipelines are stable, and costs are relatively managed. \n\nThe question always comes down to headcounts. Can you reduce headcount with an architecture shift or significantly reduce op ex? Rarely are you going to see enough value generated by an architecture shift; unless you aren‚Äôt getting real time data and need it. \n\nMy suggestion is to pick an ‚Äúup and coming‚Äù jr engineer to refractor a pipeline in snowflake and recreate it in another architecture. See what the time commitment and savings are for both, then use it to fund your next pipeline until you have a clear path forward.",
          "score": 2,
          "created_utc": "2025-12-28 20:33:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg4ky6",
          "author": "MonochromeDinosaur",
          "text": "As long as snowflake yearly cost is less than a full engineer‚Äôs salary it‚Äôs pretty cheap IMO. \n\nI‚Äôve defaulted to just using self-hosted airbyte + dbt + snowflake/postgres (depending on data size and requirements) unless I have a good reason not to.\n\nIf you do everything in airbyte + dbt a migration is literally as easy as pointing your airbyte to the new database and then compiling your models and running them to see what breaks and correcting them.\n\nThis works across all databases too it‚Äôd be just as easy to go to bigquery or others. If you set the rule for your job to stick to ANSI SQL or only use dbt utilities that support multiple adapters.\n\nThis is as someone with 9 YOE who‚Äôs done on-prem Hadoop, Spark (self rolled AWS EMR ephemeral and persisten), Databricks, bespoke frameworks, etc. \n\nThere‚Äôs really no platform as ergonomic as snowflake (BQ being a close second) + dbt + standardized ingestion tool IMO. \n\nI honestly avoid companies that don‚Äôt have a stack like this or aren‚Äôt willing to migrate nowadays. I don‚Äôt want to spend my days debugging over-engineered bespoke framework garbage/spark jobs unless I‚Äôm getting paid really well to do it.",
          "score": 2,
          "created_utc": "2025-12-28 23:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf69zc",
          "author": "Bluefoxcrush",
          "text": "Do you use version control?",
          "score": 1,
          "created_utc": "2025-12-28 20:17:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf8ssa",
              "author": "stuckplayingLoL",
              "text": "Yes. We use Github and deploy changes with GitHub Actions.",
              "score": 1,
              "created_utc": "2025-12-28 20:30:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwmp4xp",
          "author": "PossibilityRegular21",
          "text": "On one hand, I've done a bit of this before, and snowflake without VCS can get messy to manage at scale.\n\n\nOn the other hand, I love how simple snowflake is and i'm jealous of your solution. We use DBT and it's *fine* but I also miss the simplicity of tasks.",
          "score": 1,
          "created_utc": "2025-12-29 22:57:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwtx2j",
          "author": "Responsible_Act4032",
          "text": "Don't rely on the opinion of others on here. \n\nWhat I would say is that ALL technology will age out, as new advancements arrive. \n\nThe key goal of any organisation is to avoid and ensure they aren't impacted in the future by cost rises or vendor lock in, or more critically, mean you miss out on another innovative solution. \n\nTake the effort, while with Snowflake, to migrate your data to Iceberg tables. \n\nOnce you've done that, you can stick iwth snowflake, or you can evaluate other more modern vendors. Whoever they are. \n\nFull transparency, I work for one of those vendors.",
          "score": 1,
          "created_utc": "2025-12-31 13:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdwm8c",
          "author": "eljefe6a",
          "text": "Do you have anyone on the team who knows how to program? This seems like a team who only knows SQL and so every task has to be done with SQL.",
          "score": 1,
          "created_utc": "2025-12-28 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweydcc",
              "author": "stuckplayingLoL",
              "text": "Yes. Most engineers know at least enough Python to write a basic ingest from raw to Snowflake. However, our code is all over the place as we do not have any formal organization. It was just write code to get it to work rather than thinking about reusability and classes.",
              "score": 1,
              "created_utc": "2025-12-28 19:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwf1tq7",
                  "author": "eljefe6a",
                  "text": "People who know just enough Python aren't enough. Your Snowflake spend is a function of having the wrong type of data engineers and short sighted management. You won't be able to fix this completely until you've fixed staffing issues first.",
                  "score": 1,
                  "created_utc": "2025-12-28 19:55:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwggke5",
                  "author": "Sublime-01",
                  "text": "Get claude code - have claude code refactor ur scripts",
                  "score": 1,
                  "created_utc": "2025-12-29 00:13:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdz60p",
          "author": "OtherwiseGroup3162",
          "text": "Do you mind if I ask around how much is your Snowflake costs? We have about 5TB of data, and people are pushing for snowflakes, but it is hard to determine the cost before jumping in.",
          "score": 1,
          "created_utc": "2025-12-28 16:51:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwezsjc",
              "author": "stuckplayingLoL",
              "text": "I don't know what our costs look like right now (away from work thanks to holidays) but can safely assume that majority of costs is in compute over storage. We are ramping up on more streams and tasks as we barely touched the surface of the raw data that we have already ingested. Hopefully someone has more of a concrete example.",
              "score": 1,
              "created_utc": "2025-12-28 19:46:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhqx45",
                  "author": "Choperello",
                  "text": "If you don‚Äôt know how your costs are then you can‚Äôt say costs are (or aren‚Äôt a concern). You‚Äôre guessing. Go see what your costs are before costs are a concern. The first rule of optimizing is measure before doing anything.",
                  "score": 1,
                  "created_utc": "2025-12-29 04:42:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwezvsj",
              "author": "GreyHairedDWGuy",
              "text": "Storage is cheap in Snowflake and almost a non-factor for 5TB.  Compute is here the cost comes from.  If you are doing a lot of processing inside Snowflake, that is where costs mainly come from.\n\nGet a trial of Snowflake as if possible and try it yourself.",
              "score": 1,
              "created_utc": "2025-12-28 19:46:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pywd3j",
      "title": "How are you using Databricks in your company?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pywd3j/how_are_you_using_databricks_in_your_company/",
      "author": "L3GOLAS234",
      "created_utc": "2025-12-29 19:56:58",
      "score": 43,
      "num_comments": 14,
      "upvote_ratio": 0.92,
      "text": "Hello. I have many years of experience, but I've never worked with Databricks, and I'm planning to learn it on my own. I just signed up for the free edition and there are a ton of different menus for different features, so I was wondering how every company uses Databricks, to narrow the scope of what I need to learn.\n\nDo you mostly use it just as a Spark compute engine? And then trigger Databricks jobs from Airflow/other schedules? Or are other features actually useful? \n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pywd3j/how_are_you_using_databricks_in_your_company/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwn3zo1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 24,
          "created_utc": "2025-12-30 00:18:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nws9mqe",
              "author": "puzzleboi24680",
              "text": "Second this. Unity Catalog/easy analytics interface is what Databricks is good for. Then you pay their crazy expenses on everything that locks you into. There's progress on multi-compute abilities, but it's slow. Unity & plus some of their proprietary Spark features are absolutely the killer apps tho. But boy do you pay.",
              "score": 2,
              "created_utc": "2025-12-30 19:38:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwzgh1e",
              "author": "minneDomer",
              "text": "Commenter is a solutions architect for Databricks, based on their comment history - take this with a grain of salt. It‚Äôs a good platform but I‚Äôm never a fan of self promotion without disclosures, and this sub is particularly bad at it.",
              "score": 2,
              "created_utc": "2025-12-31 21:52:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoo42n",
          "author": "addictzz",
          "text": "Databricks is an all-in-one data platform.\n\nStart at the very origin of Databricks itself which is data engineering. You can build data pipelines in declarative (SQL) or procedural way (Python).\n\nThen once you start to accumulate some data, you can use the platform as data governance tool using Unity Catalog and control who can access what or check data lineage.\n\nWith sufficient amount of good quality data, you can start opening up the platform for data science team to let them train & manage ML models or create GenAI-powered application.\n\nYou can further open up the platform for your Data/BI analysts team for data analysis purpose. Databricks has this AI-powered coding assistant called Databricks Assistant which can help your data analysts with SQL or Python syntax should they need it.\n\nIf any of your software devs (or data scientist turned software engineer) are required to build web applications, there is Databricks Apps to let you build a web app based on popular frameworks like Flask/Streamlit or even NodeJS. Lakebase can also help if you need low-latency OLTP database for your web app.\n\nYou can also open up Databricks to your business users, business stakeholders, or C-level to let them ask questions about data using Genie or to let them view dashboards. If those business users accessing Databricks directly, you can minimize the array of menu down using an access control called Permissions. Or you can also integrate Databricks with BI platform of your choice like Tableau/PowerBI/Superset/Metabase.\n\nI think that covers quite a lot of use case across several teams already.",
          "score": 12,
          "created_utc": "2025-12-30 05:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlsrlu",
          "author": "LargeSale8354",
          "text": "Start with the basics. The fundamentals of what DataBricks is, what it's components are,  what their pros/cons are.\nTheir Data Analyst material goes into DataBricks SQL and what Unity Catalog is. It's good beginners stuff.\nAfter that take a look at the Data Engineer associate material. Again, Unity Catalogue features heavily in the curriculum. It will introduce basic data frame operations and configuration options.",
          "score": 4,
          "created_utc": "2025-12-29 20:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlpw6z",
          "author": "git0ffmylawnm8",
          "text": "On the data engineering side, lots of pipelines and jobs feeding into external tables for Snowflake consumption. \n\nI know data scientists also test out models in our Databricks instance",
          "score": 4,
          "created_utc": "2025-12-29 20:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlwywi",
              "author": "CozyNorth9",
              "text": "Is there something you are getting from Snowflake that you can't get in Databricks?  And I suppose the other way around, is there a need for Databricks when you have snowflake?  They both seem to solve the same problems.",
              "score": 18,
              "created_utc": "2025-12-29 20:37:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwm67bb",
          "author": "pantshee",
          "text": "We process lots of kafka flows to expose them as tables in unity catalog. Or some night batch treatments to send data to other applications. And we consume those data with powerbi or dataiku, mostly",
          "score": 1,
          "created_utc": "2025-12-29 21:22:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmnpl5",
          "author": "klubmo",
          "text": "I work for a medium sized consulting firm with many of our clients in the Fortune 500 using Databricks. All use the data engineering tools in some regard, but on the more mature end we‚Äôre using all the features. Lots of data science work (both traditional and GenAI), Apps + Lakebase, data governance, etc.",
          "score": 1,
          "created_utc": "2025-12-29 22:50:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoc3qj",
              "author": "SlappyBlunt777",
              "text": "I‚Äôm stuck in a medium sized manufacturing firm that doesn‚Äôt want to think about leaving MS SQL server. The issue is golden handcuffs at a 150k+ salary. I have an offer for a company willing to pay a bit less but is already on databricks. Is it worth taking one step back for 2 steps forward??",
              "score": 2,
              "created_utc": "2025-12-30 04:26:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwojy5i",
                  "author": "klubmo",
                  "text": "Tough to say depending on what you want from life, how the company treats you, and what future opportunities you see there.  \n\nDepending on skills and experience you can make that kind of money at lots of firms, although you are starting to get into the higher end of the range there.  That being said many of the best DEs I work with are over $200K between salary and bonus (and often potential for company stock), so don't let those gold handcuffs keep you from the life you want to live. \n\nWho knows, maybe a change in leadership at the company will eventually push you toward a Databricks or Snowflake type of platform. I see it all the time...we talk to companies like yours, they aren't interested in cloud...fast forward 3-5 years and everything changes due to a new CEO or VP.",
                  "score": 1,
                  "created_utc": "2025-12-30 05:18:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu690i",
          "author": "Unlucky_Data4569",
          "text": "We write code that moves data in s3. We only use it for etl. Queries in athena and snowflake",
          "score": 1,
          "created_utc": "2025-12-31 01:28:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxccds5",
              "author": "_Marwan02",
              "text": "Why not use emr in this case instead of dbx ?",
              "score": 1,
              "created_utc": "2026-01-02 23:24:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv2xbb",
          "author": "Born-Pirate1349",
          "text": "Databricks is nothing but Spark but just a managed version and much more efficient one. We usually use databricks for our internal testing and exploring more features. I would recommend you to go through their certification which are \"Databricks certified data engineer associate\" and \"Databricks certified data engineer professional\". These are very good certifications in terms of understanding what databricks actually are. Bit of costly but worth it. Even Udemy has similar courses you can try it out.\n\nAlways start with understanding their infrastructure and architecture and start understanding one by one features in terms of data layout optimizations, table formats, catalog layer, governance layer.\nAlso try setting up each thing in databricks and by the side you will learn all the things.\n\nFrankly speaking I haven't taken any courses but I started exploring the features and did many pocs on each of them. \nAlso my suggestion is that don't try to scope it into your usecase, try out all things and think of it as a data engineering platform journey because you will learn many things over there so. Happy learning!!",
          "score": 1,
          "created_utc": "2025-12-31 04:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx59y8u",
          "author": "Dry_Professional8254",
          "text": "Same as you. I'm about to put my hands on Databricks in my work without having any previous experience with this tool.\n\n  \nI already know the fundamentals, and I've been studying Spark like how to improve queries, etc. I want to explore the Data Factory pipelines migration to Databricks, because you gain more flexibility leaving ADF to only orchestrate these migrated pipelines.",
          "score": 1,
          "created_utc": "2026-01-01 21:41:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1tqt0",
      "title": "Why don't people read documentation",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q1tqt0/why_dont_people_read_documentation/",
      "author": "LargeSale8354",
      "created_utc": "2026-01-02 08:54:23",
      "score": 42,
      "num_comments": 46,
      "upvote_ratio": 0.85,
      "text": "I used to work for a documentation company as a developer and CMS specialist. Although the people doing the information architecture, content generation and editing were specialist roles, I learned a great deal from them. I have always documented the systems I have worked on using the techniques I've learned. \n\nI've had colleagues come to me saying  they knew I \"would have documented how it works\".  From this I know we had a findability issue.\n\nOn various Redit threads there are people who are adamant that documentation is a waste of time and that people don't read it.\n\nWhat are the reasons people don't read the documentation and are the reasons solvable? \n\nI mention findability, which suggests a decent search engine is needed.\n\nI've done a lot of work on auto-documenting databases and code. There's a lot of capability there but not so much use of the capability. \n\nI don't mind people asking me how things work but I'm one person. There's only so much I can do without impacting my other work.\n\nOn one hand I see people bemoaning the lack of documentation but on the other hand being adamant that it's not something they should do",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q1tqt0/why_dont_people_read_documentation/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx84e3s",
          "author": "Ploasd",
          "text": "I read the documentation when I need to consult it. \n\nThe likely answer is that documentation is boring. A necessary thing sure, but boring :-D",
          "score": 53,
          "created_utc": "2026-01-02 08:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa549m",
              "author": "loady",
              "text": "boring and usually out of date\n\nI see documentation as a tool to avoid meetings.  \"this thing you're asking about?  the answer is _here_ ...\"\n\nhas saved me probably a thousand hours",
              "score": 19,
              "created_utc": "2026-01-02 16:59:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbiif4",
                  "author": "Less_Veterinarian_60",
                  "text": "Same.. if a question pops up more than once its going in the docs",
                  "score": 1,
                  "created_utc": "2026-01-02 20:53:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88v2o",
          "author": "Cruxwright",
          "text": "I have coworkers that can't read past the first line in an e-mail. Hell, I barely read past the 3rd line of your post.\n\nWith that in mind, documentation sucks. I need to figure out what data is going into positions 33-48 on record type 22. I'm opening some pdf and need to scroll through 15 pages of ToC and change logs. No links in the ToC and the terms I'm searching for have either 0 or 144 hits in the document. I finally get to the file layout section. It's 8 columns wide in letter format and someone wrote paragraphs that I have to scroll past 2 words per line.\n\nI finally find the 4 references to how record type 22 is generated and the documentation just says payment type. I now need to go read vague 1-liners as to why record 22 is being generated in this or that instance. Turns out I need to go read IRS instructions on 1099R forms to figure out how this code is supposed to be used. Of course, the IRS is not referencing our internal code sets so I need to cross reference all of those too.\n\nI have read through 1000's of pages of documentation over the years and most of the time it only tells me things I already know and doesn't answer my questions.",
          "score": 30,
          "created_utc": "2026-01-02 09:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8gxw2",
              "author": "Dangerous-Skirt-9234",
              "text": "TLDR",
              "score": 14,
              "created_utc": "2026-01-02 10:57:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8ictb",
                  "author": "codykonior",
                  "text": "Documentation is too verbose and on the wrong things.",
                  "score": 11,
                  "created_utc": "2026-01-02 11:09:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx87h5u",
          "author": "gelato012",
          "text": "Keep doing documentation and point to it when needed. Don‚Äôt give up. It‚Äôs needed and the right way of working.",
          "score": 18,
          "created_utc": "2026-01-02 09:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8uo8g",
          "author": "LargeSale8354",
          "text": "Many of the comments here confirm something that has been bugging me.\n1. Writing useful documentation is a skill\n2. Information architecture is a skill\n3. Librarianship is a skill.\n\nAll of the above are massively under valued. \n\nSOLID principles are great for software development. Single responsibility and DRY work for documentation too.\n\nCMS experience taught me that documents are made up of reusable sections and templates. Being able to pull relevant sections into a document is a trick that has been around for decades.\n\nGen-AI can't polish a turd but it can roll it in glitter.\n\nIf the source documentation is well structured and focused on a specific context the Gen AI stands a much better chance of outputting what we need",
          "score": 17,
          "created_utc": "2026-01-02 12:50:36",
          "is_submitter": true,
          "replies": [
            {
              "id": "nxa31hq",
              "author": "Character-Education3",
              "text": "The ROI on documentation is intangible for business leaders. Saying \"a year from now this is going to help our devs\" means nothing. The response is \"yes very important, write good docs everyone.\"\nIf documentation correlated positively to quarterly profits you would have their undivided attention and writing documentation would be a priority. There would be documentation developers, documentation PMs, documentation test engineers, senior documentation engineers! Headcount specifically for writing and maintaining docs would increase significantly.\n\nLike preventative maintenance. If it is audited and scrutinized in a meaningful way by the FDA for example or it is mission critical for daily operations then it gets prioritized. If it is \"strongly recommended\" then it is a If your leaning your cleaning type of thing that no one wants to do until it creates a problem.\n\nSpeaking of FDA. In pharmaceutical manufacturing the FDA audits the Quality Management System which includes the documentation for all your procedures. If for example an out of date paper copy of a procedure was in a lab or on the line they will send a 483 letter to all your clients and possibly sanction you depending on the circumstances. So in pharmaceutical companies there are Document Control departments and employees specifically for documentation. If you have an SOP that says put container A on the right and employees decided that it goes on the left and didn't update the SOP that is a violation. So documentation is prioritized because it can effect the bottom line",
              "score": 5,
              "created_utc": "2026-01-02 16:50:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxa77wz",
              "author": "EarthGoddessDude",
              "text": "Please do tell us some of these techniques you mention. \n\nI agree with you but I also agree with the poster who said documentation is often times long, painful to go through, and often ultimately not very helpful. \n\nPersonally, I have landed on Documentation as Code a long time ago. I keep insisting at my job that documentation should live in GitHub and be subject to the same SDLC as code, with PRs, ‚Äúcode‚Äù reviews, etc. I‚Äôm a big fan of quarto and using it for documentation. Also mkdocs though I prefer quarto. But that‚Äôs just the tooling, it‚Äôs still missing what to document, how to document it well, and how to make sure everyone knows where it is. Building a culture around documentation is really hard because most don‚Äôt give a shit. Hell, most people don‚Äôt even give a shit about code quality either, so then caring about docs is an even bigger stretch. \n\nI‚Äôm aware of the four different types of documentation a la: \n- https://www.writethedocs.org/videos/eu/2017/the-four-kinds-of-documentation-and-why-you-need-to-understand-what-they-are-daniele-procida/\n- https://diataxis.fr/\n- https://danielsieger.com/blog/2023/04/24/framework-for-better-documentation.html\n\nBut these sometimes feel like overkill. If you can justify it‚Äôs not overkill, then you need management to agree that resources need to be dedicated to generating proper documentation, and often times that doesn‚Äôt happen, it‚Äôs just ‚Äúyea make sure this is documented‚Äù. \n\nSo genuinely curious what these techniques are and where I can learn more about them.",
              "score": 1,
              "created_utc": "2026-01-02 17:09:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxdy9th",
              "author": "reelznfeelz",
              "text": "For sure.  I will say though that gen AI is helpful in writing docs though.  It just can‚Äôt be the whole job.  Asking Claude code to ‚Äúdocument this please‚Äù will probably give you something pretty off target.  But, it‚Äôs a damned useful tool still, IMO.",
              "score": 1,
              "created_utc": "2026-01-03 05:07:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8lhtr",
          "author": "confusing-world",
          "text": "I like to read documentations, but I'll mention one problem about documentation that really gets me angry. The problem is people that don't know how to write. I'm not even talking about grammar, because I'm also bad with it (I'm not native in English). However, I don't like texts that are not straightforward to the goal.\n\nOne example is dbt documentation. This doc was driving me crazy because it was too verbose with stupid things. For example, in the section about the dbt folder structures, they spend 3 or 4 paragraphs explaining why organization is important. I don't care for it, everyone knows that organization is important, just go to the point and tell us how to organize in DBT.\nI have the doc here, you can take a look. Basically 70% of the text is useless to understand dbt organization. \nhttps://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview\n\nIn the documentation we have to be direct to the goal of the tool and explain how it works. There is no need to make stupid analogies for 5 years old kids. Who is reading the docs is expert enough to understand it.",
          "score": 12,
          "created_utc": "2026-01-02 11:37:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8j0yd",
          "author": "codykonior",
          "text": "Searchability aside, foundational knowledge is an issue.\n\nIf I document my sqlmesh, it assumes you know how sqlmesh works, the servers it runs on, have access to those servers, but also deeply know PowerShell and custom application database structures and firewalls and a bunch of other services that wrap it all up.\n\nEvery attempt at documenting it ends up boiling the ocean.\n\nEven if you take a task based approach, you still need a hell of a lot of that, including setting up your own environment from scratch. \n\nEvery task in turn has to be maintained. The more you seperate into their own articles the more of an unreadable maze of links it becomes. And then the user is drowning in information. \n\nIt's not solvable. Do what you can. But there's a reason people turn away from writing or reading it.\n\nAlso who wants to pay for you to write it? Not many.",
          "score": 5,
          "created_utc": "2026-01-02 11:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8y6dt",
          "author": "Wh00ster",
          "text": "At many companies, documentation and systems change faster than it can be written and consumed.",
          "score": 3,
          "created_utc": "2026-01-02 13:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx90iep",
          "author": "MikeDoesEverything",
          "text": "Huge difference in standards between teams as well as no universal standard between teams across the whole industry.\n\nComing from a science background, you take something like SOPs and technical documentation standards for granted.  Sure, building software isn't like operating a heavy machine or piece of technical equipment although, in my opinion, the software engineering world could learn a lot from the scientific industry on how to handle technical documentation.  \n\nOnly issue is, as people have mentioned in the thread, is that SOPs usually last a very long time in the science world (instruments are typically a big investment) whereas software can change quite rapidly multiple times.",
          "score": 5,
          "created_utc": "2026-01-02 13:29:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8h98j",
          "author": "billysacco",
          "text": "Only a handful of replies here and they cover many of the pitfalls I have seen with documentation in the field. However something is always better then nothing so I say do the best you can with documentation.",
          "score": 3,
          "created_utc": "2026-01-02 11:00:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdxyix",
          "author": "reelznfeelz",
          "text": "I‚Äôll do you one better, why don‚Äôt people \\*write\\* documentation?  I kind of like doing it.  Puts a nice bow on things, and means either future me, or some other engineer, will be able to get back up to speed quicker.  With AI tooling it‚Äôs not hard to make a basic readme while the project is fresh.",
          "score": 3,
          "created_utc": "2026-01-03 05:05:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxet6r0",
              "author": "LargeSale8354",
              "text": "This sums it up exactly for me.  I look at code I wrote that I thought was \"self documenting\". Sure, while it's fresh in your mind it my be. 12 months on \n, git blame paints a different story.\n\nAs a senior engineer I write documentation as short tutorials and link them to related material. \n\nI got co-pilot to diagram our git workflows. It took some fiddling around with prompts but eventually it spat out the drawio.svg files I wanted.",
              "score": 1,
              "created_utc": "2026-01-03 09:23:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nximwv2",
                  "author": "reelznfeelz",
                  "text": "Yes, for sure, I find there's a lot of engineers who are harsh on AI, and for sure it gets abused and 'slop' is on the rise.  But, it's a very powerful tool, given the right information, and prompts, it can do stuff like diagramming quite well.  Which saves some serious time.  Just need a QA process even if that's you looking at it to make sure it's not wrong, and read reasonably well, before you call it \"done\".",
                  "score": 1,
                  "created_utc": "2026-01-03 22:12:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8evxs",
          "author": "Hackerjurassicpark",
          "text": "Because documentation is almost always outdated",
          "score": 5,
          "created_utc": "2026-01-02 10:38:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8l17q",
          "author": "SnooGiraffes7113",
          "text": "Documentation is engineer's arch nemesis.\n\nBut no matter how much you avoid, you have to fact it.",
          "score": 2,
          "created_utc": "2026-01-02 11:33:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8pxwh",
          "author": "FooBarBazQux123",
          "text": "I just read onboarding documentation, and not much else. My reason is that documentation is easily out dated, and it‚Äôs hard to maintain.\n\nWhen we write documentation, half of the team does not read it.\n\nWith the AI though, I‚Äôm discovering the value of documentation to support LLMs prompts, and they also make it easier to maintain documentation.",
          "score": 2,
          "created_utc": "2026-01-02 12:14:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8ryd2",
          "author": "GlueSniffingEnabler",
          "text": "All my questions are approached in this order: Brain, Book (documentation), Buddy, Boss.\n\nSometimes people won‚Äôt tell you when the documentation isn‚Äôt clear enough, but it‚Äôs ok to ask them to review it first.",
          "score": 2,
          "created_utc": "2026-01-02 12:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx96snd",
          "author": "StewieGriffin26",
          "text": "Because the documentation is written by someone who left a year ago, on a process that is no longer being maintained, or has since been migrated off of.",
          "score": 2,
          "created_utc": "2026-01-02 14:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9pcrz",
          "author": "Icy_Clench",
          "text": "In my experience it‚Äôs learned helplessness. Usually coworkers think I have all the answers and ask me first. Honestly I feed the cycle because it makes me look better.",
          "score": 2,
          "created_utc": "2026-01-02 15:45:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8d72e",
          "author": "Particular_Scar2211",
          "text": "Because it's time consuming, confusing, and doesn't mention your exact use case.\nDocumentation nowadays should be integrated into an llm.",
          "score": 1,
          "created_utc": "2026-01-02 10:22:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx87yak",
          "author": "PrestigiousAnt3766",
          "text": "Its been the same for home appliances since whenever.",
          "score": 1,
          "created_utc": "2026-01-02 09:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx887hn",
          "author": "NoleMercy05",
          "text": "A tale as old as time...\n\n![gif](giphy|jtcJWWyg7vbC70xYJV)",
          "score": 1,
          "created_utc": "2026-01-02 09:35:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8pljq",
          "author": "RunOrdinary8000",
          "text": "There is not a lot of good documentation around.\nI have seen the following concepts:\n- ask someone ( means no documentation)\n- the Program is documentation ( 100k lines of Code, the documentation has been a word file recite the application, no joke)\n- as much as needed as little as possible ( best, but you needed to learn how the structure was. So first days it was documentation review, not code review for new people)\n- newspaper headliners\n- Wall of text. ( I had it once as comment. 10 lines of Code, 200 lines statistical mathematical explanation what is going on)\n- haystack documentation ( so they wrote down everything, but it has no order if you do not remember the key search word good luck. SAS application documentation is a good example)\n\nAll these do not motivate anyone to do documentation.\nAnd you know if management thinks we need to rush, documentation is the first that gets sacrificed.",
          "score": 1,
          "created_utc": "2026-01-02 12:11:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8px0m",
          "author": "0sergio-hash",
          "text": "It's not usually taught when you're learning from senior people, so it takes some people awhile to even realize that's where they should start debugging \n\nEven with AI, I usually ask for hyperlinks to the specific documentation reinforcing the answer so I can go read it in depth \n\nHowever, documentation usually is written terribly and hard to search for \n\nI have a source system I'm trying to pull data out of and I keep googling source system name plus technical documentation plus subject to find it \n\nAnd that's not every site, I have seen some tools with decently organized documentation, but in a lot of cases it sucks \n\nAlso, it's written terribly. Everything I know about postgresql and everything I know about snowflake sql I've learned from books that are essentially better written versions of the documentation \n\nThey pull all the info from there and then put it into words that are actually interesting to read ü§£ Great hustle if you can write",
          "score": 1,
          "created_utc": "2026-01-02 12:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8sgzm",
          "author": "TheHighlander52",
          "text": "Ive found two issues with documentation. It‚Äôs either overly simplified and doesn‚Äôt address real world use cases, or it‚Äôs overly verbose and difficult to find what you actually need. Once you get past the boiler plate solutions given, it can be challenging to figure out what you can or can‚Äôt do if you‚Äôve never worked with the software/product before.",
          "score": 1,
          "created_utc": "2026-01-02 12:34:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx94p1g",
          "author": "elephant_ua",
          "text": "The only documentation i ever managed to read was sql lite documentation.\n\n Everything else i read was a nighmare more or less. Especcially Microsoft - you are trying to read it and you understand less after you read the page. It just feels like it was written for people who already know what this is/does and don't actually need it. If you don't know in advance - fck you. You want to figure out what to or what this piece of software can do (also, how can it be done other way and why can i chose any of them) and the documentation just isn't written to answer these question.",
          "score": 1,
          "created_utc": "2026-01-02 13:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9ar0s",
          "author": "Lower_Sun_7354",
          "text": "Your post is a tldr, just like a lot of docs.  Plus, I dont always get the time or know which part of the docs are valuable.  I still read what I can, when I can.",
          "score": 1,
          "created_utc": "2026-01-02 14:29:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9ko4j",
          "author": "kevkaneki",
          "text": "The same reason don‚Äôt people contracts or TOS agreements. \n\nEveryone knows they should, but nobody enjoys doing it.",
          "score": 1,
          "created_utc": "2026-01-02 15:22:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9leh5",
          "author": "Gators1992",
          "text": "I think one reason people don't like to document is that they think they have to document everything.¬† Like putting comments on stuff that's obvious rather than just documenting what is useful like why you made key decisions and what the context is behind the code.¬† Writing comments for every column in a database is boring AF, but so can do a lot of that for typical companies.¬†¬†",
          "score": 1,
          "created_utc": "2026-01-02 15:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa1rhj",
          "author": "crytomaniac2000",
          "text": "Sure, I could spend weeks documenting that 3000 line stored procedure our team inherited, but no one besides me will ever read it.  Or I can use AI to document it quickly, it may or may not be right but I can say I ‚Äúused AI‚Äù and that it‚Äôs documented.  Either way, the reality is the business people will ask me what happened when it breaks and I‚Äôll have to read through the code and figure it out.",
          "score": 1,
          "created_utc": "2026-01-02 16:44:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa58zz",
          "author": "Reverie_of_an_INTP",
          "text": "The code is the documentation. I read the code.",
          "score": 1,
          "created_utc": "2026-01-02 17:00:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxanztr",
          "author": "VanillaRiceRice",
          "text": "Because there's been an explosion in the number of Engineers in the last 20 years, all churning out projects and docs at lightning pace, each with our language and style. Docs are the real killer, and IMO the real killer use-case for LLMs.",
          "score": 1,
          "created_utc": "2026-01-02 18:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbddd9",
          "author": "riomorder",
          "text": "I read a lot of Databricks documentation, because is online and I use it as google",
          "score": 1,
          "created_utc": "2026-01-02 20:28:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbiaem",
          "author": "justexisting2",
          "text": "You don't need documentation unless something breaks or a change needs to be made by someone who is unaware of how things works.",
          "score": 1,
          "created_utc": "2026-01-02 20:52:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd8jda",
          "author": "ephemeral404",
          "text": "It's a lot of work. The person reading tells the person who wrote the docs :)",
          "score": 1,
          "created_utc": "2026-01-03 02:25:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxepv2w",
          "author": "enlightenedude",
          "text": "your post is spammed by ai turd boots",
          "score": 1,
          "created_utc": "2026-01-03 08:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny03hyp",
          "author": "Davisparrago",
          "text": "People are full of work, i cant spend 1 hour reading through documentation (n times) that, in most cases, are out of date or not detailed enough.\n\nIt made sense a few decades ago in which systems weren't a live service in constant change.\n\nTry to study medicine in a body that functions differently every year",
          "score": 1,
          "created_utc": "2026-01-06 13:20:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxb40pm",
          "author": "TheOverzealousEngie",
          "text": "Because documentation is a lazy programmers tool. They have been completely replaced by well designed and effective Graphical User Interfaces. That's how the world works now, if you don't have a good GUI you don't have a good product. If you're complaining that you people don't RTFM then you're a dinosaur.",
          "score": 0,
          "created_utc": "2026-01-02 19:42:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbbtvn",
              "author": "LargeSale8354",
              "text": "Not everything has a GUI.",
              "score": 1,
              "created_utc": "2026-01-02 20:20:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q12ev1",
      "title": "Advent of code challenges solved in pure SQL",
      "subreddit": "dataengineering",
      "url": "https://clickhouse.com/blog/clickhouse-advent-of-code-2025",
      "author": "Creative-Skin9554",
      "created_utc": "2026-01-01 11:44:37",
      "score": 40,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q12ev1/advent_of_code_challenges_solved_in_pure_sql/",
      "domain": "clickhouse.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q3fp3h",
      "title": "Looking for Udemy / YouTube course recommendations for AWS Data Engineer certification",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q3fp3h/looking_for_udemy_youtube_course_recommendations/",
      "author": "Sreekar_yadav",
      "created_utc": "2026-01-04 03:57:04",
      "score": 39,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "Hi everyone,\nI‚Äôm planning to prepare for the AWS Data Engineer certification and looking for Udemy / YouTube course recommendations.\n\nBackground:\nAWS CCP certified (2 years ago)\nBasic AWS + data concepts\nLooking for hands-on, practical, exam-relevant resources (Glue, Athena, Redshift, S3, etc.).\n\nIf you‚Äôve used a course that worked well (or should be avoided), please share.\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q3fp3h/looking_for_udemy_youtube_course_recommendations/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxkepb7",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-04 03:57:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl5p1c",
          "author": "itexamples",
          "text": "* AWS Certified Data Engineer Associate 2026 - Hands On!\n* Complete AWS Certified Data Engineer Associate - DEA-C01\n* Practice Exams | AWS Certified Data Engineer - Associate",
          "score": 5,
          "created_utc": "2026-01-04 07:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkqo2o",
          "author": "Known_Example_3005",
          "text": "Same Question. \nPlease upvote so that I will know",
          "score": 3,
          "created_utc": "2026-01-04 05:15:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxtsi9",
      "title": "Implementation of SCD type 2",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pxtsi9/implementation_of_scd_type_2/",
      "author": "Rare_Decision276",
      "created_utc": "2025-12-28 15:19:49",
      "score": 33,
      "num_comments": 61,
      "upvote_ratio": 0.81,
      "text": "Hi all, \n\nWant to know how you guys implement SCD type 2? Will you write code in PySpark or do in databricks?\n\nBecause in databricks we have lakeflow declarative pipelines there we can implement in much better way compare to traditional style of implementing?? \n\nWhich one you will follow?? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pxtsi9/implementation_of_scd_type_2/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwdq9vi",
          "author": "moshujsg",
          "text": "All you need to do is add a valid from and valid to columns and when the record exists, mark last one obsolete. Mark new one current. I dont understand what else there is to it? Why would you ever use python for thid instead of just sql?",
          "score": 42,
          "created_utc": "2025-12-28 16:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdr0f8",
              "author": "financialthrowaw2020",
              "text": "There are many ways to do this, but I agree, it's such an easy thing to do that sql is the answer 9 times out of 10",
              "score": 8,
              "created_utc": "2025-12-28 16:10:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwds6ju",
                  "author": "moshujsg",
                  "text": "Yeah, whatever technique you use it seems like just sql is the answer, its optimized for these things.",
                  "score": 3,
                  "created_utc": "2025-12-28 16:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe597r",
              "author": "Embarrassed-Falcon71",
              "text": "It‚Äôs not that simple in spark + delta. You‚Äôll need a couple of lines",
              "score": 3,
              "created_utc": "2025-12-28 17:21:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe5r1p",
                  "author": "moshujsg",
                  "text": "The concept is still simple, maybe hard to write, but simple",
                  "score": 1,
                  "created_utc": "2025-12-28 17:23:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwforkx",
                  "author": "Budget-Minimum6040",
                  "text": "You will need at least a of couple of lines in every way. Writing a \"merge into\" is not a 1-liner.\n\nBe it pySpark, polars or SQL.",
                  "score": 1,
                  "created_utc": "2025-12-28 21:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe6dwv",
              "author": "kenfar",
              "text": "I've written python libraries and programs to do this a few times, and definitely prefer this to using SQL.  Some common features of these tools:\n\n   * optionally dedup data\n   * ignore some columns\n   * assign the keys - both keys for the subject as well as keys for the version of the subject\n   * transform the fields\n   * populate other fields - batch_id, extract_timestamp, load_timestamp, flags to reflect current or non-current rows, etc\n   * update the prior SCD row to reflect new timestamp - typically going from nulls or max timestamp to either the new start timestamp on the next row or that timestamp minus 1 mili/microsecond.\n   * reprocess skipped files\n\nWhile one can do that with a stored procedure or udf, I prefer writing that in a python library that can be called from anywhere, and applied to relational tables or csv files or parquet files or jsonlines files, etc.  And doing it with sql without a udf or stored proc is both a headache and almost guarantee of getting something wrong.",
              "score": 3,
              "created_utc": "2025-12-28 17:27:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe6y5u",
                  "author": "financialthrowaw2020",
                  "text": "All of this is easily done in dbt as part of the transformation, no procs needed. Doing it separately is silly.",
                  "score": 1,
                  "created_utc": "2025-12-28 17:29:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwe6yzo",
                  "author": "moshujsg",
                  "text": "Idk, im of the idea that the table should take care of itself. I feel like its such a simple sql statement that theres no reason not to just have it in sql. The other thungs like dedup or ignore rows is business logic and shouldnt be embedded with the scd logic.",
                  "score": 0,
                  "created_utc": "2025-12-28 17:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwefyg1",
              "author": "thinkingatoms",
              "text": "checking record existence on a big table is expensive\n\nso is invalidating records that no longer exists",
              "score": 1,
              "created_utc": "2025-12-28 18:13:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwegbnl",
                  "author": "moshujsg",
                  "text": "You can index. Brcause its expensive is why you just stick to sql lol. Also exoensive doesnt mean complex.",
                  "score": 0,
                  "created_utc": "2025-12-28 18:15:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe27o4",
              "author": "Quick_Assignment8861",
              "text": "We use python for it because we have a centralized package with utilities. Think of basic stuff like bronze\\_ingestion\\_scd/stacked/events/cdc and then run load logic to silver/bronze. Where it just applies notebooks and in the metadata you write to what table, if u want a snapshot, blabla.",
              "score": 1,
              "created_utc": "2025-12-28 17:06:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdi77a",
          "author": "Misanthropic905",
          "text": "What SCD2 have to do with spark or databriks?",
          "score": 27,
          "created_utc": "2025-12-28 15:24:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdiq9o",
              "author": "Rare_Decision276",
              "text": "Bro I‚Äôm trying to say treat customers data having name, address, city, state, zip code etc., in order to implement SCD type 2 will you write code in traditional way ie generating surrogate key, hash by checking flags and do joins etc or will you do in lakeflow declarative pipelines?",
              "score": -37,
              "created_utc": "2025-12-28 15:27:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdk4cp",
                  "author": "R1ck1360",
                  "text": "SCD2 is a modeling technique, it is not related to any specific framework or tool",
                  "score": 52,
                  "created_utc": "2025-12-28 15:35:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwds2vq",
          "author": "SirGreybush",
          "text": "SCD2 is built into the table schema, the code base is based on Merge or UpSert + delete tracking.\n\nI suggest you look at \"Timestamp\" column type if available on your platform, or look into MD5() function which is quite universal, to do \"hashing\" of data, and use a staging layer.\n\nWhen you design properly a SCD2 table in a bronze layer, and have a staging layer in a \"raw\" layer (you should always have a staging layer!) - then a SQL view with the hashed values (the PK columns, the entire data row) then SCD2 becomes a breeze to do. Breeze because you compare two tables, you have one PK column and one HashDIFF column (or TimeStamp from a source DB) that tells you easily what to do.\n\nOften it is deletion tracking the most difficult - because if you get differential data, how do you know what no longer exists in source? In such cases you need an extra pipeline logic just for delete tracking. Mentioning as this is often overlooked and it comes back to bite you at the end.\n\nFor example, you load new data (from a full source or differential source) truncate/load into a staging table. Make a view to create all the extra columns you need, like using MD5() in the view. Use the staging view to do the UpSerts or Merge based on your bronze layer SCD2 historical table and the staging. \n\nFor delete tracking sometimes you need a new pipeline to get a full from source but only the PK column(s), not all the columns, so the file is smaller and the process is quicker. I use a different staging table, a different pipeline, just for this. Or if you are lucky, you can get CDC from source DB and then you have 100% of all changes. If only we had this all the time.\n\nSo basically you read the same data twice, the first time the data are usually files in a datalake or a file share, but it could also be a remote database that you ODBC / OleDB into to run a query. This is usually for on-prem, but I have used ODBC + MySQL remote DBs to gather info, it works really well, and no ELT/ETL tool required - it's all SQL in a stored proc.\n\nSome people create in the staging table all the extra fields to manage for SCD2 and the tool they use to consume the raw data into staging, adds the values to the extra fields, row-by-row. I do NOT like this method, as it puts intelligence inside the ELT/ETL tool making it difficult to manage. An ELT/ETL tool should be simple, just do mapping, Lift & Shift, ideally never fail - just get the data to staging table(s) and let the next level deal with issues. CSV is hell on earth, I use the BLOB technique on foreign CSV files and parse with SQL.\n\nFWIW, this topic (SCD2, 3, 4, hybrid...) is taught in actual classes world wide, maybe look into taking some basic ETL/ELT & BI courses, at least Udemy. My first SCD2 table I did back in 1998 - just to give you an idea on how old this logic is, the actual invention I don't know who - but probably Kimball in the 90's. I had a great database teacher that was ahead of  his time.",
          "score": 7,
          "created_utc": "2025-12-28 16:15:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdkedc",
          "author": "Dry-Aioli-6138",
          "text": "I would poc both and see which fits our flow better.",
          "score": 4,
          "created_utc": "2025-12-28 15:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe400l",
          "author": "ForwardSlash813",
          "text": "SCD2 can easily be implemented in Databricks DLT (Declarative Pipeline) using PySpark or SQL.  I prefer SQL.",
          "score": 2,
          "created_utc": "2025-12-28 17:15:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe91su",
              "author": "Rare_Decision276",
              "text": "Yes, absolutely CDC plays a very good role in declarative pipelines but the thing is can we do all tasks like Adhoc queries, pipelines, any queries we can write in DLT right",
              "score": 1,
              "created_utc": "2025-12-28 17:40:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwh0zzl",
                  "author": "ForwardSlash813",
                  "text": "You can run whatever CDC-related queries you like but only within the confines of a DLT pipeline execution.",
                  "score": 2,
                  "created_utc": "2025-12-29 02:07:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf02eh",
          "author": "Healthy_Put_389",
          "text": "I prefer with simple insert statement on hashvalue of scd type 2 columns when the haha value changes and update statement to terminate non existing hashvalues",
          "score": 2,
          "created_utc": "2025-12-28 19:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhxmsk",
          "author": "NoleMercy05",
          "text": "Sql works. It's just tables",
          "score": 2,
          "created_utc": "2025-12-29 05:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6o42",
          "author": "Nemeczekes",
          "text": "Disregarding writing style worthy of 8 year old kid the lakeflow pipelines will never cover the 100% of cases. So if I am using lakeflow and I need scd2 then I would used. \n\nIt is just a quality of life improvement not a game changer",
          "score": 2,
          "created_utc": "2025-12-28 17:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjvec",
          "author": "justanator101",
          "text": "Do you use declarative pipelines currently? Does your team have the technical expertise to implement scd2 in spark? What does the rest of the codebase look like? \n\nI‚Äôd personally implement myself because we‚Äôre a very technical team and prefer having full control and visibility into what runs. However, that does come at a trade off of more complex code base.",
          "score": 1,
          "created_utc": "2025-12-28 15:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdlalz",
              "author": "Rare_Decision276",
              "text": "Yes, it has more complex code base but in databricks we have decorative pipelines we have CDC it will rescue us",
              "score": 1,
              "created_utc": "2025-12-28 15:41:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdtkrc",
                  "author": "SirGreybush",
                  "text": "CDC is the best - especially for deletes.\n\nJust be prepared that in between two CDC updates, a single record in your bronze layer can be updated multiple times. Like a new customer, so an insert in the CDC. Then more than one person doing updates on that same day.\n\nYou must decide if you want all the changes in your SCD2 (I would do that) or just capture the \"latest\".\n\nIn the \"latest\" scenario - from my other comment - using a staging layer - if the source table has columns name similar to CreatedDate & UpdatedDate, and UpdatedDate = CreatedDate on a new insert, simply get the max(UpdatedDate) in your staging - view, by using ROW\\_NUMBER() OVER (PARTITION BY ...) functionality in your staging view.\n\nTo reduce SCD2 traffic, as 99.9% of people will always use IsCurrent=1 in the SCD2 table, and only look at history when there's an actual problem. \n\nIt depends on the source system CDC and how the humans use the source system. I've seen 10+ rows of change per day on a new customer with various ERP systems.",
                  "score": 1,
                  "created_utc": "2025-12-28 16:23:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwlvh7l",
          "author": "mike-manley",
          "text": "Stored procedures. I've sometimes used AUTOINCREMENT om a table INTEGER column and other times a sequence to generate the surrogate keys.",
          "score": 1,
          "created_utc": "2025-12-29 20:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqelp",
          "author": "TripleBogeyBandit",
          "text": "I would challenge that you truly need scd 2, it can multiply the costs when compared to scd 1",
          "score": -3,
          "created_utc": "2025-12-28 16:07:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdr63w",
              "author": "financialthrowaw2020",
              "text": "This is silly. By the time you realize you need scd 2 on dim-like attributes, it's too late and you look incompetent to your entire business team. Storage is cheap.",
              "score": 4,
              "created_utc": "2025-12-28 16:11:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwduanr",
                  "author": "ProfessorNoPuede",
                  "text": "I'd even go so far as to prefer immutable over scd2 for as far into the pipeline as possible.",
                  "score": 1,
                  "created_utc": "2025-12-28 16:26:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdqvm6",
              "author": "Rare_Decision276",
              "text": "If business wants to keep history of data then we have to go with type 2 right and also their call to go whether type 3 also",
              "score": 2,
              "created_utc": "2025-12-28 16:09:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q4135w",
      "title": "Slapping a vendor's brand on hosted duckdb",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q4135w/slapping_a_vendors_brand_on_hosted_duckdb/",
      "author": "SmallAd3697",
      "created_utc": "2026-01-04 20:44:44",
      "score": 32,
      "num_comments": 56,
      "upvote_ratio": 0.85,
      "text": "Many of the big data vendors will reuse open source components like python, spark, airflow, postgres, and deltalake.  They rebrand it, and host it in their SaaS, and call it \"managed\" and/or \"easy\".  They also charge customers 50% more than if the same software were to be hosted on kubernetes or IaaS.\n\nI keep thinking that one of these vendors (perhaps databricks first) would develop a managed version of duckdb.  It would almost be a no-brainer, since the software is massively useful but is still not widely adopted.\n\nWhy hasn't this happened yet?  Are there licensing restrictions that I'm overlooking?  Or would this sort of thing cannibalize the profits made from existing components in each of these closed ecosystems?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q4135w/slapping_a_vendors_brand_on_hosted_duckdb/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxoz97e",
          "author": "badketchup",
          "text": "There is Motherduck",
          "score": 56,
          "created_utc": "2026-01-04 20:56:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp90yd",
              "author": "SmallAd3697",
              "text": "Also, nobody recognizes motherduck. \n\nI.T management would send 30k to fabric, snowflake, or databricks before they would send a penny to motherduck. (Even when paying for equivalent services.)\n\nMaybe the real question is which of the big data players will acquire that motherduck first?",
              "score": 12,
              "created_utc": "2026-01-04 21:41:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxp70uo",
              "author": "SmallAd3697",
              "text": "Right, but people might say that it isn't \"easy\" to use a separate platform.  The \"easy\" part is when these components are made part of an all-in-one SaaS like fabric, snowflake, or databricks.  (At that point users can expect to interact with duckdb from their existing PyNotebooks, and can get this data to be registered into their official \"catalog\".)\n\nUsing a separate product might create \"silos\". That is some sort of a cardinal sin, as I understand.\n\nIn other words, the full integration is what makes customers willing to pay a 50% premium to a vendor like Microsoft fabric or databricks.",
              "score": 2,
              "created_utc": "2026-01-04 21:32:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxqfouv",
                  "author": "remainderrejoinder",
                  "text": "Network effects are so big in information economics.",
                  "score": 3,
                  "created_utc": "2026-01-05 01:10:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp15qq",
          "author": "hoodncsu",
          "text": "Databricks is doing it with postgres, they call it lakebase, acquired from Neon. I'm sure someone will do it duckdb too",
          "score": 14,
          "created_utc": "2026-01-04 21:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp7nk1",
              "author": "SmallAd3697",
              "text": "Exactly.  I was very surprised it happened to a conventional database like postgres, before an analytical tool like duckdb.  Databricks should have introduced a managed version of duckdb first!",
              "score": 3,
              "created_utc": "2026-01-04 21:35:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxqamxt",
                  "author": "kaumaron",
                  "text": "Managed Postgres is nothing new though, AWS has offered it for ages. Unless there's something special about the databricks one?\n\nAlso Postgres makes way more business sense as its use cases are greater\n\nEdit: couple typos/autocorrect nonsense",
                  "score": 5,
                  "created_utc": "2026-01-05 00:44:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxqf6ml",
              "author": "sib_n",
              "text": "I'm waiting for the SQL query interface that will by itself smartly choose between running on a distributed query engine or a single machine engine like DuckDB to reduce overhead for small loads. Are Databricks or Snowflake offering this yet?",
              "score": 3,
              "created_utc": "2026-01-05 01:08:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxqwh2w",
                  "author": "SmallAd3697",
                  "text": "Snowflake recently introduced \"interactive tables\" which may be their answer to duckdb.  Fabric has always had \"semantic models\" which are analogous to duckdb (although it executes queries written in MDX and DAX, not SQL).\n\nI think databricks is the only one doesn't seem to have a memory-resident columnstore query engine for sub-second response times.  I'm guessing that is one of the reasons they introduced \"lakebase\" but  I would have preferred managed duckdb.",
                  "score": 1,
                  "created_utc": "2026-01-05 02:40:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp22kr",
          "author": "Gators1992",
          "text": "Duckdb is more of an individual tool, not something you sell as a managed service.  Like spinning up a spark cluster on AWS is a lot harder to do and manage than just pip install duckdb, which is where they bring value.  Although there is Motherduck that's based on Duckdb and likely will be ducklake providers if that takes off.",
          "score": 8,
          "created_utc": "2026-01-04 21:09:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpj864",
              "author": "SmallAd3697",
              "text": "If you compare duckdb scenarios to \"semantic models\" in fabric, you will find they are similar.  The goal is to have low-latency response times for end-user reporting.\n\nSemantic models are one of the key things that have attracted customers to power bi (fabric).  I think a managed service for duckdb would be awesome, and would often serve as a suitable alternative to expensive semantic models that we are using today. We need a mainstream alternative for semantic models, but duckdb isn't all that mainstream yet.",
              "score": -1,
              "created_utc": "2026-01-04 22:29:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxpoc9a",
                  "author": "Gators1992",
                  "text": "I don't really see a similarity.  Duckdb is basically a database,  which is different than a semantic model.  A semantic model is an intermediate layer between the database and presentation that allows you to make a data model and transforms and metrics on top of the DB.  It then generates queries that hit the database.   Duckdb is a library/cli that manages access and queries to duckdb data in a file.  You write the SQL in Duckdb and it manages the data access and presents the results.  In a semantic model you query via the model and it translates that query into SQL that it then sends to another application that executes the query and returns the results.  Latency is ultimately determined by the database because it's not going to run a query against your database faster because it's the semantic model asking.  Yeah, you can use a cache layer included in the semantic model, but you can also cache without it.\n\nThe original intent of Duckdb was to allow users to run smaller queries and process data on their laptop, not to play in the same space with managed database services like Snowflake and Databricks.  Basically they noticed that the vast majority of queries executed in a business are not massive, needing the scale of the larger platforms.  People's laptops had enough power to work with that size data so you don't need to pay for saas or hardware to host a DB.  It's a fantastic tool for what it does.\n\nMotherduck did do what you are advocating for and made a managed DB service based on Duckdb so maybe you want to check that out?  They were an independent group from the guys that built Duckdb, but work closely with them.",
                  "score": 8,
                  "created_utc": "2026-01-04 22:53:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxoz3as",
          "author": "larztopia",
          "text": ">I keep thinking that one of these vendors (perhaps databricks first) would develop a managed version of duckdb. It would almost be a no-brainer, since the software is massively useful but is still not widely adopted.\n\nThink I have seen posts that you can use duckdb as query engine in Microsoft Fabriccs now?",
          "score": 3,
          "created_utc": "2026-01-04 20:55:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpdj68",
              "author": "SmallAd3697",
              "text": "I think they load it from scratch in PyNotebooks.  It doesn't have the full blessing of Microsoft, and isn't part of the platform. \n\n.. I'm guessing it wouldn't be reachable from a remote client.  And you couldn't open a support case if your solution fell apart",
              "score": 1,
              "created_utc": "2026-01-04 22:02:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxph7o4",
          "author": "Competitive_Layer_71",
          "text": "I don't think it's so surprising given the read/write-concurrency limits, lack of a distributed architecture etc of DuckDB in its OSS version.\n\nIn contrast, there is a ton of hosted ClickHouse solutions (because ClickHouse have these features) and DuckDB and ClickHouse has a huge overlap in functionality.",
          "score": 3,
          "created_utc": "2026-01-04 22:19:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpjb7w",
          "author": "dominucco",
          "text": "I‚Äôm actually trying to do something like this. What I‚Äôm finding is that potential customers don‚Äôt grok the stack very well and feel very comfortable with a name brand like Snowflake. Just my two cents.",
          "score": 3,
          "created_utc": "2026-01-04 22:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpl9v8",
              "author": "SmallAd3697",
              "text": "Snowflake has something called \"interactive tables\" now.  I think it is less than a year old. Maybe they are just using duckdb under the hood. ;)",
              "score": 1,
              "created_utc": "2026-01-04 22:38:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxpnx9s",
                  "author": "Sp00ky_6",
                  "text": "No it‚Äôs built in house but is more like Pinot or Druid",
                  "score": 1,
                  "created_utc": "2026-01-04 22:51:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxpo1gz",
          "author": "Sp00ky_6",
          "text": "You can run duck db on a container in snowflake if you wanted to",
          "score": 3,
          "created_utc": "2026-01-04 22:52:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxprenh",
          "author": "TechMaven-Geospatial",
          "text": "Motherduck",
          "score": 3,
          "created_utc": "2026-01-04 23:09:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxq1stt",
          "author": "solo_stooper",
          "text": "Spark single ¬†node does whatever duckdb does at cheap cost. Why would Databricks bother",
          "score": 3,
          "created_utc": "2026-01-05 00:00:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrerfo",
              "author": "Nekobul",
              "text": "duckdb will be much faster than Spark on a single node for sure. Also, much simpler.",
              "score": 1,
              "created_utc": "2026-01-05 04:23:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny4c1wd",
                  "author": "solo_stooper",
                  "text": "Simpler sure. If suddenly you have a larger dataset duckdb has no room to grow whereas spark can.¬†",
                  "score": 1,
                  "created_utc": "2026-01-07 01:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxrdvhm",
          "author": "Nekobul",
          "text": "I think many of these vendors don't want people to know about duckdb because once people find, then the really important question is do they need to pay a premium for a distributed architecture that is rarely if ever used.",
          "score": 3,
          "created_utc": "2026-01-05 04:18:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxoy24u",
          "author": "rycolos",
          "text": "AFAIK, Hex uses duckdb under the hood for all internal dataframe processing",
          "score": 2,
          "created_utc": "2026-01-04 20:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp3ah6",
          "author": "Nargrand",
          "text": "I think it is still not profitable enough to be relevant. Also, I checked motherduck and they are billing $80/TB. Looks like it is not cheap to scale this solution when comparing with databricks, snowflake or cloud providers.",
          "score": 2,
          "created_utc": "2026-01-04 21:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpbmwc",
              "author": "SmallAd3697",
              "text": "That is interesting.  The 1TB space might refer to ram, not blobs.  \n\n Reporting requirements that deal with OLAP queries from end users will often cost more, and use less data.  For example, I'd guess that of all the semantic models hosted in fabric, only 0.1 pct are bigger than 1 TB. Also they are evicted from ram when not in use (as I'm guessing happens with motherduck)",
              "score": 1,
              "created_utc": "2026-01-04 21:53:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxq2spa",
                  "author": "Nargrand",
                  "text": "Makes sense, but if it is the case, they missed clarification on their website. I never used duckdb myself, but something that I want to try this year.",
                  "score": 1,
                  "created_utc": "2026-01-05 00:05:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp88kf",
          "author": "testing_in_prod_only",
          "text": "Curious what the advantage of duckdb is over databricks / spark?",
          "score": 2,
          "created_utc": "2026-01-04 21:38:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpeazv",
              "author": "SmallAd3697",
              "text": "It is an analytical tool with much higher queries per second.  \n\nTypically the data would be served entirely from columnstore in ram, rather than via blobs and cache.  As awesome as spark is for big workloads, it is sluggish and doesn't always get 10 or 100 ms response times.  Even with photon.",
              "score": 2,
              "created_utc": "2026-01-04 22:06:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxpz2ip",
                  "author": "hntd",
                  "text": "*much higher on a single machine, otherwise it‚Äôs not even a comparison. Duckdb is fine for a single machine with a small amount of data, but if you need scaling duckdb doesn‚Äôt do any of that.",
                  "score": 4,
                  "created_utc": "2026-01-04 23:47:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxq48ts",
                  "author": "testing_in_prod_only",
                  "text": "Remember the context here is big data vendors.  I agree spark has a lot of bs for small workloads, and mostly duckdb small workloads I‚Äôd think of as a columnar version of SQLite.  My question was specific to like ‚Äúwhy would databricks the big data vendors choose duckdb over spark?‚Äù",
                  "score": 2,
                  "created_utc": "2026-01-05 00:12:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxpz364",
          "author": "KieraRahman_",
          "text": "License isn‚Äôt really the problem here. DuckDB is easy to host and has a permissive license. The catch is incentives: big vendors make money on heavy, always-on compute. DuckDB is small, cheap, and super efficient. Selling ‚ÄúManaged DuckDB‚Ñ¢‚Äù as a first-class thing would push customers toward using less of the expensive stuff, so it‚Äôs more attractive as a quiet embedded engine than a flagship product.",
          "score": 2,
          "created_utc": "2026-01-04 23:47:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxws2fr",
          "author": "Creative-Skin9554",
          "text": "There's not much of a market for it.\n\nDuckDB is a nice tool that is used by people who want to quickly query a few small CSV files.\n\nThe nice thing about DuckDB is that, if you have a tiny amount of data, you can run it on your laptop and not think about setup.\n\nSo what is a managed service offering?\n\nIf you have any kind of scale, there's no reason to use DuckDB. Better tools for that job already exist.\n\nDuckDB will find itself embedded inside managed products, but just a \"managed DuckDB\" isn't going to have a big enough market to be worth it.\n\nMotherduck is fortunate in that they are the only vendor doing it right now, so they get the market that does exist. But they aren't exactly taking the world by storm.",
          "score": 2,
          "created_utc": "2026-01-05 23:30:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp013d",
          "author": "holdenk",
          "text": "Generally you want two folks who knows how the software works before you do that, and or a third level support contract with the creators (happened in the early days of Spark). Doing that is a non-trivial amount of work, the bench to recruit from for these open source projects isn‚Äôt huge.",
          "score": 2,
          "created_utc": "2026-01-04 20:59:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp3ghh",
          "author": "iamspoilt",
          "text": "Not related to DuckDB but I fully resonate with the anger on the amount of premium these companies charge for open source products. This was one of the primary reason for me starting to work on fully managed Spark clusters provisioned in your own AWS cloud with no markups on any amount of compute that you can throw. \n\nIf you are interested, please check it out: https://orchestera.com/\n\n\nAlso made an earlier post showing the potential savings especially for companies with significant spends.\n\nhttps://www.reddit.com/r/dataengineering/comments/1q1fj70/show_rdataengineering_orchestera_platform_run/",
          "score": 1,
          "created_utc": "2026-01-04 21:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpvlnl",
          "author": "Patient_Professor_90",
          "text": "Google up mdb & its licensing challenges/changes from few years back‚Ä¶ to keep cloud providers from eating free lunch from OSS",
          "score": 1,
          "created_utc": "2026-01-04 23:30:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsqdwn",
          "author": "Letter_From_Prague",
          "text": "I think GoodData is actually doing that.",
          "score": 1,
          "created_utc": "2026-01-05 10:56:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtpbmj",
          "author": "zazzersmel",
          "text": "I think it‚Äôs really funny to see this take about open source so often, when it is the literal use case open source licenses were invented for.",
          "score": 1,
          "created_utc": "2026-01-05 14:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtvfsf",
              "author": "SmallAd3697",
              "text": "People pay for the branding, not for the open source software.  And they pay more for the theoretical \"support\" as well.\n\nPersonally I don't care about those things.  I just want duckdb to get more visibility and exposure since it isn't widely adopted yet.",
              "score": 1,
              "created_utc": "2026-01-05 15:20:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1ddky",
      "title": "Non technical boss is confusing me",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q1ddky/non_technical_boss_is_confusing_me/",
      "author": "Huxley_The_Third",
      "created_utc": "2026-01-01 19:56:47",
      "score": 32,
      "num_comments": 24,
      "upvote_ratio": 0.86,
      "text": "I‚Äôm the only developer at my company. I work on a variety of things, but my primary role is building an internal platform that‚Äôs being used by our clients. One of the platform‚Äôs main functionalities is ingesting  analytics data from multiple external sources (basic data like clicks, conversions, warnings data grouped by day), though analytics is not its sole purpose and there are a bunch of other features. At some point , my boss decides he wants to ‚Äúcentralize the company data‚Äù and hires some agency out of the blue. They drafted up an outline of their plan, which involved setting up a separate database with a medallion architecture. They then requested that I show them how the APIs we‚Äôre pulling data from work, and a week later, they request that I help them pull the analytics from the existing db. they never acknowledged any of the solutions i provided for either of those things nor did they explain the Point of those 2 conflicting ideas. So I ask my boss about and he says that the plan is to ‚Äúreplace the entire existing database with the one they‚Äôre working on‚Äú. And the next time I hop on a call with them, what we discussed instead was just mirroring the analytics and any relevant data to the bronze layer. so I begin helping them set this up, and when they asked for a progress update and I show them what I‚Äôve worked on, they tell me that no, we‚Äôre not mirroring the analytics, we need to replace the entire db, including non analytical data. at this point. at this point, I tell them we need to take a step back and discuss this all together (me, then, and my boss). we‚Äôve yet to meet again,  (we are a remote company for context) , but I have literally no idea what to say to him, because it very much seems like whatever he‚Äôs trying to achieve, and whatever proposals they pitched him don‚Äôt align at all (he has no technical knowledge , and they don‚Äôt seem to fully understand what the platform does, and there were obviously several meetings I was left out of) ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q1ddky/non_technical_boss_is_confusing_me/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx4r8ym",
          "author": "MrBarret63",
          "text": "I think you calling a meeting is a good idea and keep a trail of all decisions taken so the blame game regarding wrong decisions can diminished.\n\nStay vigilant and try to provide small deliverables with the directions so they are in the loop of things (somewhat force them to stay in the loop of them)",
          "score": 33,
          "created_utc": "2026-01-01 20:03:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx51oki",
          "author": "git0ffmylawnm8",
          "text": "Here's the thing. I read and reread your post. I still can't understand what problem they're trying to solve by rebuilding everything. \n\nI'd start looking for a new job. Your boss has no idea what he's doing.",
          "score": 23,
          "created_utc": "2026-01-01 20:58:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7n8q9",
              "author": "GreyHairedDWGuy",
              "text": "This is a consulting company strategy.  They are forging a relationship with the manager and influencing what happens for their gain.  The usual excuse is that 'it will be simpler if we rebuild everything from scratch' (which could be true or not).  The consulting company will not want him to have any significant role other than as a worker bee to do work at their direction.",
              "score": 10,
              "created_utc": "2026-01-02 06:22:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx65vnf",
          "author": "Ok_Carpet_9510",
          "text": "You're in politics territory. Document conversations, send followup emails to reiterate whar is discussed in meetings, copy the boss. Ask for clarification where the bosses instructions and those of consultants are at variance. Make sure you call a meeting with the boss and consultants to get alignment on purpose of the project and deliverables.",
          "score": 12,
          "created_utc": "2026-01-02 00:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4rn35",
          "author": "Tactical_Impulse",
          "text": "Yikes! Gotta love non-technical bosses driving technical data decisions. I think your next pending meeting is key, and what you just said \"what they're pitching him vs what he's trying to achieve don't align\" is the message. Also I would emphasize to your boss that for you to be valuable you need to be included in those meetings you're being left out of. He needs to trust your judgement. At the end of the day though, don't drive yourself crazy over it. Sounds like you're doing everything right. Be transparent and demand answers.",
          "score": 14,
          "created_utc": "2026-01-01 20:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7mv1o",
          "author": "GreyHairedDWGuy",
          "text": "Sorry to hear you are going through this.  It can be very disheartening (to the point where you're better to start looking at roles elsewhere).   I have been on both sides of this coin (as a consultant for many years and also as manager/staff).  I have worked with some clueless managers/directors who some consulting company got their claws into and only cared about bleeding the customer while providing a day 1 legacy solution that ends up getting built 2 years later.\n\nYou're in a tough spot.  I could be wrong but based on what you wrote, I got the sense that you probably don't have a lot of experience and therefore the consulting company will use that against you.\n\nBest to call a meeting with your manager and the consulting company contact and lay your cards out on the table...be polite, but don't hold back either.  If you still get no support from your manager, time to look elsewhere.",
          "score": 3,
          "created_utc": "2026-01-02 06:19:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4s0x7",
          "author": "JonPX",
          "text": "Does your boss have a boss?",
          "score": 6,
          "created_utc": "2026-01-01 20:07:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4ud1f",
              "author": "Huxley_The_Third",
              "text": "He‚Äôs the CEO, it‚Äôs not a very big company. He‚Äôs not been that involved in the project aside from bringing in the agency. While all of this has been going on, I‚Äôve been continuously building and maintaining the platform based on the (also non technical) COO‚Äôs vision and requests, and that has been going really well so far, and he‚Äôs had nothing to say or do with this mess¬†",
              "score": 3,
              "created_utc": "2026-01-01 20:19:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx4v05r",
                  "author": "JonPX",
                  "text": "Then my suggestion is to start looking for another job as your boss seems to be phasing you out, and basically hired an external team of which he thought they could do your work.",
                  "score": 27,
                  "created_utc": "2026-01-01 20:22:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9iflj",
          "author": "Gnaskefar",
          "text": "As others already have pointed out, document everything, and ask about everything, and outline discrepancies in between consultants and your boss expectations, and make them answer it on the call. And document all replies, and make them clarify anything you have questions about, and tell them when/if they try to answer without giving a proper answer. Say it is not a clear answer, note it, and potentially open the door for them to get back to you with a clear answer. \n\nAnd push for that reply.\n\n\nBesides that, I noticed in your post and reply you mention they never acknowledged X or Y.\n\nI don't know if it is a cultural thing or what, but don't expect them to acknowledge anything by themselves. In general there's no reason for them to acknowledge whatever solutions you provided. They just need to complete a task.\n\nYou can always ask them for confirmation, 'did you get account for X?' or ask if the solution worked. But they usually don't have reason or time to acknowledge anything towards you, it's normal.",
          "score": 2,
          "created_utc": "2026-01-02 15:11:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9u7ov",
              "author": "Huxley_The_Third",
              "text": "What‚Äôs not normal is making technical decisions without including me, when I could have easily explained the inner workings of the platform and cleared up so many misconceptions from day 1. It took them so long to do that, and so far, No task has been completed, and I don't think they have anything to show for. And then In our latest call, the person I was talking to had zero idea about the details that had been discussed previously. It appears that someone else is now in charge on their side, with again, zero acknowledgement, and they did a terrible job getting everyone on their team on the same page. I have attempted to directly ask them about the previous solutions, but I never got a straight answer, mostly just ‚Äúno, we‚Äôre going to be doing this‚Äù.¬†\n\nMy boss‚Äôs expectations/goals are nonsensical because he‚Äôs not technical, so no attempt to explain my position to him directly ever worked. ¬†\nMaybe my mistake was not pushing harder for more control, but it‚Äôs not like my boss took my opinion when he hired them, so this was all doomed from the beginning.¬†",
              "score": 1,
              "created_utc": "2026-01-02 16:09:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5uopv",
          "author": "codykonior",
          "text": "No paragraphs no read.",
          "score": 3,
          "created_utc": "2026-01-01 23:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7nk46",
              "author": "GreyHairedDWGuy",
              "text": "wow...aren't you a ray of sunshine.",
              "score": 0,
              "created_utc": "2026-01-02 06:25:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6umsg",
          "author": "marketlurker",
          "text": "What I would do is approach my boss and tell him that it appears he is spending money for capabilities he already has. It is like buying your used car all over again. \n\nI would suggest that you want to get more for your money and have the new vendor explain what you are getting for the money. Swapping out technology rarely is justified by the business benefit.",
          "score": 1,
          "created_utc": "2026-01-02 03:04:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6y2zz",
          "author": "mcdunald",
          "text": "from what i can gather from your description, they want to basically replicate the entire raw data layer by ingesting from the sources you're working with. Then they want to review your analytics layer NOT to mirror, but to basically reverse engineer it so they can recreate it with their new data models. I assume this is done for the sake of scalability, probably because the ingestion layer or the modeling (or perhaps lack of model layers) is causing a bottleneck in the data pipeline.  \nMaybe a snowflake solution provider.",
          "score": 1,
          "created_utc": "2026-01-02 03:26:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7gpap",
              "author": "Huxley_The_Third",
              "text": "For starters, the data we‚Äôre working with is pretty simple, and we‚Äôve not hit issues with scalability. the project itself is only a couple months old and is hardly handling any data to begin with. It‚Äôs not like he even asked me, he has little technical knowledge made the decision on his own.\n\n¬†Regardless of that, the agency was onboarded. They initially requested that I provide with the documentation and api access for the services whose data we‚Äôre ingesting. They didn‚Äôt even acknowledge what I sent them, and a week or two later, they come back and tell me that they‚Äôd like to keep everything the same, and that we simply set it up to send the raw data their way and keep everything else the way it is. Throughout all this, my boss somehow thinks that the solution that they are proposing involves storing everything, including non analytical data, but I assumed that this misconception would be cleared after my last discussion with the agency. And then 3 weeks pass and there‚Äôs no word from them.¬†\n\nSomeone from the agency asks for a progress update, and realizes that the person previously in charge of the project from his team never sent me the credentials for the new db nor followed up with me after our last call. I assume at this point that the last person was off the team, but the agency never acknowledged that or made that clear in any way. I can then finally begin working on what had been discussed‚Ä¶ until we hop on a quick call to discuss progress and he insists that the objective was to replace the entire database. I made the incorrect assumption that my boss still had the wrong idea and had forced them to move forward with it, but In reality it was much worse than that. ¬†The most knowledgeable person on their team was switched out, and the new guy doesn‚Äôt understand that the existing db holds much more than analytical data, because he was literally never filled in on what the platform actually does or what was previously discussed.¬†\n\nThe only data that makes sense to be retrieved from the new database would be the analytical data. But my boss‚Äôs goal from what I gathered was to utilize the analytics outside the platform we‚Äôd built, in which case, I don‚Äôt understand his obsession with replacing the existing database in its entirety when it benefits us in now way.¬†",
              "score": 1,
              "created_utc": "2026-01-02 05:30:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8zous",
                  "author": "girlgonevegan",
                  "text": "Ugh. I swear this is a tactic some shady companies use. They know they can sell anything to certain non-technical leaders and can squeeze thousands of dollars out of them while drowning technical stakeholders in ambiguity and misdirection.",
                  "score": 1,
                  "created_utc": "2026-01-02 13:24:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny2ni2g",
          "author": "Fair-Bookkeeper-1833",
          "text": "Hey, I been in similar situation before and there's just no winning, I'd update resume and start looking now, market is tough so better start early, hopefully you find something that's an upgrade.",
          "score": 1,
          "created_utc": "2026-01-06 20:37:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxl447",
      "title": "My attempt at a data engineering project",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pxl447/my_attempt_at_a_data_engineering_project/",
      "author": "NoOwl6640",
      "created_utc": "2025-12-28 07:10:22",
      "score": 31,
      "num_comments": 12,
      "upvote_ratio": 0.85,
      "text": "Hi guys,\n\nThis is my first attempt trying a data engineering project\n\nhttps://github.com/DeepakReddy02/Databricks-Data-engineering-project\n\n(BTW.. I am a data analyst with 3 years of experience ) ",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pxl447/my_attempt_at_a_data_engineering_project/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwbvke4",
          "author": "PrestigiousAnt3766",
          "text": "What do you want to hear?\n\nI would work with a virtual environment, using pyproject.toml for dependencies.\nNow the project will work for you, but is not deployable/production grade.\n\nI don't see any databricks configuration or databricks Asset bundle. How do you run it?\n\nYou have hardcoded almost everything. I think the art of DE is to make code reusable, which your project isnt.",
          "score": 19,
          "created_utc": "2025-12-28 07:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc13fp",
              "author": "NoOwl6640",
              "text": "Any feedback would be appreciated. Don't hold back.",
              "score": 4,
              "created_utc": "2025-12-28 08:24:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwc15lr",
              "author": "NoOwl6640",
              "text": "I started by using databricks free edition",
              "score": 2,
              "created_utc": "2025-12-28 08:24:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwc4vov",
                  "author": "PrestigiousAnt3766",
                  "text": "I would investigate databricks asset bundles. That is a way to deploy your code. DABs contain workflows/jobs with which you can schedule your job. You can work with environment variables, or with key-valuepairs using argparse library.\n\nThe way I would rewrite your \"withColumnRenamed\" statement in I believe silver (bit quick n dirty) would be to write a loop over df.colums and replace spaces with _.\nYou can extend that if you want to cover other scenarios.",
                  "score": 5,
                  "created_utc": "2025-12-28 09:00:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwc1t5k",
          "author": "looctonmi",
          "text": "Writing to a single parquet file with spark is a bit of an antipattern because then the code will only run with a single node. Also since this is a databricks project, it would make sense to take advantage of auto loader.",
          "score": 9,
          "created_utc": "2025-12-28 08:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc46jy",
              "author": "NoOwl6640",
              "text": "What do you mean? Like partition the file",
              "score": 3,
              "created_utc": "2025-12-28 08:53:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdkpsa",
                  "author": "looctonmi",
                  "text": "I would read into delta lake and consider why you would use delta tables over single parquet files for your bronze and silver layers. Then I would read about common bronze -> silver patterns and how to implement incremental batch logic so that you‚Äôre not truncate and loading into silver on every run.",
                  "score": 3,
                  "created_utc": "2025-12-28 15:38:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfvyty",
          "author": "Luxi36",
          "text": "Make sure your code follow PEP8 standards as its important to follow similar coding styles as future projects/teams.\n\nYour variable and function naming is all over the place with snake title or camel case. Instead of just simply using proper snake case.\n\nAs other people mentioned you can group certain transformations better and in the Extract.py get rid of lots of indentations by using guard clauses.\n\nExample; instead of the check `if base_url is not None:` you now have an indentation for the parameters, and handle the raise exception in the `else` you can also simply do `if base_url is None: raise ...` and have the parameters on the normal indentation level.\n\n\nPage count could be handled using `enumerate` instead of doing the +1 count.",
          "score": 3,
          "created_utc": "2025-12-28 22:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwik3zc",
              "author": "NoOwl6640",
              "text": "Nice advice, I'll try to improve upon it",
              "score": 1,
              "created_utc": "2025-12-29 08:42:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz4ynz",
      "title": "How should I implement Pydantic/dataclasses/etc. into my pipeline?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pz4ynz/how_should_i_implement_pydanticdataclassesetc/",
      "author": "paxmlank",
      "created_utc": "2025-12-30 01:47:44",
      "score": 29,
      "num_comments": 22,
      "upvote_ratio": 0.87,
      "text": "tl;dr: no point stands out to me as the obvious place to use it, but I feel that every project uses it so I feel like I'm missing something.\n\nI'm working on a private hobby project that's primarily just for learning new things, some that I never really got to work on in my 5 YOE. One of these things I've learned is to \"make the MVP first and ask questions later\", so I'm mainly trying to do just that for this latest version, but I'm still stirring up some questions within myself as I read on various things.\n\nOne of these other questions is when/how to implement Pydantic/dataclasses. Admittedly, I don't know a lot about it, just thought it was a \"better\" Typing module (which I also don't know much about, just am familiar with type hints).\n\nI know that people use Pydantic to validate user input, but I know that its author says it's not a validation library, but a parsing one. One issue I have is that the data I collect largely are from undocumented APIs or are scraped from the web. They all fit what is conceptually the same thing, but sources will provide a different subset of \"essential fields\".\n\nMy current workflow is to collect the data from the sources and save it in an object with extraction metadata, preserving the response *exactly* was it was provided. Because the data come in various shapes, I coerce everything into JSONL format. Then I use a config-based approach where I coerce different field names into a \"canonical field name\" (e.g., `{\"firstname\", \"first_name\", \"1stname\", etc.} -> \"C_FIRST_NAME\"`). Lastly, some data are missing (rows and fields), but the data are consistent so I build out all that I'm expecting for my application/analyses; this is done partly in Python before loading into the database then partly in SQL/dbt after loading.\n\nInitially, I thought of using Pydantic for the data as it's ingested, but I just want to preserve whatever I get as it's the source of truth. Then I thought about parsing the response into objects and using it for that (for example, I extract data about a Pokemon team so I make a Team class with a list of Pokemon, where each Pokemon has a Move/etc.), but I don't really need that much? I feel like I can just keep the data in the database with the schema that I coerce it to and the application currently just runs by running calculations in the database. Maybe I'd use it for defining a later ML model?\n\nI then figured I'd somehow use it to define the various getters in my extraction library so that I can codify how they will behave (e.g., expects a Source of either an Endpoint or a Connection, outputs a JSON with X outer keys, etc.), but figured I don't really have a good grasp of Pydantic here.\n\nAfter reading on it some more, I figured I could use it *after* I flatten everything into JSONL and use it while I try to add semantics to the values I see, but as I'm using Claude Code at points, it's guiding me toward using it *before/during* flattening, and that just seems forced. Tbf, it's shit at times.\n\nTo reiterate, all of my sources are undocumented APIs or from webscraping. I have some control over the output from the extraction step, but I feel that I shouldn't do that in extracting. Any validation comes from having the data in a dataframe while massaging it or after loading it into the database to build it out for the desired data product.\n\nI'd appreciate any further direction.",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pz4ynz/how_should_i_implement_pydanticdataclassesetc/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwnk502",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-30 01:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnlxx5",
          "author": "Shensy-",
          "text": "I'm very new to this, just getting pydantic implemented in a project for the first time now, but since you can dump a model to JSON, wouldn't the best candidate for this be using it to perform the step where you coerce it?",
          "score": 6,
          "created_utc": "2025-12-30 01:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqby0w",
              "author": "tjger",
              "text": "This. Pydantic should be used at the object-level (i.e. application), not data ingestion. Even though one could argue that Pydantic is useful to enforce data types, I can imagine use cases  where a data column may sometimes come structured and other times unstructured, and part of the work is to transform them into either one type or the other (i.e. transform into a new column).\n\nThis way, Pydantic would be useful on higher levels / higher on the medallion transformations if you will, where objects are finally defined and their end types truly enforced.",
              "score": 2,
              "created_utc": "2025-12-30 14:00:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwnolut",
              "author": "paxmlank",
              "text": "Possibly, and that's the direction Claude Code seems to want to go in. One concern I had was that the responses are at some level a list of dicts that I want saved as a file in JSONL format - maybe I can model each dict as JSON but not write it like that just yet, I hadn't looked into it yet. The other was that I felt like this was adding semantics to the data which I didn't want to do until after I flattened/coerced it. I'm trying to do everything very simply: extract then save with metadata, flatten and save copy, add derive missing values and save, load to database/lake, analyse",
              "score": 1,
              "created_utc": "2025-12-30 02:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnqa6i",
          "author": "West_Good_5961",
          "text": "Why not load to a relational database if schema enforcement is critical?",
          "score": 4,
          "created_utc": "2025-12-30 02:21:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoxquf",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 3,
              "created_utc": "2025-12-30 07:08:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpiqf0",
                  "author": "paxmlank",
                  "text": "Fair - I thought of having my raw go straight into the DB but I was hoping on having this data eventually move into a datalake-style environment. With the idea of unified DuckDB logic running across the parsed/coerced data, I felt like I should let all parsing outside of the DB for now.\n\nIdk though, as it will help as you said :|",
                  "score": 1,
                  "created_utc": "2025-12-30 10:22:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwnqv9c",
              "author": "paxmlank",
              "text": "I am, which is why I feel like I don't need Pedantic/dataclasses but that felt like a wild sentiment so I wasn't sure",
              "score": 1,
              "created_utc": "2025-12-30 02:24:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwnsz5b",
                  "author": "raf_oh",
                  "text": "I‚Äôd think it would be so you can enforce constraints in code rather than in your db, and then load to your db. So like min/max etc",
                  "score": 4,
                  "created_utc": "2025-12-30 02:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpnwda",
                  "author": "Obvious-Phrase-657",
                  "text": "That makes sense, so maybe you don‚Äôt actually need it, I know that is a hobby project you might want to test cool stuff.\n\nI see the usage when the data you receive is generated by another system (maybe yours too, but not the target db/lake) and you use the schema as a contract and even better, if the source is also yours you can reuse the pydantic code for the API entities ian your pipeline or have a shared repository.\n\nFor instance, I had a datalake receiving streaming data from kafka, the data was produced by other teams, but they define the schema in a repo, and we pull that schema in order to handle the data and do a quick data qa validations \n\nBut if you just need to load it into a db pr datalke you can enforce the achema on the DDL. Even in a datalake you can use iceberg or delta (highly recommend for experience and resume) \n\nTldr don‚Äôt use it if you can enforce it on the ddl as it is far easier, use it if you need to enforce schema on a complex application the is based on python (or other coding language)",
                  "score": 1,
                  "created_utc": "2025-12-30 11:09:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtian6",
          "author": "YouKnowILoveMyself",
          "text": "Hi I work with Pydantic and dataclasses everyday and I think I can explain this well.\n\nPydantic - mostly used when there is any input/output that needs to be validated, a good example is let's say I want to integrate with some external service, if I have to call their endpoints with any specific payload Pydantic makes sure that my outgoing data to that service will always have these fields and I can write custom rules for a Pydantic class to enforce certain conditions. Same works for if any service decides to call my endpoints, I can stop any processing to happen if request data to my endpoint is wrong.\n\nDataclasses - used mainly for internal data transfer. An example for this is, let's say after my Pydantic validation occurs I now want to move around the fields I can set them in a dataclasses and move them around with pass by value/ reference. Now I can do this with Pydantic also but why I shouldn't. Pydantic does have an overhead when it parses the field this is very small but I've hit cases when it matters (Pydantic v2 is faster). Dataclasses are faster when I've already validated the data internal transfer is faster and makes more sense to use a dataclass when I just need to move and play around with the data. \n\nAdditional point database updates I would make a data access layer which would be validated against the schema. I use SQL alchemy here for the models some people use Pydantic here too, optional honestly. This way it makes it easier to unit test too when you abstract everything and loosely coupled. \n\nHope it clears things up!!",
          "score": 3,
          "created_utc": "2025-12-30 23:15:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwol1wu",
          "author": "ADGEfficiency",
          "text": "Function inputs and returns should be Pydantic models.",
          "score": 1,
          "created_utc": "2025-12-30 05:26:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsaow5",
          "author": "empireofadhd",
          "text": "It‚Äôs used implicitly in many frameworks. In using pyspark so it has its schema enforcement but if I did not have that I could use pydantic. I would probably use some yaml framework for field definitions though as it‚Äôs a bit more portable.",
          "score": 1,
          "created_utc": "2025-12-30 19:43:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1i5sh",
          "author": "digitalghost-dev",
          "text": "I‚Äôve been using Pydantic in my pipelines when I first read from the API to make sure the types are correct by using the BaseModel class.\n\nhttps://github.com/digitalghost-dev/poke-cli/blob/main/card_data/pipelines/defs/extract/tcgdex/extract_series.py",
          "score": 1,
          "created_utc": "2026-01-01 06:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwojss2",
          "author": "EarthGoddessDude",
          "text": "Pydantic is for input/output, stuff around the edges of your code. Dataclasses are for internal state management. Yes, they overlap and you can use both in many situations, but generally that‚Äôs how you should think of them. \n\nTbh I don‚Äôt read your post super carefully because a) it was a lot and b) not very well organized. I don‚Äôt mean to be rude, just being honest. I‚Äôm also really tired and about to go to sleep so. \n\nFrom I can tell, you definitely can, and sometimes want to, use pydantic to load in your api data. It will not only parse and validate it, but it can standardize your field naming too. You can do a lot of cool things with it, like use a TypeAdapter for places where you need different structures that come out of the same API. That being said, I do view it more as an OLTP type tool, it *feels* more row based. For DE tasks, I would reach for DuckDB or polars to read in the API or json data and flatten it. Really depends on your use case and data volume\n\nAs for landing your raw data and then processing it, that‚Äôs a valid way to do it IMO. Land your API responses as JSON and then process with whatever tool you pick.",
          "score": -2,
          "created_utc": "2025-12-30 05:17:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqg0ha",
              "author": "CrackerJackKittyCat",
              "text": "This. Pydantic is to python what DTO objects are to java codebases --- they get used at the API edge. They exist for outbound message serialization and incoming message validation and parsing into objects.\n\nThe internals of your python codebase could then use dataclasses, ORM classes, or whatever and those would have varying degrees of behavior. The messaging Pydantic classes should have close to zero behavior at all -- perhaps some convenience data accessor properties, but nothing that you'd call 'real functionality.'",
              "score": 1,
              "created_utc": "2025-12-30 14:23:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpk2cx",
          "author": "Xman0142",
          "text": "Use Scala",
          "score": 0,
          "created_utc": "2025-12-30 10:34:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0zn12",
      "title": "Switching to Databricks",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q0zn12/switching_to_databricks/",
      "author": "AdQueasy6234",
      "created_utc": "2026-01-01 08:41:02",
      "score": 28,
      "num_comments": 19,
      "upvote_ratio": 0.9,
      "text": "I really want to thank this community first before putting my question. This community has played a vital role in increasing my knowledge.\n\nI have been working with Cloudera on prem with a big US banking company. Recently the management has planned to move to cloud and Databricks came to the table.\n\nNow being a complete onprem person who has no idea about Databricks (even at the beginner level) I want to understand how folks here switched to Databricks and what are the things that I must learn when we talk about Databricks which can help me in the long run. Our basic use case include bringing data from rdbms sources, APIs etc. batch processing, job scheduling and reporting.\n\nCurrently we use sqoop, spark3, impala hive Cognos and tableau to meet our needs. For scheduling we use AutoSys.\n\nWe are planning to have Databricks with GCP.\n\nThanks again for every brilliant minds here. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q0zn12/switching_to_databricks/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx28o19",
          "author": "fvonich",
          "text": "If you have high security requirements the hardest part will be networking. Check out private link for databricks. I would recommend starting right away with terraform and find a good DevOps colleague for the migration project. \n\nIn general databricks takes care of a lot of stuff but you have to learn a lot of databricks fundamentals like Asset Bundles and Lakeflow etc.",
          "score": 8,
          "created_utc": "2026-01-01 10:32:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9eiqo",
              "author": "Dry-Aioli-6138",
              "text": "People have luved happiky without asset bundles for years, so you don't _have_ to learn it. Similar with Lakeflow.\n\nAs with any cloud migration, you will get best results if you translate your needs into the native tools, rather than trying to lift and shift. So do look at lakeflow, and do look as asset bundles in the context of how you can use them instead or with your current scripts and flows.",
              "score": 1,
              "created_utc": "2026-01-02 14:50:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1zlwv",
          "author": "PrestigiousAnt3766",
          "text": "Get DE pro certification. Will help you more than random answers here.\n\nIn general:\n\nDatabricks is quite demanding on the technical skills of DE and infra to setup properly.\n\nWhile it is not strictly necessary, i'd strongly emphasize learning sufficient Python\n\nImportant is unity catalog for permissions.\n\nLakehouse architecture.\n\nLakeflow / databricks jobs.\n\nVScode / Databricks Connect",
          "score": 12,
          "created_utc": "2026-01-01 08:54:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx29qcm",
              "author": "tiredITguy42",
              "text": "Yeah and any mistake can cost a ton of money. We are ditching DataBricks right now as we do not need it. Our data is not that big and we can do all for pennies in PostgreSQL and Kuberneties.\n\nDatabricks are nice, but you need to have plenty of casch and gppd reason to spemd it there.",
              "score": 5,
              "created_utc": "2026-01-01 10:43:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxa3335",
                  "author": "Ok-Butterscotch6249",
                  "text": "If you don‚Äôt mind sharing what was the delta in cost by percentage between the two? Used to work for Teradata and the battles between Snowflake and Teradata were epic, but there were a noticeable number of Snowflake customers who had budget problems because they didn‚Äôt foresee autoscaling as scaling their costs.",
                  "score": 1,
                  "created_utc": "2026-01-02 16:50:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxas07k",
                  "author": "RemcoE33",
                  "text": "Yeah we have:\n\n- Clickhouse as Database\n- Go for ingestion\n- Superset for \"official\" BI\n- Metabase for self service reporting and small dashboards",
                  "score": 1,
                  "created_utc": "2026-01-02 18:45:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx23mtb",
              "author": "randomName77777777",
              "text": "Good recommendations. \nI think you'll be very happy with databricks, at least I was.\n\nWith everything being around AI today, I would recommend checking out a few of the different AI features like AI functions, model serving endpoints, databricks genie and agents.",
              "score": 3,
              "created_utc": "2026-01-01 09:38:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2oa5x",
              "author": "SoggyGrayDuck",
              "text": "Who is the DE pro cert from? I'm also dealing with an on premise setup but have past experience in AWS. Unfortunately my state seems to be 100% azure. I'm wrapping up my AWS data engineer cert but think I need to move onto azure. I primarily use SQL, used a bit of python and etc with glue but I think in SQL still. Although with AI I can still fly without vibe coding. It's just a syntax thing for me, I understand the underlying concepts. \n\nI keep getting calls for SR and principal jobs but because I'm currently on premise I feel I'm a little under qualified. I need 1-3 years of cloud development and then I'm ready for that next step. I've been at small shops without anyone on top of me and left to figure things out. I need to make sure my plans fit with the larger picture. Although what I'm finding is this on premise model is TERRIBLE and I might prefer starting from scratch",
              "score": 1,
              "created_utc": "2026-01-01 13:03:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2t2ah",
                  "author": "PrestigiousAnt3766",
                  "text": "I meant the databricks one.\n\n\nDatabricks Certified Data Engineer Professional\n\n\nI cant post links.",
                  "score": 1,
                  "created_utc": "2026-01-01 13:40:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2wxn1",
          "author": "_Marwan02",
          "text": "I am in the exact same situation ! Feel free to dm to discuss",
          "score": 1,
          "created_utc": "2026-01-01 14:08:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2xfjs",
          "author": "Nekobul",
          "text": "How much data do you process daily?",
          "score": 1,
          "created_utc": "2026-01-01 14:11:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5m6d7",
          "author": "VarietyOk7120",
          "text": "Are you going to replace Cloudera with a Databricks Lake house , or build a traditional Data Warehouse in Databricks SQL ? First question",
          "score": 1,
          "created_utc": "2026-01-01 22:45:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa5vdy",
          "author": "Ok-Butterscotch6249",
          "text": "If you can pause the movement to have time to think about it and do some proper cost analysis, go for it. I say that because the best answer could be: stay with Cloudera and renegotiating the cost, deploy an on-prem object store compatible with DB and also keep flexibility with local Spark, and so on. \n\nPersonally I had an epiphany when I realized that IT is more like the fashion industry complete with ‚Äúfashion shows‚Äù (like reInvent), but our resumes are where we see if we‚Äôre fashion forward or not. The thing that always dispels the concerns about fashion are economics.",
          "score": 0,
          "created_utc": "2026-01-02 17:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf0xq1",
          "author": "mr_nanginator",
          "text": "I just finished up a major Databricks migration project, and here's my advice: run for the hills! You're FAR better off with Snowflake in just about every use-case. Jesus, you're even better off with Redshift ( ewwwww ). Seriously, if this is not set in stone, you'll have a far better experience if you can steer things towards a stable platform.",
          "score": 0,
          "created_utc": "2026-01-03 10:28:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2bwiv",
          "author": "Resident_Vermicelli2",
          "text": "Microsoft Fabrics is the future",
          "score": -10,
          "created_utc": "2026-01-01 11:05:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2etvt",
              "author": "PrestigiousAnt3766",
              "text": "Lol. The future if a lot of consultancies cleaning garbage.",
              "score": 4,
              "created_utc": "2026-01-01 11:36:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2ihi1",
              "author": "thecoller",
              "text": "‚Ä¶ and will always be",
              "score": 2,
              "created_utc": "2026-01-01 12:12:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2ytbh",
      "title": "Anyone else tired of exporting CSVs just to get basic metrics?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q2ytbh/anyone_else_tired_of_exporting_csvs_just_to_get/",
      "author": "Flat-Shop",
      "created_utc": "2026-01-03 16:25:50",
      "score": 27,
      "num_comments": 15,
      "upvote_ratio": 0.78,
      "text": "Right now I‚Äôm pulling data from a few tools, exporting CSVs, and manually stitching them together just to answer basic questions like revenue trends or channel performance. It works, but it‚Äôs slow, error-prone, and feels like busywork more than insight.\n\nNot looking for anything fancy or real time, just something that pulls data into one place and updates automatically so I‚Äôm not stuck being a data entry robot.\n\nWhat others are using here? build something yourself? Switch to a BI/dashboard tool? Or just accept spreadsheets forever?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q2ytbh/anyone_else_tired_of_exporting_csvs_just_to_get/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxgp108",
          "author": "Reach_Reclaimer",
          "text": "No because that's not data engineering. You're describing data analytics but that's also a bad way of doing analytics\n\n\nif you are doing this all the time, the data infrastructure needs improving or something actual data engineering is needed to get it into a data warehouse or lakehouse (or whatever storage system the company needs)",
          "score": 20,
          "created_utc": "2026-01-03 16:41:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhplas",
          "author": "financialthrowaw2020",
          "text": "You should hire a data engineer",
          "score": 8,
          "created_utc": "2026-01-03 19:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgu94a",
          "author": "Best_Volume_3126",
          "text": "We were in the exact same spot, CSV hell and master sheets no one trusted. We eventually moved everything into Domo, mainly because it connected to our data sources directly and kept dashboards updated without manual exports. Biggest win wasn‚Äôt fancy analytics, just saving time and reducing errors.",
          "score": 8,
          "created_utc": "2026-01-03 17:05:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgtkug",
          "author": "chock-a-block",
          "text": "Beats working in a warehouse.¬†\n\nBeyond that, this what scripting is for.¬†\n\nPython is super popular for this if the files are small.¬†\n\nI had to deal with multiple GB size files on systems without enough resources though, and strongly recommend Perl when things get big. The text handling is better than python.¬†",
          "score": 3,
          "created_utc": "2026-01-03 17:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxh5g2o",
              "author": "Skullclownlol",
              "text": "> Python is super popular for this if the files are small. \n\nEven if it's large, massive, or requires distributed computing. A lot of data libs have python integrations.",
              "score": 4,
              "created_utc": "2026-01-03 17:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhjkds",
                  "author": "chock-a-block",
                  "text": "They sure do. And lots of them work well.¬†",
                  "score": 2,
                  "created_utc": "2026-01-03 19:00:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvbogx",
              "author": "Perfect_Figure182",
              "text": "100%, anything is better than babysitting CSVs all day.\nTotally with you on scripts being the way out. Python is great when you‚Äôve got sane file sizes and decent resources. Once you‚Äôre in multi‚ÄëGB land on underpowered boxes, you really feel the difference between ‚Äúthis works‚Äù and ‚Äúthis actually finishes.‚Äù\nCurious how you usually split it: do you let the app handle the export and just script the cleanup, or are you pulling straight from APIs and skipping the UI altogether when you can?",
              "score": 1,
              "created_utc": "2026-01-05 19:20:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp6ask",
          "author": "Gators1992",
          "text": "Python script would be the easiest.  You can  also check out Duckdb that would allow you to store the history in a DB on your laptop or stick it in your file system (db is one file).  Ideally you can get the source owners to help you schedule the file creation/drop if you are doing that manually so you just need to run the python script to pull all the data together.  If you don't know python you could get AI to figure out the script for you and walk you through how to use it (i.e. download an IDE like VSCode, create an environment, etc).",
          "score": 2,
          "created_utc": "2026-01-04 21:29:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvay19",
          "author": "Perfect_Figure182",
          "text": "Manually exporting CSVs and stitching them in Sheets works, but it turns you into a part‚Äëtime data clerk.\nThe pattern that‚Äôs worked for me is:\n‚Ä¢\tHave something log into those tools on a schedule, click ‚Äúexport CSV,‚Äù and drop them all in one place (Drive or whatever).\n‚Ä¢\tRun a small job that cleans the column names, joins them, and spits out one ‚Äúmetrics‚Äù table (date, channel, revenue, etc.).\n‚Ä¢\tPoint a simple dashboard (Sheets, Metabase, Looker Studio, whatever you like) at that table and let it refresh.\nIt doesn‚Äôt have to be full-blown warehouse if you don‚Äôt want it. Even ‚Äúone cleaned Google Sheet that updates itself‚Äù is a huge upgrade.\nWhat tools are you pulling from right now? If you share the stack, can sketch how I‚Äôd wire it so you‚Äôre not exporting CSVs by hand.",
          "score": 2,
          "created_utc": "2026-01-05 19:17:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgqimb",
          "author": "PolicyDecent",
          "text": "Which platforms are you exporting CSVs from? There are lots of ways to automate it. With the new AI tools, I might recommend you to vibecode a python script doing what you do.   \nIf you have multiple sources, I'd recommend exporting data to a database / dwh, and do everything there and you can even show your numbers on dashboards that way.",
          "score": 2,
          "created_utc": "2026-01-03 16:48:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxa27l",
          "author": "Miserable_Agent_9006",
          "text": "Which tools you pulling from? There are AI data platforms out there that helps you pull from a source, do whatever cleaning/transformation you want and at a cadence and export it. Have you tried using an agentic platform that simplifies analytics engineering?",
          "score": 1,
          "created_utc": "2026-01-06 01:04:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxm1jqq",
          "author": "Asleep-Wolf2159",
          "text": "I don't know the details... but\n\n1. Export CSV from tools -> Manually\n\n2. Join the exported CSV files -> Pandas (Python) or DuckDB (using read\\_csv)\n\nPerhaps something like that? If you could provide more details, I might be able to automate it a bit more.",
          "score": 1,
          "created_utc": "2026-01-04 11:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxietdo",
          "author": "DeliriousHippie",
          "text": "You need BI tool/program/solution whatever you want to call it. Spreadsheets are a dead end and waste time, errors happen often too.\n\nFirst decide what route you want to take.\n\n1. Complete tool\n\nFor example Qlik has everything you need or more. It costs about $200 month. Relatively simple to setup and develop, large userbase for questions and help.\n\n2. Set of different tools\n\nYou need place to store your data, some database or S3 bucket or something, it costs something but not much. Then you need move your data to your chosen storage, for example Python and some orchestration tool. Lastly some tool to show your KPI's, like PowerBI, Astrato, etc. Those cost something too. Starting from zero can be difficult to decide what you need and what to use, solutions have lots of users, help and examples are easy to find.\n\n3. Build everything yourself\n\nThis depends about your skill set, how much time you want to spend with this, how large your userbase etc. Doable but I don't recommend, others have already solved all the problems you have to solve in this, takes much time and security is always worrying. No other users, help can be difficult to find.",
          "score": 0,
          "created_utc": "2026-01-03 21:32:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvbeff",
              "author": "Perfect_Figure182",
              "text": "Yeah agreed, spreadsheets hit a wall pretty fast once you‚Äôre doing this regularly.\nWhat I‚Äôve seen work for people who are stuck exporting CSVs but don‚Äôt want to go full ‚Äúdata team + warehouse‚Äù is a middle ground between your #2 and #3:\n‚Ä¢\tKeep whatever BI/dashboard you like (Power BI / Looker Studio / Metabase, etc.).\n‚Ä¢\tLet a small browser automation do the dumb work: log in, click to the right report, export CSV, drop it into a folder/Sheet on a schedule.\n‚Ä¢\tHave a simple script/job that cleans those CSVs and feeds the BI tool.\nYou still end up with ‚Äúone place for metrics‚Äù like you‚Äôre describing, but you don‚Äôt have to rebuild your whole stack or live in spreadsheets.",
              "score": 1,
              "created_utc": "2026-01-05 19:19:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxowrfp",
          "author": "Ok-Painter2695",
          "text": "*This is exactly the pain point we kept hitting. Exporting CSVs, stitching in Excel, then realizing you missed something and starting over. What helped us: there are AI tools now that can analyze your raw CSVs directly - upload, get insights, done. No ETL pipeline needed for basic questions. We use it for quick revenue/channel checks before building anything \"proper\". Saves hours of spreadsheet gymnastics.*",
          "score": 0,
          "created_utc": "2026-01-04 20:44:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzkxi4",
      "title": "At what point does historical data stop being worth cleaning and start being worth archiving?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pzkxi4/at_what_point_does_historical_data_stop_being/",
      "author": "Queasy-Cherry7764",
      "created_utc": "2025-12-30 15:27:07",
      "score": 25,
      "num_comments": 12,
      "upvote_ratio": 0.9,
      "text": "This is something I keep running into with older pipelines and legacy datasets.  \n  \nThere‚Äôs often a push to ‚Äúfix‚Äù historical data so it can be analyzed alongside newer, cleaner data, but at some point the effort starts to outweigh the value. Schema drift, missing context, inconsistent definitions‚Ä¶ it adds up fast.  \n  \nHow do you decide when to keep investing in cleaning and backfilling old data versus archiving it and moving on? Is the decision driven by regulatory requirements, analytical value, storage cost, or just gut feel?  \n  \nI‚Äôm especially curious how teams draw that line in practice, and whether you‚Äôve ever regretted cleaning too much or archiving too early. This feels like one of those judgment calls that never gets written down but has long-term consequences.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pzkxi4/at_what_point_does_historical_data_stop_being/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwqujuj",
          "author": "financialthrowaw2020",
          "text": "It's based on the user requirements. That's it. \n\n\nDoes the executive team want a dashboard that shows revenue over time from the inception of the company? Yes? Then all data feeding that dashboard must be clean and reconciled. \n\nDoes the user want a dashboard only tracking the last 7 years? Great, then we set our policies based on that and keep the raw data in its original form until the requirements change and we can easily adapt the old data to the new. It's really that simple.",
          "score": 40,
          "created_utc": "2025-12-30 15:39:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwquntd",
          "author": "ZirePhiinix",
          "text": "You don't.\n\nYou get the specs down and translate it to a cost in dollars. Unless you learn how to do that, \"expensive\" is literally not their problem.\n\nIf they see that accessing historical data for this report is costing them half a million each year, but the revenue tied to it is a fraction of that, then maybe they can learn to let it go.",
          "score": 19,
          "created_utc": "2025-12-30 15:40:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqx1o1",
          "author": "exjackly",
          "text": "As others have said, that is a business not a technical decision.  If the business wants the data enough to pay for cleaning old data and keeping it available, we clean and keep it available.\n\nIn practice, I look for migration dates.  When the business moves from one system to a replacement, the quality of the data takes a jump, and anything before that is only useful short term.  Find that date, and show them the issues from the old data; and if you can show how much it is costing to process and keep that data, it gets easier to convince them to archive and start working towards data destruction.\n\nThe company having a data retention policy helps too, as you can tie in to the definitions there.  Even better if it is regulated data with mandated destruction timelines.\n\nBut, you have to know your numbers, and understand their needs and requirements enough to make that proposal.\n\nHonestly, old data is so seldom used that by the time data is more than 2-3 years old, (most industries, not all) only aggregates ever get used, and even those drop off quickly after 5-7 years.  I've never been in a situation where we've wanted to unarchive data.",
          "score": 9,
          "created_utc": "2025-12-30 15:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrkgtg",
          "author": "klumpbin",
          "text": "After 17 years, 3 months and 12 days",
          "score": 8,
          "created_utc": "2025-12-30 17:41:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqvtte",
          "author": "Striking_Meringue328",
          "text": "The real question is what's the business case for fixing old data? Can you put a figure on the expected benefits, and does it outweigh the likely cost?",
          "score": 5,
          "created_utc": "2025-12-30 15:45:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrsx0w",
          "author": "LargeSale8354",
          "text": "Legal requirements to retain. Process requirements to fulfil legal requirements. For example,  financial regulations in the UK require companies to keep 7 years of transactions. This does not necessarily mean online, but GDPR does say that subject access requests must be completed in 30 days.\nIf you archive stuff, rehearse the process of getting it back and do dry runs every quarter. \n\n\nUser requirements come next. As people have said, make sure you put the cost in front of them. If you don't you risk being the provider of free food for an ever growing horde.\n\nWhat is your definition of fixing the data? I'm always wary of the line between fix and falsify.",
          "score": 3,
          "created_utc": "2025-12-30 18:20:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr8o0p",
          "author": "OkToe2355",
          "text": "based on use case",
          "score": 2,
          "created_utc": "2025-12-30 16:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtaqaa",
          "author": "pauloliver8620",
          "text": "depends on seasonal events, eg from sport tournaments, world cup is held every 4 years, olimpics every 4 years etc if you have similar events business might be interested in comparing then chose the seasonality that matches your requirements. As everyone else mentioned explain the cost of keeping the data so that business can put a price tag on it and see if they can make more out of it.",
          "score": 2,
          "created_utc": "2025-12-30 22:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwunpry",
          "author": "Firm_Bit",
          "text": "Wdym? Depends on the use case. If looking at years of data opens a $x opportunity and it only costs your sanity + less than $x then it‚Äôs worth doing.",
          "score": 1,
          "created_utc": "2025-12-31 03:10:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2m5al",
          "author": "gelato012",
          "text": "2 years but the business has the say.",
          "score": 1,
          "created_utc": "2026-01-01 12:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqwmeb",
          "author": "Alternative-Guava392",
          "text": "Personally, Rule of thumb : 3 years. \n\nIf business still insists on old data, get the cost + effort vs returns in terms of dollars. \n\n\"It will cost 5000$ / 30 hours to fix it. Is that an issue ?\"",
          "score": 1,
          "created_utc": "2025-12-30 15:49:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3b44p",
      "title": "1.5 YOE Data Engineer ‚Äî used many tools but lacking depth. How to go deeper?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q3b44p/15_yoe_data_engineer_used_many_tools_but_lacking/",
      "author": "Nice_Sherbert3326",
      "created_utc": "2026-01-04 00:30:22",
      "score": 25,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "I‚Äôve been working as a Data Engineer for \\~1.5 years. Stack I‚Äôve used at work:\n\n* Spark / PySpark (Databricks)\n* Azure data services & Microsoft Fabric\n* SQL, Python\n* Certs: Databricks DE Associate, Fabric DE Associate\n\nI‚Äôm trying to switch jobs but struggling to get interviews. Along with CV, I think the issue is also¬†depth, not exposure. I have exposure to other tools through my job, but to go in-depth, most online resources (YouTube, Coursera, etc.) I found are very high-level. I‚Äôve already gone through many of them and they don‚Äôt get into real design or internals.\n\nI want to go deeper into:\n\n* Spark (internals & performance)\n* Airflow\n* Snowflake\n* dbt\n* Kafka\n* AWS (beyond just S3)\n\nPaid DE platforms are often $7k‚Äì$10k, which isn‚Äôt realistic for me.\n\nQuestion:  \nFor people working as mid/senior DEs ‚Äî what resources (books, repos, blogs, projects) actually helped you understand these tools at a¬†*production*¬†level? How did you move from ‚Äúused it‚Äù to ‚Äúcan design with it‚Äù?\n\nTL;DR:¬†\\~1.5 YOE DE, used many tools but lacking depth. Intro resources are too shallow ‚Äî looking for¬†in-depth learning guidance.\n\n",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q3b44p/15_yoe_data_engineer_used_many_tools_but_lacking/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxlfrvq",
          "author": "PrestigiousAnt3766",
          "text": "I just worked. I find it very difficult to get motivated to really put in effort unless its to solve a true problem.\n\n\nWhy do you want to move after 1.5y?",
          "score": 11,
          "created_utc": "2026-01-04 08:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpx4ke",
              "author": "Nice_Sherbert3326",
              "text": "I agree, It‚Äôs tough to do a project after work.\n\nMy main reasons for wanting to move low pay and a highly micromanaged environment,. I was aware of this before joining, but after an extended job search ( almost 8 months) I accepted the offer and needed to commit for a period of time before considering a move.\n\nNow that I‚Äôm in a more stable position, I‚Äôm focusing on leveling up properly and being more intentional about my next role.",
              "score": 3,
              "created_utc": "2026-01-04 23:37:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxm2by9",
          "author": "Ordinary-Toe7486",
          "text": "I‚Äôd say find ways to be more proactive and deliver real business value with the tools you‚Äôre using at the current workplace. This will show that you don‚Äôt simply know how to use the tool, but how the tools helps add value for the business. For books I‚Äôd suggest Designing Data-Intensive Applications. \n\nAlso, I believe that you don‚Äôt become a senior DE after 1.5YOE. Find what you like, set clear goals for personal development and become more confident in what you do. During the interviews I think it‚Äôs very important to appear as a confident fella, but not arrogant.",
          "score": 10,
          "created_utc": "2026-01-04 12:02:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpy8e8",
              "author": "Nice_Sherbert3326",
              "text": "Thanks for the recommendation ‚Äî I‚Äôll check it out.\nII‚Äôm aware I still have a lot to learn, that‚Äôs why I am starting again from the fundamentals.",
              "score": 1,
              "created_utc": "2026-01-04 23:43:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxlp6r1",
          "author": "Ancient_Ad_916",
          "text": "First of all, if you have knowledge of all the mentioned tools I find it hard to believe that you are struggling to find a job. Furthermore, the best way to get senior level proficient in these tools/languages is to engage more with them, mainly on the workfloor. \n\nThere are likely not many tutorials for advanced applications because they are often niche and vary a lot per business case. Additionally many of the by you mentioned tools will likely only be really used in enterprise/large scale processes, so for that reason alone it might be hard to achieve good training on your own.",
          "score": 4,
          "created_utc": "2026-01-04 10:08:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpuhkj",
          "author": "JadeCikayda",
          "text": "You gain depth through experience and reflection. My suggestion would be to start creating small data-orientated projects hosted on Github including CI/CD with Actions.\n\nThe specific project doesn't matter, it just needs to keep your attention and involve technologies you're interested in. Work on your personal projects and gain depth.",
          "score": 2,
          "created_utc": "2026-01-04 23:24:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq0opt",
              "author": "Nice_Sherbert3326",
              "text": "Thanks for the advice ‚Äî I‚Äôll start experimenting with a few targeted projects.",
              "score": 2,
              "created_utc": "2026-01-04 23:55:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp5zzr",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-04 21:27:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp61gb",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-04 21:27:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxp63ya",
                  "author": "SupoSxx",
                  "text": "Third part:  \nTo design better pipelines, it's a ton of resources lol, for example: \n\n\\- Designing Data-Intensive Applications\n\n\\- Understanding Distributed Systems\n\n\\- Cost Effective Data Pipelines\n\n\\- Building Medallion Architecture\n\nand many others...",
                  "score": 1,
                  "created_utc": "2026-01-04 21:28:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxt70uv",
          "author": "crytek2025",
          "text": "What paid DE platforms are charging $10k?",
          "score": 1,
          "created_utc": "2026-01-05 13:03:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz62eq",
      "title": "What dbt tools you use the most?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pz62eq/what_dbt_tools_you_use_the_most/",
      "author": "SnooGiraffes7113",
      "created_utc": "2025-12-30 02:36:20",
      "score": 24,
      "num_comments": 21,
      "upvote_ratio": 0.76,
      "text": "I use dbt on a lot on various client projects. It is certainly a great tool for data management, in general. With introduction of fusion, catalog, semantic model, insights, it is becoming an all stop shop for ELT. And along with Fivetran, you are succumbing to the Fivetran-dbt-snowflake/databricks ecosystem (in most cases; there would also be uses of AWS/GCP/Azure).\n\nI was wondering what dbt features do you find most useful? What do you or your company use it for, and along with what tools? Are there some things that you wished were present or absent?\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pz62eq/what_dbt_tools_you_use_the_most/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwnw7xe",
          "author": "financialthrowaw2020",
          "text": "None of it. We use core. The core features are all we need.",
          "score": 83,
          "created_utc": "2025-12-30 02:53:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoa1el",
              "author": "MachineParadox",
              "text": "Yep core here, generating the documentation/dag is vital though",
              "score": 9,
              "created_utc": "2025-12-30 04:13:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwob7i8",
                  "author": "SnooGiraffes7113",
                  "text": "On our testing, the data column lineage from dbt misses solve connections, due to it using referential checks and sql parsing. The joins and filter miss out.",
                  "score": -3,
                  "created_utc": "2025-12-30 04:20:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwp1kjg",
          "author": "dataflow_mapper",
          "text": "For me the core value is still pretty boring stuff: models, tests, and documentation. Having transformations versioned, reviewable, and testable in SQL is what actually sticks. Everything else feels additive, but not always essential.\n\nWe mostly use dbt for the transformation layer only, sitting on top of Snowflake or Databricks, with ingestion handled elsewhere. Tests and freshness checks punch way above their weight, especially for client work where trust in the data matters more than fancy metrics layers. Lineage and docs also get used more than people admit once teams grow.\n\nThe newer features are interesting, but I have seen mixed adoption. Semantic layer sounds great, but many teams already solved that in BI tools or code. Sometimes it feels like dbt is trying to be the control plane for the whole stack, which is nice in theory but adds cognitive load. I mostly wish they focused on making the core workflow faster and simpler rather than expanding surface area.",
          "score": 9,
          "created_utc": "2025-12-30 07:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpdgbm",
          "author": "sleeper_must_awaken",
          "text": "Only core. We pushed DBT Cloud to our clients until two years back. Then got scared of their sales tactics and decided to move to DBT Core only in our consultancy.",
          "score": 10,
          "created_utc": "2025-12-30 09:34:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo8oke",
          "author": "git0ffmylawnm8",
          "text": "dbt Core. Trying to get more into using cosmos",
          "score": 5,
          "created_utc": "2025-12-30 04:05:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoh3ax",
              "author": "ps_kev_96",
              "text": "I have an article on how I got to using cosmos for a quick headstart , let me know if you need any help",
              "score": 1,
              "created_utc": "2025-12-30 04:59:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqk914",
                  "author": "milkwinner",
                  "text": "Do you mind sharing the article here?",
                  "score": 4,
                  "created_utc": "2025-12-30 14:47:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwoxh60",
          "author": "soltiamosamita",
          "text": "dbt-core, and then some python scripts for documentation/partial overwrite of incrementals/replication/browsing when the vscode extension breaks",
          "score": 6,
          "created_utc": "2025-12-30 07:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo4oog",
          "author": "anatomy_of_an_eraser",
          "text": "dbt core + dbt external tables",
          "score": 6,
          "created_utc": "2025-12-30 03:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoyoj6",
          "author": "Walk_in_the_Shadows",
          "text": "Struggling to find justification for Cloud. We have solid Infrastructure and DevOps setup internally. We don‚Äôt want Catalog, or Semantic models, or Canvas. \n\nHowever, we could do with the Mesh functionality and what they are selling as State Aware Orchestration. \n\nDoes anyone have experience of replicating these in Core?",
          "score": 3,
          "created_utc": "2025-12-30 07:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp1rph",
              "author": "wallyflops",
              "text": "I've just rolled groups out on core and moj\ndel versions are there so you can hand roll mesh I think",
              "score": 1,
              "created_utc": "2025-12-30 07:45:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwozz60",
              "author": "SnooGiraffes7113",
              "text": "Mesh and state aware not on core, to my knowledge. And also depend on your plan. However, you can get state aware using, freshess, defer and state: modified on your pipelines.",
              "score": -3,
              "created_utc": "2025-12-30 07:28:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp06wa",
          "author": "Jegan__Selvaraj",
          "text": "We mostly use dbt for models, tests, and documentation because it helps teams stay consistent as things scale. It fits well after ingestion tools like Fivetran, usually on Snowflake or Databricks. What I still wish for is simpler debugging and better visibility when things break since thats where teams lose the most time.",
          "score": 3,
          "created_utc": "2025-12-30 07:30:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqfezx",
          "author": "UltraInstinctAussie",
          "text": "I plan to try dbt jobs in Fabric when I return from holidays.¬†",
          "score": 1,
          "created_utc": "2025-12-30 14:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqpi18",
          "author": "updated_at",
          "text": "codegen (im just lazy)",
          "score": 1,
          "created_utc": "2025-12-30 15:14:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwmiz8",
              "author": "Ok-Sprinkles9231",
              "text": "It's an interesting one but unfortunately it doesn't seem to be stable. We have a lot of undocumented DBT models, missing columns etc. I was looking for a way to generate columns alongside their types automatically based on the target, which brought me to codegen but couldn't get anything out of it and eventually gave up.",
              "score": 2,
              "created_utc": "2025-12-31 12:46:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nws6mdi",
          "author": "Geraldks",
          "text": "stick to basic, core + kubernetes. for other functionalities, we go for external tools or existing stacks.",
          "score": 1,
          "created_utc": "2025-12-30 19:23:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt9bq7",
          "author": "Spookje__",
          "text": "I was pushing for DBT cloud, but core meets our needs. The cloud pricing is too much for what it offers. And I have doubts about the recent course.",
          "score": 1,
          "created_utc": "2025-12-30 22:28:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvu7hj",
          "author": "Hot_Map_7868",
          "text": "I think what most people use are features that have been there from the start and are available in Core.  \nI don't see many people using advanced dbt features like versioned models, unit tests, etc.\n\nFusion seems interesting, but I dont know how much value it has for projects < 1000 models relative to the increased vendor lock in.",
          "score": 1,
          "created_utc": "2026-01-05 20:46:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2u5l7",
      "title": "Carquet, pure C library for reading and writing .parquet files",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q2u5l7/carquet_pure_c_library_for_reading_and_writing/",
      "author": "Vitruves",
      "created_utc": "2026-01-03 13:05:14",
      "score": 23,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "Hi everyone,\n\nI was working on a pure C project and I wanted to add lightweight C library for parquet file reading and writing support. Turns out Apache Arrow implementation uses wrappers for C++ and is quite heavy. So I created a minimal-dependency pure C library on my own (assisted with Claude Code).\n\nThe library is quite comprehensive and the performance are actually really good notably thanks to SIMD implementation. Build was tested on linux (amd), macOS (arm) and windows.\n\nI though that maybe some of my fellow data engineering redditors might be interested in the library although it is quite niche project.\n\nSo if anyone is interested check the Gituhub repo : [https://github.com/Vitruves/carquet](https://github.com/Vitruves/carquet)\n\nI look forwarding your feedback for features suggestions, integration questions and code critics üôÇ\n\nHave a nice day! ",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q2u5l7/carquet_pure_c_library_for_reading_and_writing/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxnc14p",
          "author": "pseudo-logical",
          "text": "Why write a library from scratch instead of starting with nanoarrow?",
          "score": 1,
          "created_utc": "2026-01-04 16:29:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0j29f",
      "title": "Tessera ‚Äî Schema Registry for Dbt",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q0j29f/tessera_schema_registry_for_dbt/",
      "author": "Low-Sandwich-7607",
      "created_utc": "2025-12-31 17:53:02",
      "score": 23,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "Hey y'all, over the holidays I wrote Tessera (https://github.com/ashita-ai/tessera)\n\nIt's like Kafka Schema Registry but for data warehouses. If you're using dbt, OpenAPI, GraphQL, or Kafka, it helps coordinate schema changes between producers and consumers.\n\nThe problem it solves: data teams break each other's stuff all the time because there's no good way to track who depends on what. You change a column, someone's dashboard breaks, nobody knows until it's too late. The same happens with APIs as well. \n\nTessera sits in the middle and makes producers acknowledge breaking changes before they publish. Consumers register their dependencies, get notifications when things change, and can block breaking changes until they're ready.\n\nIt's open source, MIT licensed, built with Python/FastAPI. \n\nIf you're dealing with data contracts, schema evolution, or just tired of breaking changes causing incidents, have a look: https://github.com/ashita-ai/tessera\n\nFeedback is encouraged. Contributors are especially encouraged. I would love to hear if this resonates with problems you're seeing!",
      "is_original_content": false,
      "link_flair_text": "Open Source",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q0j29f/tessera_schema_registry_for_dbt/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q1r9zp",
      "title": "How can a self-taught data engineer make a step into the big community of data?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q1r9zp/how_can_a_selftaught_data_engineer_make_a_step/",
      "author": "Professional-Sun179",
      "created_utc": "2026-01-02 06:24:20",
      "score": 22,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "I‚Äôm not sure if this the right place to ask these stupid questions but I don‚Äôt know where and I apologize. \nI am literally a beginner in this field and I live in a place where the morden data architecture is not available everywhere and not popular unfortunately. My country is highly developing and I work in a sensitive governmental system where we still use very old transactional databases lol. 2 years ago I was interested of the data science field, and I randomly learned SQL or at least learned what it is, and the journey of data or at least what‚Äôs happening in the data pipelines from ingestion, streaming, integration and processing. Right now I have finished the IBM data engineering course for Python, and it was good and I like it and I took the certificate but this is not enough. I obviously learned that I must implement what I learned and will learn into projects but I kinda feel that I can start on my own. I feel like don‚Äôt need to continue with the course, but at the same time I am very lonely and overwhelmed. I have tried to look for people who are like me everywhere , and on my country‚Äôs subreddit but no use. Because no one knows English even\n\nWhat do you suggest? Is it possible to create an organization on my own? Should i continue with IBM course? And how can I find my people? Sorry for the many questions but I need human answers üòÇ. thank you so much for reading ",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q1r9zp/how_can_a_selftaught_data_engineer_make_a_step/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx8v8he",
          "author": "RopeAltruistic3317",
          "text": "Have you done the entire IBM DE professional certificate on Coursera? There are discussion forums there, though often limited to issues in the course (like bugs). Next I‚Äôd suggest AWS (overview = cloud practitioner)",
          "score": 1,
          "created_utc": "2026-01-02 12:54:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9yz4n",
              "author": "Professional-Sun179",
              "text": "As you know there are series of courses and I have 14 to go. But your feedback reassured me that I am in the right back. Thank you üôè",
              "score": 1,
              "created_utc": "2026-01-02 16:31:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxavmko",
          "author": "Embarrassed_Still608",
          "text": "I'm in this road too. All the tips that i've been getting are basically the same: Learn as much as you can in order to build a solid portfolio, because you need to show to the companies what you really can do and, always look for real experiences, solving real problems, that's what makes you a reliable professional. I'm a fresher, i don't know if i can help you much, but if you want, you can send me a message. OBS: I'm brasilian, i'm learn english too lmao",
          "score": 1,
          "created_utc": "2026-01-02 19:02:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxddp3e",
              "author": "ScottFujitaDiarrhea",
              "text": "You‚Äôd be surprised what you can do for free. Create your own personal static website and host it on s3 and create a DNS for it using Route 53. Create your infrastructure using terraform. Write a GitHub workflow file to detect changes to your html or css files (or whatever you decide to use for your website) that uploads them to your s3 bucket. Add a link in your website to your resume (there‚Äôs free hosting services). Yeah I know some of it doesn‚Äôt apply to us as DEs but it‚Äôs a good learning exercise and pasting in a well-crafted personal website with supporting features on a resume looks good üôÇ. You can do all of that for free, or at most $1-2 a month.",
              "score": 1,
              "created_utc": "2026-01-03 02:56:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhezi8",
                  "author": "Embarrassed_Still608",
                  "text": "you're right. With these free features you can build portfolios that are more valuables then some certificates",
                  "score": 1,
                  "created_utc": "2026-01-03 18:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhgizy",
          "author": "Suspicious-Juice3897",
          "text": "Hello,  \nYou will mostly either code with SQL or Pyspark for big data and pandas for small data ( python ), ( there are other stuff but focus on these two as they are the most used)   \nYou need to learn one of the platforms or tools of DE like databricks , AWS ( S3, Athena, Glue, Redshift ...), GCP ( google cloud) or azure. you can start with AWS ( it's used the most and the other ones have similar tools)  \nalso, learn about building ETL and medallion architecture ( there are other stuffs but those are fundamentals )  \nalso, check out Airflow for scheduling and other stuff  \nIf you want to handle streaming data as well, check out kafka and spark streaming  \nEverything is available in youtube for free, you can have free access to tools as well, so just watch video and redo the same thing.  \nData engineer is mostly human skills most than code skills ( I can spend days searching for the formula of an indicator and implement it in 15 mins afterward)  \nIf you have specific questions, let me know",
          "score": 1,
          "created_utc": "2026-01-03 18:47:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhgsyv",
              "author": "Suspicious-Juice3897",
              "text": "also for data quality, you can check great expectations or soda sql",
              "score": 1,
              "created_utc": "2026-01-03 18:48:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q14civ",
      "title": "Using silver layer in analytics.",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q14civ/using_silver_layer_in_analytics/",
      "author": "Kageyoshi777",
      "created_utc": "2026-01-01 13:35:51",
      "score": 21,
      "num_comments": 44,
      "upvote_ratio": 0.82,
      "text": "So.. in your company are you able to use the \"silver layer\" data for example in dashboarding, analytics etc? We have that layer banned, only the gold layer with dimensional modeled tables are viable to be used for example in tableu, powerbi. For example you need a cleaned data from a specific system/sap table - you cannot use it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q14civ/using_silver_layer_in_analytics/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx2sylp",
          "author": "hill_79",
          "text": "If you need something from Silver that isn't already modelled somewhere in Gold, you extend your model to include it. I can't think of a sensible reason to use Silver data directly in reporting.",
          "score": 71,
          "created_utc": "2026-01-01 13:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4b55e",
              "author": "trentsiggy",
              "text": "I'm wondering what this person's exact definitions of \"gold\" versus \"silver\" are.  \n\nI'm with you -- if there are situations where data that is only in a silver table is needed in a gold table, you figure out a data model that brings that data to gold.  This seems super basic and obvious.\n\nThat leads me to wonder what exactly they mean by \"gold\" and \"silver.\"",
              "score": 8,
              "created_utc": "2026-01-01 18:43:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4itys",
              "author": "Kageyoshi777",
              "text": "Because bringing a one column may take a year to be added? Because the team responsible for that particular that data model is busy with something else. So instead of having a quick solution, business guy still doing their manual sap extract dashboards.",
              "score": 9,
              "created_utc": "2026-01-01 19:21:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx4qfnd",
                  "author": "Last0dyssey",
                  "text": "Yea you're fine. Our presentation layer does not have everything we need. I blame our DW team for not collaborating with the various data teams to determine what make it in. So we use the silver layer to get what we need. Is it ideal? No but I have deadlines and need to maintain good relations with my business partners. I can't go to them and say \"sorry the data isn't in the gold layer we are SOL\". Adapt and move forward",
                  "score": 7,
                  "created_utc": "2026-01-01 19:59:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6sbb3",
                  "author": "BostonPanda",
                  "text": "Gross.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:50:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxa7keh",
                  "author": "Gators1992",
                  "text": "And that's how data lakes evolved....people sick of waiting for IT to add some new column or table to their dimensional model so they said eff it, just give me the raw data and we will figure it out.  Different companies handle this differently, like we give access selectively to silver and even bronze if it makes sense.  If I personally ran into this I guess I would turn it around on them and say \"I need access to X data and maybe other stuff in a timely manner, so can you go figure out the most efficient way to get it in some metal that I have access to\".  Or have your manager calling their manager.  These guys may have a reason why they don't want you poking around silver like there's PII or salary data or something, but ultimately they need to get the data to the consumers so they need to figure that out.",
                  "score": 1,
                  "created_utc": "2026-01-02 17:11:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7zpvu",
                  "author": "hill_79",
                  "text": "Sounds like your company has bigger issues to address - this wouldn't be acceptable in most places",
                  "score": 1,
                  "created_utc": "2026-01-02 08:14:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2y6u0",
          "author": "DynamicCast",
          "text": "Medallion architecture is not an architecture, it's an ill defined naming convention. Hands up everyone who's tried to use it and ended up with tin, copper, and platinum layers üôã.\n\n\nYou've called your dimensional model the gold layer but I'd call it silver. The datamarts and views over the dimensional model would be gold imo.\n\n\nThe reason for restricting access is to prevent dependencies from forming and exfiltration mitigation. I wouldn't give access to intermediate layers but, if there's no sensitive data, I'd give access to staging tables if there was a good reason.",
          "score": 54,
          "created_utc": "2026-01-01 14:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx35e38",
              "author": "Little_Station5837",
              "text": "What would you call a datamart in this case? Is it a joined fact and dimension table?\n\nIf not what part of the layer do you say it is when you join a fact and dimension table that a dashboard reads from?",
              "score": 3,
              "created_utc": "2026-01-01 15:03:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3f07c",
                  "author": "DynamicCast",
                  "text": "We have views that join the facts & dimensions and aggregate tables derived from facts & dims. I'd describe both as datamarts.\n\n\nI'd resist direct access to the facts and dimensions because it makes it harder to change the model.",
                  "score": 3,
                  "created_utc": "2026-01-01 15:58:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3h5qw",
              "author": "Reach_Reclaimer",
              "text": "Nah I prefer that platinum layer as the datamarts/views/reports on top of the gold layer",
              "score": 3,
              "created_utc": "2026-01-01 16:10:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxa4zrv",
              "author": "Gators1992",
              "text": "I started a fake rumor once about a diamond layer where all the premium data was at and had some users asking about access.",
              "score": 3,
              "created_utc": "2026-01-02 16:59:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx32a7r",
              "author": "GachaJay",
              "text": "Interesting. Can you give me a true example of your architecture from source to platinum plus then? I do truly find a common understanding of ‚Äúmedallion‚Äù impossible, even though we do our version of it every single day.",
              "score": 2,
              "created_utc": "2026-01-01 14:43:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3gyas",
                  "author": "DynamicCast",
                  "text": "The problem I've found with medallion is everyone has different opinions on what qualifies as bronze/copper/gold and other layers are introduced as compromises.\n\n\nI've got an analytics layer, which is accessible. That includes views to expose the dimensional model and datamarts. The landing area and everything else in-between the analytics area is inaccessible.",
                  "score": 6,
                  "created_utc": "2026-01-01 16:08:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3hoio",
                  "author": "Reach_Reclaimer",
                  "text": "I can see what they mean.\n\nI imagine it's something like copper (as the true source), bronze as data is ingested into the lakehouse, silver for cleaneddelta tables for said invested data, gold for data models/something else, platinum for views on top or reports, and then semantic layers. Probably not exactly but as a rough meaning\n\nAs to your second point I wouldn't worry about a common undertaking, it's better to see it as a rough framework rather than an exact science because every company will have different needs/requirements/governance for their data.",
                  "score": 2,
                  "created_utc": "2026-01-01 16:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx3644k",
          "author": "ChipsAhoy21",
          "text": "Nope. \n\nAllowing access to the silver layer creates dependencies the DE/analytics engineering teams cannot see. \n\nRestricting access to only gold layer allows the silver layer to be changed and optimized with the only requirement being that gold layer tables meet their SLAs.",
          "score": 9,
          "created_utc": "2026-01-01 15:08:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx37a2v",
              "author": "themightychris",
              "text": "This ^^\n\nLetting people outside AE use silver models means now you can't change them freely anymore",
              "score": 3,
              "created_utc": "2026-01-01 15:15:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2ts8a",
          "author": "Budget-Minimum6040",
          "text": "Not for DAs and Dashboards, hell no.\n\nData Scientists yes.",
          "score": 14,
          "created_utc": "2026-01-01 13:45:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx309xi",
              "author": "TheFirstGlassPilot",
              "text": "I agree. My company has a policy to offer access to the silver layer for Data Scientists. They find it useful, particularly where they want to gain insights into why something has made it into the curated / gold layer and perhaps needs to be eliminated in future. If they can see the underlying \"bad\" data, we discuss the new cleansing rules that are needed.\n\nI think collaboration is a big deal in this scenario. If silver is accessible, I don't think it should be used for actual reporting, but for considering how to make curated better. Easier said than done I suppose.",
              "score": 7,
              "created_utc": "2026-01-01 14:30:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2uksp",
              "author": "datawazo",
              "text": "Same. It's restricted to people that know what they're doing",
              "score": 4,
              "created_utc": "2026-01-01 13:51:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx36g5w",
              "author": "sagin_kovaa",
              "text": "Why not, anybody can dashboard as long as they understand the business requirement and data modeling?.",
              "score": 3,
              "created_utc": "2026-01-01 15:10:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx44s29",
                  "author": "Budget-Minimum6040",
                  "text": "Business logic resides in the semantic layer, documented and as a single source of truth.\n\nDashboards from the intermediate layer leads to everybody recreating the business logic independently.",
                  "score": 1,
                  "created_utc": "2026-01-01 18:12:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3d9h9",
              "author": "bengen343",
              "text": "Ditto. We usually draw a distinction between hardcore analysts and business analysts. Analysts in the Data Team and Data Science types can access Silver for things. Business users, BI tools, and business analysts embedded in non-data teams can only access Gold.",
              "score": 2,
              "created_utc": "2026-01-01 15:49:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxa4mpn",
              "author": "Gators1992",
              "text": "Why not DAs?  I have met plenty of data scientists that are as dumb or more with SQL than your average DA.  Hell we had a contracted data science team in last year that was trying to join fact tables to fact tables on a very standard dimensional model.  DA's also need to do analytics on data that might not be supported in your dimensional model.  We have a lot of history and relational tables in our  \"silver\" layer that are very useful for analytics, but would tend to pollute \"gold\" if you threw all that crap out there in the same schema.",
              "score": 1,
              "created_utc": "2026-01-02 16:57:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx33lyn",
          "author": "Firm-Yogurtcloset528",
          "text": "You don‚Äôt want know after so many years people at the company I work for still have different views of what medaillon architecture is and do not seem to be able to find alignment. So they think of something new üòÄ",
          "score": 2,
          "created_utc": "2026-01-01 14:52:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx39zvn",
          "author": "dasnoob",
          "text": "Our data engineering teams 'gold' layer is not gold. So we end up doing additional transformation to make it usable.\n\nDoes that count?",
          "score": 2,
          "created_utc": "2026-01-01 15:30:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3gpzi",
          "author": "VeniVidiWhiskey",
          "text": "This really depends on the governance in the organisation and how strict the data team wants to be about access, dependencies, and analytics development.\n\n\nGiving access to other layers than gold is fine if you have properly defined how and why. E.g. requiring that analytics solutions only use data from gold when in production, but allowing analysts to utilize tables from other layers for their development and testing iterations.¬†\n\n\nDogmatic approaches is reminiscent of the age-old debate/criticism of centralized data warehouses that took years to develop before anyone in the business would get data access, which reduced their value and usefulness and accelerated the development of \"shadow BI\"-setups in business departments that didn't get their needs met. You can't develop in a vacuum and expect users to accept a (to them) long delivery time just because you want to run the data through all the layers in your architecture. Of course, you also have to protect your setup and governance, but sometimes speed of delivery is more important than perfected delivery.¬†",
          "score": 2,
          "created_utc": "2026-01-01 16:07:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3y5ig",
          "author": "Ok-Sprinkles9231",
          "text": "No, you wouldn't believe what a spaghetti disaster this will create if you do and generally do not enforce this kind of restrictions.\n\nI'd even restrict access/reference to stagings/bronze layer from gold layer.",
          "score": 2,
          "created_utc": "2026-01-01 17:39:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3yuki",
          "author": "Unlucky_Data4569",
          "text": "If products are using the silver payer, it has become the gold layer. DE‚Äôs need the freedom to completely blow away anything that isn‚Äôt gold layer without worrying about downstream",
          "score": 1,
          "created_utc": "2026-01-01 17:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx60neu",
          "author": "IncortaFederal",
          "text": "Sound very limiting.  We are using a Storyteller BI app and it delivers!",
          "score": 1,
          "created_utc": "2026-01-02 00:06:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx66sl9",
          "author": "squirrel_crosswalk",
          "text": "What type of modelling is in your silver layer?\n\n\n\n\n\nI ask because if you ask 5 data modellers what a silver layer should be you will get 7 answers.",
          "score": 1,
          "created_utc": "2026-01-02 00:40:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6w1i8",
          "author": "cpsnow",
          "text": "It depends on the use case. Is the dashboard temporary? Is the dashboard for a one-off data science project? You don't want to create dependency on your silver layer, but you might want to avoid maintaining a new data product on your gold layer for some cases as well. A good way to achieve a similar result would be to source from bronze layer if the data is not available in gold.",
          "score": 1,
          "created_utc": "2026-01-02 03:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcrtpi",
          "author": "Noonecanfindmenow",
          "text": "Might be a hot take, but if I trust my users enough to access the silver layer I would rather give them direct access to the bronze layer",
          "score": 1,
          "created_utc": "2026-01-03 00:49:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlmedz",
          "author": "fidofidofidofido",
          "text": "I use the silver layer, but only because our datalake is a swamp full of toxic waste‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-04 09:43:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4jaby",
      "title": "What are some interesting Data Engineering conferences in Europe for 2026?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q4jaby/what_are_some_interesting_data_engineering/",
      "author": "alex-acl",
      "created_utc": "2026-01-05 11:28:01",
      "score": 20,
      "num_comments": 8,
      "upvote_ratio": 0.85,
      "text": "In my job, we are given the opportunity to go to a conference in Europe. I'd like to go to a deep-tech vendor-free conference that can be fun and interesting.\n\n  \nAny ideas?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q4jaby/what_are_some_interesting_data_engineering/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nxupkj8",
          "author": "Demistr",
          "text": "One that your company pays for and has good snacks.",
          "score": 5,
          "created_utc": "2026-01-05 17:41:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxt4d2q",
          "author": "Nekobul",
          "text": "Check here: [https://datasaturdays.com/](https://datasaturdays.com/)",
          "score": 2,
          "created_utc": "2026-01-05 12:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw8fsc",
          "author": "zbir84",
          "text": "Big Data London is ok, although last year's one was so rammed you had problems getting to some of the more popular presentations.",
          "score": 1,
          "created_utc": "2026-01-05 21:52:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyetzv",
              "author": "haydar_ai",
              "text": "They are looking for ‚Äúvendor-free‚Äù though. But yeah Big Data London has been too crowded for the past 2 years at least.",
              "score": 1,
              "created_utc": "2026-01-06 04:59:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxyvn3w",
                  "author": "zbir84",
                  "text": "Do they exist? I mean outside of meet ups.",
                  "score": 1,
                  "created_utc": "2026-01-06 07:12:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxz2xk5",
          "author": "pvtbobble",
          "text": "Check out DAMA chapters at locations you want to visit.  Usually vendor-free. More data architecture than engineering but the line is often blurred \n\nFind Your Local Chapter - DAMA International¬Æ https://share.google/Qwe8Be7h3IOpq7l9I",
          "score": 1,
          "created_utc": "2026-01-06 08:19:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}