{
  "metadata": {
    "last_updated": "2026-02-14 02:59:58",
    "time_filter": "week",
    "subreddit": "dataengineering",
    "total_items": 20,
    "total_comments": 250,
    "file_size_bytes": 296167
  },
  "items": [
    {
      "id": "1qy6ca4",
      "title": "AI engineering is data engineering and it's easier than you may think",
      "subreddit": "dataengineering",
      "url": "https://www.datagibberish.com/p/ai-powered-apps-dictionary-for-data-engineers",
      "author": "ivanovyordan",
      "created_utc": "2026-02-07 06:20:37",
      "score": 191,
      "num_comments": 33,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1qy6ca4/ai_engineering_is_data_engineering_and_its_easier/",
      "domain": "datagibberish.com",
      "is_self": false,
      "comments": [
        {
          "id": "o42kx1t",
          "author": "DisjointedHuntsville",
          "text": "â€œData engineering is just software engineering and itâ€™s easier than you thinkâ€\n\nâ€œSoftware engineering is just an abstraction of mathematics and itâ€™s easier than you thinkâ€\n\nâ€œMathematics is an abstraction of formally provable empirical observations and its easier than you thinkâ€\n\nFor fucks sake. . . The worst part of the data engineering world is the overkill on needless categorization of work to justify a role.",
          "score": 91,
          "created_utc": "2026-02-07 12:44:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43x5tu",
              "author": "decrementsf",
              "text": "The simplest possible form is trim out the incantations and have direct access to the upstream money printer sinecure. It's a silly game. Dilbert provided a good summary.",
              "score": 4,
              "created_utc": "2026-02-07 17:07:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48gfa9",
                  "author": "dillanthumous",
                  "text": "Don't forget to sing the appropriate psalms while communing with the Omnisiah.",
                  "score": 3,
                  "created_utc": "2026-02-08 11:00:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o448tzh",
              "author": "[deleted]",
              "text": "Itâ€™s not that serious. I thought it was a decent write up.",
              "score": 3,
              "created_utc": "2026-02-07 18:05:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o463pkf",
              "author": "TechnicallyCreative1",
              "text": "Title was click bait, the content wasn't terrible. The sentiment was that data organization is important. That is all",
              "score": 2,
              "created_utc": "2026-02-08 00:09:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o42fd55",
          "author": "mint_warios",
          "text": "AI engineering existed way before LLMs. Agree there's a lot of overlap with \"classical\" data engineering, but there's so much more to AIE/MLE than LLM pipelines and RAG mechanisms",
          "score": 64,
          "created_utc": "2026-02-07 11:59:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42fyoy",
              "author": "MonochromeDinosaur",
              "text": "MLE and AI Engineering are completely separate job descriptions. \n\nAI Engineering roles are essentially just web development using LLMs with a side of data engineering.\n\nMLE is Data Engineering+MLOps+actual understanding of ML.",
              "score": 38,
              "created_utc": "2026-02-07 12:04:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o488mih",
                  "author": "xorgeek",
                  "text": "What is MLops",
                  "score": 2,
                  "created_utc": "2026-02-08 09:46:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o445ctx",
                  "author": "Blaze344",
                  "text": "Proper AI engineering should have at least some basic ML knowledge behind the things being built. Knowing the best way to represent information from retrieval, running experiments to get the F1 score of the current solution, knowing how to debug all the moving pieces to find which one is bottle necking...\n\nThere's a lot of web dev, tho, that's true, and MLE is the one that really grits into true ML territory. It's just that current AI is \"powerful enough\" (quotes required) that you can make do without having the core skills and deliver something, just in spite of how powerful things are. Sort of how we have so much compute no one cares about delivering something memory aware nowadays, too...",
                  "score": 2,
                  "created_utc": "2026-02-07 17:48:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o42fp1f",
              "author": "ivanovyordan",
              "text": "I 100% agree with you here. But you will also agree that in 2026 AI means LLM for most people.",
              "score": 9,
              "created_utc": "2026-02-07 12:02:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42yszk",
          "author": "genobobeno_va",
          "text": "I concur. Itâ€™s all pipelines.",
          "score": 13,
          "created_utc": "2026-02-07 14:12:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43xo4e",
              "author": "decrementsf",
              "text": "There is a non zero chance this is the same as medical students experiencing every medical condition they study in sequence. Humans are a pattern recognition machine interpreting stimulus through the bounds of the information already known. What do I know? Predict what happens next. Surprise or affirms the machine is working. Output -> It's all pipelines.",
              "score": 2,
              "created_utc": "2026-02-07 17:10:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44glc8",
          "author": "gnomehearted",
          "text": "AI engineering's existence makes me want to leave the industry wholesale",
          "score": 8,
          "created_utc": "2026-02-07 18:43:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44juk7",
              "author": "ivanovyordan",
              "text": "A friend of mine left his job on Friday because he felt unappreciated contary to folks who build LLM solutions.\n\nWhat exactly is the thing that you dislike?",
              "score": -2,
              "created_utc": "2026-02-07 18:59:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45fm0k",
                  "author": "Aggravating-One3876",
                  "text": "For me itâ€™s what you can tour AI all day long but management only hears that you can do your job faster. If you can do your job faster then letâ€™s move up deadlines and if you can get push back itâ€™s pretty much â€œwell AI should make it fasterâ€.\n\nThe other issue is that AI makes senior devs spend more time cleaning up the AI code that gets put into production and it robs junior devs of experience in debugging. \n\nFor me the only reason to use AI is that deadlines become unreasonable and that is in part of people overselling what AI can do. Then when you do use AI it can give you wrong answers so hopefully you look at the code that you are putting in. \n\nSo for me AI does not benefit DE and is only to shorten deadlines and put pressure to preform, thus forcing you to use it just to keep up.",
                  "score": 2,
                  "created_utc": "2026-02-07 21:48:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ba5ct",
                  "author": "lolsillymortals",
                  "text": "Your friend sounds like a spoiled worker bee.\n\nâ€œIâ€™m not getting the glory I used to, Iâ€™m going to throw a fit and go get another job where they think what Iâ€™m doing is amazing again! One more pat on the back is all Iâ€™m asking for!â€",
                  "score": 1,
                  "created_utc": "2026-02-08 20:29:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o459gtj",
          "author": "constantly-pooping",
          "text": "probiabtistic?",
          "score": 4,
          "created_utc": "2026-02-07 21:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45atns",
              "author": "ivanovyordan",
              "text": "Ha-ha. That's how you know I don't use AI to write. :D\n\nThanks for that. Won't fix it though. It's funny",
              "score": 1,
              "created_utc": "2026-02-07 21:22:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46dqwq",
          "author": "Procrastinator9Mil",
          "text": "A post from someone who doesnâ€™t understand neither.",
          "score": 2,
          "created_utc": "2026-02-08 01:09:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47labx",
              "author": "ivanovyordan",
              "text": "Interessting. What made you think so?",
              "score": 1,
              "created_utc": "2026-02-08 06:10:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fvakj",
          "author": "Thinker_Assignment",
          "text": "Thanks for this one, the community needs to hear this more, the AI layer is definitely more easily served by DEs than new SEs as AIEs",
          "score": 2,
          "created_utc": "2026-02-09 14:50:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4h51cf",
              "author": "ivanovyordan",
              "text": "I appreciate it!",
              "score": 1,
              "created_utc": "2026-02-09 18:30:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47tdev",
          "author": "PracticalBumblebee70",
          "text": "Written by data engineerÂ ",
          "score": 1,
          "created_utc": "2026-02-08 07:23:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47w72t",
              "author": "ivanovyordan",
              "text": "True",
              "score": 2,
              "created_utc": "2026-02-08 07:49:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4drc9f",
          "author": "EviliestBuckle",
          "text": "What is ideal tech stack these days? Also can you suggest some beginners courses?",
          "score": 1,
          "created_utc": "2026-02-09 04:43:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54x5sl",
          "author": "putokaos",
          "text": "It is not. At a high level it might be, but model industrialization requires a deep understanding of math. That said, it's wise to acknowledge that AI Engineering cannot exist without Data Engineering.",
          "score": 1,
          "created_utc": "2026-02-13 09:25:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42pdpb",
          "author": "Illustrious_Role_304",
          "text": "is AI engineering is mandatory for data engineering  now ? Any recent interview experience ?",
          "score": 1,
          "created_utc": "2026-02-07 13:15:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42s4f1",
              "author": "ivanovyordan",
              "text": "From a hiring manager point of view, I'd say no.\nBut the tuth is that it's very easy and some \"broader\" knowledge can only help in interviews.",
              "score": 2,
              "created_utc": "2026-02-07 13:32:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o452rwa",
          "author": "dudeaciously",
          "text": "This is a beautiful, concise and meaningful article.",
          "score": 1,
          "created_utc": "2026-02-07 20:39:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45avt5",
              "author": "ivanovyordan",
              "text": "Thank you so much. I really appreciate this.",
              "score": 2,
              "created_utc": "2026-02-07 21:23:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1tcjp",
      "title": "It's nine years since 'The Rise of the Data Engineer'â€¦what's changed?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/",
      "author": "rmoff",
      "created_utc": "2026-02-11 09:58:52",
      "score": 156,
      "num_comments": 37,
      "upvote_ratio": 0.96,
      "text": "See title\n\nMax Beauchemin published [The Rise of the Data Engineer](https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603) in Jan 2017 (_and [The Downfall of the Data Engineer](https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b) seven months later_).\n\nWhat's the biggest change you've seen in the industry in that time? What's stayed the same?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4sncod",
          "author": "redditreader2020",
          "text": "COVID and AI. And I have more grey hair. Otherwise not much.",
          "score": 127,
          "created_utc": "2026-02-11 13:27:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tqr0r",
              "author": "updated_at",
              "text": "I'm balding",
              "score": 20,
              "created_utc": "2026-02-11 16:45:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ttb5t",
                  "author": "zerofatorial",
                  "text": "/r/tressless",
                  "score": 8,
                  "created_utc": "2026-02-11 16:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4swe8s",
          "author": "drag8800",
          "text": "Been in data since 2012. Few observations:\n\nWhat changed completely:\n- Infrastructure abstraction. In 2017 we were still debating Hadoop distributions. Now most teams never think about cluster management.\n- Analytics engineering emerged as a discipline. Beauchemin predicted DEs would need more SQL, but underestimated how much the transformation layer would specialize (dbt, semantic layers, etc.)\n- The 'modern data stack' hype cycle. Lots of point solutions that promised to solve specific problems, then consolidation as everyone realized 47 tools was too many.\n\nWhat stayed the same (unfortunately):\n- The gap between 'we have data' and 'we understand the business domain.' Still the hardest part.\n- Pipeline maintenance burden. Different failure modes now (API rate limits vs disk space), same percentage of time spent on it.\n- Stakeholder expectations vs data quality reality.\n\nWhat's genuinely better:\n- Getting started is 10x easier. A junior can have a working pipeline in a day instead of weeks.\n- The tooling for testing and observability actually exists now.\n- Version control for transformations is standard, not exotic.\n\nThe 'Downfall' article was prescient about platform engineering eating some DE work. But the semantic layer and data modeling parts got more complex, not less. Different work, roughly same headcount.",
          "score": 165,
          "created_utc": "2026-02-11 14:17:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tif2n",
              "author": "Kobosil",
              "text": ">The gap between 'we have data' and 'we understand the business domain.' Still the hardest part.\n\nas long thats the case i am not worried for my job",
              "score": 42,
              "created_utc": "2026-02-11 16:07:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vdrm1",
                  "author": "trixiethegoat",
                  "text": "same. It's a pain, but once you're the data SME for the business team and the domain SME for the data engineering team, you're golden.",
                  "score": 13,
                  "created_utc": "2026-02-11 21:25:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50rzug",
                  "author": "JBalloonist",
                  "text": "Same here.",
                  "score": 1,
                  "created_utc": "2026-02-12 18:08:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tf7td",
              "author": "cokeapm",
              "text": "What tooling for testing and observability do you recommend?",
              "score": 3,
              "created_utc": "2026-02-11 15:52:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tqu6q",
                  "author": "updated_at",
                  "text": "Monte Carlo and soda",
                  "score": 8,
                  "created_utc": "2026-02-11 16:46:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vkna7",
              "author": "umognog",
              "text": "I was just reflecting earlier today on the same point about having a working pipeline in a day rather than a week.\n\nToday, i not only put up a PPE & a prod machine and wrote \"the happy path code\" but I also had it all connected to observation, lineage and some basic tests.\n\nIn 5 hours.\n\nThat was really unheard of by a single person in my business in 2015/2016. Getting a VM alone was 4-6 weeks back then.",
              "score": 2,
              "created_utc": "2026-02-11 21:57:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50s4vz",
              "author": "JBalloonist",
              "text": "Iâ€™m glad I avoided the Hadoop phase and went straight to Spark.",
              "score": 1,
              "created_utc": "2026-02-12 18:09:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4thiax",
          "author": "mach_kernel",
          "text": "As hardware is getting better some \"big data\" is no longer that big. I see more and more developers reaching for things like DuckDB. I see an increase of robust federation solutions for cross-engine queries and optimizations.\n\nI am happy to see that the enterprise data pipeline is becoming more \"a la carte\".",
          "score": 26,
          "created_utc": "2026-02-11 16:03:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vdgpt",
              "author": "Mclovine_aus",
              "text": "So many places where the data could easily fit in a single machine, but execs fell for the big data warehouse and have bought a managed spark service like synapse - the bane of my existence.",
              "score": 10,
              "created_utc": "2026-02-11 21:23:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xxhxk",
                  "author": "One_Citron_4350",
                  "text": "Yes, they looked at it from a one-size-fits all point of view. Let's just put everything in Databricks with Spark.",
                  "score": 1,
                  "created_utc": "2026-02-12 06:58:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50tl0b",
              "author": "JBalloonist",
              "text": "Absolutely. Most of my work is done using DuckDB right now (using Lakehouses and Delta tables).",
              "score": 1,
              "created_utc": "2026-02-12 18:16:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4whsre",
          "author": "_TheDataBoi_",
          "text": "I was hired as a data engineer, but my role demands more than just data engineering starting from devops, data analysis, front end (streamlit and nextjs), business translation, some legal aspects of data processing and sharing, infra maintainability lmao.\n\n\nSince being a data engineer already would've touched the above tangents, we are now expected to take the entire thing upon ourselves. Data engineering has become the bridge connecting business to tech. Data engineers are the ones who enable decisions. We are just not in the spotlight.",
          "score": 10,
          "created_utc": "2026-02-12 00:58:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u7cyd",
          "author": "StewieGriffin26",
          "text": "Lots of consolidation to either Snowflake or Databricks. Either platform \"does it all\" now. \n\nAlso reinventing the wheel. What IBM and Oracle released back in the 80s is what Databricks and Snowflake are releasing now, just with a fancier name.",
          "score": 23,
          "created_utc": "2026-02-11 18:03:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v3n1g",
              "author": "GarboMcStevens",
              "text": "software is cyclical. People old enough age out and then you can rebrand old things as new.",
              "score": 13,
              "created_utc": "2026-02-11 20:36:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4v7pgx",
              "author": "kbisland",
              "text": "Iâ€™m wondering what IBM or Oracle released?",
              "score": 9,
              "created_utc": "2026-02-11 20:56:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vldi2",
                  "author": "StewieGriffin26",
                  "text": "https://old.reddit.com/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/o4vl9cp/",
                  "score": 1,
                  "created_utc": "2026-02-11 22:01:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xowb3",
              "author": "sib_n",
              "text": "I don't think making data warehousing work on a distributed cluster of commodity machines is just a \"fancier name\". There's a reason why Google, Yahoo and all web giants, invested in R&D to develop them from the 00', which gave birth to Hadoop. Snowflake and Databricks are abstracting this but it is still behind.  \nWhat's true is that they are still trying to reach the same level of reliability and features that those monolithic systems already had (like ACID), but some of them are not easy to reach with a distributed system. The latest big improvements towards this is the new table formats like Iceberg and Delta Lake to allow merge, time travel, column renaming and other metadata related features.",
              "score": 4,
              "created_utc": "2026-02-12 05:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yrvgz",
                  "author": "spcmnspff99",
                  "text": "As a database person, some of the original dialogue when systems like Hadoop and Spark were going mainstream was that ACID may not be as important as these other features like distributed data, etc. Before that, ACID was more of a golden rule you never broke. With this and some of these other features you mention that are rdbms standards, I see a sort of boomerang in your industry where some of the tenants were torn down in light of more important features and are now being gradually reintroduced while prioritizing said features.",
                  "score": 1,
                  "created_utc": "2026-02-12 11:45:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vdl2k",
              "author": "Mclovine_aus",
              "text": "Wha are some examples of re released features?",
              "score": 2,
              "created_utc": "2026-02-11 21:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vl9cp",
                  "author": "StewieGriffin26",
                  "text": "Databricks released temporary tables on Dec 9, 2025.  \nOracle released temporary tables in 1999.  \nIBM released temporary tables in 2001.  \nSnowflake released temporary tables in 2014.  \nSybase released temporary tables in ~1987.",
                  "score": 10,
                  "created_utc": "2026-02-11 22:00:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vetcr",
          "author": "Eleventhousand",
          "text": "Back before the term Data Engineer was a thing, I was still making design patterns and frameworks for my team.  Yes, I also spent half of my time on business problems, but I spent the other half on ensuring that we had a rock-solid and maintainable product, including tooling developed in 3GL languages as opposed to pure SQL.  There were other companies that had job duties split out - one team might handle the data modeling, another might handle the Informatica stuff, and another might handle dashboards, reports, and ad-hoc requests for insights.  I don't think much changed fundamentally, other than the job title, no different than going from being titled Programmer/Analyst one decade to Software Engineer during the next.  So, I'm not totally sold on the rise of Data Engineer.\n\nAs far as what has changed since 2017, really, just more cloud tools, more automation, etc.\n\n",
          "score": 7,
          "created_utc": "2026-02-11 21:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tcck6",
          "author": "zjaffee",
          "text": "The truth is that data engineer is a fake role that can mean tons of different things to different people in the same way roles like DevOps engineer or SRE can, increasingly ML engineer has a similar vibe. This was something very popular in the world of software development in 2017 where people were very focused on defining what large software teams should look like, along with the desire to build all sorts of new frameworks, this has died down.\n\nThere are places where a data engineer is a software engineer who owns the full stack of the data platform whatever that means, including in my cases also building data products on top of said platform. There are other places where a data engineer is someone who writes SQL largely for ETL purposes and maybe just manages the schema and type definitions of a particular data set and optimizes the routine queries that are run against said database, but even then that can be a stretch. In other cases, it might just be closer to a db admin setting privacy rules so that development teams cannot misuse PII.",
          "score": 22,
          "created_utc": "2026-02-11 15:38:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tigtg",
              "author": "Swayfromleftoright",
              "text": "Couldnâ€™t you say that about pretty much any tech role though? A data analyst at company A probably spends their time differently to one at company B",
              "score": 11,
              "created_utc": "2026-02-11 16:07:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4trbic",
                  "author": "updated_at",
                  "text": "A carpenter at company A builds tables and at the company B builds doors. Always been like that",
                  "score": 8,
                  "created_utc": "2026-02-11 16:48:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v30sf",
          "author": "Accomplished-Row7524",
          "text": "dbt and analytics engineering",
          "score": 2,
          "created_utc": "2026-02-11 20:33:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yn7pt",
          "author": "Nervous-Potato-1464",
          "text": "Been in data since 2000s in finance now as an executive. Lots of bad decisions from the top regarding infrastructure. Moving to the cloud with all our data cost a lot, moving from mainframes cost a lot, and still using sas in most areas which again costs a lot. Still using oracle databases even though we were meant to migrate off in 2014. We now have a double database solution and oracle is meant to stop soon even though the new database is not better just had more development in the past 15 years. Still ml models are scary and we don't hire people who know how to do it so they all end up half arsed. No Python server as we are big into sas although some teams use r to make models. We now have ai which isn't so bad for data as you can't just write stuff as the data is a bit unknown to the AI so it can only build small functions but they are always super complicated and anyone that wants to reuse it has to try really hard to understand the over complexity of a simple task.",
          "score": 1,
          "created_utc": "2026-02-12 11:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yyxla",
          "author": "ScientistMundane7126",
          "text": "One big change accomplished by making data engineering an explicit specialization is that we're now aware of data quality issues that were hidden, or which were just consequences of the data gathering methods prevalent before big data frameworks and methods became available for large scale aggragate mobilization. Automation amplifies problems as much as it amplifies solutions.  The data engineer gets the heat when the products of their automation designs don't meet expectations, and the QA inquiry too often reveals problems with the data itself, including missing values, data entry errors, mismatched or approximated semantics when bringing together variously sourced data, accuracy and precision problems, deliberate and accidental bias, agenda selectivity, etc.  AI is built on big data infrastructure, so its good that we have this professional layer to review our supply chains as we procede to the next generation of decision support.",
          "score": 1,
          "created_utc": "2026-02-12 12:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xftus",
          "author": "Wizardij",
          "text": "The Rise of the Dead Engineers!\n\nNobody wants to work anymore. ðŸ˜",
          "score": -1,
          "created_utc": "2026-02-12 04:32:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0ff3b",
      "title": "[AMA] Weâ€™re dbt Labs, ask us anything!",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/",
      "author": "andersdellosnubes",
      "created_utc": "2026-02-09 20:24:08",
      "score": 139,
      "num_comments": 124,
      "upvote_ratio": 0.91,
      "text": "Hi r/dataengineering â€” though some might say analytics and data engineering are not the same thing, thereâ€™s still a great deal of dbt discussion happening here. So much so that the superb mods here have graciously offered to let us host an AMA happening this **Wednesday, February 11 at 12pm ET.**\n\nWeâ€™ll be here to answer your questions about anything (though preferably about dbt things)\n\n**As an introduction, we are:**\n\n* Anders u/andersdellosnubes (DX Advocate) ([obligatory proof](https://i.imgur.com/4WzEcKM.jpeg))\n* Jason u/dbt-Jason (Director: DX, Community & AI)\n* Jeremy Cohen u/jtcohen6 (PM) ([proof](https://imgur.com/rGUclDq))\n* Grace Goheen u/dbt-grace (PM) ([extra extra proof](https://i.imgur.com/pGMhBlk.gif))\n* Sara u/schemas_sgski (Product Marketing)\n* Quigley u/dbt-quigley (dbt Core engineer) ([proof](https://imgur.com/a7e89c8b-ee7d-42d3-a249-0fa68fe8d928))\n* Zeeshan u/dbt-zeeshan (Core engineering manager) ([proof](https://i.imgur.com/EkgG2dC.jpeg))\n* Tristan Handy u/jthandy (founder/CEO)\n\n**Hereâ€™s some questions that you might have for us:**\n\n* [whatâ€™s new](https://github.com/dbt-labs/dbt-core/releases/tag/v1.11.0) in dbt Core 1.11? whatâ€™s [coming next](https://github.com/dbt-labs/dbt-core/blob/main/docs/roadmap/2025-12-magic-to-do.md)?\n* whatâ€™s the latest in AI and agentic analytics ([MCP server](https://docs.getdbt.com/blog/introducing-dbt-mcp-server), [ADE bench](https://www.getdbt.com/blog/ade-bench-dbt-data-benchmarking), [dbt agent skills](https://docs.getdbt.com/blog/dbt-agent-skills))\n* whatâ€™s [the latest](https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md) with Fusion? is general availability coming anytime soon?\n* who is to blame to `nodes_to_a_grecian_urn` corny classical reference in our [docs site](https://docs.getdbt.com/reference/node-selection/yaml-selectors)?\n* is it true that we all get goosebumps anytime anytime someone types dbt with a capital d?\n\nDrop questions in the thread now or join us live on Wednesday!\n\nP.S. thereâ€™s a dbt Core 1.11 live virtual event next Thursday February 19. It will have live demos, cover roadmap, and prizes! [Save your seat here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&utm_source=reddit&utm_campaign=q1-2027_dbt-core-live_aw&utm_content=themed-webinar____&utm_term=all_all__).\n\nedit: Hey we're live now and jumping in!\n\n>thanks everyone for your questions! we all had a great time. we'll check back in on the thread throughout the day for any follow ups!\n>\n>If you want to know more about dbt Core 1.11, next week there's a live event next week!\n>\n>[reserve your spot here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&utm_source=reddit&utm_campaign=q1-2027_dbt-core-live_aw&utm_content=themed-webinar____&utm_term=all_all__)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4irts2",
          "author": "Interesting_Tank_118",
          "text": "I love your product! Since merging with Fivetran: Whats the long term strategy of dbtlabs? i.e. will dbt cloud have even more advanced features than dbt core to get more paying customers?",
          "score": 85,
          "created_utc": "2026-02-09 23:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u1abx",
              "author": "dbt-jason",
              "text": "First of all - thank you for being a dbt user!! Before I worked at dbt Labs I spent a lot of time on this subreddit so it's great to be here chatting with you all.\n\nRe: merger / Core / Cloud\n\nThe process of merging the two companies is still underway, so thereâ€™s really not much we can say besides â€œstay tuned.â€ The details we shared back in October at Coalesce in Las Vegas ([blog](https://www.getdbt.com/blog/dbt-labs-and-fivetran-merge-announcement), [keynote](https://youtu.be/KhBsI2LQQ90?si=YWWxfJnK5J6vRxOg)) haven't changed.\n\nOne thing I'll add is that we're committed to keeping dbt Core the open source standard for data transformation. Are we going to ship great new dbt features to our paying customers? Of course. But not by keeping important functionality out of dbt Core. The way to do it is to make sure dbt Core (and Fusion) have more powerful and better functionality and then build even better features on top.\n\nGrace and the rest of the Core team can talk about some of the things we've been cooking up there. And most importantly - we \\_always\\_ want to hear from you. What would you like to see in Core?",
              "score": 8,
              "created_utc": "2026-02-11 17:35:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4tet8g",
              "author": "finally_i_found_one",
              "text": "Looks like the most voted (and most important) question went unanswered.",
              "score": -4,
              "created_utc": "2026-02-11 15:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4utnx1",
                  "author": "andersdellosnubes",
                  "text": "I think you were early to the party!",
                  "score": 2,
                  "created_utc": "2026-02-11 19:47:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u80vu",
              "author": "andersdellosnubes",
              "text": "what u/dbt-jason said! time will tell; stay tuned",
              "score": 0,
              "created_utc": "2026-02-11 18:06:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jcvg9",
          "author": "FIapdoodle",
          "text": "Some questions pertaining to Fivetran merger:\n\nSince they also acquired Tobiko Data, will we see any consolidation/standardization efforts between DBT and SQLMesh? \n\nWith Fivetran providing many data connectors, will we see a more end-to-end flow established where the ingestion and transformations will be managed together?",
          "score": 25,
          "created_utc": "2026-02-10 01:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u45gh",
              "author": "andersdellosnubes",
              "text": "u/dbt-jason already touched on this in [this reply](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u1abx/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 0,
              "created_utc": "2026-02-11 17:48:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4vknbx",
                  "author": "m1hawkgsm",
                  "text": "That's not an answer, though. It makes an explicit reference to only Fivetran / dbt as \"two companies merging\".",
                  "score": 3,
                  "created_utc": "2026-02-11 21:57:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4j8enu",
          "author": "flatulent1",
          "text": "The gap between dbt explorer/catalog and a full scale catalog like Atlan or Datahub or Alation or Secoda etc is still huge. In 2026, data is a context game. Are yall just playing it safe as a metadata provider for more expensive, external catalogs?",
          "score": 16,
          "created_utc": "2026-02-10 00:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2ypt",
              "author": "jthandy",
              "text": "Awesome question.\n\nWe have a ton of respect for what pure-play catalog providers do and how hard that is. A lot of this functionality is purely an integration game, and thatâ€™s a TON of work both in terms of the initial build and the maintenance of the integrations.\n\nWe do think that dbt metadata is really central to all analytical metadata and think that weâ€™re the best place to get that, whether thatâ€™s via artifacts or our API. And with Fusion this dbt-related metadata is getting *significantly* more advanced.\n\nWe do have a few integrations where we pull in metadata of downstream artifacts--tableau and powerbi--but certainly we are *nowhere near as comprehensive* as most pure-play catalogs. And that is ok.\n\nOur goal with Catalog is:\n\n* to provide a really amazing catalog experience that can be the ONLY catalog for small-to-medium companies.\n* to provide a really useful development tool for dbt authors at companies of all sizes\n* to provide a high-quality source of dbt metadata for companies of all sizes\n\nOver time, I fully imagine that we will continue to expand our capabilities here (as we have been!). But weâ€™re not trying to head-to-head compete against pure-play catalog companies.",
              "score": 5,
              "created_utc": "2026-02-11 17:43:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4io7a9",
          "author": "Sex4Vespene",
          "text": "Thanks for reaching out for questions, I love DBT and itâ€™s been so useful for our org. I have one question thatâ€™s somewhat of a feature request. Have you considered having an â€œintermediateâ€ type table materialization? We have several large models that have to be broken up into intermediate steps because they would either overload memory, or perform poorly due to CTEâ€™s that are called multiple times, which we can instead just process them once in a separate model. What gets annoying with this, is we donâ€™t want any of these intermediate models taking up space in our warehouse, so we have to use a custom post-hook on any end-state models to clean up the upstream intermediate models. It would be really awesome if you integrated this automatically. Let us use intermediate as a materialization strategy, and have them be autodropped once an end-state model finishes. I know you have ephemeral and view materializations, but none of those solve the problem of having too much stuff happening in the final query that uses them. Thanks again!",
          "score": 25,
          "created_utc": "2026-02-09 23:04:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4y6frm",
              "author": "Global_Bar1754",
              "text": ">Â Let us use intermediate as a materialization strategy, and have them be autodropped once an end-state model finishes.\n\nCorrect me if Iâ€™m wrong, but it would be even better if the intermediate model was dropped after the last model that directly references it is materialized, right? No need to wait for the end state model to finish?",
              "score": 1,
              "created_utc": "2026-02-12 08:23:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4iw32u",
              "author": "Chilangosta",
              "text": "Something about this feels off... You have such massive tables that they overload memory, and are forced to break apart queries into intermediate steps. But you then delete said intermediate products presumably because they take up so much space that this saves you money.... And somehow the savings are worth rebuilding it from scratch every time?\n\nI have so many questions; this isn't making sense to me. Feels like either you're overoptimizing for cost when it's really not worth it or else doing something wrong when it comes to query optimization for the intermediate products to truly not be worth keeping around. Are your joins exploding? Is storage really so expensive that rebuilding these intermediates makes sense? Are you doing too much in your final query? Without more info it's hard to say for certain, but intermediate products aside I suspect something isn't as optimal about this scenario as you think.",
              "score": 1,
              "created_utc": "2026-02-09 23:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4iyv6s",
                  "author": "Sex4Vespene",
                  "text": "Youâ€™re making some incorrect assumptions here. We are on prem, this isnâ€™t about saving cloud costs, itâ€™s about us being somewhat low on storage and trying to minimize how much useless data is just sitting around. We are working on getting budget to expand, but that doesnâ€™t happen immediately. I donâ€™t understand what you mean about doing something wrong with optimization for them to not be worth keeping around. These are models that are run on a daily/weekly/monthly cadence. The previous versions of the intermediate models are useless for the next run. They either cover a completely different date range, and even if there is overlap, we have a decent amount of updates/deletes against old records so weâ€™d want to rerun with the newest versions of the records anyways. And no, our joins arenâ€™t exploding, Iâ€™ve actually done a huge amount of work creating optimization guidelines for our models, and will go in to fine tune when needed. Since we are on prem, we arenâ€™t paying for aggregate compute usage. We have a set amount of compute and memory that is always available at any moment. If we just ran one model at a time, we would be massively underutilizing our compute. To make the most out of our compute, we run multiple models at once. But since we arenâ€™t just running one model, we have to set memory limits on all models, by dividing our available memory by the number of concurrent models we run, to ensure we donâ€™t have memory related failures. Also as I mentioned, there are cases where breaking things out into an intermediate model can have massive performance improvements. Our query engine materializes CTEâ€™s every time it is called. If you use the same CTE in a query 5 times, it quintuples the memory and compute usage, versus just running it once into a table and then just referencing the table a few times. It all makes plenty of sense, you are just evaluating it from an invalid context.",
                  "score": 12,
                  "created_utc": "2026-02-10 00:02:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4iwknd",
          "author": "RobG760",
          "text": "How will dbt core adapt to a world where streaming pipelines are starting to become more common?  How do you see dbt helping build a clean data lineage across all enterprise data regardless of whether batch or streaming tools are being leveraged?",
          "score": 26,
          "created_utc": "2026-02-09 23:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u7z63",
              "author": "jtcohen6",
              "text": "candidly - streaming pipelines have been \"starting to become more common\" for as long as I can remember (since \\~2018)  \n  \nok in seriousness - what i can say is:\n\n* the big innovation over the past \\~decade has been the development of streaming pipelines *written in SQL*. (you don't need to know java/scala! power to the data team!)\n* there are dbt adapters for streaming-first/native DWHs that support an all-SQL interface - Materialize has had an adapter for years, and we see that the Clickhouse adapter has been quite popular of late\n* batch DWHs Snowflake + Databricks have both rolled out support for their own streaming solutions - Dynamic Tables and Materialized Views / Streaming Tables, respectively - with full SQL support. those features are supported as materialization types in dbt, a pretty decent number of folks are using them, but for <5% of all the models in their project.\n\nso - while we see folks adopting streaming pipelines for specific use cases, when there is a specific business need that justifies the added cost/complexity - we still see batch (with hourly/daily refresh) as \"good enough\" for >90% of data transformations.  \n  \nI think dbt's role is continuing to serve as an abstraction across *both* batch and streaming pipelines - both kinds of data products still need version control, testing, documentation, CI/CD, ... - and they should be written in SQL, as dbt models :)",
              "score": 6,
              "created_utc": "2026-02-11 18:06:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ua3zq",
              "author": "muneriver",
              "text": "this may be interesting to you: https://docs.getdbt.com/best-practices/how-we-handle-real-time-data/1-intro",
              "score": 3,
              "created_utc": "2026-02-11 18:16:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j35rw",
          "author": "midnighttyph00n",
          "text": "when will fusion support python  models",
          "score": 11,
          "created_utc": "2026-02-10 00:26:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lgyag",
              "author": "pseudo-logical",
              "text": "It already does for the big three (snow dbx and bq) which says something about their marketing and comms",
              "score": 3,
              "created_utc": "2026-02-10 11:11:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tyoik",
                  "author": "jtcohen6",
                  "text": "u/pseudo-logical is right - Fusion added supported for Python models way back in \\[v2.0.0-preview.81\\]([https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md#200-preview81---december-09-2025](https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md#200-preview81---december-09-2025)) - across Snowflake + Databricks + BigQuery  \n  \nto the point about marketing/comms (fair!) - we haven't made a big announcement (yet), since we're still testing out this functionality in real-world projects with help from folks in the community. we know we've shipped *a lot* of stuff in Fusion over the past \\~6 months, and we're trying to do a better job of distinguishing between \"this is new and could use your help testing\" vs. \"this is ready for production *right now*.\" you can expect to hear more from us in the next \"Fusion diary\" (long awaited & soon to drop!). the big remaining piece before GA is figuring out how \"static analysis\" can/should work with Python models (which, for obvious reasons, can't be analyzed as SQL) - [https://github.com/dbt-labs/dbt-fusion/discussions/1042](https://github.com/dbt-labs/dbt-fusion/discussions/1042)\n\nfyi - you can always check here to see the latest table of dbt feature support / what's still missing in Fusion - [https://docs.getdbt.com/docs/fusion/supported-features](https://docs.getdbt.com/docs/fusion/supported-features)\n\nu/midnighttyph00n \\- how are you using Python models in dbt today? on which data warehouse? are you down to give them a try on Fusion? :)  \n  \n**tl;dr - Python models are in Preview (like the rest of Fusion), and will be GA when Fusion goes GA**",
                  "score": 6,
                  "created_utc": "2026-02-11 17:23:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4j4wea",
          "author": "HC-Klown",
          "text": "Please, when will you support write audit publish pattern as a materialization option? I want to build the model in some temp environment then test it and then â€œdeployâ€ it without having to rely on data branching features from for example Nessie or LakeFs. \n\nI know this can get more involved with views etc but do you guys have anything related to this in your roadmap? Or would recommend building a custom materialization?",
          "score": 8,
          "created_utc": "2026-02-10 00:36:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tyerj",
              "author": "andersdellosnubes",
              "text": "So, SDF had WAP built into how it was working, and we plan to bring it to dbt in the future! But for now the key priority for us is to make dbt projects run on Fusion without adding too many changes yet to the overall dbt framework.\n\nThere's other tools out there that do more exactly what you're saying.\n\nWe haven't spent too much time in this space. I share your view that DVC or Pachyderm or LakeFS, while great, feel like overkill. My brain has been somewhat sniped by the fact that Apache Iceberg tables have a concept of branching. For me this is very compelling in it's simplicity. In this world you don't need to make a \"view\" rather a \"branch\" of a prod table for local dev.\n\nOrthogonally, we're rather bullish on sampling prod data locally, which is a different pattern than a direct WAP pattern",
              "score": 3,
              "created_utc": "2026-02-11 17:21:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4tz94t",
              "author": "oliverlrj",
              "text": "Hi, may I know how you are implementing write audit publish currently? I am currently in the midst of playing with iceberg, dbt and nessie. Do you use dbt macros to create a new branch for writing of data to the audit branch, or a dbt pre-run hook etc? \n\nWould love to see how people currently do it, thank you!",
              "score": 2,
              "created_utc": "2026-02-11 17:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u1uii",
                  "author": "jtcohen6",
                  "text": "I think there are two different patterns that get lumped into \"WAP\" or \"blue/green\":  \n\\-  (1) for each individual model, first build an intermediate model with the new data, run tests against it (unique, not\\_null, etc) if (and only if) all the tests pass, complete the materialization (swap, merge, ...). if the tests fail - don't; or put the failing rows into \"quarantine\", then keep going...  \n\\- (2) for a group of models, or an entire environment (dev/CI/CD) - create a separate DWH schema (or Iceberg \"branch\"), run all tests, then only if all the tests pass, swap the entire schema/branch into production. this has the advantage of supporting tests that span across multiple models, or allowing sanity-checking / human-in-the-loop verification for final models/metrics at the end of the DAG before re-deploying. this obviously *doesn't* work if the dev/CI/CD environment has a subset of data, or different data from production (scrubbed/anonymized).\n\nI think u/HC-Klown's original question is asking after (1), as a per-model materialization strategy - but just to say, there are pros/cons to either approach, and I don't think we've landed cleanly on which one to pursue (or some combination of both)",
                  "score": 1,
                  "created_utc": "2026-02-11 17:38:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l0vwi",
          "author": "paujas",
          "text": "What are the priority developments dbt-Labs are concentrating on ?\n\nWhat is the long term vision for dbt-core and dbt-cloud?",
          "score": 7,
          "created_utc": "2026-02-10 08:39:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u0au5",
              "author": "dbt-jason",
              "text": "There's a *ton* going on right now! In terms of priorities, here's how I'm thinking about it.  \n\n\n* First and always - ensure that dbt is a great way to do data work for organizations all across the world. You'd be surprised by how much nuance there is in keeping that up. Addressing bugs and community issues, adapters changes from the underlying databases and new releases for things like dbt-utils. I came to dbt from running a data team and more than anything else the question is how can we continue to be a trusted piece of the analytics stack for data teams across the world.\n* Beyond that, I think of the next set of priorities across three primary buckets - the dbt Fusion engine, AI and emerging standard.\n   1. The dbt Fusion engine is a big area of investment for us and we've seen that for projects that are able to successfully adopt, the rust based, strongly typed version of dbt brings a ton of improvements. Things like speed, improved developer experience and the sql comprehension features that power the VS code extension work great. And also - we've seen some challenges in complex projects getting migrated over to Fusion\n   2. AI - I've been obsessed with AI since way before I worked at dbt Labs (fun fact I was an r/futurology mod when it had less than 1000 users). I really believe that AI is going to radically reshape how we do data work - and giving dbt users the tools to do that *right* is very top of mind.\n   3. Emerging standards - Iceberg, arrow and more. We've been watching Iceberg for a few years and now we're starting to see real customers using Iceberg in production for cheaper / more flexible data ingestion, and to use dbt across multiple data platforms",
              "score": 2,
              "created_utc": "2026-02-11 17:30:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rnmxg",
          "author": "OrneryBlood2153",
          "text": "Honestly .. What is going on .. ? \ncore is till getting big fix as releases,\nFusion still in beta,\nCloud was said to go under a single umbrella with five tran,\nOpen engine,\nOpen catalog,\nNow that sqlmesh is also under the same umbrella what happens to the ideas in that tool.\n\nFeels like too much is said but not sure what's the direction of this product for ,\nEnd users - open source and cloud users,\nOpen source contributors - adapter and core ,\ncore vs fusion - what to use going forward.. I don't think both products are going to get equal attention.",
          "score": 8,
          "created_utc": "2026-02-11 08:40:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ky1in",
          "author": "pseudo-logical",
          "text": "Ok, I'll bite, _is_ Fusion going to be GA anytime soon? I feel bad for the devs who have had to keep their foot on the gas for nearly a full year since someone on the e-team decided to launch prematurely.",
          "score": 6,
          "created_utc": "2026-02-10 08:11:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2m80",
              "author": "schemas_sgski",
              "text": "Hey! Really appreciate the interest in Fusion. The team has def been busy building a new engine for dbt that is fast, stable, has parity with the features in dbt Core, and builds on it in important ways. This takes time and we want to make sure we nail it for y'all. More news on Fusion GA coming soon. Stay tuned!",
              "score": 1,
              "created_utc": "2026-02-11 17:41:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iwxwj",
          "author": "Proudly_Funky_Monkey",
          "text": "Is there a better way to manage the lifecycle of parallel pipelines?Â \n\n\nContext: we use DBT core to build features for ml models. New data is ingested weekly, and from it new model features are built through a tree of 30+ very complicated tables. Models are then fed these latest features. We're pretty happy with this.Â \n\n\nBut when we want/need to develope new/different model features, we really struggle with versioning. we only have one database: production. So end up duplicating the entire tree of tables with _version[] appended. The development is then done in the version suffixed tables until eventually it eventually becomes prod and the old tables/definitions are deleted.Â \n\n\nWhy is this bad? Massive PRs, drift between trees during dev, significant risk of manual mistakes, entire tree must be duplicated even for small changes (complexity and cost).\n\n\nCan DBT help with our architecture problems?",
          "score": 4,
          "created_utc": "2026-02-09 23:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1ez7",
              "author": "SpookyScaryFrouze",
              "text": "You could change the generate_schema_name macro in order for the target name to be appended to the schema name.\n\nThat way you would have 2 schemas : ml_models_prod and ml_models_dev, in the same database.",
              "score": 6,
              "created_utc": "2026-02-10 08:44:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r2ls2",
                  "author": "ianitic",
                  "text": "Yup that's what we do at work. There's all contracts and model versioning in dbt as well though we haven't needed the latter yet.",
                  "score": 2,
                  "created_utc": "2026-02-11 05:32:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4j2h0o",
              "author": "fsm_follower",
              "text": "I donâ€™t have a solution for you but have felt this.  Would the ability for dev pipelines to pull almost all the tables from prod then only generate those in your diff be a solution?",
              "score": 3,
              "created_utc": "2026-02-10 00:23:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tytwy",
                  "author": "andersdellosnubes",
                  "text": "interesting. can you say more?",
                  "score": 1,
                  "created_utc": "2026-02-11 17:23:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4orc2h",
              "author": "infomocrat",
              "text": "Are you familiar with model versions?   \n[https://docs.getdbt.com/docs/mesh/govern/model-versions](https://docs.getdbt.com/docs/mesh/govern/model-versions)  \nbest practice guide: [https://docs.getdbt.com/best-practices/how-we-mesh/mesh-6-coordinate-versions](https://docs.getdbt.com/best-practices/how-we-mesh/mesh-6-coordinate-versions)",
              "score": 2,
              "created_utc": "2026-02-10 21:21:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4tyqqa",
              "author": "andersdellosnubes",
              "text": "Yes, dbt should be able to help with that. It is a pretty detailed use case you have so we wonâ€™t be able to go into a lot of details but I feel like a combination of using [dbt clone](https://docs.getdbt.com/reference/commands/clone) and/or [deferral](https://docs.getdbt.com/reference/node-selection/defer) to build only the minimum set of models for a given ML feature branch.  \n  \nThen, my first instinct would also be to try to use different schemas or database to separate the output of the different LM pipelines..\n\n  \nAlso lots of others have contributed ideas that you might find helpful as well",
              "score": 2,
              "created_utc": "2026-02-11 17:23:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4yea8u",
              "author": "Global_Bar1754",
              "text": "Would you potentially be open to Python based solutions (completely foss frameworks) outside of dbt? If so could you share about how big your datasets are? The process you described is a huge operational risk (as you already know and alluded to) and solutions that require any kind of manual version management are gonna be a pain to stay on top of. Thereâ€™s pretty lightweight python framework solutions out there that likely wouldnâ€™t require that.Â ",
              "score": 2,
              "created_utc": "2026-02-12 09:41:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50t0vt",
              "author": "Proudly_Funky_Monkey",
              "text": "Thanks all! I will look into each of these proposals.",
              "score": 1,
              "created_utc": "2026-02-12 18:13:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ng6a3",
          "author": "domscatterbrain",
          "text": "What's the big roadmap for DBT?\n\nYou must know that, while the acquisition shenanigans are good for your team (hopefully) it made many people who's been a long fan of DBT furious.",
          "score": 3,
          "created_utc": "2026-02-10 17:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ua1g1",
              "author": "dbt-jason",
              "text": "Shared some thoughts on [this here](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u1abx/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)!",
              "score": 2,
              "created_utc": "2026-02-11 18:16:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ngrbx",
          "author": "uglylookingguy",
          "text": "Whatâ€™s the biggest mistake teams make when adopting dbt that doesnâ€™t show up until months later in production?",
          "score": 4,
          "created_utc": "2026-02-10 17:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3iam",
              "author": "schemas_sgski",
              "text": "I have seen two extremes.  \n  \nI have seen teams trying to come up with the end to end data pipeline design on paper, without writing any model, and then trying to implement it.  \n  \ndbt is made for agility, so, my experience is that people should write models, iterate, refactor, deprecate, improve etcâ€¦ With CI/CD in place there is not a lot of risk in modifying existing models, not like with platforms not backed by git and version control.  \n  \nThe other extreme is people going a bit wild on the models they build and not focusing on PR reviews and overall coherence. After a few months a dbt project can become complex if not managed enough. What I recommend here is to set some tools like [dbt-projet-evaluator](https://github.com/dbt-labs/dbt-project-evaluator) as soon as possible so that best practices are baked into the project from its inception.",
              "score": 2,
              "created_utc": "2026-02-11 17:45:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u972o",
              "author": "andersdellosnubes",
              "text": "sometimes, as the case in all engineering, just because you can do something that feels smart, doesn't mean that you should.\n\nI'm not so extreme as folks who say\n\n>the best code is the code you don't write\n\nhowever, with great jinja powers comes great responsibility. now with Fusion, some users have had to rethink their pure jinja automation solutions, especially when they mess with the DAG or ask the DWH for these things\n\n",
              "score": 2,
              "created_utc": "2026-02-11 18:12:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4ud52q",
              "author": "andersdellosnubes",
              "text": "some others from colleauges:\n\n>too many jobs and not understanding that a dbt DAG creates dependencies for you. stop doing steps like `dbt build -s model_a`. use selectors and tags instead!\n\nalso\n\n>ensure you've set up your CI/CD environments correctly at the outset and don't do a staging environment unless you really really need to\n\none more\n\n>never have multiple dbt projects that are copies of the same repo called like `project-a-staging` and `project-a-prod` \\-- it's a mess. thank me later\n\n  \nlast. lol\n\n>do not override `ref()` unless you have some batshit crazy insane reason and know what you're doing\n\nhard agree. overriding critical dbt jinja macros connotes great power, but comes with great responsiblitly! \"KISS\" rules the day",
              "score": 2,
              "created_utc": "2026-02-11 18:30:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o5oy4",
          "author": "Adrien0623",
          "text": "When will the fix for dbt-redshift connector will come so that materialized view refresh stops failing randomly?",
          "score": 4,
          "created_utc": "2026-02-10 19:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tzzko",
              "author": "Longjumping-Pin-3235",
              "text": "dbt just issues the DDL, right?",
              "score": 2,
              "created_utc": "2026-02-11 17:29:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u03gj",
              "author": "dbt-zeeshan",
              "text": "Hey I see a couple that seem related [\\#1256](https://github.com/dbt-labs/dbt-adapters/issues/1256) and [\\#1499](https://github.com/dbt-labs/dbt-adapters/issues/1499). If you share the link we'll definitely take a look!",
              "score": 1,
              "created_utc": "2026-02-11 17:29:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4oj2su",
          "author": "uncertainschrodinger",
          "text": "What are some new trends, ideas, competitors, and schools of thought that you find to be the biggest threat to the principles and roadmap of dbt?",
          "score": 4,
          "created_utc": "2026-02-10 20:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u8n48",
              "author": "dbt-jason",
              "text": "So dbt arose in popularity because of a very specific set of technological circumstances. Cloud data warehouses allowed you to load more data into yuor warehouse, but there wasn't a great way to manage this. Compute was cheap, storage was cheap, but getting useful, structured answers was challenging and difficult.  \n  \nThat technological paradigm isn't going anywhere, but it is going to be changed *a lot* by AI. Something that I'm thinking a lot about is how to make \"the dbt lifestyle\" remains relevant in the AI era.  \n  \nFor example, we saw an interesting article published [this week](https://substack.com/@groupby1/p-187428984) claiming that the \"Modern Data Stack\" worked great for humans, but there are other tools that will perform better for agents. I think it's important to take an honest look here - the experiment run in this wasn't very rigorous from what I can tell, but it might be pointing at something real.  \n  \nBut I really do believe we have an interesting role to play in the AI world. For example - last year we shipped the [dbt MCP server](https://github.com/dbt-labs/dbt-mcp) and we just released [dbt agent skills](https://github.com/dbt-labs/dbt-agent-skills) but the thing that's really exciting to me here is some testing we've been doing around how Fusion (because it's faster, has more metadata awareness and is more strongly typed) can be the best of both worlds- a great experience for human developers and powering agentic workloads.",
              "score": 1,
              "created_utc": "2026-02-11 18:09:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4k3rdf",
          "author": "ActEfficient5022",
          "text": "What would you say you do here?",
          "score": 7,
          "created_utc": "2026-02-10 04:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tkp7y",
              "author": "turboDividend",
              "text": "i mean, it can build depencys which would be useful in some use cases. ive alway been kind of skeptical of this tool though",
              "score": 3,
              "created_utc": "2026-02-11 16:17:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4r2udj",
              "author": "ianitic",
              "text": "I have people skills and I'm good at dealing with people so the engineers don't have to.",
              "score": 2,
              "created_utc": "2026-02-11 05:34:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u5sa4",
              "author": "andersdellosnubes",
              "text": "~~Well, I generally come in at least fifteen minutes late, ah, I use the side door - that way Lumbergh can't see me, heh heh - and, uh, after that I just sorta space out for about an hour~~.\n\nWell, I generally I ship Fusion Diaries a few days late, I work with Slack notifications off - that way u/dbt-jason can't ping me, heh heh - and, uh, after that I just sorta refresh Hacker News until links about data show up\n\nalternatively phrased: Developer Experience Advocacy!",
              "score": 1,
              "created_utc": "2026-02-11 17:56:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kkib8",
          "author": "Ramshizzle",
          "text": "Do you have any plans for improving how dbt deployment pipelines are configured, build and run?\n\nAt my company we are brand new to dbt-cloud. So far I really love dbt and all its capabilities. There is one issue I'm having, which is the only way it seems we can create an ETL pipelines from dbt-cloud is to manually click together a 'deployment job'.\n\nI am already experimenting with dbt-jobs-as-code, but while that is a great tool, it seems like that is still in early development.\n\nAt this moment we are considering getting an outside scheduler/orchestration tool. Which would be a shame in my opinion.",
          "score": 3,
          "created_utc": "2026-02-10 06:09:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4oruyl",
              "author": "infomocrat",
              "text": "terraform? [https://registry.terraform.io/providers/dbt-labs/dbtcloud/latest/docs](https://registry.terraform.io/providers/dbt-labs/dbtcloud/latest/docs)\n\nTalk: [https://www.getdbt.com/resources/coalesce-on-demand/coalesce-2024-why-analytics-engineering-and-devops-go-hand-in-hand](https://www.getdbt.com/resources/coalesce-on-demand/coalesce-2024-why-analytics-engineering-and-devops-go-hand-in-hand)",
              "score": 2,
              "created_utc": "2026-02-10 21:23:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u8jbq",
              "author": "schemas_sgski",
              "text": "as an orchestration nerd, I love this question - and welcome to dbt cloud! This is an area of dbt that we spend a lot of time thinking about, and one exciting feature we have recently shipped is [state-aware orchestration](https://docs.getdbt.com/docs/deploy/state-aware-about). It lets you build and run based on whatâ€™s actually changed (using state comparison and deferral), which can make pipelines a lot more efficient, especially as your project grows. There are also some [advanced configs](https://docs.getdbt.com/docs/deploy/state-aware-setup#advanced-configurations) that you can set up to customize at a more granular level and specify what actually gets run. This does require fusion. If you can share what you are looking for when scheduling jobs I'm happy to share some other functionality of the dbt orchestrator.",
              "score": 1,
              "created_utc": "2026-02-11 18:09:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m6uf5",
          "author": "Existing_Wealth6142",
          "text": "What is your perspective on open table formats like Iceberg,  Delta Lake, and Hudi? If you are positive on the technology, what do you think are the most important features still lacking from either the formats or dbt in order to maximize their value?",
          "score": 3,
          "created_utc": "2026-02-10 14:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u7ktd",
              "author": "andersdellosnubes",
              "text": "omg what a nerd snipe of a question!\n\nfor me what i'm most looking forward too are features that make it so that it just works for us analytics engineers! also my white whale forever has been a \"multi-engine stack\" with one iceberg catalog but heterogenous query engines.\n\nwhat's missing? support for the Iceberg Rest Catalog is still spotty amongst vendors, and the IRC itself needs some improvement (performance & federated, user-level authentication).\n\nif Iceberg and other formats are working as they should, we as analytics engineers should hardly ever have to thing of them!\n\nI also answered more in [this answer](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u3x71/) to u/Longjumping-Pin-3235",
              "score": 2,
              "created_utc": "2026-02-11 18:04:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ucvfm",
                  "author": "Existing_Wealth6142",
                  "text": "thanks for the response! A multi engine stack is also on my wishlist. Do you have a thought on which catalog best facilitates that? I've been wondering about either R2 or Lakekeeper as they seem like they might be credibly neutral",
                  "score": 2,
                  "created_utc": "2026-02-11 18:29:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lsn8q",
          "author": "GarpA13",
          "text": "Are you planning to abandon dbt core? What is your vision for on-premise software?",
          "score": 5,
          "created_utc": "2026-02-10 12:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u6yk6",
              "author": "dbt-grace",
              "text": "**TLDR;** No, we are actively shipping new features to dbt Core! \n\nWrote up a longer response [here](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u5986/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) :)",
              "score": 2,
              "created_utc": "2026-02-11 18:01:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n1bel",
          "author": "Ready-Marionberry-90",
          "text": "What problem do you exist to solve?",
          "score": 4,
          "created_utc": "2026-02-10 16:35:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u130q",
              "author": "andersdellosnubes",
              "text": "juicy! the thing that drew me to dbt originally was Tristan (founder/CEO) answering this question on the [Software Engineering Podcast: dbt](https://softwareengineeringdaily.com/2021/09/28/dbt-data-build-tool-with-tristan-handy-2/)\n\ndbt exists to solve the problem that data analysts don't have a career path beyond:  \n\"manage more dashboards\" or \"manage more people\".\n\nimho, dbt exists to solve that socio-technical problem and others common amongst data practitioners",
              "score": 1,
              "created_utc": "2026-02-11 17:34:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ju1xp",
          "author": "superconductiveKyle",
          "text": "Hey dbt Crew, I'm Kyle Eaton, Head of Growth at Agno (https://github.com/agno-agi/agno). I've been a big fan of your product for a long time now and specifically the community you all created. I'm the Former Head of Growth at Great Expectations, so I've been following for a long time.\n\nWe recently asked some of our users who we should partner with, and dbt came up. Seeing your AMA reminded me about this. Would love the chat with you all about what a partnership with Agno and dbt could look like! \n\nHope you have fun with your AMA! ",
          "score": 2,
          "created_utc": "2026-02-10 03:03:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u609j",
              "author": "andersdellosnubes",
              "text": "hey u/superconductiveKyle IIRC our paths have crossed before, at least i've been in GE slack for many years. feel free to reach out on LinkedIn would love to say hi and talk shop",
              "score": 2,
              "created_utc": "2026-02-11 17:57:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ubj2z",
                  "author": "superconductiveKyle",
                  "text": "Hello! That's awesome. Just sent a connect via LinkedIn. ",
                  "score": 2,
                  "created_utc": "2026-02-11 18:23:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lhwd6",
          "author": "finally_i_found_one",
          "text": "RemindMe! 4 days",
          "score": 2,
          "created_utc": "2026-02-10 11:19:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4li1k1",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 4 days on [**2026-02-14 11:19:40 UTC**](http://www.wolframalpha.com/input/?i=2026-02-14%2011:19:40%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/o4lhwd6/?context=3)\n\n[**2 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2F1r0ff3b%2Fama_were_dbt_labs_ask_us_anything%2Fo4lhwd6%2F%5D%0A%0ARemindMe%21%202026-02-14%2011%3A19%3A40%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201r0ff3b)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 2,
              "created_utc": "2026-02-10 11:20:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lim0t",
          "author": "set92",
          "text": "I'm deciding in moving my code from Airflow to Dbt using Cosmos. The idea is that instead of having custom sql code with jinja, I can move everything to dbt and let it run everything. \n\nI do this to improve in logging/debugging, and easiness. I suppose the speed/easiness is going to be there. But not sure about the logging part. Does dbt returns the output of queries, or is something that we can modify or specify in our own?",
          "score": 2,
          "created_utc": "2026-02-10 11:25:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ofuk2",
              "author": "Comfortable-Power175",
              "text": "Have you looked into dbt elementary? This is how we monitor model runs. FWIW we have an airflow + dbt + cosmos with elementary setup and are extremely happy",
              "score": 2,
              "created_utc": "2026-02-10 20:27:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4oqz3a",
                  "author": "set92",
                  "text": "Thanks, I'll check it. Although in general I was thinking more in the outputs itself. Like we have Snowflake, and the Operator of Airflow doesn't show you the results of the query by default, and in Snowflake itself only the owner of the query can check the results for 24h only. So, I thought maybe with dbt I could log this, like for example when I do COPY INTOs be able to see if a file has failed, and other information that comes on that results table, but I suppose the best will be to find some free moment and test it myself.",
                  "score": 2,
                  "created_utc": "2026-02-10 21:19:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u8xq8",
              "author": "dbt-grace",
              "text": "Hey! I would point you to a few different places: \n\n* [log jinja function](https://docs.getdbt.com/reference/dbt-jinja-functions/log) for logging custom messages\n* [exceptions](https://docs.getdbt.com/reference/dbt-jinja-functions/exceptions#warn) for raising warnings/errors\n* [dbt show](https://docs.getdbt.com/reference/commands/show) command for displaying a preview of results from a query\n* [documentation on events and logs](https://docs.getdbt.com/reference/events-logging)",
              "score": 2,
              "created_utc": "2026-02-11 18:11:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ualfm",
              "author": "andersdellosnubes",
              "text": "interesting! what are you looking for w.r.t. logging? Are you saying you to log the result of queries? Most folks don't, preferring to keep the data where it is.\n\n  \nI'm not sure if you've looked much into dbt Fusion, but it's [telemetry](https://docs.getdbt.com/docs/fusion/telemetry) is based on OTel, quite robust and just as extensible as dbt Core.\n\nlike u/Comfortable-Power175 said, there's also great dbt packages that help with logging and monitoring",
              "score": 1,
              "created_utc": "2026-02-11 18:18:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uwxqi",
                  "author": "set92",
                  "text": "Anything that could help me debug ETLs when something fail. When something fails I like to see what it ran, and what was the output of everything in case I can detect where it failed. I would not be able to understand everything, but seeing that a file of X customer didn't get ingested, or that X parameter was not set as I thought could help me debug the problems faster. \n\nBut yep, in Snowflake they told me that the results of queries were only visible for 24h by the owner of the query, no one else, not even an accountadmin could see the results. Which for me seems madness, and completely different than, if I'm not wrong, BQ, where anyone can see all the queries run, and that let you see/analyze any query and see what was processed, or didn't run as intended.\n\nFor copy into maybe the list of files, or be able to filter and only get the ones that failed to ingest, and trigger an alarm with it.\n\nAnother example could be when I run a MERGE on Snowsight (UI of Snowflake) it returns me the inserted and merged cols I think it is. I built some custom code to retrieve those values to sql variable and store them in a logging table. But it would be good if the tool has the functionality on its own. \n\nOr there is a query where I retrieve the max date of the ingested data, store it in a sql variable, and use it later. From what @dbt-grace said, I would be able to use [log jinja functions](https://docs.getdbt.com/reference/dbt-jinja-functions/log) to print those to the log, that way if we have a problem in the future I will be able to see which data we processed.",
                  "score": 2,
                  "created_utc": "2026-02-11 20:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4q9uw0",
          "author": "infomocrat",
          "text": "What have been your favorite Coalesce talks of all time?\n\nIf you could wish any talk into existence for a future Coalesce (ok, dbt Summit) what would it be?",
          "score": 2,
          "created_utc": "2026-02-11 02:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ubgz8",
              "author": "dbt-grace",
              "text": "I love it when community members give talks sharing their custom solutions! It's very inspiring to see what people are doing in the wild, and (selfishly) helpful when building out our roadmap for dbt Core. [Mariah Rodger's talk on testing](https://www.youtube.com/watch?v=hxvVhmhWRJA) is one of the many community projects that motivated us to build out-of-the-box support for unit testing into dbt :)",
              "score": 2,
              "created_utc": "2026-02-11 18:22:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u1kaf",
              "author": "dbt-zeeshan",
              "text": "Definitely u/dbt-grace [live-coding UDFs](https://youtu.be/aMUAQjqTKtc?si=vN4H-TEBf4EHpntw&t=1130) while roller blading!",
              "score": 1,
              "created_utc": "2026-02-11 17:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ub9ov",
              "author": "andersdellosnubes",
              "text": "The Magic School Bus themed talk with u/dbt-grace about dbt Mesh  \n[https://www.youtube.com/watch?v=FAsY0Qx8EyU](https://www.youtube.com/watch?v=FAsY0Qx8EyU)",
              "score": 1,
              "created_utc": "2026-02-11 18:21:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tuemk",
          "author": "Longjumping-Pin-3235",
          "text": "Iceberg  \nShould it be the default for a new project or greenfield snowflake environment?\n\nIs the complexity worth it?\n\nWhat catalog would you reach for for the most compatible multi-engine read / write?",
          "score": 2,
          "created_utc": "2026-02-11 17:03:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3x71",
              "author": "andersdellosnubes",
              "text": "This feels like the most important question re: Iceberg, huh?\n\nThere's lots of opinions here but my take: Iceberg isn't a free lunch.  \n  \nYou need a business case and the pros need to outweigh the drawbacks. This was actually discussed as well in [a recent episode ](https://www.getdbt.com/blog/apache-iceberg-and-the-catalog-layer)of the Analytics Engineering podcast. There's another episode due out soon where Tristan and I discuss the future of Iceberg.  \n  \nIceberg brings you the flexibility around where your data is stored and what compute you pick, but it adds complexity in having to manage an iceberg catalog. There are a few iceberg catalogs out there but many of them support â€œpartâ€ of the iceberg spec, and finding who supports what is not super easy.  \n  \nIf you are all-in on a data platform and wonâ€™t need cross data warehouse compute my take is that Iceberg would not be worth the effort today.\n\nHowever my prevailing opinion is that the Iceberg project is to data what standard-sized ship containers do for global trade. It's not necessarily \"exciting\" per se, but it has undeniable impacts on end users in that it's easier to get your ~~goods~~ data from point A to point B",
              "score": 1,
              "created_utc": "2026-02-11 17:47:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zi31j",
                  "author": "Longjumping-Pin-3235",
                  "text": "Thanks for the thoughtful reply u/andersdellosnubes !",
                  "score": 2,
                  "created_utc": "2026-02-12 14:29:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tujdn",
          "author": "infomocrat",
          "text": "With fusion and state aware orchestration, a lot of issues around multi-project orchestration can be solved. What problems do you see that \\*still exist\\* to be solved for more complex implementations of dbt involving multiple interconnected projects, and what are some strategies you see for addressing them?",
          "score": 2,
          "created_utc": "2026-02-11 17:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u5588",
              "author": "andersdellosnubes",
              "text": "short-answer: yes!\n\ncan you share what complex scenarios you've seen that you think can be addressed?",
              "score": 1,
              "created_utc": "2026-02-11 17:53:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4u97qc",
                  "author": "jtcohen6",
                  "text": "short-answer: seconded!\n\nthe challenges I keep hearing about are more \"socio-technical\" - improvements to dbt framework / platform / related tooling can help, but probably aren't sufficient in a vacuum:  \n\\- managing and discovering data products across multiple teams  \n\\- communicating data quality and freshness data contracts for public dbt models.  \n\\- sharing reusable macro code in (private) packages, with (semantic? calendar?) versioning for breaking changes to those macros (which are still untyped in dbt...)  \n\\- supporting multi-project deployments across \\*multiple data platforms\\* (think: upstream project on Databricks / downstream project on Snowflake, with a common Iceberg catalog across both)  \n\\- model-level access controls (more granular than project-level access)  \n\\- ... lots of other things I'm sure I'm not thinking/hearing about ...",
                  "score": 1,
                  "created_utc": "2026-02-11 18:12:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tyszp",
          "author": "Longjumping-Pin-3235",
          "text": "Did u/dbt-grace watch How To Trick People Into Thinking You can Juggle?  \n[https://youtube.com/shorts/trKLrl6y\\_Vc?si=Y9zMa2dQMt\\_ZCgU0](https://youtube.com/shorts/trKLrl6y_Vc?si=Y9zMa2dQMt_ZCgU0)",
          "score": 2,
          "created_utc": "2026-02-11 17:23:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u24fq",
              "author": "dbt-grace",
              "text": "LOL - you can actually thank my high school drama teacher for that one ([from deep in the archives](https://imgur.com/QakqCAT))",
              "score": 3,
              "created_utc": "2026-02-11 17:39:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mrppy",
          "author": "sleeper_must_awaken",
          "text": "How will you prevent companies being charged $350 / seat / month for an enterprise DBT Cloud subscription of three years when they need more than 8 seats? Â This is a real story Iâ€™ve seen at two of our clients. That amount is just inordinate and betrays the trust companies and consultancies have placed in your DBT Labs.\n\nFor me, DBT Labs needs to regain my trust. Iâ€™ve recommended many companies to use DBT Core and DBT Cloud since 2019, but am hesitant to continue recommending it.\n\nCompare that pricing with GitHub, which also has runners, incident management, traceability, collaboration, auditing and IdP integration (among many other things).",
          "score": 5,
          "created_utc": "2026-02-10 15:50:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kq035",
          "author": "Tall_Working_2146",
          "text": "Your certifications are interesting but the price is high, are there no initiative to make learning resources more accessible for students and people abroad?",
          "score": 2,
          "created_utc": "2026-02-10 06:56:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4oqywm",
              "author": "infomocrat",
              "text": "The learning resources are all free: [https://learn.getdbt.com/catalog](https://learn.getdbt.com/catalog)   \nCertifications are half off if you take them at coalesce, also partners can get discounts. But certifications are really only important in certain situations (for partners, if employer requires it to prove your knowledge). I wouldn't worry about the certs unless you need it. If you want to prove your knowledge, put up a personal project on your github.",
              "score": 2,
              "created_utc": "2026-02-10 21:19:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4uo2rq",
              "author": "[deleted]",
              "text": "Truly agree with others: ignore certs and just take the learning resources cause as others have said, they're all free!!",
              "score": 2,
              "created_utc": "2026-02-11 19:21:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o8pge",
          "author": "Ok-Sentence-8542",
          "text": "Why did you abandon dbt core and the open source community?",
          "score": 2,
          "created_utc": "2026-02-10 19:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u5986",
              "author": "dbt-grace",
              "text": "Hi! This is Grace (Product Manager of dbt Core). Can you say more about why you feel like dbt core and the open source community has been abandoned? This is the area of our product I think about all day, so Iâ€™m both sorry to hear that you feel a sense of abandonment and very open to feedback on how my team can revive your confidence that we remain dedicated to both dbt core and the open source community.  \n  \nI am proud of our two big releases of dbt Core last year (`v1.10` and `v1.11`) that included some features Iâ€™ve personally been wanting to bring to life for a longtime now ([sample mode](https://docs.getdbt.com/docs/build/sample-flag) and [user-defined functions](https://docs.getdbt.com/docs/build/udfs), to name two). And, weâ€™ve added even more codebases to the â€œopen sourceâ€ bucket - we just re-licensed [metricflow under Apache 2](https://www.getdbt.com/blog/open-source-metricflow-governed-metrics), and Fusion includes some [net-new Apache 2 code](https://www.getdbt.com/blog/new-code-new-license-understanding-the-new-license-for-the-dbt-fusion-engine).  \n  \nIn case you missed it, we also put out a [roadmap update in December](https://github.com/dbt-labs/dbt-core/blob/main/docs/roadmap/2025-12-magic-to-do.md) in the dbt-core repo! Iâ€™d encourage you to give it and read, and let me know your thoughts.\n\nWhen I have heard this type of sentiment from other community members, itâ€™s often coming from a feeling of frustration that we havenâ€™t been as responsive to issues and pull requests in our open source repos over the past year. I feel that frustration as well and am working hard to get us back on track! Candidly, our team has been split across two focuses this past year: helping to build the new dbt Fusion engine and evolving the dbt framework across both Core and Fusion. Iâ€™m very excited to be onboarding a lot of new team members to dbt Core (this month!) to revitalize our activity in our open source repos and continue to execute on the dbt Core roadmap.",
              "score": 1,
              "created_utc": "2026-02-11 17:54:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ync46",
                  "author": "Ok-Sentence-8542",
                  "text": "Hi Grace, thank you for the transparent response. I truly appreciate the work your team is doing on features like UDFs and Sample Mode.\n\nHowever, I want to be candid: many of us have seen this pattern in the industry before, and weâ€™ve experienced it with dbt Labs specifically. It was the same with the MetricFlow server. While the logic was open, the essential infrastructure to run it effectively was kept behind a paywall.\n\nThis creates a sense that dbt Core is effectively being placed in maintenance modeâ€”receiving incremental updatesâ€”while the 'real' future and performance leaps, like the Rust-based dbt Fusion engine, are gated under the ELv2 license.\n\nTo a developer, it feels like dbt Core is the 'bait' to get us into the ecosystem, but the 'magic' that makes the tool 2026-ready is no longer truly open. If the most transformative innovations continue to happen outside of the Apache 2.0 scope, itâ€™s hard to feel that dbt Core remains a first-class citizen rather than just a legacy entry point for dbt Cloud.",
                  "score": 3,
                  "created_utc": "2026-02-12 11:06:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tqp2d",
          "author": "J0hnDutt00n",
          "text": "Will there ever be more dynamic ways of orchestrating jobs in Cloud?",
          "score": 1,
          "created_utc": "2026-02-11 16:45:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4twpgq",
              "author": "andersdellosnubes",
              "text": "my interest is piqued -- can you share more about what you mean by \"dynamic\"?",
              "score": 1,
              "created_utc": "2026-02-11 17:13:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uh9cn",
                  "author": "J0hnDutt00n",
                  "text": "Sorry for the vague question, scenario: build fails on one model that succeeded yesterday and employ a WAP strategy. Will there ever be the ability to automatically re-execute a build but with that stale data model from yesterday to unblock those downstream models with an *asterisk of stale prod data model used? This already is essentially capable for child mesh projects but not others.",
                  "score": 2,
                  "created_utc": "2026-02-11 18:49:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tvna4",
          "author": "infomocrat",
          "text": "What do you think is the most under-appreciated feature of dbt core?",
          "score": 1,
          "created_utc": "2026-02-11 17:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tz803",
              "author": "andersdellosnubes",
              "text": "dbt seed! csv's (while ugly) aren't going away for anytime soon! when i started in data a decade ago. \"getting data in the database\" was like half the coding work to be done in my Jupyter notebook. \\`dbt seed\\` makes it so easy",
              "score": 1,
              "created_utc": "2026-02-11 17:25:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4ucd7m",
              "author": "dbt-grace",
              "text": "I am a big fan of all of the ways you can customize your dbt project to meet your org's specialized needs - specifically, I've seen a lot of creative problem-solving using [custom materializations](https://docs.getdbt.com/guides/create-new-materializations?step=1)!",
              "score": 1,
              "created_utc": "2026-02-11 18:26:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0fky1",
      "title": "Visualizing full warehouse schemas is useless, so I built an ERD tool that only renders the tables you're working on",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/706ayxj8yiig1.gif",
      "author": "Spiritual_Ganache453",
      "created_utc": "2026-02-09 20:29:55",
      "score": 135,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0fky1/visualizing_full_warehouse_schemas_is_useless_so/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4lr57a",
          "author": "ThroughTheWire",
          "text": "Pretty cool! think it could be useful for presentations for sure. maybe decent as a documentation tool as well.. I definitely do not want to ever manually draw or create UML tables ever again lol",
          "score": 7,
          "created_utc": "2026-02-10 12:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4odebs",
          "author": "DungKhuc",
          "text": "This looks very neat!\n\nOne thing though, besides showing relationship when pointing to a foreign key, what else does this do?",
          "score": 3,
          "created_utc": "2026-02-10 20:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ozeep",
              "author": "Spiritual_Ganache453",
              "text": "Yeah, here's the list.\n\n**Primary:**\n\n* Real-time SQL Canvas synchronization\n* MySQL parser with full syntax support\n* Drag-to-connect foreign key creation\n* Inline column editing on canvas\n* Drag-and-drop column reordering (swap/insert modes)\n* Customizable auto-formatting (indentation, casing)\n* Toggleable Brainstorm mode\n* Instant bulk rendering\n* Project & snapshots creation\n* Version history timeline\n\n**Upcoming:**\n\n* Cardinality support\n* Auto-generated SQL queries on connection hover\n* Diagram-wide search with auto-framing\n* Visual diff mode for team collaboration\n\nMore info: [sqlestev.com](http://sqlestev.com)",
              "score": 3,
              "created_utc": "2026-02-10 21:58:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o52v4nr",
          "author": "DoomsdayMcDoom",
          "text": "What library did you use for the canvas and diagrams?",
          "score": 1,
          "created_utc": "2026-02-13 00:23:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o536opn",
              "author": "Spiritual_Ganache453",
              "text": "Reactflow",
              "score": 1,
              "created_utc": "2026-02-13 01:32:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2pumc",
      "title": "I built a website to centralize articles, events and podcasts about data",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/0ovtcb4jc0jg1.png",
      "author": "alphter",
      "created_utc": "2026-02-12 10:20:28",
      "score": 131,
      "num_comments": 16,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2pumc/i_built_a_website_to_centralize_articles_events/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o50du5l",
          "author": "sparkplug49",
          "text": "Could we get an rss feed so I can pull this into my feed reader?",
          "score": 16,
          "created_utc": "2026-02-12 17:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o517aj7",
              "author": "szrotowyprogramista",
              "text": "Echoing this. An RSS feed would be really nice.",
              "score": 5,
              "created_utc": "2026-02-12 19:20:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54wna5",
              "author": "alphter",
              "text": "It's in the backlog but I need to improve some other stuff first to be able to build a super clean RSS feed. Stay tuned!",
              "score": 2,
              "created_utc": "2026-02-13 09:20:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53arfq",
          "author": "michael-day",
          "text": "Love the name and branding",
          "score": 2,
          "created_utc": "2026-02-13 01:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53vf1h",
          "author": "OpinionSad2896",
          "text": "There is an include filter. Would be great if we have an exclude list too",
          "score": 2,
          "created_utc": "2026-02-13 04:08:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54wq5g",
              "author": "alphter",
              "text": "Yes, I need to find a proper way to do this :)",
              "score": 1,
              "created_utc": "2026-02-13 09:20:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54vike",
          "author": "Narrow-Tower58",
          "text": "Super cool! I am writing a blog about data topics - thanks for making my research 10x faster :D",
          "score": 2,
          "created_utc": "2026-02-13 09:09:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z15fg",
          "author": "hamisphere",
          "text": "wow that looks so cool! thanks for sharing",
          "score": 1,
          "created_utc": "2026-02-12 12:52:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o506uzh",
          "author": "Far-Criticism928",
          "text": "how do you source articles? ",
          "score": 1,
          "created_utc": "2026-02-12 16:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o508ssl",
              "author": "alphter",
              "text": "hi! it's a combination of multiple stuff but mostly i'm using RSS feeds coupled with AI post-processing to generate summaries and tagging.",
              "score": 3,
              "created_utc": "2026-02-12 16:39:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50q1ag",
                  "author": "decrementsf",
                  "text": "moltbook is spreading. Claude is in the room with us right now.",
                  "score": -1,
                  "created_utc": "2026-02-12 17:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50dc8g",
          "author": "evanazz",
          "text": "Super cool! I was planning on doing something like this with an LLM layer on top that to help me find topics to research and write about. Is a direction you'd be interested taking this in? I'm happy to hook that up & fund it, ofc.Â ",
          "score": 1,
          "created_utc": "2026-02-12 17:00:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50j4m1",
          "author": "digitalghost-dev",
          "text": "Great idea!",
          "score": 1,
          "created_utc": "2026-02-12 17:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z363m",
          "author": "BicycleSignal6557",
          "text": "i'll definitely take a look !",
          "score": 1,
          "created_utc": "2026-02-12 13:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zu73i",
          "author": "Thinker_Assignment",
          "text": "Finally someone did it!",
          "score": 1,
          "created_utc": "2026-02-12 15:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5014i0",
          "author": "studentofarkad",
          "text": "This is amazing!! Thank you",
          "score": 1,
          "created_utc": "2026-02-12 16:03:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3a05s",
      "title": "Has anyone read Oâ€™Reillyâ€™s Data Engineering Design Patterns?",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/2uu32wxil5jg1.jpeg",
      "author": "xean333",
      "created_utc": "2026-02-13 00:07:17",
      "score": 131,
      "num_comments": 36,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r3a05s/has_anyone_read_oreillys_data_engineering_design/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54wkr9",
          "author": "minato3421",
          "text": "Yeah I went through the book. Felt pretty trivial to be honest. But I have an experience of 7 years in this field. So, nothing in that book felt new. It is worth reading for beginners though",
          "score": 77,
          "created_utc": "2026-02-13 09:19:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55p5uz",
              "author": "kaumaron",
              "text": "I feel like that's many books these days",
              "score": 19,
              "created_utc": "2026-02-13 13:09:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o56xyk6",
              "author": "tylerriccio8",
              "text": "Do you recommend anything more advanced? I have multiple yoe, not really looking for basic patterns",
              "score": 3,
              "created_utc": "2026-02-13 16:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59b3l5",
                  "author": "Character-Education3",
                  "text": "Probably books more focused on architecture and your business domain",
                  "score": 1,
                  "created_utc": "2026-02-14 00:09:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56rkjc",
              "author": "Thespck",
              "text": "What would you recommend to a junior data engineer? I find CGPT very useful when I ask to help me improve a pipeline or to teach me fundamentals or whatâ€™s best and why not other ways. However, I learnt about slow changing dimensions by reading Designing Data Intensive Applications by Martin Kleppmann (also Oâ€™Reilly)",
              "score": 5,
              "created_utc": "2026-02-13 16:26:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o554lj9",
          "author": "Kobosil",
          "text": "liked the code examples\n\none of the better books in my opinion",
          "score": 15,
          "created_utc": "2026-02-13 10:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5501f5",
          "author": "phizero2",
          "text": "Yeah, ok book. Isnt the best but worth  checking.",
          "score": 15,
          "created_utc": "2026-02-13 09:52:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55791p",
              "author": "dadadawe",
              "text": "Which one is the best?",
              "score": 17,
              "created_utc": "2026-02-13 10:58:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55fj3a",
                  "author": "PutridSmegma",
                  "text": "Designing data-intensive applications from Klepmann",
                  "score": 31,
                  "created_utc": "2026-02-13 12:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54wnwm",
          "author": "putokaos",
          "text": "Absolutely. It's a fantastic book full of not just practical advice, but also the proper way of solving the most common scenarios. I'd recommend it to any data engineer.",
          "score": 9,
          "created_utc": "2026-02-13 09:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54zw8j",
          "author": "Astherol",
          "text": "Good book",
          "score": 9,
          "created_utc": "2026-02-13 09:51:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55be37",
          "author": "SoggyGrayDuck",
          "text": "Anyone have a great book/link on medallion architecture? I get it but I feel like it's essentially \"let agile define your model\" and id like to read a good resource on it.",
          "score": 3,
          "created_utc": "2026-02-13 11:33:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55ptcr",
              "author": "TechnologySimilar794",
              "text": "Building medalion architecture by Piethein Stengholt ",
              "score": 5,
              "created_utc": "2026-02-13 13:13:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55sydw",
                  "author": "SoggyGrayDuck",
                  "text": "Can you answer one question, does medallion architecture target spark based workflows? The big thing I'm trying to get straight in my head is where do traditional data models come into play. Some say they're not used anymore and others say that's what their silver layer is and yet others say it's the gold layer. I have a feeling it's being wedged into situations it doesn't actually work for. Or they don't really understand and are just updating the terms they use based on what they read or see.",
                  "score": 3,
                  "created_utc": "2026-02-13 13:31:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o55s8vi",
                  "author": "SoggyGrayDuck",
                  "text": "Thank you, looking it up/ordering",
                  "score": 1,
                  "created_utc": "2026-02-13 13:27:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o56bpft",
                  "author": "TheOneWhoSendsLetter",
                  "text": "Recommended.",
                  "score": 1,
                  "created_utc": "2026-02-13 15:10:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56c367",
              "author": "TheOneWhoSendsLetter",
              "text": "Besides Stengholt, *Data Lakes for Dummies* by Alan Simon",
              "score": 2,
              "created_utc": "2026-02-13 15:12:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57om0t",
          "author": "Awkward-Cupcake6219",
          "text": "Good book, especially for mid level engineers. If you have around 5+ good quality YOE it could fill some gaps.\n\nMore than that? I guess it is nice to have it on the shelf for a quick look, but honestly you could \"have quick look\" on the internet too as I expect you to know what questions to ask at this point.",
          "score": 3,
          "created_utc": "2026-02-13 19:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57v1lu",
              "author": "xean333",
              "text": "That was about my assessment as well. Iâ€™m at a decade plus at this point so Iâ€™ll probably skip it",
              "score": 1,
              "created_utc": "2026-02-13 19:36:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o559dei",
          "author": "Firm-Requirement1085",
          "text": "Just started chapter 2 and the small code examples are using spark, should I learn the basics of spark before continuing?",
          "score": 4,
          "created_utc": "2026-02-13 11:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55f83q",
              "author": "BrunoLuigi",
              "text": "Do you know python?",
              "score": 5,
              "created_utc": "2026-02-13 12:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55ih90",
                  "author": "Firm-Requirement1085",
                  "text": "Yes I use python-polars for ingestion/standardizing csv files but the company I'm at uses snowflake so haven't touch spark",
                  "score": 1,
                  "created_utc": "2026-02-13 12:26:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56b7zx",
              "author": "TheOneWhoSendsLetter",
              "text": "Because of the book? No need to. The solutions there are language-agnostic.",
              "score": 1,
              "created_utc": "2026-02-13 15:07:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55td0s",
          "author": "Salfiiii",
          "text": "The book itself is a nice reference but nothing I would consider reading through thoroughly.\n\nSkim over the concepts and come back to it if you ever need it.\n\nNothing revolutionary though, if you have couple years on your back you probably heard of > 90% already.",
          "score": 2,
          "created_utc": "2026-02-13 13:33:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56b2re",
          "author": "TheOneWhoSendsLetter",
          "text": "It's a very good book. You'll find value in the situations and problems addressed and the way of thinking and solutions' caveats that it exposes.",
          "score": 2,
          "created_utc": "2026-02-13 15:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o571ac2",
          "author": "ruibranco",
          "text": "ddia for the concepts, this one for the copy-paste recipes - they complement each other more than people think",
          "score": 2,
          "created_utc": "2026-02-13 17:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o578h6j",
          "author": "pacopac25",
          "text": "I want to buy the book solely because the fish's clenched teeth, frowning, and thousand-mile-stare eyes accurately represent how I feel when I read the Spark documentation.",
          "score": 2,
          "created_utc": "2026-02-13 17:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o553wkd",
          "author": "Ok_Appearance3584",
          "text": "Excellent reference book",
          "score": 3,
          "created_utc": "2026-02-13 10:28:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56r5ib",
          "author": "gman1023",
          "text": "I really enjoyed it, has practical problems and patterns one would need in data engineering. like someone said, one of the better books. \n\n\n\nyou can get it for free here (that's how i got it):  \n[Data Engineering Design Patterns](https://buf.build/resources/data-engineering-design-patterns)",
          "score": 1,
          "created_utc": "2026-02-13 16:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59e1fg",
              "author": "Interesting_Strain90",
              "text": "This never worked, i tried three different emails.",
              "score": 1,
              "created_utc": "2026-02-14 00:27:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59w26j",
                  "author": "LoaderD",
                  "text": "Are they company emails? Usually these companies don't let you sign up with a random email because they use this as a way to generate sales leads. \n\nIf you don't have a job and therefore, no company email, there are better books to get started that you should get before this book.",
                  "score": 1,
                  "created_utc": "2026-02-14 02:20:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r15015",
      "title": "2026 State of Data Engineering Report - 1000+ responses from data engineers",
      "subreddit": "dataengineering",
      "url": "https://www.linkedin.com/posts/josephreis_recently-i-surveyed-1101-of-you-about-the-share-7426990778536583168-fqMr/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAajovEBZaTvKT0qIqHq9ItYb5C1EMVsVSY",
      "author": "DungKhuc",
      "created_utc": "2026-02-10 16:12:59",
      "score": 127,
      "num_comments": 8,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r15015/2026_state_of_data_engineering_report_1000/",
      "domain": "linkedin.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4qlhfo",
          "author": "dreyybaba",
          "text": "This is a lovely insight. Thanks for putting this together",
          "score": 8,
          "created_utc": "2026-02-11 03:31:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r3ata",
              "author": "DungKhuc",
              "text": "All credits go to Joe Reis, so I'm not putting anything together except cross posting the links from his discord channel :)",
              "score": 8,
              "created_utc": "2026-02-11 05:38:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rvfui",
          "author": "rmoff",
          "text": "super useful. love the fully-functioning enterprise version too ðŸ¤£ https://joereis.github.io/super_corporate_pdm_survey/\n\n(Crystal reports, anyone?)",
          "score": 8,
          "created_utc": "2026-02-11 09:54:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s7av0",
              "author": "flashpoints80",
              "text": "He really went the extra mile!",
              "score": 3,
              "created_utc": "2026-02-11 11:38:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4x3kq9",
              "author": "shoppedpixels",
              "text": "I like the duplicate or erroneous data like Data Analyst listed twice or misspellings persisting, gives a feel of authenticity.",
              "score": 1,
              "created_utc": "2026-02-12 03:10:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sxjor",
          "author": "ironmagnesiumzinc",
          "text": "AI helping only 29% of respondents with debugging was extremely surprising. I think this is maybe the best part about LLMs",
          "score": 3,
          "created_utc": "2026-02-11 14:24:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xmn4r",
              "author": "Oxford89",
              "text": "It is THE most common use I have for AI. I don't even bother reading logs anymore. I just feed them to AI and get an answer faster than I would be able to scroll to the error.",
              "score": 2,
              "created_utc": "2026-02-12 05:24:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4scz0g",
          "author": "observability_geek",
          "text": "I'm so surprised only to see that only 6.8% are using EDA and the big gap between enterprise usage and SMBs. \n\n# \n\n",
          "score": 0,
          "created_utc": "2026-02-11 12:21:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qym03a",
      "title": "Coinbase Data Tech Stack",
      "subreddit": "dataengineering",
      "url": "https://www.junaideffendi.com/p/coinbase-data-tech-stack",
      "author": "mjfnd",
      "created_utc": "2026-02-07 18:52:11",
      "score": 88,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1qym03a/coinbase_data_tech_stack/",
      "domain": "junaideffendi.com",
      "is_self": false,
      "comments": [
        {
          "id": "o45yw4e",
          "author": "Relative-Cucumber770",
          "text": "Might be a rookie question, but: What's the point of using Snowflake for warehousing if they're already using Databricks (Unity Catalog)? ",
          "score": 31,
          "created_utc": "2026-02-07 23:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45zmr3",
              "author": "mjfnd",
              "text": "Multiple teams owning different stacks or in the middle of migration which could take years.\n\n\nI can resonate with their stack as we also used DBX for processing core pipelines and BI related workflows on Snowflake linked to Tableau.",
              "score": 14,
              "created_utc": "2026-02-07 23:44:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o48ybve",
              "author": "a_lic96",
              "text": "DiversificaciÃ³n, Risk hedging, avoiding full vendor lock-in, as well as to have more contractual power during negotiations",
              "score": 4,
              "created_utc": "2026-02-08 13:24:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o47xgiy",
          "author": "PeitersSloppyBallz",
          "text": "Technology bingo much?",
          "score": 6,
          "created_utc": "2026-02-08 08:01:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45oflq",
          "author": "joeblk73",
          "text": "If you are on AWS why use Looker a GCP product ?",
          "score": 5,
          "created_utc": "2026-02-07 22:36:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45r700",
              "author": "halfrightface",
              "text": "looker core vs studio. studio is what google data studio used to be and probably what you're thinking of. they're using core as a semantic layer on top of snowflake to leverage lookml to build their views/explores.",
              "score": 11,
              "created_utc": "2026-02-07 22:52:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o46d75u",
              "author": "Vautlo",
              "text": "Depending on the needs of the organization, Looker can beat Quicksight in a lot of ways. I think the value is in the modelling/semantic layer, governance, and being git native/BI as code. \n\nI've been through a migration from Tableau to Looker, as well as standing up and maintaining a self hosted Looker instance, both at AWS shops. Quicksight wasn't really considered as an option for either project - one was in the public sector and they put a lot of value on the governance baked into Looker, and the other was scared off of anything primarily UI driven and really valued the idea of BI as code.\n\nThe public sector project was pre-acquisition. I don't recall the costs from back then, but I'd bet that it was less of a factor than today.\n\nQuicksight is way less expensive, though I still doubt I'd choose it if I was the first data hire at a standup today. There are just too many no contract/free options to create decent reports that would satisfy a startup for quite a while.",
              "score": 3,
              "created_utc": "2026-02-08 01:06:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o46ftwb",
                  "author": "joeblk73",
                  "text": "What does modelling and semantic layer mean here ?",
                  "score": 1,
                  "created_utc": "2026-02-08 01:22:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45u6te",
              "author": "mjfnd",
              "text": "I think this is very common, the main reason is Looker is great and popular and it used to be a standalone product, not sure if that's true now, can we just buy looker instead of onboarding to GCP?\n\nWe also had Looker with AWS Stack.",
              "score": 2,
              "created_utc": "2026-02-07 23:09:49",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o45ou0v",
              "author": "data4u",
              "text": "I was wondering the same",
              "score": 1,
              "created_utc": "2026-02-07 22:38:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4576qi",
          "author": "theath5",
          "text": "Do you know if they use dbt for transformations?",
          "score": 2,
          "created_utc": "2026-02-07 21:03:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o458ejf",
              "author": "mjfnd",
              "text": "I couldn't find any mention of DBT publicly, let me know if you have any insights.",
              "score": 3,
              "created_utc": "2026-02-07 21:09:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o467bki",
                  "author": "ActEfficient5022",
                  "text": "I would have to assume databricks provides transformations I don't see what dbt would add to that given the diagram",
                  "score": 4,
                  "created_utc": "2026-02-08 00:30:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48006f",
          "author": "No_Airline_8073",
          "text": "Databricks and Snowflake and Starrocks and Looker and Airflow as well. Lot of redundancy.\nWhy not just use Databricks scheduler and warehouse and get rid of snowflake and airflow. I can understand why looker over Databricks-redash and maybe starrocks for few things",
          "score": 3,
          "created_utc": "2026-02-08 08:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4phuig",
              "author": "alittletooraph3000",
              "text": "Maybe someone who works for CB can chime in here but if they're using multiple compute platforms, seems pretty unlikely that they'd migrate off an orchestrator that's neutral to everything. ",
              "score": 1,
              "created_utc": "2026-02-10 23:35:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1yl3v",
      "title": "Hired as a data engineer in a startup but being used only for building analytics dashboards, how do i pivot",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r1yl3v/hired_as_a_data_engineer_in_a_startup_but_being/",
      "author": "aks-786",
      "created_utc": "2026-02-11 14:18:44",
      "score": 79,
      "num_comments": 34,
      "upvote_ratio": 0.93,
      "text": "Am a solo Data Engineer at a startup. I was hired to build infrastructure and pipelines, but leadership doesn't value anything they can't \"see.\"\n\nI spend 100% of my time churning out ad-hoc dashboards that get used once and forgotten. Meanwhile, the AI team is getting all the praise and attention, even though my work supports them. Also, i think they can now build rdbms in such a way that DE work would not be required in sometime\n\nRight now, I feel like a glorified Excel support desk. How do I convince leadership to let me actually do Engineering work, or is this a lost cause and look for switch?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r1yl3v/hired_as_a_data_engineer_in_a_startup_but_being/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4symz4",
          "author": "codykonior",
          "text": "> Also, i think they can now build rdbms in such a way that DE work would not be required in sometime. \n\nUhhh. Yeah. Sure. Any day now for the past 3 decades.",
          "score": 69,
          "created_utc": "2026-02-11 14:29:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4t9ipo",
              "author": "aks-786",
              "text": "But what if data size is low ðŸ¥². Do they need columnar database for this?",
              "score": -12,
              "created_utc": "2026-02-11 15:25:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4u97sy",
                  "author": "Subject_Fix2471",
                  "text": "data size and data complexity are separate, can have a \"low\" amount of data that's complex enough to greatly benefit from a relational db. ",
                  "score": 23,
                  "created_utc": "2026-02-11 18:12:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4x2k24",
                  "author": "IndependentTrouble62",
                  "text": "A well modeled database is like a tailored tux. Its never out of style.",
                  "score": 11,
                  "created_utc": "2026-02-12 03:04:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4xxmaj",
                  "author": "sib_n",
                  "text": "The explanation is that this AI team is doing the data engineering for their need, not that there is no DE. It's possible that they would be thankful for someone to do it for them, maybe you can try asking them. If this doesn't work and you can't find DE work, go somewhere else. But don't neglect the fact that building analytics dashboard is great experience for a DE. It is usually the main downstream usage of a DE's work and it is common to ask DEs to do dashboarding, especially in small structures.",
                  "score": 2,
                  "created_utc": "2026-02-12 06:59:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tlske",
          "author": "ianitic",
          "text": "Doing more of the infrastructure and pipelines would likely make the praise and attention situation worse. That was how it was when I was at a small company.\n\nReport builder got the largest raises due to visibility. I built the pipelines, infrastructure, and ml models. I could build reports too but just didn't have the time. The discrepancy got so bad that by the time I left the report builder had double my salary.\n\nThe only way to fix it is to leave.",
          "score": 47,
          "created_utc": "2026-02-11 16:22:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tuano",
              "author": "aks-786",
              "text": "Okay I see. Thanks for the feedback",
              "score": 7,
              "created_utc": "2026-02-11 17:02:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4ymuu1",
              "author": "bamboo-farm",
              "text": "No. The way to fix is to do less. \n\nDE is the sweetest job. \n\nItâ€™s only the worst because we do too much",
              "score": 4,
              "created_utc": "2026-02-12 11:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54om3d",
                  "author": "Afedzi",
                  "text": "I agree",
                  "score": 1,
                  "created_utc": "2026-02-13 08:04:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wsnbn",
              "author": "OkMaize9773",
              "text": "Stop giving quality data to the report builder and enjoy the show",
              "score": 5,
              "created_utc": "2026-02-12 02:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4x00ou",
                  "author": "ianitic",
                  "text": "I'm not there anymore, left for greener pastures lol",
                  "score": 3,
                  "created_utc": "2026-02-12 02:48:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sxzha",
          "author": "Key_Post9255",
          "text": "Look somewhere else",
          "score": 88,
          "created_utc": "2026-02-11 14:26:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tkpu0",
              "author": "mrbartuss",
              "text": "Easier said than done in the current job market",
              "score": 44,
              "created_utc": "2026-02-11 16:17:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w7lov",
                  "author": "Key_Post9255",
                  "text": "He's tagged as the plumber. \n\nUnluckily for him there is no one above him to give him value or \"defend\" him in front of the leadership.\n\nHe would have to find someone to sponsor him or put him in a different situation, but it seems no one is going to do that. It doesn't matter how beautiful the data will be, what tools he uses, whatever else..he is just the nerd in the back doing the PC stuff. No one understands what he's doing. Other people get the merit. Get out if you can because you'll get nothing from this company ðŸ˜€",
                  "score": 14,
                  "created_utc": "2026-02-11 23:59:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ue5s9",
          "author": "randomuser1231234",
          "text": "If youâ€™re writing dashboards that arenâ€™t being used, that means youâ€™re working on someoneâ€™s â€œoh Iâ€™m curiousâ€ questions and not addressing the actual business needs.\n\nLearn how your company works, how it actually makes money, what the market differentiators are. Get cozy with the engineering managers and the finance manager and learn what they give a shit about. Make data artifacts and dashboards that answer THOSE questions.",
          "score": 18,
          "created_utc": "2026-02-11 18:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4to7na",
          "author": "Yuki100Percent",
          "text": "I'm also a solo data person at a startup. First off, if you're at a decent company/team, when your work is related to somebody's that got attention, they would or should mention your work too. What you just said kinda indicates a not-so-good-culture.   \n  \nAnyway, I don't know how much tasks or tickets you get a day/week, you should start asking project priorities or start building one yourself and propose it to the exec team. And how they impact the business for the better. This also helps you become more visible if you've been just working with other folks in the company, but not with exec team. Not sure who you report to, but asking these kinds of questions to your direct report also helps. If not, I might start looking for a new place to work for.   \n  \nAlso, realistically what you can do is to start building infra and pipelines along with what you're doing. You satisfy the current needs and start working on things you think are important or necessary to do what they ask you to do (reporting, dashboarding). \n\nOverall, I'd try to communicate a lot more and see what they say. And depending on that, you either start looking for a new job or decide to do things differently going forward. \n\nDM me if you have any other questions! ",
          "score": 12,
          "created_utc": "2026-02-11 16:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vosuz",
              "author": "blue_leader27",
              "text": "hey Iâ€™m also in your position can I dm you",
              "score": 2,
              "created_utc": "2026-02-11 22:18:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w3m7n",
                  "author": "Yuki100Percent",
                  "text": "Yup feel free",
                  "score": 1,
                  "created_utc": "2026-02-11 23:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tui8g",
              "author": "aks-786",
              "text": "Thanks for the feedback. I will reach out to you in DM once I think about this",
              "score": 1,
              "created_utc": "2026-02-11 17:03:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uz1fb",
          "author": "cky_stew",
          "text": "When you say the AI team is piggybacking on your work, that would sound like youâ€™re doing some data engineering in order to feed their models/LLMs, but if thatâ€™s not the case- then how does that situation look?\n\nHow do these ad-hoc dashboards look? Are you just writing one off queries on raw data to populate them? In my experience ad hoc reports are a symptom of a lack of good models (and pipelines that build those models) going into your BI tools. If thatâ€™s the case itâ€™s a pretty easy sell to your line manager that building a proper pipeline would mean dashboards get built faster and have way more reusability.\n\nIf youâ€™re the solo data engineer, then youâ€™re the sole authority and the only one who can explain why it is a problem. Have you raised this with them? If youâ€™re just complying with the requests of things they think they want to see, then theyâ€™re gunna be stuck in a loop of forcing through ad hoc things - because itâ€™s all they know.\n\nYou mention being an excel support desk too - this definitely shouldnâ€™t be happening - spreadsheets can be avoided in almost all cases these days (with some exceptions). I LOVE it when someone requests a spreadsheet because itâ€™s an opportunity for me to ask them â€œout of curiosityâ€ what are they doing with the spreadsheet - then you almost always get given an opportunity to solve a problem that they didnâ€™t even know existed, this can make people very happy and thatâ€™s the most satisfying part of this job, I find.\n\nMaybe Iâ€™ve got the completely wrong take here but it sounds like the company hasnâ€™t been exposed to a modern data stack before and are doing things the old way, if youâ€™ve already shown them how it could be better (time optimisation, data reliability, and undiscovered insights being the selling points that execs hear) - then fair enough leave - if you havenâ€™t youâ€™re sitting on a golden opportunity, cause it sounds like youâ€™re the only authority. Best wishes going forward - but this sounds like youâ€™re not doing data engineering at all and would be a red flag to me if I were to interview you for an engineering role and you spoke about this sort of setup.",
          "score": 5,
          "created_utc": "2026-02-11 20:13:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w1f65",
          "author": "SaintTimothy",
          "text": "My job since forever has been to try and automate myself out of a job. I keep creating automation, pr enhancing old ones, and they keep bringing up more and more stuff that needs it.\n\n\nYou making the donuts every day manually is the opposite of automation. Taking what, of that, can be automated is your job. Either they recognize you did a good job, or you still do a good job and only work one hour a day and surf reddit the other 7. OR you keep manually making the donuts every day because that's fun.",
          "score": 5,
          "created_utc": "2026-02-11 23:24:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4up0n4",
          "author": "sciencewarrior",
          "text": "One possible opportunity is tooling. See where the AI team is spending time with manual tasks and propose ways to simplify their workflow.",
          "score": 2,
          "created_utc": "2026-02-11 19:25:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4won63",
          "author": "rdmcoloring",
          "text": "Transition to AI engineer, just do whatever you were doing and just add a chatgpt API call in between",
          "score": 2,
          "created_utc": "2026-02-12 01:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xdtj7",
          "author": "MindlessTime",
          "text": "Find a way to be necessary. By necessary I mean without your knowledge something important would fail in a costly way. Maybe you simplify data pulls for the accounting team and without you they canâ€™t create financial statements. Maybe you create and maintain the data that goes into the CRM, without which all the marketing campaigns would fail. Even if leadership doesnâ€™t â€œseeâ€ the work, someone will say â€œIâ€™m screwed without this personâ€ and youâ€™ll be fine.",
          "score": 2,
          "created_utc": "2026-02-12 04:18:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yfw11",
          "author": "oscarmch",
          "text": "What you're lacking is marketing.\n\nData Engineering is always backend, it always goes unnoticed. My only advice is that you don't use tools to get a CV. \n\nPlan to use tools according to Business' needs",
          "score": 2,
          "created_utc": "2026-02-12 09:57:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ymo9f",
          "author": "bamboo-farm",
          "text": "This is easy. \n\nThey wonâ€™t see you unless youâ€™re solving pain. \n\nYou need to find the balance between them feeling some pain and doing your job. \n\nIf they donâ€™t feel any pain, youâ€™re doing too much as youâ€™re not solving anything. \n\nIf above doesnâ€™t work, coast, learn some shitz and move. \n\nWorlds your oyster.",
          "score": 2,
          "created_utc": "2026-02-12 11:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z4kql",
          "author": "Oddly_Energy",
          "text": "> How do I pivot?\n\nPun intended?",
          "score": 2,
          "created_utc": "2026-02-12 13:14:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u3x8p",
          "author": "Firm_Communication99",
          "text": "Do data science stuffâ€” r2 , basic stats",
          "score": 1,
          "created_utc": "2026-02-11 17:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vsm0s",
          "author": "Eleventhousand",
          "text": "Talk to your boss in the one in one and ask to be given more DE work .\n\n\nBe nice about it though, don't go in there trashing other data related work like you did in here.",
          "score": 1,
          "created_utc": "2026-02-11 22:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vxfsc",
          "author": "HeyNiceOneGuy",
          "text": "How are ad-hoc dashboards that only get used once supporting your AI team?",
          "score": 1,
          "created_utc": "2026-02-11 23:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51exyl",
          "author": "shropshireladwales",
          "text": "I would say for now our jobs as data engineers if to get info to people, if you can do that with what you have happy days otherwise make the case and link the fact that you canâ€™t make the dashboard without the good pipeline",
          "score": 1,
          "created_utc": "2026-02-12 19:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54oz5u",
          "author": "Afedzi",
          "text": "Sad to hearâ€¦",
          "score": 1,
          "created_utc": "2026-02-13 08:07:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v3iia",
          "author": "Accomplished-Row7524",
          "text": "SELECT ...\nFROM ...\n   PIVOT ( <aggregate_function> ( <pivot_column> ) [ [ AS ] <alias> ]\n            FOR <value_column> IN (\n              <pivot_value_1> [ [ AS ] <alias> ] [ , <pivot_value_2> [ [ AS ] <alias> ] ... ]\n              | ANY [ ORDER BY ... ]\n              | <subquery>\n            )\n            [ DEFAULT ON NULL (<value>) ]\n         )\n\n[ ... ]",
          "score": 0,
          "created_utc": "2026-02-11 20:35:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2wvxg",
      "title": "Being pushed out of job, trying to plan next steps",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/",
      "author": "octacon100",
      "created_utc": "2026-02-12 15:46:59",
      "score": 71,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "First post for a while, hope this is ok. Spent roughly 5 years at my current job, all with excellent reviews each year, survived the last round of layoffs, had my performance review which basically said don't make any thing and start putting process in place while the ceo just looked at me in disgust. So I'm thinking I'm pretty much on the way out as the company is planning to buy software that makes what I'm doing irrelevant (Has its own data warehouse, it's own way of loading data, etc).\n\nOur company is currently all on prem for work, so a big shared drive is our datalake, sql server is our database, and the best I've been able to do to improve/modernize things was to introduce Prefect for our orchestration, make my own libraries in python to make loading data easier, show the usages of PowerBI and Tableau and create a data warehouse that did what the company wanted to do, but now has decided was a waste of time.\n\nI've started go through the AWS Data Engineering Exam and Snowflake exams, and I have projects on Github that show the use of Amazon S3, Athena, and Glue, so I can at least point to those and say I have cloud experience that I've set up myself. I've been applying to jobs, but I usually get stopped where they are looking for cloud experience. \n\nI've been working with data for almost 20 years now, so I'm hoping my experience can help in terms of getting a job. Does anyone have any advice out there for how to get an in on cloud experience or what places look for with cloud experience? Would the certifications be enough?\n\nAny help is greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o50agi9",
          "author": "snarleyWhisper",
          "text": "Hey there , I had a similar thing happen to me a few years ago. Our whole department was outsourced and then fired unceremoniously. I did some courses on snowflake , databricks and fabric they all have free training and tiers. I would get to the end of the rounds with an interview and usually they would go with someone who had more direct experience with their exact stack. Itâ€™s frustrating but ultimately I ended up landing somewhere that the tooling was less important since Iâ€™m setting a lot of it up. But generally if you focus on one of the top data platforms thatâ€™s your main top of funnel filter.",
          "score": 24,
          "created_utc": "2026-02-12 16:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52k3oh",
              "author": "octacon100",
              "text": "Yeah Iâ€™ve had the same experience, make it to interviews do ok, get rejected for someone that has the experience. Good idea to go for jobs where there are starting to make the move.",
              "score": 3,
              "created_utc": "2026-02-12 23:20:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54je8a",
          "author": "viniciusjooj",
          "text": "You donâ€™t sound underqualified, you sound mis-positioned. 20 years in data isnâ€™t erased because the stack changed. Before stacking more certs, Iâ€™d get really clear on how you want to position yourself. A structured strengths/work-style assessment (CareerExplorer, Pigment, etc.) can actually help frame your narrative. Are you a systems architect? a data reliability person? a modernization lead? That clarity matters in interviews. Certs help, but story + positioning is what gets you past the cloud filter.",
          "score": 21,
          "created_utc": "2026-02-13 07:16:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o511l9o",
          "author": "Historical-Fudge6991",
          "text": "Honestly the cloud transition isnâ€™t too bad. Iâ€™d recommend John Savill on YT. I find the biggest hurdles are RBAC (thankful for a good IT team) and understanding solutions. Thereâ€™s 50 ways to make a record with cloud but the core DE principles will always apply. You could checkout Databricks if you want to leverage your python xp",
          "score": 7,
          "created_utc": "2026-02-12 18:53:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52ksfi",
              "author": "octacon100",
              "text": "Havenâ€™t heard of John Savill, Iâ€™ll have to check him out. Thanks. Yeah IAM has been a whole thing for sure. Even close code has issues with that.",
              "score": 1,
              "created_utc": "2026-02-12 23:24:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50c5ss",
          "author": "rotr0102",
          "text": "If you decide to spend some time learning cloud keep in mind some vendors offer free trials, and then you can restart them with different throw away email accounts. So, you could start up a snowflake trial with dbt trial and build some models. Just save everything locally so you can rebuild the environment quickly when you need to restart your free trial.",
          "score": 13,
          "created_utc": "2026-02-12 16:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52o7qk",
          "author": "DoomsdayMcDoom",
          "text": "Job market is tough for us older guys.  I was in your shoes at a point in time.  Thatâ€™s when I started my own consulting company and havenâ€™t looked back since.  I gained the cloud experience I was lacking as I gained more clients.  Now I just run the business and let the younger guys worry about learning the next and greatest tech.",
          "score": 5,
          "created_utc": "2026-02-12 23:43:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52pnjp",
              "author": "octacon100",
              "text": "I did try a bit of that, getting the first clients is tough. Tried linked in with local firms, have previous people Iâ€™ve worked with that might be looking for people. Any hints on how to get your first client? Thatâ€™s where Iâ€™m getting stuck. Upwork seems like spending money on a slot machine.",
              "score": 1,
              "created_utc": "2026-02-12 23:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o53c81g",
                  "author": "DoomsdayMcDoom",
                  "text": "My first gig I worked with a local recruiter and asked to go corp to corp.  Then I started to attend start-up networking events and started getting word of mouth from clients.",
                  "score": 2,
                  "created_utc": "2026-02-13 02:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51z5w1",
          "author": "cky_stew",
          "text": "I mean dude youâ€™ve kinda been playing on hard mode - as long as you donâ€™t get a case of â€œweâ€™ve-always-done-it-this-way-itisâ€ then you will understand the concepts, problems, and risks of bad system design as it all applies in the cloud too. Iâ€™d focus on the concepts of orchestrators, transformation tools, and OLAP dbs, rather than try to get direct experience with any particular set of tools - stuff tends to be a bit mix and match sometimes. If you need to do something to get it on your resume just to get past the recruiters then go for some certifications - but you should be fine in an interview with someone who knows the deal - youâ€™ll probably enjoy the cloud compared to on prem - you get so much more done itâ€™s great!",
          "score": 2,
          "created_utc": "2026-02-12 21:33:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52jtuf",
              "author": "octacon100",
              "text": "Thanks, it helps to read this.",
              "score": 1,
              "created_utc": "2026-02-12 23:19:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o55hig1",
          "author": "Sufficient_Example30",
          "text": "The best thing to do is ,\nStudy while on the job and do the minimum to get by and keep applying.\nJump at the first chance",
          "score": 2,
          "created_utc": "2026-02-13 12:20:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56ab3v",
          "author": "aMare83",
          "text": "I genuinly think cloud experience is a bit overrated. Ãt the end of the day there you also have storage, computation, databases and semi or unstructured file formats.\n\nI don't think you could not pick up the level of cloud knowledge you need in 2 weeks. Your database design, pipeline creation and orchestration experience worth gold comparing to young guys who can click here and there on AWS or Azure UI.\n\nIf I was an employer I would give you the chance for sure.",
          "score": 2,
          "created_utc": "2026-02-13 15:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5262qu",
          "author": "redditreader2020",
          "text": "Snowflake has great free training and docs.",
          "score": 1,
          "created_utc": "2026-02-12 22:06:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0rt35",
      "title": "Are people actually use AI in data ingestions? Looking for practical ideas",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r0rt35/are_people_actually_use_ai_in_data_ingestions/",
      "author": "[deleted]",
      "created_utc": "2026-02-10 05:03:57",
      "score": 59,
      "num_comments": 27,
      "upvote_ratio": 0.9,
      "text": "Hi All,  \n\n\nI have a degree in Data Science and am working as a Data Engineer (Azure Databricks)  \n\n\nI was wondering if there are any practical use cases for me to implement AI in my day to day tasks. My degree taught us mostly ML, since it was a few years ago. I am new to AI and was wondering how I should go about this? Happy to answer any questions that'll help you guys guide me better. \n\nThank you redditors :)",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0rt35/are_people_actually_use_ai_in_data_ingestions/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4kv87x",
          "author": "SharpRule4025",
          "text": "The biggest practical win right now is using LLMs to extract structured data from unstructured web sources. Scrape a product page, get back clean JSON with price, description, specs fields instead of maintaining brittle CSS selector pipelines that break every time the source site changes a div class.\n\nAlso useful for classifying and routing incoming data during ingestion - deciding which pipeline a document goes through based on content type rather than hardcoded rules.\n\nFor Databricks specifically, you could experiment with running smaller models to do schema inference on messy source data before it hits your bronze layer. Saves a lot of manual mapping work.",
          "score": 83,
          "created_utc": "2026-02-10 07:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l6x25",
              "author": "pceimpulsive",
              "text": "I would use the AI to generate the CSS selector pipeline.\n\nOnce you get an error reading you can re-run the CSS selector generator.\n\nThis way you don't burn tokens like crazy, and you get higher performance too!",
              "score": 30,
              "created_utc": "2026-02-10 09:38:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lfyi5",
              "author": "Ultimate_Foreigner",
              "text": "For data ingestion with AI, getting back clean JSON from a web page can be tricky and easily break but using [Pydantic AI](https://ai.pydantic.dev/) would likely help here - basically data validation for LLM responses with auto retries etc.\n\nFor any use case other than web scraping, I donâ€™t really think it is worth trying to wedge in any LLM steps here. Data integration is really a solved problem that would only be hindered by adding in superfluous AI tooling.",
              "score": 6,
              "created_utc": "2026-02-10 11:02:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lpq9e",
              "author": "tadtoad",
              "text": "This is brilliant! I need to crawl a page where the html changes frequently enough to make traversing the page a nightmare because of the daily monitoring. I think this LLM JSON output would work for me. Thanks for sharing!",
              "score": 3,
              "created_utc": "2026-02-10 12:20:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kyet9",
          "author": "drag8800",
          "text": "honestly the biggest win for us has been using LLMs during validation. not type checking, but catching semantic weirdness that rules miss. like when a field is technically valid but contains \"N/A\" or \"TBD\" or \"pending\" and those all mean different things downstream. having an LLM tag those during ingestion saves so much debugging later.\n\nother thing that's been useful is throwing sample records at an LLM when you inherit a data source with garbage documentation. \"what do these fields probably mean and what types should they be\" gets you 80% there way faster than playing detective.\n\nfor actual pipeline dev i've been using claude code to scaffold ingestion jobs. not shipping the code directly but it's good at recognizing patterns for common sources like REST APIs or SFTP drops. still review everything but cuts initial dev time.\n\nwhat hasn't worked: trying to be clever with dynamic schema evolution. sometimes you want the pipeline to fail loudly when something breaks, not silently adapt and cause problems downstream.\n\nif you're on databricks, check out unity catalog's AI stuff for metadata enrichment. more governance side but still useful.",
          "score": 16,
          "created_utc": "2026-02-10 08:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lefc5",
          "author": "Which_Roof5176",
          "text": "Yep, people use â€œAIâ€ in ingestion, but mostly around the pipeline, not inside it: schema mapping, data quality checks, log/alert summarization, and writing connector/ETL code faster.",
          "score": 5,
          "created_utc": "2026-02-10 10:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qjm4c",
              "author": "GAZ082",
              "text": "mmmh, how you would use it for data quality without sharing the actual data?",
              "score": 1,
              "created_utc": "2026-02-11 03:19:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lqe14",
          "author": "tadtoad",
          "text": "I use LLMs for classification/tagging. A stage in my pipeline requires classification of the ingested data into one of 100 categories. I send the category list and the content and get by the right category. It barely costs anything.",
          "score": 4,
          "created_utc": "2026-02-10 12:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r4ayg",
              "author": "Desperate_Pumpkin168",
              "text": "Could you please elaborate on how you have set up llm to do this",
              "score": 1,
              "created_utc": "2026-02-11 05:46:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tl98i",
                  "author": "tadtoad",
                  "text": "Itâ€™s pretty straightforward. I have a huge list of product names in my database that are not categorized. I pull each product name, add it to my prompt (along with a list of categories), then send it to OpenAIâ€™s api. It then returns the right category from my list, which I then store in my database.",
                  "score": 2,
                  "created_utc": "2026-02-11 16:20:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l72ii",
          "author": "pceimpulsive",
          "text": "Just hell naww to me.\n\nI want my data ingestions to be very fast and have as little dependencies as possible, I also don't want to them to change when openAI changes their guardrails or guts their model a little more to save costs ....",
          "score": 7,
          "created_utc": "2026-02-10 09:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lzgx5",
              "author": "Skullclownlol",
              "text": "> I want my data ingestions to be very fast and have as little dependencies as possible, I also don't want to them to change when openAI changes their guardrails or guts their model a little more to save costs ....\n\nExactly the same here. Ingestion = source copy, no transformations.",
              "score": 1,
              "created_utc": "2026-02-10 13:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4odhna",
                  "author": "pceimpulsive",
                  "text": "I do ELT,\n\nSmall transforms via uoserts.\n\nE.g. my source system stores timestamps as epoch and a few fields are ints that I want as enumerated strings. I achieve this via a view in a staging layer in the destination DB.\n\nOutside that though... It's copy copy",
                  "score": 1,
                  "created_utc": "2026-02-10 20:16:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n1hn0",
          "author": "mckey86",
          "text": "I guess U can use automation",
          "score": 2,
          "created_utc": "2026-02-10 16:36:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l984h",
          "author": "DungKhuc",
          "text": "I'm using AI to ingest news that's relevant to the user profile from different news feeds. LLM is used to transform the news into signals (in JSON format) for UI to consume.",
          "score": 1,
          "created_utc": "2026-02-10 10:01:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lijg7",
          "author": "Nearby_Fix_8613",
          "text": "Heading our data science and ml dept\n\nIts a blessing and a curse for us",
          "score": 1,
          "created_utc": "2026-02-10 11:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lsw2m",
          "author": "reditandfirgetit",
          "text": "Data analysis. Using AI to find fast answers or confirm your theories. For example, a properly trained model could help catch fraud",
          "score": 1,
          "created_utc": "2026-02-10 12:42:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m96v8",
          "author": "ppsaoda",
          "text": "I'm working on medical datasets. And it's messy with clinical notes, so we have developed in-house LLM model to classify diagnosis. Other than that, not much except helping to write code based on my ideas.",
          "score": 1,
          "created_utc": "2026-02-10 14:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rtwi9",
              "author": "dillanthumous",
              "text": "How do you deal with data loss and hallucinations. Sounds extremely high risk.",
              "score": 1,
              "created_utc": "2026-02-11 09:40:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4satzc",
                  "author": "ppsaoda",
                  "text": "We have dedicated staffs to validate.",
                  "score": 1,
                  "created_utc": "2026-02-11 12:05:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mcjuk",
          "author": "share_insights",
          "text": "Great conversation. For those training models (even toy models) and looking for ways to make money off of their hard work, we'd love to chat. We believe (read: know) there is a market for the intelligence encapsulated in the code.",
          "score": 1,
          "created_utc": "2026-02-10 14:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kn3yl",
          "author": "Prestigious-Bath8022",
          "text": "Depends what you call AI.\n\n",
          "score": 1,
          "created_utc": "2026-02-10 06:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ldkj9",
          "author": "Reach_Reclaimer",
          "text": "Unless it's for actually scraping data, there's no reason to use it over a traditional source as far as I'm aware. Would be more expensive for little gain and no ability to troubleshoot",
          "score": 1,
          "created_utc": "2026-02-10 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ki7ze",
          "author": "Thinker_Assignment",
          "text": "I'm co-founder of an oss ingestion library so I can give you some community observations \n\nFirst, everyone uses LLMs for coding at this time, some do it completely by chat interface. We support them with tools to do so with less bad consequences, and faster.\n\nSecond, there's a small group of people that does a lot of ingestion from unstructured sources like multimodal and social media, or in document heavy industries. Those folks do an order of magnitude more ingestion than the rest of the community combined - so the LLM data processing use cases far outweigh normal data engineering in data engineering work at this time.\n\nOn the other hand we're moving towards complete agentic coding, Wes recently said python is going to no longer be coded by humans but agents. So maybe learn in that direction. Check out skills, they are the latest thing that works well.",
          "score": -8,
          "created_utc": "2026-02-10 05:50:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzswfm",
      "title": "are we a dime a dozen?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1qzswfm/are_we_a_dime_a_dozen/",
      "author": "turboDividend",
      "created_utc": "2026-02-09 03:08:12",
      "score": 57,
      "num_comments": 39,
      "upvote_ratio": 0.93,
      "text": "hearing alot of complaining on the cscareers subreddit and one comment that stuck out was that the OP was a front end guy and one of the responders said being a react/node.js guy isnt special. sometimes i feel the same way about being an  etl guy who does alot of sql.....",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1qzswfm/are_we_a_dime_a_dozen/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4exg5o",
          "author": "Tender_Figs",
          "text": "I left corporate finance for analytics and then data engineering across a 15-year time frame. I feel more of a commodity now than I did when I was a staff accountant unless I really focus on using my domain experience. It is also why I am trying to formally get out of data engineering. \n\nI'm in a group that is constantly obsessed about tooling while paying no attention to technical debt or scale, nor the purpose of what we do in the first place. It feels very far removed from any \"so what?\" in my opinion, and yes, I do understand what the \"so what?\" is.",
          "score": 70,
          "created_utc": "2026-02-09 11:02:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fabxw",
              "author": "A_Poor_Economist",
              "text": "I read an article once that said there are 2 types of IT. IT that's just tech nerds and IT that is tied to Finance.\n\nTech nerds tend to get obsessed with shiny new objects. Finance tied ones tended to weigh tradeoffs and more \"so what?\"\n\nA gross overgeneralization but there seems to be tech people obsessed with just the tech and people who can do more strategic thinking about what to use when and why. \n\nOf course if you're on an island that sucks. Been there.\n\nWhat are you trying to transition to?",
              "score": 28,
              "created_utc": "2026-02-09 12:45:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fni7x",
                  "author": "Gedrecsechet",
                  "text": "The 2 types are in the name. Information and Technology. In my 25years plus working from the trenches up have noticed some people care about the tech and don't care about info it holds and others are visa versa.  Both are necessary but one exists for the purposes of the other. Like plumbing, the pipes exist to carry water and have no purpose without it.",
                  "score": 13,
                  "created_utc": "2026-02-09 14:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gvdug",
                  "author": "scarredMontana",
                  "text": "I used to work as software eng. at a famous investment bank in NYC (now I'm at a fintech in Times Sq.), but you quickly learn that engineering is 2nd and business is first. Everything is done with the attitude of 1) how can we make money and stay in business and then 2) how do we make this resilient and scalable. It pains me now to work with engineers who are just to up their own ass to realize why we're building things.",
                  "score": 6,
                  "created_utc": "2026-02-09 17:45:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fgd3h",
              "author": "Tender_Figs",
              "text": "To answer where I want to go - I would love to get back into analytics with a very heavy finance bend that may also mix data science into it. Itâ€™s where I was headed in 2022 before detouring into data engineering.",
              "score": 9,
              "created_utc": "2026-02-09 13:24:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fr4ur",
                  "author": "RandomAccount0799",
                  "text": "What made you detour into data engineering? Do you prefer analytics to corporate finance?",
                  "score": 2,
                  "created_utc": "2026-02-09 14:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fewnn",
              "author": "lozinge",
              "text": "Its a good point - also raises the question of how far can domain experience get you / how much of a moat is it?\n\nDo you know what you're going to try breaking into?",
              "score": 3,
              "created_utc": "2026-02-09 13:15:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gfoy5",
          "author": "THBLD",
          "text": "Yes and no. I don't know where your skill level is but:\nthe ability of people to actually do quality in-depth SQL, writing functions and procedures, while understanding how it works under the hood is very far and few ppl. \n\nAs someone who's done a lot of ETL and works very heavily with SQL procedures - I do ALSO unfortunately feel that's it's nowadays not a very appreciated skill set, given the complexity to do it well.\n\nEveryone is way too focused on tool sets, AI and relying on Python for way too much. Proper SQL is still very powerful in the right hands and NEEDED.",
          "score": 14,
          "created_utc": "2026-02-09 16:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gq18t",
              "author": "turboDividend",
              "text": "yeah dude, like...I can understand using python to parse unstructured data or something, like scrapring a website or pulling information out of a word doc/flat file/etc but i dont see the purpose of using it for doing regular ETL type stuff when a proper database can do alot of the heavy lifting, granted you know what you're doing.\n\nI've made functions in sql, stored procs, know window functions , cte, temp tables very well and have even used cursors ( lol ) . I understand how pivots work and have done outer joins/cross joins (this seems to be a bit outdated though) ive come across it in older dbs",
              "score": 3,
              "created_utc": "2026-02-09 17:19:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4f9l9n",
          "author": "ThroughTheWire",
          "text": "If you're working in a cost center rather than a profit center then it can really feel that way. I do recognize that my job is almost completely subject to automation within the next few years and so I need to make sure I'm in a position to be an influential decision maker from a business / technical point of view rather than just following the directions of management/stakeholders.",
          "score": 31,
          "created_utc": "2026-02-09 12:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fnbol",
              "author": "turboDividend",
              "text": "most of these jobs are cost centers unfortunately. what data engineering jobs are profit centers? working in trading, Adtech, or supporting some sort of high volume sales business?",
              "score": 9,
              "created_utc": "2026-02-09 14:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4fowi1",
                  "author": "snarleyWhisper",
                  "text": "Iâ€™m an embedded data engineer within a sales org. All my projects have a very specific roi and are low cost. They are happy to keep expanding the scope and investing in it because they are cost sensitive but are getting business value.",
                  "score": 7,
                  "created_utc": "2026-02-09 14:14:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fqgsp",
                  "author": "BoringGuy0108",
                  "text": "If you're driving sales or cost reductions, you could be considered a profit center. It depends on your KPIs. Is your goal to accomplish a minimum expectation at the lowest possible cost? You're a cost center. Is your goal to drive the most overall business value and cost is just one factor? You're a profit center. Or at least more of one. \n\nAlso, if your company's product is technology that requires data movement, you're a profit center by default. At my company, IT is broadly a cost center, but my team is often treated more like a profit center because we actively drive sales.",
                  "score": 6,
                  "created_utc": "2026-02-09 14:23:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fqve8",
                  "author": "thisfunnieguy",
                  "text": "there are companies that sell data or data products\n\nadtech is one example but not the only example.\n\n",
                  "score": 3,
                  "created_utc": "2026-02-09 14:25:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fg4mc",
          "author": "CorpusculantCortex",
          "text": "Idk i work as a 700 person global saas company that provides an industry specific data product that has a dedicated data science team. And I get roped into so many random etl and automation jobs because I'm the one who knows python and does etl stuff. So im not feeling very dime a dozen.",
          "score": 16,
          "created_utc": "2026-02-09 13:23:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fn25q",
              "author": "turboDividend",
              "text": "if you know python/sql and are good at it, it seems like you can write your ticket.",
              "score": 2,
              "created_utc": "2026-02-09 14:04:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4gxvz8",
                  "author": "outlier_fallen",
                  "text": "Huh? Every data engineer should know python and SQL. This kind of contradicts the point of this thread.. now I'm questioning it",
                  "score": 5,
                  "created_utc": "2026-02-09 17:57:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fqmz4",
          "author": "thisfunnieguy",
          "text": "how \"special\" do you need to be? theres like 400 million ppl in this country. Something like 200-300million of that are adults.\n\n  \n",
          "score": 7,
          "created_utc": "2026-02-09 14:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4golu2",
          "author": "cokeapm",
          "text": "Branch out deeper into infra. Learn cloud, bigger tools like spark, Kafka, etc. Just go bigger. As you grow less and less people compete with you. Also git gud at being a value provided. Learn how to talk to different people, how to drive initiatives, etc",
          "score": 3,
          "created_utc": "2026-02-09 17:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hv1vd",
          "author": "RoleAffectionate4371",
          "text": "As a staff engineer, in a 100+ person data org, I very much do not feel a dime a dozen. \n\nItâ€™s still exceptionally hard to hire skilled engineers with agency, work ethic, and an ounce of business instinct. \n\nThe key is to not just be someone who writes etl. But someone who influences the bottom line through data engineering",
          "score": 4,
          "created_utc": "2026-02-09 20:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h5llq",
          "author": "TA_poly_sci",
          "text": "If you are good at DE, very much no",
          "score": 1,
          "created_utc": "2026-02-09 18:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jk7v6",
          "author": "molodyets",
          "text": "If you donâ€™t understand the business and how other departments think and just want to be an order taker - yes",
          "score": 1,
          "created_utc": "2026-02-10 02:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mxehf",
              "author": "Accomplished_Cloud80",
              "text": "I think just take orders is priority. Understand business and share ideas always good and being part of the team.",
              "score": 0,
              "created_utc": "2026-02-10 16:17:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4nohss",
                  "author": "molodyets",
                  "text": "being able to take orders is table stakes. if you can't do more than that, you are going to have a very hard time in a down job market.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:21:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4k9uh5",
          "author": "JuggernautSad10",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-02-10 04:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ki7mo",
          "author": "szrotowyprogramista",
          "text": "Surfing cscareers to get a read on the market is kind of like reading feedback about parcel delivery services online (in absolute terms). People who are doing fine are not posting there. People who are anxious, venting or desperate are. So you're dealing with some serious sampling bias. If you value your mental health I see no point visiting that subreddit.\n\nWith that said - yeah. We are a dime a dozen. So are most people working in the more-or-less modern and commercially relevant branches of software engineering. We are not special, neither are react/node devs, neither are ios/android mobile devs, neither are infra/devops, neither are (run-of-the-mill) data scientists, etc. The non-dime-a-dozen people are those that work in more niche technologies, like dedicated graph DB engineers, or SAP stack devs or maybe these few guys that some US state government urgently hired in 2020 because they knew FORTRAN. \n\nIt's not obvious to me that one is better than the other. If you're niche, you probably have one or two employers that really need your services, but once they finally retire the legacy application they had that required you - you're in a bad position, because you're probably not finding another job. If you're a dime a dozen - you face a lot of competition, but it would be relatively easier to find something new if you lose your job. (This is of course ignoring how good you are - of course there's always many beginners and few extremely knowledgeable people, but that is true with any technology.)\n\nAlso, just a small remark, I'm not sure it is informative to say \"does a lot of SQL\". SQL is a language that has broad support across different databases and data processing engines, it's one thing to be doing a lot of SQL on a huge on-prem Postgres cluster, another to be doing a lot of SQL on a managed platform like Snowflake.",
          "score": 1,
          "created_utc": "2026-02-10 05:50:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ltqm3",
          "author": "Commercial-Ask971",
          "text": "Back in the day I was power bi developer who would establish metrics and discuss how to calculate things or why, however I felt bad because I was chased for silly things like metric doesnt fit to one MD expectations.. or â€žlets make this report more aestheticâ€ etc so I switched to data engineering and while I rarely make anything related to metrics anymore (just deliver semantic model and finish), now I feel like nobody cares and just expect me to move and shape the data then get into next. â€ž if it works it worksâ€ so no time for technical debt or scabalility unless something fair miserably, then an asap fix is required.. I feel that I do more â€žvaluableâ€ things but I guess only mine imagination. In both positions I feel like third wheel. When would it end?",
          "score": 1,
          "created_utc": "2026-02-10 12:47:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r741h",
          "author": "Hear7y",
          "text": "I work in a pretty big company, and if we were a dime a dozen, they wouldn't call me and members of the team I'm part of to develop and lead everything, that has to do with that, if that were true. We have people in Asia, LATAM and so on.\n\nIf it weren't illegal, I'd probably be booked 400%.\n\nIt's the same with software engineers - there are many, most are however just not good, the difference is that DE actually requires a significant amount of non-technical knowledge, on top of a constantly shifting tech stack (why most people say DE is not entry-level).\n\nIt is absurdly difficult to find skilled and knowledgeable people whose hand you don't have to constantly hold, but they somehow demand massive payment.\n\nSo, no, I don't think we're a dime a dozen and AI use is actually making the inadequate people painfully obvious. Just because it is a force multiplier and 0 multiplied by anything is still 0. :D",
          "score": 1,
          "created_utc": "2026-02-11 06:09:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ixyal",
          "author": "LCuevad",
          "text": "switch to data platform ",
          "score": 1,
          "created_utc": "2026-02-09 23:57:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r12ckn",
      "title": "How do you justify confluent cloud costs to leadership when the bill keeps climbing?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r12ckn/how_do_you_justify_confluent_cloud_costs_to/",
      "author": "Funny-Affect-8718",
      "created_utc": "2026-02-10 14:34:12",
      "score": 55,
      "num_comments": 70,
      "upvote_ratio": 0.89,
      "text": "Our confluent bill just hit $18k this month and my manager is freaking out. We're processing around 2 million events daily, but between cluster costs, connector fees, and moving data around we're burning through money.\n\n\n\nI tried explaining that kafka needs this setup, showed him what competitors charge, but he keeps asking why we can't use something cheaper, and honestly starting to wonder the same thing. We're paying top dollar and I still spend half my time fixing cluster issues.\n\n\n\nHow do you prove it's worth it when your boss sees the bill and goes pale, we're a series b startup so every dollar counts, what are teams using these days that won't drain your budget but also won't wake you up with alerts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r12ckn/how_do_you_justify_confluent_cloud_costs_to/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4mqup0",
          "author": "DungKhuc",
          "text": "Without knowing your requirements, that sounds like a very small amount of data to move for 18k / month",
          "score": 91,
          "created_utc": "2026-02-10 15:46:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4oem9n",
              "author": "TA_poly_sci",
              "text": "<30 events per second, <60 if we say most are concentrated during the day. Not nothing, but sure as hell not something that requires spending 18k a month.",
              "score": 20,
              "created_utc": "2026-02-10 20:22:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ofl0j",
              "author": "shoppedpixels",
              "text": "It is likely the connectors, I do t believe you can sleep them for lower environments or cost savings. Maybe over those to a self managed connect cluster or just a container environment?",
              "score": 4,
              "created_utc": "2026-02-10 20:26:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mxg9w",
          "author": "Skullclownlol",
          "text": "> How do you prove it's worth it when your boss sees the bill and goes pale, we're a series b startup so every dollar counts, what are teams using these days that won't drain your budget but also won't wake you up with alerts?\n\nEverything non-realtime/non-streaming. Literally almost everything else will be cheaper.",
          "score": 37,
          "created_utc": "2026-02-10 16:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mykmx",
          "author": "Prinzka",
          "text": "In fact I do the opposite.  \nI use what would be high cloud cost to justify keeping our Kafka on-prem.  \nWe stream trillions of events per day for basically the same cost as your confluent cloud setup by just running Kafka on-prem.",
          "score": 35,
          "created_utc": "2026-02-10 16:22:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ol59v",
              "author": "yesoknowhymayb",
              "text": "Just curious, are you including extra labour cost? If there is any.",
              "score": 4,
              "created_utc": "2026-02-10 20:52:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4oqrql",
                  "author": "Prinzka",
                  "text": "In my internal calculations yes, labour and on prem compute cost. \nI meant our licensing is the same cost as OP's confluent cloud cost.  \nIncluding all costs and comparing to confluent cloud for our volume it was more than an order of magnitude more expensive to stay on-prem.   \n\nAlso keep in mind that to run 24/7 without outages you still need at least 2 people (realistically 3) to maintain things even if you're using confluent cloud.  \nWe're expected to provide real time security feeds without downtime, so we can't just switch off after hours.",
                  "score": 5,
                  "created_utc": "2026-02-10 21:18:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4nhcqa",
          "author": "vish4life",
          "text": "2 million events for $18k on Kafka? We process 1 billion+ events on AWS MSK Kafka < 15k. A single Kafka node on basic cloud machines can easily handle 100-500 mil events per day. You are struggling to handle 2 mil events. \n\nSomething has gone terribly wrong with your setup.",
          "score": 36,
          "created_utc": "2026-02-10 17:49:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nmvti",
          "author": "sleeper_must_awaken",
          "text": "Talk to Confluent sales rep: either consult to bring the cost down by 50% or weâ€™re out.\n\nBut 2M events per day is literally only 23 events per second. Thatâ€™s nothing. An old Raspberry Pi could process at least 2000 events per second with two fingers in its nose. Youâ€™re being screwed, either by Confluent or by incompetence in configuration, probably both.",
          "score": 21,
          "created_utc": "2026-02-10 18:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mpqgv",
          "author": "sazed33",
          "text": "Your manager is right, you don't need Kafka",
          "score": 41,
          "created_utc": "2026-02-10 15:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mzd0s",
          "author": "ReporterNervous6822",
          "text": "2 million daily and using Kafka ðŸ˜­ðŸ˜­",
          "score": 13,
          "created_utc": "2026-02-10 16:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n36oc",
              "author": "Sex4Vespene",
              "text": "I mean if they need streaming, itâ€™s a decent enough reason to go with Kafka is it not? Although I have to wonder if they really need streaming",
              "score": 6,
              "created_utc": "2026-02-10 16:43:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mr22r",
          "author": "tedward27",
          "text": "You should be embarrassed at spending so much for so littleÂ ",
          "score": 51,
          "created_utc": "2026-02-10 15:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mot9n",
          "author": "No_Lifeguard_64",
          "text": "If you can't afford streaming data then don't do streaming. That's just the reality of it. Whats the business difference between streaming and microbatch? If you aren't actually actioning on that real-time data then just move to airflow or something.",
          "score": 40,
          "created_utc": "2026-02-10 15:37:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mjaol",
          "author": "TheOverzealousEngie",
          "text": "why do you need streaming? Why not data replication?",
          "score": 22,
          "created_utc": "2026-02-10 15:10:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ngr6f",
          "author": "dfwtjms",
          "text": "That's cloud. In reality a smartwatch and sqlite could handle that workload.  \n  \nThe truth is that nothing justifies those bills. Don't fall for the sunk cost fallacy and build something better.",
          "score": 9,
          "created_utc": "2026-02-10 17:46:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mv223",
          "author": "wqrahd",
          "text": "If you are on aws, why not kinesis?",
          "score": 7,
          "created_utc": "2026-02-10 16:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n39mj",
          "author": "Truth-and-Power",
          "text": "What about microbatch instead of streaming?",
          "score": 3,
          "created_utc": "2026-02-10 16:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pmq6n",
          "author": "DarthCalumnious",
          "text": "Get a VM with fast nvme and just throw your junk into clickhouse.",
          "score": 3,
          "created_utc": "2026-02-11 00:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mrh6o",
          "author": "Training_Refuse7745",
          "text": "You can try serverless approach if cluster keeps on going down. That is more reliable. I have 1.5yoe but recently in my current project we replaced things with serverless and it is not failing and also the costs are low. We have batch ingestion so I am not 100% sure about streaming.",
          "score": 2,
          "created_utc": "2026-02-10 15:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qll3n",
              "author": "kingglocks",
              "text": "Lambda + step functions?",
              "score": 1,
              "created_utc": "2026-02-11 03:31:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4myi9v",
          "author": "ThroughTheWire",
          "text": "Would need to know/understand why you need Kafka in the first place to give you a real answer tbh",
          "score": 2,
          "created_utc": "2026-02-10 16:22:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p5ugs",
          "author": "zikawtf",
          "text": "OP could you bring us more context, I am really curious about the stack and business context to justify using Kafka (not just as tool, but as a business requirement)",
          "score": 2,
          "created_utc": "2026-02-10 22:30:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4morxd",
          "author": "Nekobul",
          "text": "That's ridiculous. You can process that amount of data easily on a single machine using SSIS and it will be dirt cheap.",
          "score": 6,
          "created_utc": "2026-02-10 15:36:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nh8sa",
              "author": "dfwtjms",
              "text": "Or even better, open source stack without Microslop.",
              "score": 7,
              "created_utc": "2026-02-10 17:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pxemt",
                  "author": "wubalubadubdub55",
                  "text": "Whatâ€™s up with Microsoft hate for everything? \n\nLike what have the engineers who develop SSIS done to you thatâ€™s so bad that you have to ridicule it? \n\nMan you people are pathetic and full of hate.",
                  "score": -1,
                  "created_utc": "2026-02-11 01:04:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nrqru",
                  "author": "Nekobul",
                  "text": "How is that better if you have to pay consultants top dollars to create and maintain that solution?",
                  "score": -3,
                  "created_utc": "2026-02-10 18:36:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4negya",
              "author": "IndependentTrouble62",
              "text": "Truth...",
              "score": 1,
              "created_utc": "2026-02-10 17:35:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nnb1u",
          "author": "Flacracker_173",
          "text": "You need to self host a Kafka connect cluster and probably lower your data retention period and topic replicas.",
          "score": 1,
          "created_utc": "2026-02-10 18:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nncn0",
          "author": "baby-wall-e",
          "text": "2 millions per day is to tiny. You should consider refactoring your system, moving away from confluent.",
          "score": 1,
          "created_utc": "2026-02-10 18:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nt44o",
          "author": "Sad_Monk_",
          "text": "for 2 million events what is stopping you from micro batching?",
          "score": 1,
          "created_utc": "2026-02-10 18:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nvfjm",
          "author": "Brave_Affect_298",
          "text": "We will soon have to choose between Google Pub/Sub and Confluent Kafka. Is Pub/Sub cheaper?",
          "score": 1,
          "created_utc": "2026-02-10 18:53:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ocfl9",
          "author": "dan_the_lion",
          "text": "Why do you feel like you need to justify the bill instead of exploring alternatives which might be better fit for your use case? \n\nWhat are you using Kafka for exactly? Is it just data replication from databases to warehouses or is it actually a queue/streaming backbone of your application? \n\nIf itâ€™s just to enable analytics there are many options you can consider. Do you need log-based CDC?",
          "score": 1,
          "created_utc": "2026-02-10 20:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p45u0",
          "author": "Hackerjurassicpark",
          "text": "You donâ€™t. Weâ€™re moving to pub sub",
          "score": 1,
          "created_utc": "2026-02-10 22:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pwavn",
          "author": "wubalubadubdub55",
          "text": "Youâ€™re wasting so much money for so little. \n\nA .NET worker service + Postgres db can do that in a small VM without breaking a sweat for a tiny fraction of that cost.",
          "score": 1,
          "created_utc": "2026-02-11 00:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q6iol",
          "author": "thisfunnieguy",
          "text": ">kafka needs this setup\n\nThere are things like MSK via AWS\n\n  \nim not saying thats right for you, but there are a number of other options.",
          "score": 1,
          "created_utc": "2026-02-11 01:59:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qpr8j",
          "author": "codykonior",
          "text": "Replace it with a bash script on a VM.\n\nPro: $18k cheaper\n\nCon: Does not look fancy on the resume and HA DR are extra work.\n\n/s but ... ðŸ˜€",
          "score": 1,
          "created_utc": "2026-02-11 03:59:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r817v",
          "author": "Front-Ambition1110",
          "text": "I think you are overprovisioning. 2 million events is not that huge.Â ",
          "score": 1,
          "created_utc": "2026-02-11 06:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rs3km",
          "author": "calimovetips",
          "text": "at 2m events a day that bill sounds more like overprovisioning or connector sprawl than pure volume, iâ€™d break down cost per million events and map it to actual business use cases so leadership sees value per stream, not just a lump sum.\n\nhave you modeled what self managed or a lighter managed kafka setup would cost once you factor in ops time and on call?",
          "score": 1,
          "created_utc": "2026-02-11 09:23:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sbdnm",
          "author": "observability_geek",
          "text": "You don't because you can run enterprise kafka without confluent.. I really don't understand why orgs pay for kafka if they can use Strimzi. [https://github.com/strimzi/strimzi-kafka-operator](https://github.com/strimzi/strimzi-kafka-operator) if you are in the EU you can use [axual.com](http://axual.com) for the governance.",
          "score": 1,
          "created_utc": "2026-02-11 12:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sdjz9",
          "author": "Los_Cairos",
          "text": "As others in this thread have said, your costs are crazy given the volume.\n\nI work at a data streaming company (not Confluent) and we have customers doing 25x your daily events for 1-1.5x the cost.\n\nSo either something is really off in your setup (e.g. something about your connector setup, or you've got some super high custom data retention duration), or you're paying wayyyy too much and you need to talk to your rep.\n\nEdit: better cost estimate",
          "score": 1,
          "created_utc": "2026-02-11 12:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sixbi",
          "author": "trentsiggy",
          "text": "Are you processing them realtime?  If so, do you *have* to process them realtime?",
          "score": 1,
          "created_utc": "2026-02-11 13:00:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o569hmw",
          "author": "Lingonberry_Feeling",
          "text": "Iâ€™m generally all for the working man, but your manager is right.\n\nSomething is very wrong with your setup, not knowing anything about it, 2 million events for 18k way too much.",
          "score": 1,
          "created_utc": "2026-02-13 14:59:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2nrv3",
      "title": "Am I cooked?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/",
      "author": "Slik350",
      "created_utc": "2026-02-12 08:08:21",
      "score": 45,
      "num_comments": 30,
      "upvote_ratio": 0.75,
      "text": "Will keep this as short and sweet as possible.\n\nJoined current company as an intern gave it 1000% got offered full time under the title of:\n\nJunior Data Engineer.\n\nDespite this being my title the nature of the company allowed me work with basic ETL, dash boarding, SQL and Python. I also developed some internal streamit applications for teams to input information directly into the database using a user friendly UI.\n\nWhy am I potentially cooked?\n\nData stack consists of Snowflake, Tableau and and Snaplogic (a low code drag and drop etl tool). I realised early that this low code tool would hinder me in the future so I worked on using it as a place to experiment with metadata based ingestion and create fast solutions. \n\nNow that Iâ€™ve been placed on work for a year that is 80% non DE related aka SQL copying/report bug fixing Whilst initially Iâ€™d go above and beyond to build additional pipelines and solutions I feel as though Iâ€™ve burnt out.\n\nI asked to alter this work flow to something more aligned with my role this time last year. I was told Iâ€™d finally be moving onto data product development this year April (in effect Iâ€™ve been begging to just do what I should have been doing) and Iâ€™ve realised even if I begin this work in April Iâ€™m still at almost three years experience with the same salary I was offered when I went full time and no mention or promise of an increase.\n\nI know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesnâ€™t interest me whatsoever. At this rate my performance will continue to drop for lack of any incentive to continue besides collecting this current pay check.\n\nIâ€™ve had some interviews which are offering 20-25% more than my current role, interpersonally I succeed and am able to progress but in the technical sections I struggle without resources. Iâ€™d say Iâ€™m a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwersâ€¦\n\nHas anyone been in a similar position and what did you do to move on from it?\n\nTldr:\nAlmost at 3 years experience, level of experience technically lagging behind timeframe due to exposure at work being limited and lack of personal growth. Getting interviews but struggling with answering without resources.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4y4w9w",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-12 08:08:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y5h9v",
          "author": "Typical_Priority3319",
          "text": "Ridiculously far off from cooked. Create an itemized list of the things you donâ€™t know that you either\n a) have been asked in interviews already \n B) think u might get in future interviews based off of research\n\nStart looking at videos on YouTube to understand those concepts. Find excuses to learn those concepts at work whenever possible , but u might just have to do lil mini side projects to crystallize the concepts",
          "score": 121,
          "created_utc": "2026-02-12 08:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ymu1l",
              "author": "Slik350",
              "text": "Thank you for this; will do this asap. Iâ€™ve been doing some side projects but can definitely up the effort and make it along with practice more of my focus instead of my current day to day tasks.",
              "score": 8,
              "created_utc": "2026-02-12 11:02:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ztity",
                  "author": "SRMPDX",
                  "text": "Set up a personal GitHub. Work on side projects that are interesting and fill knowledge gaps. Document what you did, why you did it (be honest about upskilling), what issues you had in doing this the first time, maybe even \"what if do differently next time\", and make the repos public. Put a link on your resume. \n\nPotential hiring managers would love to see someone with initiative that can self learn and solve problems. Instead of answering random questions about syntax (15+ YoE and I still suck at syntax sometimes) they can talk to you about your code. Don't fall into the trap of letting a chat bot write all the code though.",
                  "score": 10,
                  "created_utc": "2026-02-12 15:27:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y5yzc",
          "author": "mh2sae",
          "text": "Snowflake and Tableau are used at Big Tech. Your tasks look to me aligned with what I would expect of a Junior DE. \n\nSnowflake itself is less infra than the counterparts at AWS/GCP but still quite complex and with plenty of options to optimize, do ML, ETL pipes, orchestrate scripts...\n\n  \nThere is plenty you can automate in Snowflake to either do more technical work or sell at interviews. Look into cost optimization, infra as code, documentation, optimizing queries...",
          "score": 40,
          "created_utc": "2026-02-12 08:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yn08p",
              "author": "Slik350",
              "text": "Cost ptimisation is something Iâ€™ve not had much time to look into sounds valuable and interesting will give it a look, thanks",
              "score": 5,
              "created_utc": "2026-02-12 11:03:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zzcqq",
                  "author": "randomuser1231234",
                  "text": "Re: cost optimizationâ€”look into predicate pushdown, and how this affects SQL query costs. Learn how to read query explains if you donâ€™t already know. Use that to make the queries youâ€™re copy/pasting BETTER.",
                  "score": 4,
                  "created_utc": "2026-02-12 15:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y83wn",
          "author": "Sensitive-Sugar-3894",
          "text": "DE is boring. After you suffer to make it all work, it becomes boring as it should.\n\nSnowflake, Databricks whatever, are just another thing in the jungle. The good jobs are over MySQL, very old Psql... In old Perl or Bash scripts with horrible embedded SQL. Your dream is to move to dbt and if you do, it will be boring again.\n\nData Engineering is not Systems Engineering. Want excitement, move out from DE.",
          "score": 30,
          "created_utc": "2026-02-12 08:40:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwzdc",
              "author": "xean333",
              "text": "I mean this earnestly: data modeling and building OLAP/OLTP systems has never gotten old for me after 11 yoe. Though I do agree - when done right, the only people that should find it thrilling are the freaks (myself included) that get off to the beautiful machinery of a well oiled platform. In other words, itâ€™s usually a sign youâ€™ve done something right if the build is boring to basically everyone who looks at it",
              "score": 17,
              "created_utc": "2026-02-12 15:44:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z146v",
              "author": "mcgrst",
              "text": "There is a reason interesting times is a curse!Â ",
              "score": 7,
              "created_utc": "2026-02-12 12:52:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59kapu",
              "author": "under_stroke",
              "text": "My experience is that the market is expading the role to Analytics Engineer, where some level of data intelligence is done whislt implementing most of the DE technical attributes. I assume the reason why Snowflake and low/no code is getting more popular is to remove technical barriers so data professionals can potentially spend more time shipping business value. In the last 7-8 years I saw a great increase in end-to-end more or so fullstack data professional. ",
              "score": 1,
              "created_utc": "2026-02-14 01:05:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ypj1a",
          "author": "domscatterbrain",
          "text": ">I'd say I'm a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwers...\n\nYou have a strong base, mate. \n\nDon't worry, Google is your friend. And now AI will get you the answers faster than reading the entire forum discussion. Well, as long as you ask the right questions.",
          "score": 8,
          "created_utc": "2026-02-12 11:25:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yxo8t",
          "author": "tasker2020",
          "text": "3 years in is a good time for your first job hop.  Youâ€™ll get a raise and broaden your experience.",
          "score": 6,
          "created_utc": "2026-02-12 12:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9tva",
          "author": "LeaveTheWorldBehind",
          "text": "Great DE advice in here. Speaking strictly from career perspective, it's typical to feel that boredom/itch around 2-3 years and it's always good to make your needs/expectations relatively clear. If you want to keep growing, don't wait 3 years to share that or 1 year. Keep at it consistently, talk with your manager or your manager+1 about the things you're interested in, ideas you have for other things.\n\nIf you want more, push for more. You're most useful when you're engaged and that'll often show. It doesn't always mean staying with the same company, but often times it does. I've made business cases while in low paid roles that turned into better work.",
          "score": 5,
          "created_utc": "2026-02-12 13:44:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z2i9k",
          "author": "WallyMetropolis",
          "text": "You joined as an intern. They know you need to learn. They offerered you the job because they believe you can.Â ",
          "score": 4,
          "created_utc": "2026-02-12 13:01:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zicj5",
          "author": "Apprehensive-Ad-80",
          "text": "3 years in, DEFINITELY not cooked. Hell even if this was 5 years in Iâ€™d say youâ€™re fine. If youâ€™re struggling on the technical evaluations during interviews make that a priority in your current jobâ€¦ instead of finding an online example or previous work to build from, do it from scratch and only use references when you fail",
          "score": 5,
          "created_utc": "2026-02-12 14:31:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zl8z8",
          "author": "Leather-Replacement7",
          "text": "Low code wrangling skills mean you will still have intuition. Learning syntax takes practice. Practice leetcodes, stratascratch. Get yourself an AWS account, or play with some tech via docker compose. I bet you know more than you think. Sadly imposter syndrome in data engineering doesnâ€™t ever go away but it gets better. I have 10 years experience, Iâ€™ve hardly used pyspark, because everywhere Iâ€™ve worked prefers an elt approach to transformation or the data simply isnâ€™t big enough. Thereâ€™s just so many ways to skin a cat in our field, one way isnâ€™t necessarily better than another.",
          "score": 3,
          "created_utc": "2026-02-12 14:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53w94w",
          "author": "DiscussionCritical77",
          "text": "'I know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesnâ€™t interest me whatsoever.'\n\nI'm 46 and that has been like 30% of my working career. Jobs naturally conclude when they are no longer useful to your career progression. What you do now, is you figure out what the next move to your career is, you train up for it with certs and side projects, you level up, and you change jobs and get a fat raise in the process.",
          "score": 3,
          "created_utc": "2026-02-13 04:14:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54ceo3",
          "author": "Dense___",
          "text": "Iâ€™m in the same boat too, my tech stack is limited to SAP enterprise software that came out in 2008 that crashes every other time I use it... I am 2 years in at my first job in DE and work with so many legacy tools but I know Iâ€™m at least understanding the concepts. Currently spending all my spare time to learn more modern tools and technologies so I can get an easy 20-30% bump at the next place lol",
          "score": 3,
          "created_utc": "2026-02-13 06:16:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o561n9f",
          "author": "FantasticEquipment69",
          "text": "I believe what you're looking for is working for an IT solution/consultancy company (of course under the data team)\n\nBeing on the vendor side will give you many of what you're looking for like: \n1- being exposed to different problems, different technologies, different businesses (banking, telecom, health care,...) \n2- avoiding the part where \"whenever the job is done it become boring\", aka avoiding working in operation and waiting for something to fail just to find something to work on.",
          "score": 3,
          "created_utc": "2026-02-13 14:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z14cv",
          "author": "OhNo171",
          "text": "I once thought that too, but no/low code tools wonâ€™t hinder you, specially in your early years. On the contrary, Id say it makes you focus on what really matters - how to better optimize your pipeline and think more about the end product instead of worrying about language/semantics. I wont say its not important to learn to code, but in the future, regardless if you use spark, pandas, scala, python, ruby, the core etl development skillsets are still there.",
          "score": 2,
          "created_utc": "2026-02-12 12:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51hmjg",
          "author": "Specific-Sandwich627",
          "text": "You should take time for your rest. Rest is part of work. The fact that youâ€™re burning out is already a serious sign. You need to try to address this as soon as possible. Try to relieve yourself mentally and shift the focus of your body and mind to different activities for a few hours before sleep. You could change your diet, try new foodsâ€”maybe cooking or going for walks somewhere that feels closer to your soul. This is very important.",
          "score": 2,
          "created_utc": "2026-02-12 20:10:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53tjub",
          "author": "Icy_Clench",
          "text": "My advice to you is if you think your current company can be doing their data engineering better, lead the way forward. You will move up in no time if you can show value in doing things differently.\n\nWhen I was promoted to DE at my company (analyst for 3 months prior), they also used a drag and drop GUI hooked up to an OLTP database, and frankly the datasets sucked. They had no mechanism to retain historical data for SCDs so there was no historical accuracy. They had created like 6 surviving OBT tables after 3 years and they spent all of their time fixing constant bugs. The data analysts had created a shadow data warehouse that actually ran 95% of the reports.\n\nSo, in my first 6 months I learned about the company and processes and pushed for better practices: git, OLAP database (Snowflake), and proper modeling. I produced 4 datasets that were correctly modeled. I also optimized the daily runtime of the warehouse from 3 hours to 1 hour, and reduced a weekly pipeline from 10 hours to 1 minute, so I built a lot of cred as an expert.\n\nSecond year the company was ready and budgeted for a migration to Snowflake and git. The team was doing PRs and I was setting the standards for code reviews. We set up the platform with basic tooling to deploy, ingest, and transform data, including CDC and SCDs. I introduced some project management frameworks to my manager as well.\n\nThis year Iâ€™m hammering on modeling and tooling. I have shown them the value of conformed dimensions and I do the data modeling. I basically mandated no more unmodeled data and no more shadow warehouse moving forward. We also have big capability gaps between ingestion, transformation, automated testing (unit/data quality), orchestration, and more. Iâ€™ve put together a roadmap to address these for this year.",
          "score": 2,
          "created_utc": "2026-02-13 03:55:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58up94",
          "author": "Equivalent_Effect_93",
          "text": "Absolutely not, the hardest part is getting through the door. Learn on your own, build portfolio don't wait for your employer to train you on the toys you wanna play with. 5 years ago I started automating unversionned SAS files on a local Cron server, almost a laptop. Now I'm a senior data engineer playing with a massive databricks systems, elt, streaming and batch, debezium+kafka ingestion, mlops and model serving. Build stuff you wanna try, then when a good opportunity arise you have proof you can deliver in an enterprise setting (even low code tool), and a portfolio with a few project, properly versionned, even maybe deployable through CI/CD. Keep up the good work.",
          "score": 2,
          "created_utc": "2026-02-13 22:34:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y6fnq",
          "author": "Thinker_Assignment",
          "text": "been there, moved jobs. different times\n\nMaybe you can start working on changing the situation by looking for data sources your saas does not provide and creating pipelies for those, run them on snowpark or gh actions if you have nothing else.  \nor consider if the saas is worth replacing with code\n\nalso why so much report bugfixing? look into dimensional modeling for self service, canonical models, maybe if you have better architecture you don't have so much ad hoc work. tableau is not great for self service, it operates under the paradigm that the analyst spoon feeds most things, its both too complex and too weak to be powerful for business user for self service. Most engineers see tableau as a \"busy tool\" and prefer things like metabase etc for this reason",
          "score": 2,
          "created_utc": "2026-02-12 08:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o510r5l",
          "author": "RelevantScience4271",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-02-12 18:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9jd6",
          "author": "tsk93",
          "text": "Short answer: No\n\nI'm pretty much in a similar situation as u are. Just that i'm a data analyst hoping to move into DE one day. Current job feels kinda mundane and u are looking for smth else to grow. Personally I use certifications to build foundational knowledge and move to projects later on. Believe in yourself, u got this.",
          "score": 1,
          "created_utc": "2026-02-12 13:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zutox",
          "author": "starrorange",
          "text": "Your dumb if you think this is bad for a fresh grad",
          "score": 1,
          "created_utc": "2026-02-12 15:33:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0wwrn",
      "title": "Our company successfully built an on-prem \"Lakehouse\" with Spark on K8s, Hive, Minio. What are Day 2 data engineering challenges that we will inevitably face?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r0wwrn/our_company_successfully_built_an_onprem/",
      "author": "seaborn_as_sns",
      "created_utc": "2026-02-10 10:08:31",
      "score": 44,
      "num_comments": 48,
      "upvote_ratio": 0.93,
      "text": "I'm thinking \n\n\\- schema evolution for iceberg/delta lake  \n\\- small file performance issues, compaction\n\nWhat else? \n\nAny resources and best practices for on-prem Lakehouse management?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0wwrn/our_company_successfully_built_an_onprem/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4la04a",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-10 10:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lccan",
          "author": "liprais",
          "text": "minio will be your biggest pain of ass",
          "score": 53,
          "created_utc": "2026-02-10 10:30:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lde2s",
              "author": "jupacaluba",
              "text": "I second that. Just reading gave me the itch",
              "score": 6,
              "created_utc": "2026-02-10 10:39:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ldpak",
              "author": "seaborn_as_sns",
              "text": "is it because they abandoned foss? what else is there for on-prem? ceph?",
              "score": 5,
              "created_utc": "2026-02-10 10:42:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4lkli5",
                  "author": "rmoff",
                  "text": "garage, seaweedfs, apache ozone, and several others. depends what you need. I wrote about it here (although from a PoC/demo perspective, not production usage): https://rmoff.net/2026/01/14/alternatives-to-minio-for-single-node-local-s3/",
                  "score": 5,
                  "created_utc": "2026-02-10 11:42:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ljukg",
                  "author": "liprais",
                  "text": "i am running hdfs ,works smooth",
                  "score": 2,
                  "created_utc": "2026-02-10 11:36:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nln7l",
                  "author": "Colafusion",
                  "text": "Itâ€™s also AGPL, which depending on what youâ€™re doing can be a massive issue.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m127x",
              "author": "543254447",
              "text": "Can't agree with you more. Literally cannot delete some files for no reason.......\n\nAlways run into weird error with spark due to it.....",
              "score": 2,
              "created_utc": "2026-02-10 13:32:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4met9m",
                  "author": "seaborn_as_sns",
                  "text": "how big is your dataeng/dataops team?",
                  "score": 1,
                  "created_utc": "2026-02-10 14:47:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4p6daw",
              "author": "zikawtf",
              "text": "What justify the MinIO as a storage tool in production environment? I mean, store data is cheap, so why not S3?",
              "score": 1,
              "created_utc": "2026-02-10 22:33:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4sx6sn",
                  "author": "seaborn_as_sns",
                  "text": "airgapped environment, data residency regulations, etc",
                  "score": 1,
                  "created_utc": "2026-02-11 14:22:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4t1sjq",
              "author": "ludflu",
              "text": "wait, people use minio in production?!",
              "score": 1,
              "created_utc": "2026-02-11 14:46:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lh0gn",
          "author": "Gold_Ad_2201",
          "text": "it sounds like you buit now a 20 year old architecture.\n1. is spark the only access to data? what about lower latency? trino, duckdb?\n2. hive partitioning will only delay your problems. you def need to look into table formats (iceberg, delta). and more importantly - they are also designed badly. you need to look into having catalog with them to have the good speed\n3. I assume minio and k8s are because you have some requirement to have air gapped env? if not, do consider S3/blob to save your maintenance team",
          "score": 17,
          "created_utc": "2026-02-10 11:12:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mf2uk",
              "author": "seaborn_as_sns",
              "text": "1. experimenting on trino too  \n2. we have iceberg and delta too, unified hive catalog. should we adopt polaris or something else do you think?  \n3. yes we need airgapped. i think ceph is better option but no experience to advocate for it.  ",
              "score": 4,
              "created_utc": "2026-02-10 14:48:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4mwg0o",
              "author": "Doto_bird",
              "text": "Do you have any experience with MotherDuck (from DuckDB)? They critized iceberg and delta quite harshly in their announcement video and they addressed those issues (in their opinion), but I've never talked with anyone who's actually used it for big data workloads yet.",
              "score": 2,
              "created_utc": "2026-02-10 16:12:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mxgfv",
                  "author": "Gold_Ad_2201",
                  "text": "didn't use it in production, no. their comments are fair. but let's see if this tech becomes adopted and supported. their idea of DuckLake sounds pretty logical but other than MotherDuck I didn't hear of any commercial implementation.\nbut duckDB itself is awesome engine!",
                  "score": 2,
                  "created_utc": "2026-02-10 16:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4le43p",
          "author": "dragonnfr",
          "text": "Run aggressive compaction (bin-packing, 128MB targets). For schema evolution, only add fields. Check Delta docs for OPTIMIZE + ZORDER BY on small files.",
          "score": 4,
          "created_utc": "2026-02-10 10:46:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4leirv",
              "author": "seaborn_as_sns",
              "text": "any tool to monitor general health of delta tables or do teams build inhouse monitoring scripts?",
              "score": 1,
              "created_utc": "2026-02-10 10:50:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltl6j",
          "author": "Hackerjurassicpark",
          "text": "Upgrading your K8S, Hive and Minio when your current versions go EOL",
          "score": 5,
          "created_utc": "2026-02-10 12:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfa2j",
              "author": "seaborn_as_sns",
              "text": "you think thats near-term (2yrs) or bit later? ",
              "score": 2,
              "created_utc": "2026-02-10 14:49:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4p5ima",
                  "author": "Hackerjurassicpark",
                  "text": "Depends on the version youâ€™re using. Go check the EOL dates for the exact version youâ€™re using",
                  "score": 1,
                  "created_utc": "2026-02-10 22:28:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ldmck",
          "author": "FunAd6672",
          "text": "Data quality checks become your real Day 2 job not pipelines.",
          "score": 3,
          "created_utc": "2026-02-10 10:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4le9qw",
              "author": "seaborn_as_sns",
              "text": "how do you manage them? via dbt or inhouse tools via great expectations or something? ",
              "score": 2,
              "created_utc": "2026-02-10 10:47:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lsdb6",
          "author": "Eitamr",
          "text": "Minio is for testing, avoid on prod if you can",
          "score": 3,
          "created_utc": "2026-02-10 12:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfh1k",
              "author": "seaborn_as_sns",
              "text": "even enterprise minio? the \"aistor: Exabyte-Scale Storage Engineered for the AI Era\"",
              "score": 1,
              "created_utc": "2026-02-10 14:50:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltxgx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 3,
          "created_utc": "2026-02-10 12:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfyxl",
              "author": "seaborn_as_sns",
              "text": "thanks so much! what was the decision-making process that you guys arrived to that stack? followed some tried-and-tested blueprint from some similar company's experience or arrived purely based on internal discussions and evaluations?",
              "score": 1,
              "created_utc": "2026-02-10 14:53:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtd7n",
          "author": "ShanghaiBebop",
          "text": "Governance and access management will be a PITA.Â ",
          "score": 3,
          "created_utc": "2026-02-10 15:58:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4syj5q",
              "author": "seaborn_as_sns",
              "text": "any limitations to apache ranger?",
              "score": 1,
              "created_utc": "2026-02-11 14:29:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n92oq",
          "author": "SuperTangelo1898",
          "text": "Ghost objects that exist in the backend but don't exist in Minio's front end UI object manager",
          "score": 2,
          "created_utc": "2026-02-10 17:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nbpub",
          "author": "swapripper",
          "text": "Tenancy/Cost attribution \nGovernance/PII masking / RLS\nLogs/Lineage/Observability/Performance monitoring\nSemantic layer possibly\nCDC if you need it\nEasy abstractions for backfills/backups/compaction/cleanup",
          "score": 2,
          "created_utc": "2026-02-10 17:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nw2e9",
          "author": "efxhoy",
          "text": "Just curious, how much data do you have? 1TB? 100TB?Â ",
          "score": 2,
          "created_utc": "2026-02-10 18:56:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sxtzp",
              "author": "seaborn_as_sns",
              "text": "around 10TB in now legacy data warehouse in total",
              "score": 1,
              "created_utc": "2026-02-11 14:25:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pn6dc",
          "author": "Due_Carrot_3544",
          "text": "Whats your total data volume stored right now?",
          "score": 2,
          "created_utc": "2026-02-11 00:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sxv14",
              "author": "seaborn_as_sns",
              "text": "around 10TB in now legacy data warehouse in total",
              "score": 1,
              "created_utc": "2026-02-11 14:25:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lg82w",
          "author": "reallyserious",
          "text": "How can one handle access control like row level security and table level security?",
          "score": 1,
          "created_utc": "2026-02-10 11:05:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfc28",
              "author": "seaborn_as_sns",
              "text": "experimenting with ranger now",
              "score": 3,
              "created_utc": "2026-02-10 14:49:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4psyls",
          "author": "Rich-Ad5460",
          "text": "May I ask how long does it take to build this? And with how many people?",
          "score": 1,
          "created_utc": "2026-02-11 00:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sy3ft",
              "author": "seaborn_as_sns",
              "text": "total 10\\~ people built this as poc, ops + engineering",
              "score": 1,
              "created_utc": "2026-02-11 14:26:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n14a3",
          "author": "ChinoGitano",
          "text": "Why use Hive when Unity Catalog is now open-source?  Governance and performance may be your biggest headache â€¦ assuming you actually have the component integration licked. ðŸ˜…",
          "score": 1,
          "created_utc": "2026-02-10 16:34:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzb76a",
      "title": "Tech stack in my area has changed?How do I cope",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1qzb76a/tech_stack_in_my_area_has_changedhow_do_i_cope/",
      "author": "Sufficient_Example30",
      "created_utc": "2026-02-08 15:05:00",
      "score": 42,
      "num_comments": 16,
      "upvote_ratio": 0.84,
      "text": "So basically my workplace of 6 years has become very toxic so I wanted to switch.\nOver there i mainly did spark (dataproc),pub sub consumers to postgres,BQ and Hive tables ,Scala and a bit of pyspark and SQL\nBut I see that the job market has shifted.\nNowadays \nThey are asking me for knowledge of\nKubernetes\nDocker\nAnd alot of questions regarding networking along with Airflow \nHonestly I don't know any of these.\nHow do I learn them in a quick manner.\nLike realistically how much time do I need for airflow,docker and kubernetes ",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1qzb76a/tech_stack_in_my_area_has_changedhow_do_i_cope/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o49jswk",
          "author": "Darkendfearz",
          "text": "Honestly you can learn the basics of kubernetes and enough information to pass most interviews in a day. I also have used airflow a ton and would consider myself pretty comfortable with the tool but I have never been asked an interview question about it. Docker is also super simple to learn. Just take a step back, breath, and reading about it before freaking out. What kind of networking questions are you getting?",
          "score": 45,
          "created_utc": "2026-02-08 15:28:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49l6nb",
              "author": "Sufficient_Example30",
              "text": "Very wierd ones.\nHow do you setup Ray clusters.\nWhat are the firewalls rules you would need to enable\nIf the company services are behind a NAT gateway how do you ensure that you can hit them.\nIt's very wierd nowadays \nWhy use spark serverless over a spark cluster\nHow does networking work there .\nHow do you ensure IP resource management so they don't exhaust more IPs than needed.\nI expected these to be either platform engineer interview questions but I have never done any of this.\nI would just build the image,use the subnetwork provided to me and would usually be done",
              "score": 9,
              "created_utc": "2026-02-08 15:35:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4cxu6p",
                  "author": "West_Good_5961",
                  "text": "These arenâ€™t DE questions. Thatâ€™s cloud engineer territory. Theyâ€™re probably looking for a unicorn who can do 3 jobs at once and still not pay them enough.",
                  "score": 10,
                  "created_utc": "2026-02-09 01:54:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4cnju7",
                  "author": "sib_n",
                  "text": "Yeah, they need a platform/devops/cloud/network engineer to set up their data platform, before the data engineering starts and they are trying to take the shortcut to find someone who can do both. If you're not experienced in setting such as thing, I would move one. I feel like you will be studying for weeks to match the requirements, it's not just learning the Kubernetes basics, only for them to take a guy who has years of experience in cloud platform set up. They will eventually focus on data engineer hiring.",
                  "score": 5,
                  "created_utc": "2026-02-09 00:55:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o49lz26",
                  "author": "Darkendfearz",
                  "text": "That's actually wild. What kind of companies are you interviewing for?",
                  "score": 4,
                  "created_utc": "2026-02-08 15:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4a9c6t",
          "author": "tiredITguy42",
          "text": "Wait, there was a time when tech-stack did not change each few months?\n\nBut seriously. Fake it till you make it. All these technologies are easy to use when you are forced to work with them. Just watch some videos, so you have some idea what it is about.\n\nThere are differences in details for each company as the DevOps team is different in each place.",
          "score": 6,
          "created_utc": "2026-02-08 17:33:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bo7c8",
          "author": "turboDividend",
          "text": "data engineering is becoming dev/data ops it seems. you need to be a networking gguy and a pipeline guy. \n\nmost of the devops ppl i met were not developers, not knocking them but it wasnt what they were about.",
          "score": 3,
          "created_utc": "2026-02-08 21:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4d0ilb",
          "author": "calimovetips",
          "text": "you donâ€™t need deep kubernetes, just enough to run and debug jobs. airflow plus docker can be picked up in a few weeks, kubernetes basics in about a month if you practice consistently.",
          "score": 2,
          "created_utc": "2026-02-09 02:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dq9j8",
          "author": "EviliestBuckle",
          "text": "What is data stack these days?",
          "score": 1,
          "created_utc": "2026-02-09 04:35:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eavf3",
          "author": "al_tanwir",
          "text": "Binge watch a few Kubernetes tutorial on YouTube, and you're good to go. :)",
          "score": 1,
          "created_utc": "2026-02-09 07:22:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4onjgv",
          "author": "LilParkButt",
          "text": "The way I see things, data engineering is getting split into DataOps, Analytics Engineering, and MLOps.",
          "score": 1,
          "created_utc": "2026-02-10 21:03:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2gu5c",
      "title": "Should I prioritize easy/medium or hard questions from DataLemur as a new graduate?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/",
      "author": "SIumped",
      "created_utc": "2026-02-12 02:03:26",
      "score": 39,
      "num_comments": 8,
      "upvote_ratio": 0.85,
      "text": "Hi all, I'll be graduating June so I'm currently applying to data roles with previous data engineering internships at a T100 company. I've picked up DataLemur and I'm somewhat comfortable with all easy/medium questions listed. Should I walk through these again to ensure I am 100% confident in answering these, or should I move onto hard questions?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4xwal1",
          "author": "Specific-Mechanic273",
          "text": "You can move to hard as they'll strengthen the skills required in medium. Most DataLemur hard questions feel like they're combining a lot of concepts. Tbh in most interviews you're asked medium level questions. \n\nJust be sure you're able to answer these question patterns (copy pasted from my Notion, let me know if i should clarify something):\n\n\\- Ordinal & Ranking Patterns (first, second, third, latest X per group) -> row\\_number() + dense\\_rank() + rank()\n\n\\- Rolling / Sliding Aggregations (rolling x-day average, running total etc.) -> sum/avg/count window function + \"ROWS BETWEEN N PRECEDING AND CURRENT ROW)\n\n\\- LAG / LEAD Window Functions (year-over-year changes)\n\n\\- Metric by Dimension (e.g. revenue by department) -> GROUP BY + join\n\n\\- Self Joins (often used in hierarchies)\n\n\\- Anti joins (find what's missing)\n\n\\- Conditional aggregation (count(case when x = y then 1 end))\n\n\\- CTEs\n\n\\- Knowing functions to manipulate dates (get month/year from timestamp, date diff, add time, ...)\n\n  \nWith this you'll be able to answer 99% of all interview questions",
          "score": 10,
          "created_utc": "2026-02-12 06:47:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z8xef",
              "author": "NickSinghTechCareers",
              "text": "good overview of skills here!",
              "score": 1,
              "created_utc": "2026-02-12 13:39:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xbnxt",
          "author": "NickSinghTechCareers",
          "text": "Hi! DataLemur founder here â€“ glad to hear you've been grinding SQL & Python on the site. I think moving onto the hard questions is good, if you've already done the easy/medium problems. You can always re-visit the Mediums again, and try to speed through them, after going through a few dozen hard problems. You just might be surprised how much faster you can go, after practicing on harder problems, and getting better at pattern recognition. \n\nBesides DataLemur, I think having a proper project to talk about is also super important for Data Engineering interviews. Hopefully, this can be sourced from a past internship â€“ but if not, go make a real portfolio project that's end-to-end, deployed (with a live link), that's also key-word rich (so use AWS, PostgreSQL, Airflow, Python, etc.). ",
          "score": 22,
          "created_utc": "2026-02-12 04:03:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xfgzz",
              "author": "WildLandShark",
              "text": "Hey Nick! I recently went through some of your hard SQL questions on DataLemur in preparation for BI Engineer interview. The questions were super helpful for refreshing myself on some querying techniques that I don't use all that often. I ended up receiving an offer, so thank you for creating such a helpful resource.\n\nI'm wondering though, how do you source your questions? I'm especially curious as there are a wide variety of companies that are listed as question sources.",
              "score": 6,
              "created_utc": "2026-02-12 04:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y71z8",
                  "author": "dyogenys",
                  "text": "Is this whole thing an ad?",
                  "score": 12,
                  "created_utc": "2026-02-12 08:29:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4z8p24",
                  "author": "NickSinghTechCareers",
                  "text": "glad it was helpful. for question sources â€“ many people tell me them, and then I change up the details slightly to go around NDA / maintain privacy. with like 175k+ followers on linkedin, and 50k copies sold of my book, enough people just LinkedIn DM me or email me their interview experience, ask for advice, feedback, etc. I also do a ton of 1:1 coaching, where we also go through past interviews they've had, and seen where they struggled or could improve. \n\nfinally â€“ i got a ton of it from Glassdoor, Reddit, Blind, and Medium back when I started DataLemur a few years ago. ",
                  "score": 5,
                  "created_utc": "2026-02-12 13:38:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50zyev",
              "author": "w_savage",
              "text": "Is DataLemur a free site? I've never ran across it before.",
              "score": 1,
              "created_utc": "2026-02-12 18:45:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r08mq6",
      "title": "Explain ontology to a five year old",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r08mq6/explain_ontology_to_a_five_year_old/",
      "author": "ephemeral404",
      "created_utc": "2026-02-09 16:22:56",
      "score": 38,
      "num_comments": 22,
      "upvote_ratio": 0.91,
      "text": "Not absolutely to 5 yo but need your help explaining ontology in simpler words, to a non-native English speaker, a new engineering grad",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r08mq6/explain_ontology_to_a_five_year_old/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4h52d7",
          "author": "_OMGTheyKilledKenny_",
          "text": "You start with a controlled vocabulary for a set of values something can be.  Like Houston, Dallas, Austin are cities in Texas.  \n\nYou add a taxonomy on top of this to say Texas is a state and the aforementioned cities are located in the geographical and legal boundaries of the state ofTexas and another taxonomy that says Texas, California, Florida are states and are located is country named USA and follow/benefit from its federal laws. \n\nThen you can add an ontology that defines relationships between taxonomies, such as cities follow the laws of states and countries can enter free trade agreements, defensive cooperation with one another etc.  \n\nThen you can draw logical statements from these, like if John is a farmer in midlands, Texas he can sell beef to a company in Brazil free for tariffs. \n\nYou can build knowledge graphs on top of these ontologies that can ground LLMs into context specific answers.",
          "score": 43,
          "created_utc": "2026-02-09 18:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jtcbs",
              "author": "Leading-Inspector544",
              "text": "Is that not just a knowledge graph?",
              "score": 3,
              "created_utc": "2026-02-10 02:59:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k46jg",
                  "author": "Idiot_LevMyskin",
                  "text": "Ontology is the data model for Knowledge graph.",
                  "score": 12,
                  "created_utc": "2026-02-10 04:08:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gg3qz",
          "author": "ResidentTicket1273",
          "text": "In simple terms, an ontology is a map of \"types of things that exist\" and the kinds of relationships those things can be expected to have with one another.\n\nIn data engineering terms, it's a bit like a formalised conceptual data model where the concepts have defined expected relationships with one another.\n\nMore advanced ontologies can be constructed to accept fragmented or incomplete information and define rules to help infer other facts about the things referenced that aren't explicitly provided in the inputs.\n\nFor example, we might have a data stream that imports records about a person and their parents. We might define a relationship that says \"A sibling is defined as someone who shares the same parents.\" The ontology can then (given enough input data) infer these additional relationships logically, even though they've not been expressly provided by the data.",
          "score": 14,
          "created_utc": "2026-02-09 16:32:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4h0jxg",
              "author": "dyogenys",
              "text": "Piggybacking on the correct description to say, it's kind of like types in programming languages, except it's for diverse facts represented in machine readable syntax instead of just data types.",
              "score": 1,
              "created_utc": "2026-02-09 18:09:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iccvb",
          "author": "Mclovine_aus",
          "text": "To piggyback, who is using ontologies in their work. I only ever hear ontology brought up by data execs as a buzzword or golden future state.  But obviously thatâ€™s just my area of the world.",
          "score": 7,
          "created_utc": "2026-02-09 22:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4if2ts",
              "author": "pceimpulsive",
              "text": "Not my area but we are starting to work towards it \n\nTelco sector.\n\nWe are starting with knowledge graphs, ontology is next I think.",
              "score": 1,
              "created_utc": "2026-02-09 22:17:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4snjo8",
              "author": "Level-School-2022",
              "text": "It's becoming more popular because it's a key pillar of Palantir (Foundry and Other Platforms) which are hot right now.",
              "score": 1,
              "created_utc": "2026-02-11 13:28:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ig8oz",
          "author": "dudeaciously",
          "text": "Taxonomy is a system of naming things.  So we know how to name fighter jets as they are created.  And we will never call a fruit F-117.\n\nOntology says that given a lot of things in a system that have a reasonable set of names, we want to know what things are very similar, what are slightly similar, and what is very different.  They might be similar in some property like taste of fruits vs. crunchiness.\n\nSo, in the end, we can add new things with good system of names, and we will know what they are related to.  \n\nWe end up with groups of things that are of one category.  Then groups of groups, etc.",
          "score": 4,
          "created_utc": "2026-02-09 22:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4htn0x",
          "author": "iwantthisnowdammit",
          "text": "Types of things which exist and a verb to describe their relationship.",
          "score": 2,
          "created_utc": "2026-02-09 20:30:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lx8yi",
          "author": "tatum106",
          "text": "A structured, digital representation of your business",
          "score": 2,
          "created_utc": "2026-02-10 13:10:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m65k6",
          "author": "NotSure2505",
          "text": "[https://www.reddit.com/r/agiledatamodeling/comments/1r11hl2/what\\_is\\_ontology\\_eli5\\_please/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/agiledatamodeling/comments/1r11hl2/what_is_ontology_eli5_please/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
          "score": 2,
          "created_utc": "2026-02-10 14:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i0n0b",
          "author": "ChinoGitano",
          "text": "What Lecun is pushing about â€¦ *world model*, but domain-specific?",
          "score": 1,
          "created_utc": "2026-02-09 21:04:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k5p9w",
          "author": "one-step-back-04",
          "text": "When you say ontology, do you mean how things are categorized, like in knowledge graphs?",
          "score": 1,
          "created_utc": "2026-02-10 04:18:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gf4jc",
          "author": "frombsc2msc",
          "text": "What do you mean with ontology? Iâ€™ve never heard it be used in my domain at least?",
          "score": -1,
          "created_utc": "2026-02-09 16:28:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kme2m",
              "author": "DJ_Laaal",
              "text": "Very common in healthcare and other regulated industries like finance and asset management.",
              "score": 2,
              "created_utc": "2026-02-10 06:25:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ksu29",
                  "author": "frombsc2msc",
                  "text": "Ah oke! Thanks.",
                  "score": 1,
                  "created_utc": "2026-02-10 07:22:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r383ef",
      "title": "Is Microsoft OneLake the new lock-in?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r383ef/is_microsoft_onelake_the_new_lockin/",
      "author": "AwayCommercial4639",
      "created_utc": "2026-02-12 22:49:03",
      "score": 38,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I was running some tests on OneLake the other day and I noticed that its performance is 20-30% worse than ADLS. \n\nThey have these 2 weird APIs under the hood: Redirect and Proxy. Redirect is only available to Fabric engines and likely is some internal library for translating OneLake paths to ADLS paths. Proxy is for everything else (including 3rd party engines) and is probably just as it sounds some additional compute layer to hide direct access to ADLS.\n\nI also think that there may be some caching on Fabric side which is only working for Fabric engines...\n\nMy scenario - run a query from Snowflake or Spark k8s against an Iceberg table on ADLS and on OneLake. The performance is not the same! OneLake is always worse especially for tables with lots of files...\n\nSo here is my fear - OneLake is not ADLS. It is NOT operating as open storage. It is operating as a premium storage for Fabric and a sub optimal storage for everything else...\n\nJust use ADLS then.. Yes, we do. But every time I chat with our Microsoft reps they are pushing and pushing me to use OneLake. I am concerned that one day they will just deprecate ADLS in favour of OneLake.\n\nLook Fabric might be decent if you love Power BI, but our business runs on 2 clouds. We have transactional workloads on both, and no way are we going to egress all that data to one cloud or another for analytics. Hence we primarily run an open stack and some multi cloud software like Snowflake.\n\nWhat is wrong with ADLS? Why. do they keep pushing to OneLake? Is this is the next lock-in?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r383ef/is_microsoft_onelake_the_new_lockin/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o54w8tm",
          "author": "Tribaal",
          "text": "Yes, itâ€™s the next lock in. Thatâ€™s it.",
          "score": 32,
          "created_utc": "2026-02-13 09:16:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56693j",
              "author": "Brilliant-Gur9384",
              "text": ">Just use ADLS then.. Yes, we do. But every time I chat with our Microsoft reps they are pushing and pushing me to use OneLake. I am concerned that one day they will just deprecate ADLS in favour of OneLake.\n\nYes, this mirrored the OneDrive fiasco. Remember how it would re-install it every time you deleted it? I guess they finally stopped pushing it because it stayed off, but for a good 2 years, it was a battle to keep OneDrive off an ms machine.\n\n>Yes, itâ€™s the next lock in. Thatâ€™s it.\n\nBingo. If you know AI, then you know these big tech companies want control of data. That's the real value and they know it. The big winners are going to be people/companies who \"get\" this early and choose the right platforms because if you get stuck on a platform that changes agreements after the fact, that could be a major ouch.",
              "score": 6,
              "created_utc": "2026-02-13 14:43:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55onta",
          "author": "TowerOutrageous5939",
          "text": "Iâ€™m hope they push us. My CIO is already very much over MS I would have for this to be the tipping point",
          "score": 5,
          "created_utc": "2026-02-13 13:06:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57m5em",
          "author": "m1nkeh",
          "text": "Yes, itâ€™s massive lock in.. itâ€™s ADLS with shackles",
          "score": 2,
          "created_utc": "2026-02-13 18:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o563he7",
          "author": "TheRealStepBot",
          "text": "Fabric and one lake is crap. ðŸ“Ž",
          "score": 3,
          "created_utc": "2026-02-13 14:28:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57g6dz",
          "author": "Sea-Meringue4956",
          "text": "Onelake is on top of ADLS and yet costs 10x more the last time I checked. I dont think though that ADLS will stop to exist.",
          "score": 2,
          "created_utc": "2026-02-13 18:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55y9yx",
          "author": "thecoller",
          "text": "Not long ago, reading OneLake from non-Fabric computer was 3x more expensive than reading from Fabric. Hopefully customers keep up the pressure and keep choosing ADLS so the rest of the barriers come down.",
          "score": 1,
          "created_utc": "2026-02-13 14:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57m9pc",
              "author": "m1nkeh",
              "text": "Oh, is this no longer the case, the 3x thing?",
              "score": 1,
              "created_utc": "2026-02-13 18:53:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57u3qw",
                  "author": "thecoller",
                  "text": "Fortunately not, as of a couple of months ago",
                  "score": 1,
                  "created_utc": "2026-02-13 19:31:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56ues6",
          "author": "Low_Second9833",
          "text": "Trust your instincts.",
          "score": 1,
          "created_utc": "2026-02-13 16:39:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57to2a",
          "author": "Frosty-Practice-5416",
          "text": "So happy I don't have to think about that crap anymore",
          "score": 1,
          "created_utc": "2026-02-13 19:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57fqgh",
          "author": "thpeps",
          "text": "Hey - I'm a PM with OneLake, the performance you're experiencing is not expected, can you send me a DM so we can get in touch and I can help debug what you're seeing. Thanks!",
          "score": -2,
          "created_utc": "2026-02-13 18:22:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57mclg",
              "author": "m1nkeh",
              "text": "DM me if you need a referral out of MS âœŒï¸",
              "score": 2,
              "created_utc": "2026-02-13 18:53:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qzxr0f",
      "title": "How are you debugging and optimizing slow Apache Spark jobs without hours of manual triage in 2026?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1qzxr0f/how_are_you_debugging_and_optimizing_slow_apache/",
      "author": "AdOrdinary5426",
      "created_utc": "2026-02-09 07:19:23",
      "score": 37,
      "num_comments": 10,
      "upvote_ratio": 0.95,
      "text": "We've seen Spark jobs dragging on forever lately: stages with skew, small files, memory spills, or bad shuffles that take hours to pinpoint, even with the default Web UI. We stare at operator trees and executor logs, guess at bottlenecks, then trial-and-error code changes that sometimes make it worse.\n\nOnce the job is running in production, the standard Spark UI is verbose and overwhelming, leaving us blind to real-time issues until it's too late.\n\nKey gaps frustrating us right now\n\n*  Default Spark UI hard to read with complex plans and no clear heat maps for slow stages.\n*  No automatic alerts on common perf killers like small files IO, data skew, or partition imbalances during runs.\n*  Debugging relies on manual log parsing and guesswork instead of actionable insights or code suggestions.\n*  No easy way to rank issues by impact (e.g., cost or runtime delta) across jobs or clusters. Team spends too much time firefighting instead of preventing repeats in future pipelines.\n\nSpark is our core engine but we're still debugging it like it's 2014. Anyone running large-scale Spark (Databricks, EMR, on-prem) solved this at scale without dedicated perf engineers?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1qzxr0f/how_are_you_debugging_and_optimizing_slow_apache/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4eedot",
          "author": "noobcoder17",
          "text": "I'll give it to you straight, YOU DON'T.\n\n\nThere's all these talk, all this theory about spark optimizations and debugging, yada yada.Â \n\n\nIn reality you have to spend those hours in triage.Â \n\n\nBut usually once you get a hang of it, you shouldn't be spending too many hours next time.Â ",
          "score": 35,
          "created_utc": "2026-02-09 07:55:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jmx7c",
              "author": "ubelmann",
              "text": "Some of it is taking time to adhere to best practices in the first place.  You could argue it's overkill in some cases, but being proactive about things like too many small files, being thoughtful about partitions and such can go a really long way.\n\nBut I agree that once you've seen a few bad jobs, you get better at finding the root causes more quickly, or maybe I should say less slowly.",
              "score": 1,
              "created_utc": "2026-02-10 02:21:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jolho",
                  "author": "noobcoder17",
                  "text": "Yep! it's always \"BEST\" to follow best practices. On the contrary, what i've noticed in my experience working for many years in this field, engineers are rushed to deliver and the people who are taking these arch decisions are also in a rush and juggling around multiple projects and they give us the best practice that they know of which may not always work in production. \n\nI've found, no matter how much we do, things will break at one point and that's the real opportunity to learn and optimize.   \n  \nHeck i used to think big tech is immune to this, remember times when google, chatgpt went down? or the Korian guy making this \"gangnam style\" video and breaking youtubes views pipeline?- all were learning opportunities and DE is same as such. ",
                  "score": 1,
                  "created_utc": "2026-02-10 02:31:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4enuhm",
          "author": "not-an-AI-bot",
          "text": "Follow good practices, avoid functions that are already documented for slow performance, avoid too much custom code, use modularity, specially the ones you know perform very well. And keep logs for steps/substeps so you can monitor and adjust where necessary.",
          "score": 5,
          "created_utc": "2026-02-09 09:29:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ji1vi",
          "author": "MonochromeDinosaur",
          "text": "You donâ€™t this is unfortunately the reality of self managed spark.\n\nLooking at the SQL execution DAG in the spark UI for long steps and finding the longest duration ones or bottle necks is what I would do for performance. It was the only useful view\n\nThankfully I moved on to Snowflake and hoping to never look back.",
          "score": 3,
          "created_utc": "2026-02-10 01:53:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eazm1",
          "author": "Efficient_Agent_2048",
          "text": "Well, I would say the key is moving from reactive debugging to proactive instrumentation. Integrate structured logging and metrics, for example SparkListener events, Ganglia, Prometheus, so you get live alerts for small files, skew, or memory spills. Use automated stage analysis to highlight the top N bottlenecks by runtime or cost impact. Combine that with CI tests for job profiles, so regressions get flagged before hitting prod. It is not magic, but layering metrics, alerts, and profiling drastically reduces the manual triage.",
          "score": 5,
          "created_utc": "2026-02-09 07:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f75os",
              "author": "One_Citron_4350",
              "text": "What do you mean by automated stage analysis? How do you do it?I'm curious, is it something you implemented internally because I don't know of such a tool.",
              "score": 5,
              "created_utc": "2026-02-09 12:22:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4em6fu",
              "author": "DeepFryEverything",
              "text": "Suggestions for tooling? Our platform team has set up Grafana, but I am not sure how to plug that into Databricks-clusters.",
              "score": 1,
              "created_utc": "2026-02-09 09:12:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}