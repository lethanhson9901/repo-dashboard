{
  "metadata": {
    "last_updated": "2026-01-02 16:25:50",
    "time_filter": "week",
    "subreddit": "dataengineering",
    "total_items": 50,
    "total_comments": 671,
    "file_size_bytes": 763842
  },
  "items": [
    {
      "id": "1q034du",
      "title": "Senior Data Engineer Experience (2025)",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q034du/senior_data_engineer_experience_2025/",
      "author": "ElegantShip5659",
      "created_utc": "2025-12-31 03:55:05",
      "score": 689,
      "num_comments": 95,
      "upvote_ratio": 0.99,
      "text": "I recently went through several loops for Senior Data Engineer roles in 2025 and wanted to share what the process actually looked like. Job descriptions often don’t reflect reality, so hopefully this helps others.\n\nI applied to 100+ companies, had many recruiter / phone screens, and advanced to full loops at the companies listed below.\n\n# Background\n\n* Experience: 10 years (4 years consulting + 6 years full time in a product company)\n* Stack: Python, SQL, Spark, Airflow, dbt, cloud data platforms (AWS primarily)\n* Applied to mid large tech companies (not FAANG-only)\n\n# Companies Where I Attended Full Loops\n\n* Meta\n* DoorDash\n* Microsoft\n* Netflix\n* Apple\n* NVIDIA\n* Upstart\n* Asana\n* Salesforce\n* Rivian\n* Thumbtack\n* Block\n* Amazon\n* Databricks\n\n# Offers Received : SF Bay Area\n\n* **DoorDash** \\-  Offer not tied to a specific team (**ACCEPTED**)\n* **Apple** \\- Apple Media Products team\n* **Microsoft** \\- Copilot team\n* **Rivian** \\- Core Data Engineering team\n* **Salesforce** \\- Agentic Analytics team\n* **Databricks** \\- GTM Strategy & Ops team\n\n# Preparation & Resources\n\n1. **SQL & Python**\n   * Practiced complex joins, window functions, and edge cases\n   * Handling messy inputs primarily json or csv inputs.\n   * Data Structures manipulation\n   * Resources: stratascratch & leetcode\n2. **Data Modeling**\n   * Practiced designing and reasoning about fact/dimension tables, star/snowflake schemas.\n   * Used AI to research each company’s business metrics and typical data models, so I could tie Data Model solutions to real-world business problems.\n   * Focused on explaining trade-offs clearly and thinking about analytics context.\n   * Resources: AI tools for company-specific learning\n3. **Data System Design**\n   * Practiced designing pipelines for batch vs streaming workloads.\n   * Studied trade-offs between Spark, Flink, warehouses, and lakehouse architectures.\n   * Paid close attention to observability, data quality, SLAs, and cost efficiency.\n   * Resources: *Designing Data-Intensive Applications* by Martin Kleppmann, *Streaming Systems* by Tyler Akidau, YouTube tutorials and deep dives for each data topic.\n4. **Behavioral**\n   * Practiced telling stories of ownership, mentorship, and technical judgment.\n   * Prepared examples of handling stakeholder disagreements and influencing teams without authority.\n   * Wrote down multiple stories from past experiences to reuse across questions.\n   * Practiced delivering them clearly and concisely, focusing on impact and reasoning.\n   * Resources: STAR method for structured answers, mocks with partner(who is a DE too), journaling past projects and decisions for story collection, reflecting on lessons learned and challenges.\n\n**Note:** Competition was extremely tough, so I had to move quickly and prepare heavily. My goal in sharing this is to help others who are preparing for senior data engineering roles.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q034du/senior_data_engineer_experience_2025/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwuuw6d",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-31 03:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuwh9h",
          "author": "smartdarts123",
          "text": "Did most of those places put you through the standard leetcode style coding screens?",
          "score": 53,
          "created_utc": "2025-12-31 04:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuww26",
              "author": "ElegantShip5659",
              "text": "NVIDIA, Block and Netflix were typical LC. The rest were mostly Data Structure Manipulation, cleaning up messy JSON and deriving few aggregations. And typical SQL style Q's",
              "score": 82,
              "created_utc": "2025-12-31 04:07:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuy3h8",
                  "author": "smartdarts123",
                  "text": "That's cool, thanks for sharing your experience. Did you do any specific prep for coding screens or did you find that your existing experience was sufficient to feel your way through the problems?\n\nFor example, I'm pretty sure I'd breeze through json parsing, data manipulations, etc, but for things that are more leetcode style, I need to study.",
                  "score": 15,
                  "created_utc": "2025-12-31 04:15:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuwwfv",
              "author": "Odd_Strength_9566",
              "text": "+1",
              "score": 0,
              "created_utc": "2025-12-31 04:08:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuz1ca",
          "author": "discussitgal",
          "text": "Can you share some examples of data structure manipulations? Was it basic array dicts and pandas?",
          "score": 28,
          "created_utc": "2025-12-31 04:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuzp7n",
              "author": "ElegantShip5659",
              "text": "Yes. Mostly arrays and dicts.",
              "score": 28,
              "created_utc": "2025-12-31 04:26:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwtox8",
                  "author": "Seven_Minute_Abs_",
                  "text": "Can you give an example of?",
                  "score": 4,
                  "created_utc": "2025-12-31 13:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuxy2o",
          "author": "on_the_mark_data",
          "text": "Those are all great offers! Beyond TC, is there a reason why you chose Doordash over others?",
          "score": 21,
          "created_utc": "2025-12-31 04:14:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuyz6o",
              "author": "ElegantShip5659",
              "text": "TC definitely played a role, to be honest, but what really drew me to the team I got matched to were the growth opportunities and the kind of projects and work the team was doing. It just felt like the right fit for me.",
              "score": 23,
              "created_utc": "2025-12-31 04:21:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvbgy7",
          "author": "Sad-Tomato3450",
          "text": "Impressive. For the above listed companies did you apply directly or via networking. Wanted to understand the schematics given the market is overwhelmingly saturated with more applicants than there are openings.\nCongratulations for the new beginnings !!!",
          "score": 18,
          "created_utc": "2025-12-31 05:49:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvmnqj",
              "author": "ElegantShip5659",
              "text": "It’s a mix of direct applications and networking via linkedin. For me direct applications worked for the most part.",
              "score": 11,
              "created_utc": "2025-12-31 07:22:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvnzv9",
                  "author": "Sad-Tomato3450",
                  "text": "Nice to hear that direct application is still working wonders. Let me know if you will be open to do a resume overview for me. I am trying to get an opinion for someone who is battle tested :)",
                  "score": 7,
                  "created_utc": "2025-12-31 07:34:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv0v6c",
          "author": "MichelangeloJordan",
          "text": "Congrats OP! New year, new job. Hope both are good!",
          "score": 14,
          "created_utc": "2025-12-31 04:34:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv1jf2",
              "author": "ElegantShip5659",
              "text": "Thank you",
              "score": 3,
              "created_utc": "2025-12-31 04:38:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvi3id",
          "author": "codemega",
          "text": "I interviewed with DoorDash a couple years ago and I struggled with two rounds:\n\nData modeling - they gave me some weird metric that I needed to build tables for. It was difficult to even understand what the metric was. I was a bit lost on it.\n\nSystem design - the interviewer asked me to design a url shortener. This is a classic SWE sys design question but I had little idea of how to do it as a data engineer.\n\nAnyway, interviews can vary a lot depending on who you get matched up with. Congrats for getting through.",
          "score": 7,
          "created_utc": "2025-12-31 06:42:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvojc7",
              "author": "ElegantShip5659",
              "text": "Agreed, my DoorDash interview was generic. I got matched with a team after clearing the interview. I guess things may have changed or you may have interviewed for a team with a specific need?",
              "score": 5,
              "created_utc": "2025-12-31 07:39:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww8eu6",
                  "author": "R0kies",
                  "text": "Good stuff this post buddy! Regarding data modeling, you mentioned it revolved around metrics, as commenter mentioned it too, it's seems usual. \n\nHow does it go around? They give you facts with 100 columns and 20 dimensions and you are supposed to pick what columns are needed and in what relationship? \n\nWas it mostly OLAP or OLTP too?",
                  "score": 2,
                  "created_utc": "2025-12-31 10:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv2mww",
          "author": "wiseyetbakchod",
          "text": "This helps, thanks a lot.",
          "score": 6,
          "created_utc": "2025-12-31 04:45:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv32gt",
          "author": "Pristine-Trainer7109",
          "text": "Congrats on your offers! Can you share some of the data modeling questions?",
          "score": 6,
          "created_utc": "2025-12-31 04:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv3t1u",
              "author": "ElegantShip5659",
              "text": "Almost all companies came up with a typical \"design a data model for a xxxx( for ex: music streaming service)\"  Focus was primarily on agreeing upon metrics with the interviewer, defining core objects, building the facts and dimensions, writing SQL at the end to achieve those metrics. Sometimes had to draw a visual to represent the metric. A few companies started with providing list of metrics directly and had to work around the model accordingly.",
              "score": 24,
              "created_utc": "2025-12-31 04:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwv5vva",
                  "author": "sureveS_Snape",
                  "text": "Congrats on your offer(s). Do you have any resource recommendations for data modeling?",
                  "score": 4,
                  "created_utc": "2025-12-31 05:08:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv96lq",
          "author": "leonseled",
          "text": "Any other resources for streaming workloads? I’m a mid-level engineer looking to upskill in 2026. One of my goals is adding streaming proficiency to my toolbelt.",
          "score": 4,
          "created_utc": "2025-12-31 05:32:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvn417",
              "author": "ElegantShip5659",
              "text": "Tech blogs, youtube videos. I focussed on Kafka and Flink architecture. Zach Wilson and Darshil Parmar on Youtube",
              "score": 5,
              "created_utc": "2025-12-31 07:26:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv5cm0",
          "author": "cmcclu5",
          "text": "Excellent insights. I don’t know how you’re getting all these interviews, though. I have a decade+ in the same tech stacks plus others and can’t even get a returned phone call. Enjoy that new job and paycheck! Good New Years’ present!",
          "score": 5,
          "created_utc": "2025-12-31 05:04:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnmyc",
              "author": "ElegantShip5659",
              "text": "I completely agree, the market has been brutal even for very strong profiles. Honestly, i think i was just lucky.",
              "score": 3,
              "created_utc": "2025-12-31 07:31:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv0du5",
          "author": "Garcon_sauvage",
          "text": "TC?",
          "score": 2,
          "created_utc": "2025-12-31 04:30:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv1tq5",
              "author": "ElegantShip5659",
              "text": "DM'd",
              "score": 4,
              "created_utc": "2025-12-31 04:40:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwytmhk",
                  "author": "x1084",
                  "text": "I'm curious as well, if you don't mind sharing. I'm also in a HCOL area, albeit not Bay Area level.",
                  "score": 1,
                  "created_utc": "2025-12-31 19:48:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwv1jky",
              "author": "Funny-Message-9282",
              "text": "Total compensation. It includes your base salary + Bonus + RSUs (Restricted Stock Units)",
              "score": 0,
              "created_utc": "2025-12-31 04:38:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv1cze",
          "author": "DRUKSTOP",
          "text": "Ive interviewer at Meta, DD, Stripe, TikTok, and Amazon over the last 2 years and they all dus typical Leetcode. So interesting to see some may have changed.",
          "score": 2,
          "created_utc": "2025-12-31 04:37:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv1t3d",
              "author": "ElegantShip5659",
              "text": "Meta and DD have the same format from years I suppose. The others are mostly team dependent in my exp. Amazon has made some changes - they now do a 75 min 1st round covering coding, data modeling, system design and LP all in 75 min. And then a 4-5 team loop",
              "score": 6,
              "created_utc": "2025-12-31 04:40:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwuyml",
                  "author": "DRUKSTOP",
                  "text": "So meta was 5 python + 5 SQL leetcode, all in 50 minutes?",
                  "score": 2,
                  "created_utc": "2025-12-31 13:42:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwv28le",
              "author": "vuachoikham167",
              "text": "Just curious, do those companies ever ask you hard Leetcode Qs or mostly easy-medium?",
              "score": 3,
              "created_utc": "2025-12-31 04:43:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv2pen",
                  "author": "ElegantShip5659",
                  "text": "For data engineering, I wouldn’t expect anything beyond medium, though Netflix and NVIDIA asked me hard questions.",
                  "score": 10,
                  "created_utc": "2025-12-31 04:46:25",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv91k6",
          "author": "Pranu12",
          "text": "OP, Congratulations on your new offers. Really impressed with the amount of hardwork that went on during your preparation. I'm a 4 year experienced person who really wants to secure a job in a product based company. Could you please guide me PLEASEEE!!",
          "score": 2,
          "created_utc": "2025-12-31 05:31:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo05p",
              "author": "ElegantShip5659",
              "text": "Sure i'll DM",
              "score": 2,
              "created_utc": "2025-12-31 07:34:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvdki2",
          "author": "verus54",
          "text": "Any recommendations on how to get through on the resume screen? I seem to be getting auto rejected by bigger companies and I get plenty of interviews at smaller companies. 3YOE consulting + 1YOE on product.",
          "score": 2,
          "created_utc": "2025-12-31 06:05:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnduy",
              "author": "ElegantShip5659",
              "text": "Only thing i did outside of normal was optimize my resume to include keywords to match ATS systems. I used Jobscan and ChatGpt",
              "score": 3,
              "created_utc": "2025-12-31 07:28:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvgwdf",
          "author": "IcyRashid",
          "text": "Congratulations OP. This is a constructive post. Did any of those companies ask coding questions using Spark, or were they only plain SQL and Python questions?",
          "score": 2,
          "created_utc": "2025-12-31 06:32:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvoeyx",
              "author": "ElegantShip5659",
              "text": "Mostly plain SQL and Python. Upstart and Salesforce did specifically ask to write code using PySpark.",
              "score": 3,
              "created_utc": "2025-12-31 07:38:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvhik7",
          "author": "LelouchYagami_",
          "text": "Really cool of you for sharing.",
          "score": 2,
          "created_utc": "2025-12-31 06:37:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwviv85",
          "author": "XcytekOfficial",
          "text": "I appreciate this entry. Really helpful.",
          "score": 2,
          "created_utc": "2025-12-31 06:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvo7yf",
          "author": "RDTIZFUN",
          "text": "Congrats and thanks for the post (would be cool if you could add a rough estimate +/- $10k-$20k of actual TC offers from those companies and whether they were remote or hybrid/onsite, thanks).",
          "score": 2,
          "created_utc": "2025-12-31 07:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvspyz",
          "author": "Alternative-Guava392",
          "text": "Legend run of interviews! Congratulations and thanks for the notes.",
          "score": 2,
          "created_utc": "2025-12-31 08:18:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxlcqd",
          "author": "PipelineInTheRain",
          "text": "Congrats on receiving/accepting the offer! Out of curiosity, how much do you feel your experience with AWS played a part in your interviews?",
          "score": 2,
          "created_utc": "2025-12-31 16:05:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxvbej",
          "author": "MassyKezzoul",
          "text": "Thanks for sharing, this help a lot. Can you share an estimate of the TC offers of those companies ?",
          "score": 2,
          "created_utc": "2025-12-31 16:55:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy3q51",
          "author": "Foreign_Yam3729",
          "text": "Thanks for sharing !! Any suggested tutorials/links for the below things:\n1. System design round ( esp for data)\n2. Data modelling apart from Kimball/chatgpt as mentioned earlier",
          "score": 2,
          "created_utc": "2025-12-31 17:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwykwck",
          "author": "Pucci800",
          "text": "Congratulations! This is really impressive and helps a ton.",
          "score": 2,
          "created_utc": "2025-12-31 19:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv88p3",
          "author": "Deiice",
          "text": "Which ressource(s) helped you the most to prepare for these?",
          "score": 2,
          "created_utc": "2025-12-31 05:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvqz8m",
              "author": "ElegantShip5659",
              "text": "ChatGPT and Youtube for Product Sense, Design and Data Modeling. LC and Stratascratch for Coding.",
              "score": 1,
              "created_utc": "2025-12-31 08:02:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvkg1f",
          "author": "Impossible-Appeal660",
          "text": "OP, What's the TC offered (if you dont mind sharing)?",
          "score": 2,
          "created_utc": "2025-12-31 07:02:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvolb9",
              "author": "ElegantShip5659",
              "text": "TC is 430 after final negotiations.",
              "score": 15,
              "created_utc": "2025-12-31 07:40:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv33yq",
          "author": "Realistic_Ad_5409",
          "text": "Anyone has experience with Capitalone powerday for lead data engineer role they can share?",
          "score": 1,
          "created_utc": "2025-12-31 04:49:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvg5rh",
          "author": "Initial_Response_799",
          "text": "Hey just a doubt \nI’m a junior data analyst trying to pivot into Data Engineering. Do u have any advice for me?\nOr is it very difficult to switch from here ?",
          "score": 1,
          "created_utc": "2025-12-31 06:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo5ls",
              "author": "ElegantShip5659",
              "text": "Industry now has a new role Data Analytics Engineer which is very close to Data Analyst. Have you had a chance to explore those roles?",
              "score": 3,
              "created_utc": "2025-12-31 07:35:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvolrp",
                  "author": "Initial_Response_799",
                  "text": "I’ve heard about analytics engineer but haven’t seen a lot of openings the thing is tbh I don’t like being an analyst I like the building thinking of systems kinda thing. I had to take this cus this was the only role I was close to getting at the time so looking to switch into something more engineering oriented",
                  "score": 1,
                  "created_utc": "2025-12-31 07:40:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvgq7s",
          "author": "ChainEnvironmental58",
          "text": "Congrats! Any specific resource or platform you recommend or any guide for Python preparation specifically for DEs? thanks",
          "score": 1,
          "created_utc": "2025-12-31 06:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvoaf4",
              "author": "ElegantShip5659",
              "text": "I have hands on python experience in my job, so outside of that I practiced LC Easy and Medium.",
              "score": 2,
              "created_utc": "2025-12-31 07:37:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvtzmd",
          "author": "BusinessRoyal9160",
          "text": "Thanks for the detailed information.\n\nI am on the same boat as you and I am struggling to find good resources for Data System Design. Could you please share the resources e.g. YouTube playlists which you followed?",
          "score": 1,
          "created_utc": "2025-12-31 08:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvujof",
          "author": "Khazard42o",
          "text": "Congrats! What resources did you use for product sense and whats your approach (and resources if any) for communication with stakeholders of different technical skills?",
          "score": 1,
          "created_utc": "2025-12-31 08:35:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvywyi",
          "author": "xorgeek",
          "text": "Congratulations on all ur offers.\n\nCould share some concrete resources of data system design",
          "score": 1,
          "created_utc": "2025-12-31 09:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvz0je",
          "author": "FuckTheStateofOhio",
          "text": "Just out of curiosity, what did your experience in consulting look like? Were you client facing? Was your firm primarily focused on data projects? Also feel free to DM me if you don't feel like disclosing this info publicly.",
          "score": 1,
          "created_utc": "2025-12-31 09:18:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvz2dt",
          "author": "ReginaldLlama3",
          "text": "Congrats. Happy New Year indeed",
          "score": 1,
          "created_utc": "2025-12-31 09:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww81jt",
          "author": "Constant_Vegetable13",
          "text": "Hey Congartulations! What was your TC? Can you please DM me? Thanks.",
          "score": 1,
          "created_utc": "2025-12-31 10:43:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwmaoh",
          "author": "soh219",
          "text": "Congrats, how did you structure your resume, I’ve been consulting for the last 4 yrs and I’m about to enter the job market",
          "score": 1,
          "created_utc": "2025-12-31 12:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwvqg3",
          "author": "ForPosterS",
          "text": "Congrats! Did you get any questions around cloud/AWS/data services in your interviews or was it largely around Python, SQL and data modeling? Also, in Python topics would you recommend looking into python specifically for someone who is preparing for interviews? Should the focus be more on data manipulation like arrays, dicts, pyspark or object oriented programming?\n\nAlso, did any place expect data bricks?",
          "score": 1,
          "created_utc": "2025-12-31 13:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwy8n0",
          "author": "m98789",
          "text": "What is your education background?",
          "score": 1,
          "created_utc": "2025-12-31 14:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx0jqz",
          "author": "CometChaserStarGazer",
          "text": "Congratulations OP! Could you share some prep materials for System Design? I recently interviewed at 3 companies from your list and I feel like the system design round was my weakest.",
          "score": 1,
          "created_utc": "2025-12-31 14:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx31cj",
          "author": "mindwrapper13",
          "text": "Wow this is amazing! Congrats! How much time did it take to prepare all this? What about topics like Spark etc ? Do you already know them from experience?",
          "score": 1,
          "created_utc": "2025-12-31 14:30:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx8u7w",
          "author": "halfrightface",
          "text": "nice notes and congrats. i've been doing SDE rounds lately and this is pretty similar to my experiences as well. were you going from SDE to SDE and looking for a pay bump or from DE to SDE? also did you apply to any analytics engineering roles?",
          "score": 1,
          "created_utc": "2025-12-31 15:02:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx9n68",
          "author": "_Marwan02",
          "text": "Hello,\n\nDid you have any DSA rounds? If so, how difficult were they?\nFor the SQL test, was it more LeetCode-style questions or real-world scenarios?",
          "score": 1,
          "created_utc": "2025-12-31 15:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0azs7",
          "author": "noobcoder17",
          "text": "This is very helpful OP. Happy new year and wishing you the best in your new role.   \nAre you on a work VISA or citizen? Asking to gauge the market for people on a work visa.",
          "score": 1,
          "created_utc": "2026-01-01 00:53:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0cc0v",
          "author": "siav8",
          "text": "TC?",
          "score": 1,
          "created_utc": "2026-01-01 01:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0wso7",
          "author": "solo_stooper",
          "text": "Wow. Nice! Congrats! Were your 4 years of consulting were about tech consulting? I have a somewhat similar background with 3+ years in civil engineering consulting before switching to tech. Also no CS bachelor’s but a masters in analytics. ",
          "score": 1,
          "created_utc": "2026-01-01 03:17:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0x0t3",
          "author": "solo_stooper",
          "text": "Curious about your examples of handling stakeholder disagreements and influencing teams without authority. Genuinely interested even for my personal development!",
          "score": 1,
          "created_utc": "2026-01-01 03:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1hg4l",
          "author": "GMUsername",
          "text": "Hey! Appreciate the post and the resources you used to study. Just wondering if the offers you received were based on location? Any offers remote? Not looking to move from my current location",
          "score": 1,
          "created_utc": "2026-01-01 05:55:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx26kag",
          "author": "Tall_Working_2146",
          "text": "Man congratulations on your new job, would you be free in mentoring an engineering student graduating in 1.5 years? I would like to spend it working on specializing in data/cloud although I have a roadmap for it being under the guidance of an expert sure is rewarding, won't take much of your time promise and I promise to be as teachable as clay.",
          "score": 1,
          "created_utc": "2026-01-01 10:09:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4xbyv",
          "author": "Happy_guy_1980",
          "text": "How much were these places paying for Sr Data Engineers?",
          "score": 1,
          "created_utc": "2026-01-01 20:35:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx65nsc",
          "author": "No_Cat_8466",
          "text": "First of all, congratulations on your offers! Thanks so much for sharing this it's an absolute gold mine. I've been trying to figure out a plan to pivot for the 2026 hiring cycle (if there even is one), so this is super helpful.\n\nReally glad to hear that direct applications are still effective! Would you consider doing a post about your resume and how you approached it? I think a lot of people would find that valuable.",
          "score": 1,
          "created_utc": "2026-01-02 00:33:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6r1qm",
          "author": "kushagraketo21",
          "text": "Hey thats a very elaborate post, puts things into perspective for someone preparing for a similar role.",
          "score": 1,
          "created_utc": "2026-01-02 02:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvhd7a",
          "author": "next-strangr",
          "text": "thanks , this is helpful. Please check dm",
          "score": 1,
          "created_utc": "2025-12-31 06:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwya1p3",
          "author": "Available_Fig_1157",
          "text": "How many experiences do you have ?",
          "score": 0,
          "created_utc": "2025-12-31 18:07:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwyar8n",
          "author": "TheITGuy93",
          "text": "+1",
          "score": 0,
          "created_utc": "2025-12-31 18:11:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv7t34",
      "title": "Am I crazy or is kafka overkill for most use cases?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pv7t34/am_i_crazy_or_is_kafka_overkill_for_most_use_cases/",
      "author": "Vodka-_-Vodka",
      "created_utc": "2025-12-25 07:14:02",
      "score": 260,
      "num_comments": 132,
      "upvote_ratio": 0.95,
      "text": "Serious question because I feel like I'm onto something.\n\nWe're processing maybe 10k events per day. Someone on my team wants to set up a full kafka cluster with multiple servers, the whole thing. This is going to take months to set up and we'll need someone dedicated just to keep it running.\n\nOur needs are pretty simple. Receive data from a few services, clean it up, store in our database, send some to an api. That's it.\n\nCouldn't we just use something simpler? Why does everyone immediately jump to kafka like it's the only option?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pv7t34/am_i_crazy_or_is_kafka_overkill_for_most_use_cases/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvua8qn",
          "author": "IDoCodingStuffs",
          "text": "Because resume driven development, of course.",
          "score": 405,
          "created_utc": "2025-12-25 07:37:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv6ym7",
              "author": "Polus43",
              "text": "Exactly this. Quite literally destroying my firm right now. \n\nNon-stop moving data back and forth between new applications with data quality, errors and pipeline failures increasing with every new project.\n\nLike being paid millions to move furniture from Dallas to New York. Then New York back to Dallas. Then Dallas back to New York.",
              "score": 80,
              "created_utc": "2025-12-25 13:16:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvw50md",
                  "author": "ojedaforpresident",
                  "text": "Don’t forget to paint it in Dallas and sand it all off again in Sacramento on the way back to New York.",
                  "score": 30,
                  "created_utc": "2025-12-25 17:02:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvy6wvu",
                  "author": "beefz0r",
                  "text": "Combine that with operations that are focused on getting tickets out of their queue rather than actually trying to provide (long term) solutions. Not blaming engineers per se, rather management with their SLA boners\n\nHow many times I looked into an issue myself, so I could pinpoint exactly where *they* need to look",
                  "score": 3,
                  "created_utc": "2025-12-26 00:33:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvucl4c",
              "author": "ocean_800",
              "text": "I mean like what's wrong with that? Gotta keep your skills sharp in this world",
              "score": 21,
              "created_utc": "2025-12-25 08:02:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuk3ma",
                  "author": "mclovin12134567",
                  "text": "By failing in the most important skill of problem solving?",
                  "score": 36,
                  "created_utc": "2025-12-25 09:24:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvueqrt",
                  "author": "exergy31",
                  "text": "Skills, that as established in the context of this post, are usually not relevant for the job.\nAnd if u need to flash Kafka to pass the interviews, there are other problems.\nIts an abuse of trust by the team to push for things you know are not needed",
                  "score": 35,
                  "created_utc": "2025-12-25 08:25:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwamnu",
                  "author": "IDoCodingStuffs",
                  "text": "You also need to maintain your professionalism and ethics. \n\nManipulating your employer/client/customer to pay you 10 times the cost of what they actually need just for your own learning goes against those.",
                  "score": 11,
                  "created_utc": "2025-12-25 17:35:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwpbkr",
                  "author": "fresh4days",
                  "text": "Then learn Kafka on the side, if you can’t address the problem with the appropriate solution and use some tool that is hot I am gonna think you don’t know what you’re doing and you just follow trends",
                  "score": 1,
                  "created_utc": "2025-12-25 19:00:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu92vs",
          "author": "GDangerGawk",
          "text": "If you need few months to set up a kafka cluster, it is probably an overkill for your organization.",
          "score": 231,
          "created_utc": "2025-12-25 07:25:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvueh34",
              "author": "GreyHairedDWGuy",
              "text": "My thoughts exactly",
              "score": 31,
              "created_utc": "2025-12-25 08:22:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvuil7f",
              "author": "ings0c",
              "text": "And also makes the value proposition of a cloud solution like Confluent cloud or MSK much more attractive.\n\nFWIW I don’t trust MSK. With the lambda integration it would get stuck consuming messages, silently. AWS support were useless and I ended up rolling my own consumer using the CLI. My experiences with Confluent cloud have only been positive.\n\nUnless you really like managing your own infrastructure for some reason, just use an off-the-shelf managed offering.\n\nThat said, the volume here is tiny - it’s one event every 10 seconds - unless it’s all burst traffic just make a PUT /api/events/{id} route and call it a day. You can still “do event driven architecture” like that and it’s vastly less complex.",
              "score": 35,
              "created_utc": "2025-12-25 09:07:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyi8il",
              "author": "laStrangiato",
              "text": "I would say it is a U curve. If you are small it takes months. If you are massive enterprise it takes months.",
              "score": 3,
              "created_utc": "2025-12-26 01:49:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvjt8l",
              "author": "Prinzka",
              "text": "I suspect installing notepad on their laptops might be too much work for them",
              "score": 2,
              "created_utc": "2025-12-25 14:51:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuckd6",
          "author": "canihelpyoubreakthat",
          "text": "I spent multiple years on a team responsible for self hosted Kafka clusters ingesting millions of messages/ ~10s of GB per second. We were a bit lean but we probably had 25% of an engineer dedicated full time.\n\nMonths to set up a cluster sounds pretty insane, but not sure if you mean eng time or calendar time. It will definitely take some dedicated time, but at your scale would also be insane to have a full time engineer manage.\n\nI second one of the other comments suggesting some managed service. Even sns+sqs.\n\nI think Kafka could be fine, but it sounds like you're not confident in your orgs infrastructure capabilities.\n\nEdit: followup thought after re-reading the post. Maybe a simple always on RPC service is a better fit here. Extremely simple, totally reasonable for thousands of requests per day. Sprinkle some request retry policy in there and you have a highly reliable system.",
          "score": 61,
          "created_utc": "2025-12-25 08:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw08e8a",
              "author": "Hot-Profession4091",
              "text": "You have a 1/4 person dedicated _now_ but you’ve already battle hardened your deployment. I’m willing to bet it took an entire FTE or more for quite a long time before your company was able to scale back on the labour.",
              "score": 3,
              "created_utc": "2025-12-26 10:42:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw22dx9",
                  "author": "canihelpyoubreakthat",
                  "text": "Not entirely wrong, but we're dealing with multiple clusters and dozens of brokers each. The scale OP is talking about should be a matter of days to set up, especially now in 2025. It also depends on your existing infrastructure and eng practices, of course. Battle hardening is something you tend to amortized over time.",
                  "score": 1,
                  "created_utc": "2025-12-26 17:58:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu87sa",
          "author": "finite_user_names",
          "text": "I suppose the question is whether you are expecting this volume to increase considerably, but no that sounds like too much for your current loads.",
          "score": 34,
          "created_utc": "2025-12-25 07:16:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuj7ib",
          "author": "sionescu",
          "text": "10k events per day are 7 per minute. At that rate Kafka is completely overkill, even a SQL database if you need to bring it up just for this task. You can simply write them to the file system, one event per file.",
          "score": 55,
          "created_utc": "2025-12-25 09:14:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuvpoo",
              "author": "thinkingatoms",
              "text": "sometimes it's not about the throughput it's about the latency",
              "score": 5,
              "created_utc": "2025-12-25 11:31:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0rprf",
                  "author": "ElkRadiant33",
                  "text": "Kafka still isn't needed for that",
                  "score": 3,
                  "created_utc": "2025-12-26 13:31:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw23ds9",
                  "author": "canihelpyoubreakthat",
                  "text": "A database at 0.1 QPS will be perfectly fine for latency.",
                  "score": 2,
                  "created_utc": "2025-12-26 18:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvv6lt5",
          "author": "Icy_Addition_3974",
          "text": "You're not crazy. 10k events/day is \\~0.1 events/sec. Kafka is designed for millions/sec.\n\n\nFor your use case (receive → clean → store → forward), you have simpler options:\n\n\nIf you want Kafka semantics without Kafka ops:\n\n\nLiftbridge — append-only log, consumer groups, replay, offsets — but a single 16MB Go binary on NATS. No JVM, no ZooKeeper, no dedicated person to babysit it.\n[github.com/liftbridge-io/liftbridge](http://github.com/liftbridge-io/liftbridge)\n\n\nEven simpler options:\nRedis Streams (if you're already running Redis)\nPostgres + LISTEN/NOTIFY (if volume stays low)\nSQS/SNS (if you're on AWS and don't need replay)\n\n\nThe \"immediately jump to Kafka\" thing happens because it's the default answer in every system design interview and blog post. Doesn't mean it's right for every scale.\n\n\nAt 10k/day, you could literally write events to a file and process them with a cron job. Don't let your team spend months on infrastructure you don't need.",
          "score": 13,
          "created_utc": "2025-12-25 13:13:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhj3gx",
              "author": "klimaheizung",
              "text": "Liftbridge - I don't know it, but it says it is designed with golang first in mind. It makes it sound like the JVM is a problem (it's not) and says that it doesn't need zookeeper while making it sound like kafka needs zookeeper (it doesn't anymore).\n\nI think as soon as someone wants a high available system, kafka is probably the best choice, since all other solutions will inevitably have to deal with all the same problems that come from it. Maybe Liftbridge has some advantages in some areas (it's newer and can learn from kafka's mistakes) but the general problems of a HA solution will be there. Cannot escape CAP.\n\nPostgres + LISTEN/NOTIFY is an awesome solution if ACID is needed, but it'll force you to implement a lot of logic to not miss events. If one finds kafka too hard to setup, then postgres as a message broker is 100x harder.\n\nRedis Streams - doesn't deal witht he split brain problem, so you'll easily run into inconsistencies or data loss if you don't know what you are doing.\n\n  \nPersonally I think that with a small number such as 10k events per day, the best way are cloud native solutions where the HA part is managed for you and the number of events is low enough to not cause problems. It usually comes with vendor lockin though.\n\n  \nUltimately, there is no really good solution. Kafka is probably still one of the best IF HA is needed, even if it feels way too heavy.",
              "score": 2,
              "created_utc": "2025-12-29 03:53:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwdm3is",
              "author": "ConstructionInside27",
              "text": "Gotta 2nd using postgres if it's already in your stack. LISTEN, NOTIFY and UPDATE SKIP LOCKED are expressly designed to make a reliable, durable message queue with very little code. It won't scale horizontally but you could probably 1000x your current load",
              "score": 1,
              "created_utc": "2025-12-28 15:45:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvugd69",
          "author": "paturnio",
          "text": "I started with Kafka two years ago, in a use case similar to yours. In the beginning, fewer than 5k messages per day, now about 180k. \n\nThe cluster is managed by AWS and is still using the minimal size you can provision (2vcpu, 2GB RAM).\n\nMaybe even my use case could be an overkill, but it helps me to sleep at night.",
          "score": 11,
          "created_utc": "2025-12-25 08:43:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvue8e9",
          "author": "_nku",
          "text": "At that low rate you don't need any message / event architecture at all IMHO. Build a small synchronous api that writes to your DB and that other API and move on to higher value tasks. If you are worried about uptime put AWS sqs in front (or whatever as a service KISS message queue you have at hand).\nPlugging a self hosted message infrastructure in front at that load in a small load small team scenario creates more failure modes than it prevents. \n\nTo the actual question: yes, Kafka is overkill to the large majority of cases.  Dumb as a service pipes like sqs scale unbelievably high and can for practical purposes be considered \"always up\"",
          "score": 21,
          "created_utc": "2025-12-25 08:20:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuai1c",
          "author": "chrisrrawr",
          "text": "as opposed to what? without knowing what is competing for your time there's no reason not to learn new skills and prepare to scale.",
          "score": 26,
          "created_utc": "2025-12-25 07:40:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuiva7",
              "author": "zebba_oz",
              "text": "10k events per day? They could scale 100 times and still won’t need a cluster",
              "score": 19,
              "created_utc": "2025-12-25 09:10:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvujuev",
                  "author": "chrisrrawr",
                  "text": "and? kafka is more than just throughput. plenty of use cases not mentioned by op that they might want later. scaling isn't just throughput, and focusing on whether they *need* to be preparing to scale ignores the primary aspect of my comment.",
                  "score": -6,
                  "created_utc": "2025-12-25 09:21:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuarms",
              "author": "minato3421",
              "text": "This should be the right question to answer.",
              "score": 6,
              "created_utc": "2025-12-25 07:43:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvufhal",
              "author": "jWas",
              "text": "Probably a single server and a couple of scripts.",
              "score": 6,
              "created_utc": "2025-12-25 08:33:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuhb7t",
                  "author": "chrisrrawr",
                  "text": "I meant the other constraints on their time. This is obviously something the team wants to do to upskill and learn new things (resume driven development) so whether or not it's 'appropriate' depends on what the competing factors are.",
                  "score": 1,
                  "created_utc": "2025-12-25 08:53:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvhxjr",
          "author": "FunPaleontologist167",
          "text": "10k per day? You could set up a postgres queue and call it a day",
          "score": 6,
          "created_utc": "2025-12-25 14:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvublra",
          "author": "Ok_Cancel_7891",
          "text": "The guy is just trying to keep his job, and is doing in a right way.",
          "score": 21,
          "created_utc": "2025-12-25 07:52:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv7ctt",
              "author": "Polus43",
              "text": "Yup, creating work",
              "score": 3,
              "created_utc": "2025-12-25 13:19:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxkuqy",
              "author": "GoodLyfe42",
              "text": "Or get a new higher paying job somewhere else",
              "score": 2,
              "created_utc": "2025-12-25 22:12:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvui0ya",
          "author": "vish4life",
          "text": "A kafka setup which can handle 10-20k messages per day should take less than a day to setup and shouldn't require a dedicated engineer. If it going to take longer, then Kafka isn't for your organization",
          "score": 11,
          "created_utc": "2025-12-25 09:01:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvux536",
          "author": "Alive-Primary9210",
          "text": "It is overkill for most use cases. 10k per day is around 7 events per minute on average. This is nothing, absolutely nothing for modern hardware.  \n  \nAt this load, just insert into a database, maybe put your cloud providers queueing solution in between if the traffic is bursty.",
          "score": 6,
          "created_utc": "2025-12-25 11:46:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvujxba",
          "author": "LargeSale8354",
          "text": "We are in AWS and put way more than that through SQS. I dare say GCP PubSub or Azure Service Queues would more than cope too.\n\nBuy the complex stuff when you need the complex stuff,  not before. Otherwise you get the cost of running it without the benefits materialising",
          "score": 4,
          "created_utc": "2025-12-25 09:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv3lvr",
          "author": "KingRush2",
          "text": "You definitely do not need Kafka. If you want something event driven, just go for sns/sqs with a lambda trigger off the queue. ",
          "score": 4,
          "created_utc": "2025-12-25 12:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvufk17",
          "author": "Justbehind",
          "text": "99.9% of use-cases could easily be solved with a simple batch-job. \n\n\nStreaming immensely complicates things. You should shy away from it, unless strictly necessary.",
          "score": 18,
          "created_utc": "2025-12-25 08:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuhium",
              "author": "Murky-Sun9552",
              "text": "Yeah we used it for high volume telephony data where we needed streaming batch jobs for everything else.",
              "score": 5,
              "created_utc": "2025-12-25 08:55:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvunul7",
          "author": "johnnygalat",
          "text": "No, it's not overkill for most use cases. But it probably is for yours if it takes you months to setup a cluster - it should take you a day to setup (no zookeeper in kafka 4) and a few days to hammer out specific settings (broker.id, node.id, setting quorum) and you're good to go. \n\nBut as quite a few pointed out - event driven arch gets complicated fast when you start working with multiple consumers per consumer group and forget about transactional consistency and message ordering (its possible but hard).",
          "score": 3,
          "created_utc": "2025-12-25 10:05:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvur4nq",
          "author": "titpetric",
          "text": "Event queues I've seen tend to work on a sql database and if there is volume (M++/day), i know redis lists work just as well, and it's not like zeroq, nanoq, NATS and many other queue solutions don't get traction\n\nIt's a technology choice. Supposedly the decision is logged with reasoning and alternatives considered",
          "score": 3,
          "created_utc": "2025-12-25 10:41:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv308f",
          "author": "GachaJay",
          "text": "I don’t get it. I haven’t seen a need to move past an Azure Service Bus messaging queue in any business scenario that isn’t related to telemetry. Even then, event grids work fine. Why would anyone ever want to host this when it so cheap for anything operational computing in cloud environments?",
          "score": 3,
          "created_utc": "2025-12-25 12:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw3wu9",
          "author": "georgewfraser",
          "text": "Kafka isn’t even relevant for most use cases. The core function of Kafka is single-message durability. This is a very expensive feature that comes with many tradeoffs. When you need it you really need it but in most data engineering workflows the source is durable. If you have a durable source, every other use case of Kafka can be accomplished in a simpler way using ordinary data structures and boring primitives like files.",
          "score": 3,
          "created_utc": "2025-12-25 16:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw356ld",
          "author": "Randy_McKay",
          "text": "Check RedPanda",
          "score": 3,
          "created_utc": "2025-12-26 21:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk03v8",
              "author": "mbroberg",
              "text": "+1 Redpanda Serverless could be a good option. I haven't seen Aiven mentioned in this thread yet, but they recently launched a new free forever Kafka tier for their managed service. \n\nJust another couple options if you don't want to go the Amazon MSK / IBM Confluent route. (Disclosure: I work for Redpanda.)",
              "score": 3,
              "created_utc": "2025-12-29 15:10:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu8zai",
          "author": "umognog",
          "text": "If its an existing setup and its simpler to just slap a new topic in, bash on is always my answer.\n\nBut in many, many cases, its just not needed, especially in analytics. I wouldnt go putting effort into standing it up unless the use case actually called for it and it doesnt exist.",
          "score": 2,
          "created_utc": "2025-12-25 07:24:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvugg02",
          "author": "oceaniadan",
          "text": "Having to flatten Avro becomes tiresome very quickly",
          "score": 2,
          "created_utc": "2025-12-25 08:44:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvatfo",
          "author": "KingJulien",
          "text": "Why can’t you use eventbridge or sqs+sns or redis? So much simpler and no setup",
          "score": 2,
          "created_utc": "2025-12-25 13:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvqn55",
          "author": "dmart89",
          "text": "You're better off just using SQS out of the box. 10k is noth",
          "score": 2,
          "created_utc": "2025-12-25 15:35:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyccvo",
          "author": "PhantomSummonerz",
          "text": "Kafka for one event every \\~8 seconds? I think you need to do better than that. How about you code a stream processing engine yourself in C++ or Rust, just to be sure?\n\nJokes aside, it's a terrible recommendation. A simple python implementation is probably more than enough, based on your description.\n\nSometimes (as someone mentioned) it's resume-driven development, other times people just want to get hands-on experience with different technologies (sometimes complex ones) so they become more skilled. The best question is to ask them \"why?\". When one proposes technology implementations they should have some pros/cons in mind. Let's hear why they arrived to \"kafka is better\".",
          "score": 2,
          "created_utc": "2025-12-26 01:08:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuabal",
          "author": "Kaze_Senshi",
          "text": "Maybe you could start with a smaller and easier setup AWS managed Kafka instance as a proof of concept and compare with other tools such as Pulsar or RabbitMQ to show your team that it is not required.",
          "score": 2,
          "created_utc": "2025-12-25 07:38:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvua4cb",
          "author": "only4ways",
          "text": "Only 10K events per day? Hmmm..  \n(60 seconds) x (60 minutes) x (24 hours) = 86K hits per day.  \nIf the request/response time is about a second, which is above of an average response time - your current configuration should be fine :)",
          "score": 2,
          "created_utc": "2025-12-25 07:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvucps6",
              "author": "pag07",
              "text": "Your calculation does not make any sense.\n\nBut yes there are about 86k seconds in a day.",
              "score": 13,
              "created_utc": "2025-12-25 08:03:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvudhnh",
                  "author": "gwestr",
                  "text": "He is saying it is 0.12 requests per second, which is very slow. Things get interesting at 100 requests per second.",
                  "score": 4,
                  "created_utc": "2025-12-25 08:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvus2tn",
          "author": "Beautiful-Hotel-3094",
          "text": "There is only one correct answer here. CDD - CV driven dev. You process 10-1000000 events a day? All you need is fucking pure python at most.",
          "score": 2,
          "created_utc": "2025-12-25 10:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu9szf",
          "author": "No-Guess-4644",
          "text": "I like Kafka because it’s extensible and uniform between multiple things. \nIt’s good design at the ground level so you can get bigger later, easilly",
          "score": 2,
          "created_utc": "2025-12-25 07:33:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuj53p",
              "author": "pceimpulsive",
              "text": "But at what cost if their entire org today is 10k a day... Like..\n\nA standard very small RDS (like 4 core, 16gb ram) would be able to federate even 1m rows a day without a sweat~\n\nKafka is extreme overkill for this org...",
              "score": 10,
              "created_utc": "2025-12-25 09:13:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvv9wkc",
                  "author": "No-Guess-4644",
                  "text": "Fair. I’ve always had servers that were more than powerful enough, kinda just available to me.",
                  "score": 1,
                  "created_utc": "2025-12-25 13:39:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuff9p",
          "author": "coreytrevorlahey69",
          "text": "A single node, single cluster, single container kafka setup could do this with very little RAM/CPU.",
          "score": 1,
          "created_utc": "2025-12-25 08:33:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv0bev",
          "author": "RunOrdinary8000",
          "text": "Sadly you have not mentioned how resilient your existing setup is towards errors.\n\nIf your DB has no reload policies, then a messaging system could be beneficial, because it will track which events have been consumed by the DB and which are still open.\nKafka I would only consider if you want to replace the DB.\n\nAn lightweight alternative could be Qpid.\n https://qpid.apache.org/index.html\n\nI personally would setup a process which fetches the data or receives the data and is able to write to disk into files.\nThis makes it very simple approach to handle the data.\nHowever your DB load pipelines need to handle then the errors and issues. The benefit is that you have strong control over the data and you can use existing strategies.\nIt will also scale. The drawback is you will have the typical batch delays. So the data freshness will be in a timeframe.\n\nMaybe every 5 minutes or once an hour will be fine.\nHere is one article for a possible setup. https://medium.com/@thishantha17/build-a-simple-python-rest-api-with-apache2-gunicorn-and-flask-on-ubuntu-18-04-c9d47639139b\n\nThe ApI and business that consumed the data define the requirement how fast you need to deliver. I guess from the way you describe the case there are no set requirements.",
          "score": 1,
          "created_utc": "2025-12-25 12:17:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv8v9q",
          "author": "DenselyRanked",
          "text": "I don't think using Kafka as a part of your design is overkill, but if it is going to take months of planning to set up a cluster, then a managed service or message queue would be better suited for your use case. It will take a much larger throughput before the service fees exceed the cost of a dedicated engineer.",
          "score": 1,
          "created_utc": "2025-12-25 13:31:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvva4zy",
          "author": "txmail",
          "text": "IMO that is way overkill for 10k events unless they are coming in a very narrow window. My last gig used massive kafka clusters to handle up to 40k EPS -- and it performed well.\n\nIf this is a somewhat well distributed 10k events PER DAY I would just throw up a SQLite database with a simple queue table.",
          "score": 1,
          "created_utc": "2025-12-25 13:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvbmb9",
          "author": "themightychris",
          "text": "why not just use Postgres? 99% of the time at nonhuge scales the answer is to just use Postgres",
          "score": 1,
          "created_utc": "2025-12-25 13:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvc9hv",
          "author": "dheetoo",
          "text": "Try pg-boss brother",
          "score": 1,
          "created_utc": "2025-12-25 13:56:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvh5jx",
          "author": "Decent_Comparison_41",
          "text": "Check out Strimzi maybe. I used it to set up a Kafka cluster within days. Could be done faster if you know what you are doing. I was brand new to Kafka terminology when I did that. It behaves decently for now. I used xk6 to load test it. \n\nI would like to know what other people think of Strimzi.",
          "score": 1,
          "created_utc": "2025-12-25 14:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvntp9",
          "author": "wbrd",
          "text": "Unless you are using Kafka for the special features it has, it's overkill. There are multiple flavors of MQ that will run circles around Kafka for speed and cost if all you want is pub/sub.\nOf the dozen or so installs I've been around, only one of them actually used Kafka for something that you couldn't do with just a couple of boxes running ActiveMQ.",
          "score": 1,
          "created_utc": "2025-12-25 15:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvu2fy",
          "author": "wubalubadubdub55",
          "text": "Just use a SQL database and use a simple .NET 10 API and you’ve got yourself an extremely cheap and reliable solution.\n\nYou absolutely don’t need Kafka. In fact most of the projects don’t need Kafka.",
          "score": 1,
          "created_utc": "2025-12-25 15:56:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvvuvv",
          "author": "Small_Sir_1641",
          "text": "Yep it is! I have seen both on app development and on data side how it makes things more complex then it needs to be\n\nFrom a data engineering side - unless you are a big company with the resources to afford and support - it is an overkill",
          "score": 1,
          "created_utc": "2025-12-25 16:07:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvybio",
          "author": "CaptainFoyle",
          "text": "Works well if you need some strong metamorphosis",
          "score": 1,
          "created_utc": "2025-12-25 16:22:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw0y13",
          "author": "Plane_Expression2000",
          "text": "and for 100k per day? (I’m in the same situation, we’re looking to change our infrastructure)\nWe process XML files. \n\nCurrently we have ActiveMQ where we only put the XML id stored in the database, and so at each processing step we retrieve the xml from the id in the database.\n\nWouldn’t it be simpler instead of retrieving the XML from the database at each step to directly “push” it transformed to the next step?\n\nI have approximately 100K long XML files per day to process​​​​​​​​​​​​​​​​",
          "score": 1,
          "created_utc": "2025-12-25 16:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw45lv",
          "author": "BenchOk2878",
          "text": "Use raw AWS SQS.\nThat msg/sec is not worth even setup RabbitMQ.",
          "score": 1,
          "created_utc": "2025-12-25 16:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw4bpd",
          "author": "LouDSilencE17",
          "text": "The worst part is when the person who decides on this stuff moves to another company. You're left maintaining this complicated system they set up while they're off building the next overengineered thing somewhere else",
          "score": 1,
          "created_utc": "2025-12-25 16:57:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw4hbx",
          "author": "Typhon_Vex",
          "text": "Most business requesters can’t answer this simple question :\n\nYou need real-time data ? So some person will really sit at this solution 24/7 and when. Something happens at 12:05 , he will do something significant at 12:10 ?\nAnd what will that be ?",
          "score": 1,
          "created_utc": "2025-12-25 16:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw4prh",
          "author": "CurrentBridge7237",
          "text": "you are not crazy . i see this all the time where teams adopt kafka because that's what most people use, not because they really need it. The maintenance burden is a lot and most teams don't account for it upfront",
          "score": 1,
          "created_utc": "2025-12-25 17:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw5ghw",
          "author": "In2da",
          "text": "We almost the same mistake buut i managed to convince my company to use synadia for passing messages between services instead and thank god. Does everything we need without requiring a dedicated kafka expert on the team.",
          "score": 1,
          "created_utc": "2025-12-25 17:04:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwbe3z",
          "author": "izzle10",
          "text": "yes, sqs would be much simpler and if your db is postgres then pgmq will be fine for 10k messages",
          "score": 1,
          "created_utc": "2025-12-25 17:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwbv24",
          "author": "cran",
          "text": "Kafka takes less than an hour to set up. Half a day if you set up Terraform and CI/CD. It’s a pretty well-supported basic stack. There are some telling issues with your team that anyone thinks it is too much or that it would take months to set up. It’s a very basic, very reliable component.",
          "score": 1,
          "created_utc": "2025-12-25 17:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwg8b5",
          "author": "corbosman",
          "text": "I think it's overkill for a lot of use cases. We had a choice between Kafka and RabbitMQ and picked RabbitMQ. It's been working perfectly for what we need and is so much easier.",
          "score": 1,
          "created_utc": "2025-12-25 18:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxku3t",
          "author": "Phil_P",
          "text": "Kafka is the solution that everyone jumps to by default. We almost made that mistake, but decided to do our due diligence. NATS came up as something to consider. After a deep dive into comparing them, we decided that NATS was far more versatile and easier to configure. Single binary and a config file, server side subject filtering, scaling readers without having to change sharding.",
          "score": 1,
          "created_utc": "2025-12-25 22:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxlnlx",
          "author": "AdeptnessThis7503",
          "text": "Maybe try NATS.",
          "score": 1,
          "created_utc": "2025-12-25 22:17:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvykv6n",
          "author": "maggias",
          "text": "Just use confluent cloud, its simple to setup and this load might fit within the free tier or be very cheap to run. You still get all the benefits of kafka without the huge initial investment and can experiment if its a good fit for you. Setting up a dedicated cluster for this workload is borderline crazy in my opinion. And no you don’t need kafka for this kind of load (at least not throughput wise). It has other benefits though like schema registry and stream processing capabilities, but depends on your use case and what you are looking for.",
          "score": 1,
          "created_utc": "2025-12-26 02:07:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyo6jr",
          "author": "biglerc",
          "text": "Yes. Kafka is frequently overkill and used for the wrong workloads. Why? Resume driven development.",
          "score": 1,
          "created_utc": "2025-12-26 02:30:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzk8e3",
          "author": "voycey",
          "text": "It is extremely overkill in this case, if you are going to be scaling to billions sometime in the next few months then maybe but otherwise absolutely not.  \nOne thing I will say is that stream-first / Kappa Architectures do have their advantages over batch workflows but only when you have the streaming infrastructure taken care of for you (GCP Pub/Sub, SQS, Kinesis at a push).",
          "score": 1,
          "created_utc": "2025-12-26 06:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzlwyf",
          "author": "Rude_Victory_5344",
          "text": "It feels like the OP is exaggerating the difficulty of deploying and maintaining Kafka. I get that operations can be a headache if the traffic is massive, but calling it 'incredibly complex' and requiring 'dedicated personnel' at this scale seems like a bit much. In my view, the reason Kafka is actually a reasonable choice here is precisely because it’s such a standard—most engineers are familiar enough with it to handle it. That's better than coming up with some 'creative' custom solution, which would likely be much harder to maintain.",
          "score": 1,
          "created_utc": "2025-12-26 06:51:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzn28w",
          "author": "kekekepepepe",
          "text": "You are totally fine.\n\nKafka, Spark, Kubernetes, etc are an overkill for 97-98% of companies and workloads.\n\nData has definitely grown compared to the past decade, but it doesn’t mean that the data YOU deal with has, especially to the point you need to distribute it across multiple servers.\n\nAs for Kafka, it’s a great product but if you don’t leverage its entire capabilities and ability to scale - then you don’t need to take a plane for a 2 kilometer walk",
          "score": 1,
          "created_utc": "2025-12-26 07:02:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzx6rg",
          "author": "dataflow_mapper",
          "text": "You’re not crazy. Kafka solves very specific problems at scale and a lot of teams reach for it way before they actually have those problems.\n\nAt 10k events per day you’re barely stressing anything. The operational overhead of running Kafka will likely dwarf the actual data processing work, and you’ll end up debugging brokers and configs instead of delivering value. I’ve seen this happen more than once.\n\nPeople default to Kafka because it’s familiar, resume friendly, and feels future proof. In reality, queues, pub/sub, or even a simple scheduled job can be more reliable and easier to reason about for years. You can always migrate later if volume or complexity actually demands it.\n\nThe real question to ask is what failure modes you need to handle today, not what might happen at hypothetical scale.",
          "score": 1,
          "created_utc": "2025-12-26 08:45:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw02sbv",
          "author": "hungvo_",
          "text": "Hmm but how can setting up a Kafka cluster take months?",
          "score": 1,
          "created_utc": "2025-12-26 09:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw07kdq",
          "author": "n0o0o0p",
          "text": "Any reason not to use NATS Jetstream? I've been using it for a while and setting up a cluster and working with it is super easy.",
          "score": 1,
          "created_utc": "2025-12-26 10:33:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0kupv",
          "author": "UmbrellaTheorist",
          "text": "You can consider kafka when you have around 5k events per second. Less than 5k events per second works fine directly into a normal database like postgres. And even postgres has all sorts of queueing add-ons so you can probably go a lot more. These are rule-of-thumb rules.\n\nWhatever script you want to produce events in kafka can go directly to the database most likely.",
          "score": 1,
          "created_utc": "2025-12-26 12:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw29g6q",
          "author": "BrownBearPDX",
          "text": "10 million a day yes, 10,000 a day, you gotta be kidding. Any reasonably efficient ETL pipeline ishould handle this sort of input no problem unless you have serious spikes or some other data flow issues that I’m not understanding. Unless the cleanup process for each message is extravagant or complicated or something else which could possibly take up more temporary storage than the pipes and normal backflow can handle, you’re asking for a world of extra problems, and as you said somebody to take care of them specifically hired for that reason. Dumb, dumb, dumb, dumb, dumb. When I was working as a software developer for website, 10,000 page loads was what we handled every hour and a page load is much more heavy than normal transforms on data and we were just using normal tools to do that. Kafka is no walk in the park. Even if you guys decide to use a queue system, I’d recommend one of the managed services at any of the cloud hosting services. At least, then you’re not spinning this thing up on your own and managing everything, and even if your guys think that that’s simple and straightforward, Kafka is one of the most complicated pieces of valuable infrastructure that can be injected into a pipeline. It’s got its uses and it’s got its place and it’s got a reputation as valuable and complicated for a reason.  I don’t think you guys need anything close to this. Keep it simple, silly. Kiss.",
          "score": 1,
          "created_utc": "2025-12-26 18:34:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw50sff",
          "author": "DC_Punjab",
          "text": "I set something similar with Microsoft  fabric RTI in about 45 min.  Stored in eventhouse ran my clean scripts via notebooks and even merged other tables into the gold tables.    Super easy",
          "score": 1,
          "created_utc": "2025-12-27 04:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbm24g",
          "author": "EducationalWedding48",
          "text": "Check out cribl.   They have a useful free license as well.",
          "score": 1,
          "created_utc": "2025-12-28 06:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcsmun",
          "author": "peterxsyd",
          "text": "I reckon use NATS. With Claude as good as it is these days if you know what you are doing you could set it up in a day or two, and be production ready in 1-2 weeks.\n\nI think RedPanda Kafka should not take much longer than that though maybe 2-4.",
          "score": 1,
          "created_utc": "2025-12-28 12:41:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd0hqi",
          "author": "DataSigh",
          "text": "I know, I know: Tinybird https://www.tinybird.co/blog/apache-kafka-alternatives Full disclosure - I work there - you can use the events api to stream data in, run the analytics you want and get the results via an API. Job done.",
          "score": 1,
          "created_utc": "2025-12-28 13:38:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwddgas",
          "author": "purpletentacIe",
          "text": "NATS",
          "score": 1,
          "created_utc": "2025-12-28 14:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6qyu",
          "author": "randbytes",
          "text": "now everything in tech is designed based on the expectation that you will see exponential growth someday or someone wants to add skills.",
          "score": 1,
          "created_utc": "2025-12-28 17:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfhfz8",
          "author": "Physical-Compote4594",
          "text": "10k events spread over a 10-hour day is 1000 events per hour, 16.67 per minute, or one event every 3 seconds. But days are 24 hours, so one event every 7 seconds. You can handle this with pencil and paper.\n\nFire the guy who thinks Kafka is a good idea.",
          "score": 1,
          "created_utc": "2025-12-28 21:12:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhh2mh",
          "author": "klimaheizung",
          "text": "What's the alternative to kafka if you want a solution that is HA (so you don't have to worry that your pipeline breaks during the night because a single server machine crashed)?",
          "score": 1,
          "created_utc": "2025-12-29 03:41:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhkli5",
          "author": "shikhar-bandar",
          "text": "A serverless solution would be a good fit so you are only paying for usage. Check out s2.dev.",
          "score": 1,
          "created_utc": "2025-12-29 04:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk1a5g",
          "author": "davewritescode",
          "text": "10k events per day on Kafka is wasteful.  Why not just use a cloud service like Kinesis for volumes that low?\n\nThe work of running and patching Kafka is absolutely not worth the price of operating it.",
          "score": 1,
          "created_utc": "2025-12-29 15:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuib3p",
          "author": "Inner-Peanut-8626",
          "text": "I have a feeling there are other things your organization can use Kafka for.  Are you able to share what kind of organization it is?\n\nAnd it doesn't take months to setup Kafka.  I took the Confluent Fundamentals Accreditation and it takes minutes to do with Docker.",
          "score": 1,
          "created_utc": "2025-12-31 02:38:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvum7wr",
          "author": "Nekobul",
          "text": "You can process 10k events easily on a single machine with SSIS.",
          "score": 1,
          "created_utc": "2025-12-25 09:48:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuvvgk",
          "author": "thinkingatoms",
          "text": "lost me at takes a few months to set up.  is OP a business person or otherwise clueless about kafka setup?  ask AI to build various docker containers, ezpz",
          "score": 1,
          "created_utc": "2025-12-25 11:32:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvfp7c",
      "title": "New table format announced: Oveberg",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pvfp7c/new_table_format_announced_oveberg/",
      "author": "EarthGoddessDude",
      "created_utc": "2025-12-25 15:31:08",
      "score": 182,
      "num_comments": 31,
      "upvote_ratio": 0.85,
      "text": "Because I apparently don’t know how to type Iceberg into my phone properly, even after 5 attempts. Also announcing FuckLake. Both hostable on ASS. ",
      "is_original_content": false,
      "link_flair_text": "Meme",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pvfp7c/new_table_format_announced_oveberg/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvvravf",
          "author": "DynamicCast",
          "text": "A shitpost on my product shilling sub?",
          "score": 155,
          "created_utc": "2025-12-25 15:39:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvudkd",
              "author": "FuckTheStateofOhio",
              "text": "While we're here, check out this cool thing I've been building: FuckLake.ai",
              "score": 44,
              "created_utc": "2025-12-25 15:58:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvygj4",
                  "author": "EarthGoddessDude",
                  "text": "Are you hosting on us-east-2 (because fuck Ohio)?",
                  "score": 26,
                  "created_utc": "2025-12-25 16:22:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvw33d7",
          "author": "DeepFryEverything",
          "text": "Awesome! I have a really large table - approx fifteen rows and thee columns each. Can I use spark to process this data using Oveberg?",
          "score": 101,
          "created_utc": "2025-12-25 16:50:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw6xat",
              "author": "EarthGoddessDude",
              "text": "Why would you use anything else?\n\n![gif](giphy|pPhyAv5t9V8djyRFJH|downsized)",
              "score": 48,
              "created_utc": "2025-12-25 17:13:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwiz2n",
                  "author": "Clean-Health-6830",
                  "text": "If you can’t put it on your resume, what’s the point?",
                  "score": 10,
                  "created_utc": "2025-12-25 18:24:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwkuo6",
                  "author": "ThortheAssGuardian",
                  "text": "Archaic python-based codebase",
                  "score": 3,
                  "created_utc": "2025-12-25 18:35:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvw13i8",
          "author": "mild_entropy",
          "text": "A shit post in data engineering on Christmas?? 😍🎁🎄",
          "score": 56,
          "created_utc": "2025-12-25 16:38:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwe87w",
          "author": "StarWars_and_SNL",
          "text": "Requirements:\n\n* 5 years of managing Fucklake in a lead role \n\n* Implementing Oveberg across multiple Crackflake clusters\n\n* 10 years data engineering and infrastructure in ASS, certification preferred\n\n* SOL window functions \n\n* Microsoft Anole\n\n* Pythonk\n\n* dbteet \n\n* DataTicks",
          "score": 42,
          "created_utc": "2025-12-25 17:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxex45",
              "author": "reelznfeelz",
              "text": "This thread is making me laugh way harder than it should.   All the competing standards plus marketing BS is actually pretty absurd when you think about it though.",
              "score": 5,
              "created_utc": "2025-12-25 21:36:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxn9r6",
                  "author": "EarthGoddessDude",
                  "text": "🙏",
                  "score": 3,
                  "created_utc": "2025-12-25 22:28:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0276g",
              "author": "vuachoikham167",
              "text": "No certs tho? No chance bud 😤",
              "score": 4,
              "created_utc": "2025-12-26 09:38:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1558t",
              "author": "Real-Mine-1367",
              "text": "Hired!! 💯💯",
              "score": 3,
              "created_utc": "2025-12-26 14:58:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwlpfw",
          "author": "iknewaguytwice",
          "text": "How easy is it to import shit from my excel spread cheeks? Our Analyst is already stretched pretty wide, will it take time to warm them up, or can he bend over and dig in?",
          "score": 16,
          "created_utc": "2025-12-25 18:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvuve2",
          "author": "adappergentlefolk",
          "text": "pornhub data platform looking good",
          "score": 14,
          "created_utc": "2025-12-25 16:01:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwromh",
              "author": "anakaine",
              "text": "DataTable breaches stepsister at lakehouse.",
              "score": 5,
              "created_utc": "2025-12-25 19:14:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxw7h1",
              "author": "Budget-Minimum6040",
              "text": "What are you doing step lake?",
              "score": 6,
              "created_utc": "2025-12-25 23:25:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5o2xy",
              "author": "BringtheBacon",
              "text": "Damn they must have so many weird data points",
              "score": 1,
              "created_utc": "2025-12-27 07:30:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvw5wh9",
          "author": "VipeholmsCola",
          "text": "According to rules you have to disclose if you work for a service!",
          "score": 10,
          "created_utc": "2025-12-25 17:07:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwbf78",
              "author": "Optimal-Builder-2816",
              "text": "Full disclosure I work for ASS",
              "score": 17,
              "created_utc": "2025-12-25 17:40:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw08jq6",
              "author": "RBeck",
              "text": "I work for a service.",
              "score": 1,
              "created_utc": "2025-12-26 10:43:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwmdbp",
          "author": "Dependent-Yam-9422",
          "text": "I’m creating a data pipeline that processes two events per day into FuckLake, using FuckDB for querying of course. Do you think I need to set up a Kafka cluster for this?",
          "score": 10,
          "created_utc": "2025-12-25 18:44:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwlc8r",
          "author": "its_PlZZA_time",
          "text": "Can oveberg scale to store enough data to fill the ever-growing pit of emptiness inside me?",
          "score": 7,
          "created_utc": "2025-12-25 18:38:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxmlki",
              "author": "EarthGoddessDude",
              "text": "It’s webscale, so yes. Stay strong brother ✊\n\nEdit: or sister or whatever",
              "score": 6,
              "created_utc": "2025-12-25 22:23:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxbxny",
          "author": "Dunworth",
          "text": "Add some text about how it, \"Empowers the next generation of AI experiences,\" and I bet you'd get 100MM in funding easy. lol",
          "score": 7,
          "created_utc": "2025-12-25 21:18:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxml4b",
          "author": "Data_Wolf",
          "text": "Hey use our ZeroFucks ai agent to manage your FuckLakes with exceptional ZeroFucks performance and our ZeroLogz to really fuckify your fuckery",
          "score": 3,
          "created_utc": "2025-12-25 22:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxseqi",
          "author": "sheepsqueezers",
          "text": "I guarantee some idiot managers will start talking about \"oveberg\" to clients. 🙄🙄🙄",
          "score": 4,
          "created_utc": "2025-12-25 23:00:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyq631",
          "author": "Ok_Adeptness4967",
          "text": "On the one hand our current platform works just fine. On the other hand, Oveberg is a new shiny term that I have not heard of yet and know nothing about, which means we will begin migration plans immediately.",
          "score": 2,
          "created_utc": "2025-12-26 02:43:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvznezx",
          "author": "crazylilrikki",
          "text": "Is there built-in functionality to import from Microsoft Access? Just so you're aware, the database I'm wanting to migrate from Access is almost 1 GB so I totally understand if Oveberg can't handle such a massive amount of BIG DATA^TM.",
          "score": 2,
          "created_utc": "2025-12-26 07:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0w09e",
          "author": "Resquid",
          "text": "Is this a \"crashout?\"",
          "score": 1,
          "created_utc": "2025-12-26 14:00:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv5dzu",
      "title": "The scent of a data center",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/rg5qybt84a9g1.jpeg",
      "author": "PossibilityRegular21",
      "created_utc": "2025-12-25 04:38:09",
      "score": 162,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Meme",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pv5dzu/the_scent_of_a_data_center/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvv9cvu",
          "author": "stuporous_funker",
          "text": "These outages, are making me THIRSTY",
          "score": 37,
          "created_utc": "2025-12-25 13:34:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvhjqi",
              "author": "EarthGoddessDude",
              "text": "SERENITY NOW!",
              "score": 11,
              "created_utc": "2025-12-25 14:35:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvw8m4p",
          "author": "digitalghost-dev",
          "text": "us-west-2 gang 🙌",
          "score": 7,
          "created_utc": "2025-12-25 17:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwwfui",
          "author": "samudrin",
          "text": "Jerry Sienfeld was never particularly funny.",
          "score": -6,
          "created_utc": "2025-12-25 19:43:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2koet",
              "author": "LiftSleepRepeat123",
              "text": "He literally plays the straight guy on the show. Everyone else is funny. George, Kramer, Newman, even Elaine.",
              "score": 2,
              "created_utc": "2025-12-26 19:33:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyja7j",
              "author": "Yourdataisunclean",
              "text": "![gif](giphy|USnfWeCOHTHB3WX0aY)",
              "score": 2,
              "created_utc": "2025-12-26 01:56:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyp6g6",
                  "author": "samudrin",
                  "text": "Meh. He's a genocide supporter and into jail bait, and like I said, not particularly funny.",
                  "score": 0,
                  "created_utc": "2025-12-26 02:37:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1px1ufp",
      "title": "For people who have worked as BOTH Data Scientist and Data Engineer: which path did you choose long-term, and why?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1px1ufp/for_people_who_have_worked_as_both_data_scientist/",
      "author": "Mean_Addendum_4698",
      "created_utc": "2025-12-27 16:27:39",
      "score": 157,
      "num_comments": 75,
      "upvote_ratio": 0.92,
      "text": "I’m trying to decide between Data Science and Data Engineering, but most advice I find online feels outdated or overly theoretical. With the data science market becoming crowded, companies focusing more on production ML rather than notebooks, increasing emphasis on data infrastructure, reliability, and cost, and AI tools rapidly changing how analysis and modeling are done, I’m struggling to understand what these roles really look like day to day. What I can’t get from blogs or job postings is real, current, hands-on experience, so I’d love to hear from people who are currently working (or have recently worked) in either role: how has your job actually changed over the last 1–2 years, do the expectations match how the role is advertised, which role feels more stable and valued inside companies, and if you were starting today, would you choose the same path again? I’m not looking for salary comparisons, I’m looking for honest, experience-based insight into the current market.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1px1ufp/for_people_who_have_worked_as_both_data_scientist/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw7r18i",
          "author": "t9h3__",
          "text": "I enjoy Engineering roles more as the feedback loops are more immediate:\n* You automate something, free up manual work\n* You can optimize a pipeline and save cost\n* If the pipeline doesn't fail you did a good job\n\nIn DS it's more research, experimental and often it can happen that you work on something for weeks until you figure that e.g. Prediction quality isn't good enough or certain data sources are missing",
          "score": 207,
          "created_utc": "2025-12-27 16:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8jk8y",
              "author": "Pale_Squash_4263",
              "text": "There’s is nothing more satisfying than saving people time. The amount of times someone has said “oh my god you have no idea how long I was spending doing this manually” never gets old. Sometimes my job is hard but this kind of response makes it worth it",
              "score": 45,
              "created_utc": "2025-12-27 19:14:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdei18",
                  "author": "Drkz98",
                  "text": "Same, I'm new as DE, I only have 8 months working but I received a lot of those comments because in my company a lot and I mean a lot was manually done, there was a process where the responsible of watching some marketing costs had to go to every supplier website, email and messages to have the daily cost, this was time consuming,  I created a lot of pipelines and now everything lives together in a simple dashboard, they only need to enter, filter dates and that's all, they have all the information at hand, when I showed them, they told me that I saved their lives, it's amazing sometimes.",
                  "score": 3,
                  "created_utc": "2025-12-28 15:04:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7y4pz",
              "author": "painteroftheword",
              "text": "I've dabbled in a bit of data science but ultimately the limitations of forecasts and layman's understanding of statistics mean they have little business engagement/impact.\n\nI have much greater impact/engagement taking business data and presenting it ways that enable varying levels of oversight and insights into business performance and process compliance.\n\nI do both data engineering and BI.",
              "score": 33,
              "created_utc": "2025-12-27 17:26:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwkkygd",
              "author": "Ok_Wishbone_3927",
              "text": "Yep, that feedback loop is exactly why i have switched from DS to full time Data Engineering. I was spending so much time doing Data Engineering work as a DS, but was so stressed out because either the modeling wasn’t happening (DE work was needed) or wasn’t producing the desired results (experimentation and failure are REQUIRED, Data quality was not up to snuff, leadership didn’t get it,  etc.). “Done” was not defined and that stressed me out tremendously and impacted my work life balance. \n\nI switched so that i could build something, know it works, put it into production and really try to have an impact on improving data quality and data understanding for an organization….work that will lead to quality analytics. \n\nI hope to stay at this org, help modernize the data pipelines and processes, understand the data really well, and do some DS side quests as i go. \n\nFor career advice, i think DE work is better for job security. DS seems like a nice to have for a lot of midsized and older companies—despite the 2012-2020 hype. A lot of companies around me never got their analytics processes in place, nor their data strategy sorted out to really make use of and operationalize advanced analytics at scale. The DE and DataOps work needs to be done to enable DS and advanced analytics, and a lot of non-cloud-native companies in my geographic area are still trying to figure it out. \n\nThat being said, the business intelligence practices at these same companies is sophisticated, provides tremendous value and is enabled by ETL pipelines and data warehouse artifacts. \n\nTLDR; So while i absolutely love the work and research of data science, I’ve switched to data engineering due to my perception of DE job security and for my own work life balance.",
              "score": 1,
              "created_utc": "2025-12-29 16:50:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2w6j8",
              "author": "QuantWiz_E",
              "text": "There's a strong sense of achievement when a program works as intended. Data engineering projects are often smaller and more self-contained than software engineering programs. You get to design it yourself and see it actually work. That feels really good.",
              "score": 1,
              "created_utc": "2026-01-01 14:02:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw805fa",
          "author": "ilikedmatrixiv",
          "text": "I choose for Data Engineering pretty early in my career and I'm very happy I did so.\n\nWhen you're a Data Scientist, one of your key goals is to extract meaning from data. You run models and try to get insights. It is very often the case though that the data either doesn't show what you would like, or it kind of does, but with pretty low certainty. Even more often the data is so bad that you couldn't really draw any conclusions from it.\n\nThis puts you in a bind. You have to go explain your conclusions to the sales guys and they can't work with 'it sort of shows X, but also kind of Y'. They need to sell a product and they will. So they'll massage your conclusions in ways that you might even not agree with. Or you'll be pressured to massage the data, which I've been asked to do. I always covered my ass with emails although I luckily never needed them.\n\nAs a Data Engineer on the other hand, there is no discussion. Either the data is in the required spot in the required form or it isn't. At the end of the day, your product is finished and whether or not someone can actually do anything with it downstream is not your concern. It's a very liberating feeling and it means I have barely any stress related to work.\n\nI'm not the only person I've encountered who feels similarly and I've even met a few Data Scientists who made a similar switch later in their career for similar reasons.\n\nAnother saving grace is that Data Scientists are a dime in a dozen nowadays. With the AI boom everyone and their cat tried to get some certification somewhere and the quality varies wildly. Data engineering isn't as sexy and there's much fewer people in the field I have the impression. I started before the AI boom, so having seniority at this point in time is very helpful as well.",
          "score": 64,
          "created_utc": "2025-12-27 17:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwczeac",
              "author": "Inner_Butterfly1991",
              "text": "Yep I started in data science and we'd meet with this VP once/month and have a deck to present all our findings and recommendations and for anything that confirmed what he wanted to do he'd sing our praises and pass our work up the chain to justify it, anything contrary he'd have further questions and other factors he thought we weren't considering, and when we addressed those in the deck next month there was a new set of concerns, always moving goalposts. But the biggest wakeup call was I was working on some financial modeling during covid and asked to rank order the top states by a metric and find out how many states included would make our metric x. This was a large regulated company that was going to go to regulators and say \"due to our analysis of the top x states, our metric is y\". But y is what they'd already decided on and x was what they used \"data science\" to calculate such that they could have the data point they wanted to share to regulators.\n\n\nNow I work in data engineering and there's still politics obviously you can't escape that, but it's certainly not as blatant and I've never been asked to essentially make up numbers to reach a conclusion.",
              "score": 3,
              "created_utc": "2025-12-28 13:30:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7qbzr",
          "author": "PrestigiousAnt3766",
          "text": "DE. Found that I couldn't care less about company data as compared to neuro which I trained in. DE skills transfer a lot better between companies as a consultant.",
          "score": 55,
          "created_utc": "2025-12-27 16:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8g92q",
              "author": "Ddog78",
              "text": "Right that's all it boils down to me. I understand the actual underlying data. But don't ask me to care about shit like what impacts the business line and all. Rats ass.",
              "score": 10,
              "created_utc": "2025-12-27 18:57:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7pl4l",
          "author": "BarfingOnMyFace",
          "text": "No opinion, but at least the questions on this sub are better. Someone on r/cscareers was asking if they should go in to computer science or nursing, and all these people on the are telling the guy “nursing”… 😂😂 at least here, the jobs are atleast both dealing with data in some context…",
          "score": 43,
          "created_utc": "2025-12-27 16:43:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7x1ge",
              "author": "RevolutionaryGain823",
              "text": "Redditors in tech have no idea how difficult other jobs are. Redditors will complain they’re “oppressed” cos they have to go into an air conditioned office twice a week to mess around on a laptop and that those lazy nurses have it so easy cos all they have to do is make life or death decisions while cleaning up literal shite and occasionally be threatened or even assaulted by unstable patients",
              "score": 42,
              "created_utc": "2025-12-27 17:21:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8d6xl",
                  "author": "WhipsAndMarkovChains",
                  "text": "While I agree Redditors don’t know other jobs and fields, I think you’re being quite unfair as to why people are recommending nursing. I have not once seen anyone here claim nurses have it easy and are lazy. People are recommending it because unlike CS jobs, nurses aren’t facing mass layoffs ever CEOs claim we have AI nurses now while really hiring nurses in India for cheaper.",
                  "score": 11,
                  "created_utc": "2025-12-27 18:42:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8rni4",
                  "author": "ditalinidog",
                  "text": "My gf is a nurse and we make similar salaries. There’s no way I’d prefer the schedule or emotional and physical stress of her job, but she’d also be deathly bored doing mine. It’s just very different skillsets and workflow, I’m not sure how someone even narrows it down to those paths aside from looking at the best paying stuff with just a bachelors.",
                  "score": 8,
                  "created_utc": "2025-12-27 19:57:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8bcm1",
                  "author": "Equivalent_Run_4788",
                  "text": "No one is saying that engineers are \"oppressed\" or that nurses are \"lazy\". Where are you getting this from?\n\nSince r/cscareers was mentioned, the reason those people are recommending non-tech is because the job market for entry level SWEs is atrocious right now. And that sub is 90% new grads, many of whom are genuinely struggling to get employed.\n\nThere is definitely something to be said about the younger generation being overcoddled and not understanding the value of work, but that's not the problem here.",
                  "score": 3,
                  "created_utc": "2025-12-27 18:33:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8mbu5",
                  "author": "scarredMontana",
                  "text": "I wouldn't say nurses have to make life or death decisions lol, but much respect to them nonethless",
                  "score": -3,
                  "created_utc": "2025-12-27 19:29:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8c7kk",
              "author": "tel-tec",
              "text": "lol if I could do it all over again I would go into nursing. Nurses seems to have more fun. I would say nursing seems to have better job security. That said, I love my job in tech, and lifestyle it provides. One thing I appreciate about our job is that it is project based. So I tend to take it easy until  it is  CRUNSH time. I would recommend becoming knowable in networking since data science jobs does pose the possibility becoming more scarce due to ML. A job is a job at the end of the day!",
              "score": -1,
              "created_utc": "2025-12-27 18:37:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw84diz",
          "author": "i_hate_budget_tyres",
          "text": "Do you like maths?  DS.  Are you curious, have a discovery mindset?  DS. \nDo you like building stuff, to technical specifications?  DE.\n\nFor me the answer was simple.  I naturally gravitated towards technical specifications and engineering solutions.  I was interested in maths, but having done some statistics courses, it wasn’t a mountain I wanted to climb.\n\nPersonally I wouldn’t over think it.  People swap between DS and DE all the time.  A large company will probably help you transition, pay for training etc because they know there is a great deal of synergy between these teams.  ie.  DE’s will productionise DS models so knowing both sides isn’t a waste of time.\n\nWhere I currently work, DS is seen as a ‘luxury’.  They will increase headcount when times are good, and conversely, DS’s are the first people out the door when times are bad.  This is because they generally work on ‘optimisations’, I guess just like how you would pause remodelling your house if you lost your job and didn’t have income coming in.  DE on the other hand is much more stable.  They need people to keep existing pipelines running, even if new things aren’t being built.  It was how I ended up in DE, I was made redundant as a DS but told I could stay on if I switched teams.  I thought I’d do this while I searched for a new DS position, but I found my proclivities more suited to DE, so never left and never thought about switching back.\n\nGenerally the people who make the best DE’s have done Computer Science or Engineering degrees that involve data transfers like Telecoms.  The people who make the best DS’s have done Science or Mathematics degrees.  Maybe start out in the field where your degree gives you the best foundations?  Otherwise it will be an uphill struggle to get timely promotions.  Make a start and see how you go.",
          "score": 16,
          "created_utc": "2025-12-27 17:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbvhn4",
              "author": "farm3rb0b",
              "text": "Master's in math here - I have some extra thoughts to liking math = DS. It's highly dependent on what you like about math. I think the person above me hit the nail on the head with stats specifically being the potential translator. \n\nMost math is very logical - there are rules about the way things work. Formulas exist and can be derived. Proofs may have been the bane of many in high school geometry, but they work - things *work* in math, much like engineering. DS throws a lot of ambiguity into the equation - it feels more like research/investigation to me. Trial and error. Which features give me the best fit? Which model? While there are flow charts to help decide, it can still be hard to define \"done\".\n\nDE is far more logic based. As others have said - you get requirements, you meet them. Most organizations have a specified tech stack, leaving little to no ambiguity about how to go about meeting requirements.\n\nAs a math person who loves math for the logic/puzzle solving pieces, I'd pick DE every time. I'd pick DA over DS, too.",
              "score": 3,
              "created_utc": "2025-12-28 07:30:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7rsqa",
          "author": "ColdPorridge",
          "text": "DE. In general DS code quality is really, really low, for a lot of reasons that are endemic to the industry. If you’re comfortable writing the hackiest shit and don’t ever want code review, DS maybe is for you. \n\nIf you like having standards and building in a team, architecting systems, doing anything beyond notebooks and SQL, DE is a better bet. \n\nIt’s entirely possible to be a diligent DS with Eng skill and code quality standards but you’d struggle to find a company/team that actually values that in their DS.",
          "score": 23,
          "created_utc": "2025-12-27 16:54:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd09h3",
              "author": "Inner_Butterfly1991",
              "text": "One of my previous jobs was in a data science org and I was teaching them the basics of git. You would have thought I was teaching them how to do brain surgery with the reaction some pretty senior level folks had to it. This was an enterprise initiative due to a bad model being deployed to production without any actual code review, someone just typed a coefficient wrong and there were no checks or anything. Even after I convinced people to use git, they would just write all of their models and check in Excel and jupyter notebook files into a single commit and not review anything.",
              "score": 2,
              "created_utc": "2025-12-28 13:36:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwikndd",
                  "author": "MisterSixfold",
                  "text": "IMO both of you are not even talking about DS.\nThis just sounds like crappy data analyst work.\n\nAny proper DS should be able writing production quality code.\n\nWorking in notebooks? Data analyst\nWorking in Excel? Data analyst\nWorking with PowerBI? Data analyst",
                  "score": 0,
                  "created_utc": "2025-12-29 08:47:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8b4e8",
          "author": "ScholarlyInvestor",
          "text": "They can both be fun. People have raised many good points here. Mostly leaning toward DE. If there are folks out there considering DS… IMO, Data Scientists in many businesses require a very high tolerance for ambiguity. Most companies are not sure why they hired them. Early in 2010s it was considered the hottest job in tech. People hired data scientists like crazy. Defining and realizing ROI is always challenging. However, if you land in a group with good vision, leadership, and infrastructure (tech and $), it can be extremely rewarding.",
          "score": 7,
          "created_utc": "2025-12-27 18:32:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8wgro",
          "author": "fvonich",
          "text": "Bro, you ask this in a data engineering sub.",
          "score": 5,
          "created_utc": "2025-12-27 20:23:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7vp4l",
          "author": "Alternative-Guava392",
          "text": "Both are great fields, a bit similar but also quite different. I've been a data engineer for 5 years but worked as a data scientist for 1 year 2 years back. Currently I'm the only data engineer in a data science team so my work revolves around data science. \n\nBiggest difference :\n- Data science requires creativity. Lot of A/B tests to see what works and what does not. Something could sound efficient on paper but not as much in business. Experimentation is important in data science.\n\n- Less focus on infrastructure as a data scientist. ML ops or data ops takes care of it most times.\n\n- Accuracy is important in data engineering. Data needs to be ingested and modelled accurately otherwise the data is doubted. Data quality and data sanity are important. Work and what pipelines to design is very much defined by requirements and business. There isn't experimentation.",
          "score": 4,
          "created_utc": "2025-12-27 17:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7wek7",
              "author": "Alternative-Guava392",
              "text": "Also in recent years a lot of data science is just using OpenAI or Gemini. But real data science requires statistics and prediction modelling, feature engineering.\nData scientists actually run python scripts or notebooks in production too. It isn't just academia. \nBut the infrastructure for handling those notebooks and python scripts is production-grade. Sometimes pre-trained data science models are stored as objects / files and used in production.",
              "score": 1,
              "created_utc": "2025-12-27 17:18:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7vze3",
          "author": "SirGreybush",
          "text": "I’m a DE with college plus Uni-level BI courses, so at best a certificate. \n\nThe DS I have worked with, two of them, one is PhD in mathematics and the other has two master degrees of which one was mathematics. A Kimball Wizard. \n\nThey taught me some cool algorithms for 3D spatial alignment of data (the Z being time) and I taught some of them better SQL.\n\nI would redo their SQL optimized to the OLTP platform (MSSQL/Oracle/DB2) with views and staging, add table specific optimization as needed. \n\nTheir SQL told me what they needed for their ML model, I optimized it and made it 100% repeatable with a dedicated DB for snapshots. \n\nSo they could choose a snapshot time and get back instantly the exact same data, so the only variable is their model code. \n\nWe made a great team. We were all 2 year temps at a big TelCo.",
          "score": 4,
          "created_utc": "2025-12-27 17:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7ri6f",
          "author": "Mindless_Let1",
          "text": "If you have a PhD and actual domain you specialise in then data science is great. If it's just data analyst but cooler name - data engineering is better",
          "score": 3,
          "created_utc": "2025-12-27 16:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7v1p2",
          "author": "Wide_Guava6003",
          "text": "DE gives way more broad set of skills as you usually have to learn a larger stack and the most important part (with my experience) is that inside DE you actually have to write good quality code and with better quality coding skills with a larger stack you are better off. \n\nOf course also depends on company, as for instance one company I worked in the DS was more of a full stack data developer which was exactly what I wanted to do. I changed companies to a general DS and it the skills in my teams DS is really narrow compared to what I thought.",
          "score": 3,
          "created_utc": "2025-12-27 17:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7zmzs",
          "author": "ask-the-six",
          "text": "I’ve worked with DSs both as a DS and DE. I call myself a DE and fill that role regardless of the actual responsibilities because I’ve seen too many red flags in the DS space that I know the folks making resource decisions see too. Biggest problem is not a fault of DS folks. Companies can’t afford the adjacent roles and supporting roles to allow successful DS work. This leads to DS roles picking up work they shouldn’t be focusing on. I’ve yet to encounter a DS role where value is being created consistently. Common loop I see: experimental POCs get made with a patch work of CSV files and no long term integration plan or even thought, dumped on the business with a prayer that it gets into production somehow. As others have said there’s much more structure in the DE space. For me at the end of the day it’s job security. All things equal, I think as a matter of scale a good DE role produces more measurable value to the company than a good DS.",
          "score": 3,
          "created_utc": "2025-12-27 17:34:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw87r77",
          "author": "DataOwl666",
          "text": "I am a data scientist but I need to sharpen my data engineering skills. Any ideas?",
          "score": 3,
          "created_utc": "2025-12-27 18:15:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8zbv7",
              "author": "n_ex",
              "text": "Think of a DS task that would involve different sources of data to be consolidated into one - real time open apis, sql tables, csv files, etc. Figure out what the data model should look like, where to store it, then implement the pipelines - they can be just python functions (ran at certain cadence using a scheduler maybe, if you want it to stay fresh) to start with.\nThat should be enough to get you started, then if you like it try tools like dbt, Airflow, databricks, cloud services, etc.",
              "score": 4,
              "created_utc": "2025-12-27 20:39:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw90ehv",
                  "author": "DataOwl666",
                  "text": "Thanks",
                  "score": 1,
                  "created_utc": "2025-12-27 20:45:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw84w8j",
          "author": "Dunworth",
          "text": "I wore both hats for a long time, but I'm pretty firmly in DE for now. I more or less made the switch because I started feeling kind of bad about the ethics of the pre-LLM data science world, and that feeling has only gotten worse since then. That being said, I'm getting nominated to put the DS hat back on because every team needs to be using \"AI\" in my current role and I'm the only one with experience in the area on my team. Though I'm not sure how long that's going to last, since I'd rather build an actual model instead of just throw an LLM at the problem.",
          "score": 3,
          "created_utc": "2025-12-27 18:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw93y11",
              "author": "parts_of_speech",
              "text": "Can you elaborate on the ethics part pls ?",
              "score": 1,
              "created_utc": "2025-12-27 21:04:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9pu6s",
                  "author": "Dunworth",
                  "text": "Sure, but just so that it's said ahead of time: I'm don't claim my ethics as some objective truth or even that they're rational. \n\nI got out of data science a little bit after GDPR went fully into effect and just seeing how that played out made me feel weird about the privacy aspect. Sure, a GDPR request wipes out a user's PII, but your user activity is all anonymized to the point that it passes compliance  and no further, so you're never really forgotten as far an ML model is concerned. The compliance guys signed off on all of it, so as far as the business was concerned, everything was good. I didn't really feel like that was morally correct, so I switched over to DE. \n\nThere's also the whole training data procurement being largely a pinky swear that it was obtained through legitimate means that we've seen crop up with chatGPT in the past few years. It was way less obvious when deep learning was the cool thing, but it was definitely happening.",
                  "score": 1,
                  "created_utc": "2025-12-27 23:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7zo1n",
          "author": "MadT3acher",
          "text": "I used to be a data scientist and then transitioned to DE by accident and then DE lead, a quick stint into platform engineering and now back into DE.\n\nHonestly DS is fun, but most of the time the business doesn’t know what they are trying to solve. We had to dumb down a lot of time our research to have actionable “insights” and understandable research.\n\nI got into DE because I had the most experience on the team with ETL and software engineering, this led to me building the foundation and the infra our team used (and is still using even though none of the original people still work there, but I have echoes and it seems what we built holds up after several years). Things like automated deployment of prod with IaC, quality tests and a bit of telemetry.\n\nOne of the great things from my experience as a DS that translated to DE, was good feature engineering, because I could translate easily the needs of DS into things we could do on the platform.\n\nAnyway, I stayed in the field because it’s more fun to me to see pipelines go green, faster outputs, reliable stuff going day to day and automation. My friends still in DS seem to be focusing also a ton more on LLM lately and it’s not always the happiest experience.",
          "score": 2,
          "created_utc": "2025-12-27 17:34:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8dgsb",
          "author": "goblueioe42",
          "text": "Interesting question. I’ve mostly moved to pure platform Data Engineering roles and I am looking to move to Machine Learning Engineering in the future. I have a DS academic background. I find that DS work doesn’t have the code background needed as compared to DE. MLE/ platform seems to be a sweet spot with more focus on hard engineering but with some modeling built in. I think MLE is less exhaustive on call and less likely to be outsourced. So I am starting to look to pivot based on DE being heavily outsourced and MLE/platform Eng/ product ( analytics engineering) not being outsourced.",
          "score": 2,
          "created_utc": "2025-12-27 18:43:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8gyoy",
          "author": "addictzz",
          "text": "I want to say I like data scientist more but deep in my heart actually I enjoy Engineering role better. Something about optimizing performance, ensuring pipeline operations runs well, and dataOps intrigues me. Data Science is more about research & experimentation. Without properly scoped problem statement and sufficient good data, you may end up aimless.\n\nHowever from business perspectives, data science has more potential in giving more business impact. Anyway if you want a hybrid of both, try MLOps or ML Deployment Engineer.",
          "score": 2,
          "created_utc": "2025-12-27 19:01:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8qbu0",
          "author": "cmcclu5",
          "text": "I’ve done both. I enjoyed the weird problems I got while I was a data scientist, but I enjoy the structure of data engineering more, plus I feel like the true average salary is better with a larger job market. I’ve always worked for startups, too, so I get to do a little of everything instead of being hard-locked into pure DE. When I consult, I generally do data science work, though. Much more interesting problems.",
          "score": 2,
          "created_utc": "2025-12-27 19:50:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8t0aj",
          "author": "ditalinidog",
          "text": "I’m a DA that gets both DE and DS projects cause our specific team is small. Honestly I feel the impact of DE projects a lot quicker because they often either focus on automation or improving access to data. We did one very impactful LLM project but the rest of the ML stuff has been experimental with varying interest from stakeholders. I imagine it would vastly depend on the field, but in ours it feels like people are more impressed with clean data and quick processes than setting up more predictive analytics. \n\nI would also say the experimentation of DS projects can be a little more frustrating / slow. A lot of time spent testing something that turns out to not really work and then going back to the drawing board. And sometimes you have to look back to the DE side of the pipeline because the data just isn’t clean or accessible enough.",
          "score": 2,
          "created_utc": "2025-12-27 20:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7ppnr",
          "author": "Illustrious_Web_2774",
          "text": "Data engineer. I find data scientist (traditional ML / analytics) role too boring.",
          "score": 4,
          "created_utc": "2025-12-27 16:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7un4u",
              "author": "Axel_F_ImABiznessMan",
              "text": "What makes engineering more interesting out of curiosity?",
              "score": 3,
              "created_utc": "2025-12-27 17:08:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7yrp4",
                  "author": "Illustrious_Web_2774",
                  "text": "It's not that engineering is inherently more interesting, but it offers more guaranteed rewarding experience.\n\n\nMany data projects felt like a waste of time (no tangible result). While I could always deliver something meaningful in a engineering project.",
                  "score": 11,
                  "created_utc": "2025-12-27 17:30:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9gb3m",
                  "author": "Fit-Employee-4393",
                  "text": "As a ML oriented DS the most fun part of it is the DE kinda stuff like feature engineering. Modeling is honestly kind of annoying/boring unless you’re doing cutting edge stuff, which is rarely necessary. Pretty much just choose big model, do hyperparam optimization, set up MLflow, and then spend a ton of time making sure the stakeholder is actually using what they asked for. Then you have to prove impact somehow, which is only there if it’s being used.\n\nStats oriented DS can be very interesting, because experiments are fun to set up, but god forbid your stakeholders disagree with the results.\n\nI’m looking at DE roles for this reason. They ask for data, you give them data. I see DE work as fun logic puzzles.",
                  "score": 2,
                  "created_utc": "2025-12-27 22:11:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8lmc8",
          "author": "PandaJunk",
          "text": "I work at a smaller org, so I do a bit of both. Combing both gives me insight into the complete protyping to production life cycle, which is pretty interesting. And now with more agentuc LLM workflows (e.g., opencode), I don't have to do a bunch of the boilerplate stuff, and instead get to focus on the bigger picture things.",
          "score": 1,
          "created_utc": "2025-12-27 19:25:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8yurx",
          "author": "varwave",
          "text": "Ignoring titles, ask yourself if you want to research or be a developer and look at job requirements and follow that rigorously. Software engineering and research roles has always existed and titles and their meaning change over time \n\nPersonally, I’m in a hybrid role. I occasionally use my statistics background. But mostly for understanding what’s happening under the hood, when developing internal full stack applications + ETL to assist medical research. At least in healthcare, the tried and true methods are the most used and generative AI doesn’t affect selection of a “best” method…if logistic regression works well, then it gets used. My colleagues generally fall more firmly into SWE, data analyst or quantitative PhD researcher roles. The PhDs come up with what methods to use. I write software \n\nThe non-researcher “data scientist” roles that are at the BS/MS level are more competitive with the rise of cash cow MS programs that can produce graduates that can kinda code and kinda do analysis. Networking is big here too. You’re probably better off doing one of the two well and kinda able to do the other",
          "score": 1,
          "created_utc": "2025-12-27 20:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8zimv",
          "author": "reelznfeelz",
          "text": "DE because I went independent and that’s the work that keeps coming in.",
          "score": 1,
          "created_utc": "2025-12-27 20:40:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw958mc",
          "author": "n_ex",
          "text": "I worked as a DS in a few places that didn’t have the DE role, so I did everything. At some point a friend wanted to refer me for a DS role, so I sent her my CV and she mentioned that it read more like a DE CV. I failed the interview as I wasn’t senior enough, but that led me into DE, as that comment stayed with me and helped me realize that was the part I enjoyed doing the most - making sure the data is correct, clean, fresh and modelled well for the use-case :)\n\nI also realized I found DS work quite boring - it felt like i was mostly just chasing slightly better accuracy for weeks by experimenting with different parameters. So I started applying for DE roles and have been in DE ever since. \n\nI enjoy working with people who use the data, to understand their requirements and provide the necessary data so that is correct, fresh, easily accessible, etc. I love problem solving and that’s probably the most fun part for me - to understand the data I need to understand the inner workings of a system, so I got to learn about many different things (e.g. ecommerce, production, supply chains, finance, etc) and got exposed to many different tools (cloud platforms, libraries, orchestrators, databases, etc). \n\nI tried a few other roles in IT before getting into data science (QA, full stack dev), but found them too monotonous. \n\nOverall, I really like the work and find it challenging and exciting :)",
          "score": 1,
          "created_utc": "2025-12-27 21:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw98hmp",
          "author": "Flav265",
          "text": "DE has clearly more demand right now, at least where I live. \nAs for myself, I am a DE but I like to do some DS sometimes. I think the role of ML engineer exist for people who want to do both and that is what I will try to move to next year",
          "score": 1,
          "created_utc": "2025-12-27 21:29:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9he6i",
          "author": "Doto_bird",
          "text": "Don't try and choose man. Take the good opportunities coming your way and don't overthink it. You'll probably do great in either role and the better you get the more you'll enjoy it.",
          "score": 1,
          "created_utc": "2025-12-27 22:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9hqfv",
          "author": "Maskrade_",
          "text": "It's better to think in terms of skillsets, not titles. These titles didn't really exist a few years back, and I doubt they will exist much in the future. \n\nTo clarify, neither job title is meaningful since the responsibilities and tasks vary dramatically from business to business, industry to industry. \n\nA data scientist at Meta does something dramatically different from a data scientist at Macy's, or a manufacturer, or the military. \n\nIn my experience there are very, very, very few profitable or true applications of what people would call \"data science\" in the economy. I'd scope the skillsets of data science to predictive analytics & productionizing machine learning algorithms either inside a product (like a website or a machine), or, inside of a company function. \n\nTo be specific, I think it makes sense for \"data scientists\" to A/B test a change to the social feed at Meta, or, maybe working on a self-driving car problem at Wayne or Tesla. This is because they'll have a better command of statistics and algorithms. \n\nMany other tasks get assigned to 'data scientists' which are way outside of that scope, that it gets to the point where the words become meaningless - if you research the origin of the word \"data science\" and you'll see why this is so confusing. \n\n\"Data engineering\" is a bit more tangible and 'standard' in my opinion, because it is mostly a rebrand of  traditional IT and data warehousing work inside of a company, which has been a valuable skillset for decades now. \n\nAll of that aside, I think the difference between most large administrative jobs is actually personality, not skillet, especially with AI. So, maybe gravitate to where your personality fits best and you'll be fine.",
          "score": 1,
          "created_utc": "2025-12-27 22:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9ntdk",
          "author": "22Maxx",
          "text": "I work a lot of with data while covering from a wide scope from business expert to DA, to DS to DE (not specialized in either role).\n\nFrom my experience DS is a niche that 99% of all companies don't & can't afford. Additionally the requirements for an excellent DS are in many cases significantly higher compared with DE. Unless you are working with \"simple/standard\" data, a DS need to have very good domain knowledge in addition to maths/stats & programming knowledge. Obviously you will rarely find people that have this kind of broad skillset. Another thing to consider is that a good data foundation & good data engineering is required to enable data science. For DE the job description is much clearer and also the task focus narrower. The DE skillset is highly transferable which allows for more options (switching to a different industry is easily possible, not so much in DS).\n\nIn summary DS roles only make sense with (large) organization that have a good data infrastructure in place. In all other cases you will very likely spend only a fraction of your time with actually DS work. The rest of the time you will try to get the data you need (business/DE work) while being ask to present the results from it (DA work).",
          "score": 1,
          "created_utc": "2025-12-27 22:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9qjkc",
          "author": "ironmagnesiumzinc",
          "text": "I prefer engineering but end up doing more data science/ml because there’s more jobs for it currently (at least in my industry). Also that’s what my studies were. Also data engineer interviews seem to be way harder for me (or in general) it seems like.",
          "score": 1,
          "created_utc": "2025-12-27 23:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwacxn5",
          "author": "Trick-Interaction396",
          "text": "DS is more interesting but a lot of my work ended up in the garbage. DE is less interesting but ALWAYS useful. I prefer hybrid roles.",
          "score": 1,
          "created_utc": "2025-12-28 01:14:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaw0zc",
          "author": "Own_Astronomer_2659",
          "text": "I worked as both DE and DS in both consulting and product companies. \n\nDE is essential backbone for any kind of reports in the organisation. DE work is invaluable and forms the base for DS in any organisation. Good DE designs lead to better data management \n, governance, and, data maturity of the company. Building custom frameworks and data products is a greenfield experience which can catapult your career and depth of knowledge. \n\nDS is more of a costcenter to the company which is a nice to have function. DS teams need to justify their work because they predict stuff about the company and how their predictions CAN benefit the company. Very few use-cases are GenAI based and most of them are statistical problems when you look at the structured data. \nAt the moment, Most of the DS outcomes are packaged into a product from all major cloud providers, which leads to MLOps - maintenance of DS lifecycle. The bleeding edge jobs are where DS PhD’s work out the statistical functions and the math for the nuanced problem statement and save huge money by not spending it on GPU’s. \n\nSo, DE is more desirable(from my experience).",
          "score": 1,
          "created_utc": "2025-12-28 03:08:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwazzt5",
          "author": "NoblestOfSteeds",
          "text": "Worked in both, landed in DS. Internal DS opportunities better than DE. \n\nPros of my DS tenure: agency. My team and my role is highly valued and trusted so our ideas and input make real impact. Plus math is fun!\n\nPros of DE: see ticket take ticket. Much less ambiguous. Code quality is higher because of better engineering.",
          "score": 1,
          "created_utc": "2025-12-28 03:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb4q2t",
          "author": "sumonigupta",
          "text": "DE because I need A to B destination and with DS, you rarely know if B is next to A or Z is next to A",
          "score": 1,
          "created_utc": "2025-12-28 04:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb8pq7",
          "author": "thisfunnieguy",
          "text": "At the time I made the choice I thought that \n- most DS teams were 3-7 person shops with limited career tracks\n- a lot of “great” DS roles were for PHDs and I was woefully under educated (only a BS)\n- every DS event I went to talked about the same limiting factor — data access/pipelines. \n- those same events had plenty of stories of ppl doing tons of work that never got approved (not good enough metrics, cost, politics)\n\nThat implied a much bigger demand and opportunity on one end. So I went all in on jobs as a DE.",
          "score": 1,
          "created_utc": "2025-12-28 04:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb90vt",
          "author": "younggungho91",
          "text": "However in my experience, management loves to sell DS then DE. It's fancier and the staff gets paid better.",
          "score": 1,
          "created_utc": "2025-12-28 04:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbbvo6",
          "author": "kerkgx",
          "text": "I chose DE because I don't really like talking to people explaining my features or my analysis on a dataset.",
          "score": 1,
          "created_utc": "2025-12-28 04:50:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbjjw8",
          "author": "Appropriate-Voice407",
          "text": "I chose engineering because I’m drawn to the idea of building infrastructure—it’s grounded, tangible, and practical. I started in data science, but it often felt much looser: everyone had their own interpretation of what it should be, and in many cases upper management didn’t really care about the numbers or insights. Decisions were often made regardless, or based on selectively chosen metrics that supported a predefined narrative.\n\nAs a result, data science can feel messier and more unpredictable as a career path. The experience varies widely depending on the company. I also strongly prefer working in organizations where data is core to the product itself, rather than merely a support function for operations—this matters for both roles, but especially for data science.\n\nI also wouldn’t stress too much about choosing the right one, by experience the only way of really knowing is experience it and see what you like more, its very easy to switch between these 2 anytime.",
          "score": 1,
          "created_utc": "2025-12-28 05:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbqct9",
          "author": "charlesowo445",
          "text": "Guys help, I started of as An analyst but now I have made both models and pipeline , what's should my title be , data scientist or data engineer now?",
          "score": 1,
          "created_utc": "2025-12-28 06:44:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc1qer",
          "author": "set92",
          "text": "Both are nice, and maybe you need to experience both to have the real experience.\n\nIn DS I liked be able to plot the data, play with models, every day was something new to play with. But I left because I felt my maths were not good enough, impostor syndrome.\n\nDE I feel is closer to code, I spend most of the time with DAGs on Airflow. It's more boring since is always lines of codes, and thinking of all the possible cases on how the data can break, and worrying about the data not being ingested, creating tests, having the raw data for debugging later, not understanding why Airflow doesn't have a good communication with Snowflake.. I feel what I do now can affect more departments, so I worry more 😅.",
          "score": 1,
          "created_utc": "2025-12-28 08:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcrku9",
          "author": "peterxsyd",
          "text": "Several years as a data scientist, then data engineer, then software engineer / low latency trading engineer. If you are starting, I’d go data analyst/ ”scientist” so that you understand the data, business context without going through someone else. Otherwise, you’ll be handicapped long-term.\n\nThen i’d go data engineer as your technical skills improve, and software engineer only if you are that way inclined.\n\nUnless you are building AI models with a PhD for anthropic, the lack of clarity and consistency in what a Data Scientist actually does will plague you, and you are at the mercy of too many people who want different things. Running an ML model over data is trivial, and relies on… producing a good dataset, which is data engineering.\n\nThese days Claude makes most grunt work super easy, so focusing on those fundamentals with enough math foundation to be “accurate”… will set you up pretty well provided you take the time to lean everything properly and focus on delivering what the business actually wants. Then you will do fine, and work out what you like. Good luck.",
          "score": 1,
          "created_utc": "2025-12-28 12:32:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf199d",
          "author": "sebastiandang",
          "text": "Its kinda hard, bc when you ask a group of DE you will get the aswser:DE but when you decide to ask a group of DS: its DS. However, DE and DS dont have too much overlapping, its depends on your education background, if you love researching, do stuffs that impacts on business, chaging the views on stakeholders => DS.  \nIf you love to work technical stuff, DE archh, data flows, bring the Data and Engineering => DE",
          "score": 1,
          "created_utc": "2025-12-28 19:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgcf8k",
          "author": "TurboMuffin12",
          "text": "Data Engineering - there are more real needs.  Even in this world of AI bs the reality is the business doesn't have sufficient data / piplines to answer simple questions.",
          "score": 1,
          "created_utc": "2025-12-28 23:51:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgykp8",
          "author": "tecedu",
          "text": "i sit in the middle but if I could choose something, it would be DE, my mind is not built to be a data scientist, I can hack around things using blogs or paper but cant create anything from scratch.",
          "score": 1,
          "created_utc": "2025-12-29 01:53:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw874d0",
          "author": "konwiddak",
          "text": "I did a few years of Data Science and then moved into Data Engineering. Personally I'm finding the Data Engineering more rewarding. It delivers real and significant business value quickly. Honestly, for most businesses, Data Science really doesn't bring that much value. It's the cherry on the top once you've otherwise honed your business. Data Engineering allows you to get to the place where the business is honed enough for Data Science to be worthwhile.",
          "score": 1,
          "created_utc": "2025-12-27 18:12:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1iezn",
      "title": "Can we do actual data engineering?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q1iezn/can_we_do_actual_data_engineering/",
      "author": "marketlurker",
      "created_utc": "2026-01-01 23:22:22",
      "score": 134,
      "num_comments": 55,
      "upvote_ratio": 0.82,
      "text": "Is there any way to get this subreddit back to actual data engineering? The vast majority of posts here are how do I use <fill in the blank> tool or compare <tool1> to <tool2>. If you are worried about how a given tool works, you aren't doing data engineering. Engineering is so much more and tools are near the bottom of the list of things you need to worry about. \n\n<rant>The one thing this subreddit does tell me is that the Databricks marketing has earned their yearend bonus. The number of people using the name medallion architecture and the associated colors is off the hook. These design patterns have been used and well documented for over 30 years. Giving them a new name and a Databricks coat of paint doesn't change that. It does however cause confusion because there are people out there that think this is new.</rant>",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q1iezn/can_we_do_actual_data_engineering/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx5tvyt",
          "author": "rycolos",
          "text": "I'll take someone asking how to do an scd 2 snapshot in dbt a million times over some doofus sharing his AI-written linkedin or substack hype shitpost for \"conversation\"",
          "score": 191,
          "created_utc": "2026-01-01 23:28:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5ug0l",
              "author": "SirGreybush",
              "text": "Upvotey for doofus use. Happy Festivus.",
              "score": 21,
              "created_utc": "2026-01-01 23:31:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx5wymg",
              "author": "5pitt4",
              "text": "I'm new, where can i learn about scd 2 (and I'm assuming there are other types like1 or 3)?",
              "score": 6,
              "created_utc": "2026-01-01 23:45:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx67twz",
                  "author": "TheOneWhoSendsLetter",
                  "text": "Kimball's Data Warehouse Toolkit",
                  "score": 30,
                  "created_utc": "2026-01-02 00:46:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx84gvz",
                  "author": "daguito81",
                  "text": "SCD is Slowly Changing Dimesions. And it’s basically type 1 and 3 \n\nSimplifying a bit here, before the NF police comes. Imagine you have a table for customers, in that table you have an address. I have many transactions every day but “a new customer” or change a customer is not something that happens all the time. So that table changes “Slowly”   That’s the SCD. It’s a fancy way to say “A dimensional table that changes” \n\nNow type 1 vs type 2 basically determines what you do when something changes. \n\nLet’s say you changed your address. I can just go and update the customer table with your id and your new address and call it a day. That’s a Type 1. \n\nHowever let’s say you need to retain all the addresses you’ve had historically. So you create some new metadata columns. A “current” column, and a couple of date columns. \n\nSo now when you changed your address I basically copy your line changed the old one “current” to false than when th change happens. And the new one  set current to true. \n\nSo now if I want your address infilter by your id and tha current is True. If I want your old ones. Filter by current is False and the date columns will order them. \n\nDoing that “basically not replacing the line but instead creating a new one for every change “ makes it a Type 2 \n\nSo SCD1 and SCD2 are basically those",
                  "score": 7,
                  "created_utc": "2026-01-02 08:59:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6m7nf",
                  "author": "marketlurker",
                  "text": "It isn't just slowly changing dimensions you need to understand. Look at all the types of SCDs.\n\nThere are different types of tables, like associative, when to use them and when not to. I'm looking at you temp tables (vs CTEs).\n\nThe different types of normalization and why no one outside of an educational environment using anything beyond 3NF. Has anyone used Boyce-Codd in real life? Understand why so many crappy tools out there think the world begins and ends with 1NF (lists).\n\nUnderstand the different types of joins and when to use them.\n\nThere are so many useful things to understand in DE. I am starting to use someone using the phrase \"medallion architecture\" or \"gold layer\" as a canary in the coal mine for a relative newbie in the discipline.",
                  "score": 7,
                  "created_utc": "2026-01-02 02:13:26",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx7mqke",
                  "author": "its_PlZZA_time",
                  "text": "The Wikipedia article is actually pretty good\n\nhttps://en.wikipedia.org/wiki/Slowly_changing_dimension",
                  "score": 1,
                  "created_utc": "2026-01-02 06:18:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx8zfcw",
                  "author": "Thinker_Assignment",
                  "text": "wikipedia, google. We host some colab code demos that come up when you google.\n\ndid you know why we call it slowly? because if it changes faster than our load interval (like daily) then we do not capture the change - so it only works with \"slowly\" changing dimensions\n\nSCD2 is the \"main\" you use for historisation and all the rest from 2+ are just derivatives for different use cases",
                  "score": 1,
                  "created_utc": "2026-01-02 13:22:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6u95m",
                  "author": "SRMPDX",
                  "text": "Claude. Ok I'm only slightly kidding.",
                  "score": 1,
                  "created_utc": "2026-01-02 03:02:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5xaqz",
                  "author": "beta_ketone",
                  "text": "Google, wikipedia...",
                  "score": 0,
                  "created_utc": "2026-01-01 23:47:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5wpx1",
          "author": "SmallAd3697",
          "text": "There are so many different types of folks using big data tools.  Many aren't even coders, aside from using a minimal amount of notebook-hosted python and SQL.\n\nI think it is important to realize that many folks build solutions by connecting a bunch of software components together using configuration. They focus on these third-party components and show little interest in underlying software engineering concepts. Some may not even know the implication when choosing between rowstore and columnstore, or between one language/runtime or another.\n\nSometimes this just seems like a community of chefs who talk about microwave dinners, and they focus primarily on the type of microwave that cooks the fastest. I find that data engineers can become quickly stunted, as compared to other types of software engineers.",
          "score": 25,
          "created_utc": "2026-01-01 23:44:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5u1gs",
          "author": "ThroughTheWire",
          "text": "unfortunately most people nowadays aren't doing \"actual\" data engineering anymore. that's probably like 10-20 percent of people with the title",
          "score": 70,
          "created_utc": "2026-01-01 23:29:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx849wp",
              "author": "Feisty_Following9720",
              "text": "I was saying that 10 years ago. Its gotta be lower now.",
              "score": 5,
              "created_utc": "2026-01-02 08:58:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx9ux4s",
              "author": "Little_Kitty",
              "text": "I feel a lot of the people here are actually just configuring some json held inside a python script to change filenames / paths.  Very little thinking about memory usage, spilling to disk, cluster complexities, optimal data types, incrementalisation etc.",
              "score": 1,
              "created_utc": "2026-01-02 16:12:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5ugma",
          "author": "peterxsyd",
          "text": "Dude -  I think that - today - people think that's what data engineering is. And sadly it's what it's become - a funnel for overpriced DataBricks and Microsoft. And all the consultants with partnerships with those guys recommend them - and the CIO's accept the recommendations and it's all a big fugaze.\n\nData modelling and engineering actual information about what is going on in a business to show it which is so much more important - I actually genuinely don't think people focus there now. And those companies don't care at all - because - people are paying 10K a month (or more) for tables when they have 5 million rows, still hitting spark java jvm out of memory errors and a postgres database would literally solve their problems way easier and more effectively.\n\nI'm with ya.",
          "score": 40,
          "created_utc": "2026-01-01 23:31:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7tqq4",
              "author": "Ddog78",
              "text": "It's a bit sad to be honest.\n\nI was curious so I applied for a few positions. Nearly all the interviewers were focused on either tech stack or weird architectural questions that clearly had one right answer.",
              "score": 2,
              "created_utc": "2026-01-02 07:18:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx8givh",
              "author": "girlgonevegan",
              "text": "As someone who has to work in a Marketing Automation Platform at the enterprise level dealing with high volumes of data, I appreciate this insight. I’ve never held a DE title as it’s not even formally my background, but the work I do feels like engineering and often feels more complex than what seems to get the most funding and resources. It’s hard for me to wrap my head around. It’s as if decision makers cannot differentiate between production databases and warehouses. Bill Inmon wrote about the “[Data Warehouse Blues](https://www.linkedin.com/posts/billinmon_activity-7412163904841109504-58UF?utm_source=share&utm_medium=member_ios&rcm=ACoAAAYK0cMB7NggJ0_c4Cnsb2WcD4klavNLF8E),” and I finally feel like I’m not crazy.",
              "score": 1,
              "created_utc": "2026-01-02 10:53:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8h12q",
                  "author": "girlgonevegan",
                  "text": "What’s astonishing to me is that I have been apart of companies where the internal IT department will admit they are doing these implementations KNOWING they will not produce the desired outcome. It’s as if they themselves are just going through the motions because this is their “bullshit job,” and they blame the vendors because they don’t think any of it actually works (because they’ve never seen it since they don’t have the skillset/understanding of application data).",
                  "score": 1,
                  "created_utc": "2026-01-02 10:57:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5xrfs",
          "author": "Automatic_Red",
          "text": "I'm just happy the Excel formatting questions are fairly minimal.",
          "score": 10,
          "created_utc": "2026-01-01 23:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6j4ys",
          "author": "riv3rtrip",
          "text": "There isn't anything to talk about. Do you want me to post my SQL queries or something",
          "score": 15,
          "created_utc": "2026-01-02 01:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx78kge",
              "author": "ExOsc2",
              "text": "There's always\n\n    SELECT JEFF\n    FROM JEFF\n    WHERE JEFF\n    GROUP BY JEFF\n    HAVING JEFF\n    QUALIFY JEFF OVER (PARTITION BY JEFF ORDER BY JEFF) JEFF",
              "score": 12,
              "created_utc": "2026-01-02 04:34:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7w5bd",
                  "author": "TheOneWhoSendsLetter",
                  "text": "    ORDER BY JEFF DESC\n    LIMIT JEFF\n    OFFSET JEFF ROWS",
                  "score": 4,
                  "created_utc": "2026-01-02 07:41:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7qyiw",
                  "author": "WhipsAndMarkovChains",
                  "text": "MY NAME JEFF",
                  "score": 3,
                  "created_utc": "2026-01-02 06:53:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx90o7z",
              "author": "Thinker_Assignment",
              "text": "as a dev and vendor i think tool discussion is important because without it, we have no quality, just sales pitches. Of course, dev tool discussion, not what we do here bc here we don't discuss nuance, we just parrot commonly accepted points.\n\nBc devtools are not general b2c consumer goods, paid ads don't work (CR is tiny, ads are expensive) so it's either devtool vendors try to reach communities via devrels, or they stop trying and instead of building good tools they build a salesforce and sell shit to your manager which quickly becomes your problem.\n\nThen you can come on here and complain about how you're hating your job and developing no future-proof skills while asking what is SCD2 because you never developed the ability to help yourself and a question you could learn the answer to in 5min suddenly becomes your personality.\n\nBut the real problem is different: There's very little love for the craft out there. Producing useful content takes time and thinking that the vast majority will not invest unless they get something out of it - such as marketing.\n\nI would love to read interesting content but frankly it's mostly vendors and consultants that take the time to express.",
              "score": 1,
              "created_utc": "2026-01-02 13:30:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx643bw",
          "author": "NoleMercy05",
          "text": "Did you pass the Data Engineering PE exam?\n\nIt's just a made up Engineering title with no real definition.",
          "score": 11,
          "created_utc": "2026-01-02 00:25:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5vn83",
          "author": "wingman_anytime",
          "text": "At my company, “Data engineering” has turned into a slop bucket of people who don’t have the rigor to engineer robust solutions, but had some data science background or some SQL or Python experience, so they throw them into our Data org. \n\nHaving been working on data warehouses and ETL pipelines since 2001, I just shake my head and keep on doing my job.",
          "score": 15,
          "created_utc": "2026-01-01 23:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9l79g",
              "author": "Busy-Ad1968",
              "text": "My colleagues once needed access to a computing cluster to calculate a result based on a table with several 10 million rows. Their code took 6 hours to calculate the result, and they said optimization wouldn't help. They requested a server with a GPU, wrote requests to a neighboring department, and went to meetings about it... well, you get the idea... In the end, after I optimized their code, the processing took less than a minute on a regular laptop. I was surprised that they tried to hush it up as quickly as possible and were very unhappy, because they had big plans for cooperation with the neighboring department, and it even benefited them that their code was running so slowly.",
              "score": 1,
              "created_utc": "2026-01-02 15:25:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6i9ro",
          "author": "StewieGriffin26",
          "text": "Just you wait, Databricks just recently rolled out.... stored procedures. Lol",
          "score": 4,
          "created_utc": "2026-01-02 01:49:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7xcj1",
          "author": "VEMODMASKINEN",
          "text": "Actual data engineering is building data platforms and tools. Like what Google did in the early 2000's with MapReduce and Yahoo with Hadoop. \n\n\nNo one here does that that kind of software engineering focused on data.\n\n\nMost people here just do ETL. In other words they're what we used to call ETL Devs. OP included. ",
          "score": 4,
          "created_utc": "2026-01-02 07:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5uhlu",
          "author": "shockjaw",
          "text": "You gotta point. There’s more to this discipline than just arguing over which company we should pay money to.",
          "score": 9,
          "created_utc": "2026-01-01 23:31:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6uo6q",
          "author": "SRMPDX",
          "text": "Sure, go for it. I'll read it and maybe contribute. What's stopping you?",
          "score": 3,
          "created_utc": "2026-01-02 03:04:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx70gim",
          "author": "dubh31241",
          "text": "While we're on this topic. Can the majority of the current state of data engineering workflows be solved by running containerized sparks jobs in k8, storing data in some kind of columnar format and using a catalog system?\n\n I am DevOps/Infrastructure engineer coming into the data engineering space. I feel like Databricks, Snowflake and AWS's clusterfuck of services are doing relatively the same thing and charging a shit ton for this. I get the extended data governance part, but thats just rules and business logic. What am I missing?",
          "score": 3,
          "created_utc": "2026-01-02 03:41:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx783z3",
              "author": "PrestigiousAnt3766",
              "text": "Yeah, that basically describes databricks. \n\n\nYou pay for convenience and not having to manage infra.",
              "score": 1,
              "created_utc": "2026-01-02 04:31:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6aogd",
          "author": "CallAnAmbulancee",
          "text": "i am new into data engineering field and i am just keep hearing influencers and people in youtube just keep talking in the tools and this stuff, so how to truly learn data engineering and it's real concepts?",
          "score": 3,
          "created_utc": "2026-01-02 01:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6xa7r",
              "author": "pekingducksoup",
              "text": "I'm probably going to get some hate on this, but I find that asking LLM questions helps. \nStart with high level i.e. \"please explain the fundamentals of data engineering\"\nAsk for references to books, articles, websites etc. \nDrill down on each of the topics, drill down further for patterns etc. \n\n\nI had a skim through the fundamentals of data engineering by Reis and Housley, that seems like a pretty good place to start.\n\n\nModelling is also pretty important for DE work, Kimbell is good for understanding concepts of data delivery. Even if you don't use star schema.\nInmon books are also pretty good from what I understand, I haven't read them though.\nThere are a others. Data Vault is pretty interesting as well, but I don't like implementing it personally.",
              "score": 7,
              "created_utc": "2026-01-02 03:21:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx6hhuv",
              "author": "peterxsyd",
              "text": "Avoid listening to influencer shit shows go read a valid book on it and ideally work and get mentored by someone who knows what they are doing.",
              "score": 3,
              "created_utc": "2026-01-02 01:44:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx6u0ex",
              "author": "marketlurker",
              "text": "Check out the comment [here](https://www.reddit.com/r/dataengineering/comments/1q1iezn/comment/nx6m7nf/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).",
              "score": 2,
              "created_utc": "2026-01-02 03:00:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5uhdm",
          "author": "Winter-Statement7322",
          "text": "“The vast majority of posts here are how do I use <fill in the blank> tool or compare <tool1> to <tool2>. If you are worried about how a given tool works, you aren't doing data engineering.”\n\nSome people are coming from other technologies and are legitimately interested in how a technology they’ve previously used compares to one this are going to/will have to use",
          "score": 4,
          "created_utc": "2026-01-01 23:31:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6tvcw",
              "author": "marketlurker",
              "text": "That is a fair use case, but it has little to do with data engineering. Product comparison is just one small area in this discipline. So many other things can have a much larger impact on your system.",
              "score": 0,
              "created_utc": "2026-01-02 02:59:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7dsm5",
          "author": "ithinkiboughtadingo",
          "text": "No we're just going to keep asking how to switch from [insert whatever career path here] into DE over and over until the heat death of the universe",
          "score": 1,
          "created_utc": "2026-01-02 05:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx88ru4",
          "author": "one-step-back-04",
          "text": "Hard agree on the spirit of this. Even in reality bhaaai, most real data engineering work I see has very little to do with debating tools and a lot to do with **constraints**. Bad source data, unclear ownership, changing business logic, SLAs no one documented, and pipelines that need to survive people, not just demos.\n\nTools come and go. For me, the actual work is deciding what deserves to be modeled, what can be wrong, what must never be wrong, and how fast the business needs to know. That’s the part that rarely gets discussed.\n\nAlso yes to the medallion point. Renaming long-standing patterns isn’t innovation, it’s branding. Useful shorthand maybe, but it becomes a problem when people think the diagram is the architecture.\n\nWould love to see more posts on failure modes, trade-offs, and “this looked clean on paper but blew up in prod.” That’s where the engineering actually lives to me.",
          "score": 1,
          "created_utc": "2026-01-02 09:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8jo89",
          "author": "mailed",
          "text": "Most of us are/were serving analytics and machine learning use cases. Analytics solutions have been created using high level abstractions built by software engineers for decades. It was GUI driven tools for a long time and now it's these SaaS environments like Databricks or Fabric or Snowflake. This won't ever change. It's why a majority of data engineers still can't or won't write code outside of SQL and don't need to.  \n  \nIf you want to do what you think is \"actual\" data engineering, go join a company as a software engineer on projects that handle tons of live data. A lot of transactional systems are handling more volume than your average analytics solution. It's what DDIA was written about (and why I don't recommend data engineers reading it a priority - because of the above paragraph).  \n  \nI personally have left the field for somewhat similar reasons (alongside having one too many data engineering teams tell me I was more of a dev/devops/infrastructure guy).",
          "score": 1,
          "created_utc": "2026-01-02 11:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx62h3n",
          "author": "Silent_Calendar_4796",
          "text": "Data engineering is dead, AI replaced it",
          "score": -19,
          "created_utc": "2026-01-02 00:16:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx66wx3",
              "author": "Lucade2210",
              "text": "You are the problem. And also apparently not a data engineer.",
              "score": 9,
              "created_utc": "2026-01-02 00:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx83vux",
                  "author": "Silent_Calendar_4796",
                  "text": "Coping hard I see",
                  "score": -4,
                  "created_utc": "2026-01-02 08:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyecpd",
      "title": "Mid Senior Data Engineer struggling in this job market. Looking for honest advice.",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pyecpd/mid_senior_data_engineer_struggling_in_this_job/",
      "author": "This_Bird6184",
      "created_utc": "2025-12-29 05:58:33",
      "score": 107,
      "num_comments": 58,
      "upvote_ratio": 0.9,
      "text": "Hey everyone,\n\nI wanted to share my situation and get some honest perspective from this community.\n\nI’m a data engineer with 5 years of hands-on experience building and maintaining production pipelines. Most of my work has been around Spark (batch + streaming), Kafka, Airflow, cloud platforms (AWS and GCP), and large-scale data systems used by real business teams. I’ve worked on real-time event processing, data migrations, and high-volume pipelines, not just toy projects.\n\nDespite that, the current job hunt has been brutal.\n\nI’ve been applying consistently for months. I do get callbacks, recruiter screens, and even technical rounds. But I keep getting rejected late in the process or after hiring manager rounds. Sometimes the feedback is vague. Sometimes there’s no feedback at all. Roles get paused. Headcount disappears. Or they suddenly want an exact internal tech match even though the JD said otherwise.\n\nWhat’s making this harder is the pressure outside work. I’m managing rent, education costs, and visa timelines, so the uncertainty is mentally exhausting. I know I’m capable, I know I’ve delivered in real production environments, but this market makes you question everything.\n\nI’m trying to understand a few things:\n\n\t•\tIs this level of rejection normal right now even for experienced data engineers?\n\n\t•\tAre companies strongly preferring very narrow stack matches over fundamentals?\n\n\t•\tIs the market simply oversaturated, or am I missing something obvious in how I’m interviewing or positioning myself?\n\n\t•\tFor those who recently landed roles, what actually made the difference?\n\nI’m not looking for sympathy. I genuinely want to improve and adapt. If the answer is “wait it out,” I can accept that. If the answer is “your approach is wrong,” I want to fix it.\n\nAppreciate any real advice, especially from people actively hiring or who recently went through the same thing.\n\nThanks for reading.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pyecpd/mid_senior_data_engineer_struggling_in_this_job/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwiz4vz",
          "author": "ThroughTheWire",
          "text": "you definitely buried the lede with the visa timelines bit being buried in the post.\n\ndealing with visas is a huge headache for companies rn, especially in the USA. not sure where you are from or if this is about the job market in America specifically but in general people are really avoiding making visa hires everywhere rn",
          "score": 70,
          "created_utc": "2025-12-29 11:02:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj5irf",
              "author": "financialthrowaw2020",
              "text": " Yep. It's also just logical: there are many unemployed people who are citizens. The jobs should go to them before anyone else.",
              "score": 38,
              "created_utc": "2025-12-29 11:56:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwmwync",
              "author": "reviverevival",
              "text": "There's also a compliance factor, where I work we have government (state and federal) clients and it's possible that with changes coming down the pipe that non-us citizens won't even be allowed to look at the relevant data. They are a minority of our customers, and the restrictions aren't even set in stone yet, but it doesn't make sense for us to take the risk and hire someone who can't work with the data from _all_ our customers if there's any other option. (Not to even mention the visa uncertainties)",
              "score": 5,
              "created_utc": "2025-12-29 23:40:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi6w74",
          "author": "vikster1",
          "text": "if you are us based and have a visa, maybe that's a reason too? maybe some companies don't want the extra layer of complexity given the current administration and their \"handling\" of humans that could maybe not be born in the us.",
          "score": 57,
          "created_utc": "2025-12-29 06:42:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiepmw",
              "author": "JohnPaulDavyJones",
              "text": "One big issue adjacent to that is that a lot of data teams just can’t sponsor a visa-holder, and they’ll make that clear in the job posting, but a boatload of folks who need visas still apply in the hopes that they’ll get hired and then, a few months in, spring the “Hey, I need visa support” message.\n\nMy old firm had that exact thing happen to them, and it burnt them so hard that so they just outright started filtering any resumes for folks who came from overseas and did a grad degree here, graduating less than a year prior to application. Ironically, one of the only folks who got through and got hired explicitly put that he was a naturalized US citizen on his resume.\n\nThat single filter removed probably 75% to 80% of our 1,000+ applicants per position, every time. Probably caught plenty of eligible folks with that broad of a net.",
              "score": 26,
              "created_utc": "2025-12-29 07:51:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwj5v24",
                  "author": "financialthrowaw2020",
                  "text": "Yep. Sponsorship doesn't work in this economy where there are plenty of good engineers in the labor pool. Visas are for when you can't find Americans to do the work. Plenty of Americans available right now.",
                  "score": 12,
                  "created_utc": "2025-12-29 11:59:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwifi2p",
                  "author": "vikster1",
                  "text": "agreed. same in germany. the amount of unwanted applications from India is astounding. no one wants to deal with that.",
                  "score": 24,
                  "created_utc": "2025-12-29 07:59:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwjt89x",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 2,
                  "created_utc": "2025-12-29 14:33:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwig5v1",
          "author": "RunnyYolkEgg",
          "text": "Technical people tend to be very…specific when it comes to soft skills. I’m not saying that this is your case but how good do you think you are when it comes to that? \n\nIn my experience, I got jobs that I’m not even qualified for just because they liked me. A recurrent comment I hear around peers who are interviewing candidates is that, because of the amount of experience required for the position, they know you will learn whatever they throw at you so they focus on getting someone who clicks with them. Good communication, good presentation, knows how to move the conversation without making it awkward, shows ownership.\n\nNot an awkward guy in a hoodie asking for full remote position and just tanking questions in the interviews. Unfortunately, extroverted people get rewarded.\n\nJust trying to help and give you my experience. Good luck man!",
          "score": 24,
          "created_utc": "2025-12-29 08:05:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlc3hz",
              "author": "lzwzli",
              "text": "This.",
              "score": 2,
              "created_utc": "2025-12-29 18:56:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi5778",
          "author": "AZjackgrows",
          "text": "the job market is brutal right now and most companies are hiring internal whenever they can to retain talent. orgs are treading water seeing if the other shoe drops on this economic reset. \n\nfor what it’s worth, try to highlight business impact just as much as your technical skills. the missing piece that any org is looking for is someone who can work with the business to help identify, communicate and develop requirements that lead to value delivery. there’s still a need for those who can run a transformation vs managing break-fix. \n\nif you want to go the ai buzz route, talk about how your work can lay foundation for ai and clean up data to enable agentic capabilities. \n\ni’d also speak to governance. it demonstrates that you don’t just build architecture that can’t be managed as systems evolve. we’re all stuck in legacy data environments that look like spaghetti on the back end. lots of us on the business side (who know our way around data) are getting really tired of hearing that simple requests will take months or can’t be done due to capacity constraints. \n\nmy company is buying all these things rather than building them right now bc we don’t have the internal staff with these future-forward skillsets… i’d much rather be building them in house but we just don’t have the people who can do it.",
          "score": 9,
          "created_utc": "2025-12-29 06:28:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi5ell",
              "author": "AZjackgrows",
              "text": "and think of looking at larger fortune 500 companies that have core business in supply chain, retail or finance vs technology firms.",
              "score": 1,
              "created_utc": "2025-12-29 06:30:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj5af7",
          "author": "financialthrowaw2020",
          "text": "It's the visa. We're not hiring non-citizens. The market is already tough on citizens right now, so visa holders don't even get a first look. There are already too many talented engineers needing jobs and that's how it's supposed to work: the existing jobs are supposed to go to the citizens of this country before anyone else.",
          "score": 25,
          "created_utc": "2025-12-29 11:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi2f4l",
          "author": "No_Lifeguard_64",
          "text": "At 5 years of experience I wouldn't call you senior at all. You would barely be mid level at my company.\n\nWithout seeing your resume or anything the most actionable advice I can give you is network and look in unconventional spaces. Most jobs are gotten through connections and that is especially true in today's AI landscape where both company and applicant are using AI. Companies using AI to screen resumes and applicants using AI resumes. Its just bots talking to each other. That being said, almost every job I have gotten I have applied into the void and gotten lucky.\n\n As far as unconventional spaces, more people than you think need a data engineer. Its not just Fortune 500 tech companies. Medicine needs data engineers, agriculture needs data engineers, non-profits especially need data engineers.",
          "score": 54,
          "created_utc": "2025-12-29 06:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj9jxr",
              "author": "value_here",
              "text": "I've got about 3.5 years xp doing data engineering tasks as my main workload and I just took a new job, and they are insisting on giving me Senior DE as my title. Every company is different about titles.",
              "score": 15,
              "created_utc": "2025-12-29 12:28:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi3sog",
              "author": "This_Bird6184",
              "text": "That’s fair, and I appreciate the honest perspective.\n\nTitles definitely vary a lot by company. I’m not particularly attached to the “senior” label itself. What I care more about is the scope of work. In my previous roles, I’ve owned production pipelines end to end, worked on large-scale Spark and streaming systems, and supported business-critical use cases. Some companies call that senior, others call it mid-level, and I’m okay with that distinction.\n\nI fully agree with you on networking and the AI screening problem. It really does feel like bots talking to bots right now, which makes the process feel detached from actual engineering ability. I’ve landed roles both through cold applications and through luck, so your point about “applying into the void” resonates.\n\nThe unconventional spaces advice is solid. I’ve started expanding beyond traditional tech companies and am actively looking into healthcare, education, logistics, and other domains where data engineering is needed but hiring is less automated. That shift already feels more promising than chasing only big-name tech roles.\n\nIf you don’t mind sharing, I’d be curious how you’ve approached networking into those spaces. Did you focus on referrals, direct outreach to managers, or something else that worked consistently for you?",
              "score": 12,
              "created_utc": "2025-12-29 06:16:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwi9315",
                  "author": "lemonfunction",
                  "text": "can you try framing your experience around business needs/requirements? recruiters and hiring managers only glance at tech experience, but a good resume with business wins will make your resume stand out a ton.\n\nassuming you're applying in the usa, sponsorship might be adding to the turbulence you're experiencing",
                  "score": 10,
                  "created_utc": "2025-12-29 07:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwja74t",
              "author": "0MEGALUL-",
              "text": "Seniority is determent by responsibility, ownership and output, not by amount of years experience. \n\nYears of experience gives you more room to grow in those 3, but it’s not where the value is. It’s just something employers say to negotiate salary down. The value is in responsibility, ownership and output.\n\nNot dying isn’t really an accomplishment, is it? Neither a guarantee to be more skilled than someone with less experience.",
              "score": 7,
              "created_utc": "2025-12-29 12:33:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwk0kqs",
                  "author": "No_Lifeguard_64",
                  "text": "I am not giving complete product ownership to someone with only 5 years of experience but you do you.",
                  "score": 0,
                  "created_utc": "2025-12-29 15:12:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj2m03",
              "author": "GuhProdigy",
              "text": "\nThe title of data engineer has been mainstream for 10 maybe 12 years. saying 5 years isn’t senior, when in a lot of orgs you have principal positions, manager positions, and director positions in data engineering is overtly obtuse. Reminds me of the meme of company’s asking for more experience using a technology than the technology has even existed for. Stop parroting HR talking points that are meant to rationalize why they didn’t give you a promotion. \n\nWe all know YOE doesn’t equal technical prowess. I’m having too teach a 50 year old with over a decade in data git.",
              "score": 15,
              "created_utc": "2025-12-29 11:32:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjzuex",
                  "author": "SELECTaerial",
                  "text": "The title of data engineer has been mainstream for more like 3-5yrs…where are you getting this 10-12? The term didn’t even exist 12-15yrs ago\n\n10-12yrs ago there were software engineers or ETL developers or database developers…not data engineers",
                  "score": 1,
                  "created_utc": "2025-12-29 15:08:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwl8wyo",
                  "author": "gjionergqwebrlkbjg",
                  "text": "I'm sorry, are you under impression that people who were dealing with data modelling and other aspects like that before the title came to be suddenly disappeared? Who do you think was working on data warehouses, hadoop and whatnot?",
                  "score": 1,
                  "created_utc": "2025-12-29 18:42:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkbp71",
              "author": "Natural-Intelligence",
              "text": "What about a person with 10 YOE in SWE but 2 YOE in data engineering? What about the 3 YOE data engineer who passionate about the field and is much more productive than your 10 YOE data engineers?\n\nGate keeping with seniority is an excellent way to get rid of your high performers. Unfortunately, the pay is tied to the title.",
              "score": 3,
              "created_utc": "2025-12-29 16:06:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkfn35",
                  "author": "No_Lifeguard_64",
                  "text": "You're trying to be pedantic for no reason. Sorry my opinion hit you in the feelings but in this instance, all I have to go on is the stated years of experience and on average, I wouldn't trust someone with 5 years of experience with product ownership.",
                  "score": 2,
                  "created_utc": "2025-12-29 16:25:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjzgjc",
              "author": "SELECTaerial",
              "text": "I had the same initial thought. 5yrs isn’t senior anything imo",
              "score": 3,
              "created_utc": "2025-12-29 15:06:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwidndg",
          "author": "rajekum512",
          "text": "Sorry this might be the true case. There was a senior DE role opened in my company and they are open to remote  roles. But the position got backfilled by internal transfer who is half DE only but very strong domain experience. Second important issue is visa problem..We are ok with stable resource who doesn't bring heavy baggage of unstability",
          "score": 4,
          "created_utc": "2025-12-29 07:42:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj55qm",
          "author": "kali-jag",
          "text": "Where are you based out off?\n\nAlso from my expirience, December to feberuary is very dry period for interviews...",
          "score": 4,
          "created_utc": "2025-12-29 11:53:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjgk20",
          "author": "TheCamerlengo",
          "text": "Here is a story…my company recently posted an ad for a data engineer (mid to senior level). Within 2 days we were overwhelmed with candidates, easily over 300. We explicitly said no sponsorship in the ad. What did we get?\n\n98% Indian candidates that were in some sort of work visa limbo. Most of the students with student work visas that were expiring within the next couple years, some h1-b or married to h1-b candidates. \n\nThere were a few green card holders and 2 US born candidates that were extremely unqualified for the role. \n\nAlso every resume seemed to come from the same template and pretty much looked the same with different schools, companies, starting dates but mostly the same skills and general experience. It was very difficult to go thru the stack of candidates because very few stood out as different. \n\nI think this market is broken. Qualified candidates have trouble getting thru the noise and it is flooded with foreign workers. My guess is that most employees are staying out of the market for now and waiting for conditions to improve. Those looking for work are those that were downsized or student workers looking to land someplace to start an h1-b process. \n\nMy analysis may be incorrect but this is what we experienced posting on LinkedIn and indeed.",
          "score": 5,
          "created_utc": "2025-12-29 13:18:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiswsz",
          "author": "omonrise",
          "text": "do you also talk to recruiters using chatgpt like you do to us? could be the reason 😜",
          "score": 5,
          "created_utc": "2025-12-29 10:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigsl5",
          "author": "btown75",
          "text": "Hiring is a pain.  Thousands of submissions per job posting, resumes all look the same since they are written by ChatGPT, lying on application knockout questions, using AI to answer questions DURING the interview (sometimes obvious when they do this), and now I’m hearing from others they caught people using AI deepfakes on behalf of others.  Sponsoring has been too complicated for a while now, and am still getting plenty of applicants that don’t need it.  With modern tools maturing, lakehouse best practices solidifying, and AI automation, I just don’t need as big of a team as I used to either.  \n\nSorry, friend, I know that doesn’t help you.  Just rant from my side on the state of things.  Good luck to you.  I’ve been in similar situations when I was younger, and probably will again. Just don’t give up, keep trying and don’t be afraid to reinvent yourself if needed.",
          "score": 2,
          "created_utc": "2025-12-29 08:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwirqh9",
          "author": "testEphod",
          "text": "The vague feedback is also a way to avoid lawsuits, therefore it is done in an ambiguous way.",
          "score": 2,
          "created_utc": "2025-12-29 09:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj5d42",
          "author": "kali-jag",
          "text": "Alzo if you in us on h1b or other, that very well would be a reason.\n\nMy brother in law is working for half his usual salary(he comes with 18 years of expirience).\n\n Some of my friend who are good are struggling with some rejection reason very often late in tge interview process..",
          "score": 2,
          "created_utc": "2025-12-29 11:55:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjzb32",
          "author": "SELECTaerial",
          "text": "TIL 5yrs experience is considered mid-senior.",
          "score": 2,
          "created_utc": "2025-12-29 15:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu9x8u",
          "author": "TheSchlapper",
          "text": "If you are in a decent sized city and willing to do hybrid or on-site, then I haven’t heard of a better time to be in data.\n\nIf you are only applying to remote jobs then the market is pretty much in the air and I wish you the best of luck.\n\nEdit: not a good time to be a visa holder in really any capacity tbh unless you are top 1% in skills or something",
          "score": 2,
          "created_utc": "2025-12-31 01:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwicqbc",
          "author": "AlynaRoe",
          "text": "which country?",
          "score": 2,
          "created_utc": "2025-12-29 07:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiwu39",
          "author": "peterxsyd",
          "text": "Focus on escaping the noise and the pile:\n\n\\- reach out to recruiters with genuine but concise and respectful messages, where you are adding value by letting them know you are available have xyz skills and can start y\n\n\\- focus heaps of time on having a ready to go portfolio of exactly what you can do so they immediately see that, your future employer sees that and they go they can do that here now no risk no trouble I get that if I pay for you\n\n\\- make sure your Linked in is presented well, that your experience shows a clear line of growth and breadth of experience\n\n\\- tailor your resume for every job you apply for. Also, remember, even before you reach the hiring manager - the gates you need to get passed is ATS and the recruiter. Make sure you do\n\n\\- message the recruiter directly (and politely) after you apply so they know you are interested and so you are memorable. They don’t like or call that there are 200 people applying for a position. Literally, it’s just annoying for them. Go around the pile.\n\n\\- make sure your communication skills are polished, and that you don’t waffle, and are really positive about your experience, skills, and team work.\n\nThis will help I promise.",
          "score": 2,
          "created_utc": "2025-12-29 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj70a5",
          "author": "NoleMercy05",
          "text": "5 yrs XP is not 'mid-senior'. Lol.",
          "score": 2,
          "created_utc": "2025-12-29 12:08:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwijnda",
          "author": "Financial_Anything43",
          "text": "Which industries have you targeted?",
          "score": 1,
          "created_utc": "2025-12-29 08:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjw327",
          "author": "goldenfoxinthewild",
          "text": "Hijacking this post to ask - would the situation change if one already has a spousal visa in the US?",
          "score": 1,
          "created_utc": "2025-12-29 14:49:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk1agh",
          "author": "lmp515k",
          "text": "No point in hiring recently graduated masters right now, most of the resumes are fabrications and frankly they are just not very good and AI is nipping at their heels.  Also, NOBODY CARES about your certifications.",
          "score": 1,
          "created_utc": "2025-12-29 15:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl6dm2",
          "author": "lzwzli",
          "text": "If you're in a situation where you are heading into a visa expiration, you probably need to be peepared to head home. The uncertainty around visas is making every company pause all hiring that require a visa. \n\nDo you have a job now or you just graduated?",
          "score": 1,
          "created_utc": "2025-12-29 18:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm0myx",
          "author": "Lucade2210",
          "text": "5 year senior? Lol",
          "score": 1,
          "created_utc": "2025-12-29 20:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnthca",
          "author": "apono4life",
          "text": "I’m sorry you are struggling, because you are in a visa it may be difficult… also you may want to look at a lower title level. 5 years isn’t a ton of experience at this point.",
          "score": 1,
          "created_utc": "2025-12-30 02:38:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqjr9a",
          "author": "Great_Assistant_7854",
          "text": "A lot of people seem to be struggling too right now with the tech industry job market",
          "score": 1,
          "created_utc": "2025-12-30 14:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuqsam",
          "author": "SeaworthinessDue3355",
          "text": "The visa thing is a big deal in the US. I work for a company that has lots of mid level positions open but we aren’t going to sponsor.",
          "score": 1,
          "created_utc": "2025-12-31 03:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx06ddh",
          "author": "No_Resolution8717",
          "text": "What is your CTC if I may ask with 5YOE?",
          "score": 1,
          "created_utc": "2026-01-01 00:25:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk3cb8",
          "author": "Brilliant-Gur9384",
          "text": "My unpopular take is the demand's not coming back. I added clients this year with AI tooling I've built that eliminates the need for the entire data stack. This is growing (replacing entire tech teams). I get that this comment will be viewed as unpopular here, but I'm trying to help you zoom out here. \n\nWhat do you think it looks like in 5 years?\n\nEven for people employed, where do you think this industry is heading?\n\nIf you're smart, think ahead and take action now.",
          "score": 1,
          "created_utc": "2025-12-29 15:26:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjcei0",
          "author": "dasnoob",
          "text": "5 years is not much experience. That's your issue. Title inflation is real at a lot of companies.",
          "score": 0,
          "created_utc": "2025-12-29 12:49:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw9ydl",
      "title": "Kafka setup costs us a little fortune but everyone at my company is too scared to change it because it works",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pw9ydl/kafka_setup_costs_us_a_little_fortune_but/",
      "author": "Worldly-Volume-1440",
      "created_utc": "2025-12-26 17:35:50",
      "score": 104,
      "num_comments": 35,
      "upvote_ratio": 0.93,
      "text": "We're paying about 15k monthly for our kafka setup and it's handling maybe 500gb of data per day. I know that sounds crazy and it is but nobody wants to be the person who breaks something that's working.\n\nThe guy who set this up left 2 years ago and he basically over built everything expecting massive growth that never happened. We've got way more servers than we need and we're keeping data for 30 days when most of it gets used in the first few hours, basically everything is over provisioned.\n\nI've tried to bring up optimizing this like 5 times and everyone just says \"what if we need that capacity later\" or \"what if something breaks when we change it\". Meanwhile, we're losing money on servers that barely do anything most of the time. I finally convinced them to add gravitee to at least get visibility into what we're actually using and it confirmed what I suspected, we're wasting so much capacity. The funniest part of it is we started using kafka for pretty simple stuff like sending notifications between services and now it's this massive thing nobody wants to touch\n\nAnyone else dealing with this? Big kafka setup is such an overkill for what a lot of teams need but once you have it you're stuck with it",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pw9ydl/kafka_setup_costs_us_a_little_fortune_but/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw22l5u",
          "author": "chrisonhismac",
          "text": "$15k as a % of what overall infra costs and what revenue? Does the system work? Reliable and manageable? \n\nCost of time and salaries is probably way more than the $5-10k you save by fixing.",
          "score": 117,
          "created_utc": "2025-12-26 17:59:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw277tw",
              "author": "endless_sea_of_stars",
              "text": "Yeah, $15k a month to many organizations is a rounding error.\n\nOP needs to state his business better.  Instead of general pleas for \"optimization.\"  He needs to say: \"by doing X we can save Y over two years for the price of Z.\"",
              "score": 64,
              "created_utc": "2025-12-26 18:23:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3lmvj",
              "author": "J_robintheh00d",
              "text": "Right that like one senior programmers salary. Must be a small company",
              "score": 11,
              "created_utc": "2025-12-26 22:57:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2jhn0",
          "author": "Belmeez",
          "text": "This feels like yet another post that’s missing basic business context. As data professionals, we honestly have to do better.\n\n15k a month for a system that’s already working, and would take hundreds of man hours to rework, just to bring it down to what? 10k? 5k?\n\nIs that actually worth it? That’s the real question.\n\nI see this at my work too. Engineers want to crack open a system and redesign it because it’s “over provisioned” or “over engineered,” without factoring in the cost of our time or the opportunity cost. What else could we be building instead? Is there a higher value problem we could be solving?\n\nIs the business really so growth starved and initiative starved that refactoring a working system is the best use of effort right now?",
          "score": 55,
          "created_utc": "2025-12-26 19:27:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5w8bx",
              "author": "LEO-PomPui-Katoey",
              "text": "I'm a tech director in a software services company (we build systems for other customers and bill by the hour).\n\nI have junior/medior devs come in and without approval refactor large chunks of systems that were running fine because they didn't agree with the coding style or the architecture already in place. I've fired people over it as they wasted time we cannot bill and introducing new bugs in a system that was otherwise working just fine.\n\nThere always needs to be a commercial case for any refactoring. How much ROI is the business gonna get from this refactoring? What are the risks and mitigations to counter those risks? \n\nWith the refactoring we also need regression testing, updating documentation, reviewing requirements, run a new VAPT. In my opinion it's never really worth it if there's no hard significant amount of money we would save or efficiencies in the business we would get. What you can potentially do is introduce a new approach to this system for new functionalities you're building going forward and slowly piece by piece refactor the old system, it will be the least disruptive and you can do it under the radar.",
              "score": 10,
              "created_utc": "2025-12-27 08:49:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw65qk4",
                  "author": "Belmeez",
                  "text": "This is a good approach. Good for you for being resilient on it. There is no greater sin in my mind than refactoring code because you don’t agree with its architecture or style.\n\nIt’s a waste of time and resources, and my biggest gripe is that time is finite. It’s our most precious resource and refactoring or redesigning is at the bottom of that priority list. We could be building new capabilities, features, talking with customers or users, there is so much we could do that’s way more valuable",
                  "score": 5,
                  "created_utc": "2025-12-27 10:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3oydc",
              "author": "donalhunt",
              "text": "100% agree. Use it as a reason to check if capacity management is something that needs to be tracked (it may be over provisioned now but if no-one is tracking you'll wake up one day and find you're dropping data / putting data at risk). I'm surprised by how common it is to have no understanding of what capacity is provisioned / what headroom exists / what the process is for accepting updates to the planned capacity / usage. This should be driving budget conversation.",
              "score": 4,
              "created_utc": "2025-12-26 23:16:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2b81c",
          "author": "m1nkeh",
          "text": "I read this post and you’re failing to tell me the most important thing… why should the business care to prioritise this?\n\nAnd no, “it costs $15k” is not a sufficient answer..",
          "score": 51,
          "created_utc": "2025-12-26 18:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1yt7i",
          "author": "Firm_Bit",
          "text": "Answer the questions - what if x?\n\nWhat is the upside and downside within the larger picture. Is the extra cloud cost chump change compared to what you’d get by working on something else? \n\nIf not and you still can’t get buy in then they don’t trust you. You need to learn to be well liked and trusted and then you can pitch large fundamental changes.",
          "score": 12,
          "created_utc": "2025-12-26 17:39:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2gdln",
          "author": "zbir84",
          "text": "Are you being actively asked to reduce this cost? If not just forget about this, for your own sanity ;)",
          "score": 9,
          "created_utc": "2025-12-26 19:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2tkso",
          "author": "DenselyRanked",
          "text": "Agreeing with a few of the others comments about what \"little fortune\" means. If $15k is a significant chunk of your teams budget then take the time to spec out the new architecture and potential savings.",
          "score": 4,
          "created_utc": "2025-12-26 20:22:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw23f8l",
          "author": "eljefe6a",
          "text": "Sounds like no one knew what they were doing when it was done and no one knows what they're doing now. I'm guessing the 500 gb is due to using JSON instead of a binary format. That change alone would save 50 percent conservatively.",
          "score": 13,
          "created_utc": "2025-12-26 18:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2o890",
          "author": "kekekepepepe",
          "text": "You are paid to solve business problems. If costs, or at least the potential savings are not a top priority, then it’s not. You can think of it as a problem but it’s also a gift.\n\nThere indeed is value in legacy/just-working systems - they generate money. If you can make a granular plan to replace parts of the infrastructure with ease - you increase the likelihood of your management accepting it. And if they decline - say thanks and spend your time on stuff that matters more, to them, not to you.",
          "score": 3,
          "created_utc": "2025-12-26 19:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw32ius",
          "author": "Obvious-Phrase-657",
          "text": "Are you paying for this?  I mean, it sounds like nobody considers this as an issue, however if it actually fails after the fix (even if is not related) you will be in a very difficult situation",
          "score": 3,
          "created_utc": "2025-12-26 21:11:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3i91l",
          "author": "Outrageous_Tailor992",
          "text": "Putting aside TCO/ROI.. can someone stop bitchin' and actually provide a way on how to go about optimizing this?  \n\nJust curious TBH",
          "score": 2,
          "created_utc": "2025-12-26 22:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4hb6l",
          "author": "Thin_Original_6765",
          "text": "Fuck man, just leave it be. It's not like you're paying out of your own pocket.\n\nYour thing works so just focus on something else. What's so fun about perpetual migration work to save a few bucks?",
          "score": 2,
          "created_utc": "2025-12-27 02:11:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4onco",
          "author": "empireofadhd",
          "text": "Paying a consultant making a new solution olution for 6 months to a year can be 200k euro. Also having a cheaper system that breaks down constantly will force allocation of staff or consultants which costs 5-10k I. Work hours. Add to that cost of data loss, interruptions in business etc. \n\nHow big share of the company depends on this? Is it one manager who wants to have a sales chart once per month? Or is it operational data?",
          "score": 2,
          "created_utc": "2025-12-27 02:58:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw51plk",
          "author": "Icy_Addition_3974",
          "text": "$15k/month for service notifications is a $180k/year tax on organizational inertia.\n\n\nFor what you're describing, Liftbridge would handle it: same log semantics, 16MB Go binary, no JVM cluster to maintain.\ngithub.com/liftbridge-io/liftbridge\n\n\nThe \"what if we need it later\" argument: you've had 2 years of data showing you don't. That's not caution, that's waste.",
          "score": 2,
          "created_utc": "2025-12-27 04:27:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5dfj8",
              "author": "CryptoPilotApp",
              "text": "Thanks for actually bringing up a solution!! And yeah golang is perf for this",
              "score": 2,
              "created_utc": "2025-12-27 05:56:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5t4fq",
          "author": "Gold_Ad_2201",
          "text": "what exactly are you using Kafka for? as a message queue, as event hub, for realtime analytics? do you add/remove partitions or they are static? did you try if managed Kafka is cheaper?",
          "score": 2,
          "created_utc": "2025-12-27 08:19:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6c9b9",
          "author": "drewau99",
          "text": "Without moving away from Kafka there are probably a few things u could do to save money.  Just guessing though as you’ve provided very little detail…\nRetention could be more aggressive.\nUse compression.  Saves on storage and can improve throughput.  \nAWS have Express Brokers which can scale in and out on demand.",
          "score": 2,
          "created_utc": "2025-12-27 11:26:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3omt4",
          "author": "Nekobul",
          "text": "How much time do you estimate will take to redesign the process?",
          "score": 1,
          "created_utc": "2025-12-26 23:14:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw66v55",
          "author": "eccentric2488",
          "text": "Switch to Pub/Sub",
          "score": 1,
          "created_utc": "2025-12-27 10:34:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw72779",
          "author": "Dr_alchy",
          "text": "Let's talk, Kafka is one of my specialties!",
          "score": 1,
          "created_utc": "2025-12-27 14:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7ev6j",
          "author": "CyberWarfare-",
          "text": "We’ve been using AWS’s MSK for our Kafka clusters and it’s fantastic. Everything is incredibly easy to manage and AWS handles any updates or auto-scaling requirements.  They’ve even recently introduced auto-scaling for storage.",
          "score": 1,
          "created_utc": "2025-12-27 15:49:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwrfuj",
          "author": "Responsible_Act4032",
          "text": "Do the math. If it's costing you a fortune, there are as many object storage based kafka options that are simple, will do what you need, and save you 80% in cost. \n\nAt that point you can just pay someone to come in and get the new thing set up. \n\nSee KIP-1150, or any of the vendors equivalent versions.",
          "score": 1,
          "created_utc": "2025-12-31 13:20:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2kd5a",
          "author": "TheOverzealousEngie",
          "text": "Companies like this play like they're going to have money forever, like no one will ever be able to undercut them ever.  You have to be the responsible one. If you like the people, like the job and want to keep it ; do yourself a favor and do what a pro would do: parallelize. Find a really good alternative, test both and rinse and repeat until you have a winner. Run both A and B for a month and see how  if cheaper is in fact a reality. Now make the business case: you're the technical expert .. the what if's are yours. No business would refuse something cheaper if all things else were equal.",
          "score": 1,
          "created_utc": "2025-12-26 19:31:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5e40l",
              "author": "transpostmeta",
              "text": "How long until this change including all overhead breaks even and starts saving money? Could the involved people spend their time on other things that benefit the business more? Constantly re-working running systems can cost more that cloud bills.",
              "score": 3,
              "created_utc": "2025-12-27 06:01:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2l7fd",
          "author": "wbrd",
          "text": "I think there needs to be a licensing organization that people have to go through before they use Kafka. In most cases I've encountered they were using Kafka as MQ and not ever using the stuff that only Kafka can do.",
          "score": 1,
          "created_utc": "2025-12-26 19:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5ed0g",
              "author": "transpostmeta",
              "text": "What else would you use for medium- to large scale event-based architectures?",
              "score": 1,
              "created_utc": "2025-12-27 06:03:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7a3pl",
                  "author": "wbrd",
                  "text": "Active MQ works well. In the early 2000s I worked on a project where we did 30 million events per day on a single box. We clustered a few for redundancy, but the machine wasn't even taxed. Later we added a bunch of boxes so we could last for 24 hours without a consumer and keep everything in RAM for performance reasons. We didn't write to disk because our SLA from end to end was ~240ms.\nKafka likely couldn't do it on the same hardware just because of the extra stuff it does with each event, and definitely not in the time of spinning platters.",
                  "score": 1,
                  "created_utc": "2025-12-27 15:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw486ix",
          "author": "Few_Noise2632",
          "text": "reading comments here make me sick. people really don't give a fuck about anything. and the \"how much time you will spend\" question is so dumb when the context is resizing a cluster in 2025 when you have claude code lol",
          "score": -3,
          "created_utc": "2025-12-27 01:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6413i",
              "author": "Budget-Minimum6040",
              "text": "Undocumented prod system + LLM garbage? Yeah, what could go wrong ...",
              "score": 2,
              "created_utc": "2025-12-27 10:06:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwa32hp",
                  "author": "Few_Noise2632",
                  "text": "man he said it is a kafka cluster. resizing is not a rocket science if you have at least a bit of engineering skill",
                  "score": 1,
                  "created_utc": "2025-12-28 00:17:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1174f",
      "title": "The Data warehouse blues by Inmon, do you think he's right about Databricks & Snowflake?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q1174f/the_data_warehouse_blues_by_inmon_do_you_think/",
      "author": "Tall_Working_2146",
      "created_utc": "2026-01-01 10:26:23",
      "score": 104,
      "num_comments": 112,
      "upvote_ratio": 0.98,
      "text": "Bill Inmon posted on substack saying that Data-warehousing got lost in the modern data technology.\n\nIn a way that companies are now mistakenly confusing storage for centralization and ingestion for integration. Although I agree with the spirit of his text, he does take a swing at Databrick&Snowflake, as a student I didn't have the chance to experiment with these plateforms yet so I want to know what experts here think.\n\nLink to the post : [https://www.linkedin.com/pulse/data-warehouse-blues-bill-inmon-sokkc/](https://www.linkedin.com/pulse/data-warehouse-blues-bill-inmon-sokkc/)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q1174f/the_data_warehouse_blues_by_inmon_do_you_think/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx2hiqn",
          "author": "Peanut_-_Power",
          "text": "I think Bill is right about the symptoms; however, I think he misidentifies the cause.\n\nModern data architecture has shifted; not because data warehousing is dead, but because it is no longer the only thing organisations need. Today’s platforms have to support analytics, data science, and increasingly AI workloads alongside traditional BI.\n\nThe criticism of Databricks and Snowflake feels a little unfair; they are not trying to replace data warehousing fundamentals, they are trying to support multiple workloads. Both platforms can absolutely deliver a well-designed data warehouse if the right discipline is applied.\n\nIn my experience, the real issue is people rather than platforms; there is a strong tendency to chase modern tools and certifications while neglecting core concepts such as data modelling and integration. I regularly see engineers openly say they have no interest in modelling, which I would argue is foundational to being effective in this space.\n\nSo I agree with the spirit of the post; that we have lost sight of fundamentals. I do not think modern platforms are the culprit; they simply expose gaps in skills and architectural thinking that were always there.",
          "score": 138,
          "created_utc": "2026-01-01 12:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2vjgz",
              "author": "Nekobul",
              "text": "Up until recently both of these platforms didn't have any integration tools. They relied on third-parties like Fivetran and dbt to get the data somewhat into shape. But at a huge cost because the columnar databases are not exactly designed for integration work. And then they slapped the keyword \"modern\" to mask the issues and make everyone assume all previous solutions are outdated or not relevant. I believe that is the main critique by Mr.Inmon and I totally agree with him. People were sold a lie and both of these companies willingly participated in that lie.",
              "score": 20,
              "created_utc": "2026-01-01 13:58:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx32kar",
                  "author": "EarthGoddessDude",
                  "text": "Omg. You wrote an entire paragraph without mentioning SSIS once. A new year indeed! 🎉",
                  "score": 22,
                  "created_utc": "2026-01-01 14:45:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx44uax",
                  "author": "TheThoccnessMonster",
                  "text": "You wrote that entire paragraph without acknowledging that the people that build Databricks built Spark and that very much IS the new modern. \n\nWe migrated  latency sensitive workload off Hadoop/MapR that treated the warehouse cluster as basically an API, wholesale, over to S3 + SQL Warehouses. Something originally we thought would not be possible. I admit that much of the fundamentals of the old ways don’t have perfect facsimiles in the new world but doing Spark on the old cluster just for batch jobs vs.  now using it for everything (technically) is possible.",
                  "score": 7,
                  "created_utc": "2026-01-01 18:13:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5ry3s",
                  "author": "GreyHairedDWGuy",
                  "text": "I don't think they were sold a lie per se.  I agree with what you say about these vendors leaving integration to other tools/vendors but customers need to be smart enough to understand this and read between the lines and fill in the gaps. \n\nSo many people want a simple 'one size fits all' solution and want it so bad that they overlook needing to think for themselves.",
                  "score": 3,
                  "created_utc": "2026-01-01 23:17:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx46b4w",
                  "author": "BarfingOnMyFace",
                  "text": "Same",
                  "score": 0,
                  "created_utc": "2026-01-01 18:20:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx5rb6b",
              "author": "GreyHairedDWGuy",
              "text": "I also thought it was a bit misplaced to call out Databricks and Snowflake in such a way.  They are merely specialized infrastructure platforms that support the goals of enterprise analytics (and data warehousing...however someone wants to define it).  The main issue is 'people' as you say. This is partly (maybe mostly) caused by consulting companies selling a dream and quick wins when nothing about integrating organizational data is not generally a simple task.  I've seen it time an time again.   There is also a part of this that is due to education.  Many current practitioners also want quick wins, have not really done this type of work before and end up learning the wrong lessons.  I'm a decade younger than Bill and Ralph but have been doing this sort of work for 25+ years.  I see many consultants sell this this a a technology project when it is only partly this.\n\nI think a part of this started with 'schema on read', 'agile (get er done quickly and fail fast) and the era around early 2000's when companies just wanted dashboards (the tip of the iceberg) and didn't care about how that happened.\n\napologies...I'm ranting now :)",
              "score": 4,
              "created_utc": "2026-01-01 23:13:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2mud6",
              "author": "Tall_Working_2146",
              "text": "From what I read online, it feels that the core difference between Databricks and Snowflake is that Databricks is more of multiple Workloads Plateform and Snowflake is kind of closer to Datawarehousing (ACID transactions etc)",
              "score": 0,
              "created_utc": "2026-01-01 12:51:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2owmo",
                  "author": "NW1969",
                  "text": "Possibly true about Snowflake a few years ago, not the case now",
                  "score": 9,
                  "created_utc": "2026-01-01 13:08:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2oa4i",
          "author": "Kardinals",
          "text": "I think the core issue here is not Databricks or Snowflake. I mostly agree with the other commentator who said the cause is misidentified. In data analytics and engineering the real problems are and always have been people and processes. Technology only enables them. What actually got us here is weak data management and governance.\n\nThat said, I also partly agree with the criticism. Modern data stack tooling has reached a point where many organizations think that throwing Databricks or Snowflake at every data problem and calling it done is enough. This is especially true in organizations where IT (which should not be the owner of data in the first place) is expected to “fix” data issues. They look for technological solutions to what are, in reality, problems of processes, roles, ownership, and culture. In that sense, the frustration expressed by Inmon is very much understandable.\n\nSo the concept of the data warehouse did not fail. Technology simply made it easier to avoid the hard work. Vendors and consultants sold a convenient illusion. Integration and enterprise data are not a tooling problem, they are a design choice and an organizational commitment, and a governance problem. Expecting a platform to solve that is exactly the mistake that keeps repeating itself. Blaming modern platforms for the decline of data warehousing feels like blaming a database for poor data modeling. The real issue is that many organizations never built enterprise data capabilities in the first place.",
          "score": 37,
          "created_utc": "2026-01-01 13:03:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx46qzw",
              "author": "slowboater",
              "text": "You go from disagreeing (on what u would argue are semantics of sorts) in your first paragraph to totally agreeing in the last. Its a fine distinction, but i think from a big picture tells the same story. Its not just snowflake and databricks, all of the big 3 cloud providers are guilty too. The core problem/cause that links both of your nuanced agree/disagreee statements (and which you and bill acknowledge) is that these service providers willingly sell the lie that proper storage forms are not needed. Which is a mistruth at best and straight up deception at worst. Ive seen this happen several times, often because the people being sold/(lied) to are high level csuites with not experience in data modeling and as that sign off tumbles down the ladder til it gets in front of a skilled DE, no one above them believes (or has the necessary understanding/capacity to) that these older models are necessary anymore (and then gets frustrated when their apps arent fast enough and think the DE is judt picking a bone/lazy about their implementation blaming it on this foreign/alien concept of a proper warehouse structure, that they must so clearly have because its just a digital object where all their disorganized crap goes, and thats what the provider's solution architect said). Idk if some of these execs are really that naive that a 'solutions architect' is just a salesman, theyre slightly aware/embarassed enough of their lack of understanding of the concept to feel they cant admit it and play along in the room, or if they really just get caught up in the sway of wishful thinking (i.e. AI will solve all!!!). I tend to lean towards the last case here and as someone commented on the linkedin post (not on *substack* yet?) It feels a function/product of the quarterly/yearly reporting structure of corporate america.  I sure as hell know my last company did NOT like my diagnosis that 15 years of unstructured (worse, fragmented unstructure) raw data with no underlying, bought in, agreed upon process documentation would take 2 years minimum for me as a one man team magician to set up a modern architecture that could merge with SAP and make something clean enough to run near-live visualizations from. When i came in they had excel sheets everywhere with fragile AF VBA scripts linking across a whole plant to a shared drive... was literally told by one the the heads of IT to just use fabric to directly ingest these sheets like itd be that easy... On a level its almost a tradgedy of the commons in regards to our collective understanding of these concepts battling with our collective understanding to \"look\" busy via measured outputs at an \"acceptable\" pace from the glass office in the sky and therefore no EDW is getting the time or attention it needs for the past 5 years.",
              "score": 5,
              "created_utc": "2026-01-01 18:22:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx91yql",
                  "author": "Nekobul",
                  "text": "Thank you for the thoughtful comment! You have described the state of the data warehousing market very well.",
                  "score": 1,
                  "created_utc": "2026-01-02 13:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3g1ei",
              "author": "KWillets",
              "text": "But Snowflake will buy me lunch if I do that.",
              "score": 3,
              "created_utc": "2026-01-01 16:04:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4707c",
                  "author": "slowboater",
                  "text": "🤣 no so legitimately this tho... did discover there were kickback arrangements for the highest IT directors at my last place...",
                  "score": 3,
                  "created_utc": "2026-01-01 18:23:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx45feq",
              "author": "TheThoccnessMonster",
              "text": "Right fucking here, the actual answer.",
              "score": 3,
              "created_utc": "2026-01-01 18:15:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx5sf68",
              "author": "GreyHairedDWGuy",
              "text": "right on the money",
              "score": 2,
              "created_utc": "2026-01-01 23:19:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2w9ii",
              "author": "Nekobul",
              "text": "Integration is a tooling problem. Both platforms didn't provide basic capabilities, relying on third-party vendors and custom code to get the job done in mostly hacky, non-systematic and non-reusable way.",
              "score": -1,
              "created_utc": "2026-01-01 14:03:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2imrv",
          "author": "JonPX",
          "text": "The issue has always been the same, companies can't make modeling easy so they just sell it as not necessary ",
          "score": 24,
          "created_utc": "2026-01-01 12:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2kkkt",
              "author": "PrestigiousAnt3766",
              "text": "Its not easy. Its an undervalued skill.",
              "score": 21,
              "created_utc": "2026-01-01 12:31:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx421vt",
                  "author": "jamjam125",
                  "text": ">It’s not easy.It’ss an undervalued skill.\n>\nGenuinely curious why is data modeling an undervalued skill?",
                  "score": 1,
                  "created_utc": "2026-01-01 17:59:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2vpcr",
              "author": "Nekobul",
              "text": "Modeling is the actual work and it cannot be automated with finger snapping.",
              "score": 10,
              "created_utc": "2026-01-01 13:59:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3x0za",
                  "author": "One-Employment3759",
                  "text": "Modelling also can't be solved by subscribing to some prescriptive modelling technique.\n\n\nWhich is why I think Inmon and Kimball are fluff and it's ultimately about understanding the business needs and the data domain.",
                  "score": 5,
                  "created_utc": "2026-01-01 17:33:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3fysd",
              "author": "MyNameDebbie",
              "text": "Getting into this space some as an experienced dev. Traditionally when I thought of data warehousing data modeling(star schemas) was necessary as massive compute wasn’t ubiquitous. Now you can throw tons of compute at it (more $$ for databricks/snowflake). \n\nMy question is with modern object store what data modeling is useful? Any resources available for this?",
              "score": -1,
              "created_utc": "2026-01-01 16:03:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3hlts",
                  "author": "JonPX",
                  "text": "Imagine if you went to IKEA, you wanted to buy something, and they gave you 50 locations of where to find every individual item you need to build the thing you want? If the warehouse picker is fast enough, that is just as good as everything in a prepackaged packet with all the items you need?\n\nData modeling is useful because it brings together all linked data in a format that is perfect for its intended usage. If that is dimensional or relational or DataVault or a mix of them, that doesn't matter.\n\nOr as a development equivalent, you don't write everything in a single routine that does everything, right? You make different classes, sub-routines etc.",
                  "score": 5,
                  "created_utc": "2026-01-01 16:12:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx31xe1",
          "author": "Firm-Yogurtcloset528",
          "text": "In my experience I see big companies having dedicated teams doing data governance and even data modeling that are totally separated from the business teams and not knowledgeable about data warehousing, who are suppose to take control of the whole modern lake house concept with enormous amount of money spend and the ones owning budgets clueless what is money well spend or not. They get handed awards from Data ricks and Snowflake for being amazingly innovative  thus buying into middle management who passes on the BS to upper management like it is the best thing since sliced bread.",
          "score": 5,
          "created_utc": "2026-01-01 14:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2g53w",
          "author": "West_Good_5961",
          "text": "![gif](giphy|fqtyYcXoDV0X6ss8Mf)",
          "score": 20,
          "created_utc": "2026-01-01 11:49:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2kivs",
              "author": "ProfessorNoPuede",
              "text": "That's the right meme on several levels. Good start of the Year.",
              "score": 4,
              "created_utc": "2026-01-01 12:31:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2mvzy",
              "author": "Tall_Working_2146",
              "text": "![gif](giphy|IPG1x2rrFiWB527EVg)",
              "score": 1,
              "created_utc": "2026-01-01 12:52:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2k3r0",
          "author": "PrestigiousAnt3766",
          "text": "I think its mostly about the trade-off of having the luxury to model vs getting the data out there quicky.\n\n\nToday everyone wants data to do whatever ai, bi, experiments. Requirements change rapidly. You see a push to model as late as you can get away with. \n\n\nStrong emphasis on modeling within an org slows everything down. Not many people can model, and shared data models are difficult to design and take time.\n\n\nSo, I think having multiple / decentralized models are the way for now.",
          "score": 11,
          "created_utc": "2026-01-01 12:27:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3d8zi",
              "author": "Separate_Newt7313",
              "text": "@PrestigiousAnt3766 your comment is giving me heartburn.\n\nA data model explains how a business' data fits together and what it means, so people can use it consistently and correctly.\n\nData modeling is largely detective work. It's hitting the streets, talking to the people who really know how the business works, and why the data look the way they do. \n\nSample conversation: \n\"Is this sales line item for a single product or for the entire transaction? Oh it's a roll-up for all transactions in the entire month? Whoops! Glad I asked!!! Where can I get the data for each line item?\"\n\nHow the _hell_ are you going to be piping raw data into a dashboard, LLM, or an ML model, and expect anything other than garbage to come out? Do you put crude oil directly into your car, too‽\n\nAt the end of the day, the main reason why data science is hard is because data modeling is hard, not because using PyTorch is hard.",
              "score": 14,
              "created_utc": "2026-01-01 15:48:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3xp95",
                  "author": "dbrownems",
                  "text": "But the business context required for modeling is one of the main reasons you need multiple/decentralized models.",
                  "score": 1,
                  "created_utc": "2026-01-01 17:37:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3ljpo",
                  "author": "PrestigiousAnt3766",
                  "text": "Have you ever worked for ML or datascientists? They want to access raw and unmodeled data. Thats why in medallion structure you have a bronze layer.\n\nFor BI there is value in modeling but in all companies I have been to as a consultant the stories I hear are an overwhelmed central modeling team and business tired of waiting for their changes.",
                  "score": 0,
                  "created_utc": "2026-01-01 16:33:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2x50h",
              "author": "Tall_Working_2146",
              "text": "but is modeling a luxury really? I thought that was the backbone of every useful analytical system, OLAP, semantic models on powerBI, isn't it the entire point to have well designed- single source of truth the way to go ?",
              "score": 6,
              "created_utc": "2026-01-01 14:09:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2z47j",
                  "author": "Nekobul",
                  "text": "Exactly! That is what Mr.Inmon has always said:\n\n\\* Single Source of Truth!  \n\\* Single Source of Truth!  \n\\* Single Source of Truth!",
                  "score": 1,
                  "created_utc": "2026-01-01 14:22:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx302rp",
                  "author": "PrestigiousAnt3766",
                  "text": "Datascience, apis, integrations, flat tables dont necessarily need a dimensional model. If you do realtime, modelling is prohibitively slow.\n\nIf you have regulatory requirements than yes, you need to model.",
                  "score": -1,
                  "created_utc": "2026-01-01 14:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2vup3",
              "author": "Nekobul",
              "text": "When you get different numbers from different models what do you do?",
              "score": 3,
              "created_utc": "2026-01-01 14:00:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx30x3e",
                  "author": "PrestigiousAnt3766",
                  "text": "Accept it.",
                  "score": -2,
                  "created_utc": "2026-01-01 14:35:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2vuut",
          "author": "Yonko74",
          "text": "I think he’s bang on the money. Not necessarily with singling out Databricks and Snowflake, but the general principle is spot on.\n\nWe’re not talking about the relatively few large-scale organisations here, but the bulk of small to mid tier companies who suddenly discovered they desperately needed ‘data’ solutions, and were sold the dream by snake oil salesmen.\n\nFor too long these organisations were happy to  see ‘data’ as an isolated function that they could chuck cheap engineering labour on top of a plethora of ever-changing tech stacks (that all do the same fundamental thing)\n\nNow though the chickens are coming home to roost and the AI boom is flagging how such actions create inconsistency, miss governance, wrecks quality and builds layer upon layer of technical debt.\n\nThe sooner we get back to viewing data as an asset rather than a product, the better imo.",
          "score": 3,
          "created_utc": "2026-01-01 14:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2pq4v",
          "author": "totalsports1",
          "text": "It is a fact that most companies don't follow any sort of warehousing principles. But the fact is whatever that is being done across orgs gets stuff done. Reporting/BI is a cost centre in most companies, ultimately they're measured by how well they serve business. But this haphazard approach is a problem when the going gets tough. Suddenly everyone is worried about cost and eyes fall on the BI team with so many data analysts. No org is going to prioritize cost/efficiency over time to market while building the team from grounds up.",
          "score": 2,
          "created_utc": "2026-01-01 13:15:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2x4l7",
              "author": "Nekobul",
              "text": "They are not following principles because most organizations were sold the myth these platforms will do the job for them at very low cost. It was a rush to the bottom and now comes the time to pay the price.",
              "score": 1,
              "created_utc": "2026-01-01 14:09:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2wnba",
          "author": "GachaJay",
          "text": "To be fair, Databricks is actively pitching it wants to be utilized in operational workloads as well. Same with Fabric. They want to remove the need for separate SQL warehouses to capture the whole market.",
          "score": 2,
          "created_utc": "2026-01-01 14:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2zk4c",
              "author": "Nekobul",
              "text": "I'm waiting for Databricks to start building pig farms. They can use the generated waste to increase the amount of generated BS because it is not enough right now.",
              "score": 5,
              "created_utc": "2026-01-01 14:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2zogk",
                  "author": "GachaJay",
                  "text": "If it’ll make that IPO the right number, they’ll do just about anything",
                  "score": 3,
                  "created_utc": "2026-01-01 14:26:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4kx28",
          "author": "blobbleblab",
          "text": "What's he smoking, both platforms can and do (I have built them) deliver data warehouses if that's what's needed. It's all about what the customer needs at the other end. If they don't need a data warehouse though, you don't make one as it's a lot of design effort.",
          "score": 2,
          "created_utc": "2026-01-01 19:32:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5o0g6",
          "author": "Livelife123123",
          "text": "They are just tools. The last thing you want is a tool that does everything half assed.\n\nA real life warehouse isn't useful if anyone can dump anything anywhere inside. It stops becoming a warehouse and looks more like a dump.",
          "score": 2,
          "created_utc": "2026-01-01 22:55:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6i386",
          "author": "peterxsyd",
          "text": "It is 100% true. I have seen companies ingest data and have an integration per end user request or report, as opposed to a data warehouse actually modelling the business from the multiple upstream source systems. That is the whole point of data engineering.",
          "score": 2,
          "created_utc": "2026-01-02 01:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx939yf",
              "author": "Nekobul",
              "text": "Absolutely correct.",
              "score": 1,
              "created_utc": "2026-01-02 13:46:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2ld3t",
          "author": "ProfessorNoPuede",
          "text": "The criticism of databricks and snowflake is a miss. It's not about the tool, so why attack those? They're quite good at what they do, especially compared to what came before.\n\nSecondly, no reflection here? None? Perhaps there is a reason the enterprise data warehouse always failed? A better reason than \"they don't understand it\". Organisations are able to grasp very complex concepts and execute on them if the urgency and value are there.\n\nData Integration is apparently hard. Well shucks. Why is it hard and why is it not perceived as valuable enough to solve relative to the complexity?",
          "score": 4,
          "created_utc": "2026-01-01 12:39:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2ww9m",
              "author": "Nekobul",
              "text": "Of course it is about the tool. These platforms are built on columnar databases and those technologies are not suitable for integration. However, the vendors have pushed hacky, dumb solutions to somehow make the integration work. Yet when the underlying technology is not suitable it also makes the processing highly inefficient and expensive.",
              "score": 0,
              "created_utc": "2026-01-01 14:07:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2x91j",
                  "author": "ProfessorNoPuede",
                  "text": "I'm not sure why columnar databases don't allow you to join or process in compute? I see no reason why columnar (be it in parquet, SQL server or snowflake) wouldn't be suited for data integration? It's not the best for low latency random access joins, but that's about it.",
                  "score": 3,
                  "created_utc": "2026-01-01 14:10:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2vu4m",
          "author": "VarietyOk7120",
          "text": "Databricks said they were gonna replace the Data Warehouse with the Lake House. They said that SQL was outdated. \n\nWhen that didn't happen, they then pivoted to releasing Databricks SQL. So he's right there. \n\nI don't think you can knock Snowflake as much however. He seems to be saying they haven't focused as much on integration. You can use a range of third party integration tools with platforms like Snowflake, Fabric and Databricks SQL. \n\nThe main thing is, whatever platform you choose, make sure you understand DW modeling and design fundamentals",
          "score": 2,
          "created_utc": "2026-01-01 14:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx304ka",
              "author": "Nekobul",
              "text": "They have not focused on the integration work at all and it is plainly evident. However, they have willingly sold the lie you can do integration with their platforms, conveniently avoiding the fact their systems are not suitable for that. These platforms have caused huge damage that people are yet to experience.",
              "score": -1,
              "created_utc": "2026-01-01 14:29:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5lxdu",
                  "author": "VarietyOk7120",
                  "text": "Well, you're right, I have seen Databricks implementations (Lake house) that have done huge damage , where the customer is left with something barely usable, having spent a lot on consulting fees etc. The one customer is now building an old style Data Warehouse on a regular database platform just to get reporting back to where it was",
                  "score": 2,
                  "created_utc": "2026-01-01 22:43:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx344dj",
          "author": "I_Am_Robotic",
          "text": "Ok I’m a newbie - I’m not completely following the argument. Can someone help me? \n\nIs he saying people are just dumping data into the data lake and not actually making sense of it so it’s useful to the business? Isn’t the whole concept of medallion architecture in dbx exactly about it being useful by the time it gets to gold? And aren’t both dbx and snowflake largely intended for non-transactional purposes?\n\nIf so, how’s any of this the fault of snowflake or dbx?",
          "score": 1,
          "created_utc": "2026-01-01 14:55:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3wh7l",
              "author": "Nekobul",
              "text": "That's precisely what these platforms have encouraged and what tools like Fivetran have assisted. Dump the data and then people downstream will grab whatever they want and model it and use it. The problem is the people downstream rarely have understanding what is the data context and semantic. The outcomes can be totally different from one analyst to another.\n\nPeople say these platforms are not responsible for the situation. However, the vendors pushing these platforms are the ones who have encouraged such approach for years because it is highly profitable for them. Instead of doing the computation and modeling once, you now have proliferation of models who are trying to do similar stuff. Some people have said in the comments you should accept that and move on because that is the price you have to pay for velocity and agility. I'm not sure what kind of velocity they are talking about if you don't know whether the data you are dealing is garbage. Garbage is garbage.",
              "score": 2,
              "created_utc": "2026-01-01 17:31:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4cwcz",
          "author": "gigatexalBerlin",
          "text": "The company I work for has been around for almost 15 years, been employing people, survived the pandemic, been making moves etc etc and there's not a single fact or dimension table anywhere.\n\nBest I can describe is a bunch of degenerate dim fact tables with no ERD to think of... it's all just in the senior analysts heads.\n\nI keep championing more structure and since we're a Snowflake shop I think we'll get it because some of the tools we are looking to use rely on a semantic layer which will require analytics to bake into a standard-ish format the relationships between tables and columns and id the primary keys and foreign keys etc....\n\nI hope we can then at least get to data marts that are their own star/snowflake schema'd pieces of art that are sane and easily understandable... I can dream though.",
          "score": 1,
          "created_utc": "2026-01-01 18:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5u8k2",
          "author": "BlueMercedes1970",
          "text": "I don’t know what he is talking about. ETL has always been the hardest part of data warehousing and these platforms provide those tools so where is the issue ?",
          "score": 1,
          "created_utc": "2026-01-01 23:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx933ki",
              "author": "Nekobul",
              "text": "These platforms didn't provide any ETL tools for many years. These vendors promoted a workaround called \"ELT\" as alternative to a proper ETL platform at a huge extra cost for the customers. They have only recently started to introduce ETL tooling after they reached the point where the ELT hack is no longer sustainable.",
              "score": 1,
              "created_utc": "2026-01-02 13:45:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx78ilg",
          "author": "kthejoker",
          "text": "Old man yells at cloud (data platforms)\n\nIn what world does Snowflake or Databricks *not* want to be the core integration point for all systems upstream and downstream?\n\nBeing the sticky compute in the middle of all of your source systems and your valuable use cases is literally where all the money is.\n\nThis is literally just ranting about something he wishes were true but is not - that it's the technology's fault.\n\nWhen the actual issue always has been people who buy the technology hoping it lets them avoid the hard work of translating their business processes into data models, insights, and solutions ...\n\nDiscovering it does not ..\n\nand then blaming the technology for not eliminating the hard work (oh well! On to the next technology....)\n\nDisclaimer: I work at Databricks",
          "score": 1,
          "created_utc": "2026-01-02 04:34:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx94kgs",
              "author": "Nekobul",
              "text": "\\* Your company doesn't have any unique technology that is not available to other major players in the market. Your crown-jewel Photon native execution Spark engine is now replicated by both Microsoft and Snowflake. Soon, there will be completely free OSS replacement on the market, too.  \n\\* Your company promoted the medallion methodology which is ridiculous.  \n\\* Your company didn't provide proper ETL tooling for many years, pushing the ELT as the quick but terrible workaround.  \n\\* Your company doesn't provide Databricks for installation and running on-premises. Therefore, all the testing and development and optimization work has to be paid by the minute in the cloud.  \n\\* Your distributed platform is not needed by the vast majority of the market. Most data work can be done on a single machine using SQL Server or DuckDB.",
              "score": 0,
              "created_utc": "2026-01-02 13:54:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8kfqf",
          "author": "codek1",
          "text": "This does seem to have really kicked off! Joe Reiss jumped onto it too.",
          "score": 1,
          "created_utc": "2026-01-02 11:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8znm0",
          "author": "rodrig_abt",
          "text": "I've not yet met a single company without some garbage data and/or pipeline. Whatever the reason (people, processes, tool mess, politics, or all of them), the reality is that you do not raise at the level of your business priorities but fall to the level of your systems...and if your systems are \"bad,\" bad things happen. Data has always been complicated (even with the term \"data\" itself), but no matter how complicated, you can always trace a predictable path to achieve something. Data warehousing is a great example: it started as a way to solve reporting and analytics problems. Regardless of industry, size, or type of data, the data-warehousing modeling approach can provide a predictable path to achieve reporting and analytics goals, no matter how complicated. You didn't need many tools: just extraction, transformation, and loading. And yes, the devil is in the details, you're right, but that's precisely where things get complex when you start adding too many moving parts (tools) that create complexity. Complexity is always bad. Always. Remove complexity first, then work out complications.",
          "score": 1,
          "created_utc": "2026-01-02 13:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2nl4g",
          "author": "sleeper_must_awaken",
          "text": "Unhinged article without root cause analysis. Many leaders figured out decades ago that enterprises need information (or data) management and governance. Without these, all you have are tools, but no hands, no guidelines, no direction, no accountability, no improvement processes…\n\nYou’re misguided if you believe you can use cheap consultancies to enable and strategically leverage a core asset of your organization: data.",
          "score": -3,
          "created_utc": "2026-01-01 12:58:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2wj50",
              "author": "Nekobul",
              "text": "These platforms didn't provide any integration tools. Everyone was left on their own to come up with a way to shape the data. That is major issue that these vendors only recently acknowledged/understood exists.",
              "score": 0,
              "created_utc": "2026-01-01 14:05:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx6jzql",
                  "author": "BlueMercedes1970",
                  "text": "What are you talking about? They have Spark, SQL and Python.",
                  "score": 1,
                  "created_utc": "2026-01-02 01:59:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2uegp",
          "author": "Nekobul",
          "text": "Using columnar, non-volatile databases for data integration is expensive, highly inefficient, with high latency and frankly dumb. However, both of these major players have sold that hack/concept to the masses with great success. And then they called it \"modern\". What a joke.\n\nWake up, people!",
          "score": -1,
          "created_utc": "2026-01-01 13:50:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5tptx",
              "author": "BlueMercedes1970",
              "text": "You aren’t making sense. What integration are you talking about and for what purpose? If you are building a data warehouse or data lake then both of these platforms are perfectly valid.",
              "score": 2,
              "created_utc": "2026-01-01 23:27:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5uo82",
                  "author": "Nekobul",
                  "text": "How do you \"massage\" the data to make it usable?",
                  "score": 1,
                  "created_utc": "2026-01-01 23:32:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pufkm8",
      "title": "Data Quality on Spark — A Practical Series (Great Expectations, Soda, DQX, Deequ, Pandera)",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pufkm8/data_quality_on_spark_a_practical_series_great/",
      "author": "ivan_kurchenko",
      "created_utc": "2025-12-24 05:31:36",
      "score": 60,
      "num_comments": 16,
      "upvote_ratio": 0.97,
      "text": "I'm planning to work on Data Quality improvement project at work so decided to start with current tools evaluation. So decided to write a blog series along the way.\n\n1. [**Part 1 — Great Expectations**](https://levelup.gitconnected.com/data-quality-on-spark-part-1-greatexpectations-fd4ffa126ca0)\n2. [**Part 2 — Soda**](https://levelup.gitconnected.com/data-quality-on-spark-part-2-soda-97d5d32e2d8b)\n3. [**Part 3 — DQX**](https://levelup.gitconnected.com/data-quality-on-spark-part-3-dqx-f0335b8ff07d)\n4. [**Part 4 — Deequ**](https://levelup.gitconnected.com/data-quality-on-spark-part-4-deequ-d82e8c2344ae)\n5. [**Part 5 — Pandera**](https://levelup.gitconnected.com/data-quality-on-spark-part-5-pandera-05cd494d5b01)",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pufkm8/data_quality_on_spark_a_practical_series_great/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvqnn0z",
          "author": "hopefullythathelps",
          "text": "Could we have a part 6 just use SQL and maybe yaml files and a lookup table and no framework needed",
          "score": 11,
          "created_utc": "2025-12-24 16:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqqr90",
              "author": "ivan_kurchenko",
              "text": "I'm planning also do another post for Dabaricks specifically, if that would be interesting - it does SQL based alerting.\n\nAdditionally, what you are describing is doing Soda already and doing its pretty good already.",
              "score": 4,
              "created_utc": "2025-12-24 16:57:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvv35rt",
              "author": "Zeddyorg",
              "text": "That would require a part 7 - teach your business stakeholders SQL",
              "score": 1,
              "created_utc": "2025-12-25 12:43:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrcc16",
          "author": "arconic23",
          "text": "Dbt has also quite some nice data tests / unit test capabilities",
          "score": 4,
          "created_utc": "2025-12-24 18:54:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzajog",
              "author": "ivan_kurchenko",
              "text": "Thanks for advice, I'll have a look.",
              "score": 1,
              "created_utc": "2025-12-26 05:12:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuvynf",
          "author": "Fizzrocket",
          "text": "At my work I have been implementing the first three. \n\nSoda Core was pretty unintuitive and limiting in terms usability.\n\nGX had a lot of features paywalled which used to come for free.\n\nI am currently implementing DQX and so far it seems promising. It being Databricks native helps when the rest of your stack is also located there.",
          "score": 2,
          "created_utc": "2025-12-25 11:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzaplu",
              "author": "ivan_kurchenko",
              "text": "Thanks, How it goes with DQX so far? Do you feel this is everything you need or something is missing?",
              "score": 1,
              "created_utc": "2025-12-26 05:14:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzp00p",
                  "author": "Fizzrocket",
                  "text": "It has been largely positive. The primary challenges encountered thus far relate to the occasional instability of the DQEngine and the typical delays associated with our stakeholder.\n\nThe ability to develop custom checks using the same syntax as the pre-built options is a notable advantage. However, due to our testers' current proficiency levels in PySpark, I have initially implemented a framework that accommodates custom SQL inputs. It sucks that we can't really utilise DQX to it's full potential but we have to start somewhere. \n\nGiven our organization's full adoption of Databricks, the out-of-the-box integration has been very handy!",
                  "score": 1,
                  "created_utc": "2025-12-26 07:20:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzv882",
              "author": "-crucible-",
              "text": "I haven’t looked for a while - what is GE now paywalling? I was hoping to go back that way.",
              "score": 1,
              "created_utc": "2025-12-26 08:24:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqjjk9",
          "author": "siddartha08",
          "text": "First impressions: \"Oh NO not ISO standards!\" \nI'll give it a more in depth read later.",
          "score": 1,
          "created_utc": "2025-12-24 16:19:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrgdpp",
          "author": "nonamenomonet",
          "text": "Thanks! I will read this. Also can I dm you?",
          "score": 1,
          "created_utc": "2025-12-24 19:16:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrlhma",
              "author": "ivan_kurchenko",
              "text": "Sure",
              "score": 1,
              "created_utc": "2025-12-24 19:46:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwk0dg",
          "author": "mamaBiskothu",
          "text": "Anyone who says GE is a practical way to get meaningful DQ is clueless.",
          "score": 1,
          "created_utc": "2025-12-25 18:30:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxbx7y",
          "author": "Particular_Scar2211",
          "text": "From my experience GE is pretty hard to set up.\nToo much configuration from the get go.\n\nThis is a perfect time for this post since I want to implement quality checks for my in-transit (dataframes) data inside databricks jobs. 🙏\n\nSeveral questions:\n1. Is dqx is the only framework that lets you separate invalid from valid data?\n2. What's the speed comparison between all frameworks?\n3. What about alerts (i know GE has slack and email integration)?\n\nThanks 🙏",
          "score": 1,
          "created_utc": "2025-12-25 21:18:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzbzpe",
              "author": "ivan_kurchenko",
              "text": "Thanks.\n\n1. For Spark yes. Pandera supports it only for pandas/polars: [https://pandera.readthedocs.io/en/stable/drop\\_invalid\\_rows.html#drop-invalid-rows](https://pandera.readthedocs.io/en/stable/drop_invalid_rows.html#drop-invalid-rows)\n\n2. That's a very good question, thanks. I did not test performance aspect in details, because in many cases I was running it locally on relatively small dataset. \n\n3. Soda Cloud supports I believe, other three (DQX, Deequee, Pandera) are focused primarily on Data Quality itself.",
              "score": 2,
              "created_utc": "2025-12-26 05:24:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwc9b0t",
                  "author": "Particular_Scar2211",
                  "text": "Thanks for elaborating!",
                  "score": 1,
                  "created_utc": "2025-12-28 09:44:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwbwdt",
      "title": "What data engineering decision did you regret six months later, and why?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pwbwdt/what_data_engineering_decision_did_you_regret_six/",
      "author": "AMDataLake",
      "created_utc": "2025-12-26 18:56:07",
      "score": 57,
      "num_comments": 54,
      "upvote_ratio": 0.92,
      "text": "What was your experience?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pwbwdt/what_data_engineering_decision_did_you_regret_six/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw3qurc",
          "author": "Hackerjurassicpark",
          "text": "Democratising access to anyone across the org to create DBT models. What we thought was a fantastic way to alleviate burden on the DE team, turned out to be a mess of thousands of DBT spaghetti models, many doing very similar but slightly different things. Costs skyrocketed. We’re clawing back ownership and shutting off people progressively now.\n\nIn hindsight, we should’ve expected this. We were too stupid and naive",
          "score": 95,
          "created_utc": "2025-12-26 23:28:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw41s06",
              "author": "umognog",
              "text": "Had a DS ask if they can do their own DBT.\n\nI actually considered it for a bit..then realised hell no when i spotted a transaction taking 70 hours...and they thought that was ok.",
              "score": 32,
              "created_utc": "2025-12-27 00:33:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhmr59",
              "author": "burningburnerbern",
              "text": "6 months later you’ll be posting:\n\n“Creating bottlenecks by limiting access to create DBT models. Requests have become overwhelming and burdensome. We’re giving back ownership and granting access progressively now”\n\nCan’t win 😂",
              "score": 2,
              "created_utc": "2025-12-29 04:15:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwikz9g",
                  "author": "Hackerjurassicpark",
                  "text": "IMO this is a better problem to have. Atleast my guys won’t get fired and maybe even justify to hire more",
                  "score": 3,
                  "created_utc": "2025-12-29 08:50:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwnq4my",
                  "author": "Nixxy_445",
                  "text": "Federation is the answer, however it needs strong governance which unfortunately is hard. Thing is well managed data takes time!! Building something for reuse is harder than just whipping up an adhoc query.",
                  "score": 1,
                  "created_utc": "2025-12-30 02:20:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw6hmj7",
              "author": "m1nkeh",
              "text": "DBT is a fucking blight",
              "score": 5,
              "created_utc": "2025-12-27 12:15:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw509ye",
              "author": "PickRare6751",
              "text": "You don’t even do code reviews?",
              "score": 1,
              "created_utc": "2025-12-27 04:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw50wd2",
                  "author": "Hackerjurassicpark",
                  "text": "Have you heard of the data mesh… yeah that… each team’s lead was incharge of their own team’s code… yeah we were really stupid in hindsight.",
                  "score": 11,
                  "created_utc": "2025-12-27 04:21:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwb84ce",
                  "author": "thisfunnieguy",
                  "text": "Code reviews won’t save you. It can look OK but unless you see some profiling or run time you might not realize how bad things are.",
                  "score": 2,
                  "created_utc": "2025-12-28 04:23:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw6qhui",
              "author": "TheThoccnessMonster",
              "text": "There’s a reason that DE, MLOPS and Data Science are different paths and despite the delusions of the individual engineer,  one of them should NEVER be doing the others jobs.\n\nYou can add PO/PM and Scrum lord to that list of exclusives as well.",
              "score": 0,
              "created_utc": "2025-12-27 13:23:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw69g1g",
              "author": "M4A1SD__",
              "text": "I know you already admitted it was a dumb idea… not trying to rub it in, but yeah that’s a really idiotic idea I can’t believe you want though with that",
              "score": -6,
              "created_utc": "2025-12-27 10:59:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6cjin",
                  "author": "Mr_Again",
                  "text": "Self serve is a pretty common idea. In big orgs with a small data eng team and high quality in the surrounding teams like DS etc, it makes a lot of sense. DE gets to code review the models but doesn't spend all day writing them. Skilled engineers in the surrounding teams and DE code review make sure that the quality of the models stays high. We also used to embed an \"analytics engineer\" in each team to assist with this.",
                  "score": 5,
                  "created_utc": "2025-12-27 11:29:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4gyc4",
          "author": "figshot",
          "text": "Database names. Schema names. Table names. View names.\n\nNaming is hard.",
          "score": 65,
          "created_utc": "2025-12-27 02:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5m43x",
              "author": "SuggestAnyName",
              "text": "In my current org, we had a SQL server DB named MongoDB.",
              "score": 34,
              "created_utc": "2025-12-27 07:12:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6mw0k",
                  "author": "aphillippe",
                  "text": "MongoDB_final_v2_test_donotuse_prod",
                  "score": 20,
                  "created_utc": "2025-12-27 12:57:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6jqtz",
                  "author": "aLokilike",
                  "text": "Awesome. This is the kind of nameslop I hope to someday achieve.",
                  "score": 6,
                  "created_utc": "2025-12-27 12:33:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2sjfo",
          "author": "PickRare6751",
          "text": "Believe in serverless computing, until at some point everything throws random errors, and not able to troubleshoot",
          "score": 69,
          "created_utc": "2025-12-26 20:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw60k91",
              "author": "Rude-Needleworker-56",
              "text": "Can you provide a bit more details? What was the platform stack used, could you later identify the cause and what you did?",
              "score": 3,
              "created_utc": "2025-12-27 09:32:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6tvzi",
              "author": "s4swordfish",
              "text": "i’m curious on how this happens as well",
              "score": 1,
              "created_utc": "2025-12-27 13:46:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwkzyz5",
              "author": "bemna94",
              "text": "Very intrigued, tell us more",
              "score": 1,
              "created_utc": "2025-12-29 18:01:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw4o6n3",
          "author": "codykonior",
          "text": "Probably choosing sqlmesh. It was about six months of development and building patches for it. The week it went into production FiveTran bought both it and dbt, and now development has slowed from daily releases to minor patches once a month, which is worrying. \n\nIt still works. It's still a cool concept. I still hit my deadline. It's open source so I can still keep using it.\n\nBut because it's arguably a dead end platform (until we see real evidence otherwise, which will take a year or two of observation), nobody will want to learn it, and so it will be difficult to ever hand off to a successor. \n\nI imagine whoever it is would probably want to rewrite it in something else with a more definitive future. Maybe dbt (I don't have any experience in it so don't know how much work that is). Maybe Fabric, but Fabric was not baked when I started earlier this year, and still seems to have major issues and is tens or hundreds of thousands of dollars expensive versus zero dollars in a random VM. \n\nSo I don't totally regret it. I made the best decision I could at the time and filled a business need. Still, if I knew, I'd probably have gone in another direction. I just don't know how.",
          "score": 15,
          "created_utc": "2025-12-27 02:55:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6hqjt",
              "author": "m1nkeh",
              "text": "Yeah, SQLmesh is dead my friend..\n\nWould you consider spark declarative pipelines?",
              "score": 3,
              "created_utc": "2025-12-27 12:16:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6z7bn",
                  "author": "codykonior",
                  "text": "I couldn't say, but, I don't foresee another 6 months being provided to learn and retool in something else.",
                  "score": 1,
                  "created_utc": "2025-12-27 14:20:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw59mjh",
          "author": "MemesMakeHistory",
          "text": "Building custom tooling when there were open-source or vendor managed options available.\n\nThe custom tooling did help with reducing the learning curve across the org and got us delivering faster, but it meant we had to continuously support it going forward.",
          "score": 12,
          "created_utc": "2025-12-27 05:25:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6dfq3",
          "author": "NoleMercy05",
          "text": "Trying to migrate executives off Excel Desktop into a formal db + reporting + excel export. \n\nWhy,  you can't pry Excel away from many business leaders or executives with any alternative -  they just want the numbers to manipulate in excel.  They won't trust anything else.",
          "score": 10,
          "created_utc": "2025-12-27 11:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4j5ej",
          "author": "Upbeat-Conquest-654",
          "text": "Trying to build an abstract data model into the ETL pipeline. It turned out that a significant effort was necessary to fit new data into the existing data model and adding different data sources became an ordeal. Abstraction is great, but it should be it's own separate step and, if possible, performed on the spot, e.g. by views.",
          "score": 6,
          "created_utc": "2025-12-27 02:23:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw54gis",
              "author": "NoConversation2215",
              "text": "Hey, can you pls elaborate. I am involved in something like this and wonder if there are specific lessons I can use. Thank you!",
              "score": 1,
              "created_utc": "2025-12-27 04:47:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5dytk",
                  "author": "Upbeat-Conquest-654",
                  "text": "We get data from other teams via Kafka in JSON format. The idea was to have our own generic data model. This would add a layer of abstraction to hide the complexity of the source data. Unfortunately, despite my best efforts, the generic data model was not suitable. When we added another data source with supposedly similar data, it had a few properties that were different. It just didn't fit into the abstract data model and trying to transform it into this shape made a complex mess. When source data changed structure, it was a lot of work to change the entire pipeline. \n\nMy learning was to store the data as close as possible to the original structure, which makes it easy to react to changes. Another error was to abstract too early, based on a single type of data. I still think abstraction is a good idea, but I would move it further downstream, e.g. into views on the stored data. Basically, going for ELT rather than traditional ETL.",
                  "score": 2,
                  "created_utc": "2025-12-27 06:00:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwnqtn3",
              "author": "Nixxy_445",
              "text": "Data vault.",
              "score": 1,
              "created_utc": "2025-12-30 02:24:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwc6dz2",
          "author": "miljoneir",
          "text": "Corporate IT forced us to move to the cloud, only using microsoft products. Since we only have oracle db experience, consulants came over to guide us in \"the right direction\".\n\n\nNot bad per se, we have in house developed tools in apex/plsql that allow \"programmers\" to manipulate data using poorly written scripts and ui's. We need to get rid of all of that since it became unmaintainable. \n\n\nThose consultants insisted on doing all our data processing in spark - which is complete overkill for us (we process daily data syncs for clinical trials - hardly 50 mb of data per trial). They mentioned along the way \"oh yeah pandas is a thing too\". \n\n\nSo pandas it became, running on synapse.\nFirst time it went to production, it all broke, we got hit by the unstable datatypes in pandas and have to code around that all the time. Also the manipulations we do are relatively complex so the codebase isn't all that readable despite really trying to keep it simple (I hate the .loc function).\n\n\nNow learned that polars/duckdb are a thing, and most of that could have been avoided :(\n\n\nBonus, now management wants all those \"programmers\" to be on board too with the python/pandas thing.\nThis became a babysitter job, because most are unwilling to learn or don't even have the mindset to do problem solving. They just want the simple scripts back. That allowed non technical people to be hired for that \"programmer\" position, and was a huge mistake imo.",
          "score": 3,
          "created_utc": "2025-12-28 09:15:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq7fnw",
              "author": "dukas-lucas-pukas",
              "text": "The last few sentences gave me horrors. I’m a lead at my current org, and overall it’s pretty good, but we have people “like that” on our team and it is brutal. Constantly questioning anything that can be seen as difficult by them because they don’t want to learn any programming fundamentals.\n\nE.g. it took me a year to convince someone on our team that terraforming wasn’t the devil versus creating resources via the AWS management console.",
              "score": 2,
              "created_utc": "2025-12-30 13:34:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5qh08",
          "author": "CatgirlYamada",
          "text": "I think I'm about to regret my decision deploying my own Flink cluster. Shit is so damn hard to build and manage and it's been a month since my supervisor told me to create a \"real time data pipeline\". Luckily we might have a clear to revert to batch processing with Spark, but I'll probably stick to this method for another while to see whether we could actually break the ceiling.",
          "score": 2,
          "created_utc": "2025-12-27 07:53:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6hvuc",
              "author": "m1nkeh",
              "text": "I mean, unless you’ve got a very very specific use case for needing Flink..  Spark structured streaming all day every day over it",
              "score": 3,
              "created_utc": "2025-12-27 12:17:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6kadz",
                  "author": "CatgirlYamada",
                  "text": "It was my first choice before opting to Flink, but I don't know how to translate Debezium BSON into Paimon table schema. Spark is only really good for batch processing for Paimon table sink.\n\nAt least with Flink CDC you could either define both sink and source table with Flink SQL or just use YAML file that automatically creates both source and sink table.",
                  "score": 1,
                  "created_utc": "2025-12-27 12:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo0qqr",
          "author": "TheseShopping5409",
          "text": "Raw dogging Power BI 💀",
          "score": 2,
          "created_utc": "2025-12-30 03:18:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6irmg",
          "author": "dataflow_mapper",
          "text": "Locking into a tool or pattern too early without real volume to justify it. I’ve seen teams over engineer streaming pipelines or adopt a shiny framework when batch would have been fine for a long time. Six months later you are maintaining complexity that delivers very little extra value. The regret usually is not the tool itself, but underestimating the long term cost of operating it with a small team.",
          "score": 1,
          "created_utc": "2025-12-27 12:24:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2gd7v",
          "author": "Ship_Psychological",
          "text": "all of them. if you dont then your not improving fast enough.",
          "score": -26,
          "created_utc": "2025-12-26 19:10:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2ma3j",
              "author": "Pab_Zz",
              "text": "If you regret every decision you make after 6 months you're not a very good data engineer....",
              "score": 65,
              "created_utc": "2025-12-26 19:42:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3c3s6",
                  "author": "Swayfromleftoright",
                  "text": "Probably applies to any job tbh",
                  "score": 10,
                  "created_utc": "2025-12-26 22:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw4gonc",
              "author": "thickmartian",
              "text": "We must be on LinkedIn or something, cause it's the only place I've seen that perfect mix of sounding profound while delivering the most absurd take that can be.",
              "score": 8,
              "created_utc": "2025-12-27 02:07:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbnc2x",
                  "author": "theoriginalmantooth",
                  "text": "If this were LinkedIn he would’ve responded to every comment with “totally agree!” 🤦‍♂️",
                  "score": 1,
                  "created_utc": "2025-12-28 06:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw36auv",
              "author": "Noonecanfindmenow",
              "text": "This is ridiculous.\n\nQuantity of decisions isn't a metric for growth, nor is empty bravado.\n\nIf you regret every decision you make, then the one thing that's clearly not improving is your decision making ability.",
              "score": 8,
              "created_utc": "2025-12-26 21:31:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw39qp0",
              "author": "One-Salamander9685",
              "text": "~~no~~ all regrets",
              "score": 4,
              "created_utc": "2025-12-26 21:50:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4fke4",
                  "author": "Table_Captain",
                  "text": "** regerts",
                  "score": 2,
                  "created_utc": "2025-12-27 02:00:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw5mgzr",
              "author": "SuggestAnyName",
              "text": "You should stop taking decisions. It's not your cup of tea.",
              "score": 3,
              "created_utc": "2025-12-27 07:15:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4by93",
              "author": "ThePunisherMax",
              "text": "Worst take ive ever seen.",
              "score": 2,
              "created_utc": "2025-12-27 01:37:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6d9xp",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -10,
          "created_utc": "2025-12-27 11:36:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6ty51",
              "author": "haragoshi",
              "text": "😆🤣😆 good luck",
              "score": 2,
              "created_utc": "2025-12-27 13:46:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6v9wj",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2025-12-27 13:55:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyr86l",
      "title": "One Tool/Skill other than SQL and Python for 2026",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pyr86l/one_toolskill_other_than_sql_and_python_for_2026/",
      "author": "crushed_peppe",
      "created_utc": "2025-12-29 16:48:13",
      "score": 54,
      "num_comments": 20,
      "upvote_ratio": 0.92,
      "text": "If you had to learn one tool or platform beyond SQL and Python to future-proof your career in 2026, what would it be?\n\nI’m a Senior Database Engineer with 15+ years of experience, primarily in T-SQL (≈90%) with some C#/.NET. My most recent role was as a Database Engineering Manager, but following a layoff I’ve returned to an individual contributor role.\n\nI’m noticing a shrinking market for pure SQL-centric roles and want to intentionally transition into a Data Engineering position. Given a 6-month learning window, what single technology or platform would provide the highest ROI and best position me for senior-level data engineering roles?\n\nEdit: Thank you for all your responses. I asked ChatGPT and this is what it thinks I should do, please feel free to critic:\n\nGiven your background and where the market is heading in 2026, **if I had to pick exactly one tool/skill beyond SQL and Python**, it would be:\n\n# Apache Spark (with a cloud-managed flavor like Databricks)\n\nNot Airflow. Not Power BI. Not another programming language. **Spark.**",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pyr86l/one_toolskill_other_than_sql_and_python_for_2026/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwkkgoe",
          "author": "AutoModerator",
          "text": "Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-29 16:48:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwknvbf",
          "author": "Thanael124",
          "text": "Cloud stack, IaC and CI/CD.",
          "score": 70,
          "created_utc": "2025-12-29 17:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkq3tk",
              "author": "Beautiful-Hotel-3094",
              "text": "Correct answer. Basically: everything",
              "score": 21,
              "created_utc": "2025-12-29 17:14:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwuxbq7",
              "author": "JBalloonist",
              "text": "Yep, this is my answer. Learned IaC and CI/CD mostly in the last year at a previous role and made my new role so much easier.",
              "score": 1,
              "created_utc": "2025-12-31 04:10:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwl05iz",
          "author": "mrbartuss",
          "text": "people skills",
          "score": 28,
          "created_utc": "2025-12-29 18:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl8rp7",
              "author": "PaulSandwich",
              "text": "Unironically, yes.   \n   \nEqually/more important, business skills.   \nAI is weakest at understanding business nuance and where the operational value in data is. Writing and optimizing basic pipelines is trivial these days.    \n   \nTurning data into KPIs that make or save your employer money will never go out of style and, thankfully, it's where AI fails the hardest.",
              "score": 5,
              "created_utc": "2025-12-29 18:41:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqc3uy",
          "author": "analyticspitfalls",
          "text": "I am going old school. Data modeling. Both Kimball and Inmon methods.  \n\nIf you can be a part of building Stupidly simple data for you and others to use. You will be a ninja!!!\n\nI would bet less than 5% of the people I have worked with know this dark art - and those that do are dangerous.",
          "score": 21,
          "created_utc": "2025-12-30 14:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr0kpi",
              "author": "bugtank",
              "text": "This is the way",
              "score": 3,
              "created_utc": "2025-12-30 16:08:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwklcp2",
          "author": "MikeDoesEverything",
          "text": ">I’m noticing a shrinking market for pure SQL-centric roles and want to intentionally transition into a Data Engineering position\n\nIn my opinion, it has been like that since I became a DE which was around 5 years ago.\n\n>Given a 6-month learning window, what single technology or platform would provide the highest ROI and best position me for senior-level data engineering roles?\n\nYou'd fit well within an organisation which is Microsoft leaning.  Pick up the Azure stack as you go as your skills slot directly into it.\n\nI have worked with a lot of people who have your exact stack and they all have what you have which is a lot of T-SQL and not a huge amount of general purpose programming.  This is both good and bad.  Good because if you applied to a Microsoft heavy company, you're probably going to get in.  Bad because it means everybody on your team is likely to be like you i.e. not a huge pool of skills to learn from.  If that isn't a problem for you, then that's cool.  The job advertised is also likely to be on prem only.  Again, depends what you want to do on a daily basis as there are going to be some on-prem only jobs with your stack which pay really well.\n\nIf you want to get away from a Microsoft stack, brush up on DE and cloud fundamentals as well as pick up Python.",
          "score": 13,
          "created_utc": "2025-12-29 16:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkz91v",
              "author": "aisakee",
              "text": "This, the azure stack is a no-brainer if you want to land a job quickly. Most big companies (no startups nor tech companies) use the Microsoft package so they by default go to Azure for everything.",
              "score": 3,
              "created_utc": "2025-12-29 17:57:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwku2x8",
          "author": "beiendbjsi788bkbejd",
          "text": "Docker",
          "score": 6,
          "created_utc": "2025-12-29 17:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkkg83",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 4,
          "created_utc": "2025-12-29 16:48:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu8rxn",
          "author": "dev_lvl80",
          "text": "I was purely TSQL dev/dba decade ago. Totally understand what are you trying to do.\n\nMy 2c on core skills in demand\n\n\\- SQL (absolutely must wit deep level of expertise in optimization)\n\n\\- Python (leet code medium level)\n\n\\- Databricks + PySpark or Snowflake\n\n\\- DBT/Airflow \n\n\\- Data Modeling (surprise)\n\n\\- A bit AI/ML\n\nPS in you case Databricks/Pyspark looks about right",
          "score": 2,
          "created_utc": "2025-12-31 01:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkwq4z",
          "author": "ScholarlyInvestor",
          "text": "I’d recommend reading up on Fundamentals of Data Engineering itself. Maybe start with the book by Reis/Housley. There are many tools in the marketplace but betting on Databricks or Snowflake besides boning up on cloud stack will help.",
          "score": 2,
          "created_utc": "2025-12-29 17:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl15b1",
          "author": "exjackly",
          "text": "Future proof is going to require cloud.  Pick a stack - AWS is the leader, but I've been seeing more companies choose Azure (good with your T-SQL background) and GCP.\n\nStart with the portions that match your experience (the databases and cloud storage) and start working your way out to the data transformers, pipelines/orchestrators, CI/CD.  \n\nIaC isn't something to ignore, but it shouldn't be a priority until you are comfortable with the rest unless you wind up at a mid-small place that doesn't have dedicated infra roles.",
          "score": 2,
          "created_utc": "2025-12-29 18:06:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpgcgg",
          "author": "jjopm",
          "text": "SQL and Python",
          "score": 2,
          "created_utc": "2025-12-30 10:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpr5w7",
          "author": "asevans48",
          "text": "Start with the cloud, data streaming and automation, and basic data quality tools. Your stack sounds legacy. Ive written 1 ssis package with a c# program in 4 years even with sql server. Get some cloud certs to start. Also, data governance is becoming the hot topic. I am pushing hard to avoid the literal budget destruction of microsoft fabric with open source tools. Fabric-like patterns and AI tools will be prevalent. Autonomous systems might be a thing. A fabric cert is probably a good idea. CDC is still a good skill to have. Personally leaning into AI and ml education and certs. We are close to self-service data analytics. In saas land api automation is key. Less and less folks offer free or cheap.database replication.",
          "score": 1,
          "created_utc": "2025-12-30 11:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqxjrq",
          "author": "Immediate-Pair-4290",
          "text": "If AI is supposedly going to replace coders then you need to be able to turn business problems into solutions. No need to learn another language. Tools like DuckDB make Spark obsolete for the majority of companies. It’s not really that different from SQL anyway other than the storage formats.",
          "score": 1,
          "created_utc": "2025-12-30 15:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwku43q",
          "author": "BrupieD",
          "text": "There will always be a market for what I think of as \"last mile\" tools. That is, tools that transform raw data into the consumed form. If management/end users in your company likes Excel, maybe VBA is still a good choice. If people want reports and dashboards, PowerBi might be it.",
          "score": -5,
          "created_utc": "2025-12-29 17:34:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqd9j7",
              "author": "Budget-Minimum6040",
              "text": "> maybe VBA is still a good choice\n\nYou can't be serious.\n\nHe/she wants to go from DBA to DE. VBA is a piece of shit you won't touch with both careers.",
              "score": 8,
              "created_utc": "2025-12-30 14:08:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyen4a",
      "title": "Kafka - how is it typically implemented ?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pyen4a/kafka_how_is_it_typically_implemented/",
      "author": "AdFormal9428",
      "created_utc": "2025-12-29 06:13:47",
      "score": 54,
      "num_comments": 20,
      "upvote_ratio": 0.95,
      "text": "Hi all,\n\nI want to understand how Kafka is typically implemented in a mid sized company and also in large organisations.\n\nStreaming is available in Snowflake as a Streams and Pipes (if I am not mistaken) and presume other platforms such as AWS (Kinesis) Databricks provide their own version of streaming data ingestion for Data Engineers.\n\nSo what does it mean to learn Kafka ? Is it implemented separately outside of the tools provided by the large scale platforms (such as Snowflake, AWS, Databricks) and if so how is it done ?\n\nAsking because I see Joh descriptions explicitly mention Kafka as a experience requirement while also mentioning Snowflake as required experience . What exactly are they looking at and how is it different to know Snowflake streams and separately Kafka.\n\nIf Kafka is deployed separately to Snowflake / AWS / Databricks, how is it done? I have seen even large organisations put this as a requirement.\n\nTrying to understand what exactly to learn in Kafka, because there are so many courses and implementations - so what is a typical requirement in a mid to large organization ?\n\n  \n\\*Edit\\* - to clarify - I have asked about streaming, but I meant to also add Snowpipe. ",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pyen4a/kafka_how_is_it_typically_implemented/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwi3esg",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-29 06:13:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi4bsj",
          "author": "Abdul_DataOps",
          "text": "Think of Kafka as the Central Nervous System of the enterprise while Snowflake/Databricks are the Brain (Storage/Compute).\n\n1. Is it deployed separately?\nYs. In 99% of large organizations Kafka is a completely separate standalone cluster. It usually lives in its own VPC on AWS (MSK) or is managed via Confluent Cloud. It sits upstream from your data warehouse.\n\n2. Why use Kafka if Snowflake has Streams?\nBecause Snowflake Streams are proprietary. If u use Snowflake Streams you are locked into Snowflake. But a large enterprise has 50 other systems that need that data (Microservices, Real-time Dashboards, ML Models, Fraud Detection). None of those can read from a Snowflake Stream efficiently in real-time.\n\n3. Typical Implementation Pattern\nSource (App/DB) -> [Kafka Producer] -> Kafka Topic -> [Kafka Connect / Snowpipe] -> Snowflake\n\nu learn Kafka to handle the transport layer. \nu learn Snowflake Streams to handle the ingestion/transformation layer once the data lands. They are complementary not competitive.",
          "score": 94,
          "created_utc": "2025-12-29 06:21:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiea66",
              "author": "poinT92",
              "text": "Solid answer",
              "score": 8,
              "created_utc": "2025-12-29 07:47:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwmbywp",
                  "author": "CulturMultur",
                  "text": "Kafka and Snowflake Streams are completely different technologies with different semantics and reasoning.",
                  "score": 3,
                  "created_utc": "2025-12-29 21:50:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwmlrlu",
              "author": "aisakee",
              "text": "Are all orgs required to use stream jobs? I mean, many JDs contains Kafka as a req but since I started as DE I've never had to use streams.. which cases are optimal for this technology if you're not a FAANG?",
              "score": 2,
              "created_utc": "2025-12-29 22:39:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj8yrc",
          "author": "addictzz",
          "text": "Kafka is basically a big buffer for your immense influx stream of data so that your data consumers do not get overwhelmed.\n\nWhen it is listed as required skills, it is either requiring you to manage and fine-tune Kafka clusters (if the company is not using a managed service Kafka already), develop Kafka connectors/sinks/sources, or require you to develop Kafka consumers.",
          "score": 8,
          "created_utc": "2025-12-29 12:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwie29u",
          "author": "robverk",
          "text": "Kafka is used to decouple your processing. Ingest just reads and pushes messages in. Now 5 processes that need to read raw source data can read that topic. One of them can clean, normalize, parse and push into the next topic. Etc\n\nYou van build high performant but very flexible chains of processing while each step can be a simple unit of: input - process - output. If you need more performance for processing a topic you can just add more consumers/producers so it fits well within distributed processing frameworks.",
          "score": 3,
          "created_utc": "2025-12-29 07:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjmoh3",
          "author": "jduran9987",
          "text": "In most cases you aren’t touching a Kafka cluster as a data engineer. You typically have SWE or platform folks managing everything. I would just focus on ingesting events from a topic either by sending them to S3 or writing a consumer. At some point, those ingested events are stored in a Snowflake table.",
          "score": 4,
          "created_utc": "2025-12-29 13:55:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjs8a8",
              "author": "AdFormal9428",
              "text": "Thank you. What technologies do I need to learn to be able to \"sending them to S3 or writing a consumer\" ?",
              "score": 1,
              "created_utc": "2025-12-29 14:27:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuxvzs",
          "author": "Front-Ambition1110",
          "text": "Kafka is a pub/sub system, for decoupling producers and consumers, with a very robust ecosystem (Kafka Connect, KSQL, etc). Data streaming/replication/CDC is just one use of Kafka. I imagine most Kafka uses are not oriented to data engineering, but more publisher-broker-consumer backend apps.\n\n\"Kafka experience requirement\" is I guess more about using, configuring, and maintaining a clustered pub/sub system. It's worth learning (partitions, consumer groups, replicas, retention and cleanup policy, etc), but DON'T rush to implement it in prod! This thing is a beast",
          "score": 2,
          "created_utc": "2025-12-31 04:14:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi7noz",
          "author": "AdFormal9428",
          "text": "Thank you u/Abdul_DataOps\n\nAre there any tools or platforms that help with implementation of Kafka ? For analogy, DBT provides a way to write SQL transformations, Databricks allows easier implementation of Spark clusters etc. - similarly are there tools for Kafka or is it pure open source implementation? If tools exist - what are the tools typically used? You have mentioned MSK - I assume this is such a tool ?\n\nAlso, in what way is a Data Engineer implements Kafka? Because typically when I say Data Engineer I think of Analytics / Data Provisioning for ML etc. - essential for the data platform - and for this do Snow pipes and other such platform tools help?\n\nDo Data Engineers also build Kafka pipelines for consumption by other software applications such as microservice ? Or do the Software Devs do it themselves?",
          "score": 2,
          "created_utc": "2025-12-29 06:49:24",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwjkdq8",
              "author": "userousnameous",
              "text": "Kafka at scale gets complex. It wouldn't be data engineering, it would likely be a competent software engineering team, followed by ongoing adding of services ( integration with company auth, management and alerting capabilities, scaling).  You could reduce the burden somewhat with a AWS/GCP/Azure offering, but someone is going to have to be involved and cognizant of that system, upkeep and cost management.",
              "score": 2,
              "created_utc": "2025-12-29 13:42:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwk3gwg",
                  "author": "AdFormal9428",
                  "text": "Thank you. When I watch YouTube videos, Data Engineers specify Kafka as a technology to learn. (videos listing tech. to learn and not getting in depth).\n\nI wonder what they mean when they say Kafka. Like what programming language + specific library to learn as a Data Engineer. What tech tools to learn specifically focus for it.",
                  "score": 2,
                  "created_utc": "2025-12-29 15:26:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpg9pr",
              "author": "Exciting_Tackle4482",
              "text": "Tools like [Lenses.io](http://Lenses.io) (note: I work for them) will help simplify operating Kafka as as a data/software engineer.  They offer an operating plane for your Kafka estate.  Including to do data exploration, integration & transformation.  \n\nTo answer your last question, the idea is to make the end user (ie. software engineer in your case) as autonomous to build data pipelines in Kafka with the likes of Lenses.  Check out: [https://www.youtube.com/watch?v=Z4yeQFyZ75Y&t=131s](https://www.youtube.com/watch?v=Z4yeQFyZ75Y&t=131s)",
              "score": 1,
              "created_utc": "2025-12-30 10:00:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlf8lj",
          "author": "No_Song_4222",
          "text": "Meaning to learn Kafka can go from everything to everything.  However irrespective of your level of choice of depth the following is my opinion : \n\n1. Why it was built ? How is it different than a typical pub/sub. \n\n2. Topics, partitions, producers and consumer. Typical things like how exactly once delivery, ordering etc just know them it is good. \n\nHow deep ? On most occasions as a DE you just consume the data and dump everything in a storage layer ( the single source of truth).   \n\n\nif your job descriptions asks you to setup a cluster, manage, work at a  source level, fine tuning, making changes and coding in Java etc. You need a lot more depth in  distributed systems and understanding in Java and Scala to debug performance issues and stuff.",
          "score": 1,
          "created_utc": "2025-12-29 19:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpfmr8",
          "author": "eccentric2488",
          "text": "There is a difference between open source frameworks and managed services for these open source frameworks.",
          "score": 1,
          "created_utc": "2025-12-30 09:54:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiy6vl",
          "author": "RangePsychological41",
          "text": "If someone asked me that in real life I’d immediately ask them “what is Kafka?” Because, in all likelihood they don’t know. Which makes the point of question… questionable.",
          "score": 0,
          "created_utc": "2025-12-29 10:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi53z2",
          "author": "West_Good_5961",
          "text": "Unnecessary",
          "score": -7,
          "created_utc": "2025-12-29 06:27:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwihemd",
              "author": "Doto_bird",
              "text": "You can't say that. In fact you cant say that about most tech out there. Remember all things were built to solve a specific problem. Maybe that tech doesn't generalize as well to other problems, but it doesn't mean there isn't some niche use case it will work really well with.\n\nIn terms of Kafka, very few services scale and integrate as well with things like Flink so that you can handle insane volumes while maintaining consistent throughput. Sure, there are managed cloud solutions thst can probably do the same, but you'll be surprised how many enterprises still have massive on-prem clusters that they need to use until EOL to get value of of their investment.\n\nKafka is, however, unnecessary if you're running a small 10tps streaming service. There are easier ways to handle that.",
              "score": 2,
              "created_utc": "2025-12-29 08:16:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q04miu",
      "title": "Fellow DEs — what's your go-to database client these days?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q04miu/fellow_des_whats_your_goto_database_client_these/",
      "author": "SainyTK",
      "created_utc": "2025-12-31 05:10:47",
      "score": 53,
      "num_comments": 65,
      "upvote_ratio": 0.87,
      "text": "Been using DBeaver for years. It gets the job done, but the UI feels dated and it can get sluggish with larger schemas. Tried DataGrip (too heavy for quick tasks), TablePlus (solid but limited free tier), Beekeeper Studio (nice but missing some features I need).\n\nWhat's everyone else using? Specifically interested in:\n\n* Fast schema exploration\n* Good autocomplete that actually understands context\n* Multi-database support (Postgres, MySQL, occasionally BigQuery)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q04miu/fellow_des_whats_your_goto_database_client_these/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwv7lsq",
          "author": "PatientlyAnxiously",
          "text": "DBeaver is my go to for multi database support. The only others I use are specific to one system: SSMS for MS SQL Server and Snowsight web UI for Snowflake.",
          "score": 66,
          "created_utc": "2025-12-31 05:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwzryh",
              "author": "SmallAd3697",
              "text": "Yes, for best experience use the client provided by vendor.  No generic client will give a better experience than one that is tailored for a particular database engine.  Ssms is tailored to SQL and azure SQL and has lots of auth mechanisms for connecting to databases (as one simple example).\n\nWondering about the OP question itself.  I think certain DE's don't want to invest in learning multiple tools.  Or they have hate for a vendor (msft) and use that vendor's tools as little as possible.  In that case you are making a deliberate compromise, and that is a totally acceptable path as well.",
              "score": 2,
              "created_utc": "2025-12-31 14:11:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv6tgv",
          "author": "vikster1",
          "text": "dbeaver works. dbeaver is free. dbeaver does not try to shove ai shit down my throat. as far as I'm concerned, dbeaver is the peak of software in 2025 and most likely in 2026 as well but maybe something big changes in the next hours. idk. I'm not an oracle.",
          "score": 101,
          "created_utc": "2025-12-31 05:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvghrv",
              "author": "SRMPDX",
              "text": "You may not be an oracle, but you are a sequel server.",
              "score": 19,
              "created_utc": "2025-12-31 06:29:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwydgnc",
                  "author": "AlGoreRnB",
                  "text": "I prefer the og server tbh. I find the sequel to be shallow and pedantic.",
                  "score": 1,
                  "created_utc": "2025-12-31 18:24:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwv9st7",
              "author": "ZirePhiinix",
              "text": "Oracle DB is pretty mid. It's not really bad, but god damn I hate that I can't add a column to a `select *` without giving my table an alias.",
              "score": 3,
              "created_utc": "2025-12-31 05:36:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwwn3j5",
              "author": "Budget-Minimum6040",
              "text": "DBeaver is awesome but it only supports ODBC/JDBC which means anything else (like BigQuery dryrun giving totalBytesProcessed information on how much data = $$$ your query will cost before you send it) is not possible.\n\nSee this 6 year old issue which is still open: https://github.com/dbeaver/dbeaver/issues/4907\n\nSo DBeaver is a bad choice for DBs where you pay per queried data and developing in the browser is pure shitshow. I haven't found a solution for a proper SQL IDE that supports such cloud DBs so far ...",
              "score": 1,
              "created_utc": "2025-12-31 12:50:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx41mpi",
                  "author": "querylabio",
                  "text": "Have you tried Querylab.io?\n\nFull disclosure: I’m the founder.\n\nWe’re building a BigQuery-native IDE (no JDBC/ODBC). Beyond a clean, modern UI, we already support:\n\n* dry-run with per-query limits and daily / weekly / monthly budgets\n* per-CTE cost breakdown\n* partial execution & cost estimation for selected CTEs\n* TABLESAMPLE dev mode for cheap iteration\n* on-demand vs reservation cost comparison\n* proper handling of nested & repeated fields\n* diagnostics for expensive patterns like SELECT \\*\n* full Pipe Syntax support\n* and many more\n\nHappy to get feedback!",
                  "score": 2,
                  "created_utc": "2026-01-01 17:57:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwr975",
              "author": "Ok-Improvement9172",
              "text": "No vi mode though",
              "score": 1,
              "created_utc": "2025-12-31 13:19:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzv3eb",
                  "author": "Daemoncoder",
                  "text": "In Dbeaver? - Use Vrapper.",
                  "score": 1,
                  "created_utc": "2025-12-31 23:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvd2w6",
          "author": "addictzz",
          "text": "DBeaver. It is ugly, I don't like the interface. But it works everytime and it is free. No ads or request to upgrade to Pro version disrupting my workflow.",
          "score": 30,
          "created_utc": "2025-12-31 06:01:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnpj7",
              "author": "SainyTK",
              "text": "What databases do you connect using DBeaver?",
              "score": 2,
              "created_utc": "2025-12-31 07:31:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvp6z0",
                  "author": "addictzz",
                  "text": "Mysql, postgres, databricks",
                  "score": 2,
                  "created_utc": "2025-12-31 07:45:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv9dk4",
          "author": "SirGreybush",
          "text": "Visual Studio Code",
          "score": 41,
          "created_utc": "2025-12-31 05:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxdsmm",
              "author": "The_Wanderer33",
              "text": "Interesting tell me more…",
              "score": 3,
              "created_utc": "2025-12-31 15:28:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxmhlj",
                  "author": "ask-the-six",
                  "text": "There’s extensions for basically any database. Really convenient to make a devcontainer with all the tools installed needed per project. One example: \n\nhttps://marketplace.visualstudio.com/items?itemName=mtxr.sqltools",
                  "score": 6,
                  "created_utc": "2025-12-31 16:11:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwy10v9",
                  "author": "SirGreybush",
                  "text": "It’s made by Microsoft but is open source and has add-ons for everything, database types, Snowflake, Python, etc.",
                  "score": 1,
                  "created_utc": "2025-12-31 17:23:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvbc84",
          "author": "Few_Noise2632",
          "text": "datagrip. i have all products pack and it is pretty cheap together with all the other stuff from jetbrains (total is 180$ after 3 years of sub)\n\ndbeaver is good enough for some people but i can't afford to spend my eyes resource on that ugliness",
          "score": 39,
          "created_utc": "2025-12-31 05:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvg79f",
              "author": "BeardedYeti_",
              "text": "This should be the only answer.",
              "score": 8,
              "created_utc": "2025-12-31 06:26:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvhfgz",
                  "author": "randomName77777777",
                  "text": "Yeah, datagrip all the way. \n\nI now spend most of my days on databricks, but only because I haven't found a good way to connect it to datagrip. But for all my other data sources I use datagrip - azureSQL, big query, redshift, postgres.",
                  "score": 5,
                  "created_utc": "2025-12-31 06:36:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwzrlgk",
              "author": "scallion_2",
              "text": "You can set up git projects in DataGrip too. I work with multiple SQL repos so this is a huge benefit imo.",
              "score": 1,
              "created_utc": "2025-12-31 22:55:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvrcap",
          "author": "MichelangeloJordan",
          "text": "This is my installation of DBeaver. There are many like it, but this one is mine.",
          "score": 7,
          "created_utc": "2025-12-31 08:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvduzh",
          "author": "blueadept_11",
          "text": "Dbvisualizer for 15 years now",
          "score": 6,
          "created_utc": "2025-12-31 06:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo324",
              "author": "SainyTK",
              "text": "Interesting choice. Very solid and good looking UI. $229 one-time purchase.",
              "score": 2,
              "created_utc": "2025-12-31 07:35:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwxibfl",
                  "author": "DiabolicallyRandom",
                  "text": "Their support is excellent, very responsive, and they add features based on customer request regularly, fix bugs, etc.\n\nI used it for several years at my last gig (at my urging, the company bought licenses for our team), and they were always very helpful.\n\nThe only \"drawback\" is being Java/JDBC based, but that gives you the entire world of possible database drivers, so more of a boon IMO.",
                  "score": 3,
                  "created_utc": "2025-12-31 15:50:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwx9yxg",
              "author": "Askew_2016",
              "text": "I loved DBVisualizer but my company decommissioned it so I’m using DBBeaver now",
              "score": 2,
              "created_utc": "2025-12-31 15:08:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvom4x",
          "author": "RemcoE33",
          "text": "Look at Beekeeper",
          "score": 5,
          "created_utc": "2025-12-31 07:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww7ok1",
              "author": "firebypeace",
              "text": "I love working with Beekeeper. Paying for it helps as I like using it for Duckdb things. I'd recommend it",
              "score": 1,
              "created_utc": "2025-12-31 10:40:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww7xm7",
                  "author": "RemcoE33",
                  "text": "Yeah I pay as well. Love the project, the speed of development and the amount of db's it supports. I do use it a lot with duckdb, SQLite and Bigquery.",
                  "score": 1,
                  "created_utc": "2025-12-31 10:42:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvmky4",
          "author": "dataflow_mapper",
          "text": "I still see a lot of people settle back on DBeaver despite the complaints, mostly because it is the least bad all around option. The UI is clunky, but the schema explorer and cross database support are hard to beat once you tune it a bit.\n\nWhat has helped me more than switching clients is changing how I use them. Smaller result set limits by default, fewer auto refreshes, and leaning on the SQL editor instead of clicking around the tree constantly. That alone fixes most of the sluggish feeling.\n\nI have not found a single tool that nails fast exploration, smart autocomplete, and wide database support without tradeoffs. Most teams I know end up with one main client and a lighter secondary one for quick checks, rather than trying to force one tool to do everything.",
          "score": 3,
          "created_utc": "2025-12-31 07:21:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvxzt5",
          "author": "m915",
          "text": "Usually VS code extensions",
          "score": 5,
          "created_utc": "2025-12-31 09:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwa1t8",
          "author": "LargeSale8354",
          "text": "I liked Aquafold DataStudio. It was hell getting management to pay for licenses. \nDBeaver is OK.\nBasically,  the choice is \"What free development IDE\" will management allow?\" Not, \"What IDE allows our staff to be most productive?\"",
          "score": 3,
          "created_utc": "2025-12-31 11:02:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx140f",
              "author": "NoResolution4706",
              "text": "Using this also, my whole team is. Really does everything I need from it.",
              "score": 2,
              "created_utc": "2025-12-31 14:19:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwxcht",
          "author": "IckyNicky67",
          "text": "I’m surprised no one’s mentioned PyCharm yet. It has a great interface and it makes it so easy to switch from SQL/databases to Python (or whatever programming languages you tend to use besides of SQL)\n\nEDIT: Forgot to add that it has all three of your requirements",
          "score": 4,
          "created_utc": "2025-12-31 13:57:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxw0r7",
          "author": "AcanthisittaMobile72",
          "text": "pgAdmin or vscode/vscodium extension",
          "score": 4,
          "created_utc": "2025-12-31 16:58:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvc0qm",
          "author": "thickmartian",
          "text": "Yeah I'm using TablePlus.\n\nHappy with it. It does enough. I get most of the info (schema etc ...) I need from SQL queries anyways.\n\nAt least it's relatively pleasing on the eye...",
          "score": 3,
          "created_utc": "2025-12-31 05:53:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnrdq",
              "author": "SainyTK",
              "text": "Do you pay for TablePlus or just use a free version of it?",
              "score": 1,
              "created_utc": "2025-12-31 07:32:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvuodg",
          "author": "james2441139",
          "text": "Data architect here, but I do a fair bit of pipeline engineering as well. We are a fully MS shop, so primary setup is Synapse and MS Fabric. I have been using DB Schema Pro, and found it really useful for data modeling, exploration, design, documentation. It connects to all major databases, has fast schema exploration. Doesn't have autocomplete in the sense of something like Intellisense, but that is not important for me.\n\nTons of tools out there, even VSCode has quite a few extensions. I settled on this for now, and focusing my productivity on actual data modeling rather than tools.",
          "score": 4,
          "created_utc": "2025-12-31 08:37:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvuadv",
          "author": "Sad_Cell_7891",
          "text": "try OmniDB",
          "score": 2,
          "created_utc": "2025-12-31 08:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvxey2",
          "author": "bjatz",
          "text": "NiFi ExecuteSQL processor",
          "score": 2,
          "created_utc": "2025-12-31 09:03:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvyrff",
          "author": "Alternative-Guava392",
          "text": "Dbeaver",
          "score": 2,
          "created_utc": "2025-12-31 09:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx0nrx",
          "author": "k00_x",
          "text": "Atom. It's legacy and out of date but was great. Microsoft nerfed it so it didn't compete with vscode when they acquired GitHub.",
          "score": 2,
          "created_utc": "2025-12-31 14:16:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx2eyx",
          "author": "s-to-the-am",
          "text": "Datagrip",
          "score": 2,
          "created_utc": "2025-12-31 14:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxjst7",
          "author": "Awkward_Tick0",
          "text": "Does nobody use ssms anymore…?",
          "score": 2,
          "created_utc": "2025-12-31 15:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzdeas",
              "author": "Cupakov",
              "text": "I do but not by choice ",
              "score": 1,
              "created_utc": "2025-12-31 21:35:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxkub0",
          "author": "5pitt4",
          "text": "Datagrip community version",
          "score": 2,
          "created_utc": "2025-12-31 16:03:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy2s1v",
          "author": "IAmBeary",
          "text": "im going to get laughed at but Im using mysqlworkbench \n\nmy favorite feature is when it crashes\n\nluckily we are transitioning towards blob storage for a datalake so the app collects dust most of the time. I already have my profiles set up so there's a cost to switching clients",
          "score": 2,
          "created_utc": "2025-12-31 17:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwybrw5",
          "author": "dirks74",
          "text": "Navicat Premium",
          "score": 2,
          "created_utc": "2025-12-31 18:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0k85k",
          "author": "jayzfanacc",
          "text": "I use Azure Data Studio because I like that it feels like VS Code. I will switch to VS Code when Azure Data Studio gets deprecated.",
          "score": 2,
          "created_utc": "2026-01-01 01:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx130zd",
          "author": "Suspicious_East591",
          "text": "Datagrip with license some companies offer us license to use that but I see other teammates who prefer using dbeaver",
          "score": 2,
          "created_utc": "2026-01-01 03:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvkf8c",
          "author": "West_Good_5961",
          "text": "Really depends on the dbms. Currently using VScode extensions for everything because it’s the only application we’re allowed to install.\n\nDb Forge is very good and probably meets your requirements.",
          "score": 2,
          "created_utc": "2025-12-31 07:02:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvmups",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2025-12-31 07:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvojnx",
              "author": "SainyTK",
              "text": "For those who come across and aren't happy with DBeaver like me, you may consider trying https://sheeta.ai.\n\nFor transparency, I'm the builder of it. I know that \"AI\" is something prohibited here, but 90% of this app is not about AI. It's just another SQL client with cleaner UI with fully functional good features inspired by best tools we all know. All non-AI features are completely free.\n\nSo, please feel free to give it a chance and do let me know your thoughts.",
              "score": -18,
              "created_utc": "2025-12-31 07:39:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwjxnj",
                  "author": "soluto_",
                  "text": "Ruined an otherwise good thread.",
                  "score": 3,
                  "created_utc": "2025-12-31 12:26:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvjyfx",
          "author": "burningburnerbern",
          "text": "You’re gonna hate me but I just use the web UI",
          "score": 0,
          "created_utc": "2025-12-31 06:58:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puqkx7",
      "title": "Rust for data engineering?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1puqkx7/rust_for_data_engineering/",
      "author": "otto_0805",
      "created_utc": "2025-12-24 15:58:00",
      "score": 51,
      "num_comments": 55,
      "upvote_ratio": 0.88,
      "text": "Hi, I am curious about data engineering. Any DE using Rust as their second or third language? \n\nDid you enjoy it? Worth learning for someone after learning the fundamental skills for data engineering? \n\nIf there are any blogs, I am up to read. So please share your experience.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1puqkx7/rust_for_data_engineering/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvqficj",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-24 15:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqjgmf",
          "author": "GradientAscent713",
          "text": "Yes, and I enjoy rust but i have yet to find a scenario where I truly need rust in a data pipeline. Its hard to justify as it is very rare for a whole team to know rust. I think it’s easier to justify using it for CLI tools as tooling is less critical. \n\nOne exception may be ML data pipelines that need to do large scale text normalization before training. And I do think eventually the model trainers will also be written in rust instead of Python with FFI into C/C++ like Pytorch.",
          "score": 55,
          "created_utc": "2025-12-24 16:18:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrgm8p",
              "author": "Beautiful-Hotel-3094",
              "text": "We heavily use rust in places where we need speed, for example in some risk calculations, marginal volatility and some cases for fx forward curves interpolations. It is used in the industry, just needs a good use case.",
              "score": 13,
              "created_utc": "2025-12-24 19:18:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsbqgg",
                  "author": "Leading-Inspector544",
                  "text": "That's well outside the scope of DE, but sounds pretty cool",
                  "score": 14,
                  "created_utc": "2025-12-24 22:23:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvseicp",
              "author": "seanv507",
              "text": "Yea, but typically libraries are written in rust and exposed in python\n\nEg polars",
              "score": 6,
              "created_utc": "2025-12-24 22:41:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqtawf",
          "author": "lozinge",
          "text": "Met a guy who is a DE works for a hedge fund who has rewritten a lot of their processing pipelines in rust to great effect fwiw; do agree its probs not gonna help for most, and I love rust",
          "score": 14,
          "created_utc": "2025-12-24 17:11:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqo3ao",
          "author": "DaveMitnick",
          "text": "I ported a few basic Python functions that e.g calculate averages of milions of lists to a fully mulithreaded Rust (based on Rayon crate) and the speedup is circa about 100x. I package it as Python .whl binding via Pyo3. It’s used in prod. Now I am trying more low level stuff like reading Parquet files byte by byte to see if I can match the performance of industry tools. I would love to work on something more advanced like query engine but I am not there yet in terms of skill and experience :) I am also curious how would Airflow rewrite go (as Airflow is implemented in Python, not even Scala like Spark) with some tweaks like async but I guess it’s not physically possible for one person. It’s definitely easier to read source code of the tools I use since I started learning low level stuff.",
          "score": 22,
          "created_utc": "2025-12-24 16:43:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqwwpf",
              "author": "RustOnTheEdge",
              "text": "If you want to get experience with query engines (olap), then I can recommend [this website](https://howqueryengineswork.com/00-introduction.html). Although the examples are in Kotlin, it gives a terrific introduction to the go into a project like Datafusion, which is such an epic project I just can’t stop promoting it haha\n\nReally cool stuff!",
              "score": 6,
              "created_utc": "2025-12-24 17:31:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvs8yft",
                  "author": "daguito81",
                  "text": "DataFusion is amazing. We’re currently porting a lot of spark work into data fusion and having very good results.",
                  "score": 2,
                  "created_utc": "2025-12-24 22:05:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvriz80",
              "author": "Mr_Again",
              "text": "If airflow engine ever gets rewritten in rust I think that'll be a paid service. Isn't dbt-fusion trying essentially this now? Rewrite the engine of a popular python oss tool.",
              "score": 1,
              "created_utc": "2025-12-24 19:31:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvr44qh",
          "author": "Certain_Leader9946",
          "text": "I'm using Golang.",
          "score": 9,
          "created_utc": "2025-12-24 18:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvquorz",
          "author": "ssinchenko",
          "text": "\\> Any DE using Rust as their second or third language?\n\nI'm using it mostly for writing PySpark UDFs in my daily job. Third language (after Python and Scala).\n\n\\> Did you enjoy it?\n\nOverall I do. But it may be annoying from time to time. Especially arrow-rs I'm working with mostly. I don't know, maybe I'm just using it wrong, but sometimes it so boring to write endless boilerplate \\`ok\\_or\\`, \\`as\\_any\\`, \\`downcast\\_ref::<...>\\`, etc. for any piece of data you want to process...\n\n\\> Worth learning for someone after learning the fundamental skills for data engineering?\n\nImo learning by doing is the best way. Try to contribute something to Apache Datafusion Comet (or even to an upstream Apache Datafusion). There were a lot of small tickets and good first issues last time I checked. A lot of people around are saying that Datafusion is the future of ETL, understanding it's internals looks like a valuable skill!",
          "score": 6,
          "created_utc": "2025-12-24 17:19:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvud39f",
              "author": "Ok-Career-8761",
              "text": "what is datafusion?",
              "score": 2,
              "created_utc": "2025-12-25 08:07:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuxmwf",
                  "author": "ssinchenko",
                  "text": "[https://datafusion.apache.org/](https://datafusion.apache.org/)",
                  "score": 1,
                  "created_utc": "2025-12-25 11:51:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvrb38y",
          "author": "RunOrdinary8000",
          "text": "The rust library has all you need for batch pipelines in rust. I have only experience with the python bindings. But I can recommend it.",
          "score": 6,
          "created_utc": "2025-12-24 18:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrprn9",
              "author": "RunOrdinary8000",
              "text": "The library is called Polars. Sorry forgot to mention.",
              "score": 3,
              "created_utc": "2025-12-24 20:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrerrp",
          "author": "PurepointDog",
          "text": "Rust is great for tooling and tooling extensions (UDF-style). Polars is fantastic. The wealth of Polars extensions is also great! \n\nI have yet to write one myself, but it honestly looks pretty straightforward if the time ever comes where it makes sense to implement myself.",
          "score": 5,
          "created_utc": "2025-12-24 19:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs6pt1",
              "author": "skatastic57",
              "text": "Polars was my gateway drug to dabbling in rust. Check out this list for examples of other extensions\n\nhttps://github.com/ddotta/awesome-polars",
              "score": 3,
              "created_utc": "2025-12-24 21:52:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvr1864",
          "author": "Nemeczekes",
          "text": "It depends. Like the python was always a goat but the performance was actually from C/C++ under the hood.\n\nSo if you want to write tools to be used in DE the rust will be great. But if you do the DE itself then there is no much difference",
          "score": 4,
          "created_utc": "2025-12-24 17:54:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrp07t",
          "author": "ludflu",
          "text": "for most things, no, Rust is not the first tool I reach for. But once in a while, there's a task that's just perfect. \n\nThis summer I had to build a ingest pipeline that parses gigantic 50 GB json files (_not_ JSONL). Using Spark wouldn't make any sense- it's a single non-splittable file so you would get no parallelism. \n\nI wrote a Rust program to do streaming parsing, unnest a bunch of crazy shit and then write it out to parquet for further processing in BigQuery. \n\nRust was exactly the right tool, and the job is both faster and cheaper than anything I could have accomplished conventionally.",
          "score": 4,
          "created_utc": "2025-12-24 20:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvr8dc0",
          "author": "Dependent-Yam-9422",
          "text": "In my opinion, the main issue with Rust in DE is that there aren’t a ton of libraries out there that support distributed processing. There is a bigger community of tools out there for single-node processing in Rust so for those types of workloads it’s more doable.\n\nI personally find that the claims of Rust being super difficult to learn are overblown if you have any sort of CS background. In many ways I think it’s easier to write multithreaded applications in Rust than it is for a lot of other languages",
          "score": 3,
          "created_utc": "2025-12-24 18:33:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqmbna",
          "author": "markojov78",
          "text": "as a backend and currently data engineer, I started learning rust because of the new paradigm of memory management which I was curious about, but I simply could not find a good use case for it\n\nI think I understand what they call it \"high friction language\" because garbage collector languages ​​really get the job done and you still need a very good reason and extra time to write code in something else, rust is not a magic replacement for any of it.\n\nIt's good learning experience tho",
          "score": 5,
          "created_utc": "2025-12-24 16:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrn6le",
          "author": "Thlvg",
          "text": "Well there's using Rust and using Rust... Polars, uv, ruff and now ty are fantastic Python tools built in Rust (polars is a great replacement for pandas...). So there's that...",
          "score": 2,
          "created_utc": "2025-12-24 19:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrtgt1",
          "author": "xmBQWugdxjaA",
          "text": "We tried it out at my job, Ballista is cool but there's no general support like for Dataflow etc. so it wasn't worth the extra effort overall.",
          "score": 2,
          "created_utc": "2025-12-24 20:32:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs0dav",
          "author": "psypous",
          "text": "https://www.reddit.com/r/dataengineering/s/i7OXPgi07N",
          "score": 2,
          "created_utc": "2025-12-24 21:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs8ljz",
          "author": "WhipsAndMarkovChains",
          "text": "If you’re a Databricks user looking to find an excuse to use Rust somewhere you can check out the Rust SDK for Zerobus. https://github.com/databricks/zerobus-sdk-rs\n\nhttps://www.databricks.com/blog/announcing-public-preview-zerobus-ingest",
          "score": 2,
          "created_utc": "2025-12-24 22:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsl4rh",
          "author": "UltraPoci",
          "text": "This makes me wonder: are there data orchestrators for Rust?",
          "score": 2,
          "created_utc": "2025-12-24 23:26:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsvn0z",
          "author": "NoleMercy05",
          "text": "Do any of the major vendors have tooling support for Rust? Things change so fast I'm not sure but I'm used to seeing primarily Python. (airflow etc)",
          "score": 2,
          "created_utc": "2025-12-25 00:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu7x18",
          "author": "Used-Assistance-9548",
          "text": "We use it to extend datafusion",
          "score": 2,
          "created_utc": "2025-12-25 07:13:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv0pb9",
          "author": "cokeapm",
          "text": "Once I got a lot of performance benefits by using a rust library for processing H3 (geographical index). It was wrapped in python and it worked very well.",
          "score": 2,
          "created_utc": "2025-12-25 12:21:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwfbkm",
          "author": "peterxsyd",
          "text": "I think it holds great potential.",
          "score": 2,
          "created_utc": "2025-12-25 18:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzwuhy",
          "author": "mr_nanginator",
          "text": "Sounds like a total waste of your employer's time + money. You should use 4GLs like Python for things outside the database, and ideally get things into the database as early as possible in the pipeline so you can do all other transformations inside the database.\n\nI'm sure there are Rust people out there who will disagree, but the fact is that Rust is \\*not\\* a common skill for data engineers, and just because you \\*can\\* do something, it doesn't mean that you \\*should\\*.",
          "score": 2,
          "created_utc": "2025-12-26 08:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuwx2s",
          "author": "Ok-Sprinkles9231",
          "text": "I use [just](https://just.systems/) heavily, not necessarily DE, but maybe more like tooling. I plan to use Polars as well. All in all, for DE, it's more common to use libraries that are written in Rust rather than using Rust directly as a language. \n\nImo the situation is more or less similar to C++",
          "score": 1,
          "created_utc": "2025-12-25 11:43:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwfvlj",
          "author": "peterxsyd",
          "text": "if interested, I built this as a base data layer for Rust, aimed at improving ergonomics. \n\nIt plugs into a live streaming context with Rust's tokio, talks Parquet and Arrow files via crates that I built, as well as has '.to\\_polars()' and '.to\\_arrow()'. If you are interested in more bare bones data engineering with minimal abstractions in Rust you can do quite a lot with it.\n\n[https://github.com/pbower/minarrow](https://github.com/pbower/minarrow)",
          "score": 1,
          "created_utc": "2025-12-25 18:06:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqie4b",
          "author": "Nekobul",
          "text": "Useless in DE just as C/C++ is useless for the same reasons. Now, if you are coding OS, then it does make sense.",
          "score": 2,
          "created_utc": "2025-12-24 16:13:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqilg7",
              "author": "otto_0805",
              "text": "I will just go with Java then",
              "score": 1,
              "created_utc": "2025-12-24 16:14:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvqkw6g",
                  "author": "Embarrassed_Box606",
                  "text": "Do scala instead :)",
                  "score": 4,
                  "created_utc": "2025-12-24 16:26:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqgp0c",
          "author": "Zer0designs",
          "text": "Yes, mostly for fun! Read the Rust book, it's great.",
          "score": 1,
          "created_utc": "2025-12-24 16:04:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqngh1",
          "author": "CrowdGoesWildWoooo",
          "text": "Unless you are like 8-9/10 with your rust skill it’s unlikely to be helpful for work. This is assuming your DE work is mostly building pipelines. \n\nBelow that level you are just going to reinvent wheels but likely end up with a crappier one.\n\nHowever, if you start learning lower level language you’ll probably appreciate DSA topics more and that will certainly helps you down the line as long as you are doing coding heavy work.",
          "score": 1,
          "created_utc": "2025-12-24 16:40:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvr4w1h",
          "author": "No_Soy_Colosio",
          "text": "Try to look at what is being used in the industry. Sure you could do all your DE tasks in Rust, but you'd be hard-pressed to find libraries to make your life easier.\n\nMost Python data-related libraries utilize a lower-level language (like numpy) to provide speed.\n\nYou also have to think that you'll often have to work with other people, and having them have to learn Rust to maintain your systems is just too much in my opinion. Most DEs you find are perfectly fluent in Python.",
          "score": 1,
          "created_utc": "2025-12-24 18:14:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqnbwa",
          "author": "wallyflops",
          "text": "As much as I love Rust as a hobby it doesn't have much place in modern DE stack, I'd imagine Go does though in some of the CI over Python for speed though. I expect it to grow but it's a 'useless' skill in terms of it's unlikely to boost your salary.",
          "score": 0,
          "created_utc": "2025-12-24 16:39:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqiu01",
          "author": "No_Flounder_1155",
          "text": "Everyone saying rust isn't great/ needed is a script kiddie and can't program.",
          "score": -7,
          "created_utc": "2025-12-24 16:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqjb1i",
              "author": "Nekobul",
              "text": "Start programming in assembly. You will be even greater.",
              "score": 7,
              "created_utc": "2025-12-24 16:18:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqkjpx",
                  "author": "Reach_Reclaimer",
                  "text": "Frankly if you can't do it in machine code, you're just a script kiddie",
                  "score": 6,
                  "created_utc": "2025-12-24 16:24:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1py48h7",
      "title": "How do you explore a large database you didn’t design (no docs, hundreds of tables)?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1py48h7/how_do_you_explore_a_large_database_you_didnt/",
      "author": "Technical_Safety4503",
      "created_utc": "2025-12-28 22:16:39",
      "score": 50,
      "num_comments": 47,
      "upvote_ratio": 0.87,
      "text": "I often have to make sense of **large databases with little or no documentation**.  \nI didn’t find a tool that really helps me explore them step by step — figuring out **which tables matter** and **how they connect** in order to answer actual questions.\n\nSo I put together a small prototype to visually explore database schemas:\n\n* load a schema and get an interactive ERD\n* search across **table and column names**\n* select a few tables and automatically **reveal how they’re connected**\n\n  \n**GIF below (AirportDB example)**\n\nhttps://i.redd.it/yklm55oeq0ag1.gif\n\nBefore building this further, I’m curious:\n\n* **Do you run into this problem as well?** If so, what’s the most frustrating part for you?\n* **How do you currently explore unfamiliar databases?** Am I missing an existing tool that already does this well?\n\nHappy to learn from others — I’m doing this as a starter / hobby project and mainly trying to validate the idea.\n\nPS: this is my first reddit post, be gentle :)",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1py48h7/how_do_you_explore_a_large_database_you_didnt/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwfug5n",
          "author": "AutoModerator",
          "text": "You can find our open-source project showcase here: https://dataengineering.wiki/Community/Projects\n\nIf you would like your project to be featured, submit it here: https://airtable.com/appDgaRSGl09yvjFj/pagmImKixEISPcGQz/form\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-28 22:16:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfwqqu",
          "author": "git0ffmylawnm8",
          "text": "Start exploring critical dashboards and reports to understand what metrics are delivered and the logic used. Then start branching out to different tables and understand how they're built out",
          "score": 69,
          "created_utc": "2025-12-28 22:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh20yl",
              "author": "West_Good_5961",
              "text": "Second this. I pick a dashboard and work backwards, looking at dependent tables.",
              "score": 8,
              "created_utc": "2025-12-29 02:13:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwk9c4m",
                  "author": "El_Kikko",
                  "text": "There are people who don't do this?",
                  "score": 1,
                  "created_utc": "2025-12-29 15:55:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwlff6i",
              "author": "Mo_Steins_Ghost",
              "text": "This, and/or as a proof try to replicate all the key metrics... systematically starting from tying out grand totals to breaking it down and tying out cohorts, segments, etc.\n\nNow, mind you, this is mostly for cursory exploration BUT you should really be talking to database and product owners in an org to understand all the idiosyncrasies.  Our acquisition of another company became an eight month project of mapping out multiple source systems and not just existing databases.\n\nIt gets even more critical if you are serving executive level dashboards and financial data that become adopted as \"gold standard\".  Arguably, if you are a business analyst, you should not be touching financial data... you don't know revenue recognition rules.  But, there are companies that task non FP&A people with FP&A like responsibilities all the time and it's insane.",
              "score": 1,
              "created_utc": "2025-12-29 19:12:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwg1695",
              "author": "Technical_Safety4503",
              "text": "Seems logical: learning from what already exists from a high value business perspective. \nDo you use SQL, DBeaver, PGadmin, ... or others to do the exploration or? Or is it the database/ecosystem that dictates the tool you use?",
              "score": -1,
              "created_utc": "2025-12-28 22:51:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwg29qv",
                  "author": "git0ffmylawnm8",
                  "text": "I raw dog the SQL scripts",
                  "score": 34,
                  "created_utc": "2025-12-28 22:57:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgucgg",
          "author": "No_Flounder_1155",
          "text": "truncate every table and see what parts of the business complain.",
          "score": 62,
          "created_utc": "2025-12-29 01:29:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgvysw",
              "author": "codykonior",
              "text": "Shit on the keyboard to establish dominance.",
              "score": 16,
              "created_utc": "2025-12-29 01:38:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhre3u",
              "author": "RBeck",
              "text": "That's crazy, just rename one table at a time and see what barfs.",
              "score": 9,
              "created_utc": "2025-12-29 04:45:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwmgs2e",
                  "author": "No_Flounder_1155",
                  "text": "no, just gaslight them and tell them it must be a failure with __their__ work.",
                  "score": 3,
                  "created_utc": "2025-12-29 22:14:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwimluo",
              "author": "VipeholmsCola",
              "text": "Can you hear them yell from different locations as you go?",
              "score": 2,
              "created_utc": "2025-12-29 09:05:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfzmeu",
          "author": "ColdStorage256",
          "text": "I need this.\n\n\n\n\nI have been asked to work with a db with no docs, hundreds of tables, and the BI team is useless and won't let me know what they're doing to produce reports.\n\n\n\n\nI work in enterprise though so my question is, how does this tool work? Is it open source and self hosted? Is it totally local server? ",
          "score": 9,
          "created_utc": "2025-12-28 22:43:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi5dvo",
              "author": "chock-a-block",
              "text": "Dbeaver. ",
              "score": 2,
              "created_utc": "2025-12-29 06:30:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwg4zs9",
              "author": "Technical_Safety4503",
              "text": "That situation is exactly what triggered this project.  \nRight now it’s a very early prototype, not a product yet.\n\n**How it works today:**\n\n* Runs fully **locally** (SQLAlchemy for schema extraction, FastAPI + WebSockets for serving, JavaScript/Cytoscape for visualization), all packaged in Docker\n* You point it at a database → it extracts tables, columns, and primary/foreign keys\n* You can then **search across tables and columns** and visually reveal how selected tables connect\n* No cloud, no external services — **no data leaves your environment**\n\nI built this specifically with **enterprise constraints** in mind: no documentation, siloed BI teams, restricted access.\n\nAt the moment it’s:\n\n* not packaged\n* not polished\n* not open-source yet\n\nI wanted to validate first that others actually run into this problem before investing more time.\n\nIf this is something you’d realistically use:\n\n* **What database(s) are you on?**\n* **Would** ***read-only + local/self-hosted*** **be a hard requirement for you?**",
              "score": 3,
              "created_utc": "2025-12-28 23:11:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwg7tbs",
                  "author": "ColdStorage256",
                  "text": "I work in a regulated industry. Fully local would be hard requirement whether I get permission to use a tool, or whether there was an enterprise license involved.\n\n\n\n\nOne issue I have in particular is that keys have different column names across different tables. Is there any way to deal with that? \n\n\n\n\nIt's an absolute joke for me trying to put data together haha",
                  "score": 3,
                  "created_utc": "2025-12-28 23:27:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhnmjo",
                  "author": "Chobo1972",
                  "text": "If and when you are ready to share count me in.  Currently in an ancient SSRS environment within a 30 year old company.  So many tables (most old and broken) and so little time or documentation to know what is still usable.",
                  "score": 1,
                  "created_utc": "2025-12-29 04:21:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgd5ah",
          "author": "Striking_Meringue328",
          "text": "I generally start with the query logs",
          "score": 9,
          "created_utc": "2025-12-28 23:55:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgdmu8",
              "author": "Technical_Safety4503",
              "text": "ooeeh, that one I didn't see coming, good one! So you scan through to see which tables/columns get querried the most frequent and start your exploration from there?",
              "score": 2,
              "created_utc": "2025-12-28 23:58:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwiyf2b",
                  "author": "Thanael124",
                  "text": "You also see how the tables are joined in queries.",
                  "score": 2,
                  "created_utc": "2025-12-29 10:55:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgu30v",
              "author": "AZjackgrows",
              "text": "And the reverse of this- how often tables are being updated by their sources.",
              "score": 1,
              "created_utc": "2025-12-29 01:27:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh94dq",
          "author": "streetrider_sydney",
          "text": "Dive into system views to unravel which routines and views are most utilised. Take top 10 and explore the tables referred in them. Once you are done with them, explore next 10 and so forth. Also, check for table prefixes or nomenclature standards to see if there are types of tables - such as reference, lookup and user tables.",
          "score": 3,
          "created_utc": "2025-12-29 02:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh7k82",
          "author": "idodatamodels",
          "text": "Yes, Erwin will do this. Did you ever look at that tool?",
          "score": 6,
          "created_utc": "2025-12-29 02:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgrjut",
          "author": "robberviet",
          "text": "Starts from both start (data sources), and end (end users needs). Just focus on what is needed first for quick iterations.",
          "score": 2,
          "created_utc": "2025-12-29 01:13:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhzc05",
          "author": "mweirath",
          "text": "I use red gate’s sql toolkit. Their SQL Doc tool does a lot of this for most DBs.",
          "score": 2,
          "created_utc": "2025-12-29 05:41:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi5g4h",
          "author": "chock-a-block",
          "text": "GitHub link?",
          "score": 2,
          "created_utc": "2025-12-29 06:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwifnug",
          "author": "sjcuthbertson",
          "text": "Generally when I have this problem, there are no foreign keys declared either, so figuring out the relationships is more manual. Looks like your tool wouldn't help much with no fks?",
          "score": 2,
          "created_utc": "2025-12-29 08:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnchpa",
              "author": "Technical_Safety4503",
              "text": "You’re right — in its current form, it wouldn’t help much without declared relationships.  \nAutomated relationship detection is something I’m considering, but it’s non-trivial and easy to get wrong at scale.  \nFor now I’m validating the FK-based workflow first before expanding into heuristics.",
              "score": 1,
              "created_utc": "2025-12-30 01:04:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwp654w",
                  "author": "sjcuthbertson",
                  "text": ">Automated relationship detection\n\nYeah I probably wouldn't bother to be honest. There's no way you'd ever be able to work out programmatically that creator_id joins to resource_id, without also generating a lot of false positive relationships. I can't imagine LLMs being any better than traditional approaches at this, either.",
                  "score": 1,
                  "created_utc": "2025-12-30 08:25:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwlcfku",
          "author": "justanothersnek",
          "text": "For me, the methodical way: find SMEs from both the business side and technical side/DB team.  Get them all in a room and just hammer out the details.  Did they have wikis?  Confluence pages?  Etc.\n\n\n\n\nIf you have no such SMEs...youre F'ed.  Slightly kidding, but yeah then you can resort to some tools within the database system or external tools.  But even still, you'll maybe run into some weird esoteric business logic that is randomly there like with CASE WHEN statements.  So you can reverse engineer all you want, something can still bite you in the ass and still be F'ed.",
          "score": 2,
          "created_utc": "2025-12-29 18:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgurgz",
          "author": "cmcclu5",
          "text": "[Here’s](https://horizonanalytic.com/landing/packages/horizon-schema) something that does something similar. Loads up all the tables, visually shows relationships between tables using established keys, and lets you inspect column data types.",
          "score": 1,
          "created_utc": "2025-12-29 01:31:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhbi1f",
          "author": "AcanthaceaeOpposite",
          "text": "/RemindMe 2 weeks",
          "score": 1,
          "created_utc": "2025-12-29 03:07:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhdd2d",
          "author": "Sudden_Beginning_597",
          "text": "there are some open source auto eda tools like rath, that can automatelly discover data views/ charts with potential insights for you, useful for those large dataset which you have no idea where to start, but it consumer huge amount of resources for computation.",
          "score": 1,
          "created_utc": "2025-12-29 03:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjgff6",
          "author": "SomeDayIWi11",
          "text": "Commenting so that I can come back later",
          "score": 1,
          "created_utc": "2025-12-29 13:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwncmdf",
              "author": "Technical_Safety4503",
              "text": "I’ll give an update after doing some tweaks",
              "score": 1,
              "created_utc": "2025-12-30 01:05:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwk5pjm",
          "author": "empireofadhd",
          "text": "Find transaction tables and rebuild the model from there. Eg entries, purchases etc. They are the spinal chord of databases and the resin they exist most of the time.",
          "score": 1,
          "created_utc": "2025-12-29 15:37:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxu6ts",
      "title": "Are we too deep into Snowflake?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pxu6ts/are_we_too_deep_into_snowflake/",
      "author": "stuckplayingLoL",
      "created_utc": "2025-12-28 15:36:35",
      "score": 48,
      "num_comments": 35,
      "upvote_ratio": 0.95,
      "text": "My team uses Snowflake for majority of transformations and prepping data for our customers to use. We sort of have a medallion architecture going that is solely within Snowflake. I wonder if we are too vested into Snowflake and would like to understand pros/cons from the community. The majority of the processing and transformations are done in Snowflake. I anticipate we deal with 5TB of data when we add up all the raw sources we pull today.\n\nQuick overview of inputs/outputs:\n\nEL with minor transformations like appending a timestamp or converting from csv to json. This is done with AWS Fargate running a batch job daily and pulling from the raw sources. Data is written to raw tables within a schema in Snowflake, dedicated to be the 'stage'. But we aren't using internal or external stages.\n\nWhen it hits the raw tables, we call it Bronze. We use Snowflake streams and tasks to ingest and process data into Silver tables. Task has logic to do transformations.\n\nFrom there, we generate Snowflake views scoped to our customers. Generally views are created to meet usecases or limit the access.\n\nMajority of our customers are BI users that use either tableau or power bi. We have some app teams that pull from us but not as common as BI teams.\n\nI have seen teams not use any snowflake features and just handle all transformations outside of snowflake. But idk if I can truly do a medallion architecture model if not all stages of data sit in Snowflake. \n\nCost is probably an obvious concern. Wonder if alternatives will generate more savings.\n\nThanks in advance and curious to see responses.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pxu6ts/are_we_too_deep_into_snowflake/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwdm926",
          "author": "mamaBiskothu",
          "text": "This is no different from choosing between on prem and aws. Unless youre dealing with petabytes of data and millions in snowflake bills, it makes sense to abstract away the infrastructure part of data engineering to snowflake. Your DE team is likely half or less the size it woild need to be if you leave snowflake.",
          "score": 46,
          "created_utc": "2025-12-28 15:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweyw25",
              "author": "stuckplayingLoL",
              "text": "I think you summed it up pretty well for us. My team is not very experienced with the infrastructure aspect of AWS and really leans on 1 engineer to keep the infrastructure afloat. Thanks for raising that point.",
              "score": 6,
              "created_utc": "2025-12-28 19:41:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdovon",
          "author": "Pittypuppyparty",
          "text": "There’s a trade off. Can you save money refactoring pipelines away from snowflake? Sure. But you could probably just refactor in snowflake and save a ton of money. I think people underestimate how much snowflake does under the hood to make things work seamlessly and quickly. In my experience the added overhead of other services wasn’t worth the savings and refactoring on snowflake gave the best bang for my buck.",
          "score": 11,
          "created_utc": "2025-12-28 15:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdmxhf",
          "author": "konwiddak",
          "text": "My rule for software value is - If Snowflake pricing doubled overnight, would it be a showstopper? If it generates enough business value that you'd carry on (perhaps putting in place a migration plan) - then it's probably fine as a choice. If you're so cost sensitive that a small rise in costs would cause problems, then it's probably not the right choice.\n\nGenerally being heavily invested into Snowflake doesn't concern me too much. Most of the transformation logic is SQL. Unlike say python, or a lot of proprietary solutions SQL code written today will still be good in 20 years time. There aren't any libraries to end up unmaintained. If you *really* need to you can port to another database technology and most of your transformations will port over perfectly. You might need to spin up a solution for tasks, and change your ingestion methodology, but all that business logic will be fine still.\n\nUnless you go completely open source, and want to take on board the substantial overhead with doing that, I'd say it's pretty good value. Vendor lock in mainly comes from the fact that it's really good, rather than vendor lock in from being proprietary. Very little of it is highly proprietary unlike a lot of other ETL methodologies.\n\nWhy aren't you at least using internal stages? There's far less overhead in using COPY INTO vs INSERT unless you're inserting a trivial amount of data. External stages and pipes even less overhead and cost.",
          "score": 16,
          "created_utc": "2025-12-28 15:49:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwexosb",
              "author": "stuckplayingLoL",
              "text": "I feel like cost isn't a concern yet but at the rate that we are going, we could be scaling to higher usage and thus the conversation with cost could come up.\n\nWe are not using internal stages only because previous engineers on the team resorted to using Python write_pandas and prayed that the auto generated tables did not cause issues down the road. It's absolutely tech debt due to us running into type issues. It will be something that I will look into though, thanks!",
              "score": 1,
              "created_utc": "2025-12-28 19:36:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfv2vx",
                  "author": "kudika",
                  "text": "`write_pandas()` uses temporary internal stages btw",
                  "score": 3,
                  "created_utc": "2025-12-28 22:19:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhqlgp",
                  "author": "Choperello",
                  "text": "95% likelyhood you can cut your snowflake bill just by optimizing how you’re using it.",
                  "score": 2,
                  "created_utc": "2025-12-29 04:40:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwe47m3",
          "author": "goblueioe42",
          "text": "I have interviewed with some snowflake teams that found my non snowflake experience not helpful or they only zeroed in on snowflake and exclude perfectly good non snowflake experience. I think you are in too deep if someone without recent ( let’s say last year) snowflake experience can’t join the team easily. That’s the worry is if you exclude the talent pool too much. It doesn’t make a fun interview if the interviewers only know snowflake and no other way to approach a problem.",
          "score": 6,
          "created_utc": "2025-12-28 17:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf1a1j",
              "author": "stuckplayingLoL",
              "text": "Good perspective. I do feel like most of the complex portion of the code is within the Snowflake tasks, but the general pattern from ingesting raw data to making customer ready datasets is consistent. I don't think junior engineers could take a look at the overall architecture and understand how their day to day work fits in the model without some mentorship. But I assume that's just how it goes with data engineering.",
              "score": 2,
              "created_utc": "2025-12-28 19:53:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwf7apx",
                  "author": "goblueioe42",
                  "text": "That’s fair. As long as you can say yes I would use airflow or tasks. Or we could use snow pipe or flink or spark streaming. I see what you mean. The only worry is lockout of great candidates. I think that perspective is fair",
                  "score": 2,
                  "created_utc": "2025-12-28 20:22:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgt6pu",
          "author": "sib_n",
          "text": "Having most of your transformations coded as SQL is a good thing. SQL is the most stable tech in data for the past 30 years, so you should be able to easily port it to another SQL engine in the future if needed.  \nI think your stronger dependency, here, is using Snowflake as an orchestrator. You could move this to something open source like Airflow, Dagster or Prefect. But if it's currently working well for you and your budget is in control, don't change it just for the sake of it, wait for a rational reason.",
          "score": 4,
          "created_utc": "2025-12-29 01:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe84zq",
          "author": "Hofi2010",
          "text": "So you can call you architecture medallion even if the bronze, silver and gold outside your data warehouse. \n\nA lot of teams migrating transformation outside of snowflake or redshift or GBC. We use iceberg tables stored in s3 and use Fargate or EC2 to transform the data. Then we map the gold layer into redshift. You can do the same with snowflake. For performance probably better to ingest gold layer into snowflake.\n\nFor transformation we use duckdb on an EC2 with dbt. If you already use DBT this would be an easy shift.\n\nHere a medium article where I describe the basic principle \nhttps://medium.com/@klaushofenbitzer/save-up-to-90-on-your-data-warehouse-lakehouse-with-an-in-process-database-duckdb-63892e76676e\n\nThis architecture is as fast or even faster than snowflake for about 10% of the cost",
          "score": 5,
          "created_utc": "2025-12-28 17:35:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf1rm3",
              "author": "stuckplayingLoL",
              "text": "Thanks for this thought. I did see that some teams at my company were shifting to iceberg tables and using Airflow with AWS but wasn't sure how mature their processes were. I'll look into this topic and see how it changes things, because it sounds like a huge architectural shift in the long run.",
              "score": 1,
              "created_utc": "2025-12-28 19:55:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfi34z",
                  "author": "Hofi2010",
                  "text": "Depends on if you are optimizing for cost or ease of use.",
                  "score": 2,
                  "created_utc": "2025-12-28 21:15:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwe4so1",
          "author": "chock-a-block",
          "text": "Not according to c-level who have to listen to the whims of the BOD. \n\nThe c-level pissing contents about tech are pretty comical. ",
          "score": 2,
          "created_utc": "2025-12-28 17:19:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwewuck",
              "author": "M4A1SD__",
              "text": "BOD?",
              "score": 1,
              "created_utc": "2025-12-28 19:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwf05rg",
                  "author": "GreyHairedDWGuy",
                  "text": "board of directors (I think)",
                  "score": 3,
                  "created_utc": "2025-12-28 19:48:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf9hn6",
          "author": "Think-Trouble623",
          "text": "It seems to me that you’re facing some of the same questions I have about moving from Azure SQL to Snowflake. Things are going well, pipelines are stable, and costs are relatively managed. \n\nThe question always comes down to headcounts. Can you reduce headcount with an architecture shift or significantly reduce op ex? Rarely are you going to see enough value generated by an architecture shift; unless you aren’t getting real time data and need it. \n\nMy suggestion is to pick an “up and coming” jr engineer to refractor a pipeline in snowflake and recreate it in another architecture. See what the time commitment and savings are for both, then use it to fund your next pipeline until you have a clear path forward.",
          "score": 2,
          "created_utc": "2025-12-28 20:33:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg4ky6",
          "author": "MonochromeDinosaur",
          "text": "As long as snowflake yearly cost is less than a full engineer’s salary it’s pretty cheap IMO. \n\nI’ve defaulted to just using self-hosted airbyte + dbt + snowflake/postgres (depending on data size and requirements) unless I have a good reason not to.\n\nIf you do everything in airbyte + dbt a migration is literally as easy as pointing your airbyte to the new database and then compiling your models and running them to see what breaks and correcting them.\n\nThis works across all databases too it’d be just as easy to go to bigquery or others. If you set the rule for your job to stick to ANSI SQL or only use dbt utilities that support multiple adapters.\n\nThis is as someone with 9 YOE who’s done on-prem Hadoop, Spark (self rolled AWS EMR ephemeral and persisten), Databricks, bespoke frameworks, etc. \n\nThere’s really no platform as ergonomic as snowflake (BQ being a close second) + dbt + standardized ingestion tool IMO. \n\nI honestly avoid companies that don’t have a stack like this or aren’t willing to migrate nowadays. I don’t want to spend my days debugging over-engineered bespoke framework garbage/spark jobs unless I’m getting paid really well to do it.",
          "score": 2,
          "created_utc": "2025-12-28 23:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf69zc",
          "author": "Bluefoxcrush",
          "text": "Do you use version control?",
          "score": 1,
          "created_utc": "2025-12-28 20:17:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf8ssa",
              "author": "stuckplayingLoL",
              "text": "Yes. We use Github and deploy changes with GitHub Actions.",
              "score": 1,
              "created_utc": "2025-12-28 20:30:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwmp4xp",
          "author": "PossibilityRegular21",
          "text": "On one hand, I've done a bit of this before, and snowflake without VCS can get messy to manage at scale.\n\n\nOn the other hand, I love how simple snowflake is and i'm jealous of your solution. We use DBT and it's *fine* but I also miss the simplicity of tasks.",
          "score": 1,
          "created_utc": "2025-12-29 22:57:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwtx2j",
          "author": "Responsible_Act4032",
          "text": "Don't rely on the opinion of others on here. \n\nWhat I would say is that ALL technology will age out, as new advancements arrive. \n\nThe key goal of any organisation is to avoid and ensure they aren't impacted in the future by cost rises or vendor lock in, or more critically, mean you miss out on another innovative solution. \n\nTake the effort, while with Snowflake, to migrate your data to Iceberg tables. \n\nOnce you've done that, you can stick iwth snowflake, or you can evaluate other more modern vendors. Whoever they are. \n\nFull transparency, I work for one of those vendors.",
          "score": 1,
          "created_utc": "2025-12-31 13:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdwm8c",
          "author": "eljefe6a",
          "text": "Do you have anyone on the team who knows how to program? This seems like a team who only knows SQL and so every task has to be done with SQL.",
          "score": 1,
          "created_utc": "2025-12-28 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweydcc",
              "author": "stuckplayingLoL",
              "text": "Yes. Most engineers know at least enough Python to write a basic ingest from raw to Snowflake. However, our code is all over the place as we do not have any formal organization. It was just write code to get it to work rather than thinking about reusability and classes.",
              "score": 1,
              "created_utc": "2025-12-28 19:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwf1tq7",
                  "author": "eljefe6a",
                  "text": "People who know just enough Python aren't enough. Your Snowflake spend is a function of having the wrong type of data engineers and short sighted management. You won't be able to fix this completely until you've fixed staffing issues first.",
                  "score": 1,
                  "created_utc": "2025-12-28 19:55:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwggke5",
                  "author": "Sublime-01",
                  "text": "Get claude code - have claude code refactor ur scripts",
                  "score": 1,
                  "created_utc": "2025-12-29 00:13:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdz60p",
          "author": "OtherwiseGroup3162",
          "text": "Do you mind if I ask around how much is your Snowflake costs? We have about 5TB of data, and people are pushing for snowflakes, but it is hard to determine the cost before jumping in.",
          "score": 1,
          "created_utc": "2025-12-28 16:51:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwezsjc",
              "author": "stuckplayingLoL",
              "text": "I don't know what our costs look like right now (away from work thanks to holidays) but can safely assume that majority of costs is in compute over storage. We are ramping up on more streams and tasks as we barely touched the surface of the raw data that we have already ingested. Hopefully someone has more of a concrete example.",
              "score": 1,
              "created_utc": "2025-12-28 19:46:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhqx45",
                  "author": "Choperello",
                  "text": "If you don’t know how your costs are then you can’t say costs are (or aren’t a concern). You’re guessing. Go see what your costs are before costs are a concern. The first rule of optimizing is measure before doing anything.",
                  "score": 1,
                  "created_utc": "2025-12-29 04:42:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwezvsj",
              "author": "GreyHairedDWGuy",
              "text": "Storage is cheap in Snowflake and almost a non-factor for 5TB.  Compute is here the cost comes from.  If you are doing a lot of processing inside Snowflake, that is where costs mainly come from.\n\nGet a trial of Snowflake as if possible and try it yourself.",
              "score": 1,
              "created_utc": "2025-12-28 19:46:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q05yz4",
      "title": "Snowflake or Databricks in terms of DE career",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q05yz4/snowflake_or_databricks_in_terms_of_de_career/",
      "author": "DryYesterday8000",
      "created_utc": "2025-12-31 06:23:05",
      "score": 48,
      "num_comments": 35,
      "upvote_ratio": 0.94,
      "text": "I am currently a Senior DE with 5+ years of experience working in Snowflake/Python/Airflow. In terms of career growth and prospects, does it make sense to continue building expertise in  Snowflake with all the new AI features they are releasing or invest time to learn databricks?\n\nCurrent employer is primarily a Snowflake shop. Although can get an opportunity to work on some one off projects in Databricks.\n\nLooking to get some inputs on what will be a good choice for career in the long run. ",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q05yz4/snowflake_or_databricks_in_terms_of_de_career/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwvgzg2",
          "author": "addictzz",
          "text": "I think ultimately they are just tools. You shouldnt have problems learning both. I feel both platform is equally competitive and evolving to be future proof.",
          "score": 47,
          "created_utc": "2025-12-31 06:33:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvo221",
              "author": "PrestigiousAnt3766",
              "text": "Agree 100%.\n\n\nThat said, i exclusively work with databricks but as long as it isnt fabric you should be fine.",
              "score": 19,
              "created_utc": "2025-12-31 07:34:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvp1sx",
                  "author": "addictzz",
                  "text": "The same as I. \n\nI don't want to pick on any platform but I must say compared to DB or Snow, Fabric may not be there yet in terms of maturity.",
                  "score": 2,
                  "created_utc": "2025-12-31 07:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwn2n4",
              "author": "toem033",
              "text": "This is just a half-baked truth. You shouldn't have problems learning both but companies when hiring only look for candidates with deep expertise already. My company, which is deep in Databricks, unflinchingly prefer those who have experience with the platform.",
              "score": 5,
              "created_utc": "2025-12-31 12:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwpkhe",
                  "author": "addictzz",
                  "text": "As an employer, I too would prefer somebody who is deep in a platform I use day to day. But technical skill is not the only factor in hiring and you dont always get a good pool of people highly adept in particular platform. \n\nBut then among these 2 platforms, who can say for sure which one will have the most market share in the future? Snowflake has gone public, but Databricks is gaining a lot of traction lately.",
                  "score": 6,
                  "created_utc": "2025-12-31 13:08:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvx647",
          "author": "NeedleworkerIcy4293",
          "text": "Build the foundations in the end if foundations are solid you should be able to pick up anything",
          "score": 8,
          "created_utc": "2025-12-31 09:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0tark",
          "author": "Tushar4fun",
          "text": "Go and work on basics of spark and how bigdata works.\n\nDBX & ❄️ are just platform to work upon. Get a hands on for the interview.\n\nI’m into DE since 2013 and mostly worked on pipelines using python/sql and later on moved to python/sql/bigdata. Of course, there were other tools like airflow, kubernetes, etc were there but I never worked on DBX.\n\nRight now, I’m working on DBX in my new organisation and they hired me for my DE knowledge.\n\nI’ve seen people know DBX but they dont have any idea how to structure a project.\n\nFor example - \n\nusing notebooks in prod - there are many cons of using them on prod like no modularisation, difficulty in code review,etc\n\nWriting everything in a single script - no use of DRY coding.\n\nMaybe I know these things because I’ve worked on end to end architecture including building of API services too.\n\nWork on end to end.",
          "score": 5,
          "created_utc": "2026-01-01 02:53:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1mxip",
              "author": "idiotlog",
              "text": "How is it that a notebook cannot be modular? They accept parameters, and can invoke libraries?",
              "score": 1,
              "created_utc": "2026-01-01 06:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1w4qp",
                  "author": "Tushar4fun",
                  "text": "You just cannot make wheel out of bunch of notebooks.\n\nPlus, %run to include a notebook with the whole path in each cell is not pythonic.\n\nApart from this, if you are printing outputs on notebooks there is a huge possibility that it will produce different output on different environments.\n\nWhen you commit it to repo, even though code is same in the cells it will show diffs.\n\nNotebooks are best for data analysis, EDA and for Data scientists. But it’s simply not pythonic.",
                  "score": 1,
                  "created_utc": "2026-01-01 08:17:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1pg67",
          "author": "dataflow_mapper",
          "text": "At your level it is less about picking a winner and more about avoiding being boxed in. Snowflake expertise will stay valuable, especially if you lean into data modeling, cost control, and platform architecture rather than just writing SQL. The AI features are interesting, but they are not a moat by themselves yet.\n\nDatabricks is worth learning enough to be fluent. Not because you need to switch stacks tomorrow, but because it stretches different muscles around Spark, distributed compute, and more engineering heavy pipelines. Even a few real projects is usually enough to make your profile read as “platform agnostic” instead of “Snowflake only.”\n\nIf your employer gives you legit Databricks work, take it. You do not need to abandon Snowflake. The strongest DE profiles right now can explain tradeoffs between the two and have scars from both.",
          "score": 3,
          "created_utc": "2026-01-01 07:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0m6em",
          "author": "crevicepounder3000",
          "text": "Databricks for sure and I say that as someone with 4+ years of Snowflake experience. The job market wants Databricks and Spark. Rightly or wrong, Snowflake is seen as too expensive relative to Databricks and people are increasingly focused on cost",
          "score": 7,
          "created_utc": "2026-01-01 02:06:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvqy58",
          "author": "afahrholz",
          "text": "both snowflake and databricks are solid paths and worth knowing, snowflake for warehousing sql workflows and databricks for heavy data pipelines ml learning both over time seems like a good long term play",
          "score": 6,
          "created_utc": "2025-12-31 08:01:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy486v",
          "author": "imcguyver",
          "text": "If you look at the history of snowflake & databricks, snowflake leans towards BI, databricks leans towards DS. But both 'databases' are now platforms with so many new features added over the years that they arguably look the same. Using one or the other then comes down to personal preference.",
          "score": 4,
          "created_utc": "2025-12-31 17:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzdmj7",
          "author": "BoringGuy0108",
          "text": "Be an expert in whichever your company uses, but know the pros and cons of each. Get databricks exposure if you can at your company, but I wouldn't be too concerned if you don't get the opportunity.",
          "score": 4,
          "created_utc": "2025-12-31 21:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzr1pt",
          "author": "goblueioe42",
          "text": "Either is fine. I am on the GCP but also have snowflake experience. I’m sure knowing any of these well is the best part",
          "score": 2,
          "created_utc": "2025-12-31 22:52:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0ou62",
          "author": "vfdfnfgmfvsege",
          "text": "doesn't matter. I've run the full gamut of tools in the data space and if I need to learn something new I just pick it up.",
          "score": 1,
          "created_utc": "2026-01-01 02:23:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvih38",
          "author": "Born-Pirate1349",
          "text": "I think Databricks, firstly it is the complete lakehouse engine. The current market trend is towards the lakehouse rather than the warehouse due to vendor lock-in in terms of data. Databricks is a very good learning platform for any DE, either in their early stages or during the exploration phase. \n\nWe all know that databricks is built on top of spark engine, which is the most basic tool for any DE should be known. So when you learn databricks you will learn spark as well and all its features. All the features like open table formats like delta , catalog layer like unity and its governance capabilities and all the streaming tools. Hence this is the best platform for any DE to learn more about DE.",
          "score": -8,
          "created_utc": "2025-12-31 06:45:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzulxj",
              "author": "Infinite_Bug_8063",
              "text": "I don’t know why you are getting downvoted, but what you said is true. But I feel like Microsoft Fabric is the new trend.",
              "score": 3,
              "created_utc": "2025-12-31 23:14:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx6mmrb",
                  "author": "Born-Pirate1349",
                  "text": "Even I agree that fabric is becoming a new trend. I think people are more obsessed with snowflake because of their better performance and features but people tend to forget about the cost. For any DE I feel like databricks is a very good option in terms of basics and even fabric is good.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:16:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxzmn4",
              "author": "verus54",
              "text": "I’ve been noticing the opposite trend. I see so many more snowflake jobs than data bricks. I’ve never used snowflake, but I have used databricks. I thought snowflake was cloud agnostic, making it not vendor-locked?",
              "score": 5,
              "created_utc": "2025-12-31 17:16:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx03gfv",
                  "author": "mike-manley",
                  "text": "Snowflake is cloud agnostic... its provisioned on any of the Big Three and a region of your choosing.",
                  "score": 1,
                  "created_utc": "2026-01-01 00:07:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwyx4gm",
          "author": "Rare_Decision276",
          "text": "Databricks bro because it’s a lakehouse platform and most widely used application. Snowflake is a data warehouse.",
          "score": -7,
          "created_utc": "2025-12-31 20:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx03jdw",
              "author": "mike-manley",
              "text": "The lines are way more blurred now.",
              "score": 2,
              "created_utc": "2026-01-01 00:08:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwznwxx",
          "author": "Nekobul",
          "text": "Both of these platforms don't support on-premises. For that major reason, neither of these two are good options.",
          "score": -5,
          "created_utc": "2025-12-31 22:34:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx03lpp",
              "author": "mike-manley",
              "text": "Bro, what?",
              "score": 3,
              "created_utc": "2026-01-01 00:08:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx08wch",
                  "author": "Nekobul",
                  "text": "What is not clear?",
                  "score": -2,
                  "created_utc": "2026-01-01 00:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvn8vy",
          "author": "latent_signalcraft",
          "text": "this is a good example of where automation shines on throughput but still hides some risk. i have seen similar setups work well until quality compliance or subtle factual drift starts compounding across dozens of posts. the real leverage usually comes when there is a clear review or sampling loop so humans periodically validate outputs instead of trusting volume alone. without that scaling just amplifies small errors very quickly.",
          "score": -7,
          "created_utc": "2025-12-31 07:27:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pywd3j",
      "title": "How are you using Databricks in your company?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pywd3j/how_are_you_using_databricks_in_your_company/",
      "author": "L3GOLAS234",
      "created_utc": "2025-12-29 19:56:58",
      "score": 40,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "Hello. I have many years of experience, but I've never worked with Databricks, and I'm planning to learn it on my own. I just signed up for the free edition and there are a ton of different menus for different features, so I was wondering how every company uses Databricks, to narrow the scope of what I need to learn.\n\nDo you mostly use it just as a Spark compute engine? And then trigger Databricks jobs from Airflow/other schedules? Or are other features actually useful? \n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pywd3j/how_are_you_using_databricks_in_your_company/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwn3zo1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 24,
          "created_utc": "2025-12-30 00:18:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nws9mqe",
              "author": "puzzleboi24680",
              "text": "Second this. Unity Catalog/easy analytics interface is what Databricks is good for. Then you pay their crazy expenses on everything that locks you into. There's progress on multi-compute abilities, but it's slow. Unity & plus some of their proprietary Spark features are absolutely the killer apps tho. But boy do you pay.",
              "score": 2,
              "created_utc": "2025-12-30 19:38:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwzgh1e",
              "author": "minneDomer",
              "text": "Commenter is a solutions architect for Databricks, based on their comment history - take this with a grain of salt. It’s a good platform but I’m never a fan of self promotion without disclosures, and this sub is particularly bad at it.",
              "score": 2,
              "created_utc": "2025-12-31 21:52:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoo42n",
          "author": "addictzz",
          "text": "Databricks is an all-in-one data platform.\n\nStart at the very origin of Databricks itself which is data engineering. You can build data pipelines in declarative (SQL) or procedural way (Python).\n\nThen once you start to accumulate some data, you can use the platform as data governance tool using Unity Catalog and control who can access what or check data lineage.\n\nWith sufficient amount of good quality data, you can start opening up the platform for data science team to let them train & manage ML models or create GenAI-powered application.\n\nYou can further open up the platform for your Data/BI analysts team for data analysis purpose. Databricks has this AI-powered coding assistant called Databricks Assistant which can help your data analysts with SQL or Python syntax should they need it.\n\nIf any of your software devs (or data scientist turned software engineer) are required to build web applications, there is Databricks Apps to let you build a web app based on popular frameworks like Flask/Streamlit or even NodeJS. Lakebase can also help if you need low-latency OLTP database for your web app.\n\nYou can also open up Databricks to your business users, business stakeholders, or C-level to let them ask questions about data using Genie or to let them view dashboards. If those business users accessing Databricks directly, you can minimize the array of menu down using an access control called Permissions. Or you can also integrate Databricks with BI platform of your choice like Tableau/PowerBI/Superset/Metabase.\n\nI think that covers quite a lot of use case across several teams already.",
          "score": 10,
          "created_utc": "2025-12-30 05:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlsrlu",
          "author": "LargeSale8354",
          "text": "Start with the basics. The fundamentals of what DataBricks is, what it's components are,  what their pros/cons are.\nTheir Data Analyst material goes into DataBricks SQL and what Unity Catalog is. It's good beginners stuff.\nAfter that take a look at the Data Engineer associate material. Again, Unity Catalogue features heavily in the curriculum. It will introduce basic data frame operations and configuration options.",
          "score": 5,
          "created_utc": "2025-12-29 20:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlpw6z",
          "author": "git0ffmylawnm8",
          "text": "On the data engineering side, lots of pipelines and jobs feeding into external tables for Snowflake consumption. \n\nI know data scientists also test out models in our Databricks instance",
          "score": 5,
          "created_utc": "2025-12-29 20:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlwywi",
              "author": "CozyNorth9",
              "text": "Is there something you are getting from Snowflake that you can't get in Databricks?  And I suppose the other way around, is there a need for Databricks when you have snowflake?  They both seem to solve the same problems.",
              "score": 17,
              "created_utc": "2025-12-29 20:37:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwm67bb",
          "author": "pantshee",
          "text": "We process lots of kafka flows to expose them as tables in unity catalog. Or some night batch treatments to send data to other applications. And we consume those data with powerbi or dataiku, mostly",
          "score": 1,
          "created_utc": "2025-12-29 21:22:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmnpl5",
          "author": "klubmo",
          "text": "I work for a medium sized consulting firm with many of our clients in the Fortune 500 using Databricks. All use the data engineering tools in some regard, but on the more mature end we’re using all the features. Lots of data science work (both traditional and GenAI), Apps + Lakebase, data governance, etc.",
          "score": 1,
          "created_utc": "2025-12-29 22:50:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoc3qj",
              "author": "SlappyBlunt777",
              "text": "I’m stuck in a medium sized manufacturing firm that doesn’t want to think about leaving MS SQL server. The issue is golden handcuffs at a 150k+ salary. I have an offer for a company willing to pay a bit less but is already on databricks. Is it worth taking one step back for 2 steps forward??",
              "score": 2,
              "created_utc": "2025-12-30 04:26:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwojy5i",
                  "author": "klubmo",
                  "text": "Tough to say depending on what you want from life, how the company treats you, and what future opportunities you see there.  \n\nDepending on skills and experience you can make that kind of money at lots of firms, although you are starting to get into the higher end of the range there.  That being said many of the best DEs I work with are over $200K between salary and bonus (and often potential for company stock), so don't let those gold handcuffs keep you from the life you want to live. \n\nWho knows, maybe a change in leadership at the company will eventually push you toward a Databricks or Snowflake type of platform. I see it all the time...we talk to companies like yours, they aren't interested in cloud...fast forward 3-5 years and everything changes due to a new CEO or VP.",
                  "score": 1,
                  "created_utc": "2025-12-30 05:18:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu690i",
          "author": "Unlucky_Data4569",
          "text": "We write code that moves data in s3. We only use it for etl. Queries in athena and snowflake",
          "score": 1,
          "created_utc": "2025-12-31 01:28:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv2xbb",
          "author": "Born-Pirate1349",
          "text": "Databricks is nothing but Spark but just a managed version and much more efficient one. We usually use databricks for our internal testing and exploring more features. I would recommend you to go through their certification which are \"Databricks certified data engineer associate\" and \"Databricks certified data engineer professional\". These are very good certifications in terms of understanding what databricks actually are. Bit of costly but worth it. Even Udemy has similar courses you can try it out.\n\nAlways start with understanding their infrastructure and architecture and start understanding one by one features in terms of data layout optimizations, table formats, catalog layer, governance layer.\nAlso try setting up each thing in databricks and by the side you will learn all the things.\n\nFrankly speaking I haven't taken any courses but I started exploring the features and did many pocs on each of them. \nAlso my suggestion is that don't try to scope it into your usecase, try out all things and think of it as a data engineering platform journey because you will learn many things over there so. Happy learning!!",
          "score": 1,
          "created_utc": "2025-12-31 04:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx59y8u",
          "author": "Dry_Professional8254",
          "text": "Same as you. I'm about to put my hands on Databricks in my work without having any previous experience with this tool.\n\n  \nI already know the fundamentals, and I've been studying Spark like how to improve queries, etc. I want to explore the Data Factory pipelines migration to Databricks, because you gain more flexibility leaving ADF to only orchestrate these migrated pipelines.",
          "score": 1,
          "created_utc": "2026-01-01 21:41:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxtsi9",
      "title": "Implementation of SCD type 2",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pxtsi9/implementation_of_scd_type_2/",
      "author": "Rare_Decision276",
      "created_utc": "2025-12-28 15:19:49",
      "score": 37,
      "num_comments": 61,
      "upvote_ratio": 0.85,
      "text": "Hi all, \n\nWant to know how you guys implement SCD type 2? Will you write code in PySpark or do in databricks?\n\nBecause in databricks we have lakeflow declarative pipelines there we can implement in much better way compare to traditional style of implementing?? \n\nWhich one you will follow?? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pxtsi9/implementation_of_scd_type_2/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwdq9vi",
          "author": "moshujsg",
          "text": "All you need to do is add a valid from and valid to columns and when the record exists, mark last one obsolete. Mark new one current. I dont understand what else there is to it? Why would you ever use python for thid instead of just sql?",
          "score": 40,
          "created_utc": "2025-12-28 16:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdr0f8",
              "author": "financialthrowaw2020",
              "text": "There are many ways to do this, but I agree, it's such an easy thing to do that sql is the answer 9 times out of 10",
              "score": 7,
              "created_utc": "2025-12-28 16:10:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwds6ju",
                  "author": "moshujsg",
                  "text": "Yeah, whatever technique you use it seems like just sql is the answer, its optimized for these things.",
                  "score": 3,
                  "created_utc": "2025-12-28 16:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe597r",
              "author": "Embarrassed-Falcon71",
              "text": "It’s not that simple in spark + delta. You’ll need a couple of lines",
              "score": 3,
              "created_utc": "2025-12-28 17:21:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe5r1p",
                  "author": "moshujsg",
                  "text": "The concept is still simple, maybe hard to write, but simple",
                  "score": 1,
                  "created_utc": "2025-12-28 17:23:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwforkx",
                  "author": "Budget-Minimum6040",
                  "text": "You will need at least a of couple of lines in every way. Writing a \"merge into\" is not a 1-liner.\n\nBe it pySpark, polars or SQL.",
                  "score": 1,
                  "created_utc": "2025-12-28 21:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe6dwv",
              "author": "kenfar",
              "text": "I've written python libraries and programs to do this a few times, and definitely prefer this to using SQL.  Some common features of these tools:\n\n   * optionally dedup data\n   * ignore some columns\n   * assign the keys - both keys for the subject as well as keys for the version of the subject\n   * transform the fields\n   * populate other fields - batch_id, extract_timestamp, load_timestamp, flags to reflect current or non-current rows, etc\n   * update the prior SCD row to reflect new timestamp - typically going from nulls or max timestamp to either the new start timestamp on the next row or that timestamp minus 1 mili/microsecond.\n   * reprocess skipped files\n\nWhile one can do that with a stored procedure or udf, I prefer writing that in a python library that can be called from anywhere, and applied to relational tables or csv files or parquet files or jsonlines files, etc.  And doing it with sql without a udf or stored proc is both a headache and almost guarantee of getting something wrong.",
              "score": 4,
              "created_utc": "2025-12-28 17:27:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe6y5u",
                  "author": "financialthrowaw2020",
                  "text": "All of this is easily done in dbt as part of the transformation, no procs needed. Doing it separately is silly.",
                  "score": 0,
                  "created_utc": "2025-12-28 17:29:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwe6yzo",
                  "author": "moshujsg",
                  "text": "Idk, im of the idea that the table should take care of itself. I feel like its such a simple sql statement that theres no reason not to just have it in sql. The other thungs like dedup or ignore rows is business logic and shouldnt be embedded with the scd logic.",
                  "score": 0,
                  "created_utc": "2025-12-28 17:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwefyg1",
              "author": "thinkingatoms",
              "text": "checking record existence on a big table is expensive\n\nso is invalidating records that no longer exists",
              "score": 1,
              "created_utc": "2025-12-28 18:13:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwegbnl",
                  "author": "moshujsg",
                  "text": "You can index. Brcause its expensive is why you just stick to sql lol. Also exoensive doesnt mean complex.",
                  "score": 0,
                  "created_utc": "2025-12-28 18:15:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe27o4",
              "author": "Quick_Assignment8861",
              "text": "We use python for it because we have a centralized package with utilities. Think of basic stuff like bronze\\_ingestion\\_scd/stacked/events/cdc and then run load logic to silver/bronze. Where it just applies notebooks and in the metadata you write to what table, if u want a snapshot, blabla.",
              "score": 1,
              "created_utc": "2025-12-28 17:06:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdi77a",
          "author": "Misanthropic905",
          "text": "What SCD2 have to do with spark or databriks?",
          "score": 28,
          "created_utc": "2025-12-28 15:24:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdiq9o",
              "author": "Rare_Decision276",
              "text": "Bro I’m trying to say treat customers data having name, address, city, state, zip code etc., in order to implement SCD type 2 will you write code in traditional way ie generating surrogate key, hash by checking flags and do joins etc or will you do in lakeflow declarative pipelines?",
              "score": -38,
              "created_utc": "2025-12-28 15:27:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdk4cp",
                  "author": "R1ck1360",
                  "text": "SCD2 is a modeling technique, it is not related to any specific framework or tool",
                  "score": 53,
                  "created_utc": "2025-12-28 15:35:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwds2vq",
          "author": "SirGreybush",
          "text": "SCD2 is built into the table schema, the code base is based on Merge or UpSert + delete tracking.\n\nI suggest you look at \"Timestamp\" column type if available on your platform, or look into MD5() function which is quite universal, to do \"hashing\" of data, and use a staging layer.\n\nWhen you design properly a SCD2 table in a bronze layer, and have a staging layer in a \"raw\" layer (you should always have a staging layer!) - then a SQL view with the hashed values (the PK columns, the entire data row) then SCD2 becomes a breeze to do. Breeze because you compare two tables, you have one PK column and one HashDIFF column (or TimeStamp from a source DB) that tells you easily what to do.\n\nOften it is deletion tracking the most difficult - because if you get differential data, how do you know what no longer exists in source? In such cases you need an extra pipeline logic just for delete tracking. Mentioning as this is often overlooked and it comes back to bite you at the end.\n\nFor example, you load new data (from a full source or differential source) truncate/load into a staging table. Make a view to create all the extra columns you need, like using MD5() in the view. Use the staging view to do the UpSerts or Merge based on your bronze layer SCD2 historical table and the staging. \n\nFor delete tracking sometimes you need a new pipeline to get a full from source but only the PK column(s), not all the columns, so the file is smaller and the process is quicker. I use a different staging table, a different pipeline, just for this. Or if you are lucky, you can get CDC from source DB and then you have 100% of all changes. If only we had this all the time.\n\nSo basically you read the same data twice, the first time the data are usually files in a datalake or a file share, but it could also be a remote database that you ODBC / OleDB into to run a query. This is usually for on-prem, but I have used ODBC + MySQL remote DBs to gather info, it works really well, and no ELT/ETL tool required - it's all SQL in a stored proc.\n\nSome people create in the staging table all the extra fields to manage for SCD2 and the tool they use to consume the raw data into staging, adds the values to the extra fields, row-by-row. I do NOT like this method, as it puts intelligence inside the ELT/ETL tool making it difficult to manage. An ELT/ETL tool should be simple, just do mapping, Lift & Shift, ideally never fail - just get the data to staging table(s) and let the next level deal with issues. CSV is hell on earth, I use the BLOB technique on foreign CSV files and parse with SQL.\n\nFWIW, this topic (SCD2, 3, 4, hybrid...) is taught in actual classes world wide, maybe look into taking some basic ETL/ELT & BI courses, at least Udemy. My first SCD2 table I did back in 1998 - just to give you an idea on how old this logic is, the actual invention I don't know who - but probably Kimball in the 90's. I had a great database teacher that was ahead of  his time.",
          "score": 6,
          "created_utc": "2025-12-28 16:15:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdkedc",
          "author": "Dry-Aioli-6138",
          "text": "I would poc both and see which fits our flow better.",
          "score": 4,
          "created_utc": "2025-12-28 15:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe400l",
          "author": "ForwardSlash813",
          "text": "SCD2 can easily be implemented in Databricks DLT (Declarative Pipeline) using PySpark or SQL.  I prefer SQL.",
          "score": 2,
          "created_utc": "2025-12-28 17:15:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe91su",
              "author": "Rare_Decision276",
              "text": "Yes, absolutely CDC plays a very good role in declarative pipelines but the thing is can we do all tasks like Adhoc queries, pipelines, any queries we can write in DLT right",
              "score": 1,
              "created_utc": "2025-12-28 17:40:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwh0zzl",
                  "author": "ForwardSlash813",
                  "text": "You can run whatever CDC-related queries you like but only within the confines of a DLT pipeline execution.",
                  "score": 2,
                  "created_utc": "2025-12-29 02:07:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf02eh",
          "author": "Healthy_Put_389",
          "text": "I prefer with simple insert statement on hashvalue of scd type 2 columns when the haha value changes and update statement to terminate non existing hashvalues",
          "score": 2,
          "created_utc": "2025-12-28 19:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhxmsk",
          "author": "NoleMercy05",
          "text": "Sql works. It's just tables",
          "score": 2,
          "created_utc": "2025-12-29 05:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6o42",
          "author": "Nemeczekes",
          "text": "Disregarding writing style worthy of 8 year old kid the lakeflow pipelines will never cover the 100% of cases. So if I am using lakeflow and I need scd2 then I would used. \n\nIt is just a quality of life improvement not a game changer",
          "score": 2,
          "created_utc": "2025-12-28 17:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjvec",
          "author": "justanator101",
          "text": "Do you use declarative pipelines currently? Does your team have the technical expertise to implement scd2 in spark? What does the rest of the codebase look like? \n\nI’d personally implement myself because we’re a very technical team and prefer having full control and visibility into what runs. However, that does come at a trade off of more complex code base.",
          "score": 1,
          "created_utc": "2025-12-28 15:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdlalz",
              "author": "Rare_Decision276",
              "text": "Yes, it has more complex code base but in databricks we have decorative pipelines we have CDC it will rescue us",
              "score": 1,
              "created_utc": "2025-12-28 15:41:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdtkrc",
                  "author": "SirGreybush",
                  "text": "CDC is the best - especially for deletes.\n\nJust be prepared that in between two CDC updates, a single record in your bronze layer can be updated multiple times. Like a new customer, so an insert in the CDC. Then more than one person doing updates on that same day.\n\nYou must decide if you want all the changes in your SCD2 (I would do that) or just capture the \"latest\".\n\nIn the \"latest\" scenario - from my other comment - using a staging layer - if the source table has columns name similar to CreatedDate & UpdatedDate, and UpdatedDate = CreatedDate on a new insert, simply get the max(UpdatedDate) in your staging - view, by using ROW\\_NUMBER() OVER (PARTITION BY ...) functionality in your staging view.\n\nTo reduce SCD2 traffic, as 99.9% of people will always use IsCurrent=1 in the SCD2 table, and only look at history when there's an actual problem. \n\nIt depends on the source system CDC and how the humans use the source system. I've seen 10+ rows of change per day on a new customer with various ERP systems.",
                  "score": 1,
                  "created_utc": "2025-12-28 16:23:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwlvh7l",
          "author": "mike-manley",
          "text": "Stored procedures. I've sometimes used AUTOINCREMENT om a table INTEGER column and other times a sequence to generate the surrogate keys.",
          "score": 1,
          "created_utc": "2025-12-29 20:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqelp",
          "author": "TripleBogeyBandit",
          "text": "I would challenge that you truly need scd 2, it can multiply the costs when compared to scd 1",
          "score": -3,
          "created_utc": "2025-12-28 16:07:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdr63w",
              "author": "financialthrowaw2020",
              "text": "This is silly. By the time you realize you need scd 2 on dim-like attributes, it's too late and you look incompetent to your entire business team. Storage is cheap.",
              "score": 4,
              "created_utc": "2025-12-28 16:11:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwduanr",
                  "author": "ProfessorNoPuede",
                  "text": "I'd even go so far as to prefer immutable over scd2 for as far into the pipeline as possible.",
                  "score": 1,
                  "created_utc": "2025-12-28 16:26:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdqvm6",
              "author": "Rare_Decision276",
              "text": "If business wants to keep history of data then we have to go with type 2 right and also their call to go whether type 3 also",
              "score": 2,
              "created_utc": "2025-12-28 16:09:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pw5p4v",
      "title": "Anyone else going crazy over the lack of validation?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pw5p4v/anyone_else_going_crazy_over_the_lack_of/",
      "author": "SoggyGrayDuck",
      "created_utc": "2025-12-26 14:35:17",
      "score": 36,
      "num_comments": 27,
      "upvote_ratio": 0.91,
      "text": "I now work for a hospital after working for a bank and the way asking questions about \"do we have the right Data for what the end users are looking at in the front end?\" Or anything along those lines? I put a huge target on my back by simply asking the questions no one was willing to consider. As long as the the final metric looks positive it's going through get thumbs up without further review. It's like simply asking the question puts the responsibility back on the business and if we don't ask they can just point fingers. They're the only ones interfacing with management so of course they spin everything as the engineers fault when things go wrong. This is what bothers me the most, if anyone bothered to actually look the failure is painfully obvious. \n\nNow I simply push shit out with a smile and no one questions it. The one time they did question something I tried to recreate their total and came up with a different number, they dropped it instead of having the conversation. Knowing that this is how most metrics are created makes me wonder what the hell is keeping things on track? Is this why we just have to print and print at the government level and inflate the wealth gap? Because we're too scared to ask the tough questions? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pw5p4v/anyone_else_going_crazy_over_the_lack_of/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw11eqj",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-26 14:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw14tyg",
          "author": "Careful-Combination7",
          "text": "Welcome to planet earth. ",
          "score": 56,
          "created_utc": "2025-12-26 14:56:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw163jl",
              "author": "SoggyGrayDuck",
              "text": "This doesn't fly in finance lol",
              "score": 16,
              "created_utc": "2025-12-26 15:03:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw1c8gu",
                  "author": "compulsive_tremolo",
                  "text": "Because there are numerous agencies whose sole purpose in finance is to audit and access violations to policy, including violations to internationally agreed standards to within the interconnected global financial system.",
                  "score": 16,
                  "created_utc": "2025-12-26 15:38:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1vzrm",
                  "author": "One-Employment3759",
                  "text": "Actually I worked in finance and was surprised by how little validation there was there too.\n\n\nI built the metrics to assess banker performance. I did my best, but i had little guidance or validation. So I hope no one got fired due to a bug.",
                  "score": 3,
                  "created_utc": "2025-12-26 17:24:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9zkc2",
                  "author": "80hz",
                  "text": "I launched the brand new Finance data product and no one besides me validated the data. I'm an engineer by the way and I was Raising red flags. months later they go wow the date is really messy. I just laugh and go skiing as much as possible now.",
                  "score": 1,
                  "created_utc": "2025-12-27 23:58:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1e4u4",
          "author": "whogivesafuckwhoiam",
          "text": "I would say the issue is nobody knows how to validate a number. In finance, auditing or accounting, there are certain rules and guidance to preform validation checks. This is not the case for every industry. In some, many users just perform sanity check based on years of experiences and logical senses. If a number doesn't drift too much with the previous one, nobody has a reason to question. After all, raising a question also means creating personal burden most of the time.",
          "score": 15,
          "created_utc": "2025-12-26 15:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1ylv4",
          "author": "codemega",
          "text": "I've worked at places with no validation and at places with heavy validation and reconciliation of numbers. The places with no validation have all been bad workplaces while the ones with processes to vet numbers were good places to work at. It's not hard to draw this conclusion because if there is no validation, people don't care about what's being reported on. It speaks to a culture of poor process, lack of responsibility, and no ownership.",
          "score": 11,
          "created_utc": "2025-12-26 17:38:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4wwyy",
              "author": "codykonior",
              "text": "Where I've seen the good version, I've known the faces of the accounting team. Old school, close to the CEO, absolutely solid in what they want, describing it, and getting it. Absolutely a joy to work with. Those businesses have also been very established and traditional too. \n\nYeah, the horrible ones have been awful workplaces too 😏 But also really small ones and really big ones, where both need \"creative accounting\".",
              "score": 1,
              "created_utc": "2025-12-27 03:53:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1kbf1",
          "author": "JohnPaulDavyJones",
          "text": "Been there, brother. The folks doing the Excel work have this canned model that some guy named Rob created in 2019, and God forbid that everyone actually have a conversation about the data definitions to formalize the logic, because that would reveal that Rob’s model is actually hot garbage.\n\nThere’s a magic to inertia, in the business world. Ironically, I found it the opposite direction of your experience, at least relative to industries. My work in healthcare has been more marked by wanting to get accuracy and clarity, while my work in financial services is where I’ve run into all of the “Just don’t fucking change anything” people.",
          "score": 7,
          "created_utc": "2025-12-26 16:22:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1qmfp",
          "author": "TheEternalTom",
          "text": "I find, if you're the first person to ask the questions, you discover that there's no 'right' answer to aim at. Each department has it's own spin on the metrics... so any effort to get to a single source of truth is pointless.\n\nAt one point I was so close, turned out some were exporting my metrics into a spreadsheet and doing the same (insane) calculations and building BI around the spreadsheets...\n\nThe amount of time, effort and money big corps must lose due to shadow IT, most of which spring up because there wasn't a data team providing the data when it was needed is INSANE...",
          "score": 6,
          "created_utc": "2025-12-26 16:55:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1e8el",
          "author": "BarfingOnMyFace",
          "text": "Oh Jesus, at a hospital too… yeah, this happens to varying degrees,  but this hospital has a very bad attitude towards it. It honestly sounds disheartening and pointlessly counterproductive to me. I work in healthcare tech as well, and while sometimes people try to avoid digging too deeply due to the volume of work everyone is up against, I find that people still listen when someone raises data integrity concerns. If I wasn’t heard and was simply expected to keep my head down, I’d be out the door. But I think a little mix of the two (keep your head down, still be transparent and forthright about issues) is to be expected.",
          "score": 5,
          "created_utc": "2025-12-26 15:49:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw19ivh",
          "author": "bengen343",
          "text": "I'd say this is certainly the most common state of affairs. I've attacked this problem pretty aggressively in a couple of past jobs. I've launched entire \"No Fake Data\" campaigns complete with internal websites, stickers, and aggressive pitches to whoever my C-level overseer was there. I've generally found upper management types to be pretty receptive to getting things right, especially if they're data inclined to begin with. But if you have a big marketing organization, oh man, be prepared for some hostility.",
          "score": 10,
          "created_utc": "2025-12-26 15:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw21qfy",
              "author": "SoggyGrayDuck",
              "text": "Oh this yep this is a nonprofit that cares more about image than anything else",
              "score": 3,
              "created_utc": "2025-12-26 17:54:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwibyut",
              "author": "galeize",
              "text": "Curious how did you go about pitching it? \nWas the C-level overseer your direct?\nHow was the validation process actioned? TY",
              "score": 1,
              "created_utc": "2025-12-29 07:27:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjvwvr",
                  "author": "bengen343",
                  "text": "Each time was different depending on the existing structures and culture of the company.\n\nOne place was a bit more informal. In that case, it had been on my mind for a while, and so I had my thoughts pretty well put together. On top of that, I knew there was some general unease with the direction that the Data Team was going. One evening, it just happened that myself and the CTO (they were a couple steps above me) were the only people left in our wing of the office, so I invited them out to dinner and made the pitch. It was well received, and that was the scenario where I had a full on \"No Fake Data\" campaign where I put the proposal into a formal internal website and made stickers and superlatives I'd hand out for Data Engineers, Developers, and Product Managers who got on board with this. A big part of my pitch was to just show the rats nest of spaghetti code we had in dbt and ask, \"would you trust insights based on this code?\" That was a pretty easy conversation. And then after that, it was a matter of holding the line with stakeholders that if we didn't have real data we weren't going to guess but rather get together with engineering to make sure we were tracking things the way we needed to. Since I had the backing of the CTO I was able to alter the process that our product managers and engineers went through in such a way that their design process had to be run by me to approve the eventing and telemetry before work could begin.\n\nIn another case, the company had a really strong process for surfacing things like this. So I put together the pitch with my fellow Data Engineers at our regularly scheduled guild meeting and then just added myself to the engineering-wide Request for Comment-type meeting calendar we had. Since it was such a big inititave I had to go through several rounds before everyone was satisifed it was a good and necessary thing to but then it was approved and we were given the time to action it.\n\nAnd then two other times I was the leader of the Data Team, so in those cases it was just more of me saying, \"This is how it's gonna be, if it's my team, this is what we're working on.\"\n\nIf you mean validation more tangibly, like validating the output of the data, we usually took two approaches. If possible, we'd recreate one (or many) reports from the source system in our internal BI to ensure that our modeling was matching the source output. Then, or if that wasn't available, we'd do a combination of internal QA alongside having the domain-export stakeholders assess and approve metrics before we rolled things out into production.",
                  "score": 1,
                  "created_utc": "2025-12-29 14:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1sg54",
          "author": "throwaway0134hdj",
          "text": "Yeah this is the dilemma. If you ask too many questions you look incompetent, but if you ask too few you literally cannot do your job. I’ve been in this situation, it’s a tough middle ground to reach as you don’t want to annoy/pester ppl but you also need answers to data related questions. And yeah it can be a very industry-specific thing where they aren’t super tech friendly so in the case no one really has the answers or is accountable.",
          "score": 3,
          "created_utc": "2025-12-26 17:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw26kjr",
          "author": "Headband6458",
          "text": "I think you have 2 options: shut up and color like you're doing or take responsibility for data governance at your new company. Both have pros and cons.",
          "score": 1,
          "created_utc": "2025-12-26 18:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6jpc0",
          "author": "BringtheBacon",
          "text": "Hey man, that sounds like a lot. I hope you find this reply validating",
          "score": 1,
          "created_utc": "2025-12-27 12:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbwrm7",
          "author": "cjcottell79",
          "text": "Levers, I'm making this metric for you but how are you going to be able to influence it? \n\nAlways end up with something that looks positive (97% good vs 3% bad), pretty dashboards but not integrated into the business because there is no way to influence it.",
          "score": 1,
          "created_utc": "2025-12-28 07:42:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc4y8w",
          "author": "fourby227",
          "text": "Oh I know this. I worked in medical/biological research. If all is managed by non-technical scientists or business people, no one cares for data or integrity or what an engineer has to say. They believe they already know what the results have to be, they just need some code monkeys to produce the results as expected. An if the data does not fit the expectations, Its the engineers fault. If the data clearly shows a hypothesis is wrong, that its the data engineers fault, because literature and papers are always right.",
          "score": 1,
          "created_utc": "2025-12-28 09:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwimv21",
          "author": "galeize",
          "text": "Yes, the art of asking haha. We want to give correct data and it's taken as more work and not their problem until it becomes a problem, or yes buck is passed. \n\nI'm curious how the metric creation flow is in your nonprofit hospital setting and who owns what, especially with the many tables and ways user can document?\nIs it the data analysts who receive/convey the initial ask from management and interface w/ end users (or an EHR workflow team?) on what front end sees/uses? \n\nWhat tools does your team have for validating before pushing it out? Say is there a data dictionary or other metrics or dashboard to extrapolate back from?",
          "score": 1,
          "created_utc": "2025-12-29 09:08:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxl447",
      "title": "My attempt at a data engineering project",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pxl447/my_attempt_at_a_data_engineering_project/",
      "author": "NoOwl6640",
      "created_utc": "2025-12-28 07:10:22",
      "score": 33,
      "num_comments": 12,
      "upvote_ratio": 0.84,
      "text": "Hi guys,\n\nThis is my first attempt trying a data engineering project\n\nhttps://github.com/DeepakReddy02/Databricks-Data-engineering-project\n\n(BTW.. I am a data analyst with 3 years of experience ) ",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pxl447/my_attempt_at_a_data_engineering_project/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwbvke4",
          "author": "PrestigiousAnt3766",
          "text": "What do you want to hear?\n\nI would work with a virtual environment, using pyproject.toml for dependencies.\nNow the project will work for you, but is not deployable/production grade.\n\nI don't see any databricks configuration or databricks Asset bundle. How do you run it?\n\nYou have hardcoded almost everything. I think the art of DE is to make code reusable, which your project isnt.",
          "score": 20,
          "created_utc": "2025-12-28 07:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc13fp",
              "author": "NoOwl6640",
              "text": "Any feedback would be appreciated. Don't hold back.",
              "score": 4,
              "created_utc": "2025-12-28 08:24:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwc15lr",
              "author": "NoOwl6640",
              "text": "I started by using databricks free edition",
              "score": 2,
              "created_utc": "2025-12-28 08:24:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwc4vov",
                  "author": "PrestigiousAnt3766",
                  "text": "I would investigate databricks asset bundles. That is a way to deploy your code. DABs contain workflows/jobs with which you can schedule your job. You can work with environment variables, or with key-valuepairs using argparse library.\n\nThe way I would rewrite your \"withColumnRenamed\" statement in I believe silver (bit quick n dirty) would be to write a loop over df.colums and replace spaces with _.\nYou can extend that if you want to cover other scenarios.",
                  "score": 4,
                  "created_utc": "2025-12-28 09:00:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwc1t5k",
          "author": "looctonmi",
          "text": "Writing to a single parquet file with spark is a bit of an antipattern because then the code will only run with a single node. Also since this is a databricks project, it would make sense to take advantage of auto loader.",
          "score": 8,
          "created_utc": "2025-12-28 08:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc46jy",
              "author": "NoOwl6640",
              "text": "What do you mean? Like partition the file",
              "score": 3,
              "created_utc": "2025-12-28 08:53:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdkpsa",
                  "author": "looctonmi",
                  "text": "I would read into delta lake and consider why you would use delta tables over single parquet files for your bronze and silver layers. Then I would read about common bronze -> silver patterns and how to implement incremental batch logic so that you’re not truncate and loading into silver on every run.",
                  "score": 3,
                  "created_utc": "2025-12-28 15:38:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfvyty",
          "author": "Luxi36",
          "text": "Make sure your code follow PEP8 standards as its important to follow similar coding styles as future projects/teams.\n\nYour variable and function naming is all over the place with snake title or camel case. Instead of just simply using proper snake case.\n\nAs other people mentioned you can group certain transformations better and in the Extract.py get rid of lots of indentations by using guard clauses.\n\nExample; instead of the check `if base_url is not None:` you now have an indentation for the parameters, and handle the raise exception in the `else` you can also simply do `if base_url is None: raise ...` and have the parameters on the normal indentation level.\n\n\nPage count could be handled using `enumerate` instead of doing the +1 count.",
          "score": 3,
          "created_utc": "2025-12-28 22:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwik3zc",
              "author": "NoOwl6640",
              "text": "Nice advice, I'll try to improve upon it",
              "score": 1,
              "created_utc": "2025-12-29 08:42:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1px50yt",
      "title": "Which ETL tools are most commonly used with Snowflake?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1px50yt/which_etl_tools_are_most_commonly_used_with/",
      "author": "Commercial-Post4022",
      "created_utc": "2025-12-27 18:36:33",
      "score": 32,
      "num_comments": 50,
      "upvote_ratio": 0.89,
      "text": "Hello everyone,  \nCould you please share which data ingestion tools are commonly used with Snowflake in your organization? I’m planning to transition into Snowflake-based roles and would like to focus on learning the right tools.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1px50yt/which_etl_tools_are_most_commonly_used_with/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw8c10z",
          "author": "AutoModerator",
          "text": "Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-27 18:36:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8ibez",
          "author": "you4321",
          "text": "For me, Fivetran for raw data ingestion and dbt core to handle data transformations",
          "score": 26,
          "created_utc": "2025-12-27 19:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa1c5j",
              "author": "techinpanko",
              "text": "What's your primary reason for using dbt core vs Python?",
              "score": 1,
              "created_utc": "2025-12-28 00:08:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbvn2o",
                  "author": "Wenai",
                  "text": "Dbt is about 500 times easier, faster, more efficient than python.",
                  "score": 13,
                  "created_utc": "2025-12-28 07:32:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx886vf",
                  "author": "Thinker_Assignment",
                  "text": "it's not about dbt vs python but sql + framework (dbt) vs raw python. If you wanna compare on equal footing you need to compare dbt hamilton/ibis\n\ndbt is not faster than python, that's an apples and oranges comparison, like saying bananas are faster than roman numerals.\n\nwhat they mean is   \n\\- sql is fast and easy to work with  \n\\- dbt bings control and management\n\nVs   \n\\- pandas is a kind of runtime like SQL where you do more work  \n\\- no framework = chaos and custom code that's hard to read and maintain\n\nThis recommendation comes largely from thinking \"in the box\" of typical tools and cases that people did until now. \n\nYou could also think outside the box and look at Hamilton and Ibis - many teams are using that instead of dbt in python-first environments like ml and ai engineering",
                  "score": 1,
                  "created_utc": "2026-01-02 09:35:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8ebmp",
          "author": "Sneaky_McSlitherman",
          "text": "3rd party managed connectors from vendors like Fivetran are fairly common. Those plus DBT for modeling make for a pretty modern analytics engineering shop.",
          "score": 33,
          "created_utc": "2025-12-27 18:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8jilk",
          "author": "L3GOLAS234",
          "text": "I extensively used Airflow, S3ToSnowflakeOperator",
          "score": 7,
          "created_utc": "2025-12-27 19:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8cw6c",
          "author": "Bstylee",
          "text": "Fivetran",
          "score": 9,
          "created_utc": "2025-12-27 18:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9kpvm",
          "author": "MAValphaWasTaken",
          "text": "My place uses Informatica for now.\n\nThe rest of you can stop laughing. We're switching to Azure Data Factory.",
          "score": 5,
          "created_utc": "2025-12-27 22:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwam0ck",
              "author": "AntDracula",
              "text": "> Azure Data Factory\n\nThis increases the laughter",
              "score": 20,
              "created_utc": "2025-12-28 02:08:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwas447",
                  "author": "MAValphaWasTaken",
                  "text": "I know Talend and Informatica, zero ADF experience so far. Why is it bad? It can't possibly be worse than Informatica Cloud, can it?",
                  "score": 3,
                  "created_utc": "2025-12-28 02:44:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8vj8z",
          "author": "AnalyticalMynd21",
          "text": "Azure Data Factory. Rivery.",
          "score": 8,
          "created_utc": "2025-12-27 20:18:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhsgfx",
              "author": "JBalloonist",
              "text": "ADF to load into snowflake?",
              "score": 1,
              "created_utc": "2025-12-29 04:52:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9h75u",
          "author": "atrifleamused",
          "text": "Adf. Most of the others are just too expensive",
          "score": 3,
          "created_utc": "2025-12-27 22:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8ease",
          "author": "Amilol",
          "text": "Does not matter, pick what is right for you :)",
          "score": 5,
          "created_utc": "2025-12-27 18:47:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw95nfe",
          "author": "Rough_Mirror1634",
          "text": "We used Sling previously, it was OK. Very quick to get up and running, fast, but a few minor bugs with the various transformation options.\n\nWe will be using DLT for an upcoming project, looks cool but no first hand experience.\n\nFor our use cases, paid tools like Fivetran are prohibitively expensive. Infrastructure-as-code is higher effort to get up and running, but significantly more flexible and powerful IMO.",
          "score": 2,
          "created_utc": "2025-12-27 21:14:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9euoa",
              "author": "Rude-Needleworker-56",
              "text": "Could you share what are the data sources that you typically move data from? ( asking since I am working on a fivetran alternative)",
              "score": 1,
              "created_utc": "2025-12-27 22:03:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9vq0z",
                  "author": "Rough_Mirror1634",
                  "text": "Primarily other databases, with some SFTP flat file/CSV stuff. Think - moving data from the production database to a reporting DW in Snowflake.",
                  "score": 2,
                  "created_utc": "2025-12-27 23:36:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8b3i3",
              "author": "Thinker_Assignment",
              "text": "dlt cofounder here, i hope you will enjoy our tool - We are already a premier snowflake partner and will work with them on improved integrations.\n\nfivetran + dbt declared war on snowflake so I am doubting a happy future there.",
              "score": 1,
              "created_utc": "2026-01-02 10:03:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwa5pa6",
          "author": "dasnoob",
          "text": " We were using Airflow but it was generating a ton of errors and our on-call engineers were having to login in the middle of the night every single night to fix stuff.\n\nLast I talked to them they were moving everything out of Airflow and back into Informatica where it just hums along.",
          "score": 2,
          "created_utc": "2025-12-28 00:32:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtg1o6",
              "author": "Disastrous_Tough7612",
              "text": "Try Prefect, better logs and errors handling than Airflow, also no depenency issues and docker image problems. Dagster is other alternative, but i haven't try it yet.",
              "score": 2,
              "created_utc": "2025-12-30 23:03:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu209u",
                  "author": "dasnoob",
                  "text": "Not my decision to make as even though I take the data engineering team's output and do more transform on it to make it match what we actually need I'm not considered a part of the team.\n\nIn fact, if I give my opinion (such as two weeks ago explaining to them how Snowflake's PUT/COPY works since they were just running millions of INSERT statements) I get my hand slapped for 'stepping on their toes'.\n\nIf you are curious. They claimed to have never heard of it.",
                  "score": 1,
                  "created_utc": "2025-12-31 01:04:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwkrvd1",
          "author": "mark2347",
          "text": "Copy activity in ADF followed by stored procedure execution.  ADF works so much better than our previous AWS Airflow DAGs.",
          "score": 2,
          "created_utc": "2025-12-29 17:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa1kc4",
          "author": "techinpanko",
          "text": "Fivetran on their free plan, native postgres connector from snowflake marketplace for most data.",
          "score": 1,
          "created_utc": "2025-12-28 00:09:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa3axx",
          "author": "geek180",
          "text": "Airbyte Cloud for API sources and Azure Storage + snowpipe or stored procedures for everything else.",
          "score": 1,
          "created_utc": "2025-12-28 00:19:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa5ga2",
          "author": "siggywithit",
          "text": "Many home grown, fivetran for our db replication, Precog for SAP integrations.",
          "score": 1,
          "created_utc": "2025-12-28 00:31:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbagqu",
          "author": "anonymousme002",
          "text": "In our org, we use fivetran and dbt most of the times",
          "score": 1,
          "created_utc": "2025-12-28 04:40:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcv8ht",
          "author": "eeshann72",
          "text": "Iics",
          "score": 1,
          "created_utc": "2025-12-28 13:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkimzf",
          "author": "Born-Pirate1349",
          "text": "DBT is a must, fivetran as well. Even though we haven't used these tools in our org.",
          "score": 1,
          "created_utc": "2025-12-29 16:39:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhyo7d",
          "author": "CasualReader3",
          "text": "For those of you using dbt core, check out sqlmesh it truly has enabled me and my team to adopt true software engineering principles.\n\nThank me later",
          "score": 1,
          "created_utc": "2025-12-29 05:36:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8c0uh",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 0,
          "created_utc": "2025-12-27 18:36:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz4ynz",
      "title": "How should I implement Pydantic/dataclasses/etc. into my pipeline?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pz4ynz/how_should_i_implement_pydanticdataclassesetc/",
      "author": "paxmlank",
      "created_utc": "2025-12-30 01:47:44",
      "score": 32,
      "num_comments": 22,
      "upvote_ratio": 0.92,
      "text": "tl;dr: no point stands out to me as the obvious place to use it, but I feel that every project uses it so I feel like I'm missing something.\n\nI'm working on a private hobby project that's primarily just for learning new things, some that I never really got to work on in my 5 YOE. One of these things I've learned is to \"make the MVP first and ask questions later\", so I'm mainly trying to do just that for this latest version, but I'm still stirring up some questions within myself as I read on various things.\n\nOne of these other questions is when/how to implement Pydantic/dataclasses. Admittedly, I don't know a lot about it, just thought it was a \"better\" Typing module (which I also don't know much about, just am familiar with type hints).\n\nI know that people use Pydantic to validate user input, but I know that its author says it's not a validation library, but a parsing one. One issue I have is that the data I collect largely are from undocumented APIs or are scraped from the web. They all fit what is conceptually the same thing, but sources will provide a different subset of \"essential fields\".\n\nMy current workflow is to collect the data from the sources and save it in an object with extraction metadata, preserving the response *exactly* was it was provided. Because the data come in various shapes, I coerce everything into JSONL format. Then I use a config-based approach where I coerce different field names into a \"canonical field name\" (e.g., `{\"firstname\", \"first_name\", \"1stname\", etc.} -> \"C_FIRST_NAME\"`). Lastly, some data are missing (rows and fields), but the data are consistent so I build out all that I'm expecting for my application/analyses; this is done partly in Python before loading into the database then partly in SQL/dbt after loading.\n\nInitially, I thought of using Pydantic for the data as it's ingested, but I just want to preserve whatever I get as it's the source of truth. Then I thought about parsing the response into objects and using it for that (for example, I extract data about a Pokemon team so I make a Team class with a list of Pokemon, where each Pokemon has a Move/etc.), but I don't really need that much? I feel like I can just keep the data in the database with the schema that I coerce it to and the application currently just runs by running calculations in the database. Maybe I'd use it for defining a later ML model?\n\nI then figured I'd somehow use it to define the various getters in my extraction library so that I can codify how they will behave (e.g., expects a Source of either an Endpoint or a Connection, outputs a JSON with X outer keys, etc.), but figured I don't really have a good grasp of Pydantic here.\n\nAfter reading on it some more, I figured I could use it *after* I flatten everything into JSONL and use it while I try to add semantics to the values I see, but as I'm using Claude Code at points, it's guiding me toward using it *before/during* flattening, and that just seems forced. Tbf, it's shit at times.\n\nTo reiterate, all of my sources are undocumented APIs or from webscraping. I have some control over the output from the extraction step, but I feel that I shouldn't do that in extracting. Any validation comes from having the data in a dataframe while massaging it or after loading it into the database to build it out for the desired data product.\n\nI'd appreciate any further direction.",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pz4ynz/how_should_i_implement_pydanticdataclassesetc/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwnk502",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-30 01:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnlxx5",
          "author": "Shensy-",
          "text": "I'm very new to this, just getting pydantic implemented in a project for the first time now, but since you can dump a model to JSON, wouldn't the best candidate for this be using it to perform the step where you coerce it?",
          "score": 7,
          "created_utc": "2025-12-30 01:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqby0w",
              "author": "tjger",
              "text": "This. Pydantic should be used at the object-level (i.e. application), not data ingestion. Even though one could argue that Pydantic is useful to enforce data types, I can imagine use cases  where a data column may sometimes come structured and other times unstructured, and part of the work is to transform them into either one type or the other (i.e. transform into a new column).\n\nThis way, Pydantic would be useful on higher levels / higher on the medallion transformations if you will, where objects are finally defined and their end types truly enforced.",
              "score": 2,
              "created_utc": "2025-12-30 14:00:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwnolut",
              "author": "paxmlank",
              "text": "Possibly, and that's the direction Claude Code seems to want to go in. One concern I had was that the responses are at some level a list of dicts that I want saved as a file in JSONL format - maybe I can model each dict as JSON but not write it like that just yet, I hadn't looked into it yet. The other was that I felt like this was adding semantics to the data which I didn't want to do until after I flattened/coerced it. I'm trying to do everything very simply: extract then save with metadata, flatten and save copy, add derive missing values and save, load to database/lake, analyse",
              "score": 1,
              "created_utc": "2025-12-30 02:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnqa6i",
          "author": "West_Good_5961",
          "text": "Why not load to a relational database if schema enforcement is critical?",
          "score": 5,
          "created_utc": "2025-12-30 02:21:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoxquf",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 3,
              "created_utc": "2025-12-30 07:08:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpiqf0",
                  "author": "paxmlank",
                  "text": "Fair - I thought of having my raw go straight into the DB but I was hoping on having this data eventually move into a datalake-style environment. With the idea of unified DuckDB logic running across the parsed/coerced data, I felt like I should let all parsing outside of the DB for now.\n\nIdk though, as it will help as you said :|",
                  "score": 1,
                  "created_utc": "2025-12-30 10:22:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwnqv9c",
              "author": "paxmlank",
              "text": "I am, which is why I feel like I don't need Pedantic/dataclasses but that felt like a wild sentiment so I wasn't sure",
              "score": 1,
              "created_utc": "2025-12-30 02:24:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwnsz5b",
                  "author": "raf_oh",
                  "text": "I’d think it would be so you can enforce constraints in code rather than in your db, and then load to your db. So like min/max etc",
                  "score": 4,
                  "created_utc": "2025-12-30 02:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpnwda",
                  "author": "Obvious-Phrase-657",
                  "text": "That makes sense, so maybe you don’t actually need it, I know that is a hobby project you might want to test cool stuff.\n\nI see the usage when the data you receive is generated by another system (maybe yours too, but not the target db/lake) and you use the schema as a contract and even better, if the source is also yours you can reuse the pydantic code for the API entities ian your pipeline or have a shared repository.\n\nFor instance, I had a datalake receiving streaming data from kafka, the data was produced by other teams, but they define the schema in a repo, and we pull that schema in order to handle the data and do a quick data qa validations \n\nBut if you just need to load it into a db pr datalke you can enforce the achema on the DDL. Even in a datalake you can use iceberg or delta (highly recommend for experience and resume) \n\nTldr don’t use it if you can enforce it on the ddl as it is far easier, use it if you need to enforce schema on a complex application the is based on python (or other coding language)",
                  "score": 1,
                  "created_utc": "2025-12-30 11:09:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtian6",
          "author": "YouKnowILoveMyself",
          "text": "Hi I work with Pydantic and dataclasses everyday and I think I can explain this well.\n\nPydantic - mostly used when there is any input/output that needs to be validated, a good example is let's say I want to integrate with some external service, if I have to call their endpoints with any specific payload Pydantic makes sure that my outgoing data to that service will always have these fields and I can write custom rules for a Pydantic class to enforce certain conditions. Same works for if any service decides to call my endpoints, I can stop any processing to happen if request data to my endpoint is wrong.\n\nDataclasses - used mainly for internal data transfer. An example for this is, let's say after my Pydantic validation occurs I now want to move around the fields I can set them in a dataclasses and move them around with pass by value/ reference. Now I can do this with Pydantic also but why I shouldn't. Pydantic does have an overhead when it parses the field this is very small but I've hit cases when it matters (Pydantic v2 is faster). Dataclasses are faster when I've already validated the data internal transfer is faster and makes more sense to use a dataclass when I just need to move and play around with the data. \n\nAdditional point database updates I would make a data access layer which would be validated against the schema. I use SQL alchemy here for the models some people use Pydantic here too, optional honestly. This way it makes it easier to unit test too when you abstract everything and loosely coupled. \n\nHope it clears things up!!",
          "score": 3,
          "created_utc": "2025-12-30 23:15:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwol1wu",
          "author": "ADGEfficiency",
          "text": "Function inputs and returns should be Pydantic models.",
          "score": 1,
          "created_utc": "2025-12-30 05:26:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsaow5",
          "author": "empireofadhd",
          "text": "It’s used implicitly in many frameworks. In using pyspark so it has its schema enforcement but if I did not have that I could use pydantic. I would probably use some yaml framework for field definitions though as it’s a bit more portable.",
          "score": 1,
          "created_utc": "2025-12-30 19:43:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1i5sh",
          "author": "digitalghost-dev",
          "text": "I’ve been using Pydantic in my pipelines when I first read from the API to make sure the types are correct by using the BaseModel class.\n\nhttps://github.com/digitalghost-dev/poke-cli/blob/main/card_data/pipelines/defs/extract/tcgdex/extract_series.py",
          "score": 1,
          "created_utc": "2026-01-01 06:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwojss2",
          "author": "EarthGoddessDude",
          "text": "Pydantic is for input/output, stuff around the edges of your code. Dataclasses are for internal state management. Yes, they overlap and you can use both in many situations, but generally that’s how you should think of them. \n\nTbh I don’t read your post super carefully because a) it was a lot and b) not very well organized. I don’t mean to be rude, just being honest. I’m also really tired and about to go to sleep so. \n\nFrom I can tell, you definitely can, and sometimes want to, use pydantic to load in your api data. It will not only parse and validate it, but it can standardize your field naming too. You can do a lot of cool things with it, like use a TypeAdapter for places where you need different structures that come out of the same API. That being said, I do view it more as an OLTP type tool, it *feels* more row based. For DE tasks, I would reach for DuckDB or polars to read in the API or json data and flatten it. Really depends on your use case and data volume\n\nAs for landing your raw data and then processing it, that’s a valid way to do it IMO. Land your API responses as JSON and then process with whatever tool you pick.",
          "score": -1,
          "created_utc": "2025-12-30 05:17:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqg0ha",
              "author": "CrackerJackKittyCat",
              "text": "This. Pydantic is to python what DTO objects are to java codebases --- they get used at the API edge. They exist for outbound message serialization and incoming message validation and parsing into objects.\n\nThe internals of your python codebase could then use dataclasses, ORM classes, or whatever and those would have varying degrees of behavior. The messaging Pydantic classes should have close to zero behavior at all -- perhaps some convenience data accessor properties, but nothing that you'd call 'real functionality.'",
              "score": 1,
              "created_utc": "2025-12-30 14:23:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpk2cx",
          "author": "Xman0142",
          "text": "Use Scala",
          "score": 0,
          "created_utc": "2025-12-30 10:34:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu6fj8",
      "title": "How much DevOps do you use in your day-to-day DE work?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pu6fj8/how_much_devops_do_you_use_in_your_daytoday_de/",
      "author": "BeautifulLife360",
      "created_utc": "2025-12-23 22:01:52",
      "score": 31,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "What's your DE stack? What devops tools do you use? Open-source or proprietary?  How do they help?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pu6fj8/how_much_devops_do_you_use_in_your_daytoday_de/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvo99fb",
          "author": "JBalloonist",
          "text": "Every day as of two weeks ago, but just for deployment. Everything is in Microsoft Fabric so there are no builds.\n\nEdit: my last role was an AWS shop and the majority of my jobs ran in Docker images on ECS or Lambda.",
          "score": 8,
          "created_utc": "2025-12-24 05:37:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp2cgp",
          "author": "Ulfrauga",
          "text": "Tools: Azure Databricks and DevOps.  We use branching and PRs, so all the time for development.  Deployment is manual, but we have a process.  It's largely facilitated by the fact our DB Jobs run using git source rather than Workspace, so once it's merged, the code is effectively deployed.  Creating Jobs and any exception stuff is manual.  Next step is to enter the realms of CI/CD and automate things.  My expectation is that moving to Databricks Asset Bundles (for Jobs at least in the first place) will be the impetus for that.\n\n  \nEDIT: Team of 2.  The branching and stuff is probably for my own sanity, and seems like it's just what we should be doing, regardless of team size.",
          "score": 5,
          "created_utc": "2025-12-24 10:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrorov",
              "author": "kirdane2312",
              "text": "You are on the right track. We first started with a remote repo to host our codes and created & maintained Databricks Jobs manually. We knew that we should convert manual maintaining into code. We then moved to DAB + gitlab CI/CD for deployment to maintain those Jobs. one of the good thing is manual mistakes don't happen anymore and jobs and its resources are version controlled. \n\nNext step I can recommend is to move DDL changes on databricks to code as well. It is not possible to do it with databricks asset bundle or terraform as of now. But I can recommend liquibase open source. \n\nPS: We were as well team of 2.",
              "score": 1,
              "created_utc": "2025-12-24 20:05:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvn7z3s",
          "author": "QuietSea",
          "text": "Every day, we are a you build it, you run it shop. Spark jobs on EMR Serverless, orchestrated by Step Functions and then loaded into a self-hosted database EKS cluster. We also use Athena + Lambda for the lighter ETLs. DevOps tools include stuff like terraform, CICD GitHub actions, GitOps ArgoCD for our clusters.",
          "score": 2,
          "created_utc": "2025-12-24 01:28:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvniql6",
              "author": "jaredfromspacecamp",
              "text": "How large is the team you work on?",
              "score": 1,
              "created_utc": "2025-12-24 02:35:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvnx8t0",
                  "author": "QuietSea",
                  "text": "Our product has 4 teams with 5-6 developers each",
                  "score": 1,
                  "created_utc": "2025-12-24 04:09:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvoaym5",
          "author": "mtoto17",
          "text": "Deploy all our tooling using pulumi, ci/cd is gh actions",
          "score": 1,
          "created_utc": "2025-12-24 05:51:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpiq5v",
          "author": "Main-Public1928",
          "text": "just manually manage everything on kubernetes",
          "score": 1,
          "created_utc": "2025-12-24 12:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpocot",
          "author": "Otherwise-Pass9556",
          "text": "Quite a bit, honestly. CI/CD, infra-as-code and a lot of time waiting on builds/tests. We kept our stack the same and just used Incredibuild to speed things up when build time became the bottleneck.",
          "score": 1,
          "created_utc": "2025-12-24 13:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpqnle",
          "author": "Ra_Re_7",
          "text": "Ours is a completely AWS shop. For the IaC we are writing ours in AWS CDK python (and it creates cloudformation templates underneath) . AWS CodeDeploy and CodePipeline for deployments.\n\nAs for the DE , we mostly deal with telemetry data. Firehose writes data to S3 source bucket and lambda gets triggered to copy the file into raw bucket at correct partition. A flattering logic lambda runs after it. Eventbridge for triggers , step functions for orchestration and Dynamodb , Redshift , Glue for datawarehoue pipelines.",
          "score": 1,
          "created_utc": "2025-12-24 13:33:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrjub6",
          "author": "No_Airline_8073",
          "text": "Looking for someone with 2-5 years of experience in deploying and maintaining data infra on EKS in Bangalore. I would like to connect and have a chat.",
          "score": 1,
          "created_utc": "2025-12-24 19:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvo6kb7",
          "author": "PrestigiousAnt3766",
          "text": "We have centralized devops to the platform team.\n\nWe do terraform iac.\nDatabricks platform\nDABs for deployment.\nJobs for scheduling\nSeveral additional \"microservices\" to check health, audit, compliance of the platform.\n\nPart of the reason is that the org has 8 DEs, but 30 or 40 bi developers that need to work on this new platform. With 0 experience with python or devops. So we fascilitate that.\n\nSo DEs/BI only do code PRs for extraction and transforms.",
          "score": 1,
          "created_utc": "2025-12-24 05:16:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q13vjk",
      "title": "Best certificates nowadays for Data Engineers?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q13vjk/best_certificates_nowadays_for_data_engineers/",
      "author": "Irachar",
      "created_utc": "2026-01-01 13:10:46",
      "score": 30,
      "num_comments": 18,
      "upvote_ratio": 0.84,
      "text": "What are the best certificates to earn this 2026 as a FREELANCE DE?\n\n  \nI assume from AWS and Azure for sure.\n\n\\*Azure has the DP-700 (Fabric Data Engineer) as a new standard?\n\n  \nWhat about the rest? Databricks, dbt, snowflake, something in LLM maybe?\n\n",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q13vjk/best_certificates_nowadays_for_data_engineers/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx2q1oo",
          "author": "Wingedchestnut",
          "text": "Any Databricks/ Snowflake, Azure/AWS certification your company or client wants you to have.",
          "score": 34,
          "created_utc": "2026-01-01 13:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2qk1c",
              "author": "Irachar",
              "text": "Well I’m freelance, so my clients are basically all the market possible.",
              "score": 3,
              "created_utc": "2026-01-01 13:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3ifw1",
                  "author": "mrbartuss",
                  "text": "Then you answered your question, didn't you?",
                  "score": 10,
                  "created_utc": "2026-01-01 16:16:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4l547",
                  "author": "protonchase",
                  "text": "How do you find work as freelance?",
                  "score": 3,
                  "created_utc": "2026-01-01 19:33:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2qwj9",
          "author": "SnooGiraffes7113",
          "text": "If there are certain companies you want to work for then find out what tools they use and get those certs. If not, then any big 3 cloud cert would be good. Other than those, there is high demand for dbt, snowflake and databricks. You can try to pick those certs. Those should give you a base and also show that you understand cloud, can use databases and transform data. Finally, python and SQL are the gold standard for languages to know for DEs.",
          "score": 9,
          "created_utc": "2026-01-01 13:24:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3ffx3",
          "author": "swapripper",
          "text": "I was actually thinking about this exact thing since it’s New Year’s resolution time and all that, lol.\n\nI have a few DE-specific certs. For data engineers in general, the big ones are AWS/Snowflake/Databricks/Azure/GCP/dbt - depending on your tech stack and/or location.\n\nAs we move toward agentic workflows, I feel like streaming and real-time knowledge will become increasingly valuable. Worth checking if there are dedicated certs from major players like Apache Kafka.\n\nAlong similar lines, as AI code automation/GPU workloads grow, understanding infra deployments/ sandboxed environments will become critical. So it might be worth looking into Terraform/K8s/Nvidia certs depending on your setup.\n\nCurious to hear how others feel about this.",
          "score": 3,
          "created_utc": "2026-01-01 16:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx57bvz",
          "author": "Ill-Strawberry-3585",
          "text": "Consider the answer might be none. It really depends on what types of clients you’re targeting. As someone who’s both hired freelancers and worked as a freelancer in the startup space, certifications are basically meaningless. Your time would be relatively better spent building out a portfolio that shows you can deliver real-world value with these tools.",
          "score": 2,
          "created_utc": "2026-01-01 21:27:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9mnph",
              "author": "elraba",
              "text": "I suppose that certs are neither valued nor respected in the startup world, but they might be in more traditional and highly regulated sectors such as finance, telcos, public admin, big consulting... All depends on the type of clients you are targeting.",
              "score": 1,
              "created_utc": "2026-01-02 15:32:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx35v2i",
          "author": "PrestigiousAnt3766",
          "text": "I have (amongst others)\n - az 104 (if you do infra)\n - databricks DE professional (if you do DE in DBR).\n\n\nNow DP203 is retired I feel ms doesnt have a good DE certification anymore. \n\n\nDp600/700 I feel were mostly useful if you go fabric (which I dont).",
          "score": 1,
          "created_utc": "2026-01-01 15:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx46e13",
          "author": "Usurper__",
          "text": "Just get the pro certs",
          "score": 1,
          "created_utc": "2026-01-01 18:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pvfe",
          "author": "Dylan_SmithAve",
          "text": "I'm mostly familiar with the AWS certifications. The data analytics/data engineering certs have been updated recently. Udemy courses and practice tests have proven to be super helpful in the past, so I always recommend going that route.\n\nCloud Practitioner and AI Practitioner would be solid to go for first. Then, you can work up towards the Generative AI Developer - Professional cert. That one is still in beta, so the available courses should improve over the next 6 months. Unfortunately the Data Analytics Specialty certification that I have is no longer available.\n\nThe Data Engineer - Associate exam could also be a good one to take if you need want another test before jumping into a professional level certification!",
          "score": 1,
          "created_utc": "2026-01-01 19:56:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5wkce",
              "author": "SupoSxx",
              "text": "What do you think about Solutions Associate and Professional for a Data Engineer?",
              "score": 1,
              "created_utc": "2026-01-01 23:43:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6qmib",
          "author": "eeshann72",
          "text": "Power bi",
          "score": 1,
          "created_utc": "2026-01-02 02:40:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q12ev1",
      "title": "Advent of code challenges solved in pure SQL",
      "subreddit": "dataengineering",
      "url": "https://clickhouse.com/blog/clickhouse-advent-of-code-2025",
      "author": "Creative-Skin9554",
      "created_utc": "2026-01-01 11:44:37",
      "score": 29,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q12ev1/advent_of_code_challenges_solved_in_pure_sql/",
      "domain": "clickhouse.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pw7kgg",
      "title": "Why do BI projects still break down over “the same\" metric?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pw7kgg/why_do_bi_projects_still_break_down_over_the_same/",
      "author": "Limp_Lab5727",
      "created_utc": "2025-12-26 15:57:50",
      "score": 24,
      "num_comments": 33,
      "upvote_ratio": 0.8,
      "text": "Every BI project I’ve worked on starts the same way. Someone asks for a dashboard. The layout gets designed, filters added, visuals polished. Only later do people realize everyone has a slightly different definition of the KPIs being shown.\n\nThen comes the rework. Numbers don’t match across dashboards. Teams argue about logic instead of decisions. New dashboards duplicate old ones with tiny variations. Suddenly BI feels slow and untrustworthy.\n\nAt the same time, going full metrics and semantic layer first can feel heavy and unrealistic for fast moving teams.\n\nCurious how others handle this in practice. Do you lock metric definitions early, prototype dashboards first, or try to balance both? What actually reduced confusion long term?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pw7kgg/why_do_bi_projects_still_break_down_over_the_same/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw1htjl",
          "author": "Tee_hops",
          "text": "My past 2 companies we had documentation available for every single metric. Including logic, tables used, who currently owns it, and what dashboards it appears on.\n\n                                              \nIt's a lot of CYA documentation but really reduced the chance of these errors.",
          "score": 18,
          "created_utc": "2025-12-26 16:09:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5w96y",
              "author": "LoaderD",
              "text": "They hiring? 😭\n\nOne of my previous roles was at a company like this and going back into a “oh ask Kathy why the kpi def is that”, company is brutal.",
              "score": 2,
              "created_utc": "2025-12-27 08:49:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7bmzh",
                  "author": "Tee_hops",
                  "text": "1 might be, I got laid off from the last one",
                  "score": 3,
                  "created_utc": "2025-12-27 15:32:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw21zpj",
          "author": "Wojtkie",
          "text": "At least in my experience, this happens when you have a lot of eager excel jockeys who export the data and start adding their own calculations on top of it. \n\nThose tend to be the ex-consultant crowd who also tend to be in strategy positions with seats at the table. They start basing strategy and the communications in business meetings use the new metric definition they came up with from exporting.\n\nWith enough time you now have a disconnect in the actual definition. A glossary won’t help you, no one will read it. \n\nIf you can’t tell, I’m a bit jaded on this topic",
          "score": 17,
          "created_utc": "2025-12-26 17:55:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw45z13",
              "author": "take_care_a_ya_shooz",
              "text": "Worked for these type of people and it was awful.\n\nOne highlight…the CEO/co-founder emails me with an urgent request for “raw data for all time” on invoices noting an audit in the next 24 hours. Mind you, this was during a holiday party that I missed only due to taking care of a sick pet at home.\n\nClarify some basics, add a few bullets on parameters, and send it over, boom, hero moment. \n\nNope. Data gets sent to the CoS, who either didn’t see or ignored the params, calculated the data incorrectly, then said the data I sent “was off”. I quickly found out how he got his numbers, replied with what was wrong, and noted that my initial email described this.\n\n2 months later, when I got canned, my boss noted this as an example (was a layoff but they cited performance as a factor). When I defended my work and said that I can’t be held liable for other people using raw data incorrectly, he said I just “had to know what they were asking for even if they don’t say it” and said that they wouldn’t read documentation since they don’t have time.\n\nAll three of them were fucking former consultants.",
              "score": 6,
              "created_utc": "2025-12-27 00:59:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1gdkp",
          "author": "Firm_Bit",
          "text": "The way you actually do this is by choosing a single person that is trusted to define the metrics. Usually the person that actually has to deal with/use that info to make actual decisions. You give em a daily/weekly update on the numbers. Sql -> slack message. That’s it. Double check your logic obviously. Then you let them propagate that number. If someone else comes to you you direct them to the first person. \n\nThen over time you add the unnecessary bells and whistles like visualizations.",
          "score": 39,
          "created_utc": "2025-12-26 16:01:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2gjbr",
              "author": "DeliriousHippie",
              "text": "This is good answer. Single source of truth for metrics, one person, excel, etc doesn't matter which. Before main values are really close to correct it's pretty pointless to do much visualizations. Sometimes you have to do layout and show it to test group with incorrect values, which isn't optimal and sucks, in this case you should label it clearly with TEST. When you show people new way to see their business they tend to concentrate to values and forget visualizations completely.\n\n'How does dashboard look and feel?'\n\n'Values are way off. KPI X was too high and KPI Y too low.'\n\n'But can we move forward with layout?'\n\n'I don't know, when these values are correct?'\n\nOne approach is to use mock up of layout with clear demo values and get first version approved. Then move to data model and correct values. Last part is final visualizations.",
              "score": 3,
              "created_utc": "2025-12-26 19:11:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3uybt",
              "author": "Cruxwright",
              "text": "We had that guy but then some consultants showed up.\n\n[https://www.youtube.com/watch?v=VayElJMD-lc](https://www.youtube.com/watch?v=VayElJMD-lc)",
              "score": 1,
              "created_utc": "2025-12-26 23:53:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw83dri",
              "author": "mathbbR",
              "text": "Having one person control all the metrics is just a coping mechanism. The cure is rigorous metrics definitions, consistent sources, and replicable methodologies.",
              "score": 1,
              "created_utc": "2025-12-27 17:53:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw84l3d",
                  "author": "Firm_Bit",
                  "text": "There’s a diff between controlling and initializing. You can’t have a lot of people/departments all starting the same thing at once.",
                  "score": 1,
                  "created_utc": "2025-12-27 17:59:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw24jm2",
          "author": "Illustrious_Web_2774",
          "text": "You need someone, or a group of people who has authority over how metrics are calculated. Iterate thoroughly the calculations with them before any visualization work.\n\n\nLocking metrics or models and what not doesn't help. If the metrics are wrong, they will be reworked no matter it's locked or not.",
          "score": 6,
          "created_utc": "2025-12-26 18:09:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3o110",
          "author": "fauxmosexual",
          "text": "This is only a problem when you skip the data work of modelling and validating because you're focused on the immediate ask of the dashboard. If you start at the flashy end-user part of the work, yes you are always going to be stuck doing really expensive rework down the track, while completely destroying confidence and enthusiasm from your business users.\n\n\nYou can easily have the data definition argument before you've delivered the dashboard by getting the data in front of the people who use it (who generally aren't the people asking for the dashboard). The first thing I prototype is the semantic model with a few top level KPIs and whatever is the minimum that allows them to engage with that. Usually connecting the semantic model to Excel does the trick, otherwise a bare-bones dashboard that displays just an auditable view of the KPI.\n\n\nIt's tempting to skip the data work and go straight to the dashboarding because it feels more agile to get the bells and whistles delivered, but ultimately it costs more in rework and burned credibility and clarity that it saves. It might feel heavy and unrealistic to do the modelling early, but that's because you're caught in the sugar rush of delivering good-looking, quick, but ultimately poor solutions.",
          "score": 3,
          "created_utc": "2025-12-26 23:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj0inp",
          "author": "tedx-005",
          "text": "I think this is an inevitable consequence of the “self-service BI” fad from 2020–2022. Every vendor rode that wave, and the race became who can make it easiest to build a report, but that optimizes for a symptom, not the root problem. As more people build reports, each new question gets answered by creating yet another dashboard or a slightly different metric variant, doesn't matter which tool you use because most analytics systems are designed in this way that makes duplication almost inevitable, and it's a ticking bomb because over time the whole thing collapses under its own weight.\n\nI’ve been in both setups. In one org, I basically churned out reports on request, where speed was the currency. Stakeholders loved it and tbh it made things easier when it came time to ask for a promotion because output was highly visible. But as the organization grew, the same metrics started showing different numbers across dashboards and eventually we had to go back and pay down the debt we’d accumulated. in my current org, we do the opposite where we lock metric definitions early and invest heavily in the semantic layer. This also means we have to define business logic as code and build composable metrics early on so new questions build on existing definitions instead of spawning new dashboards, new models, and new forks of truth. We also document everything btw. \n\nThis approach is much slower (and I kinda hate it sometimes), and it’s hard to get buy-in unless the company has a certain level of maturity. Governance is hard to sell when the benefits are mostly “nothing breaks” and “people argue less.” You’ll get challenged on why it takes longer, especially by stakeholders who just want a chart for next week. But long term it’s the only approach I’ve seen that consistently reduces confusion and rebuilds trust because it makes reuse the default and makes metric drift harder to happen. So it does pay off, but it takes time and in my case, a thoughtful, forward-looking boss to make it happen.",
          "score": 3,
          "created_utc": "2025-12-29 11:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw262fb",
          "author": "Headband6458",
          "text": "It's important to define the requirements before you do the work to build the dashboard. You're going to have to do it either way, but if you do it ahead of time you can iterate with something as simple as a text document instead of something as complex as a finished dashboard. Get one of the stakeholders to define the KPIs, the data sources used in the calculations, and the calculations themselves. You might have to talk to several of them, but you need to get at least one of them to define each thing. Then put it all together and get all of the stakeholders to sign off on it. Then build the dashboard. If they want changes after the fact, bring out the final docs and make the changes there first, get them approved there first. Then change the dashboard.",
          "score": 2,
          "created_utc": "2025-12-26 18:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2khru",
          "author": "painteroftheword",
          "text": "Thats why I have a page of definitions in my reports. List of outputs, what they show, how they're calculated, inclusions and exclusions etc...\n\nIn my company we're gradually working through KPI reports, building centrally signed off reports and decommissioning the old ones.",
          "score": 2,
          "created_utc": "2025-12-26 19:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw33wba",
          "author": "Nateorade",
          "text": "Sales counts ARR by opportunities closed by sales people. \n\nFinance counts ARR as opportunities closed by reps AND consumer self serve purchases. \n\nBoth put “ARR by Month” on their chart. \n\nWho’s right?",
          "score": 2,
          "created_utc": "2025-12-26 21:18:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw37v77",
              "author": "mattiasthalen",
              "text": "Neither, they should put ”Sales ARR by Month” & ”Financial ARR by Month” on the charts.",
              "score": 1,
              "created_utc": "2025-12-26 21:40:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw38d6t",
                  "author": "Nateorade",
                  "text": "I’ve yet to see a place where that’s incentivized or governed. Finance makes charts separately from sales and publishes them in their own dashboards. \n\nIt sounds like a lot of hard work without a ton of value to be the chart police across departments?\n\nBoth are correct in their own way.",
                  "score": 3,
                  "created_utc": "2025-12-26 21:43:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6n45i",
          "author": "Fit-Feature-9322",
          "text": "I’ve seen this a lot. Dashboards surface problems fast, but metrics are what actually fix them. What worked for us was agreeing on a small set of core metrics first and letting dashboards sit on top. We use Domo, and having metric logic centralized stopped the endless why doesn’t this number match debates.",
          "score": 2,
          "created_utc": "2025-12-27 12:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweppcq",
              "author": "jessikaf",
              "text": "yeah this hits, getting alignment on metrics first saves so many pointless arguments later, dashboards just expose the mess faster. centralizing the logic makes a huge difference once more people start looking at the same numbers. way more sustainable than constantly patching charts.",
              "score": 1,
              "created_utc": "2025-12-28 18:58:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1l42r",
          "author": "snarleyWhisper",
          "text": "You are focusing on the end result the dashboard - what you need is a good data model. What you are able to build a data model around is the intersection between :\n\n1- how does your business work \n\n2- how does that align with data in source systems \n\nThis will help to create definitions when thinking about it from a process and KPI perspective. If you need help running a workshop to get good KPIs I’ve had good luck with a modified version of this approach  https://www.atlassian.com/team-playbook/plays/goals-signals-measures",
          "score": 2,
          "created_utc": "2025-12-26 16:26:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1hz90",
          "author": "kenflingnor",
          "text": "It's because of people",
          "score": 1,
          "created_utc": "2025-12-26 16:10:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2r6hc",
          "author": "Skualys",
          "text": "On my side I always start with : lets define the metrics, I sent table extract, check it. Only one person responsible of the définition with questions on what we measure / what are the decisions to make.\n\nAll what is related to the design of the dashboard is the easy part. So we see that later.",
          "score": 1,
          "created_utc": "2025-12-26 20:09:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3hg8p",
          "author": "Nearby_Fix_8613",
          "text": "I have never been on a BI/Analytics/DS project that did not define the logic/metrics first \n\nWe do this as part of the data model design and if we can’t agree on the above, it means there is not a strong enough understanding of the business and we don’t built anything until we do",
          "score": 1,
          "created_utc": "2025-12-26 22:33:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5a7e8",
          "author": "PhantomSummonerz",
          "text": "Been through this because departments couldn't agree on what \"revenue\", \"sales\" or whatever really was (hell, even management couldn't and changed terms frequently).\n\nA semantic layer is the exact solution to your problem. Either through a tool like cube.dev or plain agreement notes on a paper.",
          "score": 1,
          "created_utc": "2025-12-27 05:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5g687",
          "author": "semantics_epsacon",
          "text": "1. Get the definition of the KPI from the person who requests it\n\n2. Go to the people who understand the raw data and discuss where to find the components of the KPI\n\nThe person who needs a KPI usually does not understands the complexities of source data. There are always complex definitions what to include, and what to exclude. On top of that there are almost certainly edge cases that need to be excluded/included.\n\n  \nThis is especially true if you deal with source systems like SAP. There is a ton of semantics already in the SAP system you need to understand before you can select the right data for your KPIs.\n\nIn a scenario like that 3 roles should be involved: 1. Business defines the KPI requirement, 2. SME who understands semantics of source data and 3. data engineer.",
          "score": 1,
          "created_utc": "2025-12-27 06:19:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7ozwc",
          "author": "Gators1992",
          "text": "In my experience the semantic layer is super useful to avoid these kinds of situations despite the extra work up front.   Getting the initial requirements and buy off is always a PITA and benefits greatly from some sort of structure and leadership support.  Afterward though the users are all playing with the same curated objects out of the model, so the results are consistent.  If they do their own metrics on top of that, it's on them to stand behind that measure.  \n\nWe are currently standing up some new data sources that has some executive pressure to get it done.  One guy on the consuming team was assigned to lead it and did a bunch of wireframes, but without definitions.  We told them basically that we aren't building anything beyond a layout until they help us develop the models.  They know the business and how they manage it, so they need to do the thinking about this.  Also helps if you take a structured approach toward this like regular meetings, goals, templates and the right stakeholders in the room.",
          "score": 1,
          "created_utc": "2025-12-27 16:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwao1j7",
          "author": "curiosickly",
          "text": "Be the source!  Have all the metrics in your semantic layer and have people directly connect to it.  Everyone's stuff refreshes at the same time and they're all on the same page.",
          "score": 1,
          "created_utc": "2025-12-28 02:20:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdyu3m",
          "author": "jessicalacy10",
          "text": "feels like it's usually not the tech that breaks the things it's unclear goals, shifting requirements and data that wasn't ready in first first place. everyone wants insights but nobody agrees on what that means until way too late.",
          "score": 1,
          "created_utc": "2025-12-28 16:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwehjr8",
          "author": "SaintTimothy",
          "text": "Term governance is a thing. Making the business use more words to describe a thing when there are 6 different versions of the thing. Report splash page with term definitions, tool tips, header info...\n\n\nMaking that final card number the byproduct of the underlying report (clickthrough-able in order to drill into the details what make up that number).\n\n\n*edit- second what firm_bit said. Have one product owner who owns the definitions for that report.",
          "score": 1,
          "created_utc": "2025-12-28 18:20:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwwwf4",
      "title": "What parts of your data stack feel over-engineered today?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pwwwf4/what_parts_of_your_data_stack_feel_overengineered/",
      "author": "AMDataLake",
      "created_utc": "2025-12-27 12:39:30",
      "score": 23,
      "num_comments": 20,
      "upvote_ratio": 0.83,
      "text": "What’s your experience? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pwwwf4/what_parts_of_your_data_stack_feel_overengineered/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw6s8el",
          "author": "Quaiada",
          "text": "**Wasting time worrying about vendor lock-in**\n\n**Pursuing 100% automated CI/CD when the team has only one or two people, and there is still no valuable product in place**\n\n**Trying to build a metadata-driven framework that is more complex than simply using SQL**\n\nUsing a complex big data stack to support simple, small datasets that could easily be handled by a cheap, traditional SQL database",
          "score": 88,
          "created_utc": "2025-12-27 13:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6xgur",
              "author": "GachaJay",
              "text": "Help me understand the 3rd. Wouldn’t that save you a ton of time in ingestion of new sources as well as keeping a decent level of documentation?",
              "score": 7,
              "created_utc": "2025-12-27 14:09:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6znd6",
                  "author": "Quaiada",
                  "text": "In my experience, we always try to abstract complexity in data ingestion, and this works well until the day you realize the code doesn’t work for all cases — especially when you aim for a single piece of code to handle multiple sources.\n\nExample:\n\nYou have a source X with tables A, B, and C. So far, everything works fine — your framework supports all three tables. One day you discover that the framework doesn’t support table D. You improve the framework, and then you discover that table E also doesn’t work. This behavior becomes even more frequent when the data source is outside your domain (for example, a third-party API where you have no control over change management or even proper release notifications). In order not to impact operations, delivery speed becomes very tight, and the framework’s quality starts to degrade more and more.\n\nThen one day a Source Y appears, with a completely different logic… and the team has the idea of aggregating all the logic of the new source into the same framework used for Source X.\n\nI’m specifically talking about data ingestion, where metadata-driven approaches can actually work quite well.\n\nThe problem is when people later have the “brilliant” idea of building a framework for the Gold layer, for example… and before you know it, the team is basically building a new ORM — full of flaws and limitations — far more complex than simply having custom SQL code for each scenario.\n\n  \n\\---------------------\n\nYou mentioned documentation?\n\nHere, people don’t read documentation… and on top of that, it’s almost always incomplete and poorly written.\n\nDocumentation itself has also become a very controversial topic nowadays, often bordering on “overengineering.”",
                  "score": 11,
                  "created_utc": "2025-12-27 14:22:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwanfm",
                  "author": "empireofadhd",
                  "text": "It depends on your source. APIs, events etc can be very complex but if it’s just straight CSV, JSOn or sql databases a framework is the easiest route. \n\nI’d say framework for basic cases and custom code for the rest.",
                  "score": 1,
                  "created_utc": "2025-12-31 11:07:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw90xf1",
              "author": "HarskiHartikainen",
              "text": "This. I've seen this so many times. Metadata-driven framework that is so complicated to use that everybody hates it in the end.\n\nAlso creating some kind of abstract platform that is not vendor-locked so it is easy to move to another tech platform. Times I've seen something moved as is? Zero.\n\nThis usually happens when customers give too much money to us engineers and we start building the platform to us, not for them. Yes it might be technically correct to run everything through 100% CICD with completely metadata-driven abstract code but usually it ends up being too complicated and it starts to break down immediately when the first dev uses a shortcut to create something quickly for the customer.\n\nImo Data Vault 2.0 is the pinnacle of over-engineering when it comes to data modeling. It has some good design patterns but if you do it by the book, you are basically preparing all kinds of scenarios that come up so rarely that it is not worth it.\n\nI've been on this field for 20 years and I've seen lots of shit like basically Power BI being recreated as some kind of metadata-driven monstrosity.",
              "score": 3,
              "created_utc": "2025-12-27 20:48:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7qtiw",
              "author": "ColdPorridge",
              "text": "I don’t know your CI/CD situation but lack of CI/CD is a such a common root cause of issues that I wouldn’t skip it even if you feel like having 1-2 people doesn’t warrant it. \n\nI have a bunch of misc side projects where I’m the only dev and the only way I can remember how the deployments work is because it’s codified in CI/CD. ",
              "score": 3,
              "created_utc": "2025-12-27 16:49:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7uo5s",
                  "author": "Quaiada",
                  "text": "I agree with you that CI/CD is necessary and saves us from many problems.\n\nThe point is that sometimes the team has the perception that CI/CD must be a 100% automated flow, covering every aspect of build and deploy, unit tests, quality checks with multiple alerts, and heavy automation (over-engineered from the topic), when, at the beginning, a simple Git setup with a PR flow and a few manual steps for deployment would already be sufficient.\n\nIn fact, nowadays, in my view, CI/CD is much more a cultural topic than a coding one.\n\nI also believe that when CI/CD reaches a deeper level (when it truly makes sense), the ideal scenario is to have a dedicated, qualified professional focused on this topic, such as a DevOps engineer. A Data Engineer is not a DevOps engineer.",
                  "score": 4,
                  "created_utc": "2025-12-27 17:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw80xdw",
              "author": "mortal-psychic",
              "text": "All if these are true if you are generating less than 1gb per day. That too only for few years",
              "score": 1,
              "created_utc": "2025-12-27 17:41:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6w5uy",
          "author": "Acrobatic_Intern3047",
          "text": "All of it. Every company I worked at could’ve gotten by with nothing but SQL and a few Python scripts.",
          "score": 30,
          "created_utc": "2025-12-27 14:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwddawf",
              "author": "asilverthread",
              "text": "If most companies actually modeled data properly, and wrote better SQL, half of the data tools out there simply wouldn’t exist.",
              "score": 5,
              "created_utc": "2025-12-28 14:57:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw79r6j",
          "author": "Firm_Bit",
          "text": "I used to want the whole modern data platform thing and built it at 2 companies. \n\nMy latest job is super lean. Cron, Python scripts, sql, Postgres. \n\nSo now I think most systems are over engineered. People throw money, compute, and storage at problems instead of squeezing performance out of the basic tools and focusing on the actual business.",
          "score": 10,
          "created_utc": "2025-12-27 15:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw87r25",
              "author": "umognog",
              "text": "It really depends upon service spread & accountability.\n\nIf you have a small team and take care of a lot more than a small team should do over a number of services - say kafka, postgres, hadoop, oracle & from csv by ftp & email drops along with api requests, you kind of need a set of services to perform the management & alerting for you to avoid being caught with your pants down.",
              "score": 1,
              "created_utc": "2025-12-27 18:15:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwa5ofz",
          "author": "NoleMercy05",
          "text": "The Scrum Pipeline for sure. Over engineered and completly broken.\n\nBad data everywhere with conflicting rules if they exist.",
          "score": 6,
          "created_utc": "2025-12-28 00:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw75ing",
          "author": "AlGoreRnB",
          "text": "Probably a lot of it tbh. But when the priority from leadership is on scalability, the worst thing to do is spend forever thinking/talking about the optimal solution. In reality there are too many tools that will scale really well and too many variables when looking at a 10+ year time horizon to know for sure what I’ve over-engineered. I’d rather pick a stack quickly where the price is right and the technology is there so I can start building as opposed to spending a great deal of time over-analyzing.",
          "score": 3,
          "created_utc": "2025-12-27 14:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwczg6a",
          "author": "Qkumbazoo",
          "text": "Wasting time setting up clusters and horizontally scaling when simply adding ram, storage, and cpu would solve 90% of bottlenecks.",
          "score": 1,
          "created_utc": "2025-12-28 13:31:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd05zv",
          "author": "tiacay",
          "text": "If the engineers doing the job just right, there will be needed less engineers. It's not even something most engineers intended to do, but the supply and demand drive it that way.",
          "score": 1,
          "created_utc": "2025-12-28 13:36:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcujr2",
          "author": "dbplatypii",
          "text": "All of it. Whyyy is so much of the data engineering stack dependent on the JVM 😭",
          "score": -1,
          "created_utc": "2025-12-28 12:56:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz62eq",
      "title": "What dbt tools you use the most?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pz62eq/what_dbt_tools_you_use_the_most/",
      "author": "SnooGiraffes7113",
      "created_utc": "2025-12-30 02:36:20",
      "score": 23,
      "num_comments": 20,
      "upvote_ratio": 0.75,
      "text": "I use dbt on a lot on various client projects. It is certainly a great tool for data management, in general. With introduction of fusion, catalog, semantic model, insights, it is becoming an all stop shop for ELT. And along with Fivetran, you are succumbing to the Fivetran-dbt-snowflake/databricks ecosystem (in most cases; there would also be uses of AWS/GCP/Azure).\n\nI was wondering what dbt features do you find most useful? What do you or your company use it for, and along with what tools? Are there some things that you wished were present or absent?\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pz62eq/what_dbt_tools_you_use_the_most/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwnw7xe",
          "author": "financialthrowaw2020",
          "text": "None of it. We use core. The core features are all we need.",
          "score": 83,
          "created_utc": "2025-12-30 02:53:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoa1el",
              "author": "MachineParadox",
              "text": "Yep core here, generating the documentation/dag is vital though",
              "score": 8,
              "created_utc": "2025-12-30 04:13:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwob7i8",
                  "author": "SnooGiraffes7113",
                  "text": "On our testing, the data column lineage from dbt misses solve connections, due to it using referential checks and sql parsing. The joins and filter miss out.",
                  "score": -2,
                  "created_utc": "2025-12-30 04:20:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwp1kjg",
          "author": "dataflow_mapper",
          "text": "For me the core value is still pretty boring stuff: models, tests, and documentation. Having transformations versioned, reviewable, and testable in SQL is what actually sticks. Everything else feels additive, but not always essential.\n\nWe mostly use dbt for the transformation layer only, sitting on top of Snowflake or Databricks, with ingestion handled elsewhere. Tests and freshness checks punch way above their weight, especially for client work where trust in the data matters more than fancy metrics layers. Lineage and docs also get used more than people admit once teams grow.\n\nThe newer features are interesting, but I have seen mixed adoption. Semantic layer sounds great, but many teams already solved that in BI tools or code. Sometimes it feels like dbt is trying to be the control plane for the whole stack, which is nice in theory but adds cognitive load. I mostly wish they focused on making the core workflow faster and simpler rather than expanding surface area.",
          "score": 9,
          "created_utc": "2025-12-30 07:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpdgbm",
          "author": "sleeper_must_awaken",
          "text": "Only core. We pushed DBT Cloud to our clients until two years back. Then got scared of their sales tactics and decided to move to DBT Core only in our consultancy.",
          "score": 9,
          "created_utc": "2025-12-30 09:34:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo8oke",
          "author": "git0ffmylawnm8",
          "text": "dbt Core. Trying to get more into using cosmos",
          "score": 5,
          "created_utc": "2025-12-30 04:05:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoh3ax",
              "author": "ps_kev_96",
              "text": "I have an article on how I got to using cosmos for a quick headstart , let me know if you need any help",
              "score": 1,
              "created_utc": "2025-12-30 04:59:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqk914",
                  "author": "milkwinner",
                  "text": "Do you mind sharing the article here?",
                  "score": 5,
                  "created_utc": "2025-12-30 14:47:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwoxh60",
          "author": "soltiamosamita",
          "text": "dbt-core, and then some python scripts for documentation/partial overwrite of incrementals/replication/browsing when the vscode extension breaks",
          "score": 5,
          "created_utc": "2025-12-30 07:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo4oog",
          "author": "anatomy_of_an_eraser",
          "text": "dbt core + dbt external tables",
          "score": 6,
          "created_utc": "2025-12-30 03:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoyoj6",
          "author": "Walk_in_the_Shadows",
          "text": "Struggling to find justification for Cloud. We have solid Infrastructure and DevOps setup internally. We don’t want Catalog, or Semantic models, or Canvas. \n\nHowever, we could do with the Mesh functionality and what they are selling as State Aware Orchestration. \n\nDoes anyone have experience of replicating these in Core?",
          "score": 3,
          "created_utc": "2025-12-30 07:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp1rph",
              "author": "wallyflops",
              "text": "I've just rolled groups out on core and moj\ndel versions are there so you can hand roll mesh I think",
              "score": 1,
              "created_utc": "2025-12-30 07:45:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwozz60",
              "author": "SnooGiraffes7113",
              "text": "Mesh and state aware not on core, to my knowledge. And also depend on your plan. However, you can get state aware using, freshess, defer and state: modified on your pipelines.",
              "score": -3,
              "created_utc": "2025-12-30 07:28:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp06wa",
          "author": "Jegan__Selvaraj",
          "text": "We mostly use dbt for models, tests, and documentation because it helps teams stay consistent as things scale. It fits well after ingestion tools like Fivetran, usually on Snowflake or Databricks. What I still wish for is simpler debugging and better visibility when things break since thats where teams lose the most time.",
          "score": 3,
          "created_utc": "2025-12-30 07:30:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqfezx",
          "author": "UltraInstinctAussie",
          "text": "I plan to try dbt jobs in Fabric when I return from holidays. ",
          "score": 1,
          "created_utc": "2025-12-30 14:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqpi18",
          "author": "updated_at",
          "text": "codegen (im just lazy)",
          "score": 1,
          "created_utc": "2025-12-30 15:14:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwmiz8",
              "author": "Ok-Sprinkles9231",
              "text": "It's an interesting one but unfortunately it doesn't seem to be stable. We have a lot of undocumented DBT models, missing columns etc. I was looking for a way to generate columns alongside their types automatically based on the target, which brought me to codegen but couldn't get anything out of it and eventually gave up.",
              "score": 2,
              "created_utc": "2025-12-31 12:46:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nws6mdi",
          "author": "Geraldks",
          "text": "stick to basic, core + kubernetes. for other functionalities, we go for external tools or existing stacks.",
          "score": 1,
          "created_utc": "2025-12-30 19:23:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt9bq7",
          "author": "Spookje__",
          "text": "I was pushing for DBT cloud, but core meets our needs. The cloud pricing is too much for what it offers. And I have doubts about the recent course.",
          "score": 1,
          "created_utc": "2025-12-30 22:28:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzkxi4",
      "title": "At what point does historical data stop being worth cleaning and start being worth archiving?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pzkxi4/at_what_point_does_historical_data_stop_being/",
      "author": "Queasy-Cherry7764",
      "created_utc": "2025-12-30 15:27:07",
      "score": 21,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "This is something I keep running into with older pipelines and legacy datasets.  \n  \nThere’s often a push to “fix” historical data so it can be analyzed alongside newer, cleaner data, but at some point the effort starts to outweigh the value. Schema drift, missing context, inconsistent definitions… it adds up fast.  \n  \nHow do you decide when to keep investing in cleaning and backfilling old data versus archiving it and moving on? Is the decision driven by regulatory requirements, analytical value, storage cost, or just gut feel?  \n  \nI’m especially curious how teams draw that line in practice, and whether you’ve ever regretted cleaning too much or archiving too early. This feels like one of those judgment calls that never gets written down but has long-term consequences.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pzkxi4/at_what_point_does_historical_data_stop_being/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwqujuj",
          "author": "financialthrowaw2020",
          "text": "It's based on the user requirements. That's it. \n\n\nDoes the executive team want a dashboard that shows revenue over time from the inception of the company? Yes? Then all data feeding that dashboard must be clean and reconciled. \n\nDoes the user want a dashboard only tracking the last 7 years? Great, then we set our policies based on that and keep the raw data in its original form until the requirements change and we can easily adapt the old data to the new. It's really that simple.",
          "score": 36,
          "created_utc": "2025-12-30 15:39:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwquntd",
          "author": "ZirePhiinix",
          "text": "You don't.\n\nYou get the specs down and translate it to a cost in dollars. Unless you learn how to do that, \"expensive\" is literally not their problem.\n\nIf they see that accessing historical data for this report is costing them half a million each year, but the revenue tied to it is a fraction of that, then maybe they can learn to let it go.",
          "score": 15,
          "created_utc": "2025-12-30 15:40:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqx1o1",
          "author": "exjackly",
          "text": "As others have said, that is a business not a technical decision.  If the business wants the data enough to pay for cleaning old data and keeping it available, we clean and keep it available.\n\nIn practice, I look for migration dates.  When the business moves from one system to a replacement, the quality of the data takes a jump, and anything before that is only useful short term.  Find that date, and show them the issues from the old data; and if you can show how much it is costing to process and keep that data, it gets easier to convince them to archive and start working towards data destruction.\n\nThe company having a data retention policy helps too, as you can tie in to the definitions there.  Even better if it is regulated data with mandated destruction timelines.\n\nBut, you have to know your numbers, and understand their needs and requirements enough to make that proposal.\n\nHonestly, old data is so seldom used that by the time data is more than 2-3 years old, (most industries, not all) only aggregates ever get used, and even those drop off quickly after 5-7 years.  I've never been in a situation where we've wanted to unarchive data.",
          "score": 9,
          "created_utc": "2025-12-30 15:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrkgtg",
          "author": "klumpbin",
          "text": "After 17 years, 3 months and 12 days",
          "score": 6,
          "created_utc": "2025-12-30 17:41:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqvtte",
          "author": "Striking_Meringue328",
          "text": "The real question is what's the business case for fixing old data? Can you put a figure on the expected benefits, and does it outweigh the likely cost?",
          "score": 3,
          "created_utc": "2025-12-30 15:45:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrsx0w",
          "author": "LargeSale8354",
          "text": "Legal requirements to retain. Process requirements to fulfil legal requirements. For example,  financial regulations in the UK require companies to keep 7 years of transactions. This does not necessarily mean online, but GDPR does say that subject access requests must be completed in 30 days.\nIf you archive stuff, rehearse the process of getting it back and do dry runs every quarter. \n\n\nUser requirements come next. As people have said, make sure you put the cost in front of them. If you don't you risk being the provider of free food for an ever growing horde.\n\nWhat is your definition of fixing the data? I'm always wary of the line between fix and falsify.",
          "score": 3,
          "created_utc": "2025-12-30 18:20:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr8o0p",
          "author": "OkToe2355",
          "text": "based on use case",
          "score": 2,
          "created_utc": "2025-12-30 16:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtaqaa",
          "author": "pauloliver8620",
          "text": "depends on seasonal events, eg from sport tournaments, world cup is held every 4 years, olimpics every 4 years etc if you have similar events business might be interested in comparing then chose the seasonality that matches your requirements. As everyone else mentioned explain the cost of keeping the data so that business can put a price tag on it and see if they can make more out of it.",
          "score": 2,
          "created_utc": "2025-12-30 22:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwunpry",
          "author": "Firm_Bit",
          "text": "Wdym? Depends on the use case. If looking at years of data opens a $x opportunity and it only costs your sanity + less than $x then it’s worth doing.",
          "score": 1,
          "created_utc": "2025-12-31 03:10:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2m5al",
          "author": "gelato012",
          "text": "2 years but the business has the say.",
          "score": 1,
          "created_utc": "2026-01-01 12:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqwmeb",
          "author": "Alternative-Guava392",
          "text": "Personally, Rule of thumb : 3 years. \n\nIf business still insists on old data, get the cost + effort vs returns in terms of dollars. \n\n\"It will cost 5000$ / 30 hours to fix it. Is that an issue ?\"",
          "score": 1,
          "created_utc": "2025-12-30 15:49:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pygk5v",
      "title": "Simple ELT project with ClickHouse and dbt",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pygk5v/simple_elt_project_with_clickhouse_and_dbt/",
      "author": "AverageGradientBoost",
      "created_utc": "2025-12-29 08:03:40",
      "score": 20,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "I built a small [ELT PoC](https://github.com/Connorrmcd6/fpl-elt) using ClickHouse and dbt and would love some feedback. I have not used either in production before, so I am keen to learn best practices.\n\nIt ingests data from the Fantasy Premier League API with Python, loads into ClickHouse, and transforms with dbt, all via Docker Compose. I recommend using the provided Makefile to run it, as I ran into some timing issues where the ingestion service tried to start before ClickHouse had fully initialised, even with `depends_on` configured.\n\nAny suggestions or critique would be appreciated. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pygk5v/simple_elt_project_with_clickhouse_and_dbt/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwm7ocq",
          "author": "nxt-engineering",
          "text": "*Hello, I had a look on your project and here's my take,* \n\n# Project setup\n\nI really like your development setup :\n\n* **Docker, Docker Compose, Makefile**\n\nThis makes onboarding and local development super smooth.\n\n* `dbt.log` should probably be added to `.gitignore` (usually not something you want tracked in git).\n\n# Dependencies\n\n* In `requirements.txt`:  pin **dependencies versions** (e.g. `package==x.y.z`) to improve reproducibility and avoid unexpected breaking from packages releases.\n\n# Transformation (dbt)\n\n* When querying raw/source data, dbt best practice is to define sources in `source.yml` and reference them with `{{ source('raw', 'table_name') }}`  This allows lineage, testing and documentation.\n* Try to avoid `SELECT *`. Not a big deal for a small/solo project, but at scale it can introduce unintended changes if upstream schemas evolve (new columns, renamed columns, type changes) and can impact downstream models silently.\n* Your models appear to be **full refreshes** (not incremental), meaning tables are recreated on each `dbt run`. In that setup you generally don’t need: `now() as last_updated`   to track row creation/update date This information is already available through **Clickhouse** **system table** called `system.parts`.\n\nOverall: not a complete end-to-end project yet (missing orchestrator and probably lots of other bricks !), but the foundation and structure are solid, nice work setting this up !",
          "score": 4,
          "created_utc": "2025-12-29 21:29:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwopp73",
              "author": "AverageGradientBoost",
              "text": "Thank you for such a detailed response! Regarding the \\`source.yml\\` file, I'm assuming it just sits in the models directory since there is no raw directory?\n\nAnd that point about using the system tables in clickhouse is also really interesting, I didn't even think about looking in those tables",
              "score": 2,
              "created_utc": "2025-12-30 06:01:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwq4b9t",
                  "author": "nxt-engineering",
                  "text": "You're welcome !  \nAs for your question **: dbt** is very flexible with file organization and can find your `.yaml` files anywhere. A practice I recommend is placing them in a dedicated `models/raw/` folder for clarity. This way, you have:\n\n* `models/raw`\n* `models/staging`\n* `models/intermediate`\n* `models/marts`\n\nThis structure clearly represents the flow of data from raw to marts.",
                  "score": 1,
                  "created_utc": "2025-12-30 13:15:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwlrj41",
          "author": "Orthaxx",
          "text": "Hey ! That looks pretty cool, congrats on building it !",
          "score": 2,
          "created_utc": "2025-12-29 20:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoppzm",
              "author": "AverageGradientBoost",
              "text": "thank you!",
              "score": 1,
              "created_utc": "2025-12-30 06:01:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpbbe1",
          "author": "soltiamosamita",
          "text": "\\- you don't have to create databases besides \"raw\", dbt-clickhouse will do that for you\n\n\\- consider moving repeated configs (e,g, materialized=table) into dbt\\_project.yml, like you already did for the \"schema\" property which you can remove from the model's config(). Generally, when you will be managing dozens of models in one folder, you'd want to avoid any surprise in-model configs that you forget about",
          "score": 1,
          "created_utc": "2025-12-30 09:13:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq103j",
          "author": "Difficult-Bag1550",
          "text": "its not terrible but seems vibe-coded to me \n- would add ingestion code unit tests  \n- different python version across dockerfiles",
          "score": 1,
          "created_utc": "2025-12-30 12:53:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py90n4",
      "title": "[EU] 4 YoE Data Engineer - Stuck with a 6-month notice period and being outpaced by new-hire salaries. Should I stay for the experience?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1py90n4/eu_4_yoe_data_engineer_stuck_with_a_6month_notice/",
      "author": "Remarkable_Result596",
      "created_utc": "2025-12-29 01:42:32",
      "score": 19,
      "num_comments": 30,
      "upvote_ratio": 0.91,
      "text": "Hi All,\n\n​Looking for a bit of advice on a career struggle. I like my job quite a lot—it has given me learning opportunities that I don’t think would have materialized elsewhere—but I’ve hit some roadblocks.\n\nThe Context\n\n​I’m 26 and based in the EU. I have a Master’s in Economics/Statistics and about 4 years of experience in Data (strictly Data Engineering for the last 2).\n​My current role has been very rewarding because I’ve had the initiative to really expand my stack. I’m the \"Databricks guy\" (Admin, Unity Catalog, PySpark, ...) within my team, but lately, I’ve been primarily focused on building out a hybrid data architecture. Specifically, I’ve been focusing on the on-premise side:\n\n​Infrastructure: Setting up an on-prem Dagster deployment on Kubernetes. Also django based apps, POCing tools like OpenMetadata.\n\n​Modern Data Stack (On-prem): Experimenting with DuckDB, Polars, dbt, and dlthub to make our local setup click with our cloud environments (Azure/GCP/Fabric, onprem even).\n\n​Upcoming: A project for real-time streaming with Debezium and Kafka. I’d mostly be a consumer here, but it’s a setup I really want to see through. Definitely have a room impact the architecture there and downstream.\n​\nThe Problem\n\n​Even though I value the \"builder\" autonomy, two things are weighing on me:\n\n​The Salary Ceiling: I’m somewhat bound by my starting salary. I recently learned that a new hire in a lower position is earning about 10% more than me. It’s not a massive gap, but it’s frustrating given the difference in impact. My manager kind of acknowledges my value but says getting HR to approve a 30-50% \"market adjustment\" is unlikely.\n\n​The 6-Month Notice: This is the biggest blocker. I get reach-outs for roles paying 50-100% more and I’ve usually done well in initial stages, but as soon as the 6-month notice period comes up, I’m effectively disqualified. I probably can't move unless I resign first.\n\n​The Dilemma\n\n​I definitely don’t think I’m an expert in everything and believe there is still a whole lot of unique learning to squeeze out of my current role, and I would love to see this through. I’m torn on whether to:\n​Keep learning: Stay for another year to \"tie it all together\" and get the streaming/Kafka experience on my CV.\n​Risk it: Resign without a plan just to free myself from the 6-month notice period and become \"employable\" again.\n​Do you think it's worth sticking it out for the environment and the upcoming projects, or am I just letting myself be underpaid while my tenure in the market is still fresh?\n\n\n​TL;DR: 4 YoE DE with a heavy focus on on-prem MDS and Databricks. I have great autonomy, but I’m underpaid compared to new hires and \"trapped\" by a 6-month notice period. Should I stay for the learning or quit to find a role that pays market rate?\n\n\nEDIT:\nThanks for all the feedback. I think quitting materialized as the best move I can make given the circumstances. After looking into it, the 6-month notice period on a standard employment contract seems to be a significant gray area. Under local law, contract terms generally cannot be worse for the employee than what is written in the national statutes (which would normally be 1 month for my length of service). However, custom arrangements are possible, and there is a chance the company’s version is legally valid, meaning I might be stuck with it.\n\n​My plan: I am not making any moves yet. I am going to consult with the National Labor Inspectorate and a legal expert to get a formal opinion. I need to know if this clause is actually enforceable or if it would be thrown out of court.\n\n​If the 6 months is likely valid: I will probably resign immediately to \"start the clock\" so I can be free to look for a new job sooner.\n\n​If it is likely invalid: I will start applying for jobs like a normal human being, knowing I can legally leave much earlier.\n\n​I don’t want to risk a lawsuit or a permanent mark on my official employment record for \"abandoning work\" without being 100% sure where I stand.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1py90n4/eu_4_yoe_data_engineer_stuck_with_a_6month_notice/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwpccdv",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-30 09:23:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi9jhj",
          "author": "Hear7y",
          "text": "I've no solution, but a 6-month notice is insane and I have no clue why you'd have agreed to something like that. It makes switching jobs nigh impossible, you need to be on notice and start looking in the last 2 months, crazy.",
          "score": 50,
          "created_utc": "2025-12-29 07:05:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmmfiv",
              "author": "Remarkable_Result596",
              "text": "Well, for what it's worth, I won't ever sign anything like it again. But even in hindsight, I do think this was a good call at the time to accept the offer, get the experience it promised. I was reluctant about the 6 months naturally, but time was of the essence for me then, everything was rushed, and I didn't want to risk losing the offer by playing tough. So yeah, that's how I ended up here sadly",
              "score": 1,
              "created_utc": "2025-12-29 22:43:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwigb2a",
          "author": "L3GOLAS234",
          "text": "Is 6 months notice even legal? In my country, many employers put whatever they want, but the law says that more than X is basically voluntary.\n\nAlso, what happens if you just don't comply with the full notice period? Most likely the company doesn't want anyway to have a demotivated employee for 6 months",
          "score": 30,
          "created_utc": "2025-12-29 08:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiwimb",
              "author": "MathmoKiwi",
              "text": "I'd also be questioning if that notice period length is legal?? Especially so after they've been there for a whole 4yrs!\n\nIf true, I'd just go spend those next six months getting those project(s) done for your CV that OP wishes to accomplish, plus grab at least a couple of key certs to help plump out his CV.",
              "score": 9,
              "created_utc": "2025-12-29 10:38:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwihnlv",
          "author": "Kukaac",
          "text": "I would quit. It's a dick move to suppress your salary with a 6 month notice. If you are confident that you are underpaid you will find something else fast. \n\nAlso generally you should be asking about their current salary ranges and how your salary fits into them. The market won't care what % increase you need, so they shouldn't either. That is a very outdated culture and will result their employees leaving.",
          "score": 11,
          "created_utc": "2025-12-29 08:19:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwibk5b",
          "author": "West_Good_5961",
          "text": "6 month notice period, wtf? Are there no labour regulations in your country?",
          "score": 7,
          "created_utc": "2025-12-29 07:23:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwighax",
              "author": "Kukaac",
              "text": "For most EU countries 6 months is the maximum. But it goes both ways, if they want to fire him it would take at last 6 months.",
              "score": 7,
              "created_utc": "2025-12-29 08:08:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwmh8sk",
                  "author": "zutonofgoth",
                  "text": "In Australia you could only have a 6 month notice period under specific requirements. A data engineer would be unlikely to meet those requirements.",
                  "score": 2,
                  "created_utc": "2025-12-29 22:16:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwmx7f5",
              "author": "Remarkable_Result596",
              "text": "Well, there are! I should be having 1 month long notice period right now.  \n  \nBut the law does allow for custom arrangements between parties if they are beneficial for the employee, which is well.. subject to interpretation. Reading through the comments made me realize that I should make an effort to try and challenge it though, consult some state labour entity to get their opinion.",
              "score": 2,
              "created_utc": "2025-12-29 23:41:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj02m7",
          "author": "dataflow_mapper",
          "text": "This is one of those situations where both paths are defensible, which is what makes it hard. The on prem plus Databricks plus streaming mix you are touching is genuinely rare and very valuable, especially if you can point to concrete architectural decisions and outcomes. That said, a six month notice basically forces you to over optimize for learning at your current company, because the market is not built to wait that long.\n\nIf it were me, I would set a clear internal deadline. For example, stay long enough to see the Kafka and Debezium work through to something production like, document the impact, then reassess. If the salary gap and notice period are still unchanged, that is your signal. The risk with staying too long is that the learning curve flattens, but the compensation gap keeps widening. You want to leave while you are still running toward the market, not after it has already moved on.",
          "score": 3,
          "created_utc": "2025-12-29 11:10:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmv675",
              "author": "Remarkable_Result596",
              "text": "Thank you, this has been a refreshing read. In truth, right now there is no telling how much time will pass before the streaming setup is all agreed on, and things start to roll. My part in this part of development is also kind of an unknown, but yeah, I probably don't expect to actually be tasked with operating Kafka clusters, topics etc. In this sense, I would probably be responsible for maybe stream processing with spark or plainly batch processing of the streaming data.\n\nCome to think of it in those terms, right now is perhaps not a bad time for an internal deadline honestly. I'm currently still wrapping up my core effort - tying together the platform at its' pre-streaming phase, with on-prem Dagster, other MDS, and the cloud elements.",
              "score": 1,
              "created_utc": "2025-12-29 23:30:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwix17a",
          "author": "testEphod",
          "text": "Are you working for InsurTech or FinTech? Because they normally have these arrangements in place. If so I would ask them to reduce the six months period down to three months as this is more favorable to people who work in tech regarding future employability",
          "score": 2,
          "created_utc": "2025-12-29 10:43:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwldba2",
              "author": "Remarkable_Result596",
              "text": "I work for Pharma. But it is indeed negotiable as long as I'm the only one departing, at least that's what I'd expect. It's just that it seems I can't really find the job prior to resigning, at least not without lying about my true notice period. When i mention 'negotiable' 6 months, it still sounds like 6 months sadly. But there is a good chance I would get it shortened to like 3 months",
              "score": 2,
              "created_utc": "2025-12-29 19:02:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwl6pw1",
          "author": "Ok_Appearance3584",
          "text": "Quit now, start saving up some cash so you can live 2-3 months after contract ends. Start looking for a new job 1-2 months in advance before current contract ends. Don't sign up for contracts with 6 months notice period anymore. Where I live it's one month after four years of work from emploee side and two months from employer side.",
          "score": 2,
          "created_utc": "2025-12-29 18:31:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmfvzq",
          "author": "diusbezzea",
          "text": "Not sure which country, but where I’m from, good data engineers are hot stuff - hot enough to quit and start looking after 3 months",
          "score": 2,
          "created_utc": "2025-12-29 22:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwijewt",
          "author": "Lamyya",
          "text": "Contact a union (in your country) about the notice period in your contract",
          "score": 4,
          "created_utc": "2025-12-29 08:35:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj6nbz",
          "author": "VipeholmsCola",
          "text": "Contact union or lawyer and see if you can get out of the 6 months notice. In future, dont sig such insane contracts.\n\nIf you cant, if first threaten with leaving unless they match market. But if they deny, id quit. Then take it from there.",
          "score": 2,
          "created_utc": "2025-12-29 12:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj59n8",
          "author": "AndriusVi7",
          "text": "Few points from me.\n\n1. You're focus is way too broad, you'll end up with exposure to all, but mastery of none hence your salary will be weight down rather than up. Databricks is in high demand, so given a choice id go 100% into it. I'm a databricks guy too btw and am in 6 figure salary in spain (UK based client).\n\n2. 6 month notice doesn't mean shit, you can accept another role and give a days notice. The company can't do shit to enforce it as far as I know, you can't be forced into employment, something to do with basic human right of not being enslaved to an employer \n\n3. An employer will keep your salary low if possible, so give your notice in due to salary being far below market rate, and you'll have a pay rise within a week. You need to force their hand, or they'll call your bluff and pocked the difference.",
          "score": 1,
          "created_utc": "2025-12-29 11:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj6vdx",
              "author": "VipeholmsCola",
              "text": "They definitely can make his life miserable depending on country, laws and contract. Especially if theres none compete clauses etc",
              "score": 1,
              "created_utc": "2025-12-29 12:07:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwja9op",
                  "author": "AndriusVi7",
                  "text": "Would be good to know more details of the contract, but are there really non competes in eu for data engineering as a profession? \n\nIt just seems like the company made some dick moves on the contract to discourage jumping ship, and make it harder on salary negotiation even if they really can't do much to enforce it. Half the time putting into a contract or having an agreement is enough to buy them time to cash in.",
                  "score": 1,
                  "created_utc": "2025-12-29 12:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwlc66m",
              "author": "Remarkable_Result596",
              "text": "Thanks!\n\n1. You're probably right in terms of this being too much in a rather short period of time. For sure I have only skimmed over some of the stuff. I do think it's been valuable for starters, because it did open my mind to a lot of technologies, stuff I might have never tried otherwise - especially containerization, Kubernetes I think was a nice thing for me. But yeah, I guess it is not sustainable to try and do it all in the long run.\n2. It is a matter I'm also debating, i.e. if i can actually just ignore it. I have carried out a bit of research though and haven't really found a definitive answer on whether I actually can. The local regulations do permit custom arrangements between parties if it is 'beneficial' for the employee, which of course is subject to interpretation. I think I probably should do more in this regard, consult an actual lawyer maybe to get an honest take.",
              "score": 1,
              "created_utc": "2025-12-29 18:56:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj7kx0",
          "author": "Playful_Can_7094",
          "text": "Pretty cool to hear what you are working on brother,  hard to know what the future brings but I would love to be able to say I have similar experience in 5 years. Keep up the hustle!",
          "score": 1,
          "created_utc": "2025-12-29 12:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl8u66",
              "author": "Remarkable_Result596",
              "text": "Thanks! I must admit that it is/was a lot of trial and error and admittedly had a lot of space to do that. As someone pointed out though, my efforts are probably pointed in one too many directions. Nonetheless, it's been great to get a grasp on so many things, had a lot of fun trying for sure.",
              "score": 1,
              "created_utc": "2025-12-29 18:41:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp474e",
          "author": "SlammastaJ",
          "text": "What is the consequence of not honoring the 6-month notice period?\n\nFor example, if you received an offer, and left your current job for the new one, within 6-months, what would happen?",
          "score": 1,
          "created_utc": "2025-12-30 08:07:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpb34n",
              "author": "Remarkable_Result596",
              "text": "Theoretically, the company could sue me for the remaining months' salary and give me a \"disciplinary firing\" on my record, plus potential damages if they can prove my early exit caused specific financial loss. I fully expect them to be petty about this unless I can prove the clause was illegal from the start. As people here have implied, it likely violates the \"benefit of the employee\" principle in Polish law. Even if I'm legally in the right, fighting a bad record and a lawsuit would be a time-consuming headache before everything is eventually straightened out.",
              "score": 1,
              "created_utc": "2025-12-30 09:11:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwibky0",
          "author": "Flacracker_173",
          "text": "Your six month notice is now a two week notice. Congrats. You do not need to follow stupid “rules” like that.",
          "score": -7,
          "created_utc": "2025-12-29 07:23:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwid4bt",
              "author": "Kukaac",
              "text": "It's probably not a rule, but part of his contract. It has to go both ways, but if he leaves early he is responsible for the damage caused to the company.\n\n2 weeks notice or at will employment is almost non-existing in the EU.",
              "score": 4,
              "created_utc": "2025-12-29 07:37:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwmopq5",
                  "author": "Remarkable_Result596",
                  "text": "Yeah, where I'm from, I should be having 1 month long notice period right now. But the law does allow for custom arrangements between parties if they are beneficial for the employee. Whether 6 months is beneficial kinda depends on whether I'm getting fired or leaving lol. This is for sure a grey area, and I think I could try to challenge it. Decided against that till now, as it wasn't clear cut case for the acquainted lawyer I have asked. But perhaps that's the way to go about this, I may consult some State Labour entity to give their verdict on this",
                  "score": 1,
                  "created_utc": "2025-12-29 22:55:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1purqy3",
      "title": "New Job working with Airflow questions",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1purqy3/new_job_working_with_airflow_questions/",
      "author": "Kcrizzle87",
      "created_utc": "2025-12-24 16:50:08",
      "score": 18,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "Hello! I'm starting a new job next week working as the only software engineer in a group of data engineers. They primarily work with airflow and want my first task to be to examine their DAGs etc to work on making them efficient. \n\nThey're going to team me up with an SE from another department to help me through the process, but what are some things I could look for day 1 to try and impress my new bosses?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1purqy3/new_job_working_with_airflow_questions/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvqu95c",
          "author": "Thujaghost",
          "text": "I’d start with clarifying what they mean by efficient: do they want shorter runtimes, a smaller cheaper instance or some other metric to drop? How to get there depends on what you’re dealing with",
          "score": 27,
          "created_utc": "2025-12-24 17:16:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqupkd",
              "author": "Kcrizzle87",
              "text": "Excellent advice. This is probably what I should have asked to begin with tbh. Thank you, sincerely.",
              "score": 8,
              "created_utc": "2025-12-24 17:19:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvr83ou",
          "author": "likely-",
          "text": "Impressing your boss on day one is literally just showing up on time and getting along with colleagues.\n\nAt least in my experience it takes months in a role to bring “big wins” to my boss. They didn’t bring you in because they thought it was easy.",
          "score": 19,
          "created_utc": "2025-12-24 18:31:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrqctt",
          "author": "ludflu",
          "text": "[There was a post about airflow a while back,](https://www.reddit.com/r/dataengineering/comments/1nnzk04/comment/ng4e5va/?context=3) and I mentioned the top 3 things I often see people do wrong with DAGs.\n\n1. Don't run your ETL with PythonOperators, because that runs on the airflow instance, which doesn't scale\n2. Don't write one giant DAG that handles everything. make small, simple DAGs that are easy to understand and debug\n3. Use airflow to handle parallelism, don't try to do that yourself\n\n  \nSo I spend a bunch of my time untangling messy inefficient DAGs that break those rules, and the result is generally faster, simpler and easier to understand.",
          "score": 10,
          "created_utc": "2025-12-24 20:14:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqsqbt",
          "author": "GoTheFuckToBed",
          "text": "if there was quick wins they wouldnt hire you?",
          "score": 4,
          "created_utc": "2025-12-24 17:08:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqt6w5",
          "author": "BitPuzzleheaded5",
          "text": "Find out which dags are taking the longest and start there.",
          "score": 3,
          "created_utc": "2025-12-24 17:11:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqzj8a",
          "author": "Nielspro",
          "text": "You could examine how they monitor the run times and costs of their pipelines. If there is no monitoring then it’s difficult to have an overview of where the bottlenecks are",
          "score": 3,
          "created_utc": "2025-12-24 17:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3egx6",
          "author": "Immediate-Pair-4290",
          "text": "Look up Marc Lamberti Airflow courses. Good luck!",
          "score": 1,
          "created_utc": "2025-12-26 22:16:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pznhlf",
      "title": "Squirreling: an open-source, browser-native SQL engine",
      "subreddit": "dataengineering",
      "url": "https://blog.hyperparam.app/squirreling-new-sql-engine-for-web/",
      "author": "dbplatypii",
      "created_utc": "2025-12-30 17:05:21",
      "score": 16,
      "num_comments": 6,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Open Source",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pznhlf/squirreling_an_opensource_browsernative_sql_engine/",
      "domain": "blog.hyperparam.app",
      "is_self": false,
      "comments": [
        {
          "id": "nwswy3m",
          "author": "WideWorry",
          "text": "Nice, exactly something what I would need for my game.",
          "score": 2,
          "created_utc": "2025-12-30 21:29:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwugphv",
          "author": "Hofi2010",
          "text": "Nice work",
          "score": 2,
          "created_utc": "2025-12-31 02:29:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsi5rb",
          "author": "chock-a-block",
          "text": "GitHub link?",
          "score": 1,
          "created_utc": "2025-12-30 20:19:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsjo00",
              "author": "dbplatypii",
              "text": "[https://github.com/hyparam/squirreling](https://github.com/hyparam/squirreling)",
              "score": 1,
              "created_utc": "2025-12-30 20:26:20",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwswyvc",
              "author": "WideWorry",
              "text": "https://github.com/hyparam/squirreling",
              "score": 1,
              "created_utc": "2025-12-30 21:29:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwt0yn7",
              "author": "Ok-Improvement9172",
              "text": "From the article: https://github.com/hyparam/squirreling",
              "score": 1,
              "created_utc": "2025-12-30 21:48:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1puzstx",
      "title": "Is it appropriate to store imagery in parquet?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1puzstx/is_it_appropriate_to_store_imagery_in_parquet/",
      "author": "BitterFrostbite",
      "created_utc": "2025-12-24 23:10:46",
      "score": 16,
      "num_comments": 23,
      "upvote_ratio": 0.85,
      "text": "**Goal**:\n\nIm currently trying to build a pipeline to ingest live imagery and metadata queued in Apache Pulsar and push to Iceberg via Flink. \n\n**Issues**:\n\nI’m having second thoughts as I’m working with terabytes of images an hour and I’m struggling to buffer the data for Parquet file creation, and am seeing extreme latency for uploads to Iceberg and slow Flink checkpoint times. \n\n**Question**:\n\nIs it inappropriate to store MBs of images per row in parquets and Iceberg instead of straight S3? Having the data in one place sounded nice at the time. ",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1puzstx/is_it_appropriate_to_store_imagery_in_parquet/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvsk94m",
          "author": "SimpleSimon665",
          "text": "You're better off storing URLs in your parquet files that point to the S3 path of your images.",
          "score": 66,
          "created_utc": "2025-12-24 23:20:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvspo81",
              "author": "squirrel_crosswalk",
              "text": "I know it's two steps, but I would store a hash in the parquet, and then have a lightweight lookup db/service/whatever.  That will survive a migration.  S3 path likely won't.",
              "score": 21,
              "created_utc": "2025-12-24 23:57:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbppwt",
                  "author": "GuhProdigy",
                  "text": "Can you explain how a hash of a URL would survive a migration rather than the URL itself?\n\nUnless you are only storing the local file path relative to the bucket, the hash would completely change. Even if you are only storing the local file path, you might as well just store the local file path lol. This is seems like over engineering for something a simple update statement could account for later rather than building an entire new database for LOL.",
                  "score": 1,
                  "created_utc": "2025-12-28 06:38:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsm9rw",
              "author": "SureConsiderMyDick",
              "text": "maybe a SHA256 hash too",
              "score": 8,
              "created_utc": "2025-12-24 23:34:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsmjyp",
                  "author": "SimpleSimon665",
                  "text": "Do you mean a hash of the image to ensure integrity?",
                  "score": 1,
                  "created_utc": "2025-12-24 23:36:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsnf93",
              "author": "BitterFrostbite",
              "text": "What specifically is driving this suggestion if I may ask in regard to Parquet and Flink?",
              "score": 2,
              "created_utc": "2025-12-24 23:42:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvss04z",
                  "author": "dacort",
                  "text": "I’d do the same. Parquet is great because it is columnar, can generate metadata stats on your columns (min/max values), and doesn’t require you to read the whole file when filtering. Storing image blobs in parquet gets almost none of those benefits and is probably harder for parquet readers to decode than simply providing a reference to the file and reading the file directly.",
                  "score": 12,
                  "created_utc": "2025-12-25 00:14:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtcqeq",
              "author": "BitterFrostbite",
              "text": "With a query engine like Spark would you join on URL of the image then for my metadata stored in Iceberg? My users mainly use the data for analytics/machine learning and need to combine tens to hundreds of terabytes of images back with their metadata. \n\nFor reference I’m a software engineer picking up data engineering work since we can no longer meaningfully use the amount of data we have.",
              "score": 1,
              "created_utc": "2025-12-25 02:48:18",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvy057p",
              "author": "Global_Bar1754",
              "text": "Honestly you don’t even need parquet at all. Just store the images in s3 with your metadata in a hive partitioned path and use read_blob from duckdb to get the data",
              "score": 0,
              "created_utc": "2025-12-25 23:51:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvszs7m",
          "author": "robverk",
          "text": "The pattern to store images or any binary really is in a blobstore and you refer them in your metadata. This has advantages in deduplication, storage & retrieval efficiency. \n\nBut if you need the images directly in your processing pipeline and they are small enough and mostly unique, and you are not a very lage site, you could start out storing them in your metadata layer at the cost of some performance.",
          "score": 9,
          "created_utc": "2025-12-25 01:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvub4fi",
              "author": "NoReception1493",
              "text": "How do you ensure that the blob file is deleted when the metadata (file or a row in table) is deleted? \n\nI'm more thinking of failure cases where the request fails for whatever reason. Like you deleted the metadata but the blob deletion failed and you've got leftover blobs in S3.",
              "score": 2,
              "created_utc": "2025-12-25 07:47:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvucym5",
                  "author": "robverk",
                  "text": "This is usually solved by running a maintenance job every week or month scanning for orphaned files and removing them.\n\nAn alternative is to set an automatic age-off timestamp and update that when metadata gets updated.",
                  "score": 2,
                  "created_utc": "2025-12-25 08:06:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt8b7u",
              "author": "BitterFrostbite",
              "text": "Currently our images are in S3 and references are in Elastic, but we have so much data now we wanted to implement Iceberg and start using query engines like Spark so we could actually make use of it all. \n\nPutting the images in Parquets sounded nice since all our data would be in one place, but I wanted the communities opinion! I don’t think we would lose out on query speeds by moving the images back to blobstore. \n\nSeems like images in Parquets would just slow things down, thanks!",
              "score": 1,
              "created_utc": "2025-12-25 02:14:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsl1pz",
          "author": "figshot",
          "text": "Data Engineering Podcast had an [episode](https://www.dataengineeringpodcast.com/episodepage/lance-vector-data-lake-format-episode-442) about a columnar table format called Lance. They claimed that Parquet is poorly suited for images. I wonder if this might be worth checking out for you. I don't have a use case for this so I've never checked this out.",
          "score": 6,
          "created_utc": "2025-12-24 23:25:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvving5",
          "author": "Hofi2010",
          "text": "I cannot think of a scenario where storing image data in iceberg would have a major advantage compared to storing in blob storage such as s3.\n\nI think of the additional complexity in your pipeline to big data volumes. And the retrieval side. Spark or SQL have no built in functions to do anything with the image data directly, so it will always be a two step process to do something with the images after retrieving with an sql or spark statement. For example ocr the content of an image. You retrieve the image data from iceberg and then need some eg Python code to load it into an image object and then use libraries to ocr the image. Same process if you would store the URL to the image in the db, step 1 retrieve the url step 2 load the image and ocr with Python.\n\nI would think the performance and infrastructure compute would be more favorable with images in blob storage. You just need a much smaller spark cluster to process the iceberg files and faster retrieval of the URL and image due to much less data volumes to process.",
          "score": 4,
          "created_utc": "2025-12-25 14:43:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvvrvs",
              "author": "BitterFrostbite",
              "text": "We don’t OCT our images as they are actual photographs and stored as arrays. Our data scientists are looking at small pixel level features. So having the images in the parquets does remove some complexity for our data scientists but the pipeline is indeed way too heavy now to keep up ingest.",
              "score": 1,
              "created_utc": "2025-12-25 16:06:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwpx3w",
                  "author": "Hofi2010",
                  "text": "As I said putting ages into the table doesn’t help the data scientist much as they need to take the binary data and have to assign to an image object. The code to do that vs loading from s3 is pretty much same effort",
                  "score": 1,
                  "created_utc": "2025-12-25 19:04:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtkx9r",
          "author": "West_Good_5961",
          "text": "Nah. Process order: Image>OCR>CSV>schema transformation>whatever structured format you like.",
          "score": 1,
          "created_utc": "2025-12-25 03:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa4rve",
          "author": "wingman_anytime",
          "text": "The claim check pattern is your friend. Tale as old as time - don’t store your blobs in your message stream.",
          "score": 1,
          "created_utc": "2025-12-28 00:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsllkj",
          "author": "One-Salamander9685",
          "text": "Profile your pipeline and you'll find the answer",
          "score": -1,
          "created_utc": "2025-12-24 23:29:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyvvd1",
      "title": "My data warehouse project",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pyvvd1/my_data_warehouse_project/",
      "author": "kekekepepepe",
      "created_utc": "2025-12-29 19:38:15",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.95,
      "text": "Hello everyone,  \nI strongly believe that domain knowledge makes you a better data engineer. With that in mind, I built a personal project that models the entire history of the UFC in a dedicated data warehouse.\n\nThe project’s objective was to create analytical models and views to tackle the ultimate question: *Who is the UFC GOAT?*  \nThe stack includes **dlt** for ingestion, **dbt** for transformations, and **Metabase** for visualization.\n\nYour feedback is welcomed:  \nLink: [https://github.com/reshefsharvit/ufc-data-warehouse](https://github.com/reshefsharvit/ufc-data-warehouse)",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pyvvd1/my_data_warehouse_project/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nx2ubhp",
          "author": "marclamberti",
          "text": "Thank you for sharing!! Would love to see your projet on data project hunt 🤓",
          "score": 1,
          "created_utc": "2026-01-01 13:49:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzjflo",
      "title": "13 Apache Iceberg Optimizations You Should Know",
      "subreddit": "dataengineering",
      "url": "https://overcast.blog/13-apache-iceberg-optimizations-you-should-know-85bc25690f00",
      "author": "codingdecently",
      "created_utc": "2025-12-30 14:25:44",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pzjflo/13_apache_iceberg_optimizations_you_should_know/",
      "domain": "overcast.blog",
      "is_self": false,
      "comments": [
        {
          "id": "nws5nxi",
          "author": "ReporterNervous6822",
          "text": "Not a bad article, i wish there was overall better docs on when there is benefit in metadata compaction",
          "score": 1,
          "created_utc": "2025-12-30 19:19:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxuxev",
      "title": "Is pre-pipeline data validation actually worth it ?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pxuxev/is_prepipeline_data_validation_actually_worth_it/",
      "author": "PriorNervous1031",
      "created_utc": "2025-12-28 16:06:45",
      "score": 16,
      "num_comments": 18,
      "upvote_ratio": 0.85,
      "text": "I'm trying to focus on a niche that sometimes in data files everything on the surface looks fine, like it is completely validated, but issues appear in downstream and process break.\n\nI might not be the expert data professionals like there are in this sub, but just trying to focus on a problem and solve it. \n\nThe issues I received from people:\n\n* Enum Values drifting over time \n* CSVs with headers only that pass schema checks \n* Schema Changes\n* Upstream changes outside your control\n* Fields present but semantically wrong etc.\n\nOne thing that stood out:\n\nA lot of issues aren't hard to detect - they're just easy to miss until something fails\n\n  \nSo just wanted to know your feedback and thoughts, that is this really a problem or is it already solved or can I make it better or it isn't worth working on? Anything",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pxuxev/is_prepipeline_data_validation_actually_worth_it/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwe8n1x",
          "author": "kenfar",
          "text": "After doing this for literally decades I have to say I'm a complete convert to doing as much of this work upfront via Data Contracts as possible:\n\n   * Data Contracts can define required vs optional columns, their types, their formats (ex: phone numbers with dashes), their min/max values, their min/max string lengths, their enumerated values, and their NULL rules.  That contract can be stored in a JSON structure and shared with your upstream source - and then be the basis for their testing and yours.  Nothing else will provide as much DQ guarantees as this.  And it can be a simple library call to hand over a row/file/table to a utility for checking.\n   * Anomaly-detection is also valuable to do prior to transformation:  does your input always have 10 million rows and suddenly you only have 65?  Regardless of whether the rows are formatted correctly, you probably have a partial file.  Or how about if one of those columns is historically NULL 1% of the time, and that's ok, but now it's NULL 100% of the time.  Something has changed...\n\nI like doing the above because each of these can typically work with raw data, can catch a ton of problems pre-transform, and isn't much work.  Then once you get into the transforms you'll need to do more of the above, but in that case it'll be for items you couldn't check with raw data - like joins that fail because there's suddenly no matching rows, etc.",
          "score": 26,
          "created_utc": "2025-12-28 17:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwebp60",
              "author": "Cultural-Pound-228",
              "text": "How does one store the data pattern or data expectations in anamoly detection? Is it pre defined or an outcome of some ML model ?",
              "score": 1,
              "created_utc": "2025-12-28 17:53:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nweyg3r",
                  "author": "Nightwyrm",
                  "text": "Also check out datacontract-cli who work with the Open Data Contract Standard.",
                  "score": 4,
                  "created_utc": "2025-12-28 19:39:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nweg2h8",
                  "author": "kenfar",
                  "text": "Depends on the tool.  But even simple statistical process control (SPC) is often more than sufficient, and can simply look at previously received data.  \n\nSo, in the simplest form this could be a simple query against your raw data, counting rows by day, and comparing the current day's data against the prior 90 days of counts, to determine if the current day's data is more than X standard deviations from the mean.  Sometimes I've done this but compared the data by hour of day of week - so you could say that for a weekday, or a tuesday, between 9:00 - 10:00 our data is within 2 standard deviations of the prior 90 days.",
                  "score": 2,
                  "created_utc": "2025-12-28 18:13:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwen9kp",
                  "author": "cjnjnc",
                  "text": "For a small team we use Monte Carlo. No idea what the cost is like but it does really well at detecting schema changes, presumably uses simple ML for tracking row count changes, and supports custom SQL rule definitions that we probably use too much.\n\nSQL rule definitions effectively has become \"an internal business user input faulty data somewhere, caused a firedrill -> let's put a rule in place to catch this earlier next time\".",
                  "score": 1,
                  "created_utc": "2025-12-28 18:47:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwedxbz",
          "author": "Siege089",
          "text": "We use data contracts stored as json documents, controlled by publishers of the data at each layer that they are promising to adhere to. Each team can handle invalid data in their own way, although there are some shared tools to validate the data vs the contract. My team is the last layer before reporting essentially stitching together the gold layer and has the policy of quarantine everything that's bad and then work with upstream to understand what changed and why. Depending on the issue it could be a data contract update, could be a logic update we need to implement, or it could be data is permanently quarantined. Because we work directly with large payments everything dies early and is cautious. If someone doesn't care about our validations and consolidated output they're more than welcome to look at our upstream directly.",
          "score": 4,
          "created_utc": "2025-12-28 18:03:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj9053",
              "author": "the_travelo_",
              "text": "How exactly do you use the contracts? Who publishes it? How do you use it in your pipelines? Did you write custom code to convert the contract to low level Spark or SQL commands?",
              "score": 2,
              "created_utc": "2025-12-29 12:24:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjmrix",
                  "author": "Siege089",
                  "text": "Each team publishes them to a shared service, in our case we use a shared git repo that then deploys them to the database. We have a utility in spark that can retrieve them and provides methods to validate a dataframe against them, along with utilities such as fetching and writing data using additional metadata specified in the contracts.\n\nSo in our pipeline we use contract references to fetch contacts and read data. Once we're done with our transformations we fetch our output contract, validate our output dataframe, and writing the valid data. Any invalid data is returned to us as a separate dataframe with additional columns stuffed with metadata on why validation failed. In our case we log for alerting and write out quarantine records to a different location to be repaired, either auto heal, because something changed, such as contract got updated, or manual intervention takes place.\n\nContracts in this model form the backbone of the entire data lake (s), we monitor that each team uses the utility to validate and that helps a ton. Early on when it was new we would have issues with teams not using the utility which then effected downstream when the contracts they published weren't adhered to.  Now if we have issues it's because of weak contracts, such as missing rules or because someone didn't communicate a change. For example upstream added a new type to enum but didn't communicate it to us and we pass it through so our validation failed on that field.\n\nSince they're just json documents you can do whatever you want to build out features you need. We haven't come across anything open source that I think suits our needs so all our stuff is custom built. Our schemas can get pretty complex, and we have a history of abusing JSON fields to just stuff data in and that can cause headaches. But once setup and each team is onboarded a lot of common headaches can be removed. And strong contracts at each level acts as a forcing function to ensure upstream talks with downstream or things are caught by the client. \n\nRules can take whatever form you want, sometimes we support regex, others we have simple named rules like a list type for enums, or min/max. We also support conditional rules, so not null when colA == foo. In our model we've focused on common schemas level validations and then that leaves teams to implement their own business level validations outside the contract framework. We haven't yet gotten consensus on a way to capture a large enough common set of business rules that it makes sense to add them to contracts yet. If we ever make it that far we'd essentially be replacing a large part of the code being written with standardized configuration so I doubt it would happen besides a small subset of extremely common scenarios.",
                  "score": 2,
                  "created_utc": "2025-12-29 13:56:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nweqcal",
          "author": "addictzz",
          "text": "I'd say enforce Data contracts. Both data producers and data consumers must respect this data contract, with more weight of responsibility falls towards data producers. Perform input data validation in the front-end or data producer side. Changes to this data contract must notify the data consumers too.\n\nHowever usually it is not that easy to enforce given that schema changes and data type changes affect data consumers and data processor (ie. data engineering team) more. For data producers which is usually microservices team or front-end team, their KPI leans more towards shipping features, ensuring stable operations, and satisfactory user interface. Not ensuring rigid data schema.",
          "score": 4,
          "created_utc": "2025-12-28 19:01:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwewhc5",
          "author": "painteroftheword",
          "text": "I've recently been building some data quality reporting that compares actual system values with a set of expected system values based on criteria I've put together based on my own knowledge and what I've been able to get from SME's to fill the gaps in my knowledge.\n\nThe real problem is data quality issues were there is no independent means of determine whether or not a value is correct. Especially when dealing with data that is normally quite chaotic.\n\nActually the main problem is getting the business to actually do something about data quality issues because we have no data owners.",
          "score": 3,
          "created_utc": "2025-12-28 19:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwejb5w",
          "author": "cjnjnc",
          "text": "'Pre-pipeline data validation' just sounds like shifting left, no? I'm assuming when you say pipeline, that you are talking about a data ingestion that gets incorporated into your larger existing analytical system. That matches my experience with the schema + CSV issues you are describing.\n\nEverything looking fine on the surface but more subtle, complex assumptions about the data being broken post pipeline deployment is something I've run into often. I've had varying degrees of success in trying to proactively identify issues like this. There are a few things that worked for me, particularly when the data source is an external partner. My experience is also in smaller companies where end to end analytics is entirely my responsibility and there is limited support from other colleagues. With that in mind, this kind of validation is imo a balance between getting the pipeline into production quickly and making issue identification and maintenance as easy as possible.\n\n**Before building anything / requirements and assumption refinement:**\n\n* Get all info about the data source you can\n * Ask for data dictionaries and/or entity relationship diagrams -> only commonly seen these available in finance but solves 99% of these problems before they happen\n * Figure out all your critical assumptions and ask about them explicitly (primary keys, important relationships, etc.)\n* Identify key internal stakeholder(s) that have the business context to clarify expectations\n * Who can be your point of contact for escalating + investigating issues down the line\n * Identify what aspects of the data being ingested are critical to *your own* business' processes\n     * Primary keys and most likely a subset of columns/fields rather than everything\n         * That a field exists, is an enum of X possible options, etc.\n     * Things that if your assumption about the data become wrong or broken, then the data becomes unusable or detrimental to the business\n\n**While building:**\n\n* Codify your critical assumptions\n * If these are broken pipeline FAILS -> clear alert that mentions you as pipeline owner and your internal stakeholder with a clear business-centric message\n * Subset of critical fields that must have some characteristics\n* Quarantine/identify your non-critical assumptions\n * Should NOT fail the pipeline so that business critical data can still flow\n * More of a nice to have\n * If a non-necessary or unused field starts breaking assumptions -> alert that mentions only you as pipeline owner with clear technical context, allows you to create a backlog task which can be prioritized appropriately \n\nThere's plenty you could do to abstract and reuse a lot of this functionality in whatever your ingestion/orchestration tool you are using but personally I have been unsuccessful in advocating for this internally.\n\nI am making plenty of assumptions about the latency requirements, criticality of the data to your business, and that you have some kind of semi-mature ingestion framework with alerting capabilities. This also assumes your destination tables are pre-defined with the pipeline and you aren't using some kind of lakehouse pattern which I have less experience with. There are definitely tons of options for approaches to this so I'm curious what others have to say.\n\nI'd be happy to discuss more specific tools here but tried to keep it relatively high level and already wrote an essay. Hope this helps!",
          "score": 2,
          "created_utc": "2025-12-28 18:28:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe0kqh",
          "author": "DexTheShepherd",
          "text": "From my experience it's *typically* best to separate out the \"turning the raw data into an ingestable/readable format\", and \"making that data appear exactly as I want it in its final form\"\n\nIt may depend on your requirements however. If the data is \"invalid\" by your criteria, can you drop it on the floor? If so, maybe you can validate it at parse time. But typically you try not to do that.\n\nSchema drift can be tough to solve without a lot of other metadata you need to either derive by profiling the data, OR having users or your system declare it in absolute terms. Both are tough and have complexity in it that may not be obvious to the users of that data. \n\nA pattern I've seen that did okay was to take anything that didn't fit into our target schema, we just threw it into an additional column called `invalid`. From there we can at least write some additional SQL that looks for any records that contain something within `invalid`.\n\nAt the very least this would provide you (or whoever is using your ingested data) the knowledge that _something has changed_, without knowing exactly what. Which in our case, is usually more than enough info to address what schema drift has taken place.",
          "score": 1,
          "created_utc": "2025-12-28 16:58:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe4et8",
              "author": "PriorNervous1031",
              "text": "This makes a lot of sense.\n\nI like the idea of quarantining unexpected fields instead of rejecting rows outright especially when the primary goal is awareness rather than correctness.\n\nIn your experience, did teams usually act on those invalid signals quickly, or did they mostly serve as an early-warning system until something broke downstream?",
              "score": 2,
              "created_utc": "2025-12-28 17:17:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwe5a68",
                  "author": "DexTheShepherd",
                  "text": "Just really depends on whose using the data. If that data is being leveraged by an application, things might break immediately, needing a fix asap. But maybe it's being used to drive some analytic dashboard or graphs, in which case the breakage might not be as harmful.\n\nTo give them the awareness that their stuff might be broken, we usually just write a DAG which scanned all tables for invalid records, and threw that in a master \"this shit is broken\" table.\n\nI'd try to maybe understand how the data is used in your case. Is it critical everything is always loaded correctly? Can quarantining be okay? Etc",
                  "score": 2,
                  "created_utc": "2025-12-28 17:21:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhqbkg",
          "author": "redditreader2020",
          "text": "I would suggest data validation very early and very late for starters. Early to catch bad incoming data. Late to catch transformation bugs in your code. Then fine tune slowly over time.",
          "score": 1,
          "created_utc": "2025-12-29 04:38:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py3wlz",
      "title": "Data Analyst to Data Engineer transition",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1py3wlz/data_analyst_to_data_engineer_transition/",
      "author": "SherbertClassic3870",
      "created_utc": "2025-12-28 22:03:16",
      "score": 15,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "Hi everyone, hoping to get some guidance from the people in here. \n\nI've been a data analyst for a couple of years and am looking to transition to data engineering. \n\nI've been seeing some lucrative contracts in the UK for data engineering but tool stacks seem to be all over the place. I really have no idea where to start.\n\nAny guidance would really be appreciated! Any bootcamp recommendations or suggestions of things I should be focusing on based on market demand etc?",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1py3wlz/data_analyst_to_data_engineer_transition/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwfrsv5",
          "author": "AutoModerator",
          "text": "Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-28 22:03:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfv75n",
          "author": "t9h3__",
          "text": "Become really proficient in SQL and Python.\nAlong with some git and bash commands this can bring you very far.\n\nA lot of companies interpret the DE role a bit different and there are spectrums.\n\nSome are very technical setup up infrastructure, others rather focus on data loading and transformation (where you will have an easier time in the beginning).\n\nFor the former you will need more technical knowledge (docker containers, servers, networking), for the latter it's more about concepts like data modeling",
          "score": 9,
          "created_utc": "2025-12-28 22:20:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg7j9g",
              "author": "SherbertClassic3870",
              "text": "SQL I'm confident with. I've done some data modelling via dbt but mainly int, data mart and reporting models. \n\n\nI am working through python fundamentals at the moment whilst trying to figure out how deep I need to get when it comes to knowing python.\n\n\nI've mainly worked with Snowflake but seeing Databricks show up alot more in job ads so wondering if I should be focusing on that instead?",
              "score": 3,
              "created_utc": "2025-12-28 23:25:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwidzya",
                  "author": "t9h3__",
                  "text": "I would say concepts over tooling:\nIf you have worked on AWS for years, switching to GCP is pretty quick. Same for snowflake vs databricks vs bigquery.\n\nI would say automating extraction and loading of data would be no.1 priority for python.\nAnd along with this general coding best practices: what's good code, how to work effectively with branching, code review, deployment strategies",
                  "score": 3,
                  "created_utc": "2025-12-29 07:45:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi2h86",
              "author": "ergodym",
              "text": "Can you say more on the bash commands? Are those used for setting up Airflow jobs?",
              "score": 2,
              "created_utc": "2025-12-29 06:06:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwieczh",
                  "author": "t9h3__",
                  "text": "Bash commands are usually used to interact with servers or interact with other tools through your terminal (command line interface)\n\n* Copying / Uploading / Deleting files\n* Searching for things\n* Installing or Restarting applications on the server\n\nThese kind of things",
                  "score": 3,
                  "created_utc": "2025-12-29 07:48:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1px73zg",
      "title": "System Design/Data Architecture",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1px73zg/system_designdata_architecture/",
      "author": "Last_Coyote5573",
      "created_utc": "2025-12-27 20:02:10",
      "score": 15,
      "num_comments": 6,
      "upvote_ratio": 0.94,
      "text": "Hey folks, looking for some perspective from people who are looking for new opportunities recently. I’m a senior data engineer and have been heads-down in one role for a while. It’s been about \\~5 years since I last seriously was in the market for new opportunities, and I’m back in the market now for similar senior/staff-level roles. The area I feel most out of date on is **system design/data architecture rounds**.\n\nFor those who’ve gone through recent DE rounds in the last year or two:\n\n* In system design rounds, are they expecting a **tool-specific design** (Snowflake, BigQuery, Kafka, Spark, Airflow, etc.), or is it better to start with a **vendor-agnostic architecture** and layer tools later?\n* How deep do you usually go? High-level flow + tradeoffs, or do they expect concrete decisions around storage formats, orchestration patterns, SLAs, backfills, data quality, cost controls, etc.?\n* Do they prefer to lean more toward “design a data platform” or “design a specific pipeline/use case” in your experience?\n\nI’m trying to calibrate how much time to spend refreshing specific tools vs practicing generalized design thinking and tradeoff discussions. Any recent experiences, gotchas, or advice would be really helpful. Appreciate the help.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1px73zg/system_designdata_architecture/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw8u5z4",
          "author": "jaredfromspacecamp",
          "text": "“How would you design a streaming pipeline from our databases to redshift?” \n\n“How would you optimize the above for speed? How would you optimize it for cost?”\n\n“Design a data platform that streams from our database into a data warehouse, then serves the data back to the application” \n\nSome examples of questions I’ve got",
          "score": 10,
          "created_utc": "2025-12-27 20:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8xnci",
              "author": "Last_Coyote5573",
              "text": "and what responses have worked here or would work here? tech stack recommendations or theoretical knowledge?",
              "score": 2,
              "created_utc": "2025-12-27 20:30:03",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwafyam",
              "author": "BeardedYeti_",
              "text": "Yeah this is pretty accurate. I recently accepted a new Senior role but also had 2 other offers. I got a lot of these high level questions for the system design portion. I always tried to ask questions like real time requirements and cost requirements. Most of the time I wasn’t asked to design anything using specific tech or tools. They didnt care if i chose snowflake or databricks for example.",
              "score": 2,
              "created_utc": "2025-12-28 01:32:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw91bqu",
          "author": "casual_gamer11",
          "text": "I've been asked to design a IAM system to manage access to data. \n\nThe role required experience in data governance experience.",
          "score": 3,
          "created_utc": "2025-12-27 20:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9yylp",
              "author": "Last_Coyote5573",
              "text": "got it 👍",
              "score": 1,
              "created_utc": "2025-12-27 23:55:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgcwq",
          "author": "NeedleworkerIcy4293",
          "text": "Dm me i can help",
          "score": 1,
          "created_utc": "2025-12-28 15:14:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwuh1n",
      "title": "How to approach data modelling for messy data? Help Needed...",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pwuh1n/how_to_approach_data_modelling_for_messy_data/",
      "author": "HistoricalTear9785",
      "created_utc": "2025-12-27 10:12:11",
      "score": 14,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "I am in project where client have messy data and data is not at all modelled they just query from raw structured data with huge SQL queries with heavy nested subqueries, CTEs and Joins. queries is like 1200+ lines each that make the base derived table from raw data and on top of it PowerBI dashboards are built and PowerBI queries also have same situation as mentioned above.\n\nNow they are looking to model the data correctly but the person who have done this, left the organization so they have very little idea how tables are being derived and what all calculations are made. this is becoming a bottleneck for me. \n\nWe have the dashboards and queries.\n\n  \nCan you guys please guide how can i approach modelling the data?\n\n  \nPS I know data modelling concepts, but i have done very little on real projects and this is my first one so need guidance.",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pwuh1n/how_to_approach_data_modelling_for_messy_data/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw65gzl",
          "author": "vikster1",
          "text": "modelling has nothing to do with data quality. the model is defined either by the source or by your business needs. source example is a table from an erp system. the erp has a data model and if you pull more tables from the erp system, it's obvious what to do. more often than not your analytics model is defined by the business use case you are trying to solve/deliver. have customer data inside big messy raw data? just extract customer data so you have a nice customer table. go from there. it's not rocket science, just many many steps in the right direction until you arrive",
          "score": 9,
          "created_utc": "2025-12-27 10:20:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw682mg",
          "author": "LargeSale8354",
          "text": "I'm doing that now with a large DBT monolith.\n\nI start at the source and work forwards, and at the target and work backwards.\n\nWhere someone has a particular area of knowledge I spend time capturing that knowledge so, slowly, the gaps get filled.\n\nI start building a logical normalised model of the data. This has to be tech independent, I don't care if the physical world is NOSQL, RDBMS or whatever, I'm purely interested in what the actual data objects are, how you uniquely identify them and how they slot together. \n\nOnce you have a good logical model you have something to  drive business and technology conversations.  Expect to uncover some really dodgy processes and dubious calculations.  Also Expect to find architectural decisions that were made by someone with architect in their job title but not in their skillset.",
          "score": 4,
          "created_utc": "2025-12-27 10:46:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjc3ec",
              "author": "HistoricalTear9785",
              "text": "thanks",
              "score": 1,
              "created_utc": "2025-12-29 12:47:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw675ej",
          "author": "Ok-Working3200",
          "text": "Start with looking at the lost queries.  It's likely the code is similar, not t I mention some of the code isn't being used by the business.  Good time to drop stuff they aren't using.\n\nAnyways, by using the queries it should be come apparent what dimension tables you need and what grain you need the fact table at.  Also, you probably want to create a date table as well.",
          "score": 2,
          "created_utc": "2025-12-27 10:37:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjc2tm",
              "author": "HistoricalTear9785",
              "text": "thanks",
              "score": 1,
              "created_utc": "2025-12-29 12:47:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6d52b",
          "author": "umognog",
          "text": "Sometimes, there is a value to approaching the business that its time to redefine the measurements.\n\nWork forward with new principles, governance and avoid the same mistakes.\n\nI wouldnt even trust using the existing data as validation, because there is simply no telling if its valid itself.",
          "score": 1,
          "created_utc": "2025-12-27 11:34:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfyzvo",
          "author": "lou_expat",
          "text": "Great that the client is open to modeling correctly - many don't enough know what a data model is.  Start from outputs which give metrics filters and business logic.  Define grains (exactly what is the row and core entities) - not as simple as it seams - understand normalization tradeoffs often imposed by tools.   Extract business logic in existing SQL into documented model that business can read (get logic out of big queries).  Rebuild and validate one subject at a time (prioritizing of course).   Beware BI logic compensating for poor data modeling foundations ...",
          "score": 1,
          "created_utc": "2025-12-28 22:39:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgsqv3",
              "author": "Some_Alternative_391",
              "text": "You’re in the right spot starting from the outputs; that’s the only way to reverse‑engineer 1200‑line monsters into something sane. I’d add a really mechanical step-by-step approach: pick one dashboard, one core metric, and trace it all the way back to source tables. For that path, write down: grain, filters, joins, and every business rule in plain language. Then redesign just that slice into a small constellation: 1 fact, a few dimensions, and a clear SCD strategy. Ship and validate that before touching the rest.\n\n\n\nI’ve used dbt plus views in Snowflake and, in another shop, Fabric Warehouse with Power BI and DreamFactory to expose the cleaned model as read-only APIs for app teams, but the big win was always the same: shrink the blast radius and refactor one subject area at a time, never the whole estate in one go.",
              "score": 1,
              "created_utc": "2025-12-29 01:19:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjc1t1",
                  "author": "HistoricalTear9785",
                  "text": "Thanks bro... for guiding. immensely helpful",
                  "score": 1,
                  "created_utc": "2025-12-29 12:47:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwst2co",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-30 21:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvyoqu",
              "author": "dataengineering-ModTeam",
              "text": "Your post/comment was removed because it violated rule #6 (No seeking mentorship or job posts).\n\nWe do not intend for this space to be a place where people ask for, or advertise, referrals or job postings.  Please use r/dataengineeringjobs instead.\n\n ^*This* ^*was* ^*reviewed* ^*by* ^*a* ^*human*",
              "score": 1,
              "created_utc": "2025-12-31 09:15:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1px6nxs",
      "title": "DuckDB Concurrency Workaround",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1px6nxs/duckdb_concurrency_workaround/",
      "author": "ConsciousDegree972",
      "created_utc": "2025-12-27 19:43:36",
      "score": 14,
      "num_comments": 15,
      "upvote_ratio": 0.9,
      "text": "***Any suggestions for DuckDB concurrency issues?***\n\nI'm in the final stages of building a database UI system that uses DuckDB and later pushes to Railway (via using postgresql) for backend integration. Forgive me for any ignorance; this is all new territory for me!\n\nI knew early on that DuckDB places a lock on concurrency, so I attempted a loophole and created a 'working database'. I thought this would allow me to keep the main DB disconnected at all times and instead, attach the working as a reading and auditing platform. Then, any data that needed to re-integrate with main, I'd run a promote script between the two. This all sounded good in theory until I realized that I can't attach either while there's a lock on it.\n\nI'd love any suggestions for DuckDB integrations that may solve this problem, features I'm not privy to, or alternatives to DuckDB that I can easily migrate my database over to.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1px6nxs/duckdb_concurrency_workaround/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw92zvz",
          "author": "Own-Commission-3186",
          "text": "Using duck db storage file limits to one writer at a time. You can checkout using the duck lake extension to enable concurrent writers. This requires a postgres instance though but you'll get somewhat similar performance because it still stores data in a columnar format (parquet) and uses duckdb execution engine for querying",
          "score": 11,
          "created_utc": "2025-12-27 20:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw94jqy",
              "author": "ConsciousDegree972",
              "text": "Is Ducklake worth installing and configuring versus just migrating over to another database system, such as Postgresql? I'm using Dbeaver to manage.",
              "score": 1,
              "created_utc": "2025-12-27 21:08:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw98qsc",
                  "author": "Own-Commission-3186",
                  "text": "Just depends on the size of data, types of queries, amount of concurrency and latency you want. Can't really answer this for you but was just giving one alternative to your situation if you want to stick with using duckdb as the query engine",
                  "score": 5,
                  "created_utc": "2025-12-27 21:30:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwbe8dc",
                  "author": "MissingSnail",
                  "text": "For multi access ducklake you’ll need a postgres metadata store",
                  "score": 2,
                  "created_utc": "2025-12-28 05:06:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbfxva",
          "author": "MissingSnail",
          "text": "It really doesn't matter what file format you use, you don't want one process writing to a file at the same time that another is reading it. It leads to unpredictability. This is the problem that database systems solve. Can your writer process detach when not updating so the reader can read? Buenavista was a cool talk at one of the Duck Cons. Not sure if it’s production quality.  https://github.com/jwills/buenavista",
          "score": 3,
          "created_utc": "2025-12-28 05:19:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc27gg",
              "author": "ConsciousDegree972",
              "text": "This is interesting, thx! I’m setting up cron schedulers for both standalone scripts and irregular sequential scripts. So it would introduce a dance around it that I don’t want to have. Is Buena Vista solely for querying or can it be used for auditing data? A portion of my DB is auditing and approving new batches and then running a promote script back to my main DB.",
              "score": 1,
              "created_utc": "2025-12-28 08:35:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9asgj",
          "author": "andymaclean19",
          "text": "I think the answer to this is going to be heavily dependent on the type and size of the workload.   The frequency of change of data, number and type if queries, mix of read only and updates, concurrency levels, etc are all important and you probably need to provide more information before people can give you advice on whether, say, switching to another product is the right thing to do.",
          "score": 2,
          "created_utc": "2025-12-27 21:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc0t09",
              "author": "ConsciousDegree972",
              "text": "Totally — It’s a stock metric based system. So I’m tracking data on a day to day basis. Some running direct to my backend locally, bypassing the DB, but other scripts will run at least once daily to my DB. The regularity of queries is variable but at least quarterly, there will be a big influx of data that will need to be sifted through. I don’t want to be tiptoeing around cron schedulers, etc.",
              "score": 1,
              "created_utc": "2025-12-28 08:21:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwcx49n",
                  "author": "crispybacon233",
                  "text": "As other have said, ducklake could be great. You just need postgres for the catalog and s3 for parquet storage. Supabase could be a good option, since it comes with both out of the box. \n\nPostgres can be lightning fast if indexed properly according to your queries. I too have been building a stocks/options analytics project, and the indexed Postgres database performs extremely well.",
                  "score": 1,
                  "created_utc": "2025-12-28 13:15:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9jh0w",
          "author": "MPGaming9000",
          "text": "I think for me the answer would depend on your overall structural diagram of how the system works. Like for example in my case I built an app that has a UI & API & another SDK and they all need to talk to the DB (well the UI is just a visual layer). But from there what I had to do was have the SDK attached to the API and both the API & SDK just share the same DB instance and pass the code instance around. But if I couldn't do this, one thing I could do is create an http wrapper to allow for outside requests to hit the DB (sort of like a lightweight API around the DB server) with all the same auth and stuff of course. But... this gets impractical sometimes. Again this only works for one instance though, it's not practical for multiple instances. So beyond that it may be worth exploring other options as others have mentioned. But just my thoughts though.",
          "score": 2,
          "created_utc": "2025-12-27 22:28:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc0yjq",
              "author": "ConsciousDegree972",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2025-12-28 08:22:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwc0zbc",
                  "author": "exclaim_bot",
                  "text": ">Thank you!\n\nYou're welcome!",
                  "score": 1,
                  "created_utc": "2025-12-28 08:23:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwaue9n",
          "author": "GurSignificant7243",
          "text": "Easy peasy mate boilstream.com",
          "score": 1,
          "created_utc": "2025-12-28 02:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd3e8x",
          "author": "digEmAll",
          "text": "I have basically the same problem (still investigating how to solve it). During my investigations I found this:\nhttps://github.com/gizmodata/gizmosql\n(basically a db server around a duckdb file)\n\nAnd this https://github.com/jwills/buenavista, as others have already suggested",
          "score": 1,
          "created_utc": "2025-12-28 13:57:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkclmg",
          "author": "dodovt",
          "text": "You can make a queue and push changes to it every few changes. This way all components interface with the queue and then you only have one interface with DuckDB. However make sure to weigh the tradeoffs and try and implement guarantees on your queue ",
          "score": 1,
          "created_utc": "2025-12-29 16:11:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0pk3y",
      "title": "I feel conflicted about using AI",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q0pk3y/i_feel_conflicted_about_using_ai/",
      "author": "burningburnerbern",
      "created_utc": "2025-12-31 22:50:59",
      "score": 14,
      "num_comments": 17,
      "upvote_ratio": 0.65,
      "text": "As I’ve posted here before my skill really revolve around SQL and I haven’t gone really far with python. I know the core basics but never had I had to script anything. But with SQL I can do anything, ask me to paint the Mona Lisa using SQL? You got it boss but for the life of me I could never get past tutorial hell.\n\nI recently got put on databricks project and I was thinking that it’d be some simple star schema project but rather it’s an entire meta data driven pipeline written in spark/python. The choice was either fall behind or produce so I’ve been turning to AI to help me with creating code off of existing frameworks to fit my use case. Now I can’t help but feel guilty of being some brainless vibe coder as I take pride in the work that I produce however I can’t deny it’s been a total life saver. \n\nNo way could I write up what it provides. I really try my best to learn what and ask it to justify its decision and if there’s something that I can fix on my own I’ll try to do it for the sake of having ownership. Ive been testing the output constantly. I try to avoid having it give me opinions as I know it’s really good at gaslighting. At the end of it all ,no way in hell am I going to be putting python on my skill set. Anyway just curious as to what your thoughts are on this. ",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q0pk3y/i_feel_conflicted_about_using_ai/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwztist",
          "author": "LoaderD",
          "text": "I don’t AI code anything for production, that I couldn’t recode and explain without AI. \n\nIt’s just setting yourself up for failure. That being said, you can vibe-code, then deconstruct things and re-build them, because you still need to be producing enough to stay employed while you learn spark/python.",
          "score": 17,
          "created_utc": "2025-12-31 23:07:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzvgr5",
          "author": "thethirdmancane",
          "text": "You need to remove your ego and focus on what you are good at. For the most part AI is just going to be a better coder. Your job is to use it and adjust your workflow accordingly. \n\nAI is terrible at the big picture. AI is terrible at software engineering .That's where you come in. You need to start thinking less as a coder and more as a software engineer /architecct.\n\nRemember all that stuff you learned as a software engineer? Gathering requirements. Breaking problems down into small pieces. Incrementally building things. These are all skills that are still highly relevant.",
          "score": 63,
          "created_utc": "2025-12-31 23:19:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzzj6i",
              "author": "vanhendrix123",
              "text": "Great answer, I agree that this is the best approach",
              "score": 5,
              "created_utc": "2025-12-31 23:43:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx0vsat",
              "author": "moshujsg",
              "text": "What? This is terrible advice. You cant do arquitecting without understanding code at some level.\n\nThe solution? Learn to code. Spend as much tune as you need. You already have projects and goals, grab a book or a course, and get to it.",
              "score": -8,
              "created_utc": "2026-01-01 03:11:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2rdc6",
                  "author": "NoleMercy05",
                  "text": "Silly sounding as this:\n\nYou're not a real Coder unless you use ASM.\n\nYou should learn assembly on multiple Cpu architectures to understand what is really happening before using simple high level languages like Java.",
                  "score": 1,
                  "created_utc": "2026-01-01 13:28:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwzwuoz",
          "author": "x1084",
          "text": "Using AI coding assistants for programming is in some ways similar to how SWEs have been using Stack Overflow forever. You'll have lazy engineers who copy/paste solutions given to them and move on, and you'll have others who do their due diligence and test the solutions they find and try to understand the logic behind them. There's no getting around the adoption of AI in this space so its in your best interests to learn how to leverage it effectively. It sounds like you're on the right track.",
          "score": 23,
          "created_utc": "2025-12-31 23:27:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzzehl",
              "author": "PersonOfInterest1969",
              "text": "I love this analogy! AI is just like having the perfect SO answer available for every single one of your questions.\n\nETA: Until agentic AI that is, now it can write a whole code base.",
              "score": 3,
              "created_utc": "2025-12-31 23:43:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2ibp3",
              "author": "tiacay",
              "text": "SO at least have a validation mechanism using votes,  accepted answer. Early on using AI, when dive into completely new area, I lost a lot of time following their hallucinations before realized. It is often quicker to just reading official documents if that's available.\nBut overtime, when I'm already familiar with that stack, I can detect the deviation in AI's answer and provide follow up quaestions to got the right direction. At that point, I use AI mostly for summarizing or code snippet generation.",
              "score": 1,
              "created_utc": "2026-01-01 12:10:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2rp5f",
                  "author": "NoleMercy05",
                  "text": "Many old out of date answers are still marked 'Correct'.  New more correct answers are rejected.\n\nYou can keep it",
                  "score": 1,
                  "created_utc": "2026-01-01 13:30:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0vwbz",
              "author": "moshujsg",
              "text": "The differences is that he doesnt understand the code?",
              "score": -1,
              "created_utc": "2026-01-01 03:11:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwzx3zx",
          "author": "TheDevauto",
          "text": "Use it like you would an untested new assistant. You wouldnt trust a human with 30 days experience, so treat AI the same. Use it for small chunks of stuff and review to be sure you get it. \n\nUse it to answer questions or to present options for doing things in different ways.",
          "score": 7,
          "created_utc": "2025-12-31 23:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzx6u1",
          "author": "heisoneofus",
          "text": "You shouldn’t. Code agents, LLMs are just tools to help propel you forward. Think of it as a more practical rubber duck. It’s inherently a learning tool that helps you not “waste” time learning but also produce effective work while you are learning and acquiring useful skillset.",
          "score": 3,
          "created_utc": "2025-12-31 23:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx04p85",
          "author": "themightychris",
          "text": "Your job is to see the big picture, envision what architecture will work best long term, break the problems down, and validate solutions\n\nThose parts are all the same and are going to stay the same\n\nYou just don't have to bang on your keyboard and stack overflow for hours now to make the code that does what you want. Embrace it and reap the benefits of getting to put more of your time and energy into all the other parts",
          "score": 3,
          "created_utc": "2026-01-01 00:15:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0f2fo",
          "author": "AI--Guy",
          "text": "Not all people that Vibe Code are brainless and folks like yourself that think that way will quickly find the sleeves unemployed or working 5 layers below the guy that figured it out",
          "score": 3,
          "created_utc": "2026-01-01 01:19:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzzwdg",
          "author": "AZjackgrows",
          "text": "Use the AI but put in the time to understand what it means and find some basic exercises/courses to build baseline understanding of syntax and libraries. \n\nThe AI will fail from time to time. Also learn from the bugs it creates and ask it to explain/document the code it builds so you can understand the big picture. \n\nThe AI isn’t going anywhere. The people who will succeed in the industry are those who learn how to leverage it most effectively and understand its limitations.",
          "score": 2,
          "created_utc": "2025-12-31 23:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx00feg",
          "author": "ResidentTicket1273",
          "text": "AI generated stuff looks good only to the people who aren't experts. You need to be competent, or you might as well be anyone off the street. Experts know when AI is bullshitting (about 70% of the time, it's wrong) and you need to be in a position to know when it's confidently right, and when it's confidently wrong. There's no getting away from it - AI will always look good to managers (i.e. those who don't know what they're doing) because they don't know any better. If you want to hoover up the easy money when it all goes wrong, then get good.",
          "score": -2,
          "created_utc": "2025-12-31 23:49:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx048sd",
          "author": "ttmorello",
          "text": "Bruh pyspark its like pandas, buy some pandas course on udemy for 10 usd",
          "score": -1,
          "created_utc": "2026-01-01 00:12:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwccv7",
      "title": "Who owns data modeling when there’s no BI or DE team? (Our product engineering team needs help)",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pwccv7/who_owns_data_modeling_when_theres_no_bi_or_de/",
      "author": "Groove-Theory",
      "created_utc": "2025-12-26 19:14:37",
      "score": 13,
      "num_comments": 20,
      "upvote_ratio": 0.9,
      "text": "**Long ass post sorry. Skip to the bottom for the TL;DR questions if you don't want the backstory.**\n\n# Backstory:\n\nHowdy... not entirely sure this is the right subreddit for this (between here and the BI sub) but figured I'd start here.\n\nOk so... I'm a tech lead for our engineers working on our core product in a startup. I am NOT on the data engineering or BI side of things, but my involvement in BI matters is growing, and this is me sanity-checking what I see.\n\nOur data stack is I think ok for a startup. We source our data, which is mostly our main Postgres DB plus with a few other third party tracking sources, with 5X into our staging tables in BigQuery. Then we use dbt to bucket our data into dimensions, fact tables, and what are called \"reporting tables\" which are the highest 1-to-1 tables that are used in whatever presentation layer we use (which is Looker). Our ingestion/bootstrap logic all exists in a GitHub repo.\n\nThis entire system was originally designed and put together by a very experienced senior data engineer when we were in a scaling phase. Unfortunately, they were laid-off some time ago cuz of runway issues before they could completely finish everything. Since that time, our management has continually pushed for additional and additional reporting, but we haven’t replaced that position. And it's getting worse.\n\nToday, we have ONE business analyst (not on the eng team) with no tech skills, having learned SQL basics from ChatGPT. They create reports as best as they can, but idk how correct they are in querying stuff from the BI layer (frankly I don't care tbh, not the eng team's concern)\n\nAnyway, the business comes to us with a regular set of new reporting requirements for tables, but many of these do not make sense. At all.\n\nFor example: *\"I’d like a list of all cars, but also like a column for how much spaghetti people eat per day, and then a column of every fish in the sea, and we need a dashboard for the fish-spaghetti-car metric per month \"*. That kind of bullshit\n\nSince we still have a reduced team post-layoffs, product management has started working on sprint stories for any product improvement we do such as *“Create a reporting table for the spaghetti bullshit above\"* despite the underlying data structure being ambiguous or incorrect (and not being a spaghetti company). Which I think is pretty fucking weird that they're telling us what the actual implementation should be.\n\nWe, as software engineers, are comfortable designing application schemas and writing database queries against Postgres (and the PG layer is well formed imo). We, however, are not professionals in business intelligence, and we are facing more and more questions about dimensional design, report structure, which are questions we feel uncomfortable answering.\n\nThe most aggravating part of this process is the business will attempt almost anything rather than considering adding another senior BI or data engineering person to the staff. They have attempted to draw general engineering talent into doing business intelligence tasks when that isn’t their technical niche. They have attempted to use short-term or lower-quality consultants. Many times, they have simply pressed onward with what we understand to be an iffy model.\n\nIncreasingly I spend my time fighting off requests against our team or explaining to others why some of those requests are simply nonsensical (in a polite manner of course) but I feel I'm slowly losing that fight over time, and my head of Product/Eng is not helping me here.\n\nI always knew the business was crazy when just dealing with product AC, but I've realized they really go fucking bonkers when you talk to them about anything related to a dashboard.\n\n# My questions to ya'll\n\n*(skip to here if you didn't want to read my sob story above)*\n\nMy questions are about whether we have a common concept of \"good\" data modeling and who really is responsible. The engineering department is picking up all of this slack, and BI isn’t really our expertise. So...\n\n* When is the time for the BI/data modeling necessarily a full-time endeavor and not something that should be accomplished as part of the product engineering team, if at all? Are there any heuristics that you have observed for smaller startups?\n* Is there ever value in planning or building \"bad\" or ugly reporting tables to meet current business requirements, or is it almost always harmful?\n* If leadership wants speed and they do not have data modeling knowledge, what data governance patterns work well for you?\n* How do you communicate concepts of dimensional modeling to non-technical business audiences in a way that leads to lasting behavior change? (If at all lol)\n* Finally, if leadership is flatly unwilling to engage experienced BI/DE talent, then what is the least worst alternative you've encountered?\n\nI'm way outside my lane here as a non-DE so any advice is greatly appreciated. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pwccv7/who_owns_data_modeling_when_theres_no_bi_or_de/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nw2h5ar",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-26 19:14:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2iqtz",
          "author": "Phenergan_boy",
          "text": "> This entire system was originally designed and put together by a very experienced senior data engineer when we were in a scaling phase. Unfortunately, they were laid-off some time ago cuz of runway issues before they could completely finish everything.\n\nSounds like you need this guy back lol",
          "score": 54,
          "created_utc": "2025-12-26 19:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2lkr6",
              "author": "financialthrowaw2020",
              "text": "Yep. DE owns it. No DE, no owner. Don't take on work that isn't yours, OP, unless you wanna work 2 jobs for the same pay.",
              "score": 14,
              "created_utc": "2025-12-26 19:38:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2og8w",
                  "author": "Phenergan_boy",
                  "text": "Honestly, unless OP has equity in the business, idk why they wouldn’t try to get out. ",
                  "score": 8,
                  "created_utc": "2025-12-26 19:54:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2nwcy",
              "author": "Groove-Theory",
              "text": "I agree :(",
              "score": 5,
              "created_utc": "2025-12-26 19:51:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwcgtfr",
              "author": "BringtheBacon",
              "text": "If the very experienced senior data engineer that designed the entire data infrastructure was laid off from a data centric company, that type of poor decision would make me question operational decisions. My understanding of startups is there are many critical decision points early on that play a large role in the initial success and subsequent market growth and this presents a large question mark ❓",
              "score": 1,
              "created_utc": "2025-12-28 10:56:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2k830",
          "author": "nogodsnohasturs",
          "text": "Not being flippant, but: since you're the one asking the question, it sounds like you own it now. But maybe your job works differently than mine does. Or, you can push back and get them to hire somebody.\n\nIf you find data-savvy staff in the business, they should be data stewards of the relevant chunks, and they should be the ones who determine the exact business rules that you will need to use to stub out requirements and refine.\n\nAt that point it's likely up to the dev team to build out views for the BI analyst to connect to. \n\nYou (and the other poster) are correct that there is a gap in the organization, and ideally your BI person would fill that role, but it sounds like they don't have the technical skills to do so. \n\nSo, if you're the person in the metaphorical building with SQL chops, congratulations, and I'm sorry. \n\nTL;DR in absence of an actual semantic model owned by the DE or BI folks, your safest bet is to build views \"upstream\" that they can connect to, and to designate data stewards in the business who are responsible for making decisions about what needs to be counted and how to count it.",
          "score": 8,
          "created_utc": "2025-12-26 19:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2qbfq",
              "author": "Groove-Theory",
              "text": ">\\> safest bet is to build views upstream and designate data stewards\n\nYeah... I couldn't quite formulate it as nicely as you did but your right, it's a responsibility issue.\n\nOne framing I keep coming back to was something like \"engineering’s responsibility is to put the correct building blocks into the BI layer... now go ahead and prove to us your stupid reports actually make sense with them and we just might put that into the BI layer too\".\n\nI've prompted some business folk recently when we get a dipshit request by saying \"you know, go on Google Sheets and draft how that report should look like\". I was thinking they'd get the hint and realize their report structure was nonsensical. But they come back with some not-well-formed shit thinking it was a great draft.... mostly because I don't think they realize SQL will yell at them (or duplicate rows) once they hit the rubber on the road.\n\nBut I see your point (if I understand correctly). The business (or stewards) should be responsible for making that work as long as we have the right pieces. I shoulda kept going with it to say \"ok but go on looker and use the dims and facts to query it right\". Seems much healthier than us continually creating bespoke reporting tables that encode ambiguous logic.\n\nAnd hopefully they get off their ass and actually invest in a BI/DE resource\n\n>\\> if you're the person in the metaphorical building with SQL chops, congratulations, and I'm sorry.\n\n:(",
              "score": 3,
              "created_utc": "2025-12-26 20:04:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw7h6sc",
                  "author": "TurbulentSocks",
                  "text": "Yeah, I think this is the right take. Unfortunately, is also now your responsibility to explain why the spaghetti metric is nonsense *and* come with a better one. Good luck, because if the business doesn't see this as a technical question, they're going to insist on their approach *and* likely there will be fallout when it doesn't work.",
                  "score": 1,
                  "created_utc": "2025-12-27 16:01:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2wlj5",
                  "author": "nogodsnohasturs",
                  "text": "Yep. They need to feel the pain. If a homeowner tells an architect to design a ten foot toilet, and the contractors build it, that's on the homeowner. Good luck and solidarity!",
                  "score": 0,
                  "created_utc": "2025-12-26 20:39:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2ntdw",
          "author": "codemega",
          "text": "It sounds like you already know the issues and have your hands tied. Leadership doesn't see the problem and you can't change them. You can explain what the issues are and then just proceed to build the spaghetti stuff to the best of your ability.",
          "score": 4,
          "created_utc": "2025-12-26 19:50:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ro3y",
          "author": "bobbruno",
          "text": "If nobody will own the data/BI layer, it's bound to get messy over time. And if there's no one who knows how to design/build/manage a BI layer, it's also bound to get messy over time.\n\nThe thing is, if you only have bad reports, that's what the business will make decisions on. That and whatever they can come up with on their spreadsheets and side/shadow controls. I have no way to say I'd this is a minor issue or a huge risk, it may well be that the BI Eng who was laid off got enough working that the core operation is covered, and you'd only miss some opportunities. There are so many ways that BI can generate value, and so many ways it can be useless that it's impossible to say what your case is.\n\nYou do have the right gut feeling, but there's little hope of fixing the issue(s) without proper ownership and knowledge. From what you described, it looks like you don't even have someone who knows how to get BI requirements right. For example, if someone comes asking for a report on how many cars and how much spaghetti, you don't throw it as a ticket into the backlog - you ask about the decisions they're making, how the relationships between these things work, what's the desired outcome, how they'll use the information, along with frequency, level of detail, quality and possibly what other questions (at least what data domains) we're talking about. After that, you have to determine where this data is (and it might not be anywhere), how to source it, how to structure it for the specific kind(s) of analysis and decision they want to make, and then you have to figure out how that relates to the rest of the data model you already have. And then you may be able to build something.\n\nEven skipping half of what I described, you can't escape some of it, and you need to know what you're doing. You also need ownership from the business, because this will need to be operated and maintained, and that comes with a cost. If there's no ownership, maybe on one wants that cost.\n\nBasically, start by finding someone who can explain why this matters, and then see if they can put enough value on it to justify the DE position. Just remember that, if you start asking questions, people may assume you're taking ownership. If you think that's a career move or a problem, I also don't know for sure.",
          "score": 2,
          "created_utc": "2025-12-26 23:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2nwgt",
          "author": "PrestigiousAnt3766",
          "text": "I can imagine SWE building an api / configuring reporting related to the tool.\n\nCombining sources and modeling is a different beast, as is talking / interpreting the business. Sadly they fired your DE, sounds like they knew what they did.\n\n\nI normally don't talk about modeling concepts to business. I explain why data can/cannot be combined , and suggest easier implementations. Sometimes business wants something very complicated that is \"easy\" in their eyes, but is also really happy with an easy to build alternative if you suggest it.\n\nIf no SWE wants to do BI, I'd just make it very expensive for management. ",
          "score": 1,
          "created_utc": "2025-12-26 19:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw56bp4",
          "author": "SRMPDX",
          "text": "Since there's no DE or BI team and management doesn't want to invest do whatever takes the least amount of your time and let them deal with it. The answer is to not be cheap.",
          "score": 1,
          "created_utc": "2025-12-27 05:00:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwchb5z",
          "author": "BringtheBacon",
          "text": "I’m curious, what is the size of your startup? What is your startups general stage in the product lifecycle? Is it likely financially feasible, that a new DE focused staff would be hired soon? Sorry this reads like a reality tv show to me, I’m trying to gauge likelihood of a successful pivot",
          "score": 1,
          "created_utc": "2025-12-28 11:00:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwerka7",
              "author": "Groove-Theory",
              "text": "\n\n>what is the size of your startup\n\nright now the engineering team is 5 + me, but 60 overall (its a very CusOps heavy startup). The total eng team at its peak was a dozen some years ago after Series A (and we also used to have designers and a product team. And an actual and fantastic  Head of Eng who left after the layoffs, but now the defacto head I mentioned is one of the cofounders, whos not even a SWE by profession)\n\n\n>What is your startups general stage in the product lifecycle\n\nWe've had some form of market fit for over 6 years now and our product is I would say very mature now. \n\nI say \"some form\" because 2 years ago we've had to shift our ICP because smaller customers were churning due to inflation (hence the layoffs and our reversion to a more seed-stage startup) but larger customers could afford our product. Although that has come with new pivots to accommodate, I would say now the product fit its definitely viable and secure.\n\n>Is it likely financially feasible,\n\n Yes, it is definitely financially feasible (we will turn profitable early next year for the first time with our new customer base). The eng team has built a lot of automations to not have to scale the CusOps team further by employee count (I keep telling the CEO we won't be a \"tech company\" if we keep scaling by CusOps count)\n\nAnd we're about to sign off on a huge deal with a couple customers that will almost instantly double or triple our ARR (right now $3-4 mil, estimated $50 mil valuation based on what were previous acquisition discussions last year)\n\n\n>that a new DE focused staff would be hired soon?\n\nMonetarily, yes. There should be no problem. \n\nPolitically.... depends who else in the business gets the share of the pie with our new income windfall (my CEO has been conferring with all sections of the business, including me, on our scaling phase next year. Everyone is bracing for an explosion in growth)\n\nAnd \"Data Maturity\" is now a new vertical in our 2026 roadmap. I've made this very clear to him that gen eng cannot handle this if he wants to scale other technical parts of the core product like performance, security, observability, velocity, reliability, etc.\n\n<rant>\n\nThere's a bit of a headache from me cuz the current Head of Product/Eng, said they want to hire a data C-level person for it because he thinks the reason its a mess now is because the old DE wasn't \"a leader\" and we need \"a leader\". Me and the other engineers called bullshit to their face (politely) and said its because you let them our great DE go and couldn't invest in it but still wanted your dashboards. I dont think they understand still\n\n</rant>\n\nBut yes we COULD invest in a DE very very soon. Which is why ill be extremely pissed if he does not.\n\n\n\n\n> Sorry this reads like a reality tv show to me\n\nCall it Stockholm syndrome but this isnt even the worst I've seen in my career. Besides the clusterfuck I wrote, the actual eng architecture and stack (non-data) is the best I/we've ever built and maintained all things considered, especially for a product that is pretty complex in terms of what it does (we're not just a CRUD app, not even close)\n\nThough sometimes I do miss when I worked enterprise.",
              "score": 1,
              "created_utc": "2025-12-28 19:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pymz5m",
      "title": "10-Year Plan from France to US/Canada for Data& AI – Is the \"American Dream\" still viable for DEs?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pymz5m/10year_plan_from_france_to_uscanada_for_data_ai/",
      "author": "MassyKezzoul",
      "created_utc": "2025-12-29 13:59:26",
      "score": 13,
      "num_comments": 15,
      "upvote_ratio": 0.79,
      "text": "I’ve spent the last 3 years as a Data Engineer (Databricks) working on a single large-scale project in France. While I’ve gained deep experience, I feel my profile is a bit \"monolithic\" and I’m planning a strategic shift.\n\nI’ve decided to stay in Paris for the next 2 to 3 years to upskill and wait out the current \"complicated\" climate in the US (between the job market and the new administration's impact on visas/immigration). My goal is to join a US-based company with offices in Paris (Databricks, Microsoft) and eventually transfer to the US headquarters (L-1 visa).\n\nI want to move away from \"classic\" ETL and focus on:\n\nData Infrastructure & FinOps: Specifically DBU/Cloud cost optimization (FinOps is becoming a huge pain point for the companies I'm targeting).\n\nGovernance: Deep dive into Unity Catalog and data sovereignty.\n\nData for AI: Building the \"plumbing\" for RAG architectures and mastering Vector Databases (Pinecone, Milvus, etc.).\n\nThe Questions:\n\n- The stack i'm aiming for is it what the companies are/will looking for ?\n\n- The 3-Year Wait: Given the current political and visa volatility in the US (Trump administration policies, etc.), is a 3-year \"wait and upskill\" period in Europe seen as a smart hedge, or am I risking falling behind the US tech curve?\n\n- Targeting US offices in Paris: Are these hubs still actively facilitating internal transfers (L-1) to the US, or has the \"border tightening\" made this path significantly harder for mid-level / Senior engineers?\n\nThanks for ur time !\n\n",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pymz5m/10year_plan_from_france_to_uscanada_for_data_ai/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nwjt4p9",
          "author": "bezelsandbourbon",
          "text": "I can't speak to all your points but a couple thoughts.\n\nFocusing on finops, infrastructure and governance will continue increase in demand. More companies will inevitably scrutinize their cloud bills a compute grows. Adoption of AI will only increase that trend IMO.\n\nThe US tech scene is rapidly evolving. I'm not sure if your wait and up skill strategy will work out because I can't tell the future but I'd wager you want to be as close to the action/industry as possible if you want to make a career out of it. Whether that's the American market or the French I can't comment on as I don't know enough about the French tech scene.\n\nI think layoffs are going to be common for tech workers in the next 5 years as the world figures out what applications to AI are and what new demands are created from the technology. It's the most exciting and terrifying time to work in tech IMO",
          "score": 6,
          "created_utc": "2025-12-29 14:32:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkm4dk",
          "author": "defuneste",
          "text": "French working in the US (now also US citizen)    here.\nMore \"MS stacks\" (think Azure, SSIS, etc) than in france. I think DE jobs are still very heterogeneous not just around the stack but also around the data maturity so I am not sure if you will be \"behind\".\n\nFor the US, citizenship is a huge help (or at least 3 years residency) since lot of gov contractors require it. \n\nApplying in big corps without recommandation is hard now (my impression) and the \"first\" round is still some automated \"leetcode\" so you should prepare it even if we are in a stupid red queen theory / arms race against AI leetcode  \"cheat\" tools.",
          "score": 7,
          "created_utc": "2025-12-29 16:56:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl9smr",
              "author": "MassyKezzoul",
              "text": "I already work on Azure too. I'm not really afraid of the specific stack. Just need to learn what make a company pay a DE more than another.\n\nThanks for the feedback",
              "score": 4,
              "created_utc": "2025-12-29 18:46:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjvjhc",
          "author": "DeliciousProgress865",
          "text": "Habitants au Canada, depuis 3 ans et voyant le marché des États-Unis de près, mon avis d’un point de vue stratégique est que je ne te conseille pas nécessairement d’attendre 3 ans si tu as pour ambition d’aller aux États-Unis.\nJe profiterais de ces 2,3 ans pour justement postuler régulièrement et développer ma compréhension du marché.\n❓Pourquoi je dis qu’il ne faut pas attendre 2 ou 3 ans car la stratégie que tu as sur le papier est bonne et viable, mais les probabilités de réussite sont maigres (pas impossible) alors il faut saisir tout ce qui est envisageable pour justement maximiser ses chances et ça passe par dès maintenant postuler, développer du réseau. \nLe problème des États-Unis c’est qu’il y a beaucoup de prétendants mais très peu d’élus  \nGood luck 🤞🏽",
          "score": 5,
          "created_utc": "2025-12-29 14:46:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl7ndq",
              "author": "MassyKezzoul",
              "text": "L'idée derrière l'attente de 2 ou 3ans c'est justement pour maximiser mes chances mais en proposant un profil plus intéressant plutôt que du classique ETL-Maker. Actuellement, avec les compétences que j'ai, je ne me vois pas du tout arriver à décrocher quoi que ce soit aux USA. \n\nMerci pour ta réponse en tout cas.",
              "score": 1,
              "created_utc": "2025-12-29 18:36:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwx7g6b",
                  "author": "Beneficial_Nose1331",
                  "text": "Passes les certification databricks.\nSinon la Suisse c'est pas mal aussi.\nLe truc c'est de quitter la France le plus vite possible.",
                  "score": 1,
                  "created_utc": "2025-12-31 14:54:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwk7rod",
          "author": "thinkingatoms",
          "text": "get a job in uk with American company.  in a few years move to US. ezpz",
          "score": 2,
          "created_utc": "2025-12-29 15:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl80fy",
              "author": "MassyKezzoul",
              "text": "Why specifically UK ? I can do the same here in France ig.",
              "score": 1,
              "created_utc": "2025-12-29 18:37:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwlmemn",
                  "author": "thinkingatoms",
                  "text": "just more opportunities, if you can do the same in France more power to you",
                  "score": 1,
                  "created_utc": "2025-12-29 19:45:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwn2euo",
          "author": "chock-a-block",
          "text": "Unless your goal is to start a business, there isn’t really much for an employee.\n\nIf you have aspirations to join the ranks of management, maybe it’s better.\n\nSeconding finding an employer with an office in France. ",
          "score": 1,
          "created_utc": "2025-12-30 00:09:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpz73r",
              "author": "MassyKezzoul",
              "text": "Still think that is better than France. How much do you think a good senior DE can get as a salary in the US ? \n\nBesides, there is more interesting project in the US market.",
              "score": 1,
              "created_utc": "2025-12-30 12:40:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwqth3a",
                  "author": "chock-a-block",
                  "text": "Be very mistrustful about the top line salary number. Lots of that is soaked up in not obvious ways. Rent and healthcare that you might not use being the most obvious. \n\nOutside of finance, you’ll work hard to get low six figures and a grind. That won’t get you far at all in major metropolitan markets. \n\nI agree you might find more interesting things to work on.  \n\nTwo areas that are undergoing massive changes that have no borders are electricity, bioinformatics, genomic data science. ",
                  "score": 1,
                  "created_utc": "2025-12-30 15:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwoevp5",
          "author": "Truth-and-Power",
          "text": "Why do you want to come to the us?",
          "score": 1,
          "created_utc": "2025-12-30 04:44:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpysfj",
              "author": "MassyKezzoul",
              "text": "Having already expatriated to France from my home country, I am well-adapted to relocating into a new one. My primary motivations for moving to the us are financials and professional prestige. By that, I mean the opportunity to work on high-impact projects an gaining experience that will be highly valued upon a potential return to France ~10 years later.\n\nMy professional experience in France has revealed some limitations. I have encountered rapid salary plateaus and feel that the local work culture and pace do not align with my desire for experience and salary growth. I am seeking the more dynamic professional rhythm found in the us.",
              "score": 2,
              "created_utc": "2025-12-30 12:37:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwrwpy",
      "title": "PDFs are chaos — I tried to build a unified PDF data extractor (PDFStract: CLI + API + Web UI)",
      "subreddit": "dataengineering",
      "url": "https://v.redd.it/tjj318as7p9g1",
      "author": "GritSar",
      "created_utc": "2025-12-27 07:29:00",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Open Source",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pwrwpy/pdfs_are_chaos_i_tried_to_build_a_unified_pdf/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q0j29f",
      "title": "Tessera — Schema Registry for Dbt",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1q0j29f/tessera_schema_registry_for_dbt/",
      "author": "Low-Sandwich-7607",
      "created_utc": "2025-12-31 17:53:02",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.85,
      "text": "Hey y'all, over the holidays I wrote Tessera (https://github.com/ashita-ai/tessera)\n\nIt's like Kafka Schema Registry but for data warehouses. If you're using dbt, OpenAPI, GraphQL, or Kafka, it helps coordinate schema changes between producers and consumers.\n\nThe problem it solves: data teams break each other's stuff all the time because there's no good way to track who depends on what. You change a column, someone's dashboard breaks, nobody knows until it's too late. The same happens with APIs as well. \n\nTessera sits in the middle and makes producers acknowledge breaking changes before they publish. Consumers register their dependencies, get notifications when things change, and can block breaking changes until they're ready.\n\nIt's open source, MIT licensed, built with Python/FastAPI. \n\nIf you're dealing with data contracts, schema evolution, or just tired of breaking changes causing incidents, have a look: https://github.com/ashita-ai/tessera\n\nFeedback is encouraged. Contributors are especially encouraged. I would love to hear if this resonates with problems you're seeing!",
      "is_original_content": false,
      "link_flair_text": "Open Source",
      "permalink": "https://reddit.com/r/dataengineering/comments/1q0j29f/tessera_schema_registry_for_dbt/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1puqz1v",
      "title": "Looking for feedback on open source analytics platform I'm building",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1puqz1v/looking_for_feedback_on_open_source_analytics/",
      "author": "Psychological_Goal55",
      "created_utc": "2025-12-24 16:15:14",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "I recently started building Dango - an open source project that sets up a complete analytics platform in one command. It includes data loading (dlt), SQL transformations (dbt), an analytics database (DuckDB), and dashboards (Metabase) - all pre-configured and integrated with guided wizards and web monitoring. \n\nWhat usually takes days of setup and debugging works in minutes. One command gets you a fully functioning platform running locally (cloud deployment coming). Currently in MVP. \n\nWould this be something useful for your setup? What would make it more useful? \n\nJust a little background: I'm on a career break after 10 years in data and wanted to explore some projects I'd been thinking about but never had time for. I've used various open source data tools over the years, but felt there's a barrier to small teams trying to put them all together into a fully functional platform. \n\nWebsite: [https://getdango.dev/](https://getdango.dev/) \n\nPyPI: [https://pypi.org/project/getdango/](https://pypi.org/project/getdango/) \n\nHappy to answer questions or help anyone who wants to try it out.",
      "is_original_content": false,
      "link_flair_text": "Open Source",
      "permalink": "https://reddit.com/r/dataengineering/comments/1puqz1v/looking_for_feedback_on_open_source_analytics/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvxjsro",
          "author": "ColdStorage256",
          "text": "I've never used a tool like this but I see them posted quite often. How does it work exactly? Is it similar to when I create a skeleton Flutter app... Do I type your command and then it creates a bunch of files in my current directory? Or do I run it and then visit local host and drop a csv in to start exploring data (like the other tools posted here)?\n\n\nGenuinely curious as I've never used something outside of a module inside my project.",
          "score": 3,
          "created_utc": "2025-12-25 22:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzfsbp",
              "author": "Psychological_Goal55",
              "text": "thanks for your question!\n\nrunning `curl -sSL` [`https://getdango.dev/install.sh`](https://getdango.dev/install.sh) `| bash` (mac/linux) in terminal does:\n\n* checks prerequisites (Python 3.10+, Docker)\n* creates a venv and installs getdango + all dependencies (dlt, dbt, DuckDB, etc.)\n* asks for a project name and creates that directory with project structure\n* auto-generates config files connecting everything (dbt profiles → DuckDB, Metabase → DuckDB, dlt → DuckDB)\n\n(dango is a CLI tool you run from terminal, not a library you import - more like docker-compose or jupyter)\n\nyou can also install manually (creates venv, `pip install getdango`, `dango init`) if you prefer that workflow. the script just combines those steps with prompts.\n\nafter install, you add sources via the wizard (`dango source add`) or config files directly - CSV files, or 30+ API sources via dlt (Stripe, Google Sheets, GA4, Facebook/Google Ads, etc.), then run `dango start`. this pulls Docker images (Metabase, etc.) and starts the web UI at localhost:8800 where you can upload/sync data, access pre-configured dbt docs and Metabase, and monitor everything.\n\nso yes, it creates a complete data project skeleton with ingestion (dlt), transformation (dbt), database (DuckDB), and visualization (Metabase) all pre-wired together. you can drop CSV files in a folder, configure API sources, write SQL transformations, and build dashboards.\n\nto answer your original question: it's both - creates files (like Flutter skeleton), then you start services and use localhost. the value is having the full stack already integrated rather than as separate pieces you wire together.",
              "score": 3,
              "created_utc": "2025-12-26 05:57:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1uper",
          "author": "higeorge13",
          "text": "Nice project! \nOne comment about your website; width in mobile safari doesn’t look great. \nWhat’s the plan for cloud version? ",
          "score": 3,
          "created_utc": "2025-12-26 17:17:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw85eqv",
              "author": "Psychological_Goal55",
              "text": "Thank you! I'll look into the mobile safari layout. For cloud deployment, I'm still thinking through the right approach, something that feels familiar to those with cloud experience, but approachable if you haven't. Probably a CLI command to deploy to your cloud provider of choice. Still working through the details, hoping to have something basic out soon.",
              "score": 2,
              "created_utc": "2025-12-27 18:03:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3xodv",
          "author": "CashMoneyEnterprises",
          "text": "Something that might make it more useful is broadening the tools in each category since you're going with open source options. Something along the lines of letting an end user dlt Vs Airbyte or sqlmesh vs dbt.\n\nOverall though definitely useful if a startup or something doesn't have much in the way of data infrastructure and wants the fastest and lowest cost solution up and running asap",
          "score": 2,
          "created_utc": "2025-12-27 00:09:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8a8r0",
              "author": "Psychological_Goal55",
              "text": "Thanks for the feedback, that's a fair point! For now I went with an opinionated approach, my guess being that for small teams starting out (where I'm hoping this would be most helpful), the differences between tools like dlt vs Airbyte may not matter that much, yet some teams (based on my experience) spend too much time evaluating the \"best\" tool before getting started. That said, I'd like to keep things modular enough that swapping tools out is possible down the line, and offering tool choices during setup could make sense too.",
              "score": 1,
              "created_utc": "2025-12-27 18:27:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwa679h",
                  "author": "CashMoneyEnterprises",
                  "text": "Makes sense, also fwiw dango is very close to django - maybe good for marketing but just something to point out since it may confuse some people",
                  "score": 1,
                  "created_utc": "2025-12-28 00:35:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwcur8n",
                  "author": "umognog",
                  "text": "I think not just small teams starting out, but to any professional low on experience that needs to skill up.\n\nMy employer regularly hires graduates for much cheapness and they walk out of University with their masters in data science, extremely confident that they will smash it and they are...shit.\n\nThey dont know what a venv is or why or how to use one, efficiency is \"whats that\".\n\nSo i like to use tools like this for onboarding, because i can set a 3 week skills path helping them learn, whilst being entirely self contained. As a local venv, i can create failures and problems with known solutions, meaning its easier for the team to help coach them.\n\nAnd lastly, if they ever need to reference, its really easy to hop into the venv to check it out.",
                  "score": 1,
                  "created_utc": "2025-12-28 12:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw60toi",
          "author": "dknconsultau",
          "text": "I can see the value / use case for this where I have clients who are a bit 'big cloud' scared or prefer open source. Will def have a play with it! Thanks for sharing!",
          "score": 2,
          "created_utc": "2025-12-27 09:34:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8hba3",
              "author": "Psychological_Goal55",
              "text": "Thanks for checking it out! I've been at companies where anything involving another vendor was a nightmare, so I get that. Hope it helps, let me know if you run into any issues!",
              "score": 2,
              "created_utc": "2025-12-27 19:03:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwe7bwm",
          "author": "campbell363",
          "text": "Love it! \n\nFrom a visualization perspective, Metabase is great for the basics. To get more customization, I'd love something like this for D3.js components. For example, Python's Dash or Observable Plot JavaScript, etc.",
          "score": 1,
          "created_utc": "2025-12-28 17:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfog6r",
          "author": "AromaticAd6672",
          "text": "Incremental load options, auto recs? Maybe link with great expectations for data quality , lineage mapping , retry and recovery options (with watermarks for incremental) perhaps. Love the idea. I keep striving for  a perfect metadata driven framework but cannot land it..good luck. My firm uses synapse and feel like im  behind now",
          "score": 1,
          "created_utc": "2025-12-28 21:46:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvwpk3",
      "title": "Which coursera course is best for someone who needs to quickly build a data warehouse?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1pvwpk3/which_coursera_course_is_best_for_someone_who/",
      "author": "littlefoxfires",
      "created_utc": "2025-12-26 05:37:18",
      "score": 10,
      "num_comments": 16,
      "upvote_ratio": 0.71,
      "text": "Hi everyone,\n\nI am a data analyst currently tasked with building a data warehouse for my company. I would say I have a basic understanding of data warehousing and my python and SQL skills are beginner to mid level. I will mainly be learning on the job, but seeing as my company provide free coursera licenses, I figured I could use it and get some structured learning as well to complement my on-the-job learning. \n\nCurrently I am deciding between IBM’s data engineering  specialization and Joe Reis’s Deeplearning Ai data engineering 4-course series. I have heard negative things about IBM’s course but also that it could be good as an overview if you’re a beginner. \n\nSeeing as I would have no mentor (I am the only analyst there and the only person there to even know what data warehousing and dimensional modeling is), what I ideally want is a course that will inform me on best practices and any tradeoffs and edge cases I should consider. My organization is pretty cost sensitive and not very mature analytics wise, so in general, I really wanna avoid just following trends (e.g. using expensive tools that my org doesn’t necessarily need at this stage) and doing anything that would add technical debt. \n\nAny advice is welcome, thank you!",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1pvwpk3/which_coursera_course_is_best_for_someone_who/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "nvzdgzz",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2025-12-26 05:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzql6j",
          "author": "West_Good_5961",
          "text": "There’s a quick, easy read called The Data Warehouse Toolkit by Ralph Kimball.",
          "score": 25,
          "created_utc": "2025-12-26 07:36:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzr8ba",
              "author": "littlefoxfires",
              "text": "I am reading that too. Just wanted a course to go along as well.",
              "score": 4,
              "created_utc": "2025-12-26 07:43:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1f5u7",
          "author": "TowerOutrageous5939",
          "text": "Start simple and cheap. Pipelines dumping to storage buckets, Postgres as the warehouse. Document everything and focus on lineage early. SQL mesh or DBT help with such. \n\nI’m guessing since you don’t have a large analytics org your data is not large. That’s a good thing.",
          "score": 4,
          "created_utc": "2025-12-26 15:54:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5bg12",
          "author": "SRMPDX",
          "text": "There's no such thing as \"quickly build a data warehouse\". There's really no such thing as \"quickly build a data warehouse while learning what it is\". There's especially no such thing as \"quickly build a data warehouse, while learning what it is, for really cheap\", but good luck.",
          "score": 6,
          "created_utc": "2025-12-27 05:39:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw30ixa",
          "author": "GreyHairedDWGuy",
          "text": "read some books on the subject (Inmon, Kimball for example).  I wouldn't look at coursera videos until you've done some reading first.   The is a lot of suspect content out there.\n\nIf possible look to bring on a contract DE who has done the design/build at least a couple times before and use them as a mentor.  Costs $$ but so does doing it wrong.",
          "score": 2,
          "created_utc": "2025-12-26 21:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5szuy",
          "author": "No_Song_4222",
          "text": "Why do you want to really build it ? You having a good understanding of warerhouse does not mean your company exec also have the understanding of it. Your company exec might needs a daily excel report at the of day.\n\nIf your data requirements are small you can test the free tier version of AWS, GCP,Azure , Databricks which you can use for free all year/month long as long as you obey quota limits ?! All of them offer all services with different names, UI, and other stuff.  Including the BI tools like Looker, Quicksight etc .\n\nE.g. Google BigQuery offer 10GB and 1TB of data processing for free. After 1tB you are charged only \\~7$ for every 1Tb.\n\nGive it a shot. Come up with an estimated budget see if the company is okay with that once you are satisfied with free tier and grow outside the free tier limits ? You don't have to load all the data, scope what is important for business maybe sales data, maybe forecasting data, maybe accounts, maybe finance etc.\n\nIf is not mature analytics and data wise why bother to build a warehouse in the first place that to you being the only analyst  ? The stakeholder wants a dashboard/report of sales yesterday and you would be giving one after several months ?\n\nTo win everyone heart : First understand what your exes need . See if this is as simple as plain charts and filters in a excel/spread sheet after exporting your SQL query results as csv",
          "score": 2,
          "created_utc": "2025-12-27 08:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwowidd",
              "author": "littlefoxfires",
              "text": "My execs want the company to grow eventually analytics-wise and they’re hesitant to hire someone from the outside. \n\nWe don’t have a lot of data yet, the most is around 500k. However, I am also pulling everything from relational databases which is not the best (lots of joins…etc). We also have lots of different systems so I need to pull data from several sources just to make one dashboard. \n\nSo mainly we want to have a central place where I can easily pull the data I need. Also good for data audit and consistency as well.",
              "score": 1,
              "created_utc": "2025-12-30 06:58:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzmhom",
          "author": "WearFar1074",
          "text": "Hi, \n\nYou'll gain immense value and knowledge from a peer/mentor or even youtube than coursera/IBM courses. I'm a Staff Data Engineer transitioning into Data Science so I can tell you the online courses will not be your best bet. Find someone that can help to design it and structure it.",
          "score": 3,
          "created_utc": "2025-12-26 06:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzztbc",
              "author": "littlefoxfires",
              "text": "Not sure I can find a mentor. For youtube, do you have recommendations?",
              "score": 3,
              "created_utc": "2025-12-26 09:13:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0iges",
          "author": "Illustrious_Web_2774",
          "text": "I'd throw a generic question at perplexity research and start looking at the references.\n\n\nThen I'd bounce ideas at chatgpt.\n\n\nHaving built 4 enterprise data warehouses / platforms in different orgs from scratch, I really don't understand the value of courses.",
          "score": 3,
          "created_utc": "2025-12-26 12:18:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0z94y",
              "author": "snarleyWhisper",
              "text": "I find courses good if I need to learn a bunch of context at once. Like if I’m diving into AWS or dbx",
              "score": 3,
              "created_utc": "2025-12-26 14:21:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1t1q5",
          "author": "Muted_Jellyfish_6784",
          "text": "between those two joe reis’s series is usually the more practical option especially if you need realworld guidance on modeling  trade offs and avoiding unnecessary complexity IBM’s course is okay for a broad intro but parts of it feel a bit dated since you won’t have a mentor, it can also help to browse communities where people share lightweight modeling approaches and warehouse best practices. r/agiledatamodeling has some good discussions that line up with what you’re trying to do",
          "score": 1,
          "created_utc": "2025-12-26 17:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlktyq",
          "author": "Accomplished_Cloud80",
          "text": "I did whole data science course at Coursera. No one care for it in my resume.",
          "score": 1,
          "created_utc": "2025-12-29 19:38:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}