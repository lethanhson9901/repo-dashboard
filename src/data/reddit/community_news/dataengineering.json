{
  "metadata": {
    "last_updated": "2026-02-18 17:29:57",
    "time_filter": "week",
    "subreddit": "dataengineering",
    "total_items": 20,
    "total_comments": 332,
    "file_size_bytes": 367333
  },
  "items": [
    {
      "id": "1r73l52",
      "title": "In 6 years, I've never seen a data lake used properly",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r73l52/in_6_years_ive_never_seen_a_data_lake_used/",
      "author": "wtfzambo",
      "created_utc": "2026-02-17 11:31:23",
      "score": 398,
      "num_comments": 208,
      "upvote_ratio": 0.97,
      "text": "I started working this job in mid 2019. Back then, data lakes were all the rage and (on paper) sounded better than garlic bread.\n\nBeing new in the field, I didn't really know what was going on, so I jumped on the bandwagon too.\n\nThe premises seemed great: throw data someplace that doesn't care about schemas, then use a separate, distributed compute engine like Trino to query it? Sign me up!\n\nFast forward to today, and I hate data lakes.\n\nEvery single implementation I've seen of data lakes, from small scaleups to billion dollar corporations was GOD AWFUL.\n\nMassive amounts of engineering time spent into architecting monstrosities which exclusively skyrocketed infra costs and did absolute jackshit in terms of creating any tangible value except for Jeff Bezos.\n\nI don't get it.\n\nIn none of these settings was there a real, practical explanation for why a data lake was chosen. It was always \"because that's how it's done today\", even though the same goals could have been achieved with any of the modern DWHs at a fraction of the hassle and cost.\n\nChoosing a data lake now seems weird to me. There so much more that can be done wrong: partitioning schemes, file sizes, incompatible schemas, etc...\n\nSure a DWH forces you to think beforehand about what you're doing, **but that's exactly what this job is about**, jesus christ. It's never been about exclusively collecting data, yet it seems everyone and their dog only focus on the \"collecting\" part and completely disregard the \"let's do something useful with this\" part.\n\nI understand DuckDB creators when they mock the likes of Delta and Iceberg saying \"people will do anything to avoid using a database\".\n\nAnyone of you has actually seen a data lake implementation that didn't suck, or have we spent the last decade just reinventing RDBMS, but worse?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r73l52/in_6_years_ive_never_seen_a_data_lake_used/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5um6li",
          "author": "Secure_Firefighter66",
          "text": "All this is happening because the management needs to adapt to new technologies. \n\nMy company was running in On Prem until 1.5 years back and I was specifically hired to setup AWS + Databricks. Because the management decided its cloud era. \n\nSame tables , same dimensions, but within Databricks. \nOnly positive thing is I get paid to do this.",
          "score": 269,
          "created_utc": "2026-02-17 11:42:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5urpe6",
              "author": "Thavash",
              "text": "And then they release Databricks SQL (after all this damage) and celebrate  \"new features\" like Temp Tables and Stored Procedures",
              "score": 95,
              "created_utc": "2026-02-17 12:23:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5y6m2k",
                  "author": "BakersCat",
                  "text": "This lmao, Databricks/Delta Lake have basically spent the past 6 years reinventing what SQL Server, Postgres etc have been doing anyway for the past 20+ years.",
                  "score": 24,
                  "created_utc": "2026-02-17 22:39:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o60i12p",
                  "author": "ummitluyum",
                  "text": "The wheel reinvention cycle is real ðŸ˜‚ We spent 10 years convincing everyone SQL was \"legacy\" while building complex MapReducelihon/Scala. Now weâ€™re heroically adding SQL interfaces, ACID transactions, and constraints back because... surprise! The business needs reliability, and analysts need SQL\n\nNext step: theyâ€™ll invent Foreign Keys and rebrand it as \"revolutionary semantic linking\"",
                  "score": 3,
                  "created_utc": "2026-02-18 07:10:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5v75ss",
                  "author": "kthejoker",
                  "text": "Neither temp tables nor stored procedures are good patterns for modern OLAP style data warehouse work, even worse on a lakehouse with access to object storage.\n\nIntroducing these is capitulation to migrating legacy system, that's all",
                  "score": 2,
                  "created_utc": "2026-02-17 13:57:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5whuoy",
                  "author": "bornagainsmiles",
                  "text": "This is hilarious and the current state of the times. Lol",
                  "score": 1,
                  "created_utc": "2026-02-17 17:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5umtjl",
              "author": "tbot888",
              "text": "Donâ€™t fight it you need to get paid to do something.",
              "score": 39,
              "created_utc": "2026-02-17 11:47:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5uno9u",
                  "author": "Secure_Firefighter66",
                  "text": "Doing that",
                  "score": 8,
                  "created_utc": "2026-02-17 11:53:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5uomsr",
              "author": "runawayasfastasucan",
              "text": "\"What if we made all querying with a lag?\" - databricks.",
              "score": 59,
              "created_utc": "2026-02-17 12:01:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5v0g79",
                  "author": "Visionexe",
                  "text": "Does this not described any modern cloud platform/software as a service? ðŸ¤£ (With some exceptions.) I feel what most companies need is the reliability of IaaS, maybe with a few off the selve db systems. And thats about it.Â ",
                  "score": 8,
                  "created_utc": "2026-02-17 13:19:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5woo2m",
                  "author": "KWillets",
                  "text": "Not a lag, a coffee break.",
                  "score": 2,
                  "created_utc": "2026-02-17 18:23:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5uojwi",
              "author": "PossibilityRegular21",
              "text": "Didn't on-prem have scaling costs and maintenance challenges? I'm not strictly against it, especially with the dominance of AMG, but I couldn't imagine going back.Â ",
              "score": 17,
              "created_utc": "2026-02-17 12:00:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5uoph0",
                  "author": "Secure_Firefighter66",
                  "text": "Well for us the data size is small less than 500 GB. \n\nSo 2 tb itself is good for few years.",
                  "score": 20,
                  "created_utc": "2026-02-17 12:01:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uop72",
                  "author": "wtfzambo",
                  "text": "37signals spend the last 2 years moving away form cloud and back to on-prem and estimate a $2kk savings or something like that",
                  "score": 17,
                  "created_utc": "2026-02-17 12:01:38",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5uq2al",
                  "author": "Budget-Minimum6040",
                  "text": "Depends on the data size and data needs.",
                  "score": 6,
                  "created_utc": "2026-02-17 12:11:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5uqvaa",
              "author": "bubzyafk",
              "text": "Many companies goes with modernization as you mentioned.. but many realized itâ€™s similar to data warehouse, then people question â€œwhy need to go with datalakeâ€â€¦\n\nSince you are using Dbx already, then at least you should know.. long time back all started with big data.. 1 variable of big data is the â€œVariatiesâ€, meaning, in this world data is not only Structured.. there are semi structured like log data or IoT and unstructured.. (yeah I know nowadays newer Db has builtin json/xml reader or whatnot to process semi structured, Iâ€™m referring to old classic dwh).. there are Streaming use cases, and not only batch one.\n\nDatabricks or similar hyperscaler not only trying to solve datalake, but they introduced Lakehouse, which tldr â€œdata warehouse on top of datalakeâ€.. you can do many stuffs.\n\nCompany complained and saying, having hyperscaler/cloud/databricks is overkilled. But once your company grows, many different use cases, and if you trying to fit it into old-system dwh, many would fail when you trying to connect it all together. But when you do that to modern tech, even it can process your grandmotherâ€™s picture in binary or do some Machinelearning on top of itâ€¦",
              "score": 7,
              "created_utc": "2026-02-17 12:17:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5xuijj",
              "author": "sparkplay",
              "text": "There are other benefits of Cloud though: data retention security, connectors, no single point of failure Head Priest, etc. I've worked in one of many on premise situations where the connection speed was worse than a Cloud POC; we had and one time someone forgot to turn on the AC and all servers crashed.\n\nAlso OP's question isn't about on-prem vs Cloud but DWh vs Data Lakes.",
              "score": 5,
              "created_utc": "2026-02-17 21:40:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5yf5n9",
              "author": "iknewaguytwice",
              "text": "Not even kidding, Iâ€™ve been asked to migrate SQL views and stored procs for Fabric warehouses to replace read only database replicas.",
              "score": 3,
              "created_utc": "2026-02-17 23:24:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5yfduk",
                  "author": "Secure_Firefighter66",
                  "text": "My next project is AWS + Databricks to Microsoft Fabrics. This is due to our parent company wants to centralise the data for all subsidiaries",
                  "score": 1,
                  "created_utc": "2026-02-17 23:26:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5xsgam",
              "author": "thisisntinstagram",
              "text": "Good to know Iâ€™m not alone in this hell.",
              "score": 1,
              "created_utc": "2026-02-17 21:30:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5xu78s",
              "author": "Sufficient_Meet6836",
              "text": "Edit: nvm looked your profile a bit and other details didn't match up. Damn that would have been funny\n\nDoes your company do software as a service...\n\n>My company was running in On Prem until 1.5 years back and I was specifically hired to setup AWS + Databricks. Because the management decided its cloud era.\n\n>Same tables , same dimensions, but within Databricks.\nOnly positive thing is I get paid to do this.\n\nWe might work together LMAO",
              "score": 1,
              "created_utc": "2026-02-17 21:38:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5z55c2",
              "author": "thatguywes88",
              "text": "Sounds about what my company is in the process of startingâ€¦",
              "score": 1,
              "created_utc": "2026-02-18 01:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5unagf",
          "author": "PossibilityRegular21",
          "text": "I sort of like a bit of lake and a bit of warehouse. A common loading pattern we have been using is:\n\n\n- for streaming: source --> Kafka --> snowflake (snowpipe streaming to tables)\n\n\n- for batches: source --> AWS s3 (~lake) --> snowflake (external tables)\n\n\n- in both cases once in Snowflake: raw staged tables (bronze) --> structured, type-cast, deidentified views (silver) --> Kimball/star/mart views with metadata (gold)\n\n\nI've been liking this system so far. The key difference with streaming and batch in the above cases are that the batch method keeps the raw/bronze data in s3 via external tables, so I guess that's a \"lake\", while the streaming method loads the CDC events into a table resting in the snowflake data warehouse. We use dagster to orchestrate and dbt to run the jobs. The technologies are good - the challenges are behavioural in nature.\n\n\nThere's probably a more consistent way to do the above, but it does work. I guess the lake/s3 component just exists because it is simple and cheap to read from some provided s3 dump than to add a \"copy into\" step. We would probably would have done the same for streaming, but snowpipe streaming is a good enough solution at the moment so we can skip a redundant intermediate load to s3.",
          "score": 68,
          "created_utc": "2026-02-17 11:50:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uox5y",
              "author": "wtfzambo",
              "text": ">for batches: source --> AWS s3 (~lake) --> snowflake (external tables)\n\nWhy to S3? Why not directly to Snowflake, especially since you're already using it as a destination for other data?",
              "score": 6,
              "created_utc": "2026-02-17 12:03:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v43lg",
                  "author": "Scary-Constant-93",
                  "text": "S3 is like cheap landing zone for data much cheaper than storing everything in snowflake \n\nAlso you donâ€™t need to decide on schema or model data  first as you can store raw data as it is. \n\nAnd most importantly it acts as source of truth which you can use as replay layer it also avoids vendor locking for your raw data\n\nNothing wrong in skipping s3 but you wonâ€™t loose on above benefits",
                  "score": 29,
                  "created_utc": "2026-02-17 13:40:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uqet8",
                  "author": "Budget-Minimum6040",
                  "text": "In the end you can use every storage, it's just about saving raw payloads without knowing the schema beforehand / guarding against schema drift.",
                  "score": 12,
                  "created_utc": "2026-02-17 12:14:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uyzy7",
                  "author": "strugglingcomic",
                  "text": "Believe it or not, this can actually be cheaper at the end of the day, vs writing everything directly to physical Snowflake storage (even with the extra storage cost of an extra \"copy\" of data in S3). Also gives you the option of choosing to leave infrequently used data in the S3 storage layer, and only bring the more commonly used columns into physical Snowflake storage (or rarely, sometimes people use this pattern to filter rows and not just columns, in terms of which rows they choose to bring into Snowflake).",
                  "score": 8,
                  "created_utc": "2026-02-17 13:11:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uq1he",
                  "author": "clbarb",
                  "text": "I used to do this when I led data analytics at a small company. After a while I realized there was no need for S3 (all our data was structured) and I only did it because I was told to. Eventually I scrapped it.",
                  "score": 9,
                  "created_utc": "2026-02-17 12:11:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5vdcuv",
                  "author": "throw_mob",
                  "text": "i did it because access to files from other places was harder when files were stored in snowflake vs s3, but yes it is possible to just save files into snowflake.",
                  "score": 1,
                  "created_utc": "2026-02-17 14:31:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5vkyb5",
              "author": "MgmtmgM",
              "text": "So all of your batch tables are external tables in your raw layer? And then are you using dynamic tables on top of them to build silver?",
              "score": 1,
              "created_utc": "2026-02-17 15:10:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vm4wt",
                  "author": "pimadd_",
                  "text": "Not op, but we have a similar structure, I use Airflow to build the silver layer. Most of our sources are either apis or databases, so I built two custom dags, one ApitoS3Operator, and one DBToS3Operator which takes yaml configs as input, and then outputs it to S3, then I also have an SQLExecuteOperator which runs the script from raw to silver.",
                  "score": 3,
                  "created_utc": "2026-02-17 15:16:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5y0taj",
                  "author": "PossibilityRegular21",
                  "text": "Not using dynamic tables. As I understand it, the benefit of dynamic tables would be more if we had streamed data and we wanted low latency reads downstream, such as to send data back out of our data warehouse to salesforce. But for slow batches, we are already committing to low enough latency for tables and views in orchestrated DBT jobs.\n\n\nBasically I try to convince stakeholders that they don't need rapid access to OLAP data (they virtually never do) and 24 hr latency is virtually always enough.",
                  "score": 2,
                  "created_utc": "2026-02-17 22:09:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5umrbw",
          "author": "Splun_",
          "text": "I think datalakes exist because data-driven stuff got popular, people started accumulating more data since like 5 years ago when it was all the rage, and then suddenly huge decentralized companies figured that their data infrastructure is hot garbage.\nDatalake and databricks, although costly with money/time/resources, allows to handle that hot garbage in some way â€” easily pump in money into a solution that works within a few clicks, giving people a few tools to pull and process everything in one place.\n\nI always try to choose a proper DB like clickhouse, snowflake, whatever, whenever I can. Model the infrastructure (make it modular and scalable), create some processes, and give power to the people within some defined boundaries. Itâ€™s more work, but I feel itâ€™s easier â€” after inital cost I can go do streaming, swap out tools, optimize DB tables, create alert systems and stuff.\n\nPlus the experience managing your own files, metadata, debugging fucking notebooks is atrocious. But maybe thatâ€™s just me. I like sitting in my black terminal with a box cursorâ€¦.",
          "score": 24,
          "created_utc": "2026-02-17 11:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5up5qd",
              "author": "wtfzambo",
              "text": ">itâ€™s more work, but I feel itâ€™s easier â€” after inital cost I can go do streaming, swap out tools, optimize DB tables, create alert systems and stuff. \n\nExactly. Yet I've seen nearly nobody do this.\n\n> Plus the experience managing your own files, metadata, debugging fucking notebooks is atrocious. But maybe thatâ€™s just me. I like sitting in my black terminal with a box cursorâ€¦. \n\nI'm with you on this. If one puts notebooks in prod they should be sent to jail.",
              "score": 14,
              "created_utc": "2026-02-17 12:05:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wbluy",
                  "author": "SilverShyma",
                  "text": "There's a lot that I would never wanna do in my db or warehouse. It's actually a solid landing zone, I don't wanna deal with unnesting json ingested via APIs or store it all in my db. \n\nPlus the lake gives replayability, so i don't have to go back and talk to slow paginated APIs just to check what went wrong.",
                  "score": 3,
                  "created_utc": "2026-02-17 17:23:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5uq8bp",
              "author": "Budget-Minimum6040",
              "text": "Notebooks are not for prod. Don't run notebooks in prod.",
              "score": 9,
              "created_utc": "2026-02-17 12:12:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vakui",
                  "author": "R0kies",
                  "text": "And what do you run in prod? Sequence of scripts?",
                  "score": 2,
                  "created_utc": "2026-02-17 14:16:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5vt1ug",
                  "author": "pboswell",
                  "text": "I get what youâ€™re saying, but not everyone has the know-how to productionslize scripts. In a data mesh world, that puts the bottleneck on the engineering team to convert other folksâ€™ stuff.",
                  "score": -4,
                  "created_utc": "2026-02-17 15:50:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5unb3k",
          "author": "dadadawe",
          "text": "Data lake yes, Lakehouse no\n\nMy last 2 projects use a data lake as staging and structured store as warehouse and it works great. Tools and teams can share data onto S3 in their native format and this gets used for many things:\n\n\\- Our own operational dashboards with basically 0 extra costs, no other teams needed\n\n\\- Some local transformations we run for our own processes\n\n\\- Sharing a subset of data with other teams\n\n\\- Staging for the data warehouse (with an SQL abstraction layer)\n\nNow if you try to make your silver layer purely file based... yeah I wouldn't do it if I just have financial and sales data...",
          "score": 24,
          "created_utc": "2026-02-17 11:51:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uo6yg",
              "author": "PossibilityRegular21",
              "text": "Agreed - data lake is fine for bronze/raw. You really want well-defined schema in a data warehouse for the silver/structured layer. Otherwise you introduce so many complications around regulatory compliance, schema evolution, tests and type casting.",
              "score": 12,
              "created_utc": "2026-02-17 11:57:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5uz5sc",
                  "author": "fourby227",
                  "text": "Isnâ€™t this the idea behind a data lakehouse? An hybrid where you may use a data lake for bronze and silver/gold are data warehouses but perhaps in form of iceberg tables on s3.",
                  "score": 9,
                  "created_utc": "2026-02-17 13:12:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5up2v2",
              "author": "confusing-world",
              "text": "Hi. I'm a beginner in the field. Can you elaborate better what is the problem of using files in the silver layer? For example, using parquet there is a bad idea? What technology would you suggest in the silver layer?",
              "score": 4,
              "created_utc": "2026-02-17 12:04:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5uq2xa",
                  "author": "wtfzambo",
                  "text": "Imagine you go to class and take notes. You do this all day every day, so you end the week with a lot of notes but not really organized.\n\nYou can choose to keep them as is and try to arrange them as best you can, or you can choose to re-write them, categorize them, color code, create an index etc, even maybe transcribe them to Notion so that when you need to go and prepare for the DSA exam you don't neet to scamble through 3 binders of notes to find them, you just open Notion and in the search box type \"DSA\".",
                  "score": 7,
                  "created_utc": "2026-02-17 12:11:49",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5uq5iu",
                  "author": "dadadawe",
                  "text": "The answer is always \"it depends\". \n\nIf your primary use case is data that is inherently structured (which most business data is) then forcing it into Parquet files, building complex compute pipelines is just waste. In the end you'll flattened it into PowerBi or expose an SQL view, so why not use an SQL database, those things are great at structured workloads. Plus everyone can read SQL\n\nThis changes when you have lots of complex data formats, or your data structure changes a lot, or your use case is not analytics or simple data feeds into CRUD tools. Maybe you just have so much data that SQL would explode (unlikely nowadays, but maybe). In those cases, knock yourself out",
                  "score": 5,
                  "created_utc": "2026-02-17 12:12:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uqrla",
                  "author": "Budget-Minimum6040",
                  "text": "Technology? A database.",
                  "score": 2,
                  "created_utc": "2026-02-17 12:16:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5yxoz2",
                  "author": "pboswell",
                  "text": "Itâ€™s not a bad idea to use parquet. Every database literally just stores the data as files. It basically comes down to portability (i.e. vendor lock-in). If you go with Microsoft SQL server, youâ€™re locked into proprietary file formats. Parquet is portable and almost any technology can interact with them.",
                  "score": 2,
                  "created_utc": "2026-02-18 01:07:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5upc7h",
              "author": "wtfzambo",
              "text": "Interesting take. Lemme ask you this: why not raw directly in the DWH? Are you using a lot of unstructured data?",
              "score": 1,
              "created_utc": "2026-02-17 12:06:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5uqr8q",
                  "author": "dadadawe",
                  "text": "No, mostly JSON and structured tables. I'm sure it can be achieved too with some ETL or messaging platform, but this is the architecture that we used (my two last clients actually) and I think it works well\n\nFor me the main direct benefit is that our own team can just use the data lake data directly. We can add, remove, report etc. Whereas the persistent staging you had in older architectures would be super complex to maintain\n\nI also think there is benefit in storing your data raw in the native format for reuse later (LLM feeding for example) but that's a personal opinion\n\nEdit: it's also very helpful that our team can manage our own folder in the lake, without needing write access to the DWH. We just agree on the overall architecture and the data contracts, but for the rest we manage our own back yard. Back in the day you'd possibly need to spin up a server for that (get it approved) or have some guy's PC run in the background. In the end a datalake in this setup is nothing more than a file server with Cron jobs on steroids",
                  "score": 6,
                  "created_utc": "2026-02-17 12:16:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uwtv3",
                  "author": "vdueck",
                  "text": "In several projects I depend on other teams exporting data from their own tools. Itâ€™s much easier to tell them to dump files into storage than to explain how to load data into a database.",
                  "score": 3,
                  "created_utc": "2026-02-17 12:57:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5vd9kd",
                  "author": "TheRealStepBot",
                  "text": "Very little real world data is structured in any system of any meaningful complexity. If all you do is push data between systems you control then a warehouse is fine. Most interfaces across org boundaries are minimally documented and ever changing json. If itâ€™s critical to store all this data and process every one of these records you need to be able to have traceability back to how it was parsed and what the original payload was.",
                  "score": 0,
                  "created_utc": "2026-02-17 14:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5upteq",
          "author": "siliconandsteel",
          "text": "Because it really is a database, just leveraging cheap cloud storage.",
          "score": 7,
          "created_utc": "2026-02-17 12:09:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uqjdt",
              "author": "wtfzambo",
              "text": "it really isn't a database. Even just getting concurrent writes properly is a goddamn nightmare.",
              "score": 3,
              "created_utc": "2026-02-17 12:15:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5veman",
                  "author": "TheRealStepBot",
                  "text": "You do understand that acidity is not a requirement of all systems right? Itâ€™s a very specific ability that is used to solve very specific issues. There are no free lunches. Blanket acid guarantees are extremely expensive. \n\nBy only providing the concurrency guarantees where you need them when you need them you can independently scale various parts of the system to hit much better throughout than a single blanket guarantee like you find in a traditional database can handle. \n\nWhy do you need concurrent writes? Itâ€™s very easy to coerce concurrent writes into shard bounded writes that only need concurrency within a particular shard which is vastly more performant. Keep following this idea and you eventually get to lakes that have limited inherent concurrency guarantees.",
                  "score": 9,
                  "created_utc": "2026-02-17 14:38:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5v7nil",
                  "author": "kthejoker",
                  "text": "You can turn on isolation modes for pessimistic concurrency like a traditional database if you want to.\n\nLocks everywhere? Go for it",
                  "score": 4,
                  "created_utc": "2026-02-17 14:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5v58wl",
          "author": "nus07",
          "text": "Computing is pop culture. Pop culture holds a disdain for history. Pop culture is all about identity and feeling like youâ€™re participating. It has nothing to do with cooperation, the past or the futureâ€”itâ€™s living in the present. I think the same is true of most people who write code for money. They have no idea where [their culture came from].\nâ€”Alan Kay, in interview with Dr Dobbâ€™s Journal (2012) , DDIA\n\nMy leadership sells datalake with the idea that data scientists can do exploratory analysis on the raw unstructured data. Itâ€™s been over a year and I have yet to see any exploratory analysis or insights happen.",
          "score": 8,
          "created_utc": "2026-02-17 13:47:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wbu1c",
              "author": "wtfzambo",
              "text": "That's a very unique and interesting take. I find myself agreeing to it.",
              "score": 1,
              "created_utc": "2026-02-17 17:24:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v0k8u",
          "author": "billionarguments",
          "text": "It's the continuation of the concept of democratization of data, only on steroids. For years it's been all the rage to position data lakes as some sort of magic data library where \"data managers\" float around and browse every byte of the corporate data mass, somehow promoting and furthering those data, preferably delegating the quality and cleaning it up with the insanely over-engineered and dubious conceptual process of data stewardship, and then somehow with no-code UI design a pipeline to make perfect and automatically published and semantically described data sets that anyone can consume at every whim of middle management and executives.\n\nAnyone in this business clearly understood from the beginning that this in 99% of organizations and use cases is a utopian pipe dream. The result are what we see right now.",
          "score": 7,
          "created_utc": "2026-02-17 13:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uz7rc",
          "author": "MaverickGuardian",
          "text": "Disagree on the cost part. Depends on usage and data amounts but s3 and Athena in AWS is lot cheaper for us than spinning up redshift. And we can't use other products than what aws has to offer. Data amounts are so big that postgres can't handle adhoc aggregates fast enough anymore. Talking about multiple billions of rows tables.\n\nBut yeah. Setting things up and keeping it running in AWS is painful.",
          "score": 5,
          "created_utc": "2026-02-17 13:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wbe7l",
              "author": "wtfzambo",
              "text": "In another comment I wrote about how in some org I worked for, someone had set up a system that managed to rack up $20-40k/month in S3 costs due only to PUT requests, because they were streaming a gazillion of data in 24/7 to iceberg tables from the company's ERP.",
              "score": 1,
              "created_utc": "2026-02-17 17:22:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o615rix",
                  "author": "MaverickGuardian",
                  "text": "Yeah. S3 can get really expensive when written outside AWS.",
                  "score": 2,
                  "created_utc": "2026-02-18 10:48:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5up645",
          "author": "snackeloni",
          "text": "It's because so many people have a tool first mentality. Our staff data engineer is an aws fan boy and I've never seen such a badly implemented, convoluted and overengineered mess. As the analytics engineer I've unfortunately had very little say in all off this. And the fun part: he's the only person that seems to know how any of this works. If this guy leaves, we're fucked. I mean for management I suppose, I'm going to laugh my ass off if that happens :p",
          "score": 9,
          "created_utc": "2026-02-17 12:05:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5upjp1",
              "author": "wtfzambo",
              "text": "> It's because so many people have a tool first mentality\n\nOh man I feel this. I had a glimpse of this horror when an acquaintance of mine asked me \"what's the best tool to learn for data engineering\" and I was like \"no such thing, go study the fundamentals\" and he was pissed at me.",
              "score": 7,
              "created_utc": "2026-02-17 12:07:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5vdit0",
              "author": "TheRealStepBot",
              "text": "Just like op with is always use a database idea ironically",
              "score": 1,
              "created_utc": "2026-02-17 14:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5umqlw",
          "author": "No-Satisfaction1395",
          "text": "I donâ€™t see any reason why I would want to go back to a database after adopting Delta?",
          "score": 9,
          "created_utc": "2026-02-17 11:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uqe3o",
              "author": "wtfzambo",
              "text": "Because it's like we invented lighters, someone was not happy with it and decided to invent their own version of the lighter but it's a convoluted Rube Goldberg machine that is 1.000.000 times slower and every now and then can explode killing everyone in a mile radius.",
              "score": 2,
              "created_utc": "2026-02-17 12:14:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5uqxaz",
                  "author": "No-Satisfaction1395",
                  "text": "Idk about that, youâ€™re sort of implying that databases are always neat, tidy and faster. They suffer from the same problems. You ever seen a database thatâ€™s a mess? I have.\n\nI just donâ€™t see a reason to pick a database now, unless Iâ€™m forced",
                  "score": 9,
                  "created_utc": "2026-02-17 12:17:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5vg6t4",
                  "author": "TheRealStepBot",
                  "text": "Databases arenâ€™t general, unopinionated abstractions. They are leaky abstractions designed under specific technical constraints to serve particular uses. \n\nYes they are useful in many cases but this idea that they are some perfect abstraction is absolutely ludicrous. Most database engines can trace their histories back to a time when data was stored on tape drives and having a 10mb disk as a â€œfast cacheâ€ in front of that was impressive. They retain much of the accompanying assumptions about what one would want to store and how you would like to store it. \n\nItâ€™s not the 1970s anymore where data arrives in neatly minimalist little individual numbers and varchar arrays. \n\nThere is an absurd amount of unstructured or semi structured data floating around that need to be stored and organized and worked with and traditional databases architecturally just arenâ€™t ready to absorb that.\n\nI think this was more true 5 or 10 years ago that today as you actually are starting to see a lot more hybrid systems that look like databases but behind the scenes are actually managed lakehouses that store stuff to blob storage",
                  "score": 3,
                  "created_utc": "2026-02-17 14:46:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5urm8z",
          "author": "ReporterNervous6822",
          "text": "Maybe. I have implemented a successful data lake and data lake house. The first is just a nice lookup table against blob storage for super raw data (literally encoded chunks of bytes) that we might need at some point in time but always do when they land in s3. The lake house is a massive iceberg table about 10 trillion rows and growing which costs about 8k a month to maintain and provides massive value for the org without any fancy infrastructure other than S3.",
          "score": 4,
          "created_utc": "2026-02-17 12:22:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwx66",
              "author": "wtfzambo",
              "text": "I'm sure there are good implementations out there. My rant is due to the fact that the majority of what I have seen did not qualify as \"good\".\n\nAnd I wanted to know if I was an isolated case, or not.",
              "score": 3,
              "created_utc": "2026-02-17 12:58:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v8m0d",
          "author": "drag8800",
          "text": "only one data lake i've seen work was at a place that treated it like actual infrastructure. had a dedicated person whose entire job was lake governance - file formats, partition schemes, access patterns, everything. most places want the benefits without the discipline.\n\nthe irony is that the whole pitch was \"avoid upfront schema design\" but the ones that work have MORE discipline than traditional DWH, not less. they just chose to skip the thinking-beforehand part and paid for it in engineering time.\n\n\\~10% of orgs genuinely need a data lake for the unstructured stuff, ML pipelines, etc. the other 90% should've just used snowflake or bigquery and called it a day.",
          "score": 3,
          "created_utc": "2026-02-17 14:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wcdmm",
              "author": "wtfzambo",
              "text": ">but the ones that work have MORE discipline than traditional DWH\n\nExactly. I feel that the lvl required is higher.",
              "score": 1,
              "created_utc": "2026-02-17 17:26:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vdkd2",
          "author": "exjackly",
          "text": "Data Lake isn't about recreating a DWH in the cloud.  Though it is what a lot of places do with it.  If all you have are a dozen RDBMS systems that have transactional or MDM data, skip the lake and go straight to a DWH.  The Lake won't get you any benefits.\n\nData Lake makes sense when you are pulling a lot of silos of data together to do analytics on it.  Especially when those silos have the different types of data.\n\nIf you are pulling together video, pictures, audio files, stacks of JSON and XML files, streamed IOT readings, and GIS inputs in addition to your structured database sources, the Lake is going to make your life much easier.  \n\nYou can run the analysis processes on the video, pictures, audio, and GIS inputs in place and have that be in the lake too.  If those analysis tools get updated, it is still easy to reprocess all the impacted source data to feed it forward.\n\nThe semistructured data, similar thing - you choose what elements to bring forward, and when/how to flatten it so you can combine it with the traditional relational data.  And, you have the raw data so you can reprocess if there is a new or changed requirement.\n\nI'm still convinced however, that all of this variety is a distraction that people get caught up in.  We don't process as humans this data in binary, vector or unstructured form.  We don't actually get value out of it until it is reduced/restructured into a relational form of some sort that we can use to make a decision and take an action.",
          "score": 5,
          "created_utc": "2026-02-17 14:32:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wdvtb",
              "author": "wtfzambo",
              "text": "Correct, unfortunately most people use them for the first case you described, rather than the second.",
              "score": 1,
              "created_utc": "2026-02-17 17:33:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5wrjl9",
              "author": "KWillets",
              "text": "There is no unstructured data, only structures we haven't met yet.",
              "score": 1,
              "created_utc": "2026-02-17 18:37:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xgw2o",
          "author": "JimiZeppelin1012",
          "text": "I donâ€™t think Iâ€™ve ever seen any software architecture used properly",
          "score": 4,
          "created_utc": "2026-02-17 20:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ygaa9",
              "author": "wtfzambo",
              "text": "word",
              "score": 1,
              "created_utc": "2026-02-17 23:31:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uoc3m",
          "author": "DeliriousHippie",
          "text": "For wide variety of users there are no benefits from using data lake instead of DWH. Same goes for much of today's hype. Maybe it's always been that. I've seen many fads during my time. Self Service, Machine Learning, Business Data Warehouse, ELT, etc.\n\nYou know why Iceberg files/tables exist? Because Netflix had problems. Iceberg solves problems when you're size of Netflix. Most of my B2B customers have less than 100 million rows in their largest table, schemas don't change, 90% of tables can be easily read in one go without needing delta loads.\n\nI thought about delta loads awhile back. In past companies owned their servers and data transfer and compute was free. It didn't matter if you fetched half of the tables completely every night and ran all through transformation layer since it didn't cost anything. Now that's bad practice because in cloud everything has a cost.\n\nBut that's the way it is and has been. That's what they pay us to do.\n\nEdit: [https://www.youtube.com/watch?v=b2F-DItXtZs](https://www.youtube.com/watch?v=b2F-DItXtZs)",
          "score": 8,
          "created_utc": "2026-02-17 11:58:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5upou1",
          "author": "rupert20201",
          "text": "For very large datasets, datalake can be cheaper, faster and more flexible to implement BI than traditional EDW like Teradata. ONLY if itâ€™s large enough.",
          "score": 3,
          "created_utc": "2026-02-17 12:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ursep",
          "author": "DungKhuc",
          "text": "I don't see any reason why data lake is is bad. And it's even better if you can query that data too.\n\nIf you have an actual data warehousing problem, then build a data warehouse as the next layer after data lake.\n\nYou don't have to choose between a data lake and data warehouse.\n\nI do believe skipping data lake layer nowadays is more often than not a bad decision both tactically and strategically.",
          "score": 3,
          "created_utc": "2026-02-17 12:24:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uyabb",
              "author": "wtfzambo",
              "text": "> I don't see any reason why data lake is is bad\n\nMy take: because you can make the same mistakes you can make on a database AND a lot of other mistakes that a database would not allow you to do.\n\nWhenever I saw datalakes as the core implementation of a stack, it was obvious that a lot of concepts were completely disregarded: file sizing, partitioning structure, I/O latency, I/O cost etc...\n\nOne enterprise I worked for a few years ago was spending ~$20-40k a month in S3 PUT requests alone because someone had decided to stream their entire SAP database to Iceberg tables 24/7, non stop. Needless to say management was not happy about it, but the system they had set up was so phenomenally convoluted that it would have taken a year (pre-AI) to tear down and redo from scratch.",
              "score": 3,
              "created_utc": "2026-02-17 13:06:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vr4ee",
                  "author": "DungKhuc",
                  "text": "I mean that's not the problem with data lake, but more with bad engineering?\n\nI've seen companies wasting millions on Oracle DW, Teradata, and lately Snowflake. The set up can be as convoluted as you can imagine, and most likely not portable and hard to examine at scale.\n\nOn top of that, in my experience, different EDW providers also give you huge licensing headache, so much that most people would give up doing anything innovative.\n\nAnd as said, you don't have to pick one, picking both is usually the right choice.",
                  "score": 2,
                  "created_utc": "2026-02-17 15:40:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5v77yj",
          "author": "KWillets",
          "text": "Database ~~Management System~~\n\nI've worked on a lot of large-scale systems, and the reality is that there's little need to deconstruct the RDBMS architecture, and people who do quickly blow up their headcount. The consistency guarantees are more important at scale, not less.\n\nMy last job had hundreds of thousands of queries running daily on 2000 cores, managed by 2 people, me and a contractor. The data lake had less than a tenth of that load, managed by 4+ FTE's. The main complaint against the RDBMS was that too many people were using it (!).  \n",
          "score": 3,
          "created_utc": "2026-02-17 13:57:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vhfb7",
              "author": "DatabaseSpace",
              "text": "I work in healthcare which is heavily Azure based and i'm trying to learn new things, so I'm studying Microsoft Fabric, which is based on a specific kind of data lake.  I'm kind of a dinosaur and use SQL, Python and normal databases.  I'm trying to have an open mind about this stuff, but I just keep thinking, how is this better? is this all marketing bullshit to get money to cloud providers by monitizing every single thing that I now do almost free?  The answer from AI is always about scale so maybe I get that a little bit, but I'm not sure.  I'm going to learn it because I feel like I have to, maybe i'm wrong.  ",
              "score": 1,
              "created_utc": "2026-02-17 14:52:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wdfyu",
                  "author": "wtfzambo",
                  "text": ">so I'm studying Microsoft Fabric\n\nI'm sorry this happened to you",
                  "score": 3,
                  "created_utc": "2026-02-17 17:31:50",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5wfjeh",
                  "author": "KWillets",
                  "text": "Fabric seems to be taking a fairly reasonable approach. Just this morning in my linkedin feed I see a \"why the warehouse still matters\" story from their product people.\n\n  \n[https://www.linkedin.com/pulse/why-data-warehouse-still-matters-fabric-world-luke-matthews-ezt7e/?trackingId=BiDouGOuaLF5hkmeOk7Ulg%3D%3D](https://www.linkedin.com/pulse/why-data-warehouse-still-matters-fabric-world-luke-matthews-ezt7e/?trackingId=BiDouGOuaLF5hkmeOk7Ulg%3D%3D)",
                  "score": 1,
                  "created_utc": "2026-02-17 17:41:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vjbep",
          "author": "pragmatica",
          "text": "Data swamps have been a thing since Hadoop got popular.\n\nItâ€™s sounds great, dump your data into the lake and figure it out later.\n\nIn practice itâ€™s a mess.",
          "score": 3,
          "created_utc": "2026-02-17 15:02:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5z1kgr",
              "author": "Frosty-Hair6123",
              "text": "Yep, canâ€™t agree more. Unified lake house sounds nice, but users has to be engineers, no analysts really know how to use it unless you have some basic trino or spark knowledge. Enterprise like it because it is cheap, not user friendly",
              "score": 1,
              "created_utc": "2026-02-18 01:27:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yn9m0",
          "author": "hyper24x7",
          "text": "Thank you omg. In 20 years Ive never seen a manager actually know how a data warehouse works let alone a data lake.",
          "score": 3,
          "created_utc": "2026-02-18 00:09:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60d7qd",
          "author": "ummitluyum",
          "text": "The problem is that \"Schema-on-Read\" is the biggest lie in data engineering history. In reality, it means \"Data-Quality-Never\"\n\nWithout enforced schema on write (like in a DWH), your Data Lake turns into a Data Swamp in six months. Engineers spend 90% of their time not on insights, but on writing regexes to parse broken JSON that changed without warning. It's technical debt raised to an absolute",
          "score": 3,
          "created_utc": "2026-02-18 06:29:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61pucf",
              "author": "wtfzambo",
              "text": ">\"Schema-on-Read\" is the biggest lie in data engineering history. In reality, it means \"Data-Quality-Never\" \n\nman, I know right!",
              "score": 1,
              "created_utc": "2026-02-18 13:12:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5umd6k",
          "author": "RandomSlayerr",
          "text": "I havent ever seen it either, i think it sounds cool so some people decide to take that route even though it is complete overkill",
          "score": 3,
          "created_utc": "2026-02-17 11:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uq3kw",
          "author": "Thin_Original_6765",
          "text": "It works like technical debt. Itâ€™s meant to be a mean to get things done but not the final product itself.\n\nItâ€™s why you can find teams having well managed data lake, but across the enterprise itâ€™s a mess.",
          "score": 4,
          "created_utc": "2026-02-17 12:11:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vcifd",
          "author": "TheRealStepBot",
          "text": "You are on your soapbox yelling about stuff you obviously donâ€™t understand. \n\nMost trivially all Iâ€™ll say is the DuckDb guys created ducklake. Maybe go watch their technical talk about that as it provides a great explanation for why databases by themselves are limited as well as why blob storage by itself is limited. Traditional databases are basically concurrency managers. They suck at storing any meaningful amount of data however. \n\nLakes, lakehouses are primarily about decoupling storage from compute. It serves two functions when you do this, decreasing cost and decoupling compute scaling. You can have multiple teams scale their own trino or spark or python instances to meet their requirements. \n\nTo the degree they correctly mock religious opposition to structured databases the flip side is just as true. Religious insistence on database engines built for the needs and tradeoffs mainly of the 1970s and 80s is just stupid. \n\nThere are things traditional databases are good at but even comparatively small amounts of data can quickly begin to choke them out. Additionally their scaling properties are complex as they can run into many separate limits that can force scale out or worse yet force a scale up leading to over provisioning. \n\nDatabases are also always hot. They are virtually incapable of handling read almost never data. And you can argue but if itâ€™s almost never going to be read just thrown it away. But thatâ€™s not an argument for traditional databases itâ€™s a limitation. \n\nYou are merely lost in the hype of the technology and donâ€™t actually understand the technical tradeoffs being made. There is a ton of money chasing executives to build lakes because there are vendors with lakes to sell. Things built like this are almost always a mess. That not because of the tech but because of who is building it under what pressures. \n\nThat doesnâ€™t make them a bad an idea. They are a specific tool in the toolbox that can handle a variety of issues that affect traditional systems. They especially are good at enabling self serve data analytics, and other such democratization efforts as the materialization of some absurd table for the vpâ€™s personal use is much less likely to effect the rest of the system. \n\nThey also are very good at recording point in time snapshots of data that would be prohibitively expensive to maintain in most traditional databases which can be a critical enabler for challenging ML problems. \n\nThey go hand in hand with event sourcing systems that are recording a change feed of events rather than an absolute state. If your system doesnâ€™t have this point in time requirement itâ€™s easy to see why you would not appreciate the issues lakes set out to solve. \n\nThere are more use cases they shine at but merely because you already have an oltp database that you treat as a magic black box you donâ€™t understand is no reason to dismiss lake technology you also donâ€™t understand.",
          "score": 4,
          "created_utc": "2026-02-17 14:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5z001p",
              "author": "roararoarus",
              "text": "Great response",
              "score": 2,
              "created_utc": "2026-02-18 01:19:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5wd6c5",
              "author": "wtfzambo",
              "text": "You make a lot of assumptions about me, most of them are wrong.\n\nThis said I agree on one point:\n\n> That doesnâ€™t make them a bad an idea.\n\nTrue, they're not a bad idea. Much as dynamite isn't a bad idea. But you wouldn't give it to someone careless now, would you?\n\nNow swap dynamite with data lake, same principle.",
              "score": 1,
              "created_utc": "2026-02-17 17:30:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wyanj",
                  "author": "TheRealStepBot",
                  "text": "I would actually agree this a mostly apt explanation of the comparison. The primary building blocks are somewhat like fissile material. It can be packaged up in various useful ways. Some to build power plants and some to build bombs. Data lakes use the fissile primitives themselves to potentially very powerful effect. \n\nBut not everyone is a nuclear engineer and giving even nuclear engineers fissile material can lead to mistakes that go boom. Worse yet giving it to the homeless guy on the corner. Itâ€™s gonna go wrong.\n\nTraditional databases are like giving people specific prepackaged power plants already arranged correctly to harness the fissile material into something comparatively useful and mostly safe. \n\nI just tend to get irked by people who act as if these trades donâ€™t exist. They exist and they can give massive boosts to people who know when and how to make use of them.",
                  "score": 2,
                  "created_utc": "2026-02-17 19:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o60ic5a",
              "author": "ummitluyum",
              "text": "Fair point regarding ML and audit, but let's be honest: 90% of data lake users aren't ML engineers looking for snapshots. They are BI analysts who just want to run a simple SUM(sales), and for them, \"cold\" storage is a nightmare because every query triggers a scan of terabytes",
              "score": 1,
              "created_utc": "2026-02-18 07:13:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vaaip",
          "author": "Hofi2010",
          "text": "Even though I think datalakes as are useful not every company needs one. Same with a lakehouse. And companies listening to their AWs or Azure solution architect too much and building for scale too early. That is the beauty of a datalake actually you can start small just s3 and scale when you need it, but that doesnâ€™t do much for your solutions architects goal.",
          "score": 2,
          "created_utc": "2026-02-17 14:14:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vd6i0",
          "author": "Nearby_Fix_8613",
          "text": "Honestly I truly believe itâ€™s because most data execs are not data people and have no idea how to use data\n\nBut they make the same promise all the time, this latest tech will solve all problems , then they move on before they are held accountable for any business  impact and rinse and repeat for the next company",
          "score": 2,
          "created_utc": "2026-02-17 14:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vn501",
          "author": "UhhSamuel",
          "text": "The one thing I'll say for DWHs even if they're poorly design (unless they're not just poorly designed, but catastrophically designed): They save you money in the long run. Traditional on-prem DWH requires replacements, upkeep, and people. Within 5-7 years, most mid-to large companies will see a 100% return on investment and then it's all savings.",
          "score": 2,
          "created_utc": "2026-02-17 15:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vscg8",
          "author": "Straight-Health87",
          "text": "If I told you that 99% of the data systems I saw and worked with/on donâ€™t need more than a properly designed postgres warehouse backend, would you believe me?\n\nPeople invented all kinds of products and technologies to cater for people (usually management) who donâ€™t have a clue what data is and how it works.\n\nKeep it simple, stupid!",
          "score": 2,
          "created_utc": "2026-02-17 15:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5we990",
              "author": "wtfzambo",
              "text": ">If I told you that 99% of the data systems I saw and worked with/on donâ€™t need more than a properly designed postgres warehouse backend, would you believe me?\n\nYes.",
              "score": 2,
              "created_utc": "2026-02-17 17:35:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vwp45",
          "author": "Quaiada",
          "text": "I agree with you. I also see a lot of data lakes being built in a very poor way. But thatâ€™s not my problem. Right now i'm just a data engineer. And if you want me to do a task and are willing to pay me well for it, letâ€™s go.\n\nTo be honest, Iâ€™m tired of trying to explain things and improve the environment.\n\nStakeholders, POs, project management, tech leads, Scrum Masters, directors, and everyone else â€” the overall understanding of the solution on the business side is very low.\n\n At this point, I just want to move my tasks.\n\nAt the end of the day, itâ€™s a company policy where thereâ€™s budget available and the organization needs to spend it. So, in the end, no one really cares whether the product will deliver real value or not. What ultimately matters is the story thatâ€™s being told.",
          "score": 2,
          "created_utc": "2026-02-17 16:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w020h",
          "author": "Skullclownlol",
          "text": "> Anyone of you has actually seen a data lake implementation that didn't suck\n\nYeah, I've had the opposite experience: It has consistently been the easiest to get right in larger teams (for the parts it's good at, not to replace a DWH), even at the bank I worked at. They didn't replace DWHs though, they just fulfilled a specific role.\n\nOld source data goes to long-term archival on (extremely cheap) cold storage, ingestion doesn't break on schema changes, ingestion is idempotent and replayable, significantly cheaper costs compared to storing all source data in the DWH, DWH only serves newest revisions needed for outputs, etc...\n\nThis was all on-prem during my first 3 years at that bank, afterwards parts started to be migrated to Databricks. But only parts of the lake, and the DWH was kept on-prem. So I disagree with other commenters saying this only works either on-prem or either on the cloud.",
          "score": 2,
          "created_utc": "2026-02-17 16:24:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w8la7",
              "author": "wtfzambo",
              "text": "> They didn't replace DWHs though, they just fulfilled a specific role.\n\nAh! See this I think is one of the key differences, when people try to use lakes as if they ware data warehouses as well.",
              "score": 1,
              "created_utc": "2026-02-17 17:08:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wh9sx",
          "author": "ActionConnect5973",
          "text": "Yes, beautiful truth. Finally. Thing is we will inevitably have to deal with the optimization problem at hand formally. Data lakes are a convenient \"fire and forget\" solution and are still somewhat transparent/configurable, since you can still determine plans and abuse the infrastructure for things it wasn't intended for.\n\nSo... as somebody who likes to get their hands dirty in the name of saving costs, it offers a lot of opportunities. Is it optimal? No. Is it better than a single DWH? Absolutely, because you generally have more freedom (delta lake implementations like databricks for instance, though they are now making it harder to be a dev with hidden stuff that your jar will not contain...)",
          "score": 2,
          "created_utc": "2026-02-17 17:49:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xkm1r",
          "author": "Content-Soup9920",
          "text": "Data lakes are like communism.  Theoretically, if you would go al the way through, lift all Metadata, create a good catalog, provide self seevice data services, it could work, would be good. But nobody ever implements it \"full\" so it is always a disgrace.",
          "score": 2,
          "created_utc": "2026-02-17 20:53:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xxrxg",
          "author": "exact-approximate",
          "text": "I agree that the data lake architecture is now being abused and the original purpose of the architectural concept was lost, mainly due to vendor disinformation. At least in my view:\n\n* Data Lakes started somewhere in 2017 providing two main features; streaming unstructured data into some storage easily, and storing a lot of data cheaply outside of a DWH.\n* Data Lakes were super popular in setups which were either spark native or pricey DWH setups (Databricks, Redshift). But in parallel DWH platforms with native separation of storage and compute started to emerge (Snowflake, BigQuery). \n* After some time with companies having massive data lakes, the need for a better file format/engine came around - and Hudi/Iceberg were born from the OSS community, and Delta from Databricks.\n* Somewhere in between people just started to misuse data lakes as data warehouses because it was cheap and easy to do, and allowed for poor planning. Also open table formats became the hot new tech.\n* Today - Snowflake entered the datalake business, Databricks are entering the datawarehouse business, and AWS/BigQuery lets you do anything. \n* For primarily streaming data, a data lake ingestion is still the best architectural concept.\n\nSo no we are in a situation where any platform allegedly allows you to implement whichever architecture you want, irrespective of the roots of the platform.\n\n* You run AWS? Datalake on S3+Iceberg/Hudi+Athena with Redshift as the DWH\n* You run Snowflake? Datalake on S3+Iceberg with Snowflake as the DWH\n* You run Databricks? Datalake on S3+Delta with Databricks Compute Engine and Postgres OLTP\n* You run GCP? BigQuery + GCS + Iceberg\n\nThis is now why data lakes are misused, because all the vendors wanted a slice of any architecture even if it didn't make sense for their product.",
          "score": 2,
          "created_utc": "2026-02-17 21:55:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5y069j",
              "author": "asarama",
              "text": "At the end of the day doesn't this help consumers?\n\nOr do you feel like in the long run we are all footgunning ourselves?",
              "score": 1,
              "created_utc": "2026-02-17 22:06:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ygvi2",
                  "author": "wtfzambo",
                  "text": ">At the end of the day doesn't this help consumers?\n\nI think this is heavily up for debate. For sure, it does help AWS shareholders.",
                  "score": 1,
                  "created_utc": "2026-02-17 23:34:31",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o609vyd",
                  "author": "exact-approximate",
                  "text": "Yes it probably does as a tool no longer restricts your architecture choices, but selecting a tool should be an architecture discussion to begin with.\n\nThe native cloud providers have closed off the gaps which Snowflake and Databricks were positioned to close a while ago, and will continue to do so. I feel it's questionable why one might opt for Snowflake or Databricks in 2026 when you can do everything with a native cloud providers.\n\nOn the other hand people who have gone with Snowflake and Databricks won't be limited.\n\nSo yes the consumer does win here. The thing is that in most cases the consumer is so poorly educated that winning doesn't necessarily result in a good experience. Hence OP's frustrations.",
                  "score": 1,
                  "created_utc": "2026-02-18 06:01:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5y01ix",
          "author": "TheSchlapper",
          "text": "I started at a new mid sized company who had one guy prop up the entire medallion by himself from scratch. Now we have a single day source to call on in all of our reports. Best Iâ€™ve seen thus far\n\nBut this guy also runs Microsoft events and such so heâ€™s definitely keeping up with best practices",
          "score": 2,
          "created_utc": "2026-02-17 22:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yh1gj",
              "author": "wtfzambo",
              "text": "massive envy",
              "score": 2,
              "created_utc": "2026-02-17 23:35:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5z91xu",
                  "author": "TheSchlapper",
                  "text": "Yeah Iâ€™m realizing that if a business has sensitive data then it takes things about 10-20 years longer to catch up to current industry standards\n\nIf you can, work in an industry that doesnâ€™t base value of off PII and other strict data standards",
                  "score": 1,
                  "created_utc": "2026-02-18 02:05:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5zvwkn",
          "author": "Personal-Reflection7",
          "text": "Very recently we suggested a client to build a simple warehouse (i.e. limited data, modeled for reporting n dashboards etc) - and later move to a lakehouse when the need arises for use cases that need dumps of data for EDA etc\n\nThe C level asked us to specifically rephrase it to calling a Data Lake - despite agreeing with this route",
          "score": 2,
          "created_utc": "2026-02-18 04:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61pqwv",
              "author": "wtfzambo",
              "text": "> The C level asked us to specifically rephrase it to calling a Data Lake - despite agreeing with this route \n\nJesus christ",
              "score": 1,
              "created_utc": "2026-02-18 13:12:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5us0ek",
          "author": "Thavash",
          "text": "There is also further damage in that many young professionals never developed skills in dimensional modelling (ie how to properly design a Kimball style warehouse ) as they entered the industry during the Databricks / Data Lake mania era",
          "score": 3,
          "created_utc": "2026-02-17 12:25:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uyh6s",
              "author": "wtfzambo",
              "text": "Indeed. TBH I am one of those victims, I have to figure it out myself and it's quite difficult when no one around you is doing it.",
              "score": 3,
              "created_utc": "2026-02-17 13:07:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o60j8w6",
              "author": "ummitluyum",
              "text": "Itâ€™s the Big Data marketing brainwash. We spent 5 years being gaslit into believing \"JOINs are slow\", so everyone denormalized everything to death\n\nNow we have analysts terrified of writing a JOIN, scanning 50TB tables just to fetch three columns. The funniest part is watching them reinvent the wheel trying to enforce data integrity in this mess - basically jankily reimplementing Foreign Keys in Python inside their DAGs. Kimball is probably rolling in his grave (even though heâ€™s still alive) looking at these \"modern\" data lakes",
              "score": 1,
              "created_utc": "2026-02-18 07:21:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60wccy",
                  "author": "Thavash",
                  "text": "Just awful",
                  "score": 1,
                  "created_utc": "2026-02-18 09:23:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5uqffl",
          "author": "k00_x",
          "text": "Our work data lake is literally just a SQL Server 2019.",
          "score": 2,
          "created_utc": "2026-02-17 12:14:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uzgoc",
          "author": "neuromantic13",
          "text": "If you have a primarily spark based etl, then a data lake makes some sense, though in many cases itâ€™s easier to just have an external hive catalog, which basically does the same thing and doesnâ€™t force you to constantly do table maintenance to clean up old data. I was forced to implement iceberg to make snowflake cheaper to run so we could save on storage.",
          "score": 1,
          "created_utc": "2026-02-17 13:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v145p",
          "author": "New-Addendum-6209",
          "text": "I agree. If you don't have huge volumes of event data you don't need a data lake.",
          "score": 1,
          "created_utc": "2026-02-17 13:23:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v1tuk",
          "author": "Comfortable-Power-71",
          "text": "Preach!",
          "score": 1,
          "created_utc": "2026-02-17 13:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v2wdu",
          "author": "kevkaneki",
          "text": "But have you tried a Data Lakehouse though? \n\nlol",
          "score": 1,
          "created_utc": "2026-02-17 13:33:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v48xt",
          "author": "Eleventhousand",
          "text": "I think it worked decently for us when I worked at Amazon.  I wouldn't really recommend one for a small or medium sized company though.",
          "score": 1,
          "created_utc": "2026-02-17 13:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v4y19",
          "author": "bigdatasandwiches",
          "text": "The only time Iâ€™ve seen one work is at a startup, where I designed the architecture greenfield. I think legacy concerns prevent it as youâ€™ve seen.",
          "score": 1,
          "created_utc": "2026-02-17 13:45:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v7qy6",
          "author": "Bosshappy",
          "text": "With over 25+ years of experience, I have to say, in general, I like data lakes.  Back in ye olden days, writing ETL was touch heavy and very expensive.  Mistakes, double loads, missing loads would take a day to fix, going back to the 80s-90s all week to fix.\n\nNow itâ€™s just a matter of dropping the tables and recreating them.  With that said, data architects are notoriously spineless when talking to business.  Business will state: â€œWe need 10 TB of data, but we have no idea who will use it and whyâ€.  After the project is built and the dust settles, one guy will use it twice a year and when you go back to business with proof of the cost and effort to maintain their â€œnecessaryâ€ data, business will insist they still need it",
          "score": 1,
          "created_utc": "2026-02-17 14:00:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wdniq",
              "author": "wtfzambo",
              "text": ">With that said, data architects are notoriously spineless when talking to business.\n\nOh my god, preach! I say this all the time! No one fucking listens. It's always \"but they said they want all data and what if it scales?\". Jesus christ.",
              "score": 2,
              "created_utc": "2026-02-17 17:32:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v91rn",
          "author": "Professional_Eye8757",
          "text": "Iâ€™ve seen the same thing. Most â€œdata lakesâ€ end up as expensive dumping grounds with a thin SQL veneer slapped on top. The few that work well only do so because a disciplined team treats them like an actual database instead of a magical bucket that will somehow organize itself.",
          "score": 1,
          "created_utc": "2026-02-17 14:07:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v992h",
          "author": "RoestG",
          "text": "As I understand it a lakehouse architecture is more suited if there is a lot of demand for ad hoc analyses where there is no clear picture of the desired end result. Which would primarily be data scientists. When you are looking for uniform and standardized data sets suited for dashboards and standard vetted reports, then you would use a data warehouse, or its younger brother a data lake house. The latter has a data lake as a base layer, with a uniform and standardized layer on top which functions more as a dwh.",
          "score": 1,
          "created_utc": "2026-02-17 14:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vb3h6",
          "author": "SLTxyz",
          "text": "My org's data lake is an absolute shit show",
          "score": 1,
          "created_utc": "2026-02-17 14:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vb9sn",
          "author": "FantasticEquipment69",
          "text": "As a data engineer with 2 years of experience (specifically DWH modeling), I struggle to understand sometimes why this customer wants a Data Lake. Like fr what's wrong with the OG architecture of \"Data Sources --> Staging --> DWH\" ESPECIALLY WHEN YOUR DATA IS ONLY STRUCTURED DATA.\n\nAlso, it's quite confusing for me when do you decide that you need a data lake instead of your current running DWH?\n\nIs it just a marketing strategy (as many claims) to get big corporates to think they are outdated which will lead the mid-level/small companies to follow the trend as well?",
          "score": 1,
          "created_utc": "2026-02-17 14:20:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vh8dd",
          "author": "PizzaSounder",
          "text": "Why wouldn't you have defined schemas in a datalake?\n\nWe used it as a central store for dozens of teams and it worked well. Individual teams drop their new data on their schedule, in their format. New data merged with existing data, schema is enforced. You can move massive amounts of data in with Spark jobs. Also, I personally love time travel in Delta tables. Free snapshots, rollback protection for those \"oh shit\" updates. \n\nBest part, access is managed centrally and is in a single format. The datalake manages those transformations. You don't have team A requesting access to Team Bs data (which is SQL) and Team C requesting access to Team As data (which is a delta table), Team B requesting access to Team Cs data which is an SAP system. Then there is Team Z which only has incremental CSV files or parquet or some shit. Different systems, different technologies, different requirements. Only the datalake has to deal with that, not every team.",
          "score": 1,
          "created_utc": "2026-02-17 14:51:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vjwmr",
          "author": "defuneste",
          "text": "I will gave you an example: bigish data that get updated every 6 months but rarely revised (and revised here could be fine), same schema where you just append files in a hive partitioned parquets. \n\nDo that use cases match all types of data? hell no! but did it match a lot of analytics data? hell yes! (doing it monthly is perfectly fine) A lot of analytics related decisions should not be \"realtime data\" anyway.",
          "score": 1,
          "created_utc": "2026-02-17 15:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5we346",
              "author": "wtfzambo",
              "text": "This is the type of use case I endorse but not the type of use case that the average business (ab)uses data lakes for.",
              "score": 2,
              "created_utc": "2026-02-17 17:34:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vjxej",
          "author": "West_Good_5961",
          "text": "Data lake as a dumping ground. Then load it to data warehouse. Seems like the sensible and popular pattern.Â ",
          "score": 1,
          "created_utc": "2026-02-17 15:05:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vkw1g",
          "author": "asevans48",
          "text": "I get ya. Use them for API calls. My last boss took a year or so to cone to terms with how they werent the holy grail or data. She wasnt technical at all. Had a gov background in data analytics. I dont think data warehousing is 100% a solution either. Flat and even denormalized native tables in an olap engine are great for analytics. Its possible to save data in cloud storage in aws and gcp for n amount of time if anyone wants to build an iceberg table. Other use cases might include fintech where you may need time travel or its schemas arrive entirely in JSON via kafka which still requires a curated zone. Literally had to convince my boss that sticking 1 million custom 20 row excel files in anything othet than cloud storage for power bi was a waste.",
          "score": 1,
          "created_utc": "2026-02-17 15:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vtm3k",
          "author": "albsen",
          "text": "we are running pgduck on parquet files that are generated from OLTP databases, querying those using duckdb via pgduck is a fraction of the query time compared to SQLServer or postgres. not sure if you'd call this a datalake or a dwh. the ETL job syncronizes the schemas so that you don't have a hard time joining in pgduck.",
          "score": 1,
          "created_utc": "2026-02-17 15:52:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wd3p9",
          "author": "Kilnor65",
          "text": "As someone who has only worked with normal SQL, could you just list a couple of things that makes it worse than SQL? I always have use cases where just \"throwing the data in a pile\" would be kind of nice instead of making a bunch of new garbage tables or columns.",
          "score": 1,
          "created_utc": "2026-02-17 17:30:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x3u52",
              "author": "wtfzambo",
              "text": "> \"throwing the data in a pile\" \n\nDo this with your clean laundry the next 4 weeks and tell me if you'll still be able to find the clothes you're looking for.",
              "score": 1,
              "created_utc": "2026-02-17 19:34:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ymn21",
          "author": "DJ_Laaal",
          "text": "Datalakes were a promising concept about a decade ago when it started off as an alternative for storing semi structured and unstructured data. The traditional database technologies with a Kimball/Inmon style data architecture on top served the structured data storage and querying usecases really well. \n\nIt all turned to shit when companies (and vendors) started abusing it as a â€œthrow all your data here and weâ€™ll think about what to do with it laterâ€. It became an unorganized data swamp right out of the gate. \n\nThen came the newer vendors like Databricks and Snowflake. Layered a distributed, separate compute layer on top of the datalake, added few governance capabilities and it started to become slightly better. However, I see them going down the same path now with crap like â€œlakebaseâ€ (i.e a traditional database but on cloud storage). Why do we even need this shit? We already have dozens of database techniques that do exactly that. \n\nNowadays, I equate datalake with just scalable cloud storage and nothing more.",
          "score": 1,
          "created_utc": "2026-02-18 00:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ynndj",
          "author": "spendology",
          "text": "Data lakes have electrolytes - that's why!!",
          "score": 1,
          "created_utc": "2026-02-18 00:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z69ru",
          "author": "IllAppeal4814",
          "text": "In our case, we moved from redshift (dwh+query engine+ metadata store) to more like lakehouse (not dumping everything, but sort of partitioned based storage eg: client/yyyymm/datasource/ strategy) composed of s3, query engine + glue catalog as metadata store, in order to increase only the compute, but keeping storage cost bare mininum as we required more compute (although we were okay with current storage)\n\nWe maintained partitioned storage as our reporting were based on client filtering based OLAP query, that usually demanded aggregated result of certain time period. So it was stored to make query engine filter fast from the partitioned storage",
          "score": 1,
          "created_utc": "2026-02-18 01:51:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z7si8",
          "author": "Disastrous_Answer905",
          "text": "I just export the same excel set up everydayâ€¦",
          "score": 1,
          "created_utc": "2026-02-18 01:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zrjrr",
          "author": "Next_Comfortable_619",
          "text": "im coming from a very heavy sql server background and have been watching hundreds of hours of videos on YouTube about databricks and snowflake. databricks makes me cringe but i do like snowflake. the modern data engineering stack is a dumpster fire though. also, lol @ using python ti manipulate data instead of sql. cringe.",
          "score": 1,
          "created_utc": "2026-02-18 03:51:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uwnpq",
          "author": "MonochromeDinosaur",
          "text": "Data lake is only needed of your data is so unstructured you canâ€™t get it into a table format or so big your optimized queries are slow. \n\nAlmost no company has this problem. Hell most companies could use postgres for analytics for years. \n\nExecs fall for marketing.",
          "score": 1,
          "created_utc": "2026-02-17 12:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ur2sc",
          "author": "iammerelyhere",
          "text": "I thought I was the only one! Omg life was simpler when all I had to manage was a handful of sql servers and a file systemÂ ",
          "score": 1,
          "created_utc": "2026-02-17 12:19:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v31eu",
          "author": "peterxsyd",
          "text": "You are 100% right.Â ",
          "score": 0,
          "created_utc": "2026-02-17 13:34:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v3bvu",
              "author": "peterxsyd",
              "text": "The thing is cloud providers love databricks because it is unstable as fuck suffering from OOM java errors, so that means big data job re runs on their rented compute. Literally make microsoft more money.",
              "score": 3,
              "created_utc": "2026-02-17 13:36:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r83xku",
      "title": "Designing Data-Intensive Applications - 2nd Edition out next week",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/yhoznavzd9kg1.png",
      "author": "sspaeti",
      "created_utc": "2026-02-18 14:07:36",
      "score": 329,
      "num_comments": 35,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r83xku/designing_dataintensive_applications_2nd_edition/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o627c1o",
          "author": "GrandOldFarty",
          "text": "Nice, I cannot wait to buy it and not read it again.",
          "score": 337,
          "created_utc": "2026-02-18 14:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62a46q",
              "author": "godmade5",
              "text": "ðŸ˜­ðŸ˜­ðŸ˜­",
              "score": 17,
              "created_utc": "2026-02-18 14:59:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62k56s",
              "author": "Capt_korg",
              "text": "Those books contain a lot of information, but in my experience, they are dust collectors. \n\nAlways motivated to read them and then... Naaah not enough traction to get to the point...",
              "score": 19,
              "created_utc": "2026-02-18 15:46:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62kuye",
                  "author": "No_Lifeguard_64",
                  "text": "The problem is you are trying to read them cover to cover. Don't do that. Use the table of contents and read the chapter that is applicable to you right now.",
                  "score": 32,
                  "created_utc": "2026-02-18 15:49:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o630427",
                  "author": "LookAtYourEyes",
                  "text": "I just started reading it last week. I have a friend who has read it cover to cover more than once, and treats it like his bible. Dude is dedicated.",
                  "score": 1,
                  "created_utc": "2026-02-18 16:58:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62kysu",
              "author": "MadT3acher",
              "text": "When I was interviewing last year, I was telling myself to pick a chapter and focus for 15mn on a topic. It was a great motivator to check stuff I didnâ€™t know.\n\nOtherwise youâ€™re only going to pick it up at 2am to combat insomniaâ€¦",
              "score": 15,
              "created_utc": "2026-02-18 15:50:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o631djn",
                  "author": "kadhi_chawal2",
                  "text": ">Otherwise youâ€™re only going to pick it up at 2am to combat insomniaâ€¦\n\nOr the intense motivation spike of turning your life around at 2 am. \n\n\nGot a bit too specific xd",
                  "score": 3,
                  "created_utc": "2026-02-18 17:04:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62a4nz",
              "author": "Insighteous",
              "text": "Are you I?",
              "score": 13,
              "created_utc": "2026-02-18 14:59:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62p4y6",
              "author": "richardrietdijk",
              "text": "Thanks. Thereâ€™s coffee on my monitor now. ðŸ˜…",
              "score": 3,
              "created_utc": "2026-02-18 16:09:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6256o0",
          "author": "Effective_Degree2225",
          "text": "I purchased the 1st one years ago. i think i will buy this and read it finally. thank you",
          "score": 31,
          "created_utc": "2026-02-18 14:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62af2h",
          "author": "hau5keeping",
          "text": "Genuinely asking: what is so special about this book?",
          "score": 27,
          "created_utc": "2026-02-18 15:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62cbv1",
              "author": "amejin",
              "text": "It breaks down how to handle data flow at a very small and very large scale.\n\nIt's basically a \"how did <insert massive enterprise here> solve their data problems and what organic way did those things come about?\" cookbook. Except it's super generalized in an effort to give you insight on how data, resiliency, and consistency works, so that you can make informed decisions.\n\nBecause of that, and it shows generalities and processes, you understand why we have things like Kafka, reddis, and all the other flavors of big data ingestion and management that has wonderful names that you may or may not have thought to yourself \"what is this crap and why does AWS have 14 services for it?\"\n\nIt will help give you a mental model of data pipelines and how to design \"good\" software that will stand the test of time, with an enterprise mindset. Slow to change, consistent and reliable, and most of all, predictable.\n\nYou know - everything the AI revolution, and shit tier sys admins, have thrown out the window because \"moving fast and breaking shit\" is now an acceptable business model.",
              "score": 62,
              "created_utc": "2026-02-18 15:10:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62opw2",
                  "author": "Cloudskipper92",
                  "text": "I agree with everything you said BUT \"move fast and break shit\" has been a business model for a long long time, and I'd say isn't going anywhere either for precisely the reason you mention haha.",
                  "score": 5,
                  "created_utc": "2026-02-18 16:07:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62q2mz",
                  "author": "hau5keeping",
                  "text": "amazing answer, ty!",
                  "score": 2,
                  "created_utc": "2026-02-18 16:13:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62c030",
              "author": "MckyIsBack",
              "text": "It is a good and fairly wide introduction into the design of data-centric applications and integration technologies. It is well written and easy to read.",
              "score": 10,
              "created_utc": "2026-02-18 15:08:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62bl99",
              "author": "No_Lifeguard_64",
              "text": "This is the bible on distributed systems and data architecture.",
              "score": 15,
              "created_utc": "2026-02-18 15:06:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62ccpb",
              "author": "JohnPaulDavyJones",
              "text": "Itâ€™s a seminal work in the field, specifically for people who are making the transition from the usual building/repairing/maintenance work that we do at the mid-level/senior DE level to the actual work of *designing* systems at the staff DE/architect level.\n\nItâ€™s not as useful for the more junior folks in the field, except as a reference to help you understand why your senior technical leaders are making the design decisions that youâ€™re seeing.",
              "score": 4,
              "created_utc": "2026-02-18 15:10:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62dj29",
              "author": "mttpgn",
              "text": "It's an overview of several different data storage, transformation, and transmission technologies, organized by problem domain. Helps you think about trade offs in application design by giving a wide breadth of perspective on what types of data processing problems one sees at production-scale, what solutions are out there, and when you'd want to consider implementing certain patterns.\n\nYou could call it a well-researched response to the \"just use postgres\" meme.",
              "score": 3,
              "created_utc": "2026-02-18 15:15:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62s7dq",
              "author": "recent_mood_",
              "text": "Absolutely nothing. Itâ€™s an infodump and imo not really for data engineers",
              "score": 1,
              "created_utc": "2026-02-18 16:23:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62guhc",
          "author": "samvander",
          "text": "Would you say the previous version is quite out of date?",
          "score": 4,
          "created_utc": "2026-02-18 15:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62bucm",
          "author": "One-Neighborhood-843",
          "text": "Please put NSFW tags on books making people cry.",
          "score": 2,
          "created_utc": "2026-02-18 15:07:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62km06",
          "author": "Grinding_Hard",
          "text": "Thanks. Just a note to myself; itâ€™s not about the book, but what you do after reading it.",
          "score": 2,
          "created_utc": "2026-02-18 15:48:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62hgi6",
          "author": "meSmash101",
          "text": "Started it yesterday on Oâ€™Reilly! Finally!",
          "score": 1,
          "created_utc": "2026-02-18 15:34:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62j6zb",
          "author": "redsoxcraze12",
          "text": "As a heads up, this is released already on O'Reilly",
          "score": 1,
          "created_utc": "2026-02-18 15:42:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62rxmf",
          "author": "PitifulOpportunity99",
          "text": "I was just about to start with the 1ed (I had the book for like 3+ years)",
          "score": 1,
          "created_utc": "2026-02-18 16:21:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62y7f7",
          "author": "mikelson_6",
          "text": "Itâ€™s already available for Kindle - just bought it",
          "score": 1,
          "created_utc": "2026-02-18 16:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62yllz",
          "author": "duckenjoyer69",
          "text": "I have the old one from a couple years ago but haven't cracked it yet. Still worth going through? What do we expect the big changes to be (AI presumably)?",
          "score": 1,
          "created_utc": "2026-02-18 16:51:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6302up",
          "author": "sisyphus",
          "text": "A modern classic.  I do miss the days when everyone had books at their desk and I could make snap judgments about them by their taste.",
          "score": 1,
          "created_utc": "2026-02-18 16:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o632umr",
          "author": "paxmlank",
          "text": "RemindMe! 1 week",
          "score": 1,
          "created_utc": "2026-02-18 17:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o632yr6",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-02-25 17:11:12 UTC**](http://www.wolframalpha.com/input/?i=2026-02-25%2017:11:12%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/1r83xku/designing_dataintensive_applications_2nd_edition/o632umr/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2F1r83xku%2Fdesigning_dataintensive_applications_2nd_edition%2Fo632umr%2F%5D%0A%0ARemindMe%21%202026-02-25%2017%3A11%3A12%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201r83xku)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-18 17:11:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o634qt9",
          "author": "BlackBeard-007",
          "text": "RemindMe! 1 week",
          "score": 1,
          "created_utc": "2026-02-18 17:19:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62hpuu",
          "author": "xDragod",
          "text": "Damn. $70 though.",
          "score": 1,
          "created_utc": "2026-02-18 15:35:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3a05s",
      "title": "Has anyone read Oâ€™Reillyâ€™s Data Engineering Design Patterns?",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/2uu32wxil5jg1.jpeg",
      "author": "xean333",
      "created_utc": "2026-02-13 00:07:17",
      "score": 203,
      "num_comments": 40,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r3a05s/has_anyone_read_oreillys_data_engineering_design/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54wkr9",
          "author": "minato3421",
          "text": "Yeah I went through the book. Felt pretty trivial to be honest. But I have an experience of 7 years in this field. So, nothing in that book felt new. It is worth reading for beginners though",
          "score": 106,
          "created_utc": "2026-02-13 09:19:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55p5uz",
              "author": "kaumaron",
              "text": "I feel like that's many books these days",
              "score": 24,
              "created_utc": "2026-02-13 13:09:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o56rkjc",
              "author": "Thespck",
              "text": "What would you recommend to a junior data engineer? I find CGPT very useful when I ask to help me improve a pipeline or to teach me fundamentals or whatâ€™s best and why not other ways. However, I learnt about slow changing dimensions by reading Designing Data Intensive Applications by Martin Kleppmann (also Oâ€™Reilly)",
              "score": 7,
              "created_utc": "2026-02-13 16:26:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o56xyk6",
              "author": "tylerriccio8",
              "text": "Do you recommend anything more advanced? I have multiple yoe, not really looking for basic patterns",
              "score": 3,
              "created_utc": "2026-02-13 16:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5efcb8",
                  "author": "minato3421",
                  "text": "Designing Data Intensive Applications by Martin Kleppmann and Data Warehousing Toolkit by Kimball. Data Warehousing Toolkit is still valid in the current scenario even though companies say it isn't",
                  "score": 6,
                  "created_utc": "2026-02-14 20:48:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o59b3l5",
                  "author": "Character-Education3",
                  "text": "Probably books more focused on architecture and your business domain",
                  "score": 1,
                  "created_utc": "2026-02-14 00:09:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o554lj9",
          "author": "Kobosil",
          "text": "liked the code examples\n\none of the better books in my opinion",
          "score": 25,
          "created_utc": "2026-02-13 10:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o578h6j",
          "author": "pacopac25",
          "text": "I want to buy the book solely because the fish's clenched teeth, frowning, and thousand-mile-stare eyes accurately represent how I feel when I read the Spark documentation.",
          "score": 10,
          "created_utc": "2026-02-13 17:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5501f5",
          "author": "phizero2",
          "text": "Yeah, ok book. Isnt the best but worth  checking.",
          "score": 19,
          "created_utc": "2026-02-13 09:52:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55791p",
              "author": "dadadawe",
              "text": "Which one is the best?",
              "score": 22,
              "created_utc": "2026-02-13 10:58:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55fj3a",
                  "author": "PutridSmegma",
                  "text": "Designing data-intensive applications from Klepmann",
                  "score": 45,
                  "created_utc": "2026-02-13 12:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57om0t",
          "author": "Awkward-Cupcake6219",
          "text": "Good book, especially for mid level engineers. If you have around 5+ good quality YOE it could fill some gaps.\n\nMore than that? I guess it is nice to have it on the shelf for a quick look, but honestly you could \"have quick look\" on the internet too as I expect you to know what questions to ask at this point.",
          "score": 5,
          "created_utc": "2026-02-13 19:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57v1lu",
              "author": "xean333",
              "text": "That was about my assessment as well. Iâ€™m at a decade plus at this point so Iâ€™ll probably skip it",
              "score": 2,
              "created_utc": "2026-02-13 19:36:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54zw8j",
          "author": "Astherol",
          "text": "Good book",
          "score": 9,
          "created_utc": "2026-02-13 09:51:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55be37",
          "author": "SoggyGrayDuck",
          "text": "Anyone have a great book/link on medallion architecture? I get it but I feel like it's essentially \"let agile define your model\" and id like to read a good resource on it.",
          "score": 3,
          "created_utc": "2026-02-13 11:33:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55ptcr",
              "author": "TechnologySimilar794",
              "text": "Building medalion architecture by Piethein Stengholt ",
              "score": 9,
              "created_utc": "2026-02-13 13:13:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55sydw",
                  "author": "SoggyGrayDuck",
                  "text": "Can you answer one question, does medallion architecture target spark based workflows? The big thing I'm trying to get straight in my head is where do traditional data models come into play. Some say they're not used anymore and others say that's what their silver layer is and yet others say it's the gold layer. I have a feeling it's being wedged into situations it doesn't actually work for. Or they don't really understand and are just updating the terms they use based on what they read or see.",
                  "score": 2,
                  "created_utc": "2026-02-13 13:31:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o55s8vi",
                  "author": "SoggyGrayDuck",
                  "text": "Thank you, looking it up/ordering",
                  "score": 1,
                  "created_utc": "2026-02-13 13:27:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o56bpft",
                  "author": "TheOneWhoSendsLetter",
                  "text": "Recommended.",
                  "score": 1,
                  "created_utc": "2026-02-13 15:10:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56c367",
              "author": "TheOneWhoSendsLetter",
              "text": "Besides Stengholt, *Data Lakes for Dummies* by Alan Simon",
              "score": 2,
              "created_utc": "2026-02-13 15:12:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55td0s",
          "author": "Salfiiii",
          "text": "The book itself is a nice reference but nothing I would consider reading through thoroughly.\n\nSkim over the concepts and come back to it if you ever need it.\n\nNothing revolutionary though, if you have couple years on your back you probably heard of > 90% already.",
          "score": 4,
          "created_utc": "2026-02-13 13:33:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56b2re",
          "author": "TheOneWhoSendsLetter",
          "text": "It's a very good book. You'll find value in the situations and problems addressed and the way of thinking and solutions' caveats that it exposes.",
          "score": 4,
          "created_utc": "2026-02-13 15:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54wnwm",
          "author": "putokaos",
          "text": "Absolutely. It's a fantastic book full of not just practical advice, but also the proper way of solving the most common scenarios. I'd recommend it to any data engineer.",
          "score": 9,
          "created_utc": "2026-02-13 09:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o559dei",
          "author": "Firm-Requirement1085",
          "text": "Just started chapter 2 and the small code examples are using spark, should I learn the basics of spark before continuing?",
          "score": 4,
          "created_utc": "2026-02-13 11:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55f83q",
              "author": "BrunoLuigi",
              "text": "Do you know python?",
              "score": 4,
              "created_utc": "2026-02-13 12:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55ih90",
                  "author": "Firm-Requirement1085",
                  "text": "Yes I use python-polars for ingestion/standardizing csv files but the company I'm at uses snowflake so haven't touch spark",
                  "score": 1,
                  "created_utc": "2026-02-13 12:26:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56b7zx",
              "author": "TheOneWhoSendsLetter",
              "text": "Because of the book? No need to. The solutions there are language-agnostic.",
              "score": 1,
              "created_utc": "2026-02-13 15:07:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o56r5ib",
          "author": "gman1023",
          "text": "I really enjoyed it, has practical problems and patterns one would need in data engineering. like someone said, one of the better books. \n\n\n\nyou can get it for free here (that's how i got it):  \n[Data Engineering Design Patterns](https://buf.build/resources/data-engineering-design-patterns)",
          "score": 2,
          "created_utc": "2026-02-13 16:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59e1fg",
              "author": "Interesting_Strain90",
              "text": "This never worked, i tried three different emails.",
              "score": 1,
              "created_utc": "2026-02-14 00:27:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59w26j",
                  "author": "LoaderD",
                  "text": "Are they company emails? Usually these companies don't let you sign up with a random email because they use this as a way to generate sales leads. \n\nIf you don't have a job and therefore, no company email, there are better books to get started that you should get before this book.",
                  "score": 2,
                  "created_utc": "2026-02-14 02:20:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o571ac2",
          "author": "ruibranco",
          "text": "ddia for the concepts, this one for the copy-paste recipes - they complement each other more than people think",
          "score": 2,
          "created_utc": "2026-02-13 17:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o553wkd",
          "author": "Ok_Appearance3584",
          "text": "Excellent reference book",
          "score": 3,
          "created_utc": "2026-02-13 10:28:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2pumc",
      "title": "I built a website to centralize articles, events and podcasts about data",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/0ovtcb4jc0jg1.png",
      "author": "alphter",
      "created_utc": "2026-02-12 10:20:28",
      "score": 164,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2pumc/i_built_a_website_to_centralize_articles_events/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o50du5l",
          "author": "sparkplug49",
          "text": "Could we get an rss feed so I can pull this into my feed reader?",
          "score": 17,
          "created_utc": "2026-02-12 17:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o517aj7",
              "author": "szrotowyprogramista",
              "text": "Echoing this. An RSS feed would be really nice.",
              "score": 6,
              "created_utc": "2026-02-12 19:20:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54wna5",
              "author": "alphter",
              "text": "It's in the backlog but I need to improve some other stuff first to be able to build a super clean RSS feed. Stay tuned!",
              "score": 3,
              "created_utc": "2026-02-13 09:20:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53vf1h",
          "author": "OpinionSad2896",
          "text": "There is an include filter. Would be great if we have an exclude list too",
          "score": 3,
          "created_utc": "2026-02-13 04:08:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54wq5g",
              "author": "alphter",
              "text": "Yes, I need to find a proper way to do this :)",
              "score": 1,
              "created_utc": "2026-02-13 09:20:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53arfq",
          "author": "michael-day",
          "text": "Love the name and branding",
          "score": 3,
          "created_utc": "2026-02-13 01:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54vike",
          "author": "Narrow-Tower58",
          "text": "Super cool! I am writing a blog about data topics - thanks for making my research 10x faster :D",
          "score": 2,
          "created_utc": "2026-02-13 09:09:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z15fg",
          "author": "hamisphere",
          "text": "wow that looks so cool! thanks for sharing",
          "score": 2,
          "created_utc": "2026-02-12 12:52:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o506uzh",
          "author": "Far-Criticism928",
          "text": "how do you source articles? ",
          "score": 1,
          "created_utc": "2026-02-12 16:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o508ssl",
              "author": "alphter",
              "text": "hi! it's a combination of multiple stuff but mostly i'm using RSS feeds coupled with AI post-processing to generate summaries and tagging.",
              "score": 5,
              "created_utc": "2026-02-12 16:39:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50q1ag",
                  "author": "decrementsf",
                  "text": "moltbook is spreading. Claude is in the room with us right now.",
                  "score": -1,
                  "created_utc": "2026-02-12 17:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50dc8g",
          "author": "evanazz",
          "text": "Super cool! I was planning on doing something like this with an LLM layer on top that to help me find topics to research and write about. Is a direction you'd be interested taking this in? I'm happy to hook that up & fund it, ofc.Â ",
          "score": 1,
          "created_utc": "2026-02-12 17:00:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50j4m1",
          "author": "digitalghost-dev",
          "text": "Great idea!",
          "score": 1,
          "created_utc": "2026-02-12 17:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dlvjd",
          "author": "scenestamper",
          "text": "So google.com",
          "score": 1,
          "created_utc": "2026-02-14 18:15:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h0g25",
          "author": "belkovTV",
          "text": "I love it, but alas, as fate would have it our company is using Microsoft Fabric on the business end where I am on. Do you reckon you could add Fabric Dataaaaa to the hub for the select few of those who work with it?\n\nOne central place is indeed a great option for us who have to look at different blogs, sites, feeds and whatnot to stay up to date.",
          "score": 1,
          "created_utc": "2026-02-15 07:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z363m",
          "author": "BicycleSignal6557",
          "text": "i'll definitely take a look !",
          "score": 1,
          "created_utc": "2026-02-12 13:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zu73i",
          "author": "Thinker_Assignment",
          "text": "Finally someone did it!",
          "score": 1,
          "created_utc": "2026-02-12 15:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5014i0",
          "author": "studentofarkad",
          "text": "This is amazing!! Thank you",
          "score": 1,
          "created_utc": "2026-02-12 16:03:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6rz37",
      "title": "Just overwrote something in prod on a holiday.",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r6rz37/just_overwrote_something_in_prod_on_a_holiday/",
      "author": "Due_Rich_616",
      "created_utc": "2026-02-17 01:07:55",
      "score": 140,
      "num_comments": 32,
      "upvote_ratio": 0.95,
      "text": "No way to recover due retention caps upstream.\n\nPray for me.",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r6rz37/just_overwrote_something_in_prod_on_a_holiday/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5su1nw",
          "author": "ChipsAhoy21",
          "text": "when big fuck ups happen itâ€™s important to remember they donâ€™t happen in a vacuum. There are systems missing that should have prevented that, whether itâ€™s better backups or better access controls or better testing. \n\nWrite up a strong post mortem, and prescribe a system you can implement that will prevent someone else from doing the same thing in the future, and turn it into a win for yourself",
          "score": 166,
          "created_utc": "2026-02-17 02:55:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sy42x",
              "author": "delftblauw",
              "text": "Adding in on this great advice, make sure the post mortem doesnâ€™t sound like itâ€™s avoiding personal accountability and is forward looking.  \n\nIf you find adding in an approval gateway would be helpful, itâ€™s a world of difference between reading, â€œIf we had approval gateways for execution this would never have happenedâ€ versus â€œAdding in approval gateways for execution will help to limit production execution exposure in the futureâ€.  \n\nIf the problem was severe, I would fire the person who wrote the former and work to keep the latter.",
              "score": 45,
              "created_utc": "2026-02-17 03:20:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5skyoa",
          "author": "LaserKittenz",
          "text": "The best way to deal with the stress chemicals you are probably flooded with is to exercise. Â Â ",
          "score": 122,
          "created_utc": "2026-02-17 02:00:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sn4cm",
              "author": "Due_Rich_616",
              "text": "Genuinely couldnâ€™t stop pacing around the room. Beyond cooked. I was having such a nice day too ðŸ˜¢",
              "score": 62,
              "created_utc": "2026-02-17 02:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tklnj",
                  "author": "heretoask23",
                  "text": ":((( what do you think will happen?",
                  "score": 9,
                  "created_utc": "2026-02-17 06:00:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5scxu4",
          "author": "SimpleSimon665",
          "text": "You don't have backups to restore from?",
          "score": 32,
          "created_utc": "2026-02-17 01:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5se27t",
              "author": "Due_Rich_616",
              "text": "Not for some PII heavy tables pipelines upstream, they get purged after n days, lost maybe like 6 months of stuff :( (essentially fucked up the easiest backfilling job of my life)",
              "score": 45,
              "created_utc": "2026-02-17 01:19:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5sg5ty",
                  "author": "Atticus_Taintwater",
                  "text": "Happens to the best of us\n\n\nIn the future what you should do is not that",
                  "score": 122,
                  "created_utc": "2026-02-17 01:31:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5snz1p",
                  "author": "Prinzka",
                  "text": "Just have AI hallucinate some bs to fill in the gaps.",
                  "score": 41,
                  "created_utc": "2026-02-17 02:18:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5t2iod",
          "author": "datascientistdude",
          "text": "Having a downstream with longer retention than upstream and no way to recover (cold storage, undelete, etc) was a disaster waiting to happen anyway. You just happened to be the catalyst. Your company data warehouse and infrastructure isn't set up correctly.",
          "score": 33,
          "created_utc": "2026-02-17 03:49:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t519g",
              "author": "no_4",
              "text": "100% this, OP. This was *going to happen someday*. It just so happened it was right now. \n\nThat lack of a backup of your downstream data is the real fault. Not that it's an uncommon one.",
              "score": 14,
              "created_utc": "2026-02-17 04:05:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5snzlt",
          "author": "Patient_Professor_90",
          "text": "Send wishes for a president day miracle, for you",
          "score": 10,
          "created_utc": "2026-02-17 02:18:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sv5tb",
          "author": "SearchAtlantis",
          "text": "It's not your fault - there are policy and system issues that allowed the event to occur. The swiss cheese theory of accidents",
          "score": 11,
          "created_utc": "2026-02-17 03:02:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5suvjd",
          "author": "MonochromeDinosaur",
          "text": "I know that feeling. I caused an incident like this by deleting a kafka topic once.",
          "score": 10,
          "created_utc": "2026-02-17 03:00:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tliuh",
          "author": "Thlvg",
          "text": "One of us! One of us!\n\nEdit to add: it happened to all of us, it's almost a rite of passage at that point. And seeing the setup you described, it was going to happen at some point, it's not on you. \n\nNow use that incident to learn how you can protect yourself for next time :)",
          "score": 11,
          "created_utc": "2026-02-17 06:08:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sl8cd",
          "author": "eagerunicorn",
          "text": "Sending virtual hugs.Â ",
          "score": 7,
          "created_utc": "2026-02-17 02:02:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t250c",
          "author": "gandhishrugged",
          "text": "I know that feeling mate. Own up, and if the company is good, they will give you a break. If not, they are not worth it",
          "score": 6,
          "created_utc": "2026-02-17 03:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5szjpp",
          "author": "huessy",
          "text": "F",
          "score": 5,
          "created_utc": "2026-02-17 03:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tp2z9",
          "author": "BubbleBandittt",
          "text": "Whatâ€™s tech stack are you using? Some techs allow for time travel.",
          "score": 6,
          "created_utc": "2026-02-17 06:38:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sv9jx",
          "author": "MissionBad732",
          "text": "Their should be systems and processes in place to stop this sort of thing even being possible, so if it's any comfort this is everyone f up, you just happen to be in the center ðŸ˜…",
          "score": 7,
          "created_utc": "2026-02-17 03:02:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tjya9",
          "author": "dudeaciously",
          "text": "This happens to every one of us eventually.  Live and learn.  Tragedy plus time equals comedy.",
          "score": 3,
          "created_utc": "2026-02-17 05:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y4gm7",
          "author": "Sandinmyshoes",
          "text": "We donâ€™t know anything about your system but Iâ€™m hoping thereâ€™s some time travel mechanism. I think controls should be in place, if you give people access to a big button to destroy everything, someone will come along and press it one day.\n\nSaying that, I feel for you haha.",
          "score": 3,
          "created_utc": "2026-02-17 22:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5suczu",
          "author": "EmotionalSupportDoll",
          "text": "![gif](giphy|81xwEHX23zhvy)",
          "score": 2,
          "created_utc": "2026-02-17 02:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t8a8e",
          "author": "Embarrassed-Swim-710",
          "text": "Keep us updated bro",
          "score": 2,
          "created_utc": "2026-02-17 04:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t9uvl",
          "author": "Tee-Sequel",
          "text": "Were you playing around in a Break glass account? Can we get more details? Also, my condolences",
          "score": 2,
          "created_utc": "2026-02-17 04:39:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v98qo",
          "author": "ScroogeMcDuckFace2",
          "text": "ill pour you out one, brother",
          "score": 2,
          "created_utc": "2026-02-17 14:09:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vonea",
          "author": "purpleWord_spudger",
          "text": "On the very rare occasion something like this happens on my team, I call my friendly neighborhood DBA (aka grumpy, too-good-for-this dba) to restore the database to a new temporary location, recover what I can, and get really uptight about making our own local backups before any change action for a while. We do weekly full and daily incremental backups so this works well. The embarrassment helps too, since everyone hates exposing their own stupidity.",
          "score": 2,
          "created_utc": "2026-02-17 15:28:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5swfia",
          "author": "Afraid-Donke420",
          "text": "No staging environment?",
          "score": 1,
          "created_utc": "2026-02-17 03:09:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sry1q",
          "author": "NoleMercy05",
          "text": "![gif](giphy|d8KOpGnzaAEI7JiVUp)",
          "score": -8,
          "created_utc": "2026-02-17 02:42:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4j3bw",
      "title": "Iâ€™m planning to move into Data Engineering. With AI growing fast, do you think this career will be heavily affected in the next 5â€“10 years? Is it still a stable and good path to choose?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r4j3bw/im_planning_to_move_into_data_engineering_with_ai/",
      "author": "False_Square1734",
      "created_utc": "2026-02-14 12:10:21",
      "score": 115,
      "num_comments": 89,
      "upvote_ratio": 0.82,
      "text": "Iâ€™m planning to move into Data Engineering. With AI growing fast, do you think this career will be heavily affected in the next 5â€“10 years? Is it still a stable and good path to choose?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r4j3bw/im_planning_to_move_into_data_engineering_with_ai/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5bujal",
          "author": "ProfessorNoPuede",
          "text": "5 - 10 years? Dude, that time horizon is so far in the future, nobody knows. 5 years ago lakehouse wasn't mainstream yet. They just released the paper.\n\nIn 5 years, it could be everyone is on-premise again. Heck, it might the year of the Linux desktop.",
          "score": 311,
          "created_utc": "2026-02-14 12:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c1iuy",
              "author": "neoneat",
              "text": "After 4 years of uni, working market changed alr. I dunno how to say about 5 years term, unless telling a lie.",
              "score": 29,
              "created_utc": "2026-02-14 13:09:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d6wnn",
              "author": "Awkward-Cupcake6219",
              "text": "What I really like of this comment is \"it could be everyone is on-premise again\". I see so many companies using databricks for at most hundreds of megabytes of data. Like running over a tree with a tank to clear some farming land.",
              "score": 14,
              "created_utc": "2026-02-14 16:59:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ekwpn",
                  "author": "geek180",
                  "text": "Why do people assume small data = cloud services are pointless or â€œoverkillâ€?\n\nCloud services are *perfect* for small teams and small projects. Itâ€™s all consumption based billing that scales with you and eliminates a lot of infra setup and maintenance that small teams donâ€™t want to deal with.",
                  "score": 10,
                  "created_utc": "2026-02-14 21:19:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5flm4v",
              "author": "ThatSituation9908",
              "text": "Can someone cite the paper? Thanks in advanceÂ ",
              "score": 1,
              "created_utc": "2026-02-15 00:55:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gq3z5",
              "author": "virgilash",
              "text": "I have the feeling 2026 is going to be the year of Linux workstation ðŸ¤£",
              "score": 1,
              "created_utc": "2026-02-15 05:46:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bypcm",
          "author": "Flat_Mammoth_7010",
          "text": "My 2 cents is to learn more on domain knowledge and to understand the business meaning of data.",
          "score": 104,
          "created_utc": "2026-02-14 12:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5czf80",
              "author": "543254447",
              "text": "this is also a risky move. Once you move outnof the domain, all wasted effort. Be sure to commit to an industry before doing this.",
              "score": 29,
              "created_utc": "2026-02-14 16:22:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dymyv",
              "author": "TaartTweePuntNul",
              "text": "And the basics, never forget the basics of DE! Principles of DB management, how to handle different granularities, batch v stream,... These ideas always stay around, same with SWE best practices.\nOnce you have these two you can grow into whatever direction you want. And specializing in a specific domain will amplify this even more.",
              "score": 11,
              "created_utc": "2026-02-14 19:19:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eekwi",
                  "author": "pinamanpongole",
                  "text": "Can you suggest resources for someone who wants to become a data engineer. I do have access to aws skill builder but I fear whatever I learn there will be domain specific.",
                  "score": 1,
                  "created_utc": "2026-02-14 20:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d4v2j",
              "author": "SpareSmileBravo",
              "text": "Could you elaborate more on why more focus on domain would be helpful?",
              "score": 2,
              "created_utc": "2026-02-14 16:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d5bfa",
                  "author": "Reach_Reclaimer",
                  "text": "Why wouldn't domain knowledge be helpful? Unless you're doing pure bronze layer integration or API calling, you need to know about the data you're working with",
                  "score": 5,
                  "created_utc": "2026-02-14 16:51:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5budz2",
          "author": "AverageGradientBoost",
          "text": "we actually had a engineering all hands this week and one of the topics that came up was how the difference in AI for software engineering and data engineering is quite big and AI has a lot of catch up to do in DE. Lots of people complaining that it struggles to build queries and work with data. ",
          "score": 86,
          "created_utc": "2026-02-14 12:15:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c1ko5",
              "author": "PaddyAlton",
              "text": "This is a good point.\n\nI think this is because the behaviour of systems that use data is defined by the code, the schema, and the actual data values.\n\nYou can document schemas, but the more vague the constraints on what the data might look like, the harder it is to build something robust and the more context switching (between code and the actual contents of the upstream source) is required.\n\nNevertheless, I don't think we should rely on this as a 'moat'. It's not a fundamental constraint, more of a context engineering problem - one which people are working on solving. In the last month I've seen the emergence of agentic data analytics implementations that finally look promising. I expect some of these use cases to be cracked by the end of this year.",
              "score": 28,
              "created_utc": "2026-02-14 13:10:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cj4sn",
                  "author": "jupacaluba",
                  "text": "To a certain extent, youâ€™re right. However, Iâ€™ve used Claude to build full transformation logic and I was shocked at how good it was. \n\nItâ€™s a game changer when you donâ€™t need to worry about coding anymore.",
                  "score": 9,
                  "created_utc": "2026-02-14 14:57:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5g4fsz",
                  "author": "NoleMercy05",
                  "text": "It's a terrible and incredibly incorrect point.",
                  "score": -1,
                  "created_utc": "2026-02-15 03:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cb20p",
              "author": "nineteen_eightyfour",
              "text": "I am in our works ai workgroup and I think the biggest issue is that people suck. Iâ€™m our data scientist and I use to work for a consulting company. So Iâ€™d work with Google 1 year or oracle 1 year so I got lots of exposure. The only similarity is that everyone sucks at entering data correctly ðŸ¤£",
              "score": 10,
              "created_utc": "2026-02-14 14:10:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cgdzz",
                  "author": "anti_humor",
                  "text": "Lol yeah - even if you give it access to all the relevant docs, it won't help because the docs are incorrect. Gotta love it.",
                  "score": 1,
                  "created_utc": "2026-02-14 14:42:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cfzqt",
              "author": "anti_humor",
              "text": "I think a huge issue here is data quality. Garbage in garbage out, same as always - sometimes I think hardcore AI evangelists have never worked with messy data in the wild lol. \n\nThe issues I see in data I have to write pipelines for range from things I could see an LLM spotting, to completely normal looking data that is incorrect for super idiosyncratic reasons, to CSVs that are broken in ways that I literally made chatGPT short circuit and enter a death spiral trying to solve. \n\nIt might get there soon, but as of now nontechnical human beings touching data are more chaotic than LLMs seem able to neatly account for. Some of the information needed to contextualize all of the relevant business logic is also risky in terms of exposing to an LLM you aren't spinning up yourself. \n\nThat all being said - they definitely save me a ton of time with things like writing DDL based on a file sample or spitting out SQL and application code syntax I don't feel like looking up. I just haven't seen any evidence they could write any of the pipelines I own effectively without giving it so much context I could've just built it myself in the same time.",
              "score": 5,
              "created_utc": "2026-02-14 14:39:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5cdlc8",
              "author": "brittleirony",
              "text": "Only because they lack context about the structured data. Plenty of companies pushing agents that have decent accuracy querying structured data with suitable context (metadata, descriptions etc). That being said I haven't seen an agent writing pyspark or building a jobs (should be possible with MCP)",
              "score": 4,
              "created_utc": "2026-02-14 14:25:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5e9d6b",
              "author": "Express-Patience8874",
              "text": "There was actually a post not too long time ago. Apparently, AI agent used data and made bunch of false claims. No one checked. VP level made certain moves based on that data lol",
              "score": 2,
              "created_utc": "2026-02-14 20:16:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d027g",
              "author": "TH_Rocks",
              "text": "Microstrategy Mosaic is supposed to be able to flesh out a full schema by itself.  \nMicrostrategy already writes great SQL if the schema is properly defined.\nWe haven't upgraded yet to really test it out.",
              "score": 1,
              "created_utc": "2026-02-14 16:25:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5f22cd",
              "author": "spoopypoptartz",
              "text": "claude works perfectly for me but my teamâ€™s docs and data modeling are on point. \n\n\ncurrently experimenting to see if i can get acceptable performance on local models since i need similar capabilities in an air-gapped environment",
              "score": 1,
              "created_utc": "2026-02-14 22:54:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5fkqjn",
              "author": "idiotlog",
              "text": "This is not true. Context for the AI is just bad.",
              "score": 1,
              "created_utc": "2026-02-15 00:49:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gum6m",
              "author": "Faintly_glowing_fish",
              "text": "This is not an ability problem but rather an integration problem.   In most cases the AI just can access the same data people can and they end up having no way other than hallucinating.   When given the ability to actually access data I find it doing fine.  The hard part is to get it access while maintaining proper way of sandboxing and audit",
              "score": 1,
              "created_utc": "2026-02-15 06:26:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5cm6t8",
              "author": "_weined",
              "text": "Everything around building jobs, provisioning infrastructure, automating a portion of quality checks, and especially cookie cutter ingestion is ripe for AI. The business logic layer is not so much at risk for complex enterprises but a lot of the surrounding work DEs typically do is already able to be commoditized by AI.",
              "score": 0,
              "created_utc": "2026-02-14 15:14:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dmrx0",
              "author": "nowrongturns",
              "text": "You must not work at big tech. Itâ€™s a solved problem. I just prompt with business logic and give it context and it authors near perfect sql.",
              "score": 0,
              "created_utc": "2026-02-14 18:19:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5g4d78",
              "author": "NoleMercy05",
              "text": "![gif](giphy|ko4UuHFAOZE3jN3qRB)",
              "score": 0,
              "created_utc": "2026-02-15 03:01:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bzp4v",
          "author": "PaddyAlton",
          "text": "Two big uncertainties re: AI -\n\n1. speed of takeoff: do compounding gains make performance improvements accelerate over the next year, or do fundamental bottlenecks start to put the brakes on?\n2. performance ceiling: just _how_ good can LLM-based models get (disregarding whether they get there quickly or slowly)? \n\nI think even if AI foundation models get no better from now on, they are _already_ good enough that harnessing them and rolling them out across industry will lead to a shift in ways of working on the scale of the invention of the world wide web. So if you take this path, assume you're going to be very focused on data-engineering-applied-to-AI.\n\nNow, given the actual pace at which human institutions move, I am convinced that even if foundation models get _quite a lot better_ it will take five years to reorient a bunch of legacy businesses around the new technology. But from there, all bets are off.\n\nA reorientation around AI may well make _experienced technologists_ even more important as staff for competitive businesses, but don't expect 'data engineering' specifically to still be prominent (consider how the database administrator role has faded in salience due to the Cloud revolution, for comparison).\n\n---\n\nAll in all: don't let AI put you off. But go in with your eyes open: none of us can make precise predictions about 2035, and even on the shorter horizon you need to prepare to be flexible and stay abreast of this new technology.",
          "score": 21,
          "created_utc": "2026-02-14 12:56:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bv4fc",
          "author": "i_fix_snowblowers",
          "text": "No question that DE will be affected by AI.\n\nBut as the economists say there are \"substitution effects\" and \"income effects\".\n\nSubstitution effects are the first-order job displacement things that you're asking about.\n\nIncome effects are the overall increase in data processing as data engineering tasks become cheaper to execute. \n\nIf I'm being optimistic, I'd say that the overall increase in data processing would lead to growth in DE, it's just that the day-to-day for a DE will change. Like instead of manually fixing 5 pipelines, you monitor 50 pipelines and manage the agents that fix them.",
          "score": 17,
          "created_utc": "2026-02-14 12:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bzzth",
          "author": "i_hate_budget_tyres",
          "text": "DE is more insulated than SWE because LLM's scraping the net can't know business domain information.\n\nYou'd have to develop custom LLM's trained on data across the business that can ingest documents, databases, the codebase etc.  And keep a no doubt very expensive team to keep tuning and updating the model as business requirements change.\n\nWhere I work, SWE's are finding 'generic' LLMs much more useful than DA's, DS's and DE's.  All apps tend to resemble each other and complete codebases for all kinds of apps are available to scrape on the internet, so the models have a wealth of solutions to fall back on.\n\nAs a DE I'm finding its more a glorified google search.  If I unleash it in agentic mode, it utterly balls stuff up.  It can't see the databases nor final dashboards etc so is missing a lot of information that I hold in my head and get a steer on from SME's across the business.  This information is proprietary and often just held in peoples heads, so there is no way a 'generic' LLM would have access to it.\n\nHaving said this, I think DEâ€™s working in smaller companies with less complex tech stacks and environments and are finding â€˜genericâ€™ LLMâ€™s more useful.  I work in a multi-cloud, multi vendor environment.  We have every permutation of pipeline you can imagine weaving its way in and out of multiple platforms.  There is no way a â€˜genericâ€™ LLM can see across all of it so its usefulness is limited.",
          "score": 28,
          "created_utc": "2026-02-14 12:58:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d3vy8",
              "author": "slayerzerg",
              "text": "Pretty much. Esp industries with sensitive data. DE work is hard",
              "score": 3,
              "created_utc": "2026-02-14 16:44:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cf6s3",
          "author": "SchemeSimilar4074",
          "text": "I think DE will become a jack of all trades, with AI advancing as it is. Business people will dabble here and there and might do their own dashboards like a PowerPoint but they have too many meetings to actually do technical works. There are many components but with the help of AI, a very small team can do it all, from deploying cloud infrastructure, testing, data engineer etc.Â \n\n\nSo a DE will be expected to know SRE, SDET, DE, DA and even ML. A few years ago, my job was heavily DE and DA and now it also involves building infrastructure and test suite, which are the jobs of SRE and SDET. Its much easier to build pipelines and dashboards these days. It means more people from other tech areas transition to DE, increasing the competition. For people in the industries, as long as you're expanding your skill sets, it's OK. For someone new, it'll hard to break into, because you might even be competing with SWE, SDET etc who lost their jobs and tried to transition to DE for example.Â \n\n\nIf you wanna get into tech, be prepared to learn everything from every areas. And save a lot, just in case ðŸ˜‚",
          "score": 8,
          "created_utc": "2026-02-14 14:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bzzlq",
          "author": "TripleBogeyBandit",
          "text": "I disagree with most of the comments here. I do the more complex data engineering; Kafka, spark, building api services, touching dozens of cloud resources, etc..  ai use to suck at doing these things a year ago but now, itâ€™s kicking ass. Iâ€™ve written entire pipelines, api services, even small business apps with Claude code. I think the people that are trashing donâ€™t know how to leverage the ai tools and feel threatened by their capability. You need to learn how to use these tools effectively or youâ€™re gone.",
          "score": 28,
          "created_utc": "2026-02-14 12:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ce1ue",
              "author": "danielfrances",
              "text": "I agree, with some caveats:\n- Even if you learn how to use these tools effectively, the speed of improvement with agentic tools is insane and we might end up redundant anyways.\n- I think another category of people are ones like me - I know how to leverage the tools and see their utility while also feeling threatened by their existence.\n\nA lot of times when this topic comes up people are judging agentic tools by their capabilities right now. At my job, we test drove a bunch of AI tools last summer and it was a total fail with our codebase. I personally started seeing value around October/November. Two weeks ago our entire developer team got told we need to use Claude every day - and honestly, it is SO MUCH better today than even a few months ago.\n\nMy point is - when you see people saying things like \"it can't really handle xyz right now\" just know that statement might be false in as little as 3-6 months.\n\nAs such, I don't think we can even make predictions 2 years out. Maybe the cost of research kills a bunch of the big companies and the progress slows. Maybe the cost of subs goes so high we can't afford it. Or, maybe things keep going as they have been and agentic tools can fully automate our jobs in 2 years. We can't know right now, but you should prepare for the possibility that the hype is real, just in case.",
              "score": 5,
              "created_utc": "2026-02-14 14:28:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dc73p",
                  "author": "TripleBogeyBandit",
                  "text": "Youâ€™re spot on, anyone who writes these tools off is a fool.",
                  "score": 2,
                  "created_utc": "2026-02-14 17:26:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5f1vyw",
                  "author": "Murtz1985",
                  "text": "Exactly how I see it. Just in case.",
                  "score": 1,
                  "created_utc": "2026-02-14 22:53:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5eq05p",
              "author": "rotterdamn8",
              "text": "For sure, definitely agree that people donâ€™t realize how fast things are moving. \n\nMy company gave us the tools last year and Iâ€™ve been using them as the low hanging fruit coding assist, like â€œrewrite this pandas code for pysparkâ€. Basic stuff.\n\nBut now I keep hearing about Claude Opus 4.6 since it came out a few weeks ago, and I realized holy shit, I need to catch up.\n\nOP, my advice is to learn how to use the tools, especially Claude. Forget 5 years, things are changing quickly. It sounds like some DEs here are sleeping on it. That would be a mistake.",
              "score": 4,
              "created_utc": "2026-02-14 21:46:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g409s",
                  "author": "CluckingLucky",
                  "text": "Is this how the world felt when aws was launched? lol",
                  "score": 1,
                  "created_utc": "2026-02-15 02:58:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ccxdn",
              "author": "bubzyafk",
              "text": "I think you are right to some extent, but the other people is also right, particularly on Business domain.\n\nAI can do that exact math stuffs, e.g: Build API to connect based on SaaS documentation etc. itâ€™s a straight forward engineering with error or non error as the output.\n\nBut asking AI to replicate complex business knowledge and on top of it, some ERP complexity, is still way long to go or even impossible without Human intervention. E.g: Theoretically Net Income = revenue - Expenses, but imagine apps A provides Column ABC and XYZ to calc Expenses, and ERP B Provides col C123 and D456 to calc revenue. Requires some window function, sum, and what not.. And col name is not very descriptive enough to tell itâ€™s to calc Revenue, giving 0 context for AI to process\n\nSo, yeah.. in data engineering, AI wonâ€™t 100% do the magic.. it can help us faster to build API, make some Pyspark/scala/whatever code, sql (if connected with our metadata), give some reasoning/exploration, etc.. but it canâ€™t remove human entirely.",
              "score": 2,
              "created_utc": "2026-02-14 14:21:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dc3nr",
                  "author": "TripleBogeyBandit",
                  "text": "What youâ€™re talking about is a semantic layer and there are plenty of great examples of ai delivering exceptional performance and value when given access to a semantic layer.",
                  "score": 3,
                  "created_utc": "2026-02-14 17:26:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cnuqg",
              "author": "IAMHideoKojimaAMA",
              "text": "I dont think that's more complex de, thats like intro to de sruff",
              "score": 0,
              "created_utc": "2026-02-14 15:23:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cy4yv",
          "author": "Suspicious-Spite-202",
          "text": "Figure out how to do machine learning over relational databases and graph databases â€” relational deep learning.  Also look into graph databases as semantic layers.  Finally, figure out exploratory data analysis (eda) and visualizations via an Llm interface.  \n\nStar schemas and kimball techniques are still relevant for having a stable backbone. Basic principles of modular etl are still relevant too.",
          "score": 5,
          "created_utc": "2026-02-14 16:16:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bv0ll",
          "author": "takenorinvalid",
          "text": "I have a theory that the data engineers working on AI deliberately sabotaged its ability to do data engineering to make sure they'd be the only people with job security.Â ",
          "score": 17,
          "created_utc": "2026-02-14 12:20:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5catj6",
          "author": "PeterDowdy",
          "text": "So far, AI coders are bad at data engineering because itâ€™s hard for them to reason about the size of data - they can get the structure and transformations right but they donâ€™t do scaling and performance.",
          "score": 3,
          "created_utc": "2026-02-14 14:09:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c1q26",
          "author": "Illustrious-Pound266",
          "text": "It's already impacted data engineering. Truth is that you need to learn how to use AI in development now",
          "score": 5,
          "created_utc": "2026-02-14 13:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c1wnf",
              "author": "False_Square1734",
              "text": "Is that possible to replace data engineers then?",
              "score": 1,
              "created_utc": "2026-02-14 13:12:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5c35cw",
                  "author": "n_ex",
                  "text": "No, because you still need to guide the AI to implement the solution - though data engineers are becoming faster at implementing things (as all other developers) so companies might need less of them down the line I guess",
                  "score": 7,
                  "created_utc": "2026-02-14 13:21:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ckpj7",
                  "author": "jadedmonk",
                  "text": "AI doesnâ€™t just replace people. Itâ€™s not like you can plop an AI on a chair to replace the human. Even the best ones are only at like ~80% benchmark - imagine if you wrote wrong production code 20% of the time how poorly things would go. \n\nAI is here to make us more efficient, and it does. Perhaps as those efficiency gains are made theyâ€™ll hire less engineers, but Iâ€™ve seen the opposite as they need more engineers to build and maintain the AI",
                  "score": 3,
                  "created_utc": "2026-02-14 15:06:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5d4w9d",
                  "author": "SRMPDX",
                  "text": "Data engineers who use AI will replace those who don't",
                  "score": 2,
                  "created_utc": "2026-02-14 16:49:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5buftv",
          "author": "ThroughTheWire",
          "text": "nobody can tell you the answer for this. In my semi informed opinion I think we will have a role for a period of time in creating pipelines for data to feed AI models but that will only last for so long and may not be a role available to you by the time you switch. Otherwise I think there will be data engineering jobs and software engineering jobs over the next 10 or so years while AI technology advances exponentially and then who knows where we are after that. I wouldn't bet my entire life on moving into any software engineering discipline at this point.",
          "score": 7,
          "created_utc": "2026-02-14 12:15:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c0pw3",
          "author": "AdministrationAny136",
          "text": "In the future, I'd also focus more on domain knowledge.",
          "score": 2,
          "created_utc": "2026-02-14 13:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cg9kf",
          "author": "randomperson32145",
          "text": "Toolbox and tools, those change.",
          "score": 2,
          "created_utc": "2026-02-14 14:41:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ch7c8",
          "author": "gn-musa",
          "text": "Fancy AI eats data. Someone's got to build the damn pipelines.",
          "score": 2,
          "created_utc": "2026-02-14 14:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dl8j5",
          "author": "TheCamerlengo",
          "text": "5-10 years? Even before the emergence of AI and LLMs, that was a long stretch of time. I donâ€™t think data engineering will be a thing recognizable in 5 years as to what it is today.",
          "score": 2,
          "created_utc": "2026-02-14 18:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e6ens",
          "author": "reviverevival",
          "text": "24 months ago, the smartest people in the world could look at the evidence and call LLMs \"auto-complete on steroids\". I did not agree at the time, but it was a credible assertion. If anyone is still saying that in 2026, they are not credible based on the research that's come out since then. In 2024 people said LLMs can't solve Wordle, and they can't count the Rs in strawberry. No frontier model fails at those tasks in 2026 but people are still repeating the same memes.  \n  \nPeople who are not using AI to (effectively) code think it's supposed to be some kind of maestro that 1-shot generates you a script out of thin air. I would characterize the latest iteration of AI to be more akin to a swarm intelligence. Even a single frontier model agent is not very impressive by itself. I've found that even given a well defined list of tasks, a single Claude agent will degrade rapidly in performance because of context rot. But if you start orchestrating: creating and destroying agents for singular purposes (cattle, not pets), then the capabilities become very impressive. Your agents can make a plan, execute on it, _check the results_, adjust the plan, retry, and so forth all autonomously. This is the scary part, not because it's really good at leetcoding, because planning and iteration is something that's transferrable to all professions.  \n  \nSecondly people misunderstand, especially after the ChatGPT-5 fiasco: GPT-5 is not a single model, it is a router, a reverse proxy, that assesses your query and routes it to one of its 10s of actual models behind the curtain. Despite the marketing it was primarily a cost engineering tool for OpenAI, because it removes choice from the user about what model you are being served with. So you could be using \"ChatGPT-5.2\", and you get some shit answer because it decides your question wasn't worth its time (or their servers are overloaded, or any other reason) and routed your question to some shit model. All frontier \"models' work this way, \"Opus 4.6\" does the same. So all those seemingly cracked models that Google is sending out to dominate benchmarks? Yeah, no guarantee that's the one serving you (or is even one of the possible options).\n  \nThat means you cannot one-shot some chatbot on your free account and have an accurate picture of the state of the art. Anyone who is isn't using harnessed agents and burning tokens have no idea what they're talking about.  \n  \n18 months ago I thought the role of humans will be as model whisperers, guiding these mercurial and finicky chatbots. Well, prompt engineering came and died as a profession in less than 12 months. 3 months ago I thought the role of humans would be building mcp integrations to connect agents to the business domain. Well, Claude is pretty well capable of being pointed at any API and building an integration by itself right now.  \n  \nCurrently I would say my comparative advantages are understanding the business domain, architecture, and security. If you notice, none of these are junior attributes. As an OG AI hater, it pains me to say that if you gave me a list of projects to complete, and I had a choice between an intern or Claude Code and an intern's salary of tokens, for effectiveness I would take Claude Code every time hands down. (I'm not saying that the AI is better in every way, rather there's no comparative advantage).\n  \nAnd I don't even know how long my moats will hold. There is a degree of skill involved with using agents, which can explain why some people are getting poor results, but tbh the skill curve is not that high, and any clever tricks people think of to squeeze more performance out of the agents (e.g. chain-of-thought, ralph-loop, sub-agents) just get baked into the harness and democratized. I think the only reason people don't get better at it is a psychological aversion to AI.\n\nAgainst all odds, my company managed to assemble a competent team of AI engineers that built an in-house agentic framework that _actually delivered business value_...for about 12 months. Now I wonder what was the whole point if every analyst will be rolling with Claude Cowork on their laptop in 6-12 months. If you look at where we were just 24 months ago vs now, it's hard to imagine where we'll be 24 months from now.  \n  \nSo all that to say, we're all guessing for the next 5 years. My hope is a small business renaissance and a democratization of knowledge, but I fear we just end up in a world where 12 rich guys own everything until one day they betray each other. But I wanted to give a deep and nuanced answer, and get my own thoughts out. We can only nudge the world to the better scenario if we're honest to ourselves.",
          "score": 2,
          "created_utc": "2026-02-14 20:00:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fb1vx",
          "author": "rambouhh",
          "text": "Data is the backbone of ai, and ai is worthless without good data. Worry about learning principals, the business meaning of data, and you are going to be better off than 95% of people even if the job itself is gone or looks completely different in 5 years",
          "score": 2,
          "created_utc": "2026-02-14 23:49:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fikc4",
          "author": "CluckingLucky",
          "text": "Ok, so there are two ways to answer your question.\n\nFirst off, as u/ProfessorNoPuede pointed out, no one knows what will happen five or ten years into the future in terms of tooling or capability. No one knows what the industry is going to be calling for.\n\nHowever, and this leads to the second way to answer your question--- look at the fundamentals of what drives demand for data, and, consequently, what drives demand for data engineers.\n\nIt's these things that suck up a lot of data to deliver correlative approximations that can never be accurately used in the exact processes  data engineers excel in. LLMs.\n\nSo, synthesising this: yes, the tooling will change, but data engineers will only be riding this AI bubble as much as they have been already. Maybe more of them will be data scientists. Maybe more of them will be AI, but I doubt it.\n\nThere might be less of them. If that's the case, those that remain (or enterprise) will be very highly paid.\n\nSincerely,\n\nNo one smarter than your average CEO thought leader",
          "score": 2,
          "created_utc": "2026-02-15 00:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d60ua",
          "author": "SRMPDX",
          "text": "AI is already heavily affecting DE. There would be no AI without DE. Figure out how to use it, and how to market your knowledge to people who need it in their organization. \n\nIn AI we're seeing huge changes in 5-10 months, nobody knows what's coming in 5-10 years.",
          "score": 2,
          "created_utc": "2026-02-14 16:55:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cu6y6",
          "author": "shittyfuckdick",
          "text": "Bro im losing my mind you realize this thing is just autocomplete right? The only people saying its gonna replace devs are the CEOs of AI companies who are trying to pump the stock.Â ",
          "score": 3,
          "created_utc": "2026-02-14 15:56:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5drn5a",
              "author": "alexlazar98",
              "text": "Looking at the rest of the comments, this is really not your audience, lol. The blinders are on.",
              "score": 0,
              "created_utc": "2026-02-14 18:43:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fcifx",
                  "author": "shittyfuckdick",
                  "text": "Are you saying the blinders are on me? I could be totally wrong and im preparing for worst case scenario. but i really think most of AI is driven by hype.Â ",
                  "score": 1,
                  "created_utc": "2026-02-14 23:58:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5c7tx6",
          "author": "andrew2018022",
          "text": "I think itâ€™s the most ai proof of the â€œdataâ€ domains because there is so much analytics slop on social media nowadays, the barrier to entry of analytics and BI is so low. DE requires more technical knowledge you canâ€™t just vibe code solutions",
          "score": 1,
          "created_utc": "2026-02-14 13:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cbu19",
          "author": "hibe1010",
          "text": "Of course it will be affected- a job like that will always and was always heavily affected by latest emerging technologyÂ \n\nYou will need to stay on top of those then I am very sure it is still an interesting and challenging field to choose - just donâ€™t expect that your skill set will not have to change over timeÂ ",
          "score": 1,
          "created_utc": "2026-02-14 14:15:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ci7g4",
          "author": "DenselyRanked",
          "text": "It's a difficult question for anyone to answer. Data engineering as a practice will still exist but the methods, tools, and skill set needed will evolve. \n\nThere are smart people putting a lot of thought into this and I tend to agree with much of [this presentation](https://www.youtube.com/watch?v=Fu6JBodxqGQ), where there is not going to be a Data/Analytics Engineer title in the near future for data teams, opting instead for titles like Data Product Owners and Data Domain Experts. AI can help close the technical gap between product management and engineering, so DE's will need more emphasis on stakeholder communication and requirements gathering. ",
          "score": 1,
          "created_utc": "2026-02-14 14:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ct19x",
          "author": "Lastrevio",
          "text": "Between \"it will replace me\" and \"it's not helping me at all\" there will have to be a transitory phase where it will increase your productivity. So far, AI is barely helping in making me 5% more productive, and I'm doing quite simple things. I'll worry that it's gonna replace me when it will double my productivity, until then it's next to useless.\n\nMoreover, there are academic studies showing that programmers are 20% *less* productive using AI.",
          "score": 1,
          "created_utc": "2026-02-14 15:50:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cu9nk",
          "author": "rajekum512",
          "text": "If data engineering is falls under \"white collar\" jobs then it would be safe to be classified under endanger threat",
          "score": 1,
          "created_utc": "2026-02-14 15:56:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cx82c",
          "author": "billysacco",
          "text": "My opinion itâ€™s already starting to be affected with many employers going to the offshore combined with AI route. It will probably take a few years to come back around, the off shoring stuff always seems to go in a circular cycle. Or the AI is as good as they say (I am still skeptical on that) and things donâ€™t come back around. Hard to say.",
          "score": 1,
          "created_utc": "2026-02-14 16:11:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dvtkm",
          "author": "melvinroest",
          "text": "Yea, I think doing data engineering is AI engineering-lite in some cases",
          "score": 1,
          "created_utc": "2026-02-14 19:04:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dy6d5",
          "author": "frozengrandmatetris",
          "text": "in 5 to 10 years, the number of orgs locked into legacy low-code ETL tools like ODI and SSIS will be almost exactly the same",
          "score": 1,
          "created_utc": "2026-02-14 19:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e0ku1",
          "author": "Least-Possession-163",
          "text": "DE or anything code driven will be impacted. Companies might ask you to do everything end to end. AI augmentation will lead to fewer jobs (leaner team) but more work. 5 years is too long. 5 years back NLP was a big deal and now people have AI girlfriends. So take your bet.",
          "score": 1,
          "created_utc": "2026-02-14 19:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e656w",
          "author": "ImpressiveProgress43",
          "text": "AI is heavily affecting data engineering. It is still stable and good. I havent seen a single year where de's werent over demanded in my area.",
          "score": 1,
          "created_utc": "2026-02-14 19:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e7ksf",
          "author": "decrementsf",
          "text": "<< any professional role >> Will be affected in the next 5-10 years.\n\nFor << any professional role >> study the profession, and practice applying the current AI trends to it.",
          "score": 1,
          "created_utc": "2026-02-14 20:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h911d",
          "author": "po1k",
          "text": "It you want smth stable go with c/c++. DE is tough and far away from being stable. At this point I realized c++ might easier",
          "score": 1,
          "created_utc": "2026-02-15 08:43:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c6g4r",
          "author": "Nekobul",
          "text": "I think when the LLMs start to perform brain surgeries on their own, you will have to start worrying if your job is next. Until then, breathe! Everything will be fine.",
          "score": 1,
          "created_utc": "2026-02-14 13:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cjs47",
          "author": "itsMineDK",
          "text": "I wouldnâ€™t do it, but thatâ€™s just me.. a lot of tech folks were making bank on software engineering when i started uni, I wanted to go there but I didnâ€™t and did finance instead.. that was in 2008 l, always regretted UNTIL NOW.. \n\nitâ€™s been rough for SEs in general but things might turn around nobody has a crystal ball",
          "score": 0,
          "created_utc": "2026-02-14 15:01:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5guc05",
          "author": "Faintly_glowing_fish",
          "text": "Yes it will be very heavily affected in the future.  By 5 years much of what DE does today will be completely gone.  Some of the skills will be useless, and some of the things will be as pointless as formatting the code with your hand. But that is fine really.\n\nThe distinction between different kinds of engineering will break down a lot and you will be doing very different things",
          "score": 0,
          "created_utc": "2026-02-15 06:23:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gv5hw",
              "author": "False_Square1734",
              "text": "Then do u think IT jobs will be gone?",
              "score": 1,
              "created_utc": "2026-02-15 06:31:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gviy2",
                  "author": "Faintly_glowing_fish",
                  "text": "You will have a job that will still involve computer programs and data but I honestly donâ€™t know what you really call it.   You will be doing a mixture of what DE DS and product engineer do today and some infra too",
                  "score": 1,
                  "created_utc": "2026-02-15 06:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2wvxg",
      "title": "Being pushed out of job, trying to plan next steps",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/",
      "author": "octacon100",
      "created_utc": "2026-02-12 15:46:59",
      "score": 85,
      "num_comments": 16,
      "upvote_ratio": 0.94,
      "text": "First post for a while, hope this is ok. Spent roughly 5 years at my current job, all with excellent reviews each year, survived the last round of layoffs, had my performance review which basically said don't make any thing and start putting process in place while the ceo just looked at me in disgust. So I'm thinking I'm pretty much on the way out as the company is planning to buy software that makes what I'm doing irrelevant (Has its own data warehouse, it's own way of loading data, etc).\n\nOur company is currently all on prem for work, so a big shared drive is our datalake, sql server is our database, and the best I've been able to do to improve/modernize things was to introduce Prefect for our orchestration, make my own libraries in python to make loading data easier, show the usages of PowerBI and Tableau and create a data warehouse that did what the company wanted to do, but now has decided was a waste of time.\n\nI've started go through the AWS Data Engineering Exam and Snowflake exams, and I have projects on Github that show the use of Amazon S3, Athena, and Glue, so I can at least point to those and say I have cloud experience that I've set up myself. I've been applying to jobs, but I usually get stopped where they are looking for cloud experience. \n\nI've been working with data for almost 20 years now, so I'm hoping my experience can help in terms of getting a job. Does anyone have any advice out there for how to get an in on cloud experience or what places look for with cloud experience? Would the certifications be enough?\n\nAny help is greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o50agi9",
          "author": "snarleyWhisper",
          "text": "Hey there , I had a similar thing happen to me a few years ago. Our whole department was outsourced and then fired unceremoniously. I did some courses on snowflake , databricks and fabric they all have free training and tiers. I would get to the end of the rounds with an interview and usually they would go with someone who had more direct experience with their exact stack. Itâ€™s frustrating but ultimately I ended up landing somewhere that the tooling was less important since Iâ€™m setting a lot of it up. But generally if you focus on one of the top data platforms thatâ€™s your main top of funnel filter.",
          "score": 30,
          "created_utc": "2026-02-12 16:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52k3oh",
              "author": "octacon100",
              "text": "Yeah Iâ€™ve had the same experience, make it to interviews do ok, get rejected for someone that has the experience. Good idea to go for jobs where there are starting to make the move.",
              "score": 4,
              "created_utc": "2026-02-12 23:20:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54je8a",
          "author": "viniciusjooj",
          "text": "You donâ€™t sound underqualified, you sound mis-positioned. 20 years in data isnâ€™t erased because the stack changed. Before stacking more certs, Iâ€™d get really clear on how you want to position yourself. A structured strengths/work-style assessment (CareerExplorer, Pigment, etc.) can actually help frame your narrative. Are you a systems architect? a data reliability person? a modernization lead? That clarity matters in interviews. Certs help, but story + positioning is what gets you past the cloud filter.",
          "score": 25,
          "created_utc": "2026-02-13 07:16:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b8ibb",
              "author": "valentin-orlovs2c99",
              "text": "Yeah this is super on point. OP, the way you described what youâ€™ve done already actually screams â€œmodernization / enablementâ€ more than â€œjust a data engineer.â€\n\nYou took an onâ€‘prem mess, brought in orchestration, built libraries, introduced BI tools, and designed a warehouse. Thatâ€™s literally the narrative a lot of companies want for â€œweâ€™re stuck on old infra and need someone to drag us into 2025 without breaking everything.â€\n\nIf you frame yourself as â€œI specialize in taking legacy onâ€‘prem setups and moving them toward cloudâ€‘ready practicesâ€ then the lack of BigCorpâ€‘AWSâ„¢ experience hurts less. Pair that story with 1â€“2 focused certs and a couple of small, very concrete cloud projects and youâ€™re in a much better spot than trying to look like a generic cloud data engineer.\n\nAlso, target places that are midâ€‘transition, not already full cloud. Those folks value someone who actually knows the ugly side of onâ€‘prem.",
              "score": 4,
              "created_utc": "2026-02-14 08:47:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o511l9o",
          "author": "Historical-Fudge6991",
          "text": "Honestly the cloud transition isnâ€™t too bad. Iâ€™d recommend John Savill on YT. I find the biggest hurdles are RBAC (thankful for a good IT team) and understanding solutions. Thereâ€™s 50 ways to make a record with cloud but the core DE principles will always apply. You could checkout Databricks if you want to leverage your python xp",
          "score": 9,
          "created_utc": "2026-02-12 18:53:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52ksfi",
              "author": "octacon100",
              "text": "Havenâ€™t heard of John Savill, Iâ€™ll have to check him out. Thanks. Yeah IAM has been a whole thing for sure. Even close code has issues with that.",
              "score": 2,
              "created_utc": "2026-02-12 23:24:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50c5ss",
          "author": "rotr0102",
          "text": "If you decide to spend some time learning cloud keep in mind some vendors offer free trials, and then you can restart them with different throw away email accounts. So, you could start up a snowflake trial with dbt trial and build some models. Just save everything locally so you can rebuild the environment quickly when you need to restart your free trial.",
          "score": 13,
          "created_utc": "2026-02-12 16:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52o7qk",
          "author": "DoomsdayMcDoom",
          "text": "Job market is tough for us older guys.  I was in your shoes at a point in time.  Thatâ€™s when I started my own consulting company and havenâ€™t looked back since.  I gained the cloud experience I was lacking as I gained more clients.  Now I just run the business and let the younger guys worry about learning the next and greatest tech.",
          "score": 6,
          "created_utc": "2026-02-12 23:43:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52pnjp",
              "author": "octacon100",
              "text": "I did try a bit of that, getting the first clients is tough. Tried linked in with local firms, have previous people Iâ€™ve worked with that might be looking for people. Any hints on how to get your first client? Thatâ€™s where Iâ€™m getting stuck. Upwork seems like spending money on a slot machine.",
              "score": 1,
              "created_utc": "2026-02-12 23:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o53c81g",
                  "author": "DoomsdayMcDoom",
                  "text": "My first gig I worked with a local recruiter and asked to go corp to corp.  Then I started to attend start-up networking events and started getting word of mouth from clients.",
                  "score": 3,
                  "created_utc": "2026-02-13 02:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51z5w1",
          "author": "cky_stew",
          "text": "I mean dude youâ€™ve kinda been playing on hard mode - as long as you donâ€™t get a case of â€œweâ€™ve-always-done-it-this-way-itisâ€ then you will understand the concepts, problems, and risks of bad system design as it all applies in the cloud too. Iâ€™d focus on the concepts of orchestrators, transformation tools, and OLAP dbs, rather than try to get direct experience with any particular set of tools - stuff tends to be a bit mix and match sometimes. If you need to do something to get it on your resume just to get past the recruiters then go for some certifications - but you should be fine in an interview with someone who knows the deal - youâ€™ll probably enjoy the cloud compared to on prem - you get so much more done itâ€™s great!",
          "score": 4,
          "created_utc": "2026-02-12 21:33:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52jtuf",
              "author": "octacon100",
              "text": "Thanks, it helps to read this.",
              "score": 1,
              "created_utc": "2026-02-12 23:19:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56ab3v",
          "author": "aMare83",
          "text": "I genuinly think cloud experience is a bit overrated. Ãt the end of the day there you also have storage, computation, databases and semi or unstructured file formats.\n\nI don't think you could not pick up the level of cloud knowledge you need in 2 weeks. Your database design, pipeline creation and orchestration experience worth gold comparing to young guys who can click here and there on AWS or Azure UI.\n\nIf I was an employer I would give you the chance for sure.",
          "score": 5,
          "created_utc": "2026-02-13 15:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5g3ll5",
          "author": "peeyushu",
          "text": "I am also in a similar stage in my life/career and it almost feels like jobs are not for me. I have a long term contract with a public sector client but it can go away anytime and then, will have to hunt for new work. \n\nThe general advice about sharing stories and experiences of taking client(s) from on-prem to cloud is very sound, try publishing something on linkedin/medium or substack or do your own website. Good luck out there.  ",
          "score": 2,
          "created_utc": "2026-02-15 02:55:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5262qu",
          "author": "redditreader2020",
          "text": "Snowflake has great free training and docs.",
          "score": 1,
          "created_utc": "2026-02-12 22:06:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55hig1",
          "author": "Sufficient_Example30",
          "text": "The best thing to do is ,\nStudy while on the job and do the minimum to get by and keep applying.\nJump at the first chance",
          "score": 1,
          "created_utc": "2026-02-13 12:20:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r69f91",
      "title": "What is the maximum incremental load you have witnessed?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r69f91/what_is_the_maximum_incremental_load_you_have/",
      "author": "kaapapaa",
      "created_utc": "2026-02-16 13:19:40",
      "score": 76,
      "num_comments": 48,
      "upvote_ratio": 0.98,
      "text": "I have been a Data Engineer for 7 years and have worked in the BFSI and Pharma domains. So far, I have only seen 1â€“15 GB of data ingested incrementally. Whenever I look at other profiles, I see people mentioning that they have handled terabytes of data. Iâ€™m just curiousâ€”how large incremental data volumes have you witnessed so far?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r69f91/what_is_the_maximum_incremental_load_you_have/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5ogav0",
          "author": "Sad_Monk_",
          "text": "smsc project @ a large indian telco\n\nevery 10 min ~100 gb mini batch mode from raw log files to oracle\niâ€™ve worked in insurance telcos and now banking \n\nno one does huge loads like telcos",
          "score": 77,
          "created_utc": "2026-02-16 13:25:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qo064",
              "author": "billy_greenbeans",
              "text": "Why do telcos have such large loads? Just sheer volume of calls being placed?",
              "score": 10,
              "created_utc": "2026-02-16 19:52:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5red6w",
                  "author": "mow12",
                  "text": "telco companies usually have tens of millions of user actively making transactions every day. it could be call or sms or data,mostly.",
                  "score": 9,
                  "created_utc": "2026-02-16 22:01:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ogqbp",
              "author": "kaapapaa",
              "text": "interesting. looks domain plays a large role.",
              "score": 9,
              "created_utc": "2026-02-16 13:28:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5olg64",
          "author": "lieber_augustin",
          "text": "Iâ€™ve worked with very large telemetry datasets â€” up to 1â€“2 Pb of scanner data offloaded from autonomous test drives.\n\n\nRegarding 15Gb/day of new data -  it is already quite reasonable amount of data. If not treated properly it can \nbecome unusable very quickly. \n\nLast year I had a client who was struggling with 118 Gb of total data. \n\n\nSo Data Architecture is not about the size, itâ€™s about how you treat it :)",
          "score": 47,
          "created_utc": "2026-02-16 13:54:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5olzui",
              "author": "kaapapaa",
              "text": ">So Data Architecture is not about the size, itâ€™s about how you treat it :)\n\nðŸ’¯ \n\nUnfortunately recruiters aren't aware of it.",
              "score": 13,
              "created_utc": "2026-02-16 13:57:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5pkh5a",
                  "author": "TheOverzealousEngie",
                  "text": "It's a comment born of experience, so the true statement is Data Architecture is not about size, it's about experience. ",
                  "score": 6,
                  "created_utc": "2026-02-16 16:47:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q54r7",
              "author": "Cpt_Jauche",
              "text": "Can you elaborate what you mean by â€žtreatmentâ€œ, like give an example?",
              "score": 6,
              "created_utc": "2026-02-16 18:23:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p6x0g",
          "author": "crorella",
          "text": "In Facebook, it was common to work with tables that had 1 or 2 pb per day partition, specially in feed or ads.Â \n\nThe warehouse was around 5 exabytes in 2022.Â ",
          "score": 44,
          "created_utc": "2026-02-16 15:45:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5poykv",
              "author": "dvanha",
              "text": "holy fuckeronies",
              "score": 18,
              "created_utc": "2026-02-16 17:08:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rmm6s",
              "author": "puripy",
              "text": "I believe it would've tripped by now?",
              "score": 5,
              "created_utc": "2026-02-16 22:43:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5scsgk",
                  "author": "crorella",
                  "text": "no idea, but it is not unusual, Netflix was at 4.5 exabytes last year.",
                  "score": 6,
                  "created_utc": "2026-02-17 01:11:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5paln2",
              "author": "kaapapaa",
              "text": "Amazing.",
              "score": 2,
              "created_utc": "2026-02-16 16:02:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5rjkf2",
              "author": "Dark_Force",
              "text": "That's awesome",
              "score": 2,
              "created_utc": "2026-02-16 22:27:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pamn4",
          "author": "Lanky-Fun-2795",
          "text": "Ppl donâ€™t judge data warehouse sizes anymore. Anyone who asks that is trying to hear keywords like partitioning/indexing for optimization. Logging/snapshots can easily double or triple your typical warehouse unless you are dealing with webforms",
          "score": 11,
          "created_utc": "2026-02-16 16:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pjxg6",
              "author": "kaapapaa",
              "text": "I understand. Yet wanted to check how much data being processed in reality.",
              "score": 5,
              "created_utc": "2026-02-16 16:45:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5psv1d",
                  "author": "Lanky-Fun-2795",
                  "text": "If they care that much just say petabytes. As long as you understand the repercussions of saying so.",
                  "score": 5,
                  "created_utc": "2026-02-16 17:26:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q9xqp",
              "author": "THBLD",
              "text": "You forgot sharding.",
              "score": 1,
              "created_utc": "2026-02-16 18:45:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rb382",
                  "author": "Lanky-Fun-2795",
                  "text": "Thatâ€™s a relatively archiac concept with modern data warehouses tbh. I have taken tens of interviews in the past few weeks and I never got a single question about it.",
                  "score": 5,
                  "created_utc": "2026-02-16 21:45:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5okltr",
          "author": "LelouchYagami_",
          "text": "Last year I worked on data which had 200 million records per day. \n\nThis year I worked on data which has 600+ million records per hour!! So what seemed like big data last year is now not so big. ~1TB per hour\n\nDomain is e-commerce data",
          "score": 16,
          "created_utc": "2026-02-16 13:50:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ol2wv",
              "author": "kaapapaa",
              "text": "Nice. My profile is being judged for the low volume metrics .",
              "score": 4,
              "created_utc": "2026-02-16 13:52:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5opajx",
              "author": "selfmotivator",
              "text": "Damn! What kind of data is this?",
              "score": 1,
              "created_utc": "2026-02-16 14:15:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5p2p9h",
                  "author": "LelouchYagami_",
                  "text": "It's transformed data from API call logs. These APIs mainly take care of what customers see on the e-commerce website.",
                  "score": 2,
                  "created_utc": "2026-02-16 15:24:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qogqw",
              "author": "billy_greenbeans",
              "text": "So, broadly, what is holding all of this data? How is it accessible?",
              "score": 1,
              "created_utc": "2026-02-16 19:54:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t2nk6",
                  "author": "LelouchYagami_",
                  "text": "It's stored on S3 data lake and is made accessible through glue catalog. Mostly people use EMR to query it given the size of the data",
                  "score": 2,
                  "created_utc": "2026-02-17 03:49:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ofra2",
          "author": "liprais",
          "text": "i am running 100 + flink jobs and writing 1b rows into iceberg tables every day,qps is 30K + now,works smooth,took me a while,but it is easy, trust me ,loading data is always the easiest work to do.",
          "score": 8,
          "created_utc": "2026-02-16 13:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oivxv",
              "author": "jupacaluba",
              "text": "I wonder how much a select * would cost",
              "score": 5,
              "created_utc": "2026-02-16 13:40:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5olpvs",
                  "author": "ThePizar",
                  "text": "Depends on a lot. A system that large probably wonâ€™t let you return everything. And nor would you want to. However returning an arbitrary set of say 10 rows should be cheap",
                  "score": 2,
                  "created_utc": "2026-02-16 13:56:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5q3rjh",
                  "author": "skatastic57",
                  "text": "Limit 1",
                  "score": 1,
                  "created_utc": "2026-02-16 18:17:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5oimpk",
              "author": "Glokta_FourTeeth",
              "text": "What's your domain/industry?",
              "score": 2,
              "created_utc": "2026-02-16 13:39:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ok2om",
              "author": "taker223",
              "text": "Are those stage tables with no indexes?",
              "score": 1,
              "created_utc": "2026-02-16 13:47:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5okyer",
          "author": "chmod-77",
          "text": "AT&T messed with our plans and several months of data came in off \\~800 machines all at once. Everything scaled and handled it well, but it was a lot for me. 200-300 million records? The size is debatable due to the way its packaged, but it might have been 100 gbs.\n\nI realize this is a drop in the bucket for some of you.",
          "score": 4,
          "created_utc": "2026-02-16 13:52:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oln6g",
              "author": "kaapapaa",
              "text": "Seems like a Heavy Lifter.\n\nFor me, The volume of data is not problem, but the quality is.",
              "score": 3,
              "created_utc": "2026-02-16 13:55:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pikwr",
          "author": "ihatebeinganonymous",
          "text": "50 terabytes per day.\n\n\nOne million Kafka messages per second.",
          "score": 5,
          "created_utc": "2026-02-16 16:39:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pj3gm",
              "author": "kaapapaa",
              "text": "Social Media/ ecommerce domain?",
              "score": 1,
              "created_utc": "2026-02-16 16:41:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5pj7dx",
                  "author": "ihatebeinganonymous",
                  "text": "No. Industry.",
                  "score": 2,
                  "created_utc": "2026-02-16 16:42:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pxb1f",
          "author": "bythenumbers10",
          "text": "Once worked for a cybersec outfit that recorded spam web traffic. Whatever pinged their sensors, good, garbage, hack, anything, it got recorded and catalogued. Quite a bit of data, just continuously rolling & getting stored, gradually getting phased into \"cold storage\" in compressed formats.",
          "score": 5,
          "created_utc": "2026-02-16 17:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q60z7",
          "author": "Beny1995",
          "text": "Working in a large e comm provider our clickstream data is around 7PB at time of writing. Believe its back to 2015 so I guess thats roughly 1.7TB per day? Presumably partitioned further though.",
          "score": 4,
          "created_utc": "2026-02-16 18:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t36ee",
          "author": "its4thecatlol",
          "text": "1TB an hour across 500mm records",
          "score": 3,
          "created_utc": "2026-02-17 03:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5okraw",
          "author": "Hagwart",
          "text": "Same amounts ... 25 GB per bi monthly cycle added.",
          "score": 2,
          "created_utc": "2026-02-16 13:51:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pooo9",
          "author": "speedisntfree",
          "text": "Peter North's",
          "score": 1,
          "created_utc": "2026-02-16 17:07:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7bk2u",
      "title": "Is the Data Engineering market actually good right now?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r7bk2u/is_the_data_engineering_market_actually_good/",
      "author": "Tricky_Tart_8217",
      "created_utc": "2026-02-17 16:53:58",
      "score": 60,
      "num_comments": 46,
      "upvote_ratio": 0.87,
      "text": "I am just speaking from the perspective of a data engineer in the US, with 4 years of experience. I've noticed a lot of outreach for new data engineer positions in 2026, like 2-3 linkedin messages or emails per week. And I have not even set my profile as \"Open To Work\" or anything.\n\nHas anyone else noticed this? Past threads on this subreddit say that the market is terrible but it seems to be changing.\n\nThis is my skillset for reference, not sure if this has something to do with it. Python, SQL, AI model implementation, Kafka, Spark, Databricks, Snowflake, Data Warehousing, Airflow, AWS, Kubernetes and some Azure. All production experience",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r7bk2u/is_the_data_engineering_market_actually_good/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5x37z0",
          "author": "JohnPaulDavyJones",
          "text": "Bunch of interest in the last few weeks from younger firms. I'm on the more junior end of the senior engineer world, so I think u/SoggyGrayDuck nailed it: the firms are looking for someone to come in and fix things without actually *paying* for a senior enough engineer who could do the job right with limited support.\n\nMost of these jobs look like they blow, though. $130k-$140k to lead a couple of very junior engineers/contractors, manage the MSP relationships, and handle all prod support as well? Pass.\n\nAlso got a couple recruiter pokes from the firms that are notable for the churn in their DE orgs, like CapOne, so that's also a hard pass.",
          "score": 18,
          "created_utc": "2026-02-17 19:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5y80bd",
              "author": "the_fresh_cucumber",
              "text": "I think you hit the nail on the head here.\n\nThere are *always* job offers for senior engineers that are willing to work for below market.",
              "score": 4,
              "created_utc": "2026-02-17 22:46:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5zup70",
              "author": "turboDividend",
              "text": "yes. this is whats out there 120k-130k is a experiened midlevel person. a senior should be at 170K+",
              "score": 3,
              "created_utc": "2026-02-18 04:11:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5yumlm",
              "author": "Even_Serve7918",
              "text": "I know people in data at cap one. Do they really have high churn? None of them have ever mentioned it.",
              "score": 1,
              "created_utc": "2026-02-18 00:50:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5z3o5q",
                  "author": "JohnPaulDavyJones",
                  "text": "Maybe itâ€™s local to their Dallas office, but theyâ€™ve got a serious reputation around here for burning out their DEs and having crazy churn.",
                  "score": 1,
                  "created_utc": "2026-02-18 01:38:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5x3p4j",
              "author": "Tricky_Tart_8217",
              "text": "That's mostly what I have experienced. They don't seem to offer much of a raise from what I'm currently making. Unless I wanna gamble my job security with C1 ðŸ˜‚",
              "score": 1,
              "created_utc": "2026-02-17 19:33:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wtn0k",
          "author": "SoggyGrayDuck",
          "text": "If your profile looks senior or you have the right experience you're a hit commodity. Seems every company is looking to bring in that top tier engineer to fix everything, migrate to the cloud, help the jr engineers. They realized they have the upper hand right now and hopefully just redesigning before bringing in more mid level/senior devs. \n\nAlthough looking at some of these job descriptions I don't think I'd even want the job. The list of requirements is insane. Or you specialize in a particular software",
          "score": 42,
          "created_utc": "2026-02-17 18:46:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yr5qu",
              "author": "Wojtkie",
              "text": "A lot Iâ€™ve seen fall into this bucket of â€œwe donâ€™t know how this works but we paid too much to a contract team 4 years ago when everyone was growing but now we need to hire someone to fix itâ€",
              "score": 15,
              "created_utc": "2026-02-18 00:31:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o61d5r6",
                  "author": "SoggyGrayDuck",
                  "text": "Yeah they lean heavily on SaaS tools (kickbacks) and it seems we need to learn this lesson every 10 years. I worry AI will make it work though",
                  "score": 1,
                  "created_utc": "2026-02-18 11:48:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5x00hl",
          "author": "maxbranor",
          "text": "I live in Norway, I recently started in my company, and I still get contacted by recruiters / ceos / ctos at least once per week via linkedin/email.  Got some tempting offers, even though I'm well employed.\n\nAt the same time, we were hiring a DE for my team and it was quite challenging to find senior-ish people.\n\nIt seems that the market for senior DE is really good here: high demand, few prospects.\n\nEDIT: Sorry for the potential confusion. We already hired someone, the position is not open anymore - but good luck to those on the hunt!",
          "score": 31,
          "created_utc": "2026-02-17 19:16:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xdr58",
              "author": "the_mg_",
              "text": "Do you still need :) ? What stack do you use ?",
              "score": -1,
              "created_utc": "2026-02-17 20:20:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60gon5",
                  "author": "maxbranor",
                  "text": "We hired someone. Thanks, though  \nand good luck!",
                  "score": 1,
                  "created_utc": "2026-02-18 06:59:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5xh65t",
              "author": "Kaiserx0",
              "text": "Iâ€™d love to join the European workforce , even at minimum wage Iâ€™d love to contribute , I can send you over my resume.",
              "score": -21,
              "created_utc": "2026-02-17 20:37:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wuhct",
          "author": "ppsaoda",
          "text": "I think yes, but seasonally. Its difficult for me to apply in 3rd and 4th quarter. But now I only applied to 4 jobs, got 3 interviews with verbal offers now. 30% jump.",
          "score": 41,
          "created_utc": "2026-02-17 18:50:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x53z2",
              "author": "EricMichaelHarris99",
              "text": "Where are you based? 75% success sounds insane where I live!",
              "score": 12,
              "created_utc": "2026-02-17 19:40:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5yjop8",
                  "author": "ckal09",
                  "text": "They are in Malaysia based on their comment history",
                  "score": 6,
                  "created_utc": "2026-02-17 23:50:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5yewrv",
              "author": "isuckatpiano",
              "text": "I looked at the linked in where I live in the Midwest and senior jobs were at 100k? Seems low for a senior developer. Are they higher in yours?",
              "score": 1,
              "created_utc": "2026-02-17 23:23:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5zukuf",
                  "author": "turboDividend",
                  "text": "there has been wage compression.",
                  "score": 1,
                  "created_utc": "2026-02-18 04:11:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5xsrc6",
              "author": "Live-Duck1369",
              "text": "I donâ€™t understand? How ? How many years of experience if you donâ€™t mind me asking?",
              "score": 0,
              "created_utc": "2026-02-17 21:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xt2qh",
                  "author": "Treemosher",
                  "text": "I think the industry you're trying to work in should be part of these conversations.  \n\nI don't know where you or the person you're replying to works, but probably worth bringing up.",
                  "score": 2,
                  "created_utc": "2026-02-17 21:33:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wz4cz",
          "author": "empireofadhd",
          "text": "I think itâ€™s normal, which is amazing in a ai software apocalypse.",
          "score": 6,
          "created_utc": "2026-02-17 19:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yks8o",
              "author": "EdwardMitchell",
              "text": "Iâ€™m actually wondering how many data engineers have moved to AI. That certainly leads to the demand for traditional engineers. The ai initiatives Iâ€™ve seen are not replacing people because the AI is harder to maintain than the automations that they were replacing",
              "score": 2,
              "created_utc": "2026-02-17 23:56:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x9pr5",
          "author": "WilhelmB12",
          "text": "For mid to seniors yes, for jr's not so much.",
          "score": 6,
          "created_utc": "2026-02-17 20:01:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xtmlo",
              "author": "techinpanko",
              "text": "This. Like many other disciplines, AI is allowing folks to do more with less, so the more senior end of the spectrum gets picked up and is expected to do both his domain and the domain of a junior for maybe 10-25% more of what their salary was pre-AI. \n\nI also see in the next five years most of the seniors getting snatched up, leaving nothing but juniors in the talent pool, forcing firms to pick up junior talent.",
              "score": 2,
              "created_utc": "2026-02-17 21:36:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xwk6q",
                  "author": "WilhelmB12",
                  "text": "Tbh, DE has never been an entry level career",
                  "score": 6,
                  "created_utc": "2026-02-17 21:49:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5x6ha5",
          "author": "SQLofFortune",
          "text": "If youâ€™re a senior and willing to work in an office, yes. Also if youâ€™re willing to do senior-level work for junior-level pay.",
          "score": 4,
          "created_utc": "2026-02-17 19:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xkt5q",
          "author": "adgjl12",
          "text": "Yes and no. I do get recruiters reaching out but none of them are better than my current job. I have 6YOE now.\n\nIf you hit all or most of the below I will gladly interview:\n\n- fully remote\n- more than 160k TC\n- 15 or more days of PTO (youâ€™d be surprised how some jobs checked all boxes but only gave 5-10 days pto)\n- no industries like crypto, gambling, etc\n\nI have not had a single recruiter bring me a job with all of them so I stay put. So the market is okay as in I could find a job but hard to find an upgrade than what I have already",
          "score": 6,
          "created_utc": "2026-02-17 20:54:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5y6lne",
              "author": "Admirable_Bed7398",
              "text": "Everytime I see PTO days in US, I feel super lucky to live in the Europe with my 32 days of vacation and 14 days for any medical appointment. ",
              "score": 8,
              "created_utc": "2026-02-17 22:39:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x4wat",
          "author": "Amar_K1",
          "text": "People realising data engineer is required for AI and it is also taking over to what level reporting was 5 years back.",
          "score": 7,
          "created_utc": "2026-02-17 19:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61x799",
              "author": "ludflu",
              "text": "this is my read of the situation too! AI without ground truth data is just pure hallucination, and the clued-in business people do seem to understand this.",
              "score": 1,
              "created_utc": "2026-02-18 13:53:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wx2of",
          "author": "ianitic",
          "text": "Yup, I've been noticing the same. I've also been getting popular profile alerts even though I haven't made an update in like 3 years.",
          "score": 2,
          "created_utc": "2026-02-17 19:02:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x11wh",
          "author": "LoudSphinx517",
          "text": "What skills do you have on your profile to get reached out to ?\n\n",
          "score": 2,
          "created_utc": "2026-02-17 19:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x42q0",
              "author": "Tricky_Tart_8217",
              "text": "Python, SQL, AI model implementation, Data Warehousing, Kafka, Spark, Databricks, Snowflake, Airflow, AWS, Kubernetes and some Azure. All production experience",
              "score": 6,
              "created_utc": "2026-02-17 19:35:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60z7ql",
                  "author": "Illustrious_Role_304",
                  "text": "what exactly in AI model implementation",
                  "score": 1,
                  "created_utc": "2026-02-18 09:50:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xang2",
          "author": "DudeYourBedsaCar",
          "text": "Senior DE here and yes, anecdotally, I've noticed an uptick in recruiter messages for both hybrid and remote in the last 6 months.\n\nI'm still getting interest even though the market seems to be shit for everything else, and I'm not particularly visible on job platforms or blogs or anything like that. Seems to be purely organic.",
          "score": 2,
          "created_utc": "2026-02-17 20:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xdoh3",
          "author": "goblueioe42",
          "text": "Itâ€™s better than other fields within engineering, but still softer than a few years ago!",
          "score": 2,
          "created_utc": "2026-02-17 20:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ykz6g",
          "author": "typodewww",
          "text": "Idk Iâ€™m a fresh grad Data Engineer with like 2 month experience at my job. I just got hit up today for a Data Engineer position in person in Indiana which I wonâ€™t take.",
          "score": 2,
          "created_utc": "2026-02-17 23:57:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yoibq",
          "author": "Scared_Student7099",
          "text": "Unfortunately my latest roles have had limited software exposure, so my toolset isn't quite as wide despite my academic experience in a master's program with said tools (many companies seem to think such experience doesn't count since it isn't in production ðŸ˜®â€ðŸ’¨). Do you (or anyone else) have any tips on convincing my non-technical team to branch out into more modern data tools? We're currently mostly using bash, PostgreSQL and a drag and drop ETL tool. Because of my particular role, I'm also exposed to Databricks.",
          "score": 2,
          "created_utc": "2026-02-18 00:16:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xvwua",
          "author": "SgtSlice",
          "text": "Market is starting to pick up again.",
          "score": 2,
          "created_utc": "2026-02-17 21:46:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x7mwb",
          "author": "Slggyqo",
          "text": "4 YoE \n\nSeems pretty good to me. \n\nNot as good as 2-5 years ago, but thereâ€™s no shortage of recruiter outreach and the salaries are decent, if a bit lower across the board unless itâ€™s a role for an AI company.",
          "score": 1,
          "created_utc": "2026-02-17 19:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x8k88",
          "author": "marcelorojas56",
          "text": "How much $$$ are we talking about?",
          "score": 1,
          "created_utc": "2026-02-17 19:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xlp5x",
          "author": "starrorange",
          "text": "Which major city are you close to? I agree but Iâ€™m also getting them for nyc based",
          "score": 1,
          "created_utc": "2026-02-17 20:58:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y4s1o",
          "author": "Ok-Obligation-7998",
          "text": "No.",
          "score": 1,
          "created_utc": "2026-02-17 22:29:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zuhcx",
          "author": "turboDividend",
          "text": "i got laid off end of january. have 2 2nd round interviews already ;\\\n\n\n\nwas kinda hoping to take some time off but it looks like that might not happen",
          "score": 1,
          "created_utc": "2026-02-18 04:10:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61s9a0",
          "author": "NefariousnessSea5101",
          "text": "Man I just have 2.5 YoE, applying aggressively got great referrals, getting rejected because I miss the mark of 3+ YoE which most recruiters are looking for.",
          "score": 1,
          "created_utc": "2026-02-18 13:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61wt27",
          "author": "ludflu",
          "text": "I think AI has something to do with it. The hiring managers who have a clue realize that AI is in fact changing the business environment - but that AI doesn't work without data.",
          "score": 1,
          "created_utc": "2026-02-18 13:51:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2nrv3",
      "title": "Am I cooked?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/",
      "author": "Slik350",
      "created_utc": "2026-02-12 08:08:21",
      "score": 58,
      "num_comments": 38,
      "upvote_ratio": 0.78,
      "text": "Will keep this as short and sweet as possible.\n\nJoined current company as an intern gave it 1000% got offered full time under the title of:\n\nJunior Data Engineer.\n\nDespite this being my title the nature of the company allowed me work with basic ETL, dash boarding, SQL and Python. I also developed some internal streamit applications for teams to input information directly into the database using a user friendly UI.\n\nWhy am I potentially cooked?\n\nData stack consists of Snowflake, Tableau and and Snaplogic (a low code drag and drop etl tool). I realised early that this low code tool would hinder me in the future so I worked on using it as a place to experiment with metadata based ingestion and create fast solutions. \n\nNow that Iâ€™ve been placed on work for a year that is 80% non DE related aka SQL copying/report bug fixing Whilst initially Iâ€™d go above and beyond to build additional pipelines and solutions I feel as though Iâ€™ve burnt out.\n\nI asked to alter this work flow to something more aligned with my role this time last year. I was told Iâ€™d finally be moving onto data product development this year April (in effect Iâ€™ve been begging to just do what I should have been doing) and Iâ€™ve realised even if I begin this work in April Iâ€™m still at almost three years experience with the same salary I was offered when I went full time and no mention or promise of an increase.\n\nI know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesnâ€™t interest me whatsoever. At this rate my performance will continue to drop for lack of any incentive to continue besides collecting this current pay check.\n\nIâ€™ve had some interviews which are offering 20-25% more than my current role, interpersonally I succeed and am able to progress but in the technical sections I struggle without resources. Iâ€™d say Iâ€™m a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwersâ€¦\n\nHas anyone been in a similar position and what did you do to move on from it?\n\nTldr:\nAlmost at 3 years experience, level of experience technically lagging behind timeframe due to exposure at work being limited and lack of personal growth. Getting interviews but struggling with answering without resources.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4y4w9w",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-12 08:08:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y5h9v",
          "author": "Typical_Priority3319",
          "text": "Ridiculously far off from cooked. Create an itemized list of the things you donâ€™t know that you either\n a) have been asked in interviews already \n B) think u might get in future interviews based off of research\n\nStart looking at videos on YouTube to understand those concepts. Find excuses to learn those concepts at work whenever possible , but u might just have to do lil mini side projects to crystallize the concepts",
          "score": 134,
          "created_utc": "2026-02-12 08:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ymu1l",
              "author": "Slik350",
              "text": "Thank you for this; will do this asap. Iâ€™ve been doing some side projects but can definitely up the effort and make it along with practice more of my focus instead of my current day to day tasks.",
              "score": 9,
              "created_utc": "2026-02-12 11:02:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ztity",
                  "author": "SRMPDX",
                  "text": "Set up a personal GitHub. Work on side projects that are interesting and fill knowledge gaps. Document what you did, why you did it (be honest about upskilling), what issues you had in doing this the first time, maybe even \"what if do differently next time\", and make the repos public. Put a link on your resume. \n\nPotential hiring managers would love to see someone with initiative that can self learn and solve problems. Instead of answering random questions about syntax (15+ YoE and I still suck at syntax sometimes) they can talk to you about your code. Don't fall into the trap of letting a chat bot write all the code though.",
                  "score": 12,
                  "created_utc": "2026-02-12 15:27:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5oyvlq",
                  "author": "Reddit_User_654",
                  "text": "Have you looked at the job market in general but also in this specific field? You should feel really lucky with what you have as the situation \"out there\" is much worse. Don't be so harsh on yourself.ok...it could be better but also quite much worse so yea, thank the Lord for the stable situation you have.",
                  "score": 1,
                  "created_utc": "2026-02-16 15:05:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y5yzc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 47,
          "created_utc": "2026-02-12 08:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yn08p",
              "author": "Slik350",
              "text": "Cost ptimisation is something Iâ€™ve not had much time to look into sounds valuable and interesting will give it a look, thanks",
              "score": 4,
              "created_utc": "2026-02-12 11:03:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zzcqq",
                  "author": "randomuser1231234",
                  "text": "Re: cost optimizationâ€”look into predicate pushdown, and how this affects SQL query costs. Learn how to read query explains if you donâ€™t already know. Use that to make the queries youâ€™re copy/pasting BETTER.",
                  "score": 4,
                  "created_utc": "2026-02-12 15:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y83wn",
          "author": "Sensitive-Sugar-3894",
          "text": "DE is boring. After you suffer to make it all work, it becomes boring as it should.\n\nSnowflake, Databricks whatever, are just another thing in the jungle. The good jobs are over MySQL, very old Psql... In old Perl or Bash scripts with horrible embedded SQL. Your dream is to move to dbt and if you do, it will be boring again.\n\nData Engineering is not Systems Engineering. Want excitement, move out from DE.",
          "score": 30,
          "created_utc": "2026-02-12 08:40:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwzdc",
              "author": "xean333",
              "text": "I mean this earnestly: data modeling and building OLAP/OLTP systems has never gotten old for me after 11 yoe. Though I do agree - when done right, the only people that should find it thrilling are the freaks (myself included) that get off to the beautiful machinery of a well oiled platform. In other words, itâ€™s usually a sign youâ€™ve done something right if the build is boring to basically everyone who looks at it",
              "score": 16,
              "created_utc": "2026-02-12 15:44:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z146v",
              "author": "mcgrst",
              "text": "There is a reason interesting times is a curse!Â ",
              "score": 6,
              "created_utc": "2026-02-12 12:52:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59kapu",
              "author": "under_stroke",
              "text": "My experience is that the market is expading the role to Analytics Engineer, where some level of data intelligence is done whislt implementing most of the DE technical attributes. I assume the reason why Snowflake and low/no code is getting more popular is to remove technical barriers so data professionals can potentially spend more time shipping business value. In the last 7-8 years I saw a great increase in end-to-end more or so fullstack data professional. ",
              "score": 2,
              "created_utc": "2026-02-14 01:05:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ypj1a",
          "author": "domscatterbrain",
          "text": ">I'd say I'm a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwers...\n\nYou have a strong base, mate. \n\nDon't worry, Google is your friend. And now AI will get you the answers faster than reading the entire forum discussion. Well, as long as you ask the right questions.",
          "score": 7,
          "created_utc": "2026-02-12 11:25:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yxo8t",
          "author": "tasker2020",
          "text": "3 years in is a good time for your first job hop.  Youâ€™ll get a raise and broaden your experience.",
          "score": 4,
          "created_utc": "2026-02-12 12:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9tva",
          "author": "LeaveTheWorldBehind",
          "text": "Great DE advice in here. Speaking strictly from career perspective, it's typical to feel that boredom/itch around 2-3 years and it's always good to make your needs/expectations relatively clear. If you want to keep growing, don't wait 3 years to share that or 1 year. Keep at it consistently, talk with your manager or your manager+1 about the things you're interested in, ideas you have for other things.\n\nIf you want more, push for more. You're most useful when you're engaged and that'll often show. It doesn't always mean staying with the same company, but often times it does. I've made business cases while in low paid roles that turned into better work.",
          "score": 6,
          "created_utc": "2026-02-12 13:44:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zicj5",
          "author": "Apprehensive-Ad-80",
          "text": "3 years in, DEFINITELY not cooked. Hell even if this was 5 years in Iâ€™d say youâ€™re fine. If youâ€™re struggling on the technical evaluations during interviews make that a priority in your current jobâ€¦ instead of finding an online example or previous work to build from, do it from scratch and only use references when you fail",
          "score": 6,
          "created_utc": "2026-02-12 14:31:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z2i9k",
          "author": "WallyMetropolis",
          "text": "You joined as an intern. They know you need to learn. They offerered you the job because they believe you can.Â ",
          "score": 5,
          "created_utc": "2026-02-12 13:01:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zl8z8",
          "author": "Leather-Replacement7",
          "text": "Low code wrangling skills mean you will still have intuition. Learning syntax takes practice. Practice leetcodes, stratascratch. Get yourself an AWS account, or play with some tech via docker compose. I bet you know more than you think. Sadly imposter syndrome in data engineering doesnâ€™t ever go away but it gets better. I have 10 years experience, Iâ€™ve hardly used pyspark, because everywhere Iâ€™ve worked prefers an elt approach to transformation or the data simply isnâ€™t big enough. Thereâ€™s just so many ways to skin a cat in our field, one way isnâ€™t necessarily better than another.",
          "score": 3,
          "created_utc": "2026-02-12 14:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53w94w",
          "author": "DiscussionCritical77",
          "text": "'I know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesnâ€™t interest me whatsoever.'\n\nI'm 46 and that has been like 30% of my working career. Jobs naturally conclude when they are no longer useful to your career progression. What you do now, is you figure out what the next move to your career is, you train up for it with certs and side projects, you level up, and you change jobs and get a fat raise in the process.",
          "score": 3,
          "created_utc": "2026-02-13 04:14:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51hmjg",
          "author": "Specific-Sandwich627",
          "text": "You should take time for your rest. Rest is part of work. The fact that youâ€™re burning out is already a serious sign. You need to try to address this as soon as possible. Try to relieve yourself mentally and shift the focus of your body and mind to different activities for a few hours before sleep. You could change your diet, try new foodsâ€”maybe cooking or going for walks somewhere that feels closer to your soul. This is very important.",
          "score": 3,
          "created_utc": "2026-02-12 20:10:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53tjub",
          "author": "Icy_Clench",
          "text": "My advice to you is if you think your current company can be doing their data engineering better, lead the way forward. You will move up in no time if you can show value in doing things differently.\n\nWhen I was promoted to DE at my company (analyst for 3 months prior), they also used a drag and drop GUI hooked up to an OLTP database, and frankly the datasets sucked. They had no mechanism to retain historical data for SCDs so there was no historical accuracy. They had created like 6 surviving OBT tables after 3 years and they spent all of their time fixing constant bugs. The data analysts had created a shadow data warehouse that actually ran 95% of the reports.\n\nSo, in my first 6 months I learned about the company and processes and pushed for better practices: git, OLAP database (Snowflake), and proper modeling. I produced 4 datasets that were correctly modeled. I also optimized the daily runtime of the warehouse from 3 hours to 1 hour, and reduced a weekly pipeline from 10 hours to 1 minute, so I built a lot of cred as an expert.\n\nSecond year the company was ready and budgeted for a migration to Snowflake and git. The team was doing PRs and I was setting the standards for code reviews. We set up the platform with basic tooling to deploy, ingest, and transform data, including CDC and SCDs. I introduced some project management frameworks to my manager as well.\n\nThis year Iâ€™m hammering on modeling and tooling. I have shown them the value of conformed dimensions and I do the data modeling. I basically mandated no more unmodeled data and no more shadow warehouse moving forward. We also have big capability gaps between ingestion, transformation, automated testing (unit/data quality), orchestration, and more. Iâ€™ve put together a roadmap to address these for this year.",
          "score": 3,
          "created_utc": "2026-02-13 03:55:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54ceo3",
          "author": "Dense___",
          "text": "Iâ€™m in the same boat too, my tech stack is limited to SAP enterprise software that came out in 2008 that crashes every other time I use it... I am 2 years in at my first job in DE and work with so many legacy tools but I know Iâ€™m at least understanding the concepts. Currently spending all my spare time to learn more modern tools and technologies so I can get an easy 20-30% bump at the next place lol",
          "score": 3,
          "created_utc": "2026-02-13 06:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nmosu",
              "author": "The_g0d_f4ther",
              "text": "since you mentionned it, is SAP something useful for a DE ? For some reason i've been asked to learn it a bit for a project at my job, and at this point i fail to see how this helps my career too. Do you have any ideas on how to sell it in interviews if that's the case ?",
              "score": 2,
              "created_utc": "2026-02-16 09:25:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5otaih",
                  "author": "Dense___",
                  "text": "I think itâ€™s still kinda valuable experience due to it still being an ETL software, some companies are probably in the process (hopefully) of transferring to better platforms and might need help migrating. My manager told me to spend my free time learning SAP stuff for my role as itâ€™s a popular tool as well, but honestly I havenâ€™t put much time into it since I feel like learning newer stuff is a better use of my time. I also work in the public sector currently so that probably contributes to the older tech stack",
                  "score": 2,
                  "created_utc": "2026-02-16 14:36:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o561n9f",
          "author": "FantasticEquipment69",
          "text": "I believe what you're looking for is working for an IT solution/consultancy company (of course under the data team)\n\nBeing on the vendor side will give you many of what you're looking for like: \n1- being exposed to different problems, different technologies, different businesses (banking, telecom, health care,...) \n2- avoiding the part where \"whenever the job is done it become boring\", aka avoiding working in operation and waiting for something to fail just to find something to work on.",
          "score": 3,
          "created_utc": "2026-02-13 14:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ac546",
          "author": "ehulchdjhnceudcccbku",
          "text": ">I asked to alter this work flow to something more aligned with my role this time last year.Â \n\nThat's not the right way to approach this conversation. You need to tell them the business benefit of this change e.g. does your solution save company money (usually the best way to quantify impact), improve user experience etc.",
          "score": 3,
          "created_utc": "2026-02-14 04:07:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z14cv",
          "author": "OhNo171",
          "text": "I once thought that too, but no/low code tools wonâ€™t hinder you, specially in your early years. On the contrary, Id say it makes you focus on what really matters - how to better optimize your pipeline and think more about the end product instead of worrying about language/semantics. I wont say its not important to learn to code, but in the future, regardless if you use spark, pandas, scala, python, ruby, the core etl development skillsets are still there.",
          "score": 2,
          "created_utc": "2026-02-12 12:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58up94",
          "author": "Equivalent_Effect_93",
          "text": "Absolutely not, the hardest part is getting through the door. Learn on your own, build portfolio don't wait for your employer to train you on the toys you wanna play with. 5 years ago I started automating unversionned SAS files on a local Cron server, almost a laptop. Now I'm a senior data engineer playing with a massive databricks systems, elt, streaming and batch, debezium+kafka ingestion, mlops and model serving. Build stuff you wanna try, then when a good opportunity arise you have proof you can deliver in an enterprise setting (even low code tool), and a portfolio with a few project, properly versionned, even maybe deployable through CI/CD. Keep up the good work.",
          "score": 2,
          "created_utc": "2026-02-13 22:34:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ddepf",
          "author": "Vardonius",
          "text": "Get deep into the dbt world and using AI to build models using semantic models to answer business questions.\n\nConsider connecting LLMs with your bi layer via MCP servers and APIs.",
          "score": 2,
          "created_utc": "2026-02-14 17:32:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5g8irk",
          "author": "superjisan",
          "text": "Nope, you're not cooked. I keep a Fail Log of all the technical interviews I've failed over the years in software engineering and data engineering over the years. It has over a 100 interviews I've failed at (that I remembered to update) in the last 12+ years.\n\nBuild on your failures because that will make your great.",
          "score": 2,
          "created_utc": "2026-02-15 03:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y6fnq",
          "author": "Thinker_Assignment",
          "text": "been there, moved jobs. different times\n\nMaybe you can start working on changing the situation by looking for data sources your saas does not provide and creating pipelies for those, run them on snowpark or gh actions if you have nothing else.  \nor consider if the saas is worth replacing with code\n\nalso why so much report bugfixing? look into dimensional modeling for self service, canonical models, maybe if you have better architecture you don't have so much ad hoc work. tableau is not great for self service, it operates under the paradigm that the analyst spoon feeds most things, its both too complex and too weak to be powerful for business user for self service. Most engineers see tableau as a \"busy tool\" and prefer things like metabase etc for this reason",
          "score": 2,
          "created_utc": "2026-02-12 08:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o510r5l",
          "author": "RelevantScience4271",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-02-12 18:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i27k2",
          "author": "Ill_Negotiation3078",
          "text": "Heyz. I have just got trained in Snowflake and informatica idmc.\nGot some tips nd tricks? Goona be deployed in project pretty soon have no idea what work am goona get tho. Not goona use idmc most likely",
          "score": 1,
          "created_utc": "2026-02-15 13:06:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iv4d7",
          "author": "Certain_Leader9946",
          "text": "When I was a junior engineer I was up to 4k commits a year for multiple years. Your goal is learning on the job and working hard until we can actually trust you enough to pay you properly. This post screams you aren't ready for that.",
          "score": 1,
          "created_utc": "2026-02-15 15:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9jd6",
          "author": "tsk93",
          "text": "Short answer: No\n\nI'm pretty much in a similar situation as u are. Just that i'm a data analyst hoping to move into DE one day. Current job feels kinda mundane and u are looking for smth else to grow. Personally I use certifications to build foundational knowledge and move to projects later on. Believe in yourself, u got this.",
          "score": 1,
          "created_utc": "2026-02-12 13:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zutox",
          "author": "starrorange",
          "text": "Your dumb if you think this is bad for a fresh grad",
          "score": 1,
          "created_utc": "2026-02-12 15:33:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5p40y",
      "title": "Started a new DE job and a little overwhelmed with the amount of networking knowledge it requires",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r5p40y/started_a_new_de_job_and_a_little_overwhelmed/",
      "author": "starrorange",
      "created_utc": "2026-02-15 20:29:15",
      "score": 51,
      "num_comments": 46,
      "upvote_ratio": 0.92,
      "text": "Maybe I was naive to think it was mainly pipelining on top of a platform like azure or databricks but Iâ€™m in the middle of figuring out how to ping and turn on servers etc. Iâ€™m going to read up on Linux and some other recommended textbooks but just overwhelmed I guess. I did math in undergrad and did cs for my masters so I opted out of the networking classes thinking I would never need it. ",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r5p40y/started_a_new_de_job_and_a_little_overwhelmed/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5l2r2z",
          "author": "masapadre",
          "text": "DE means a different thing in each company.\nI had to learn too a lot of networking for my current DE position. Vnets, peerings, private endpoints, dns resolution and so on but what my company calls data engineer to me is more like a software & platform engineer",
          "score": 45,
          "created_utc": "2026-02-15 22:27:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lnlf7",
              "author": "IAMHideoKojimaAMA",
              "text": "So what data are you actually engineering ðŸ˜‚",
              "score": 6,
              "created_utc": "2026-02-16 00:28:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5oharx",
                  "author": "masapadre",
                  "text": "I extract data from text logs and then I save to MongoDB. We can call that ETL and I did a little bit of data modeling (how am I saving this to mongo, how are we going to query this data, and so on). Nothing fancy.\nI look forward to start doing things with Databrick, have a proper lakehouse, work with tables, that kind of thing. We have that in the roadmap",
                  "score": 3,
                  "created_utc": "2026-02-16 13:31:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5os6n2",
              "author": "king_booker",
              "text": "Any good resources for this? \n\n",
              "score": 1,
              "created_utc": "2026-02-16 14:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pfbq7",
                  "author": "masapadre",
                  "text": "I followed â€˜Getting Started with Azure Networking Servicesâ€™ by Houssem Dellai. Recommended if you work with Azure.",
                  "score": 3,
                  "created_utc": "2026-02-16 16:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kgjl9",
          "author": "UhhSamuel",
          "text": "Any entry into DE right now is a good opportunity, but in my 7.5 years as a DE I've never pinged or turned on a server.",
          "score": 98,
          "created_utc": "2026-02-15 20:33:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l1aer",
              "author": "Mindless_Let1",
              "text": "You've never set up an ec2 instance or nothing? Wild",
              "score": 38,
              "created_utc": "2026-02-15 22:20:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ljphy",
                  "author": "Typical_Priority3319",
                  "text": "Just copy that one value with the command and paste it into the terminalâ€¦ no not that one the other one",
                  "score": 16,
                  "created_utc": "2026-02-16 00:05:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5oe300",
                  "author": "Inner-Block8845",
                  "text": "I think you can do that through Terraform or Cloud Functions so I am pretty sure all is programming and reading documentation and stakeholders requirements.",
                  "score": 2,
                  "created_utc": "2026-02-16 13:12:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5lt8ah",
              "author": "SalamanderPop",
              "text": "7.5 years and you've never done any infrastructure work? That's crazy in this day and age.",
              "score": 11,
              "created_utc": "2026-02-16 01:02:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5m0vor",
                  "author": "UhhSamuel",
                  "text": "I've done plenty of infra as code, if that helps. But networking is almost completely a black box to me. ",
                  "score": 7,
                  "created_utc": "2026-02-16 01:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5m6azb",
              "author": "No_Airline_8073",
              "text": "We own a Kubernetes cluster, minus the networking part",
              "score": 5,
              "created_utc": "2026-02-16 02:25:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l68rs",
          "author": "DenselyRanked",
          "text": "Are you working with a cloud provider? If so, then refer to their training modules. If not, then take this as an opportunity to create a run book for your team.",
          "score": 11,
          "created_utc": "2026-02-15 22:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kgnjo",
          "author": "xean333",
          "text": "Hmm networking is usually out of scope for a data engineer. Sounds like youâ€™ve got yourself a fuzzy role my friend. Make the best of it. You studied math - networking isnâ€™t harder than abstract algebra or complex analysis. Good luck",
          "score": 33,
          "created_utc": "2026-02-15 20:33:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mylgu",
              "author": "Fit_Highway5925",
              "text": "If you're a data engineer who's focused mainly on platform engineering & infrastructure, then I don't think it's out of scope. You have to really know how networks work. \n\nThe thing about DE is that it's so broad and has many flavors that what one DE is doing may be totally different from another. \n\nI've experienced this firsthand. In my previous DE role, I did mostly ETL development and I never dealt with networks. Now I'm working mostly in data infra & platform engineering, and networks are suddenly everywhere. Good thing I have somewhat of a background from my college degree so it came in handy.",
              "score": 5,
              "created_utc": "2026-02-16 05:46:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5kkuqt",
              "author": "VEMODMASKINEN",
              "text": "Err, networking is never out of scope for anyone working in IT.Â \n\n\nIt's basically one of the few things everyone should know at least the basics of because whatever you do will involve transmitting data over networks.Â ",
              "score": 46,
              "created_utc": "2026-02-15 20:55:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5njn0g",
                  "author": "sib_n",
                  "text": "You can say this about a lot of things in IT. IT professions have a core and then depending on companies and projects you may explore different things: streaming, ML, CICD, frontend, security etc. It's common to have an infrastructure team that abstract networking for you so you can focus on the data instead, so it's normal to not have experience with it as a DE.",
                  "score": 7,
                  "created_utc": "2026-02-16 08:56:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5kqpir",
                  "author": "xean333",
                  "text": "I agree we should all know the basics of networking. I suspect OP is being held to higher expectations than day 1 networking. Maybe Iâ€™m wrong!",
                  "score": 5,
                  "created_utc": "2026-02-15 21:25:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5knjir",
                  "author": "Plus-Willingness-324",
                  "text": "This.",
                  "score": 9,
                  "created_utc": "2026-02-15 21:09:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5n7xxa",
                  "author": "zangler",
                  "text": "This",
                  "score": 1,
                  "created_utc": "2026-02-16 07:06:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5l5q0m",
                  "author": "Dry_Philosophy7927",
                  "text": "I'm a data scientist doing research and contemplating a future move to.... somewhere else. I'm scared for exactly this reason. Where/how should i plug my knowledge? Just a starter for 10 pls",
                  "score": 1,
                  "created_utc": "2026-02-15 22:44:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5l7edv",
              "author": "greenestgreen",
              "text": "what? Is very much involved. How would you set up a spark cluster if it was needed on premise?\nCollecting data from system in different networks, crossing firewalls, etc.",
              "score": 5,
              "created_utc": "2026-02-15 22:53:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5lkjqt",
                  "author": "xean333",
                  "text": "Iâ€™ve always had admin/infra guys that are responsible for setting me up with what I need",
                  "score": 9,
                  "created_utc": "2026-02-16 00:10:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5lcwsd",
                  "author": "BobBarkerIsTheKey",
                  "text": "I've only worked with spark in AWS glue jobs. I've set up small spark clusters at home with maybe 3 machines before but it would be incredibly easy to not know anything about it  because it's been abstracted away for me. ",
                  "score": 2,
                  "created_utc": "2026-02-15 23:24:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kinvv",
              "author": "PhraatesIV",
              "text": "Some of the math guys I know would certainly disagree with your last sentence :)",
              "score": 1,
              "created_utc": "2026-02-15 20:44:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l98g7",
          "author": "peterxsyd",
          "text": "Time to get the books out",
          "score": 3,
          "created_utc": "2026-02-15 23:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5letzx",
          "author": "Only-Succotash-8829",
          "text": "Having worked across all clouds, I've found Databricks and Azure to be one with the most convoluted networking, LZ architecture and RBAC setup.Â \n\n\nThat said, what kind of servers are you maintaining?Â \n\n\nTry to speak with your IT department as they may already have ownership of this. Often times theres duplication of effort happening in orgs and there may already be processes in place, just not readily triggered by the teams needing them - and instead we end up doing it ourselves as we feel capable enough, even at our own detriment long term.",
          "score": 2,
          "created_utc": "2026-02-15 23:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oajry",
          "author": "Ok-Sentence-8542",
          "text": "Well you skipped networking thats on you. It all comes back to basics but its pretty easy to learn all about networking on the internet. Just do it.",
          "score": 2,
          "created_utc": "2026-02-16 12:48:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5krrtw",
          "author": "reditandfirgetit",
          "text": "Ping as in check if it's up or ping as in the network pinging an IP address?\n\nAlso what exactly do you mean turn on a server? Do you mean setup/standup a server?",
          "score": 3,
          "created_utc": "2026-02-15 21:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpxag",
          "author": "confusing-world",
          "text": "How many years of experience do you have?",
          "score": 2,
          "created_utc": "2026-02-15 21:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ks7j3",
          "author": "Mefsha5",
          "text": "Some networking is required at the DE level, enough to build and test connections and through-put, and knowing how to expose your systems to others.\n\nMostly, its about understanding the existing network design and working within them.",
          "score": 1,
          "created_utc": "2026-02-15 21:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n877t",
          "author": "zangler",
          "text": "Some people need to understand that NOTHING you do matters without network... fucking learn some basics...like out of pure intellectual curiosity",
          "score": 0,
          "created_utc": "2026-02-16 07:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oal34",
              "author": "mcgrst",
              "text": "Yeah... I work in a very large Corp, I have people who set that up. A DE in my company wouldn't be allowed anywhere near networking, we're given APIs or database locations all the underlying stuff is managed for us.Â ",
              "score": 3,
              "created_utc": "2026-02-16 12:49:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5od6sq",
                  "author": "zangler",
                  "text": "All the more reason to understand it. It will help",
                  "score": -1,
                  "created_utc": "2026-02-16 13:06:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r7l43c",
      "title": "just took my gcp data engineer exam and even though i studied for almost a year, I failed it.",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r7l43c/just_took_my_gcp_data_engineer_exam_and_even/",
      "author": "Historical_Donut6758",
      "created_utc": "2026-02-17 22:34:40",
      "score": 50,
      "num_comments": 20,
      "upvote_ratio": 0.94,
      "text": "I am familar with the gcp environment, studied practice exams and , read the books designing data intensive applications and the fundamentals of engineering and even have some projects. \n\n\nDespite that i still failed.\n\n\nI dont know what else to say.",
      "is_original_content": false,
      "link_flair_text": "Rant",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r7l43c/just_took_my_gcp_data_engineer_exam_and_even/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5y6wra",
          "author": "No-Satisfaction1395",
          "text": "If the exam motivated you to read DDIA and absorb its teachings, you already gained more value than the certification, which is more about specific GCP tooling, would ever have.",
          "score": 68,
          "created_utc": "2026-02-17 22:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5y795e",
              "author": "paxmlank",
              "text": "This is how I see it for 95% of the time; however, if they're trying to market themselves in freelance or whatever, having a GCP cert would likely help immensely compared to not having any.\n\nBut at least take solace in the fact that you read some very important literature, OP.",
              "score": 12,
              "created_utc": "2026-02-17 22:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yplln",
          "author": "cortrev",
          "text": "I would use GCP Study Hub if you haven't. That's what I used to pass and I couldn't recommend it enough",
          "score": 20,
          "created_utc": "2026-02-18 00:22:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zw1ql",
              "author": "legendarybyson",
              "text": "I second this.",
              "score": 3,
              "created_utc": "2026-02-18 04:20:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yqbh4",
          "author": "ironmagnesiumzinc",
          "text": "I took one of the gcp exams several years ago and failed. Since then, Iâ€™ve taken the aws solutions architect, databricks spark, databricks data engineer, and sec+ and passed them all. Havenâ€™t failed another one yet. Idk if gcp is just stupid hard or back then I didnâ€™t know how to study for these things. I developed a very specific way of studying now that works for me. Anyways just wanted to tell u that so u donâ€™t lose motivation",
          "score": 10,
          "created_utc": "2026-02-18 00:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zqt75",
              "author": "RustyEyeballs",
              "text": "Do you mind sharing your process for studying?",
              "score": 1,
              "created_utc": "2026-02-18 03:47:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5zzrkv",
                  "author": "ironmagnesiumzinc",
                  "text": "First I create a google doc with each topic and % of the exam it represents. Then, I go over several recent exam dumps, try to understand ever question/answer, and add any info I donâ€™t know to the correct section of my study guide. If thereâ€™s a specific topic I donâ€™t know much about, Iâ€™ll ask Claude more about it and really fill out that section of the guide. I reorganize every so often, reread it, and keep adding until thereâ€™s enough",
                  "score": 8,
                  "created_utc": "2026-02-18 04:46:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5yq2ku",
          "author": "AnimaLepton",
          "text": "There's not a year's worth of material to study for it. If you want to take it seriously, focus on a more structured 6 or 12 week approach",
          "score": 2,
          "created_utc": "2026-02-18 00:25:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y7r74",
          "author": "empireofadhd",
          "text": "Have you tried using an ai agent as a teacher? \n\nAlso sometimes you need 2-3 real world projects to really integrate the teachings.",
          "score": 5,
          "created_utc": "2026-02-17 22:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yicfg",
              "author": "Historical_Donut6758",
              "text": "i use chatgpt and i got most of the questions right...i guess they were easy questions.\n\n\ndid ai help you",
              "score": 1,
              "created_utc": "2026-02-17 23:42:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5y7vb3",
          "author": "i_hate_budget_tyres",
          "text": "Can you get on googles get certified program?  Getting guidance as a first timer is well worth while.  They give exam tips and tricks, constantly testing your knowledge and clarifying ideas as you learn with the exam in mind.  You really should use Googles training materials.  Videos, labs etc.\n\nAlso a year is too long, you end up forgetting stuff.  The get certified program pushes people to finish in 10 to 12 weeks.  I think that is optimal, given the scope of the qualifications, and its a short enough timeframe to retain information.  Give up evenings and social life for a while and keep pushing.",
          "score": 1,
          "created_utc": "2026-02-17 22:45:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yhp3h",
          "author": "EffectiveClient5080",
          "text": "Exams donâ€™t always reflect real skills. With DDIA, projects, and a year of prep, youâ€™re clearly capable. Just tweak your approach-retake and crush it.",
          "score": 1,
          "created_utc": "2026-02-17 23:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z0anb",
          "author": "DenselyRanked",
          "text": "Did you feel that the books were helpful for the test? Did you find the training on [skills.google](http://skills.google) useful?",
          "score": 1,
          "created_utc": "2026-02-18 01:21:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z8auy",
          "author": "Thisisinthebag",
          "text": "I failed dbt 3 times, feels like questions are made to trick you to fail. Every exam I have noted down weak points and re-read chapters, sometimes had to absorb whole article for 1 question that might or might not be given. Gemini has very good ui for test questions, and if you can find leaked questions it will help you to focus on weak points",
          "score": 1,
          "created_utc": "2026-02-18 02:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61q5wm",
          "author": "LimitedInfo",
          "text": "Thatâ€™s because the exam is about Google tooling not data engineering in general. You just need to re-study based on what the exam is actually about.",
          "score": 1,
          "created_utc": "2026-02-18 13:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61thf3",
              "author": "Cyphor-o",
              "text": "Thats it, its about being the best GCP Data Engineering Consulant on the best approach to maximise cloud spending. \n\nI'm not a fan of certs in general. They're all like this.",
              "score": 1,
              "created_utc": "2026-02-18 13:33:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o61swbo",
          "author": "Cyphor-o",
          "text": "Tbh most of the questions are BS. They're catch 22's of not what a Data Engineer would do but what would a GCP Data Engineering Consultant suggest to increase cloud spending. \n\nYou'll have great knowledge, use it to build a POC put it into a public repo and use it for a portfolio, shows better than a digital cert.",
          "score": 1,
          "created_utc": "2026-02-18 13:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62li4x",
          "author": "sdrawkcabineter",
          "text": "\"It is possible to commit no mistakes, and still lose.\"\n\n>Google Cloud Uniform Route Identification Value of the Present Instanced Commit Fabric?\n\n>\"Uh, 127.0.0.1?\"\n\n>\"Correct!\"\n\n>\"So it's localhost!\"\n\n>\"... No...\"",
          "score": 1,
          "created_utc": "2026-02-18 15:52:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3vgbd",
      "title": "How MinIO went from open source darling to cautionary tale",
      "subreddit": "dataengineering",
      "url": "https://news.reading.sh/2026/02/14/how-minio-went-from-open-source-darling-to-cautionary-tale/",
      "author": "jpcaparas",
      "created_utc": "2026-02-13 17:31:12",
      "score": 48,
      "num_comments": 4,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r3vgbd/how_minio_went_from_open_source_darling_to/",
      "domain": "news.reading.sh",
      "is_self": false,
      "comments": [
        {
          "id": "o58hbde",
          "author": "VEMODMASKINEN",
          "text": "Replaced it with Garage in my homelab. Was easy to setup in my K8s cluster and I haven't really had any issues one month in so far.\n\n\nhttps://garagehq.deuxfleurs.fr/",
          "score": 12,
          "created_utc": "2026-02-13 21:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59jas6",
              "author": "PencilBoy99",
              "text": "does garage let you create/setup s3 storage on an on premises box? that's what minio did think ",
              "score": 3,
              "created_utc": "2026-02-14 00:58:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5boktq",
              "author": "current_thread",
              "text": "I've been using Rook/Ceph for the last year and had no issues whatsoever.",
              "score": 1,
              "created_utc": "2026-02-14 11:24:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b4i20",
          "author": "every_other_freackle",
          "text": "We migrated to SeaweedFS and it is perfect!",
          "score": 6,
          "created_utc": "2026-02-14 08:08:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5w346",
      "title": "Spent last quarter evaluating enterprise ETL tools",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r5w346/spent_last_quarter_evaluating_enterprise_etl_tools/",
      "author": "Justin_3486",
      "created_utc": "2026-02-16 01:24:54",
      "score": 43,
      "num_comments": 31,
      "upvote_ratio": 0.92,
      "text": "Went through a formal evaluation process for data integration tools last quarter and thought I'd share since most comparisons online feel like marketing dressed up as content. For context, mid sized company, around 50 saas data sources, snowflake as primary destination though we're also testing databricks for some ml workflows and have legacy stuff in redshift we're migrating away from.\n\nFivetran connectors are solid and reliable but the cost at scale gets uncomfortable fast, especially once you're pulling significant volume. Airbyte was interesting because of the open source angle and we liked having control, but self hosting added a whole new category of things to maintain which defeated part of the purpose for a small team. Matillion felt more oriented toward transformation than data ingestion which wasn't quite our primary use case.\n\nPrecog had more reasonable pricing and less operational overhead, though their documentation could use work and the UI takes some getting used to if you're coming from fivetran's polish. \nEach has tradeoffs depending on your scale, team size, and needs. Happy to answer questions about specifics.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r5w346/spent_last_quarter_evaluating_enterprise_etl_tools/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5m308q",
          "author": "wytesmurf",
          "text": "Do you need realtime? Realtime is expensive. Anything that does realtime gets expensive. Airflow probably has most of the connectors youâ€™re looking for. You could do a micro ETL, that would be much less cost wise. Flink or Beam pipelines are really good if you need performance \n\nFor true realtime on a budget. Look at each solution and figure out an architecture. Are there hooks, queues, api limits, query costs, etc. Do a true realtime and a micro batch solution based on how often the data changes. You will never find a tool that does everything. You can spend 3 more months looking at tools or define tools for different use case and start chipping away at",
          "score": 14,
          "created_utc": "2026-02-16 02:04:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pvgv7",
              "author": "soundboyselecta",
              "text": "Every one needs realtime till they get their cloud bill. Takes management to sit down stakeholders, rub them on their head like a little child and say easy there buddy, you need to calm down. ",
              "score": 4,
              "created_utc": "2026-02-16 17:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pwlbm",
                  "author": "wytesmurf",
                  "text": "Your right there, they tried that, I told them to wait for the bill. Then they got a 60k confluent bill and they came back to the table. Ended up with a 5 minute micro batch in airflow. It costs dollars not tens of thousands",
                  "score": 3,
                  "created_utc": "2026-02-16 17:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5m42wp",
          "author": "HC-Klown",
          "text": "We have a small team of 2 data engineers. We self host airbyte and also airflow. I must say we handle it pretty well. Airbyte does not give any issue whatsoever wrt hosting. \n\nWe create connections declaratively using terraform and orchestrate them with airflow. We barely touch the UI, only for logs and maybe the occasional manual sync. The open source and number of different connectors are worth it. \n\nOnly con for airbyte is that in our opinion the normalization step from having the data in json blob to a RDBMS table takes unnecessarily long. It stupid. \n\nTherefore, we are pivoting our RDBMS sources to ingest them with Trino. Where we use dbt to write models that serve as ingestion with no transformation. So, Trino + dbt = ingestion. With dbt incremental models you can handle all sorts of ingestion patterns.  \n\nFor our other sources such as sftp, API, sharepoint etc., we keep using airbyte. Around 90% of our sources are RDMS though.\n\nAdditionally Trino can handle reverse etl easily to another database. Moreover, we can also write them as dbt models maintaining full lineage from ingestion all the way to Reverse ETL and other exposures.",
          "score": 10,
          "created_utc": "2026-02-16 02:11:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mte5c",
              "author": "iamspoilt",
              "text": "Since you mentioned self-hosting Airflow and Airbyte, I am wondering have you folks tried self-hosting Apache Spark clusters for distributed computing as well? How has that experience been?",
              "score": 1,
              "created_utc": "2026-02-16 05:05:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rypo6",
                  "author": "HC-Klown",
                  "text": "We haven't tried that. We have an on-prem k8s cluster and we host a multi node Trino setup there. We still haven't migrated to it so it's not operating with prod data/pipelines",
                  "score": 1,
                  "created_utc": "2026-02-16 23:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5o55ji",
              "author": "Voxnihil",
              "text": "I used Meltano and noticed the same issue you did with Airbyte, incredibly slow due to the intermediate conversions to and from jsonl.\n\nAnd that was a side project with low data volume, I can't imagine it at scale.",
              "score": 1,
              "created_utc": "2026-02-16 12:09:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ma5nd",
          "author": "GreyHairedDWGuy",
          "text": "There are many roads to Rome.  What works for you may not work for others.  What are you spending on Fivetran today?  I disagree with your assessment of Matillion.  It can certainly ingest data from SaaS solutions as well as on-prem.  It's not as nice/easy to use for ingestion as Fivetran but it does work.   Fivetran can get expensive (high MAR usage) when you have cases of large datasets (SaaS or on-prem) that need to get ingested each month but also have low number of updates (meaning you cannot save MAR if rows change may times per month). \n\nI had never heard of 'Precog' before but it seems to only have been around since 2020.  Not sure I would want to partner with a company this new.\n\nHope it works out for you.",
          "score": 3,
          "created_utc": "2026-02-16 02:50:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mli8f",
          "author": "geoheil",
          "text": "For connectors consider dlt - [https://dlthub.com/](https://dlthub.com/)  see here for a more fully fledged example [https://github.com/l-mds/local-data-stack/](https://github.com/l-mds/local-data-stack/) and docs dedicated on the integration dlt + dagster here [https://docs.dagster.io/integrations/libraries/embedded-elt](https://docs.dagster.io/integrations/libraries/embedded-elt)\n\n  \nthis is a bit more involved - gives you much more power and flexibility though - and better pricing via dlt (if you want oss)",
          "score": 6,
          "created_utc": "2026-02-16 04:07:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m1uuv",
          "author": "MgmtmgM",
          "text": "Did you consider snowflakeâ€™s openflow?",
          "score": 2,
          "created_utc": "2026-02-16 01:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lyx6l",
          "author": "pungaaisme",
          "text": "1. Is it possible to list the datasources by priority or volume? In case of databased to distinguish if these are log based datasource reader or using simple queries?  \n2. do you have reverse ETL use case from snowflake back to our operational systems?  \n3. What do you use for transformation/modeeling (dbt?) is it on Prem or dbt cloud or using snowflakes dbt capabilities ?",
          "score": 1,
          "created_utc": "2026-02-16 01:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m4z9p",
          "author": "Bstylee",
          "text": "Which one can handle huge volume with tons of thrash?",
          "score": 1,
          "created_utc": "2026-02-16 02:17:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o3xxw",
          "author": "Traditional_Gap4163",
          "text": "Commenting to follow",
          "score": 1,
          "created_utc": "2026-02-16 12:00:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v0ouv",
          "author": "nocomm_07",
          "text": "\nThis is current day ELT tax. Hahah. Pay Fivetran for convenience or pay Airbyte with engineering hours. At 50 SaaS sources on a small team, self hosting anything that needs Terraform, Airflow and K8s is never truly free â€œfree.â€ And once Snowflake volume grows, row based pricing gets HUGEE. If you want a middle path, look at Integrate etl or Estuary. Fully managed but without pure volume based pricing. For most mid sized orgs a micro batch ELT approach into Snowflake every 5 to 15 minutes is enough. Real time streaming is going to cost way more than you would like.",
          "score": 1,
          "created_utc": "2026-02-17 13:21:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wonu9",
          "author": "eb0373284",
          "text": "Do some research about NiFi with DFM.",
          "score": 1,
          "created_utc": "2026-02-17 18:23:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yzqkq",
          "author": "Hot_Map_7868",
          "text": "OSS is not free when you consider the platform maintenance as you rightly said. Thereâ€™s always a trade off. What did you end up selecting?",
          "score": 1,
          "created_utc": "2026-02-18 01:18:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m0aes",
          "author": "Leading-Inspector544",
          "text": "You might consider dbx lakeflow connectors as well. Generally cheaper at scale than fivetran, as it's pay for compute rather than pay for volume of data.",
          "score": 1,
          "created_utc": "2026-02-16 01:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o8mt0",
          "author": "Technical_Depth2382",
          "text": "Hevo",
          "score": 0,
          "created_utc": "2026-02-16 12:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ucldn",
          "author": "jonas-weld",
          "text": "You might want to take a look at **Weld,** we typically see teams in your exact situation switch when Fivetran costs start climbing.\n\nWe offer significantly better pricing while keeping connectors stable and fully managed, so thereâ€™s essentially no maintenance needed on the ingestion side. After a short trial you can clearly estimate what your bill would look like based on the data youâ€™re syncing, which makes planning a lot easier.\n\nTransformation, orchestration, and reverse ETL are built into the platform as well, but getting data in is intentionally very simple. Feel free to reach out if you ever decide to test it.",
          "score": 0,
          "created_utc": "2026-02-17 10:18:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o7kl2",
          "author": "Nekobul",
          "text": "Have you consider using SSIS for your needs? The platform itself is powerful enterprise-level and there is a broad third-party extensions ecosystem with more than 300 connectors available.",
          "score": -3,
          "created_utc": "2026-02-16 12:27:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p7e22",
              "author": "Pancakeman123000",
              "text": "Fyi for anyone reading, this guy freaking loves SSIS - just look at his comment history. Whenever I see his name crop up here, I think- 'its the SSIS guy!' ðŸ˜…",
              "score": 4,
              "created_utc": "2026-02-16 15:47:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q7y7s",
                  "author": "Nekobul",
                  "text": "Thank you for the positive vibes!",
                  "score": 0,
                  "created_utc": "2026-02-16 18:36:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5om3cx",
              "author": "reddit_time_waster",
              "text": "I personally agree SSIS is still good under the following conditions:\n1) You already have SQL Server for other reasons, so SSIS is \"free\"\n2) You follow the CI/CD devops practice with the catalog and something like Azure Devops\n3) Your scaling needs are limited. Most companies actually fit in this category and just need etl between some systems or exports of less than 1m rows.\n3a) You have scaling needs, and you have a team with a good Azure practice able to run SSIS packages in Azure Data Factory.",
              "score": 2,
              "created_utc": "2026-02-16 13:58:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q7ttn",
                  "author": "Nekobul",
                  "text": "You can process more than 1m rows with SSIS. You can process hundreds of millions.",
                  "score": 3,
                  "created_utc": "2026-02-16 18:35:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4kugz",
      "title": "â€œWhat are the best resources to learn Docker from scratch?â€",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r4kugz/what_are_the_best_resources_to_learn_docker_from/",
      "author": "Effective_Bluebird19",
      "created_utc": "2026-02-14 13:38:31",
      "score": 42,
      "num_comments": 20,
      "upvote_ratio": 0.94,
      "text": "Iâ€™m a Data Engineer with around 2 years of experience and Iâ€™m trying to properly learn Docker so I can use it in real-world data pipelines.",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r4kugz/what_are_the_best_resources_to_learn_docker_from/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5c5ttz",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-14 13:38:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cfnfb",
          "author": "ThroughTheWire",
          "text": "learning docker from scratch is pretty pointless imo. learn what you need to know as you have to deal with it. in practice for most data engineers this would be running your own airflow instance locally or perhaps running a tool like Airbyte locally. both situations have tutorials that you can follow along that will essentially teach you how docker is working with a practical use case in mind. I wouldn't waste time learning how docker otherwise works.",
          "score": 31,
          "created_utc": "2026-02-14 14:37:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e5xds",
              "author": "VEMODMASKINEN",
              "text": "It's hardly a waste knowing how container internals work with namespaces and cgroups along with security and modern building best practices... We're Data *Engineers* after all.Â \n\n\nAnyone can follow a tutorial that spells out how to set something up, it won't teach you much about the tool itself though...\nÂ \n\nAnyways, here OP:\n\n\nhttps://courses.mooc.fi/org/uh-cs/courses/devops-with-docker\n\n\nAnd:\n\n\nhttps://m.youtube.com/watch?v=Utf-A4rODH8",
              "score": 12,
              "created_utc": "2026-02-14 19:57:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dqi71",
          "author": "Cloudskipper92",
          "text": "Since you (and a couple others) seem to be more or less asking for some structure to learn against, here's what I use or have used in my day-to-day.\n\n- How to install docker, and the follow-ups needed, on your Distro/OS. Windows/Mac are pretty straight-forward. Linux has some steps after install that you need to do.\n\n- Get used to navigating dockerhub, finding official image builds, and how to pull specific versions. Much like Python version pinning, you _certainly_ want to pin versions of infra.\n\n- Read the docs on the most important docker cli commands. Non-exhaustive: `docker build`, `docker run`, `docker pull`, `docker exec`, `docker container cp`.\n\n- Learn and practice making [Dockerfiles](https://docs.docker.com/reference/dockerfile/). Learn the subtle differences between `ADD` and `COPY`. How Layering works. Learn the differences between `CMD` and `ENTRYPOINT`, `ARG` and `ENV`. Learn how to expose a port on a container to the host. HINT: it isn't with the `EXPOSE` instruction and if you made it this far without being able to ping your container from your host you should go back one step ;) . Make a `.dockerignore` so you don't put anything you don't want in the container. You can ignore these instructions for now: `HEALTHCHECK`, `LABEL`, `MAINTAINER`, `ONBUILD`, `SHELL`, `STOPSIGNAL`.\n\n- Learn how networking works for Docker. Networking generally is a weak point for most SWEs, and seems to often be doubly so for DEs. In the same vein, read the docs on how Docker Volumes work and how to attach them.\n\nNow, you came to the DE Subreddit to ask this and mention you have 2 YOE already, so I'm going to also mention a couple of more specific things.\n\n1. [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html) is D E N S E but obfuscated heavily. As in, it has a lot of levers and knobs, but it mostly assumes the defaults are good enough for this. It also assumes you know docker-compose which I _did_ leave out of the top. The justification I'll give for that is that Docker Compose is great... but you should be using the dockerized airflow mentioned here as a TESTING system ONLY. Thus, get it going following thier instructions, do what you need to do, but don't assume it matches production-grade Airflow deployments.\n\n2. You could (and perhaps should) use [Astronomer's CLI](https://www.astronomer.io/docs/astro/cli/overview). I don't work for them or anything, but I have used their managed service in the past. The fact that the CLI exists for free is great and should be taken advantage of for local testing.\n\n3. Now that you've seen those two and understand how docker works and have played with the ins and outs, contrary to what others may say, I would then _AND ONLY THEN_ look into Kubernetes. No matter the system, managed or self-hosted, Airflow and it's pipelines ALWAYS run on Kubernetes behind the scenes. The way you build the image for [Airflow will change](https://gist.github.com/wesh92/4ce0634e61949a1679e2ae5a1788cb18) and, thus, how you manage dependancies and the way you need to understand how Kubernetes sees Containers versus how you've seen them thus far at this point. I cannot stress this enough though, DO NOT jump straight to this point. Everything above here should be _weeks_ of testing, toiling, and troubleshooting at a minimum before you try to introduce K8s. When you do, start with a local manager like `k3s`. I would recommend not using `minikube` or `kind` as those are \"k8s in docker\" which is a whole extra layer you don't need. The justification I'll give for including this: I like my local testing env for pipelines to be as close, if not exact, to what I will deploy. For me this means _in kubernetes using exactly what I will deploy_ with as much of the kubernetes weirdness as I can account for. If this doesn't feel important to you, please feel free to ignore!\n\nHope this helps! But please, just start with the Docker basics. You can search up a youtube if your more visually-inclined. Read through the docs and try implementing things if you're more of the experiential kind. Nothing is going to be a cheat code because these are kind of foundational tools for SWE and DE.\n\nEDIT: formatting. reddit pls",
          "score": 5,
          "created_utc": "2026-02-14 18:37:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f7qct",
              "author": "wetnmoist",
              "text": "I learned how to use it for a home server / networking. \n\nImo youâ€™re on point - the only thing Iâ€™d add is finding a project where itâ€™s actually necessary to use it rather than playing around with everything it can do.",
              "score": 2,
              "created_utc": "2026-02-14 23:29:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5c77q2",
          "author": "Outside-Storage-1523",
          "text": "I think finding a use case in your personal project is useful.",
          "score": 12,
          "created_utc": "2026-02-14 13:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cj7g0",
          "author": "gn-musa",
          "text": "Ship one thing,not tutorials.",
          "score": 3,
          "created_utc": "2026-02-14 14:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5caa8q",
          "author": "Prestigious_Radio582",
          "text": "+1 I also want to learn docker from a structured resource.",
          "score": 3,
          "created_utc": "2026-02-14 14:05:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5choaf",
          "author": "xean333",
          "text": "What do you mean learn it from scratch? Are you just saying you donâ€™t currently know anything about docker? Start with a super high level overview of the problem(s) docker solves. Then run airflow locally with it. Start now and youâ€™ll be done by the afternoon. If you want to learn it deeper, this is probably the wrong sub to ask",
          "score": 2,
          "created_utc": "2026-02-14 14:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d25t0",
          "author": "meiousei2",
          "text": "All you need to understand is how containerized networking works and basic syntax to be able to audit AI generated Dockerfiles, they're not that complex",
          "score": 1,
          "created_utc": "2026-02-14 16:36:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dti1w",
          "author": "ludflu",
          "text": "If you want to make it specific to data engineering: \n\n1. run airflow using `docker compose` to get a sense for docker. (https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)\n2. Then create a DAG that does its work by running a docker image using the DockerOperator\n3. In production, its very similar, but you'll likely run your DAG task in ECS on AWS or Cloud Run on GCP",
          "score": 1,
          "created_utc": "2026-02-14 18:52:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5duvjt",
          "author": "dumb_user_404",
          "text": "there is a course on docker by javascript mastery, its a pretty new course and will teach you about docker from basics up. \n\nWatch half of that video and then move on to video teaching you about running your own airflow or spark or what ever you need. \n\nwith the foundational knowledge from the first video, everything will fall in place. \n\nyoutube : [https://youtu.be/GFgJkfScVNU?si=8jdbZsjlI4axyuzg](https://youtu.be/GFgJkfScVNU?si=8jdbZsjlI4axyuzg)\n\n",
          "score": 1,
          "created_utc": "2026-02-14 18:59:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ekxzk",
          "author": "JBalloonist",
          "text": "I'm just good enough to be dangerous with Docker, and frankly that was all I needed. I was using it to create containers running in AWS ECS (Elastic Container Service) and Lambda. If I ran into trouble enough searching or AI questions would usually give me the answer. Occasionally I'd reach out to a coworker who had more experience with builds (but not much Python experience). ",
          "score": 1,
          "created_utc": "2026-02-14 21:19:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ey0va",
          "author": "Dawido090",
          "text": "Book \"Learn Docker in a Month of Lunches\" it's great as whole series, its bit outdated but it's great.",
          "score": 1,
          "created_utc": "2026-02-14 22:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gqrk4",
          "author": "tbot888",
          "text": "YouTube and A.I. when you need to solve a problem.\n\nReview it.\n\nAsk someone else to have a look at it too.\n\nHow Iâ€™m learning most of my stuff now.\n\nAnd the blog /youtube/stackechange etc back to the world about it.\n\nI mean thatâ€™s what computer science is all about. Â Helping a brother out working on each others stuff.\n\nThen hiring each other.",
          "score": 1,
          "created_utc": "2026-02-15 05:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cvd92",
          "author": "DoomsdayMcDoom",
          "text": "Better off learning K8/Kubernettes since thatâ€™s whatâ€™s replacing docker in every company with a hybrid or cloud infrastructure.",
          "score": -3,
          "created_utc": "2026-02-14 16:02:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dao2a",
              "author": "Black_Magic100",
              "text": "Kubernetes does not replace Docker. \n\nRecommending somebody jump straight to orchestration when they want to learn containerization is also a crazy idea.",
              "score": 8,
              "created_utc": "2026-02-14 17:19:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eesh4",
                  "author": "DoomsdayMcDoom",
                  "text": "Docker never kept up and unfortunately thatâ€™s their fault.  It could of been a better platform but technology advanced and they stayed stagnant.\n\ncontainerd is what Docker uses under the hood to manage containers. Kubernetes was like, â€œwhy not just talk to that directly?â€",
                  "score": 0,
                  "created_utc": "2026-02-14 20:45:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r383ef",
      "title": "Is Microsoft OneLake the new lock-in?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r383ef/is_microsoft_onelake_the_new_lockin/",
      "author": "AwayCommercial4639",
      "created_utc": "2026-02-12 22:49:03",
      "score": 42,
      "num_comments": 14,
      "upvote_ratio": 0.98,
      "text": "I was running some tests on OneLake the other day and I noticed that its performance is 20-30% worse than ADLS. \n\nThey have these 2 weird APIs under the hood: Redirect and Proxy. Redirect is only available to Fabric engines and likely is some internal library for translating OneLake paths to ADLS paths. Proxy is for everything else (including 3rd party engines) and is probably just as it sounds some additional compute layer to hide direct access to ADLS.\n\nI also think that there may be some caching on Fabric side which is only working for Fabric engines...\n\nMy scenario - run a query from Snowflake or Spark k8s against an Iceberg table on ADLS and on OneLake. The performance is not the same! OneLake is always worse especially for tables with lots of files...\n\nSo here is my fear - OneLake is not ADLS. It is NOT operating as open storage. It is operating as a premium storage for Fabric and a sub optimal storage for everything else...\n\nJust use ADLS then.. Yes, we do. But every time I chat with our Microsoft reps they are pushing and pushing me to use OneLake. I am concerned that one day they will just deprecate ADLS in favour of OneLake.\n\nLook Fabric might be decent if you love Power BI, but our business runs on 2 clouds. We have transactional workloads on both, and no way are we going to egress all that data to one cloud or another for analytics. Hence we primarily run an open stack and some multi cloud software like Snowflake.\n\nWhat is wrong with ADLS? Why. do they keep pushing to OneLake? Is this is the next lock-in?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r383ef/is_microsoft_onelake_the_new_lockin/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o54w8tm",
          "author": "Tribaal",
          "text": "Yes, itâ€™s the next lock in. Thatâ€™s it.",
          "score": 36,
          "created_utc": "2026-02-13 09:16:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56693j",
              "author": "Brilliant-Gur9384",
              "text": ">Just use ADLS then.. Yes, we do. But every time I chat with our Microsoft reps they are pushing and pushing me to use OneLake. I am concerned that one day they will just deprecate ADLS in favour of OneLake.\n\nYes, this mirrored the OneDrive fiasco. Remember how it would re-install it every time you deleted it? I guess they finally stopped pushing it because it stayed off, but for a good 2 years, it was a battle to keep OneDrive off an ms machine.\n\n>Yes, itâ€™s the next lock in. Thatâ€™s it.\n\nBingo. If you know AI, then you know these big tech companies want control of data. That's the real value and they know it. The big winners are going to be people/companies who \"get\" this early and choose the right platforms because if you get stuck on a platform that changes agreements after the fact, that could be a major ouch.",
              "score": 4,
              "created_utc": "2026-02-13 14:43:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55onta",
          "author": "TowerOutrageous5939",
          "text": "Iâ€™m hope they push us. My CIO is already very much over MS I would have for this to be the tipping point",
          "score": 5,
          "created_utc": "2026-02-13 13:06:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57m5em",
          "author": "m1nkeh",
          "text": "Yes, itâ€™s massive lock in.. itâ€™s ADLS with shackles",
          "score": 3,
          "created_utc": "2026-02-13 18:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55y9yx",
          "author": "thecoller",
          "text": "Not long ago, reading OneLake from non-Fabric computer was 3x more expensive than reading from Fabric. Hopefully customers keep up the pressure and keep choosing ADLS so the rest of the barriers come down.",
          "score": 2,
          "created_utc": "2026-02-13 14:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57m9pc",
              "author": "m1nkeh",
              "text": "Oh, is this no longer the case, the 3x thing?",
              "score": 2,
              "created_utc": "2026-02-13 18:53:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57u3qw",
                  "author": "thecoller",
                  "text": "Fortunately not, as of a couple of months ago",
                  "score": 2,
                  "created_utc": "2026-02-13 19:31:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o563he7",
          "author": "TheRealStepBot",
          "text": "Fabric and one lake is crap. ðŸ“Ž",
          "score": 4,
          "created_utc": "2026-02-13 14:28:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57g6dz",
          "author": "Sea-Meringue4956",
          "text": "Onelake is on top of ADLS and yet costs 10x more the last time I checked. I dont think though that ADLS will stop to exist.",
          "score": 2,
          "created_utc": "2026-02-13 18:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56ues6",
          "author": "Low_Second9833",
          "text": "Trust your instincts.",
          "score": 1,
          "created_utc": "2026-02-13 16:39:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57to2a",
          "author": "Frosty-Practice-5416",
          "text": "So happy I don't have to think about that crap anymore",
          "score": 1,
          "created_utc": "2026-02-13 19:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57fqgh",
          "author": "thpeps",
          "text": "Hey - I'm a PM with OneLake, the performance you're experiencing is not expected, can you send me a DM so we can get in touch and I can help debug what you're seeing. Thanks!",
          "score": -2,
          "created_utc": "2026-02-13 18:22:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57mclg",
              "author": "m1nkeh",
              "text": "DM me if you need a referral out of MS âœŒï¸",
              "score": 6,
              "created_utc": "2026-02-13 18:53:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3u0hi",
      "title": "For those who write data pipeline apps using Python (or any other language), at what point do you make a package instead of copying the same code for new pipelines?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r3u0hi/for_those_who_write_data_pipeline_apps_using/",
      "author": "opabm",
      "created_utc": "2026-02-13 16:37:35",
      "score": 42,
      "num_comments": 33,
      "upvote_ratio": 0.93,
      "text": "I'm building out a Python app to ingest some data from an API. The last part of the app is a pretty straightforward class and function to upload the data into S3.\n\nI can see future projects that I would work on where I'm doing very similar work - querying an API and then uploading the data onto S3. For parts of the app that would likely be copied onto next projects like the upload to S3, would it make more sense to write a separate package to do the work? Or do you all usually just copy + paste code and just tweak it as necessary? When does it make sense to do the package? The only trade-off I can think of is managing a separate repository for the reusable package",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r3u0hi/for_those_who_write_data_pipeline_apps_using/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o56ziav",
          "author": "dev81808",
          "text": "Immediately.",
          "score": 48,
          "created_utc": "2026-02-13 17:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57unhq",
              "author": "popopopopopopopopoop",
              "text": "Premature optimisation can be counter productive.",
              "score": 14,
              "created_utc": "2026-02-13 19:34:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59hiie",
                  "author": "MonochromeDinosaur",
                  "text": "Code organization is not premature optimization.",
                  "score": 18,
                  "created_utc": "2026-02-14 00:48:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o58q1zu",
                  "author": "dev81808",
                  "text": "Sure, but I've found that thoughtful early optimization is usually net positive. \n\nWith enough experience, it becomes easier to judge where early effort is worthwhile and where it isnâ€™t.",
                  "score": 3,
                  "created_utc": "2026-02-13 22:10:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bs7nx",
                  "author": "ZirePhiinix",
                  "text": "So instead of changing one package, you'll now be changing X number of files. This isn't optimization, this is making sure you're actually deploying the same thing across your system.",
                  "score": 1,
                  "created_utc": "2026-02-14 11:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o570s2o",
          "author": "Atticus_Taintwater",
          "text": "For utility stuff that often fits well in a package\n\n\nIt's a loaded question for transformation reuse. But I swear people have forgotten views exist now that python is in the mix.Â \n\n\nI see so much python module hullabaloo that could just be \"reused\" by way of a regular ass view.",
          "score": 19,
          "created_utc": "2026-02-13 17:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57aclz",
          "author": "Skullclownlol",
          "text": "Write Everything Twice\n\nUsually for deduplication, but it also works for generalization.",
          "score": 17,
          "created_utc": "2026-02-13 17:56:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58m9rr",
              "author": "opabm",
              "text": "I'm not following completely, can you explain what you mean?",
              "score": 9,
              "created_utc": "2026-02-13 21:51:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fyjif",
                  "author": "azirale",
                  "text": "Never write directly to a library/module -- make that the second write. \n\nFirst time using some specific function? Just leave it in the script? Second time writing the exact same thing for the exact same use? Write it into a module/library for the second write. \n\nLater you'll get an eye for things you want to write directly to a module, but if you're not sure just start with local only",
                  "score": 2,
                  "created_utc": "2026-02-15 02:21:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o59953d",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -9,
                  "created_utc": "2026-02-13 23:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57fs8z",
              "author": "Oct8-Danger",
              "text": "This is the way. Good balance of reusing code and having it fit your needs at a time",
              "score": 2,
              "created_utc": "2026-02-13 18:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57jor3",
          "author": "Atmosck",
          "text": "I got there recently. I wrote an internal python package that handles all the boilerplate that gets used by multiple python automations - credential management, logging configuration, s3 operations, redshift and MySQL helpers, API clients with pydantic. Published internally to CodeArtifact.\n\nThe thing that got me to actually do it and made it an easy sell as a project, was an upstream API change we weren't informed about that broke and required updating a whole bunch of things. Now that would just be a matter of updating the package and bumping the version in the projects that use it.",
          "score": 5,
          "created_utc": "2026-02-13 18:41:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56ywp6",
          "author": "davrax",
          "text": "Take a look at dlt(hub)",
          "score": 10,
          "created_utc": "2026-02-13 17:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58q490",
              "author": "toabear",
              "text": "Second this. You will still write some code, but I handles a lot for you.",
              "score": 1,
              "created_utc": "2026-02-13 22:10:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dxbnq",
              "author": "opabm",
              "text": "Looks promising but seems like another package to rely on, no? Would this help much with avoiding have to copy+paste code?",
              "score": 1,
              "created_utc": "2026-02-14 19:12:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jwtsz",
                  "author": "davrax",
                  "text": "Eh, it might remove the need to build most of what you are building.",
                  "score": 1,
                  "created_utc": "2026-02-15 18:53:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57bedk",
          "author": "Tomaxto_",
          "text": "It depends, how many other jobs  in you pipeline share the same data extraction and writing?\n\nIn my case is 90%, hence I build a â€œtoolkitâ€ package and put the reading and writing logic there, add robust tests to it, and CD with uv + S3. On my pipeline repo each jobs share them and only implement the transformations unique to each one.",
          "score": 3,
          "created_utc": "2026-02-13 18:01:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c0l94",
          "author": "umognog",
          "text": "Make a package? Hell, make a container image and use the entry point.",
          "score": 3,
          "created_utc": "2026-02-14 13:03:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5708vg",
          "author": "Big-Touch-9293",
          "text": "I have all of my cloud code hosted on a GitHub, when I push to main it gets versioned and deployed automatically to cloud. \n\nThat being said, I almost exclusively write helper functions and hardly copy paste code, if I do itâ€™s minimal. Iâ€™ll have helpers for normalization, outbound, ingestion etc and just call. By versioning I know that the best, most up to date helper is used and working. That way I know all my code is using the most up to date and nothing is obsolete/unsupported.",
          "score": 1,
          "created_utc": "2026-02-13 17:07:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57abxc",
          "author": "Clever_Username69",
          "text": "Anytime I expect to be use the code more than once I'll make it into a function (or anything at work tbh, with personal projects that can be overkill and I usually write it once messily then rewrite if I feel like it). In your case it seems worth it to have an upload to s3 function within a larger AWS class, if you're starting out and don't see the need for an entire class you can add on later. Either way think of the components that are reusable and define those somewhere to avoid repeating yourself as much as possible. Definitely dont copy/paste the same code (or try not to), it's a bad habit ",
          "score": 1,
          "created_utc": "2026-02-13 17:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57xmpu",
          "author": "dans_clam_pie",
          "text": "Fairly early but contingent on having a reasonably fast dev experience for making quick changes to the util package (eg. Not having to create a PR, wait for ci/cd pipeline publish a version etcâ€¦)\n\nInstalling the utils package as an editable python package is sometimes nice, eg:\n\ncreate your utils package and install into your dependent dev repos with â€˜uv add â€”editable /path/to/utilsâ€™ (or â€˜pip install -e â€¦â€™)",
          "score": 1,
          "created_utc": "2026-02-13 19:48:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57xrcp",
          "author": "Efficient_Sun_4155",
          "text": "If you have a coherent purpose and you know it will be used a few times in different places. Then Iâ€™d make it a package that you can maintain in one place and rely on elsewhere. \n\nFollow decent practices, git tag your versions and automate the build test and publishing of your package. Use auto doc to keep docs up to date automatically and publish them in your CI pipeline",
          "score": 1,
          "created_utc": "2026-02-13 19:49:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o588z8i",
          "author": "BihariGuy",
          "text": "From the get go. As much as it's a pain to keep things modular and super organized in the beginning, it usually pays off pretty well later.",
          "score": 1,
          "created_utc": "2026-02-13 20:45:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58yxq7",
          "author": "tecedu",
          "text": "All the time, any new repo gets pyproject.toml, a runner to build a publish to internal pypi\n\nCode goes into your package, have another folder called runscripts which calls those packages.\n\nIts helps out a lot for a lot of things, you can just pip install again when needed, even when you dont need it you can use paths using library name instead of relative or absolute paths",
          "score": 1,
          "created_utc": "2026-02-13 22:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5955z7",
          "author": "Oldmanbabydog",
          "text": "For me itâ€™s less about duplication and more about change management. If I have a code that is reused a bunch of places and I need to update it Iâ€™d rather make the update in one place than the same update in 8 different places",
          "score": 1,
          "created_utc": "2026-02-13 23:34:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bfskb",
              "author": "lightnegative",
              "text": "The downside of that of course is that (particularly with Python) you now have to test those 8 pipelines to check that they're not broken, vs just 1",
              "score": 1,
              "created_utc": "2026-02-14 09:59:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5968da",
          "author": "Adrien0623",
          "text": "I try to make my code as generic as possible to have as little work as possible in case we want to duplicate the logic for another topic or if we need to swap a source, destination or logic element",
          "score": 1,
          "created_utc": "2026-02-13 23:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o598hvl",
          "author": "skatastic57",
          "text": "I just made one package, put it on pypi and if there's some function I need a lot then I'll put it in that package. When I make a new venv, script, pipeline, etc then I always know I can just install it and use it regardless of where it will be run from.",
          "score": 1,
          "created_utc": "2026-02-13 23:54:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c5no5",
          "author": "Alonlon79",
          "text": "As best practice - always parameterize your notebooks, your pipelines etc. \nthis is programming 101 that gives you the option to reuse any code you produce by pushing different parameters through an orchestration tool (like ADF or Datafactory in Fabric).\nIf you ingestion patterns are similar this will save a bunch of time.",
          "score": 1,
          "created_utc": "2026-02-14 13:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57j14k",
          "author": "reditandfirgetit",
          "text": "If you have to write the same code more than once, make a package",
          "score": 1,
          "created_utc": "2026-02-13 18:38:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2gu5c",
      "title": "Should I prioritize easy/medium or hard questions from DataLemur as a new graduate?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/",
      "author": "SIumped",
      "created_utc": "2026-02-12 02:03:26",
      "score": 41,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Hi all, I'll be graduating June so I'm currently applying to data roles with previous data engineering internships at a T100 company. I've picked up DataLemur and I'm somewhat comfortable with all easy/medium questions listed. Should I walk through these again to ensure I am 100% confident in answering these, or should I move onto hard questions?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4xwal1",
          "author": "Specific-Mechanic273",
          "text": "You can move to hard as they'll strengthen the skills required in medium. Most DataLemur hard questions feel like they're combining a lot of concepts. Tbh in most interviews you're asked medium level questions. \n\nJust be sure you're able to answer these question patterns (copy pasted from my Notion, let me know if i should clarify something):\n\n\\- Ordinal & Ranking Patterns (first, second, third, latest X per group) -> row\\_number() + dense\\_rank() + rank()\n\n\\- Rolling / Sliding Aggregations (rolling x-day average, running total etc.) -> sum/avg/count window function + \"ROWS BETWEEN N PRECEDING AND CURRENT ROW)\n\n\\- LAG / LEAD Window Functions (year-over-year changes)\n\n\\- Metric by Dimension (e.g. revenue by department) -> GROUP BY + join\n\n\\- Self Joins (often used in hierarchies)\n\n\\- Anti joins (find what's missing)\n\n\\- Conditional aggregation (count(case when x = y then 1 end))\n\n\\- CTEs\n\n\\- Knowing functions to manipulate dates (get month/year from timestamp, date diff, add time, ...)\n\n  \nWith this you'll be able to answer 99% of all interview questions",
          "score": 11,
          "created_utc": "2026-02-12 06:47:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z8xef",
              "author": "NickSinghTechCareers",
              "text": "good overview of skills here!",
              "score": 1,
              "created_utc": "2026-02-12 13:39:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xbnxt",
          "author": "NickSinghTechCareers",
          "text": "Hi! DataLemur founder here â€“ glad to hear you've been grinding SQL & Python on the site. I think moving onto the hard questions is good, if you've already done the easy/medium problems. You can always re-visit the Mediums again, and try to speed through them, after going through a few dozen hard problems. You just might be surprised how much faster you can go, after practicing on harder problems, and getting better at pattern recognition. \n\nBesides DataLemur, I think having a proper project to talk about is also super important for Data Engineering interviews. Hopefully, this can be sourced from a past internship â€“ but if not, go make a real portfolio project that's end-to-end, deployed (with a live link), that's also key-word rich (so use AWS, PostgreSQL, Airflow, Python, etc.). ",
          "score": 23,
          "created_utc": "2026-02-12 04:03:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xfgzz",
              "author": "WildLandShark",
              "text": "Hey Nick! I recently went through some of your hard SQL questions on DataLemur in preparation for BI Engineer interview. The questions were super helpful for refreshing myself on some querying techniques that I don't use all that often. I ended up receiving an offer, so thank you for creating such a helpful resource.\n\nI'm wondering though, how do you source your questions? I'm especially curious as there are a wide variety of companies that are listed as question sources.",
              "score": 7,
              "created_utc": "2026-02-12 04:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y71z8",
                  "author": "dyogenys",
                  "text": "Is this whole thing an ad?",
                  "score": 11,
                  "created_utc": "2026-02-12 08:29:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4z8p24",
                  "author": "NickSinghTechCareers",
                  "text": "glad it was helpful. for question sources â€“ many people tell me them, and then I change up the details slightly to go around NDA / maintain privacy. with like 175k+ followers on linkedin, and 50k copies sold of my book, enough people just LinkedIn DM me or email me their interview experience, ask for advice, feedback, etc. I also do a ton of 1:1 coaching, where we also go through past interviews they've had, and seen where they struggled or could improve. \n\nfinally â€“ i got a ton of it from Glassdoor, Reddit, Blind, and Medium back when I started DataLemur a few years ago. ",
                  "score": 5,
                  "created_utc": "2026-02-12 13:38:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50zyev",
              "author": "w_savage",
              "text": "Is DataLemur a free site? I've never ran across it before.",
              "score": 1,
              "created_utc": "2026-02-12 18:45:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r83gjr",
      "title": "Microsoft UI betrayal",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/l91uf1qlc9kg1.jpeg",
      "author": "daxdaxy",
      "created_utc": "2026-02-18 13:48:32",
      "score": 41,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Meme",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r83gjr/microsoft_ui_betrayal/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o62b7sa",
          "author": "SupaWillis",
          "text": "{\"code\":\"BadRequest\",\"message\":null,\"target\":\"pipeline//runid/XXXX\",\"details\":null,\"error\":null}\n\nThank you ADF",
          "score": 31,
          "created_utc": "2026-02-18 15:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62g3nn",
          "author": "Outrageous_Let5743",
          "text": "Have you seen SSIS UI. That is even worse",
          "score": 3,
          "created_utc": "2026-02-18 15:27:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62wyc2",
          "author": "RipProfessional3375",
          "text": "Never, ever touch a technology that is not designed for the people using it. Worst mistake of my life.",
          "score": 3,
          "created_utc": "2026-02-18 16:44:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7u6sz",
      "title": "Starting my first Data Engineering role soon. Any advice?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r7u6sz/starting_my_first_data_engineering_role_soon_any/",
      "author": "xahyms10",
      "created_utc": "2026-02-18 05:13:17",
      "score": 41,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "Iâ€™m starting my first Data Engineer role in about a month. What habits, skills, or ways of working helped you ramp up quickly and perform at a higher level early on? Any practical tips are appreciated",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r7u6sz/starting_my_first_data_engineering_role_soon_any/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o60szyx",
          "author": "___ml_n",
          "text": "I've been in 3 different roles as a Data Engineer, and they've all been so wildly different to me and without knowing more specifics without your role, I can't give too specific advice. But here are some general pointers from when I first started that might help.\n\n\\- Learn general data engineering practices / lingo. You're going to hear things like: Data Governance, Data Catalogs, Data Lineage, Data Warehouse, ETL, ELT, Data Lake, OLAP, OLTP, Data Mesh, etc. You don't have to learn everything all at once, or even fully understand everything the first time. Start with who / what you'll work with, expand from there.\n\n\\- Learn SQL VERY well. Two sides to this: learn a dialect VERY well and learn a database implementation well.  \nThe former will help you with your day to day job as a junior. Learning how to solve common patterns, use common functions, solve common SQL problems, etc will help you for your whole career. For the latter, you would want to really be able to explain things like indexes (which ones to use, why), data storage internals, how to read execution plans and optimize, etc. This is something you should be picking up gradually throughout your career, and I would only expect more mid level / senior engineers to know more and more about these things. For a junior, just begin learning it slowly, but don't stress out too much. \n\n\\- Lots of companies are now on the cloud (AWS/Azure/GCP). If your company is one of them, you should learn the stack. Learn the services that your company is using, what role / problem it solves, and how to configure/work with it. Whatever your company uses, be it Azure Synapse/AWS Redshift for data warehousing, ADLS / AWS S3 for object storage, learn that tech deeply, learn how authorization/authentication works on your cloud platform, and those two alone will carry forth across cloud providers. Once you learn what you work with, you can slowly expand outwards if you so desire. \n\n\\- Additional point to the above, lots of companies also use Databricks / Snowflake. If applicable, learn what each of those companies provide in terms of offerings or services. IMHO learning either of these opens the door for more data engineering roles in the future.\n\n\\- Maybe a controversial tip, but as a software engineer turned data engineer, I personally still apply software engineering principals just through a data engineering lens. That means following best practices when writing clean code, working with things like CI/CD, git, code review, etc. This may seem like a no brainer, but not every shop hires data engineers from the software engineer / CS grad pipeline. Lots of DEs I knew came from data analyst or scientist positions, and had no clue of the SWE fundamentals. I think treating this job as a specialized SWE position will help you a LOT with the menial stuff, and it'll allow you to pivot if you ever want to.\n\nI omitted a lot, but I think this is good to start, and I think these are general enough to help you no matter what kind of DE position you're put into. ",
          "score": 38,
          "created_utc": "2026-02-18 08:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61i7ae",
              "author": "HOFredditor",
              "text": "can I DM you for some questions ?",
              "score": 1,
              "created_utc": "2026-02-18 12:24:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6045em",
          "author": "Egao4",
          "text": "Same, gonna be a new grad data engineer in July but have little to no data engineering experience and feel like I rely on AI too much. Following this post!",
          "score": 11,
          "created_utc": "2026-02-18 05:17:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62dwvo",
              "author": "typodewww",
              "text": "Iâ€™m a current new grad Data Engineer graduated May 2025 been with my company 2 months you donâ€™t have to be a all or spake expert just try your best to keep up with projects and ask questions and itâ€™s ok not to be a master of everything.",
              "score": 2,
              "created_utc": "2026-02-18 15:17:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o613937",
              "author": "Dark_Sotard",
              "text": "This basically sums up my situation",
              "score": 1,
              "created_utc": "2026-02-18 10:26:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o608qlz",
          "author": "al_coper",
          "text": "I encourage you to try to understand deeply the business; How does they work? Process, customers, the reason behind the task you are performing, etc.",
          "score": 8,
          "created_utc": "2026-02-18 05:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60rvpg",
          "author": "Online_Matter",
          "text": "Do things as simple as possible and plan for changes. What's the data use and scale of data? What's the simplest way to handle that which will benefit the company for the next two or so years? Don't spin up a hadoop cluster for something that can be done in python.Â ",
          "score": 3,
          "created_utc": "2026-02-18 08:41:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60eckq",
          "author": "No_Distribution_7987",
          "text": "Congratulations! \nTry to understand the business. Itâ€™ll help you in long way to translate data to match the business use case. Always try and understand the complete picture.",
          "score": 4,
          "created_utc": "2026-02-18 06:39:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60wi2u",
          "author": "breadncheesetheking1",
          "text": "Do you have any previous experience in data?",
          "score": 2,
          "created_utc": "2026-02-18 09:25:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o614y1g",
          "author": "redditreader2020",
          "text": "Take notes and/or journal as much as possible. This will help in so many ways. Reinforces what you are learning. Reference for when you forget or your manager asks what have you accomplished. You can mentally relax on the weekends.\n\nI recommend markdown files but find what works for you.",
          "score": 2,
          "created_utc": "2026-02-18 10:41:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o612rpb",
          "author": "Key_Card7466",
          "text": "FollowingÂ ",
          "score": 1,
          "created_utc": "2026-02-18 10:22:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62806u",
          "author": "inglocines",
          "text": "SQL and Python are going to be your best friends in this career. I am not sure about your proficiency in either, but try to solve some difficult problems in both without using AI (or may be ask AI to generate questions for you to solve).   \n  \nIn SQL, try to build common patterns in data engineering with some sample data - MERGE INTO, SCD Type 2, some small star schema design.  \n  \nIn python, know common patterns used with data structures - list, dict, set and iterators. \n\nThese will help you in first few months. You can slowly move towards understanding the bigger data engineering architecture - SQL Optimization techniques, ETL, Data Vault modelling. I recommend you get 'Fundamentals of Data Engineering' book and read it once in a while. Re-read the concepts again once every few months as it will add new perspectives as you gain experience.  \n\nOnce you master SQL and python, tools are not going to be difficult for you. You will get to see that irrespective of tool - Spark or Snowflake, Airflow or ADF (or any GUI based orchestration) - the build patterns and outcomes are almost exactly same. \n\nWhile you master technical things, also try to be curious about the business problem you are solving. It doesn't matter if you know 4 different tools, if you cannot answer what business problem you solved. For this, AI would be immensely helpful. Let's say someone wants to build a CRM dashboard for which you are building data model - You might hear terms like Sales Funnel or Conversion rate - Try to ask AI and get an overall perspective of the problem you are trying to solve. You will work with lot of business analysts who will be more than happy if you talk their language. \n\nThese should be enough for now. ",
          "score": 1,
          "created_utc": "2026-02-18 14:49:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62d1t7",
          "author": "aquabryo",
          "text": "There's no best way to do anything, it's all just tradeoffs.",
          "score": 1,
          "created_utc": "2026-02-18 15:13:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62zhmp",
          "author": "mathproblemsolving",
          "text": "Congratulations on getting first DE role! I would check with the teammates/manager about the tech stacks they are using and a head start on those. ",
          "score": 1,
          "created_utc": "2026-02-18 16:55:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}