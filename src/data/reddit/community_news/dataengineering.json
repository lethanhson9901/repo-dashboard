{
  "metadata": {
    "last_updated": "2026-02-16 17:16:25",
    "time_filter": "week",
    "subreddit": "dataengineering",
    "total_items": 20,
    "total_comments": 310,
    "file_size_bytes": 362471
  },
  "items": [
    {
      "id": "1r3a05s",
      "title": "Has anyone read O‚ÄôReilly‚Äôs Data Engineering Design Patterns?",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/2uu32wxil5jg1.jpeg",
      "author": "xean333",
      "created_utc": "2026-02-13 00:07:17",
      "score": 197,
      "num_comments": 40,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r3a05s/has_anyone_read_oreillys_data_engineering_design/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54wkr9",
          "author": "minato3421",
          "text": "Yeah I went through the book. Felt pretty trivial to be honest. But I have an experience of 7 years in this field. So, nothing in that book felt new. It is worth reading for beginners though",
          "score": 102,
          "created_utc": "2026-02-13 09:19:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55p5uz",
              "author": "kaumaron",
              "text": "I feel like that's many books these days",
              "score": 22,
              "created_utc": "2026-02-13 13:09:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o56xyk6",
              "author": "tylerriccio8",
              "text": "Do you recommend anything more advanced? I have multiple yoe, not really looking for basic patterns",
              "score": 3,
              "created_utc": "2026-02-13 16:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5efcb8",
                  "author": "minato3421",
                  "text": "Designing Data Intensive Applications by Martin Kleppmann and Data Warehousing Toolkit by Kimball. Data Warehousing Toolkit is still valid in the current scenario even though companies say it isn't",
                  "score": 7,
                  "created_utc": "2026-02-14 20:48:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o59b3l5",
                  "author": "Character-Education3",
                  "text": "Probably books more focused on architecture and your business domain",
                  "score": 1,
                  "created_utc": "2026-02-14 00:09:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56rkjc",
              "author": "Thespck",
              "text": "What would you recommend to a junior data engineer? I find CGPT very useful when I ask to help me improve a pipeline or to teach me fundamentals or what‚Äôs best and why not other ways. However, I learnt about slow changing dimensions by reading Designing Data Intensive Applications by Martin Kleppmann (also O‚ÄôReilly)",
              "score": 5,
              "created_utc": "2026-02-13 16:26:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o554lj9",
          "author": "Kobosil",
          "text": "liked the code examples\n\none of the better books in my opinion",
          "score": 25,
          "created_utc": "2026-02-13 10:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o578h6j",
          "author": "pacopac25",
          "text": "I want to buy the book solely because the fish's clenched teeth, frowning, and thousand-mile-stare eyes accurately represent how I feel when I read the Spark documentation.",
          "score": 9,
          "created_utc": "2026-02-13 17:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5501f5",
          "author": "phizero2",
          "text": "Yeah, ok book. Isnt the best but worth  checking.",
          "score": 19,
          "created_utc": "2026-02-13 09:52:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55791p",
              "author": "dadadawe",
              "text": "Which one is the best?",
              "score": 21,
              "created_utc": "2026-02-13 10:58:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55fj3a",
                  "author": "PutridSmegma",
                  "text": "Designing data-intensive applications from Klepmann",
                  "score": 43,
                  "created_utc": "2026-02-13 12:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57om0t",
          "author": "Awkward-Cupcake6219",
          "text": "Good book, especially for mid level engineers. If you have around 5+ good quality YOE it could fill some gaps.\n\nMore than that? I guess it is nice to have it on the shelf for a quick look, but honestly you could \"have quick look\" on the internet too as I expect you to know what questions to ask at this point.",
          "score": 5,
          "created_utc": "2026-02-13 19:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57v1lu",
              "author": "xean333",
              "text": "That was about my assessment as well. I‚Äôm at a decade plus at this point so I‚Äôll probably skip it",
              "score": 2,
              "created_utc": "2026-02-13 19:36:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54zw8j",
          "author": "Astherol",
          "text": "Good book",
          "score": 8,
          "created_utc": "2026-02-13 09:51:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55be37",
          "author": "SoggyGrayDuck",
          "text": "Anyone have a great book/link on medallion architecture? I get it but I feel like it's essentially \"let agile define your model\" and id like to read a good resource on it.",
          "score": 4,
          "created_utc": "2026-02-13 11:33:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55ptcr",
              "author": "TechnologySimilar794",
              "text": "Building medalion architecture by Piethein Stengholt ",
              "score": 10,
              "created_utc": "2026-02-13 13:13:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55sydw",
                  "author": "SoggyGrayDuck",
                  "text": "Can you answer one question, does medallion architecture target spark based workflows? The big thing I'm trying to get straight in my head is where do traditional data models come into play. Some say they're not used anymore and others say that's what their silver layer is and yet others say it's the gold layer. I have a feeling it's being wedged into situations it doesn't actually work for. Or they don't really understand and are just updating the terms they use based on what they read or see.",
                  "score": 2,
                  "created_utc": "2026-02-13 13:31:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o55s8vi",
                  "author": "SoggyGrayDuck",
                  "text": "Thank you, looking it up/ordering",
                  "score": 1,
                  "created_utc": "2026-02-13 13:27:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o56bpft",
                  "author": "TheOneWhoSendsLetter",
                  "text": "Recommended.",
                  "score": 1,
                  "created_utc": "2026-02-13 15:10:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56c367",
              "author": "TheOneWhoSendsLetter",
              "text": "Besides Stengholt, *Data Lakes for Dummies* by Alan Simon",
              "score": 2,
              "created_utc": "2026-02-13 15:12:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55td0s",
          "author": "Salfiiii",
          "text": "The book itself is a nice reference but nothing I would consider reading through thoroughly.\n\nSkim over the concepts and come back to it if you ever need it.\n\nNothing revolutionary though, if you have couple years on your back you probably heard of > 90% already.",
          "score": 4,
          "created_utc": "2026-02-13 13:33:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56b2re",
          "author": "TheOneWhoSendsLetter",
          "text": "It's a very good book. You'll find value in the situations and problems addressed and the way of thinking and solutions' caveats that it exposes.",
          "score": 4,
          "created_utc": "2026-02-13 15:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54wnwm",
          "author": "putokaos",
          "text": "Absolutely. It's a fantastic book full of not just practical advice, but also the proper way of solving the most common scenarios. I'd recommend it to any data engineer.",
          "score": 9,
          "created_utc": "2026-02-13 09:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o559dei",
          "author": "Firm-Requirement1085",
          "text": "Just started chapter 2 and the small code examples are using spark, should I learn the basics of spark before continuing?",
          "score": 4,
          "created_utc": "2026-02-13 11:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55f83q",
              "author": "BrunoLuigi",
              "text": "Do you know python?",
              "score": 3,
              "created_utc": "2026-02-13 12:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55ih90",
                  "author": "Firm-Requirement1085",
                  "text": "Yes I use python-polars for ingestion/standardizing csv files but the company I'm at uses snowflake so haven't touch spark",
                  "score": 1,
                  "created_utc": "2026-02-13 12:26:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56b7zx",
              "author": "TheOneWhoSendsLetter",
              "text": "Because of the book? No need to. The solutions there are language-agnostic.",
              "score": 1,
              "created_utc": "2026-02-13 15:07:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o56r5ib",
          "author": "gman1023",
          "text": "I really enjoyed it, has practical problems and patterns one would need in data engineering. like someone said, one of the better books. \n\n\n\nyou can get it for free here (that's how i got it):  \n[Data Engineering Design Patterns](https://buf.build/resources/data-engineering-design-patterns)",
          "score": 2,
          "created_utc": "2026-02-13 16:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59e1fg",
              "author": "Interesting_Strain90",
              "text": "This never worked, i tried three different emails.",
              "score": 1,
              "created_utc": "2026-02-14 00:27:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59w26j",
                  "author": "LoaderD",
                  "text": "Are they company emails? Usually these companies don't let you sign up with a random email because they use this as a way to generate sales leads. \n\nIf you don't have a job and therefore, no company email, there are better books to get started that you should get before this book.",
                  "score": 2,
                  "created_utc": "2026-02-14 02:20:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o571ac2",
          "author": "ruibranco",
          "text": "ddia for the concepts, this one for the copy-paste recipes - they complement each other more than people think",
          "score": 2,
          "created_utc": "2026-02-13 17:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o553wkd",
          "author": "Ok_Appearance3584",
          "text": "Excellent reference book",
          "score": 2,
          "created_utc": "2026-02-13 10:28:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1tcjp",
      "title": "It's nine years since 'The Rise of the Data Engineer'‚Ä¶what's changed?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/",
      "author": "rmoff",
      "created_utc": "2026-02-11 09:58:52",
      "score": 161,
      "num_comments": 37,
      "upvote_ratio": 0.96,
      "text": "See title\n\nMax Beauchemin published [The Rise of the Data Engineer](https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603) in Jan 2017 (_and [The Downfall of the Data Engineer](https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b) seven months later_).\n\nWhat's the biggest change you've seen in the industry in that time? What's stayed the same?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4sncod",
          "author": "redditreader2020",
          "text": "COVID and AI. And I have more grey hair. Otherwise not much.",
          "score": 132,
          "created_utc": "2026-02-11 13:27:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tqr0r",
              "author": "updated_at",
              "text": "I'm balding",
              "score": 21,
              "created_utc": "2026-02-11 16:45:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ttb5t",
                  "author": "zerofatorial",
                  "text": "/r/tressless",
                  "score": 9,
                  "created_utc": "2026-02-11 16:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4swe8s",
          "author": "drag8800",
          "text": "Been in data since 2012. Few observations:\n\nWhat changed completely:\n- Infrastructure abstraction. In 2017 we were still debating Hadoop distributions. Now most teams never think about cluster management.\n- Analytics engineering emerged as a discipline. Beauchemin predicted DEs would need more SQL, but underestimated how much the transformation layer would specialize (dbt, semantic layers, etc.)\n- The 'modern data stack' hype cycle. Lots of point solutions that promised to solve specific problems, then consolidation as everyone realized 47 tools was too many.\n\nWhat stayed the same (unfortunately):\n- The gap between 'we have data' and 'we understand the business domain.' Still the hardest part.\n- Pipeline maintenance burden. Different failure modes now (API rate limits vs disk space), same percentage of time spent on it.\n- Stakeholder expectations vs data quality reality.\n\nWhat's genuinely better:\n- Getting started is 10x easier. A junior can have a working pipeline in a day instead of weeks.\n- The tooling for testing and observability actually exists now.\n- Version control for transformations is standard, not exotic.\n\nThe 'Downfall' article was prescient about platform engineering eating some DE work. But the semantic layer and data modeling parts got more complex, not less. Different work, roughly same headcount.",
          "score": 171,
          "created_utc": "2026-02-11 14:17:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tif2n",
              "author": "Kobosil",
              "text": ">The gap between 'we have data' and 'we understand the business domain.' Still the hardest part.\n\nas long thats the case i am not worried for my job",
              "score": 43,
              "created_utc": "2026-02-11 16:07:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vdrm1",
                  "author": "trixiethegoat",
                  "text": "same. It's a pain, but once you're the data SME for the business team and the domain SME for the data engineering team, you're golden.",
                  "score": 14,
                  "created_utc": "2026-02-11 21:25:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50rzug",
                  "author": "JBalloonist",
                  "text": "Same here.",
                  "score": 1,
                  "created_utc": "2026-02-12 18:08:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tf7td",
              "author": "cokeapm",
              "text": "What tooling for testing and observability do you recommend?",
              "score": 3,
              "created_utc": "2026-02-11 15:52:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tqu6q",
                  "author": "updated_at",
                  "text": "Monte Carlo and soda",
                  "score": 6,
                  "created_utc": "2026-02-11 16:46:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vkna7",
              "author": "umognog",
              "text": "I was just reflecting earlier today on the same point about having a working pipeline in a day rather than a week.\n\nToday, i not only put up a PPE & a prod machine and wrote \"the happy path code\" but I also had it all connected to observation, lineage and some basic tests.\n\nIn 5 hours.\n\nThat was really unheard of by a single person in my business in 2015/2016. Getting a VM alone was 4-6 weeks back then.",
              "score": 2,
              "created_utc": "2026-02-11 21:57:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50s4vz",
              "author": "JBalloonist",
              "text": "I‚Äôm glad I avoided the Hadoop phase and went straight to Spark.",
              "score": 1,
              "created_utc": "2026-02-12 18:09:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4thiax",
          "author": "mach_kernel",
          "text": "As hardware is getting better some \"big data\" is no longer that big. I see more and more developers reaching for things like DuckDB. I see an increase of robust federation solutions for cross-engine queries and optimizations.\n\nI am happy to see that the enterprise data pipeline is becoming more \"a la carte\".",
          "score": 28,
          "created_utc": "2026-02-11 16:03:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vdgpt",
              "author": "Mclovine_aus",
              "text": "So many places where the data could easily fit in a single machine, but execs fell for the big data warehouse and have bought a managed spark service like synapse - the bane of my existence.",
              "score": 9,
              "created_utc": "2026-02-11 21:23:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xxhxk",
                  "author": "One_Citron_4350",
                  "text": "Yes, they looked at it from a one-size-fits all point of view. Let's just put everything in Databricks with Spark.",
                  "score": 1,
                  "created_utc": "2026-02-12 06:58:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50tl0b",
              "author": "JBalloonist",
              "text": "Absolutely. Most of my work is done using DuckDB right now (using Lakehouses and Delta tables).",
              "score": 1,
              "created_utc": "2026-02-12 18:16:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4whsre",
          "author": "_TheDataBoi_",
          "text": "I was hired as a data engineer, but my role demands more than just data engineering starting from devops, data analysis, front end (streamlit and nextjs), business translation, some legal aspects of data processing and sharing, infra maintainability lmao.\n\n\nSince being a data engineer already would've touched the above tangents, we are now expected to take the entire thing upon ourselves. Data engineering has become the bridge connecting business to tech. Data engineers are the ones who enable decisions. We are just not in the spotlight.",
          "score": 10,
          "created_utc": "2026-02-12 00:58:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u7cyd",
          "author": "StewieGriffin26",
          "text": "Lots of consolidation to either Snowflake or Databricks. Either platform \"does it all\" now. \n\nAlso reinventing the wheel. What IBM and Oracle released back in the 80s is what Databricks and Snowflake are releasing now, just with a fancier name.",
          "score": 23,
          "created_utc": "2026-02-11 18:03:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v3n1g",
              "author": "GarboMcStevens",
              "text": "software is cyclical. People old enough age out and then you can rebrand old things as new.",
              "score": 13,
              "created_utc": "2026-02-11 20:36:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4v7pgx",
              "author": "kbisland",
              "text": "I‚Äôm wondering what IBM or Oracle released?",
              "score": 9,
              "created_utc": "2026-02-11 20:56:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vldi2",
                  "author": "StewieGriffin26",
                  "text": "https://old.reddit.com/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/o4vl9cp/",
                  "score": 1,
                  "created_utc": "2026-02-11 22:01:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xowb3",
              "author": "sib_n",
              "text": "I don't think making data warehousing work on a distributed cluster of commodity machines is just a \"fancier name\". There's a reason why Google, Yahoo and all web giants, invested in R&D to develop them from the 00', which gave birth to Hadoop. Snowflake and Databricks are abstracting this but it is still behind.  \nWhat's true is that they are still trying to reach the same level of reliability and features that those monolithic systems already had (like ACID), but some of them are not easy to reach with a distributed system. The latest big improvements towards this is the new table formats like Iceberg and Delta Lake to allow merge, time travel, column renaming and other metadata related features.",
              "score": 4,
              "created_utc": "2026-02-12 05:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yrvgz",
                  "author": "spcmnspff99",
                  "text": "As a database person, some of the original dialogue when systems like Hadoop and Spark were going mainstream was that ACID may not be as important as these other features like distributed data, etc. Before that, ACID was more of a golden rule you never broke. With this and some of these other features you mention that are rdbms standards, I see a sort of boomerang in your industry where some of the tenants were torn down in light of more important features and are now being gradually reintroduced while prioritizing said features.",
                  "score": 1,
                  "created_utc": "2026-02-12 11:45:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vdl2k",
              "author": "Mclovine_aus",
              "text": "Wha are some examples of re released features?",
              "score": 2,
              "created_utc": "2026-02-11 21:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vl9cp",
                  "author": "StewieGriffin26",
                  "text": "Databricks released temporary tables on Dec 9, 2025.  \nOracle released temporary tables in 1999.  \nIBM released temporary tables in 2001.  \nSnowflake released temporary tables in 2014.  \nSybase released temporary tables in ~1987.",
                  "score": 11,
                  "created_utc": "2026-02-11 22:00:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vetcr",
          "author": "Eleventhousand",
          "text": "Back before the term Data Engineer was a thing, I was still making design patterns and frameworks for my team.  Yes, I also spent half of my time on business problems, but I spent the other half on ensuring that we had a rock-solid and maintainable product, including tooling developed in 3GL languages as opposed to pure SQL.  There were other companies that had job duties split out - one team might handle the data modeling, another might handle the Informatica stuff, and another might handle dashboards, reports, and ad-hoc requests for insights.  I don't think much changed fundamentally, other than the job title, no different than going from being titled Programmer/Analyst one decade to Software Engineer during the next.  So, I'm not totally sold on the rise of Data Engineer.\n\nAs far as what has changed since 2017, really, just more cloud tools, more automation, etc.\n\n",
          "score": 7,
          "created_utc": "2026-02-11 21:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tcck6",
          "author": "zjaffee",
          "text": "The truth is that data engineer is a fake role that can mean tons of different things to different people in the same way roles like DevOps engineer or SRE can, increasingly ML engineer has a similar vibe. This was something very popular in the world of software development in 2017 where people were very focused on defining what large software teams should look like, along with the desire to build all sorts of new frameworks, this has died down.\n\nThere are places where a data engineer is a software engineer who owns the full stack of the data platform whatever that means, including in my cases also building data products on top of said platform. There are other places where a data engineer is someone who writes SQL largely for ETL purposes and maybe just manages the schema and type definitions of a particular data set and optimizes the routine queries that are run against said database, but even then that can be a stretch. In other cases, it might just be closer to a db admin setting privacy rules so that development teams cannot misuse PII.",
          "score": 23,
          "created_utc": "2026-02-11 15:38:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tigtg",
              "author": "Swayfromleftoright",
              "text": "Couldn‚Äôt you say that about pretty much any tech role though? A data analyst at company A probably spends their time differently to one at company B",
              "score": 10,
              "created_utc": "2026-02-11 16:07:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4trbic",
                  "author": "updated_at",
                  "text": "A carpenter at company A builds tables and at the company B builds doors. Always been like that",
                  "score": 8,
                  "created_utc": "2026-02-11 16:48:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v30sf",
          "author": "Accomplished-Row7524",
          "text": "dbt and analytics engineering",
          "score": 2,
          "created_utc": "2026-02-11 20:33:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yn7pt",
          "author": "Nervous-Potato-1464",
          "text": "Been in data since 2000s in finance now as an executive. Lots of bad decisions from the top regarding infrastructure. Moving to the cloud with all our data cost a lot, moving from mainframes cost a lot, and still using sas in most areas which again costs a lot. Still using oracle databases even though we were meant to migrate off in 2014. We now have a double database solution and oracle is meant to stop soon even though the new database is not better just had more development in the past 15 years. Still ml models are scary and we don't hire people who know how to do it so they all end up half arsed. No Python server as we are big into sas although some teams use r to make models. We now have ai which isn't so bad for data as you can't just write stuff as the data is a bit unknown to the AI so it can only build small functions but they are always super complicated and anyone that wants to reuse it has to try really hard to understand the over complexity of a simple task.",
          "score": 1,
          "created_utc": "2026-02-12 11:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yyxla",
          "author": "ScientistMundane7126",
          "text": "One big change accomplished by making data engineering an explicit specialization is that we're now aware of data quality issues that were hidden, or which were just consequences of the data gathering methods prevalent before big data frameworks and methods became available for large scale aggragate mobilization. Automation amplifies problems as much as it amplifies solutions.  The data engineer gets the heat when the products of their automation designs don't meet expectations, and the QA inquiry too often reveals problems with the data itself, including missing values, data entry errors, mismatched or approximated semantics when bringing together variously sourced data, accuracy and precision problems, deliberate and accidental bias, agenda selectivity, etc.  AI is built on big data infrastructure, so its good that we have this professional layer to review our supply chains as we procede to the next generation of decision support.",
          "score": 1,
          "created_utc": "2026-02-12 12:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xftus",
          "author": "Wizardij",
          "text": "The Rise of the Dead Engineers!\n\nNobody wants to work anymore. üòÅ",
          "score": -1,
          "created_utc": "2026-02-12 04:32:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2pumc",
      "title": "I built a website to centralize articles, events and podcasts about data",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/0ovtcb4jc0jg1.png",
      "author": "alphter",
      "created_utc": "2026-02-12 10:20:28",
      "score": 161,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Personal Project Showcase",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2pumc/i_built_a_website_to_centralize_articles_events/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o50du5l",
          "author": "sparkplug49",
          "text": "Could we get an rss feed so I can pull this into my feed reader?",
          "score": 17,
          "created_utc": "2026-02-12 17:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o517aj7",
              "author": "szrotowyprogramista",
              "text": "Echoing this. An RSS feed would be really nice.",
              "score": 7,
              "created_utc": "2026-02-12 19:20:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54wna5",
              "author": "alphter",
              "text": "It's in the backlog but I need to improve some other stuff first to be able to build a super clean RSS feed. Stay tuned!",
              "score": 3,
              "created_utc": "2026-02-13 09:20:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53vf1h",
          "author": "OpinionSad2896",
          "text": "There is an include filter. Would be great if we have an exclude list too",
          "score": 4,
          "created_utc": "2026-02-13 04:08:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54wq5g",
              "author": "alphter",
              "text": "Yes, I need to find a proper way to do this :)",
              "score": 1,
              "created_utc": "2026-02-13 09:20:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53arfq",
          "author": "michael-day",
          "text": "Love the name and branding",
          "score": 3,
          "created_utc": "2026-02-13 01:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54vike",
          "author": "Narrow-Tower58",
          "text": "Super cool! I am writing a blog about data topics - thanks for making my research 10x faster :D",
          "score": 2,
          "created_utc": "2026-02-13 09:09:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z15fg",
          "author": "hamisphere",
          "text": "wow that looks so cool! thanks for sharing",
          "score": 2,
          "created_utc": "2026-02-12 12:52:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o506uzh",
          "author": "Far-Criticism928",
          "text": "how do you source articles? ",
          "score": 1,
          "created_utc": "2026-02-12 16:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o508ssl",
              "author": "alphter",
              "text": "hi! it's a combination of multiple stuff but mostly i'm using RSS feeds coupled with AI post-processing to generate summaries and tagging.",
              "score": 5,
              "created_utc": "2026-02-12 16:39:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50q1ag",
                  "author": "decrementsf",
                  "text": "moltbook is spreading. Claude is in the room with us right now.",
                  "score": -1,
                  "created_utc": "2026-02-12 17:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50dc8g",
          "author": "evanazz",
          "text": "Super cool! I was planning on doing something like this with an LLM layer on top that to help me find topics to research and write about. Is a direction you'd be interested taking this in? I'm happy to hook that up & fund it, ofc.¬†",
          "score": 1,
          "created_utc": "2026-02-12 17:00:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50j4m1",
          "author": "digitalghost-dev",
          "text": "Great idea!",
          "score": 1,
          "created_utc": "2026-02-12 17:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dlvjd",
          "author": "scenestamper",
          "text": "So google.com",
          "score": 1,
          "created_utc": "2026-02-14 18:15:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h0g25",
          "author": "belkovTV",
          "text": "I love it, but alas, as fate would have it our company is using Microsoft Fabric on the business end where I am on. Do you reckon you could add Fabric Dataaaaa to the hub for the select few of those who work with it?\n\nOne central place is indeed a great option for us who have to look at different blogs, sites, feeds and whatnot to stay up to date.",
          "score": 1,
          "created_utc": "2026-02-15 07:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z363m",
          "author": "BicycleSignal6557",
          "text": "i'll definitely take a look !",
          "score": 1,
          "created_utc": "2026-02-12 13:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zu73i",
          "author": "Thinker_Assignment",
          "text": "Finally someone did it!",
          "score": 1,
          "created_utc": "2026-02-12 15:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5014i0",
          "author": "studentofarkad",
          "text": "This is amazing!! Thank you",
          "score": 1,
          "created_utc": "2026-02-12 16:03:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0ff3b",
      "title": "[AMA] We‚Äôre dbt Labs, ask us anything!",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/",
      "author": "andersdellosnubes",
      "created_utc": "2026-02-09 20:24:08",
      "score": 140,
      "num_comments": 124,
      "upvote_ratio": 0.91,
      "text": "Hi r/dataengineering ‚Äî though some might say analytics and data engineering are not the same thing, there‚Äôs still a great deal of dbt discussion happening here. So much so that the superb mods here have graciously offered to let us host an AMA happening this **Wednesday, February 11 at 12pm ET.**\n\nWe‚Äôll be here to answer your questions about anything (though preferably about dbt things)\n\n**As an introduction, we are:**\n\n* Anders u/andersdellosnubes (DX Advocate) ([obligatory proof](https://i.imgur.com/4WzEcKM.jpeg))\n* Jason u/dbt-Jason (Director: DX, Community & AI)\n* Jeremy Cohen u/jtcohen6 (PM) ([proof](https://imgur.com/rGUclDq))\n* Grace Goheen u/dbt-grace (PM) ([extra extra proof](https://i.imgur.com/pGMhBlk.gif))\n* Sara u/schemas_sgski (Product Marketing)\n* Quigley u/dbt-quigley (dbt Core engineer) ([proof](https://imgur.com/a7e89c8b-ee7d-42d3-a249-0fa68fe8d928))\n* Zeeshan u/dbt-zeeshan (Core engineering manager) ([proof](https://i.imgur.com/EkgG2dC.jpeg))\n* Tristan Handy u/jthandy (founder/CEO)\n\n**Here‚Äôs some questions that you might have for us:**\n\n* [what‚Äôs new](https://github.com/dbt-labs/dbt-core/releases/tag/v1.11.0) in dbt Core 1.11? what‚Äôs [coming next](https://github.com/dbt-labs/dbt-core/blob/main/docs/roadmap/2025-12-magic-to-do.md)?\n* what‚Äôs the latest in AI and agentic analytics ([MCP server](https://docs.getdbt.com/blog/introducing-dbt-mcp-server), [ADE bench](https://www.getdbt.com/blog/ade-bench-dbt-data-benchmarking), [dbt agent skills](https://docs.getdbt.com/blog/dbt-agent-skills))\n* what‚Äôs [the latest](https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md) with Fusion? is general availability coming anytime soon?\n* who is to blame to `nodes_to_a_grecian_urn` corny classical reference in our [docs site](https://docs.getdbt.com/reference/node-selection/yaml-selectors)?\n* is it true that we all get goosebumps anytime anytime someone types dbt with a capital d?\n\nDrop questions in the thread now or join us live on Wednesday!\n\nP.S. there‚Äôs a dbt Core 1.11 live virtual event next Thursday February 19. It will have live demos, cover roadmap, and prizes! [Save your seat here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&utm_source=reddit&utm_campaign=q1-2027_dbt-core-live_aw&utm_content=themed-webinar____&utm_term=all_all__).\n\nedit: Hey we're live now and jumping in!\n\n>thanks everyone for your questions! we all had a great time. we'll check back in on the thread throughout the day for any follow ups!\n>\n>If you want to know more about dbt Core 1.11, next week there's a live event next week!\n>\n>[reserve your spot here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&utm_source=reddit&utm_campaign=q1-2027_dbt-core-live_aw&utm_content=themed-webinar____&utm_term=all_all__)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4irts2",
          "author": "Interesting_Tank_118",
          "text": "I love your product! Since merging with Fivetran: Whats the long term strategy of dbtlabs? i.e. will dbt cloud have even more advanced features than dbt core to get more paying customers?",
          "score": 84,
          "created_utc": "2026-02-09 23:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u1abx",
              "author": "dbt-jason",
              "text": "First of all - thank you for being a dbt user!! Before I worked at dbt Labs I spent a lot of time on this subreddit so it's great to be here chatting with you all.\n\nRe: merger / Core / Cloud\n\nThe process of merging the two companies is still underway, so there‚Äôs really not much we can say besides ‚Äústay tuned.‚Äù The details we shared back in October at Coalesce in Las Vegas ([blog](https://www.getdbt.com/blog/dbt-labs-and-fivetran-merge-announcement), [keynote](https://youtu.be/KhBsI2LQQ90?si=YWWxfJnK5J6vRxOg)) haven't changed.\n\nOne thing I'll add is that we're committed to keeping dbt Core the open source standard for data transformation. Are we going to ship great new dbt features to our paying customers? Of course. But not by keeping important functionality out of dbt Core. The way to do it is to make sure dbt Core (and Fusion) have more powerful and better functionality and then build even better features on top.\n\nGrace and the rest of the Core team can talk about some of the things we've been cooking up there. And most importantly - we \\_always\\_ want to hear from you. What would you like to see in Core?",
              "score": 8,
              "created_utc": "2026-02-11 17:35:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4tet8g",
              "author": "finally_i_found_one",
              "text": "Looks like the most voted (and most important) question went unanswered.",
              "score": -4,
              "created_utc": "2026-02-11 15:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4utnx1",
                  "author": "andersdellosnubes",
                  "text": "I think you were early to the party!",
                  "score": 2,
                  "created_utc": "2026-02-11 19:47:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u80vu",
              "author": "andersdellosnubes",
              "text": "what u/dbt-jason said! time will tell; stay tuned",
              "score": 0,
              "created_utc": "2026-02-11 18:06:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jcvg9",
          "author": "FIapdoodle",
          "text": "Some questions pertaining to Fivetran merger:\n\nSince they also acquired Tobiko Data, will we see any consolidation/standardization efforts between DBT and SQLMesh? \n\nWith Fivetran providing many data connectors, will we see a more end-to-end flow established where the ingestion and transformations will be managed together?",
          "score": 24,
          "created_utc": "2026-02-10 01:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u45gh",
              "author": "andersdellosnubes",
              "text": "u/dbt-jason already touched on this in [this reply](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u1abx/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 1,
              "created_utc": "2026-02-11 17:48:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4vknbx",
                  "author": "m1hawkgsm",
                  "text": "That's not an answer, though. It makes an explicit reference to only Fivetran / dbt as \"two companies merging\".",
                  "score": 4,
                  "created_utc": "2026-02-11 21:57:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4j8enu",
          "author": "flatulent1",
          "text": "The gap between dbt explorer/catalog and a full scale catalog like Atlan or Datahub or Alation or Secoda etc is still huge. In 2026, data is a context game. Are yall just playing it safe as a metadata provider for more expensive, external catalogs?",
          "score": 16,
          "created_utc": "2026-02-10 00:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2ypt",
              "author": "jthandy",
              "text": "Awesome question.\n\nWe have a ton of respect for what pure-play catalog providers do and how hard that is. A lot of this functionality is purely an integration game, and that‚Äôs a TON of work both in terms of the initial build and the maintenance of the integrations.\n\nWe do think that dbt metadata is really central to all analytical metadata and think that we‚Äôre the best place to get that, whether that‚Äôs via artifacts or our API. And with Fusion this dbt-related metadata is getting *significantly* more advanced.\n\nWe do have a few integrations where we pull in metadata of downstream artifacts--tableau and powerbi--but certainly we are *nowhere near as comprehensive* as most pure-play catalogs. And that is ok.\n\nOur goal with Catalog is:\n\n* to provide a really amazing catalog experience that can be the ONLY catalog for small-to-medium companies.\n* to provide a really useful development tool for dbt authors at companies of all sizes\n* to provide a high-quality source of dbt metadata for companies of all sizes\n\nOver time, I fully imagine that we will continue to expand our capabilities here (as we have been!). But we‚Äôre not trying to head-to-head compete against pure-play catalog companies.",
              "score": 6,
              "created_utc": "2026-02-11 17:43:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4io7a9",
          "author": "Sex4Vespene",
          "text": "Thanks for reaching out for questions, I love DBT and it‚Äôs been so useful for our org. I have one question that‚Äôs somewhat of a feature request. Have you considered having an ‚Äúintermediate‚Äù type table materialization? We have several large models that have to be broken up into intermediate steps because they would either overload memory, or perform poorly due to CTE‚Äôs that are called multiple times, which we can instead just process them once in a separate model. What gets annoying with this, is we don‚Äôt want any of these intermediate models taking up space in our warehouse, so we have to use a custom post-hook on any end-state models to clean up the upstream intermediate models. It would be really awesome if you integrated this automatically. Let us use intermediate as a materialization strategy, and have them be autodropped once an end-state model finishes. I know you have ephemeral and view materializations, but none of those solve the problem of having too much stuff happening in the final query that uses them. Thanks again!",
          "score": 24,
          "created_utc": "2026-02-09 23:04:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4y6frm",
              "author": "Global_Bar1754",
              "text": ">¬†Let us use intermediate as a materialization strategy, and have them be autodropped once an end-state model finishes.\n\nCorrect me if I‚Äôm wrong, but it would be even better if the intermediate model was dropped after the last model that directly references it is materialized, right? No need to wait for the end state model to finish?",
              "score": 1,
              "created_utc": "2026-02-12 08:23:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4iw32u",
              "author": "Chilangosta",
              "text": "Something about this feels off... You have such massive tables that they overload memory, and are forced to break apart queries into intermediate steps. But you then delete said intermediate products presumably because they take up so much space that this saves you money.... And somehow the savings are worth rebuilding it from scratch every time?\n\nI have so many questions; this isn't making sense to me. Feels like either you're overoptimizing for cost when it's really not worth it or else doing something wrong when it comes to query optimization for the intermediate products to truly not be worth keeping around. Are your joins exploding? Is storage really so expensive that rebuilding these intermediates makes sense? Are you doing too much in your final query? Without more info it's hard to say for certain, but intermediate products aside I suspect something isn't as optimal about this scenario as you think.",
              "score": 1,
              "created_utc": "2026-02-09 23:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4iyv6s",
                  "author": "Sex4Vespene",
                  "text": "You‚Äôre making some incorrect assumptions here. We are on prem, this isn‚Äôt about saving cloud costs, it‚Äôs about us being somewhat low on storage and trying to minimize how much useless data is just sitting around. We are working on getting budget to expand, but that doesn‚Äôt happen immediately. I don‚Äôt understand what you mean about doing something wrong with optimization for them to not be worth keeping around. These are models that are run on a daily/weekly/monthly cadence. The previous versions of the intermediate models are useless for the next run. They either cover a completely different date range, and even if there is overlap, we have a decent amount of updates/deletes against old records so we‚Äôd want to rerun with the newest versions of the records anyways. And no, our joins aren‚Äôt exploding, I‚Äôve actually done a huge amount of work creating optimization guidelines for our models, and will go in to fine tune when needed. Since we are on prem, we aren‚Äôt paying for aggregate compute usage. We have a set amount of compute and memory that is always available at any moment. If we just ran one model at a time, we would be massively underutilizing our compute. To make the most out of our compute, we run multiple models at once. But since we aren‚Äôt just running one model, we have to set memory limits on all models, by dividing our available memory by the number of concurrent models we run, to ensure we don‚Äôt have memory related failures. Also as I mentioned, there are cases where breaking things out into an intermediate model can have massive performance improvements. Our query engine materializes CTE‚Äôs every time it is called. If you use the same CTE in a query 5 times, it quintuples the memory and compute usage, versus just running it once into a table and then just referencing the table a few times. It all makes plenty of sense, you are just evaluating it from an invalid context.",
                  "score": 12,
                  "created_utc": "2026-02-10 00:02:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4iwknd",
          "author": "RobG760",
          "text": "How will dbt core adapt to a world where streaming pipelines are starting to become more common?  How do you see dbt helping build a clean data lineage across all enterprise data regardless of whether batch or streaming tools are being leveraged?",
          "score": 24,
          "created_utc": "2026-02-09 23:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u7z63",
              "author": "jtcohen6",
              "text": "candidly - streaming pipelines have been \"starting to become more common\" for as long as I can remember (since \\~2018)  \n  \nok in seriousness - what i can say is:\n\n* the big innovation over the past \\~decade has been the development of streaming pipelines *written in SQL*. (you don't need to know java/scala! power to the data team!)\n* there are dbt adapters for streaming-first/native DWHs that support an all-SQL interface - Materialize has had an adapter for years, and we see that the Clickhouse adapter has been quite popular of late\n* batch DWHs Snowflake + Databricks have both rolled out support for their own streaming solutions - Dynamic Tables and Materialized Views / Streaming Tables, respectively - with full SQL support. those features are supported as materialization types in dbt, a pretty decent number of folks are using them, but for <5% of all the models in their project.\n\nso - while we see folks adopting streaming pipelines for specific use cases, when there is a specific business need that justifies the added cost/complexity - we still see batch (with hourly/daily refresh) as \"good enough\" for >90% of data transformations.  \n  \nI think dbt's role is continuing to serve as an abstraction across *both* batch and streaming pipelines - both kinds of data products still need version control, testing, documentation, CI/CD, ... - and they should be written in SQL, as dbt models :)",
              "score": 5,
              "created_utc": "2026-02-11 18:06:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ua3zq",
              "author": "muneriver",
              "text": "this may be interesting to you: https://docs.getdbt.com/best-practices/how-we-handle-real-time-data/1-intro",
              "score": 3,
              "created_utc": "2026-02-11 18:16:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j35rw",
          "author": "midnighttyph00n",
          "text": "when will fusion support python  models",
          "score": 11,
          "created_utc": "2026-02-10 00:26:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lgyag",
              "author": "pseudo-logical",
              "text": "It already does for the big three (snow dbx and bq) which says something about their marketing and comms",
              "score": 3,
              "created_utc": "2026-02-10 11:11:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tyoik",
                  "author": "jtcohen6",
                  "text": "u/pseudo-logical is right - Fusion added supported for Python models way back in \\[v2.0.0-preview.81\\]([https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md#200-preview81---december-09-2025](https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md#200-preview81---december-09-2025)) - across Snowflake + Databricks + BigQuery  \n  \nto the point about marketing/comms (fair!) - we haven't made a big announcement (yet), since we're still testing out this functionality in real-world projects with help from folks in the community. we know we've shipped *a lot* of stuff in Fusion over the past \\~6 months, and we're trying to do a better job of distinguishing between \"this is new and could use your help testing\" vs. \"this is ready for production *right now*.\" you can expect to hear more from us in the next \"Fusion diary\" (long awaited & soon to drop!). the big remaining piece before GA is figuring out how \"static analysis\" can/should work with Python models (which, for obvious reasons, can't be analyzed as SQL) - [https://github.com/dbt-labs/dbt-fusion/discussions/1042](https://github.com/dbt-labs/dbt-fusion/discussions/1042)\n\nfyi - you can always check here to see the latest table of dbt feature support / what's still missing in Fusion - [https://docs.getdbt.com/docs/fusion/supported-features](https://docs.getdbt.com/docs/fusion/supported-features)\n\nu/midnighttyph00n \\- how are you using Python models in dbt today? on which data warehouse? are you down to give them a try on Fusion? :)  \n  \n**tl;dr - Python models are in Preview (like the rest of Fusion), and will be GA when Fusion goes GA**",
                  "score": 6,
                  "created_utc": "2026-02-11 17:23:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4j4wea",
          "author": "HC-Klown",
          "text": "Please, when will you support write audit publish pattern as a materialization option? I want to build the model in some temp environment then test it and then ‚Äúdeploy‚Äù it without having to rely on data branching features from for example Nessie or LakeFs. \n\nI know this can get more involved with views etc but do you guys have anything related to this in your roadmap? Or would recommend building a custom materialization?",
          "score": 10,
          "created_utc": "2026-02-10 00:36:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tyerj",
              "author": "andersdellosnubes",
              "text": "So, SDF had WAP built into how it was working, and we plan to bring it to dbt in the future! But for now the key priority for us is to make dbt projects run on Fusion without adding too many changes yet to the overall dbt framework.\n\nThere's other tools out there that do more exactly what you're saying.\n\nWe haven't spent too much time in this space. I share your view that DVC or Pachyderm or LakeFS, while great, feel like overkill. My brain has been somewhat sniped by the fact that Apache Iceberg tables have a concept of branching. For me this is very compelling in it's simplicity. In this world you don't need to make a \"view\" rather a \"branch\" of a prod table for local dev.\n\nOrthogonally, we're rather bullish on sampling prod data locally, which is a different pattern than a direct WAP pattern",
              "score": 3,
              "created_utc": "2026-02-11 17:21:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4tz94t",
              "author": "oliverlrj",
              "text": "Hi, may I know how you are implementing write audit publish currently? I am currently in the midst of playing with iceberg, dbt and nessie. Do you use dbt macros to create a new branch for writing of data to the audit branch, or a dbt pre-run hook etc? \n\nWould love to see how people currently do it, thank you!",
              "score": 2,
              "created_utc": "2026-02-11 17:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u1uii",
                  "author": "jtcohen6",
                  "text": "I think there are two different patterns that get lumped into \"WAP\" or \"blue/green\":  \n\\-  (1) for each individual model, first build an intermediate model with the new data, run tests against it (unique, not\\_null, etc) if (and only if) all the tests pass, complete the materialization (swap, merge, ...). if the tests fail - don't; or put the failing rows into \"quarantine\", then keep going...  \n\\- (2) for a group of models, or an entire environment (dev/CI/CD) - create a separate DWH schema (or Iceberg \"branch\"), run all tests, then only if all the tests pass, swap the entire schema/branch into production. this has the advantage of supporting tests that span across multiple models, or allowing sanity-checking / human-in-the-loop verification for final models/metrics at the end of the DAG before re-deploying. this obviously *doesn't* work if the dev/CI/CD environment has a subset of data, or different data from production (scrubbed/anonymized).\n\nI think u/HC-Klown's original question is asking after (1), as a per-model materialization strategy - but just to say, there are pros/cons to either approach, and I don't think we've landed cleanly on which one to pursue (or some combination of both)",
                  "score": 1,
                  "created_utc": "2026-02-11 17:38:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l0vwi",
          "author": "paujas",
          "text": "What are the priority developments dbt-Labs are concentrating on ?\n\nWhat is the long term vision for dbt-core and dbt-cloud?",
          "score": 8,
          "created_utc": "2026-02-10 08:39:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u0au5",
              "author": "dbt-jason",
              "text": "There's a *ton* going on right now! In terms of priorities, here's how I'm thinking about it.  \n\n\n* First and always - ensure that dbt is a great way to do data work for organizations all across the world. You'd be surprised by how much nuance there is in keeping that up. Addressing bugs and community issues, adapters changes from the underlying databases and new releases for things like dbt-utils. I came to dbt from running a data team and more than anything else the question is how can we continue to be a trusted piece of the analytics stack for data teams across the world.\n* Beyond that, I think of the next set of priorities across three primary buckets - the dbt Fusion engine, AI and emerging standard.\n   1. The dbt Fusion engine is a big area of investment for us and we've seen that for projects that are able to successfully adopt, the rust based, strongly typed version of dbt brings a ton of improvements. Things like speed, improved developer experience and the sql comprehension features that power the VS code extension work great. And also - we've seen some challenges in complex projects getting migrated over to Fusion\n   2. AI - I've been obsessed with AI since way before I worked at dbt Labs (fun fact I was an r/futurology mod when it had less than 1000 users). I really believe that AI is going to radically reshape how we do data work - and giving dbt users the tools to do that *right* is very top of mind.\n   3. Emerging standards - Iceberg, arrow and more. We've been watching Iceberg for a few years and now we're starting to see real customers using Iceberg in production for cheaper / more flexible data ingestion, and to use dbt across multiple data platforms",
              "score": 2,
              "created_utc": "2026-02-11 17:30:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rnmxg",
          "author": "OrneryBlood2153",
          "text": "Honestly .. What is going on .. ? \ncore is till getting big fix as releases,\nFusion still in beta,\nCloud was said to go under a single umbrella with five tran,\nOpen engine,\nOpen catalog,\nNow that sqlmesh is also under the same umbrella what happens to the ideas in that tool.\n\nFeels like too much is said but not sure what's the direction of this product for ,\nEnd users - open source and cloud users,\nOpen source contributors - adapter and core ,\ncore vs fusion - what to use going forward.. I don't think both products are going to get equal attention.",
          "score": 7,
          "created_utc": "2026-02-11 08:40:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ky1in",
          "author": "pseudo-logical",
          "text": "Ok, I'll bite, _is_ Fusion going to be GA anytime soon? I feel bad for the devs who have had to keep their foot on the gas for nearly a full year since someone on the e-team decided to launch prematurely.",
          "score": 5,
          "created_utc": "2026-02-10 08:11:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2m80",
              "author": "schemas_sgski",
              "text": "Hey! Really appreciate the interest in Fusion. The team has def been busy building a new engine for dbt that is fast, stable, has parity with the features in dbt Core, and builds on it in important ways. This takes time and we want to make sure we nail it for y'all. More news on Fusion GA coming soon. Stay tuned!",
              "score": 1,
              "created_utc": "2026-02-11 17:41:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iwxwj",
          "author": "Proudly_Funky_Monkey",
          "text": "Is there a better way to manage the lifecycle of parallel pipelines?¬†\n\n\nContext: we use DBT core to build features for ml models. New data is ingested weekly, and from it new model features are built through a tree of 30+ very complicated tables. Models are then fed these latest features. We're pretty happy with this.¬†\n\n\nBut when we want/need to develope new/different model features, we really struggle with versioning. we only have one database: production. So end up duplicating the entire tree of tables with _version[] appended. The development is then done in the version suffixed tables until eventually it eventually becomes prod and the old tables/definitions are deleted.¬†\n\n\nWhy is this bad? Massive PRs, drift between trees during dev, significant risk of manual mistakes, entire tree must be duplicated even for small changes (complexity and cost).\n\n\nCan DBT help with our architecture problems?",
          "score": 4,
          "created_utc": "2026-02-09 23:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1ez7",
              "author": "SpookyScaryFrouze",
              "text": "You could change the generate_schema_name macro in order for the target name to be appended to the schema name.\n\nThat way you would have 2 schemas : ml_models_prod and ml_models_dev, in the same database.",
              "score": 5,
              "created_utc": "2026-02-10 08:44:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r2ls2",
                  "author": "ianitic",
                  "text": "Yup that's what we do at work. There's all contracts and model versioning in dbt as well though we haven't needed the latter yet.",
                  "score": 2,
                  "created_utc": "2026-02-11 05:32:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4j2h0o",
              "author": "fsm_follower",
              "text": "I don‚Äôt have a solution for you but have felt this.  Would the ability for dev pipelines to pull almost all the tables from prod then only generate those in your diff be a solution?",
              "score": 3,
              "created_utc": "2026-02-10 00:23:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tytwy",
                  "author": "andersdellosnubes",
                  "text": "interesting. can you say more?",
                  "score": 1,
                  "created_utc": "2026-02-11 17:23:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4orc2h",
              "author": "infomocrat",
              "text": "Are you familiar with model versions?   \n[https://docs.getdbt.com/docs/mesh/govern/model-versions](https://docs.getdbt.com/docs/mesh/govern/model-versions)  \nbest practice guide: [https://docs.getdbt.com/best-practices/how-we-mesh/mesh-6-coordinate-versions](https://docs.getdbt.com/best-practices/how-we-mesh/mesh-6-coordinate-versions)",
              "score": 2,
              "created_utc": "2026-02-10 21:21:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4tyqqa",
              "author": "andersdellosnubes",
              "text": "Yes, dbt should be able to help with that. It is a pretty detailed use case you have so we won‚Äôt be able to go into a lot of details but I feel like a combination of using [dbt clone](https://docs.getdbt.com/reference/commands/clone) and/or [deferral](https://docs.getdbt.com/reference/node-selection/defer) to build only the minimum set of models for a given ML feature branch.  \n  \nThen, my first instinct would also be to try to use different schemas or database to separate the output of the different LM pipelines..\n\n  \nAlso lots of others have contributed ideas that you might find helpful as well",
              "score": 2,
              "created_utc": "2026-02-11 17:23:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4yea8u",
              "author": "Global_Bar1754",
              "text": "Would you potentially be open to Python based solutions (completely foss frameworks) outside of dbt? If so could you share about how big your datasets are? The process you described is a huge operational risk (as you already know and alluded to) and solutions that require any kind of manual version management are gonna be a pain to stay on top of. There‚Äôs pretty lightweight python framework solutions out there that likely wouldn‚Äôt require that.¬†",
              "score": 2,
              "created_utc": "2026-02-12 09:41:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50t0vt",
              "author": "Proudly_Funky_Monkey",
              "text": "Thanks all! I will look into each of these proposals.",
              "score": 1,
              "created_utc": "2026-02-12 18:13:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ng6a3",
          "author": "domscatterbrain",
          "text": "What's the big roadmap for DBT?\n\nYou must know that, while the acquisition shenanigans are good for your team (hopefully) it made many people who's been a long fan of DBT furious.",
          "score": 3,
          "created_utc": "2026-02-10 17:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ua1g1",
              "author": "dbt-jason",
              "text": "Shared some thoughts on [this here](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u1abx/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)!",
              "score": 2,
              "created_utc": "2026-02-11 18:16:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ngrbx",
          "author": "uglylookingguy",
          "text": "What‚Äôs the biggest mistake teams make when adopting dbt that doesn‚Äôt show up until months later in production?",
          "score": 5,
          "created_utc": "2026-02-10 17:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3iam",
              "author": "schemas_sgski",
              "text": "I have seen two extremes.  \n  \nI have seen teams trying to come up with the end to end data pipeline design on paper, without writing any model, and then trying to implement it.  \n  \ndbt is made for agility, so, my experience is that people should write models, iterate, refactor, deprecate, improve etc‚Ä¶ With CI/CD in place there is not a lot of risk in modifying existing models, not like with platforms not backed by git and version control.  \n  \nThe other extreme is people going a bit wild on the models they build and not focusing on PR reviews and overall coherence. After a few months a dbt project can become complex if not managed enough. What I recommend here is to set some tools like [dbt-projet-evaluator](https://github.com/dbt-labs/dbt-project-evaluator) as soon as possible so that best practices are baked into the project from its inception.",
              "score": 2,
              "created_utc": "2026-02-11 17:45:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u972o",
              "author": "andersdellosnubes",
              "text": "sometimes, as the case in all engineering, just because you can do something that feels smart, doesn't mean that you should.\n\nI'm not so extreme as folks who say\n\n>the best code is the code you don't write\n\nhowever, with great jinja powers comes great responsibility. now with Fusion, some users have had to rethink their pure jinja automation solutions, especially when they mess with the DAG or ask the DWH for these things\n\n",
              "score": 2,
              "created_utc": "2026-02-11 18:12:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4ud52q",
              "author": "andersdellosnubes",
              "text": "some others from colleauges:\n\n>too many jobs and not understanding that a dbt DAG creates dependencies for you. stop doing steps like `dbt build -s model_a`. use selectors and tags instead!\n\nalso\n\n>ensure you've set up your CI/CD environments correctly at the outset and don't do a staging environment unless you really really need to\n\none more\n\n>never have multiple dbt projects that are copies of the same repo called like `project-a-staging` and `project-a-prod` \\-- it's a mess. thank me later\n\n  \nlast. lol\n\n>do not override `ref()` unless you have some batshit crazy insane reason and know what you're doing\n\nhard agree. overriding critical dbt jinja macros connotes great power, but comes with great responsiblitly! \"KISS\" rules the day",
              "score": 2,
              "created_utc": "2026-02-11 18:30:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o5oy4",
          "author": "Adrien0623",
          "text": "When will the fix for dbt-redshift connector will come so that materialized view refresh stops failing randomly?",
          "score": 4,
          "created_utc": "2026-02-10 19:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tzzko",
              "author": "Longjumping-Pin-3235",
              "text": "dbt just issues the DDL, right?",
              "score": 2,
              "created_utc": "2026-02-11 17:29:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u03gj",
              "author": "dbt-zeeshan",
              "text": "Hey I see a couple that seem related [\\#1256](https://github.com/dbt-labs/dbt-adapters/issues/1256) and [\\#1499](https://github.com/dbt-labs/dbt-adapters/issues/1499). If you share the link we'll definitely take a look!",
              "score": 1,
              "created_utc": "2026-02-11 17:29:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4oj2su",
          "author": "uncertainschrodinger",
          "text": "What are some new trends, ideas, competitors, and schools of thought that you find to be the biggest threat to the principles and roadmap of dbt?",
          "score": 4,
          "created_utc": "2026-02-10 20:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u8n48",
              "author": "dbt-jason",
              "text": "So dbt arose in popularity because of a very specific set of technological circumstances. Cloud data warehouses allowed you to load more data into yuor warehouse, but there wasn't a great way to manage this. Compute was cheap, storage was cheap, but getting useful, structured answers was challenging and difficult.  \n  \nThat technological paradigm isn't going anywhere, but it is going to be changed *a lot* by AI. Something that I'm thinking a lot about is how to make \"the dbt lifestyle\" remains relevant in the AI era.  \n  \nFor example, we saw an interesting article published [this week](https://substack.com/@groupby1/p-187428984) claiming that the \"Modern Data Stack\" worked great for humans, but there are other tools that will perform better for agents. I think it's important to take an honest look here - the experiment run in this wasn't very rigorous from what I can tell, but it might be pointing at something real.  \n  \nBut I really do believe we have an interesting role to play in the AI world. For example - last year we shipped the [dbt MCP server](https://github.com/dbt-labs/dbt-mcp) and we just released [dbt agent skills](https://github.com/dbt-labs/dbt-agent-skills) but the thing that's really exciting to me here is some testing we've been doing around how Fusion (because it's faster, has more metadata awareness and is more strongly typed) can be the best of both worlds- a great experience for human developers and powering agentic workloads.",
              "score": 1,
              "created_utc": "2026-02-11 18:09:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4k3rdf",
          "author": "ActEfficient5022",
          "text": "What would you say you do here?",
          "score": 7,
          "created_utc": "2026-02-10 04:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tkp7y",
              "author": "turboDividend",
              "text": "i mean, it can build depencys which would be useful in some use cases. ive alway been kind of skeptical of this tool though",
              "score": 3,
              "created_utc": "2026-02-11 16:17:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4r2udj",
              "author": "ianitic",
              "text": "I have people skills and I'm good at dealing with people so the engineers don't have to.",
              "score": 2,
              "created_utc": "2026-02-11 05:34:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u5sa4",
              "author": "andersdellosnubes",
              "text": "~~Well, I generally come in at least fifteen minutes late, ah, I use the side door - that way Lumbergh can't see me, heh heh - and, uh, after that I just sorta space out for about an hour~~.\n\nWell, I generally I ship Fusion Diaries a few days late, I work with Slack notifications off - that way u/dbt-jason can't ping me, heh heh - and, uh, after that I just sorta refresh Hacker News until links about data show up\n\nalternatively phrased: Developer Experience Advocacy!",
              "score": 1,
              "created_utc": "2026-02-11 17:56:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kkib8",
          "author": "Ramshizzle",
          "text": "Do you have any plans for improving how dbt deployment pipelines are configured, build and run?\n\nAt my company we are brand new to dbt-cloud. So far I really love dbt and all its capabilities. There is one issue I'm having, which is the only way it seems we can create an ETL pipelines from dbt-cloud is to manually click together a 'deployment job'.\n\nI am already experimenting with dbt-jobs-as-code, but while that is a great tool, it seems like that is still in early development.\n\nAt this moment we are considering getting an outside scheduler/orchestration tool. Which would be a shame in my opinion.",
          "score": 3,
          "created_utc": "2026-02-10 06:09:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4oruyl",
              "author": "infomocrat",
              "text": "terraform? [https://registry.terraform.io/providers/dbt-labs/dbtcloud/latest/docs](https://registry.terraform.io/providers/dbt-labs/dbtcloud/latest/docs)\n\nTalk: [https://www.getdbt.com/resources/coalesce-on-demand/coalesce-2024-why-analytics-engineering-and-devops-go-hand-in-hand](https://www.getdbt.com/resources/coalesce-on-demand/coalesce-2024-why-analytics-engineering-and-devops-go-hand-in-hand)",
              "score": 2,
              "created_utc": "2026-02-10 21:23:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u8jbq",
              "author": "schemas_sgski",
              "text": "as an orchestration nerd, I love this question - and welcome to dbt cloud! This is an area of dbt that we spend a lot of time thinking about, and one exciting feature we have recently shipped is [state-aware orchestration](https://docs.getdbt.com/docs/deploy/state-aware-about). It lets you build and run based on what‚Äôs actually changed (using state comparison and deferral), which can make pipelines a lot more efficient, especially as your project grows. There are also some [advanced configs](https://docs.getdbt.com/docs/deploy/state-aware-setup#advanced-configurations) that you can set up to customize at a more granular level and specify what actually gets run. This does require fusion. If you can share what you are looking for when scheduling jobs I'm happy to share some other functionality of the dbt orchestrator.",
              "score": 1,
              "created_utc": "2026-02-11 18:09:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m6uf5",
          "author": "Existing_Wealth6142",
          "text": "What is your perspective on open table formats like Iceberg,  Delta Lake, and Hudi? If you are positive on the technology, what do you think are the most important features still lacking from either the formats or dbt in order to maximize their value?",
          "score": 3,
          "created_utc": "2026-02-10 14:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u7ktd",
              "author": "andersdellosnubes",
              "text": "omg what a nerd snipe of a question!\n\nfor me what i'm most looking forward too are features that make it so that it just works for us analytics engineers! also my white whale forever has been a \"multi-engine stack\" with one iceberg catalog but heterogenous query engines.\n\nwhat's missing? support for the Iceberg Rest Catalog is still spotty amongst vendors, and the IRC itself needs some improvement (performance & federated, user-level authentication).\n\nif Iceberg and other formats are working as they should, we as analytics engineers should hardly ever have to thing of them!\n\nI also answered more in [this answer](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u3x71/) to u/Longjumping-Pin-3235",
              "score": 2,
              "created_utc": "2026-02-11 18:04:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ucvfm",
                  "author": "Existing_Wealth6142",
                  "text": "thanks for the response! A multi engine stack is also on my wishlist. Do you have a thought on which catalog best facilitates that? I've been wondering about either R2 or Lakekeeper as they seem like they might be credibly neutral",
                  "score": 2,
                  "created_utc": "2026-02-11 18:29:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lsn8q",
          "author": "GarpA13",
          "text": "Are you planning to abandon dbt core? What is your vision for on-premise software?",
          "score": 4,
          "created_utc": "2026-02-10 12:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u6yk6",
              "author": "dbt-grace",
              "text": "**TLDR;** No, we are actively shipping new features to dbt Core! \n\nWrote up a longer response [here](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/comment/o4u5986/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) :)",
              "score": 2,
              "created_utc": "2026-02-11 18:01:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n1bel",
          "author": "Ready-Marionberry-90",
          "text": "What problem do you exist to solve?",
          "score": 4,
          "created_utc": "2026-02-10 16:35:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u130q",
              "author": "andersdellosnubes",
              "text": "juicy! the thing that drew me to dbt originally was Tristan (founder/CEO) answering this question on the [Software Engineering Podcast: dbt](https://softwareengineeringdaily.com/2021/09/28/dbt-data-build-tool-with-tristan-handy-2/)\n\ndbt exists to solve the problem that data analysts don't have a career path beyond:  \n\"manage more dashboards\" or \"manage more people\".\n\nimho, dbt exists to solve that socio-technical problem and others common amongst data practitioners",
              "score": 1,
              "created_utc": "2026-02-11 17:34:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ju1xp",
          "author": "superconductiveKyle",
          "text": "Hey dbt Crew, I'm Kyle Eaton, Head of Growth at Agno (https://github.com/agno-agi/agno). I've been a big fan of your product for a long time now and specifically the community you all created. I'm the Former Head of Growth at Great Expectations, so I've been following for a long time.\n\nWe recently asked some of our users who we should partner with, and dbt came up. Seeing your AMA reminded me about this. Would love the chat with you all about what a partnership with Agno and dbt could look like! \n\nHope you have fun with your AMA! ",
          "score": 2,
          "created_utc": "2026-02-10 03:03:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u609j",
              "author": "andersdellosnubes",
              "text": "hey u/superconductiveKyle IIRC our paths have crossed before, at least i've been in GE slack for many years. feel free to reach out on LinkedIn would love to say hi and talk shop",
              "score": 2,
              "created_utc": "2026-02-11 17:57:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ubj2z",
                  "author": "superconductiveKyle",
                  "text": "Hello! That's awesome. Just sent a connect via LinkedIn. ",
                  "score": 2,
                  "created_utc": "2026-02-11 18:23:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lhwd6",
          "author": "finally_i_found_one",
          "text": "RemindMe! 4 days",
          "score": 2,
          "created_utc": "2026-02-10 11:19:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4li1k1",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 4 days on [**2026-02-14 11:19:40 UTC**](http://www.wolframalpha.com/input/?i=2026-02-14%2011:19:40%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/o4lhwd6/?context=3)\n\n[**2 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2F1r0ff3b%2Fama_were_dbt_labs_ask_us_anything%2Fo4lhwd6%2F%5D%0A%0ARemindMe%21%202026-02-14%2011%3A19%3A40%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201r0ff3b)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 2,
              "created_utc": "2026-02-10 11:20:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lim0t",
          "author": "set92",
          "text": "I'm deciding in moving my code from Airflow to Dbt using Cosmos. The idea is that instead of having custom sql code with jinja, I can move everything to dbt and let it run everything. \n\nI do this to improve in logging/debugging, and easiness. I suppose the speed/easiness is going to be there. But not sure about the logging part. Does dbt returns the output of queries, or is something that we can modify or specify in our own?",
          "score": 2,
          "created_utc": "2026-02-10 11:25:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ofuk2",
              "author": "Comfortable-Power175",
              "text": "Have you looked into dbt elementary? This is how we monitor model runs. FWIW we have an airflow + dbt + cosmos with elementary setup and are extremely happy",
              "score": 2,
              "created_utc": "2026-02-10 20:27:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4oqz3a",
                  "author": "set92",
                  "text": "Thanks, I'll check it. Although in general I was thinking more in the outputs itself. Like we have Snowflake, and the Operator of Airflow doesn't show you the results of the query by default, and in Snowflake itself only the owner of the query can check the results for 24h only. So, I thought maybe with dbt I could log this, like for example when I do COPY INTOs be able to see if a file has failed, and other information that comes on that results table, but I suppose the best will be to find some free moment and test it myself.",
                  "score": 2,
                  "created_utc": "2026-02-10 21:19:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u8xq8",
              "author": "dbt-grace",
              "text": "Hey! I would point you to a few different places: \n\n* [log jinja function](https://docs.getdbt.com/reference/dbt-jinja-functions/log) for logging custom messages\n* [exceptions](https://docs.getdbt.com/reference/dbt-jinja-functions/exceptions#warn) for raising warnings/errors\n* [dbt show](https://docs.getdbt.com/reference/commands/show) command for displaying a preview of results from a query\n* [documentation on events and logs](https://docs.getdbt.com/reference/events-logging)",
              "score": 2,
              "created_utc": "2026-02-11 18:11:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ualfm",
              "author": "andersdellosnubes",
              "text": "interesting! what are you looking for w.r.t. logging? Are you saying you to log the result of queries? Most folks don't, preferring to keep the data where it is.\n\n  \nI'm not sure if you've looked much into dbt Fusion, but it's [telemetry](https://docs.getdbt.com/docs/fusion/telemetry) is based on OTel, quite robust and just as extensible as dbt Core.\n\nlike u/Comfortable-Power175 said, there's also great dbt packages that help with logging and monitoring",
              "score": 1,
              "created_utc": "2026-02-11 18:18:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uwxqi",
                  "author": "set92",
                  "text": "Anything that could help me debug ETLs when something fail. When something fails I like to see what it ran, and what was the output of everything in case I can detect where it failed. I would not be able to understand everything, but seeing that a file of X customer didn't get ingested, or that X parameter was not set as I thought could help me debug the problems faster. \n\nBut yep, in Snowflake they told me that the results of queries were only visible for 24h by the owner of the query, no one else, not even an accountadmin could see the results. Which for me seems madness, and completely different than, if I'm not wrong, BQ, where anyone can see all the queries run, and that let you see/analyze any query and see what was processed, or didn't run as intended.\n\nFor copy into maybe the list of files, or be able to filter and only get the ones that failed to ingest, and trigger an alarm with it.\n\nAnother example could be when I run a MERGE on Snowsight (UI of Snowflake) it returns me the inserted and merged cols I think it is. I built some custom code to retrieve those values to sql variable and store them in a logging table. But it would be good if the tool has the functionality on its own. \n\nOr there is a query where I retrieve the max date of the ingested data, store it in a sql variable, and use it later. From what @dbt-grace said, I would be able to use [log jinja functions](https://docs.getdbt.com/reference/dbt-jinja-functions/log) to print those to the log, that way if we have a problem in the future I will be able to see which data we processed.",
                  "score": 2,
                  "created_utc": "2026-02-11 20:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4q9uw0",
          "author": "infomocrat",
          "text": "What have been your favorite Coalesce talks of all time?\n\nIf you could wish any talk into existence for a future Coalesce (ok, dbt Summit) what would it be?",
          "score": 2,
          "created_utc": "2026-02-11 02:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ubgz8",
              "author": "dbt-grace",
              "text": "I love it when community members give talks sharing their custom solutions! It's very inspiring to see what people are doing in the wild, and (selfishly) helpful when building out our roadmap for dbt Core. [Mariah Rodger's talk on testing](https://www.youtube.com/watch?v=hxvVhmhWRJA) is one of the many community projects that motivated us to build out-of-the-box support for unit testing into dbt :)",
              "score": 2,
              "created_utc": "2026-02-11 18:22:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u1kaf",
              "author": "dbt-zeeshan",
              "text": "Definitely u/dbt-grace [live-coding UDFs](https://youtu.be/aMUAQjqTKtc?si=vN4H-TEBf4EHpntw&t=1130) while roller blading!",
              "score": 1,
              "created_utc": "2026-02-11 17:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ub9ov",
              "author": "andersdellosnubes",
              "text": "The Magic School Bus themed talk with u/dbt-grace about dbt Mesh  \n[https://www.youtube.com/watch?v=FAsY0Qx8EyU](https://www.youtube.com/watch?v=FAsY0Qx8EyU)",
              "score": 1,
              "created_utc": "2026-02-11 18:21:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tuemk",
          "author": "Longjumping-Pin-3235",
          "text": "Iceberg  \nShould it be the default for a new project or greenfield snowflake environment?\n\nIs the complexity worth it?\n\nWhat catalog would you reach for for the most compatible multi-engine read / write?",
          "score": 2,
          "created_utc": "2026-02-11 17:03:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3x71",
              "author": "andersdellosnubes",
              "text": "This feels like the most important question re: Iceberg, huh?\n\nThere's lots of opinions here but my take: Iceberg isn't a free lunch.  \n  \nYou need a business case and the pros need to outweigh the drawbacks. This was actually discussed as well in [a recent episode ](https://www.getdbt.com/blog/apache-iceberg-and-the-catalog-layer)of the Analytics Engineering podcast. There's another episode due out soon where Tristan and I discuss the future of Iceberg.  \n  \nIceberg brings you the flexibility around where your data is stored and what compute you pick, but it adds complexity in having to manage an iceberg catalog. There are a few iceberg catalogs out there but many of them support ‚Äúpart‚Äù of the iceberg spec, and finding who supports what is not super easy.  \n  \nIf you are all-in on a data platform and won‚Äôt need cross data warehouse compute my take is that Iceberg would not be worth the effort today.\n\nHowever my prevailing opinion is that the Iceberg project is to data what standard-sized ship containers do for global trade. It's not necessarily \"exciting\" per se, but it has undeniable impacts on end users in that it's easier to get your ~~goods~~ data from point A to point B",
              "score": 1,
              "created_utc": "2026-02-11 17:47:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zi31j",
                  "author": "Longjumping-Pin-3235",
                  "text": "Thanks for the thoughtful reply u/andersdellosnubes !",
                  "score": 2,
                  "created_utc": "2026-02-12 14:29:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tujdn",
          "author": "infomocrat",
          "text": "With fusion and state aware orchestration, a lot of issues around multi-project orchestration can be solved. What problems do you see that \\*still exist\\* to be solved for more complex implementations of dbt involving multiple interconnected projects, and what are some strategies you see for addressing them?",
          "score": 2,
          "created_utc": "2026-02-11 17:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u5588",
              "author": "andersdellosnubes",
              "text": "short-answer: yes!\n\ncan you share what complex scenarios you've seen that you think can be addressed?",
              "score": 1,
              "created_utc": "2026-02-11 17:53:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4u97qc",
                  "author": "jtcohen6",
                  "text": "short-answer: seconded!\n\nthe challenges I keep hearing about are more \"socio-technical\" - improvements to dbt framework / platform / related tooling can help, but probably aren't sufficient in a vacuum:  \n\\- managing and discovering data products across multiple teams  \n\\- communicating data quality and freshness data contracts for public dbt models.  \n\\- sharing reusable macro code in (private) packages, with (semantic? calendar?) versioning for breaking changes to those macros (which are still untyped in dbt...)  \n\\- supporting multi-project deployments across \\*multiple data platforms\\* (think: upstream project on Databricks / downstream project on Snowflake, with a common Iceberg catalog across both)  \n\\- model-level access controls (more granular than project-level access)  \n\\- ... lots of other things I'm sure I'm not thinking/hearing about ...",
                  "score": 1,
                  "created_utc": "2026-02-11 18:12:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tyszp",
          "author": "Longjumping-Pin-3235",
          "text": "Did u/dbt-grace watch How To Trick People Into Thinking You can Juggle?  \n[https://youtube.com/shorts/trKLrl6y\\_Vc?si=Y9zMa2dQMt\\_ZCgU0](https://youtube.com/shorts/trKLrl6y_Vc?si=Y9zMa2dQMt_ZCgU0)",
          "score": 2,
          "created_utc": "2026-02-11 17:23:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u24fq",
              "author": "dbt-grace",
              "text": "LOL - you can actually thank my high school drama teacher for that one ([from deep in the archives](https://imgur.com/QakqCAT))",
              "score": 3,
              "created_utc": "2026-02-11 17:39:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mrppy",
          "author": "sleeper_must_awaken",
          "text": "How will you prevent companies being charged $350 / seat / month for an enterprise DBT Cloud subscription of three years when they need more than 8 seats? ¬†This is a real story I‚Äôve seen at two of our clients. That amount is just inordinate and betrays the trust companies and consultancies have placed in your DBT Labs.\n\nFor me, DBT Labs needs to regain my trust. I‚Äôve recommended many companies to use DBT Core and DBT Cloud since 2019, but am hesitant to continue recommending it.\n\nCompare that pricing with GitHub, which also has runners, incident management, traceability, collaboration, auditing and IdP integration (among many other things).",
          "score": 4,
          "created_utc": "2026-02-10 15:50:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kq035",
          "author": "Tall_Working_2146",
          "text": "Your certifications are interesting but the price is high, are there no initiative to make learning resources more accessible for students and people abroad?",
          "score": 4,
          "created_utc": "2026-02-10 06:56:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4oqywm",
              "author": "infomocrat",
              "text": "The learning resources are all free: [https://learn.getdbt.com/catalog](https://learn.getdbt.com/catalog)   \nCertifications are half off if you take them at coalesce, also partners can get discounts. But certifications are really only important in certain situations (for partners, if employer requires it to prove your knowledge). I wouldn't worry about the certs unless you need it. If you want to prove your knowledge, put up a personal project on your github.",
              "score": 2,
              "created_utc": "2026-02-10 21:19:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4uo2rq",
              "author": "[deleted]",
              "text": "Truly agree with others: ignore certs and just take the learning resources cause as others have said, they're all free!!",
              "score": 2,
              "created_utc": "2026-02-11 19:21:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o8pge",
          "author": "Ok-Sentence-8542",
          "text": "Why did you abandon dbt core and the open source community?",
          "score": 2,
          "created_utc": "2026-02-10 19:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u5986",
              "author": "dbt-grace",
              "text": "Hi! This is Grace (Product Manager of dbt Core). Can you say more about why you feel like dbt core and the open source community has been abandoned? This is the area of our product I think about all day, so I‚Äôm both sorry to hear that you feel a sense of abandonment and very open to feedback on how my team can revive your confidence that we remain dedicated to both dbt core and the open source community.  \n  \nI am proud of our two big releases of dbt Core last year (`v1.10` and `v1.11`) that included some features I‚Äôve personally been wanting to bring to life for a longtime now ([sample mode](https://docs.getdbt.com/docs/build/sample-flag) and [user-defined functions](https://docs.getdbt.com/docs/build/udfs), to name two). And, we‚Äôve added even more codebases to the ‚Äúopen source‚Äù bucket - we just re-licensed [metricflow under Apache 2](https://www.getdbt.com/blog/open-source-metricflow-governed-metrics), and Fusion includes some [net-new Apache 2 code](https://www.getdbt.com/blog/new-code-new-license-understanding-the-new-license-for-the-dbt-fusion-engine).  \n  \nIn case you missed it, we also put out a [roadmap update in December](https://github.com/dbt-labs/dbt-core/blob/main/docs/roadmap/2025-12-magic-to-do.md) in the dbt-core repo! I‚Äôd encourage you to give it and read, and let me know your thoughts.\n\nWhen I have heard this type of sentiment from other community members, it‚Äôs often coming from a feeling of frustration that we haven‚Äôt been as responsive to issues and pull requests in our open source repos over the past year. I feel that frustration as well and am working hard to get us back on track! Candidly, our team has been split across two focuses this past year: helping to build the new dbt Fusion engine and evolving the dbt framework across both Core and Fusion. I‚Äôm very excited to be onboarding a lot of new team members to dbt Core (this month!) to revitalize our activity in our open source repos and continue to execute on the dbt Core roadmap.",
              "score": 1,
              "created_utc": "2026-02-11 17:54:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ync46",
                  "author": "Ok-Sentence-8542",
                  "text": "Hi Grace, thank you for the transparent response. I truly appreciate the work your team is doing on features like UDFs and Sample Mode.\n\nHowever, I want to be candid: many of us have seen this pattern in the industry before, and we‚Äôve experienced it with dbt Labs specifically. It was the same with the MetricFlow server. While the logic was open, the essential infrastructure to run it effectively was kept behind a paywall.\n\nThis creates a sense that dbt Core is effectively being placed in maintenance mode‚Äîreceiving incremental updates‚Äîwhile the 'real' future and performance leaps, like the Rust-based dbt Fusion engine, are gated under the ELv2 license.\n\nTo a developer, it feels like dbt Core is the 'bait' to get us into the ecosystem, but the 'magic' that makes the tool 2026-ready is no longer truly open. If the most transformative innovations continue to happen outside of the Apache 2.0 scope, it‚Äôs hard to feel that dbt Core remains a first-class citizen rather than just a legacy entry point for dbt Cloud.",
                  "score": 3,
                  "created_utc": "2026-02-12 11:06:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tqp2d",
          "author": "J0hnDutt00n",
          "text": "Will there ever be more dynamic ways of orchestrating jobs in Cloud?",
          "score": 1,
          "created_utc": "2026-02-11 16:45:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4twpgq",
              "author": "andersdellosnubes",
              "text": "my interest is piqued -- can you share more about what you mean by \"dynamic\"?",
              "score": 1,
              "created_utc": "2026-02-11 17:13:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uh9cn",
                  "author": "J0hnDutt00n",
                  "text": "Sorry for the vague question, scenario: build fails on one model that succeeded yesterday and employ a WAP strategy. Will there ever be the ability to automatically re-execute a build but with that stale data model from yesterday to unblock those downstream models with an *asterisk of stale prod data model used? This already is essentially capable for child mesh projects but not others.",
                  "score": 2,
                  "created_utc": "2026-02-11 18:49:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tvna4",
          "author": "infomocrat",
          "text": "What do you think is the most under-appreciated feature of dbt core?",
          "score": 1,
          "created_utc": "2026-02-11 17:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tz803",
              "author": "andersdellosnubes",
              "text": "dbt seed! csv's (while ugly) aren't going away for anytime soon! when i started in data a decade ago. \"getting data in the database\" was like half the coding work to be done in my Jupyter notebook. \\`dbt seed\\` makes it so easy",
              "score": 1,
              "created_utc": "2026-02-11 17:25:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4ucd7m",
              "author": "dbt-grace",
              "text": "I am a big fan of all of the ways you can customize your dbt project to meet your org's specialized needs - specifically, I've seen a lot of creative problem-solving using [custom materializations](https://docs.getdbt.com/guides/create-new-materializations?step=1)!",
              "score": 1,
              "created_utc": "2026-02-11 18:26:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0fky1",
      "title": "Visualizing full warehouse schemas is useless, so I built an ERD tool that only renders the tables you're working on",
      "subreddit": "dataengineering",
      "url": "https://i.redd.it/706ayxj8yiig1.gif",
      "author": "Spiritual_Ganache453",
      "created_utc": "2026-02-09 20:29:55",
      "score": 138,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0fky1/visualizing_full_warehouse_schemas_is_useless_so/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4lr57a",
          "author": "ThroughTheWire",
          "text": "Pretty cool! think it could be useful for presentations for sure. maybe decent as a documentation tool as well.. I definitely do not want to ever manually draw or create UML tables ever again lol",
          "score": 9,
          "created_utc": "2026-02-10 12:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4odebs",
          "author": "DungKhuc",
          "text": "This looks very neat!\n\nOne thing though, besides showing relationship when pointing to a foreign key, what else does this do?",
          "score": 3,
          "created_utc": "2026-02-10 20:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ozeep",
              "author": "Spiritual_Ganache453",
              "text": "Yeah, here's the list.\n\n**Primary:**\n\n* Real-time SQL Canvas synchronization\n* MySQL parser with full syntax support\n* Drag-to-connect foreign key creation\n* Inline column editing on canvas\n* Drag-and-drop column reordering (swap/insert modes)\n* Customizable auto-formatting (indentation, casing)\n* Toggleable Brainstorm mode\n* Instant bulk rendering\n* Project & snapshots creation\n* Version history timeline\n\n**Upcoming:**\n\n* Cardinality support\n* Auto-generated SQL queries on connection hover\n* Diagram-wide search with auto-framing\n* Visual diff mode for team collaboration\n\nMore info: [sqlestev.com](http://sqlestev.com)",
              "score": 3,
              "created_utc": "2026-02-10 21:58:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o52v4nr",
          "author": "DoomsdayMcDoom",
          "text": "What library did you use for the canvas and diagrams?",
          "score": 1,
          "created_utc": "2026-02-13 00:23:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o536opn",
              "author": "Spiritual_Ganache453",
              "text": "Reactflow",
              "score": 1,
              "created_utc": "2026-02-13 01:32:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r15015",
      "title": "2026 State of Data Engineering Report - 1000+ responses from data engineers",
      "subreddit": "dataengineering",
      "url": "https://www.linkedin.com/posts/josephreis_recently-i-surveyed-1101-of-you-about-the-share-7426990778536583168-fqMr/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAajovEBZaTvKT0qIqHq9ItYb5C1EMVsVSY",
      "author": "DungKhuc",
      "created_utc": "2026-02-10 16:12:59",
      "score": 128,
      "num_comments": 8,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r15015/2026_state_of_data_engineering_report_1000/",
      "domain": "linkedin.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4qlhfo",
          "author": "dreyybaba",
          "text": "This is a lovely insight. Thanks for putting this together",
          "score": 8,
          "created_utc": "2026-02-11 03:31:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r3ata",
              "author": "DungKhuc",
              "text": "All credits go to Joe Reis, so I'm not putting anything together except cross posting the links from his discord channel :)",
              "score": 8,
              "created_utc": "2026-02-11 05:38:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sxjor",
          "author": "ironmagnesiumzinc",
          "text": "AI helping only 29% of respondents with debugging was extremely surprising. I think this is maybe the best part about LLMs",
          "score": 4,
          "created_utc": "2026-02-11 14:24:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xmn4r",
              "author": "Oxford89",
              "text": "It is THE most common use I have for AI. I don't even bother reading logs anymore. I just feed them to AI and get an answer faster than I would be able to scroll to the error.",
              "score": 2,
              "created_utc": "2026-02-12 05:24:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rvfui",
          "author": "rmoff",
          "text": "super useful. love the fully-functioning enterprise version too ü§£ https://joereis.github.io/super_corporate_pdm_survey/\n\n(Crystal reports, anyone?)",
          "score": 7,
          "created_utc": "2026-02-11 09:54:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s7av0",
              "author": "flashpoints80",
              "text": "He really went the extra mile!",
              "score": 3,
              "created_utc": "2026-02-11 11:38:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4x3kq9",
              "author": "shoppedpixels",
              "text": "I like the duplicate or erroneous data like Data Analyst listed twice or misspellings persisting, gives a feel of authenticity.",
              "score": 1,
              "created_utc": "2026-02-12 03:10:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4scz0g",
          "author": "observability_geek",
          "text": "I'm so surprised only to see that only 6.8% are using EDA and the big gap between enterprise usage and SMBs. \n\n# \n\n",
          "score": 0,
          "created_utc": "2026-02-11 12:21:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4j3bw",
      "title": "I‚Äôm planning to move into Data Engineering. With AI growing fast, do you think this career will be heavily affected in the next 5‚Äì10 years? Is it still a stable and good path to choose?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r4j3bw/im_planning_to_move_into_data_engineering_with_ai/",
      "author": "False_Square1734",
      "created_utc": "2026-02-14 12:10:21",
      "score": 115,
      "num_comments": 89,
      "upvote_ratio": 0.82,
      "text": "I‚Äôm planning to move into Data Engineering. With AI growing fast, do you think this career will be heavily affected in the next 5‚Äì10 years? Is it still a stable and good path to choose?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r4j3bw/im_planning_to_move_into_data_engineering_with_ai/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5bujal",
          "author": "ProfessorNoPuede",
          "text": "5 - 10 years? Dude, that time horizon is so far in the future, nobody knows. 5 years ago lakehouse wasn't mainstream yet. They just released the paper.\n\nIn 5 years, it could be everyone is on-premise again. Heck, it might the year of the Linux desktop.",
          "score": 311,
          "created_utc": "2026-02-14 12:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c1iuy",
              "author": "neoneat",
              "text": "After 4 years of uni, working market changed alr. I dunno how to say about 5 years term, unless telling a lie.",
              "score": 29,
              "created_utc": "2026-02-14 13:09:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d6wnn",
              "author": "Awkward-Cupcake6219",
              "text": "What I really like of this comment is \"it could be everyone is on-premise again\". I see so many companies using databricks for at most hundreds of megabytes of data. Like running over a tree with a tank to clear some farming land.",
              "score": 14,
              "created_utc": "2026-02-14 16:59:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ekwpn",
                  "author": "geek180",
                  "text": "Why do people assume small data = cloud services are pointless or ‚Äúoverkill‚Äù?\n\nCloud services are *perfect* for small teams and small projects. It‚Äôs all consumption based billing that scales with you and eliminates a lot of infra setup and maintenance that small teams don‚Äôt want to deal with.",
                  "score": 10,
                  "created_utc": "2026-02-14 21:19:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5flm4v",
              "author": "ThatSituation9908",
              "text": "Can someone cite the paper? Thanks in advance¬†",
              "score": 1,
              "created_utc": "2026-02-15 00:55:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gq3z5",
              "author": "virgilash",
              "text": "I have the feeling 2026 is going to be the year of Linux workstation ü§£",
              "score": 1,
              "created_utc": "2026-02-15 05:46:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bypcm",
          "author": "Flat_Mammoth_7010",
          "text": "My 2 cents is to learn more on domain knowledge and to understand the business meaning of data.",
          "score": 104,
          "created_utc": "2026-02-14 12:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5czf80",
              "author": "543254447",
              "text": "this is also a risky move. Once you move outnof the domain, all wasted effort. Be sure to commit to an industry before doing this.",
              "score": 29,
              "created_utc": "2026-02-14 16:22:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dymyv",
              "author": "TaartTweePuntNul",
              "text": "And the basics, never forget the basics of DE! Principles of DB management, how to handle different granularities, batch v stream,... These ideas always stay around, same with SWE best practices.\nOnce you have these two you can grow into whatever direction you want. And specializing in a specific domain will amplify this even more.",
              "score": 11,
              "created_utc": "2026-02-14 19:19:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eekwi",
                  "author": "pinamanpongole",
                  "text": "Can you suggest resources for someone who wants to become a data engineer. I do have access to aws skill builder but I fear whatever I learn there will be domain specific.",
                  "score": 1,
                  "created_utc": "2026-02-14 20:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d4v2j",
              "author": "SpareSmileBravo",
              "text": "Could you elaborate more on why more focus on domain would be helpful?",
              "score": 2,
              "created_utc": "2026-02-14 16:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d5bfa",
                  "author": "Reach_Reclaimer",
                  "text": "Why wouldn't domain knowledge be helpful? Unless you're doing pure bronze layer integration or API calling, you need to know about the data you're working with",
                  "score": 5,
                  "created_utc": "2026-02-14 16:51:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5budz2",
          "author": "AverageGradientBoost",
          "text": "we actually had a engineering all hands this week and one of the topics that came up was how the difference in AI for software engineering and data engineering is quite big and AI has a lot of catch up to do in DE. Lots of people complaining that it struggles to build queries and work with data. ",
          "score": 86,
          "created_utc": "2026-02-14 12:15:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c1ko5",
              "author": "PaddyAlton",
              "text": "This is a good point.\n\nI think this is because the behaviour of systems that use data is defined by the code, the schema, and the actual data values.\n\nYou can document schemas, but the more vague the constraints on what the data might look like, the harder it is to build something robust and the more context switching (between code and the actual contents of the upstream source) is required.\n\nNevertheless, I don't think we should rely on this as a 'moat'. It's not a fundamental constraint, more of a context engineering problem - one which people are working on solving. In the last month I've seen the emergence of agentic data analytics implementations that finally look promising. I expect some of these use cases to be cracked by the end of this year.",
              "score": 28,
              "created_utc": "2026-02-14 13:10:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cj4sn",
                  "author": "jupacaluba",
                  "text": "To a certain extent, you‚Äôre right. However, I‚Äôve used Claude to build full transformation logic and I was shocked at how good it was. \n\nIt‚Äôs a game changer when you don‚Äôt need to worry about coding anymore.",
                  "score": 9,
                  "created_utc": "2026-02-14 14:57:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5g4fsz",
                  "author": "NoleMercy05",
                  "text": "It's a terrible and incredibly incorrect point.",
                  "score": -1,
                  "created_utc": "2026-02-15 03:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cb20p",
              "author": "nineteen_eightyfour",
              "text": "I am in our works ai workgroup and I think the biggest issue is that people suck. I‚Äôm our data scientist and I use to work for a consulting company. So I‚Äôd work with Google 1 year or oracle 1 year so I got lots of exposure. The only similarity is that everyone sucks at entering data correctly ü§£",
              "score": 10,
              "created_utc": "2026-02-14 14:10:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cgdzz",
                  "author": "anti_humor",
                  "text": "Lol yeah - even if you give it access to all the relevant docs, it won't help because the docs are incorrect. Gotta love it.",
                  "score": 1,
                  "created_utc": "2026-02-14 14:42:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cfzqt",
              "author": "anti_humor",
              "text": "I think a huge issue here is data quality. Garbage in garbage out, same as always - sometimes I think hardcore AI evangelists have never worked with messy data in the wild lol. \n\nThe issues I see in data I have to write pipelines for range from things I could see an LLM spotting, to completely normal looking data that is incorrect for super idiosyncratic reasons, to CSVs that are broken in ways that I literally made chatGPT short circuit and enter a death spiral trying to solve. \n\nIt might get there soon, but as of now nontechnical human beings touching data are more chaotic than LLMs seem able to neatly account for. Some of the information needed to contextualize all of the relevant business logic is also risky in terms of exposing to an LLM you aren't spinning up yourself. \n\nThat all being said - they definitely save me a ton of time with things like writing DDL based on a file sample or spitting out SQL and application code syntax I don't feel like looking up. I just haven't seen any evidence they could write any of the pipelines I own effectively without giving it so much context I could've just built it myself in the same time.",
              "score": 5,
              "created_utc": "2026-02-14 14:39:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5cdlc8",
              "author": "brittleirony",
              "text": "Only because they lack context about the structured data. Plenty of companies pushing agents that have decent accuracy querying structured data with suitable context (metadata, descriptions etc). That being said I haven't seen an agent writing pyspark or building a jobs (should be possible with MCP)",
              "score": 4,
              "created_utc": "2026-02-14 14:25:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5e9d6b",
              "author": "Express-Patience8874",
              "text": "There was actually a post not too long time ago. Apparently, AI agent used data and made bunch of false claims. No one checked. VP level made certain moves based on that data lol",
              "score": 2,
              "created_utc": "2026-02-14 20:16:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d027g",
              "author": "TH_Rocks",
              "text": "Microstrategy Mosaic is supposed to be able to flesh out a full schema by itself.  \nMicrostrategy already writes great SQL if the schema is properly defined.\nWe haven't upgraded yet to really test it out.",
              "score": 1,
              "created_utc": "2026-02-14 16:25:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5f22cd",
              "author": "spoopypoptartz",
              "text": "claude works perfectly for me but my team‚Äôs docs and data modeling are on point. \n\n\ncurrently experimenting to see if i can get acceptable performance on local models since i need similar capabilities in an air-gapped environment",
              "score": 1,
              "created_utc": "2026-02-14 22:54:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5fkqjn",
              "author": "idiotlog",
              "text": "This is not true. Context for the AI is just bad.",
              "score": 1,
              "created_utc": "2026-02-15 00:49:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gum6m",
              "author": "Faintly_glowing_fish",
              "text": "This is not an ability problem but rather an integration problem.   In most cases the AI just can access the same data people can and they end up having no way other than hallucinating.   When given the ability to actually access data I find it doing fine.  The hard part is to get it access while maintaining proper way of sandboxing and audit",
              "score": 1,
              "created_utc": "2026-02-15 06:26:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5cm6t8",
              "author": "_weined",
              "text": "Everything around building jobs, provisioning infrastructure, automating a portion of quality checks, and especially cookie cutter ingestion is ripe for AI. The business logic layer is not so much at risk for complex enterprises but a lot of the surrounding work DEs typically do is already able to be commoditized by AI.",
              "score": 0,
              "created_utc": "2026-02-14 15:14:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dmrx0",
              "author": "nowrongturns",
              "text": "You must not work at big tech. It‚Äôs a solved problem. I just prompt with business logic and give it context and it authors near perfect sql.",
              "score": 0,
              "created_utc": "2026-02-14 18:19:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5g4d78",
              "author": "NoleMercy05",
              "text": "![gif](giphy|ko4UuHFAOZE3jN3qRB)",
              "score": 0,
              "created_utc": "2026-02-15 03:01:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bzp4v",
          "author": "PaddyAlton",
          "text": "Two big uncertainties re: AI -\n\n1. speed of takeoff: do compounding gains make performance improvements accelerate over the next year, or do fundamental bottlenecks start to put the brakes on?\n2. performance ceiling: just _how_ good can LLM-based models get (disregarding whether they get there quickly or slowly)? \n\nI think even if AI foundation models get no better from now on, they are _already_ good enough that harnessing them and rolling them out across industry will lead to a shift in ways of working on the scale of the invention of the world wide web. So if you take this path, assume you're going to be very focused on data-engineering-applied-to-AI.\n\nNow, given the actual pace at which human institutions move, I am convinced that even if foundation models get _quite a lot better_ it will take five years to reorient a bunch of legacy businesses around the new technology. But from there, all bets are off.\n\nA reorientation around AI may well make _experienced technologists_ even more important as staff for competitive businesses, but don't expect 'data engineering' specifically to still be prominent (consider how the database administrator role has faded in salience due to the Cloud revolution, for comparison).\n\n---\n\nAll in all: don't let AI put you off. But go in with your eyes open: none of us can make precise predictions about 2035, and even on the shorter horizon you need to prepare to be flexible and stay abreast of this new technology.",
          "score": 21,
          "created_utc": "2026-02-14 12:56:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bv4fc",
          "author": "i_fix_snowblowers",
          "text": "No question that DE will be affected by AI.\n\nBut as the economists say there are \"substitution effects\" and \"income effects\".\n\nSubstitution effects are the first-order job displacement things that you're asking about.\n\nIncome effects are the overall increase in data processing as data engineering tasks become cheaper to execute. \n\nIf I'm being optimistic, I'd say that the overall increase in data processing would lead to growth in DE, it's just that the day-to-day for a DE will change. Like instead of manually fixing 5 pipelines, you monitor 50 pipelines and manage the agents that fix them.",
          "score": 17,
          "created_utc": "2026-02-14 12:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bzzth",
          "author": "i_hate_budget_tyres",
          "text": "DE is more insulated than SWE because LLM's scraping the net can't know business domain information.\n\nYou'd have to develop custom LLM's trained on data across the business that can ingest documents, databases, the codebase etc.  And keep a no doubt very expensive team to keep tuning and updating the model as business requirements change.\n\nWhere I work, SWE's are finding 'generic' LLMs much more useful than DA's, DS's and DE's.  All apps tend to resemble each other and complete codebases for all kinds of apps are available to scrape on the internet, so the models have a wealth of solutions to fall back on.\n\nAs a DE I'm finding its more a glorified google search.  If I unleash it in agentic mode, it utterly balls stuff up.  It can't see the databases nor final dashboards etc so is missing a lot of information that I hold in my head and get a steer on from SME's across the business.  This information is proprietary and often just held in peoples heads, so there is no way a 'generic' LLM would have access to it.\n\nHaving said this, I think DE‚Äôs working in smaller companies with less complex tech stacks and environments and are finding ‚Äògeneric‚Äô LLM‚Äôs more useful.  I work in a multi-cloud, multi vendor environment.  We have every permutation of pipeline you can imagine weaving its way in and out of multiple platforms.  There is no way a ‚Äògeneric‚Äô LLM can see across all of it so its usefulness is limited.",
          "score": 28,
          "created_utc": "2026-02-14 12:58:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d3vy8",
              "author": "slayerzerg",
              "text": "Pretty much. Esp industries with sensitive data. DE work is hard",
              "score": 3,
              "created_utc": "2026-02-14 16:44:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cf6s3",
          "author": "SchemeSimilar4074",
          "text": "I think DE will become a jack of all trades, with AI advancing as it is. Business people will dabble here and there and might do their own dashboards like a PowerPoint but they have too many meetings to actually do technical works. There are many components but with the help of AI, a very small team can do it all, from deploying cloud infrastructure, testing, data engineer etc.¬†\n\n\nSo a DE will be expected to know SRE, SDET, DE, DA and even ML. A few years ago, my job was heavily DE and DA and now it also involves building infrastructure and test suite, which are the jobs of SRE and SDET. Its much easier to build pipelines and dashboards these days. It means more people from other tech areas transition to DE, increasing the competition. For people in the industries, as long as you're expanding your skill sets, it's OK. For someone new, it'll hard to break into, because you might even be competing with SWE, SDET etc who lost their jobs and tried to transition to DE for example.¬†\n\n\nIf you wanna get into tech, be prepared to learn everything from every areas. And save a lot, just in case üòÇ",
          "score": 8,
          "created_utc": "2026-02-14 14:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bzzlq",
          "author": "TripleBogeyBandit",
          "text": "I disagree with most of the comments here. I do the more complex data engineering; Kafka, spark, building api services, touching dozens of cloud resources, etc..  ai use to suck at doing these things a year ago but now, it‚Äôs kicking ass. I‚Äôve written entire pipelines, api services, even small business apps with Claude code. I think the people that are trashing don‚Äôt know how to leverage the ai tools and feel threatened by their capability. You need to learn how to use these tools effectively or you‚Äôre gone.",
          "score": 28,
          "created_utc": "2026-02-14 12:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ce1ue",
              "author": "danielfrances",
              "text": "I agree, with some caveats:\n- Even if you learn how to use these tools effectively, the speed of improvement with agentic tools is insane and we might end up redundant anyways.\n- I think another category of people are ones like me - I know how to leverage the tools and see their utility while also feeling threatened by their existence.\n\nA lot of times when this topic comes up people are judging agentic tools by their capabilities right now. At my job, we test drove a bunch of AI tools last summer and it was a total fail with our codebase. I personally started seeing value around October/November. Two weeks ago our entire developer team got told we need to use Claude every day - and honestly, it is SO MUCH better today than even a few months ago.\n\nMy point is - when you see people saying things like \"it can't really handle xyz right now\" just know that statement might be false in as little as 3-6 months.\n\nAs such, I don't think we can even make predictions 2 years out. Maybe the cost of research kills a bunch of the big companies and the progress slows. Maybe the cost of subs goes so high we can't afford it. Or, maybe things keep going as they have been and agentic tools can fully automate our jobs in 2 years. We can't know right now, but you should prepare for the possibility that the hype is real, just in case.",
              "score": 5,
              "created_utc": "2026-02-14 14:28:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dc73p",
                  "author": "TripleBogeyBandit",
                  "text": "You‚Äôre spot on, anyone who writes these tools off is a fool.",
                  "score": 2,
                  "created_utc": "2026-02-14 17:26:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5f1vyw",
                  "author": "Murtz1985",
                  "text": "Exactly how I see it. Just in case.",
                  "score": 1,
                  "created_utc": "2026-02-14 22:53:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5eq05p",
              "author": "rotterdamn8",
              "text": "For sure, definitely agree that people don‚Äôt realize how fast things are moving. \n\nMy company gave us the tools last year and I‚Äôve been using them as the low hanging fruit coding assist, like ‚Äúrewrite this pandas code for pyspark‚Äù. Basic stuff.\n\nBut now I keep hearing about Claude Opus 4.6 since it came out a few weeks ago, and I realized holy shit, I need to catch up.\n\nOP, my advice is to learn how to use the tools, especially Claude. Forget 5 years, things are changing quickly. It sounds like some DEs here are sleeping on it. That would be a mistake.",
              "score": 4,
              "created_utc": "2026-02-14 21:46:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g409s",
                  "author": "CluckingLucky",
                  "text": "Is this how the world felt when aws was launched? lol",
                  "score": 1,
                  "created_utc": "2026-02-15 02:58:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ccxdn",
              "author": "bubzyafk",
              "text": "I think you are right to some extent, but the other people is also right, particularly on Business domain.\n\nAI can do that exact math stuffs, e.g: Build API to connect based on SaaS documentation etc. it‚Äôs a straight forward engineering with error or non error as the output.\n\nBut asking AI to replicate complex business knowledge and on top of it, some ERP complexity, is still way long to go or even impossible without Human intervention. E.g: Theoretically Net Income = revenue - Expenses, but imagine apps A provides Column ABC and XYZ to calc Expenses, and ERP B Provides col C123 and D456 to calc revenue. Requires some window function, sum, and what not.. And col name is not very descriptive enough to tell it‚Äôs to calc Revenue, giving 0 context for AI to process\n\nSo, yeah.. in data engineering, AI won‚Äôt 100% do the magic.. it can help us faster to build API, make some Pyspark/scala/whatever code, sql (if connected with our metadata), give some reasoning/exploration, etc.. but it can‚Äôt remove human entirely.",
              "score": 2,
              "created_utc": "2026-02-14 14:21:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dc3nr",
                  "author": "TripleBogeyBandit",
                  "text": "What you‚Äôre talking about is a semantic layer and there are plenty of great examples of ai delivering exceptional performance and value when given access to a semantic layer.",
                  "score": 3,
                  "created_utc": "2026-02-14 17:26:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cnuqg",
              "author": "IAMHideoKojimaAMA",
              "text": "I dont think that's more complex de, thats like intro to de sruff",
              "score": 0,
              "created_utc": "2026-02-14 15:23:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cy4yv",
          "author": "Suspicious-Spite-202",
          "text": "Figure out how to do machine learning over relational databases and graph databases ‚Äî relational deep learning.  Also look into graph databases as semantic layers.  Finally, figure out exploratory data analysis (eda) and visualizations via an Llm interface.  \n\nStar schemas and kimball techniques are still relevant for having a stable backbone. Basic principles of modular etl are still relevant too.",
          "score": 5,
          "created_utc": "2026-02-14 16:16:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bv0ll",
          "author": "takenorinvalid",
          "text": "I have a theory that the data engineers working on AI deliberately sabotaged its ability to do data engineering to make sure they'd be the only people with job security.¬†",
          "score": 17,
          "created_utc": "2026-02-14 12:20:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5catj6",
          "author": "PeterDowdy",
          "text": "So far, AI coders are bad at data engineering because it‚Äôs hard for them to reason about the size of data - they can get the structure and transformations right but they don‚Äôt do scaling and performance.",
          "score": 3,
          "created_utc": "2026-02-14 14:09:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c1q26",
          "author": "Illustrious-Pound266",
          "text": "It's already impacted data engineering. Truth is that you need to learn how to use AI in development now",
          "score": 5,
          "created_utc": "2026-02-14 13:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c1wnf",
              "author": "False_Square1734",
              "text": "Is that possible to replace data engineers then?",
              "score": 1,
              "created_utc": "2026-02-14 13:12:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5c35cw",
                  "author": "n_ex",
                  "text": "No, because you still need to guide the AI to implement the solution - though data engineers are becoming faster at implementing things (as all other developers) so companies might need less of them down the line I guess",
                  "score": 7,
                  "created_utc": "2026-02-14 13:21:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ckpj7",
                  "author": "jadedmonk",
                  "text": "AI doesn‚Äôt just replace people. It‚Äôs not like you can plop an AI on a chair to replace the human. Even the best ones are only at like ~80% benchmark - imagine if you wrote wrong production code 20% of the time how poorly things would go. \n\nAI is here to make us more efficient, and it does. Perhaps as those efficiency gains are made they‚Äôll hire less engineers, but I‚Äôve seen the opposite as they need more engineers to build and maintain the AI",
                  "score": 3,
                  "created_utc": "2026-02-14 15:06:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5d4w9d",
                  "author": "SRMPDX",
                  "text": "Data engineers who use AI will replace those who don't",
                  "score": 2,
                  "created_utc": "2026-02-14 16:49:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5buftv",
          "author": "ThroughTheWire",
          "text": "nobody can tell you the answer for this. In my semi informed opinion I think we will have a role for a period of time in creating pipelines for data to feed AI models but that will only last for so long and may not be a role available to you by the time you switch. Otherwise I think there will be data engineering jobs and software engineering jobs over the next 10 or so years while AI technology advances exponentially and then who knows where we are after that. I wouldn't bet my entire life on moving into any software engineering discipline at this point.",
          "score": 7,
          "created_utc": "2026-02-14 12:15:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c0pw3",
          "author": "AdministrationAny136",
          "text": "In the future, I'd also focus more on domain knowledge.",
          "score": 2,
          "created_utc": "2026-02-14 13:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cg9kf",
          "author": "randomperson32145",
          "text": "Toolbox and tools, those change.",
          "score": 2,
          "created_utc": "2026-02-14 14:41:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ch7c8",
          "author": "gn-musa",
          "text": "Fancy AI eats data. Someone's got to build the damn pipelines.",
          "score": 2,
          "created_utc": "2026-02-14 14:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dl8j5",
          "author": "TheCamerlengo",
          "text": "5-10 years? Even before the emergence of AI and LLMs, that was a long stretch of time. I don‚Äôt think data engineering will be a thing recognizable in 5 years as to what it is today.",
          "score": 2,
          "created_utc": "2026-02-14 18:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e6ens",
          "author": "reviverevival",
          "text": "24 months ago, the smartest people in the world could look at the evidence and call LLMs \"auto-complete on steroids\". I did not agree at the time, but it was a credible assertion. If anyone is still saying that in 2026, they are not credible based on the research that's come out since then. In 2024 people said LLMs can't solve Wordle, and they can't count the Rs in strawberry. No frontier model fails at those tasks in 2026 but people are still repeating the same memes.  \n  \nPeople who are not using AI to (effectively) code think it's supposed to be some kind of maestro that 1-shot generates you a script out of thin air. I would characterize the latest iteration of AI to be more akin to a swarm intelligence. Even a single frontier model agent is not very impressive by itself. I've found that even given a well defined list of tasks, a single Claude agent will degrade rapidly in performance because of context rot. But if you start orchestrating: creating and destroying agents for singular purposes (cattle, not pets), then the capabilities become very impressive. Your agents can make a plan, execute on it, _check the results_, adjust the plan, retry, and so forth all autonomously. This is the scary part, not because it's really good at leetcoding, because planning and iteration is something that's transferrable to all professions.  \n  \nSecondly people misunderstand, especially after the ChatGPT-5 fiasco: GPT-5 is not a single model, it is a router, a reverse proxy, that assesses your query and routes it to one of its 10s of actual models behind the curtain. Despite the marketing it was primarily a cost engineering tool for OpenAI, because it removes choice from the user about what model you are being served with. So you could be using \"ChatGPT-5.2\", and you get some shit answer because it decides your question wasn't worth its time (or their servers are overloaded, or any other reason) and routed your question to some shit model. All frontier \"models' work this way, \"Opus 4.6\" does the same. So all those seemingly cracked models that Google is sending out to dominate benchmarks? Yeah, no guarantee that's the one serving you (or is even one of the possible options).\n  \nThat means you cannot one-shot some chatbot on your free account and have an accurate picture of the state of the art. Anyone who is isn't using harnessed agents and burning tokens have no idea what they're talking about.  \n  \n18 months ago I thought the role of humans will be as model whisperers, guiding these mercurial and finicky chatbots. Well, prompt engineering came and died as a profession in less than 12 months. 3 months ago I thought the role of humans would be building mcp integrations to connect agents to the business domain. Well, Claude is pretty well capable of being pointed at any API and building an integration by itself right now.  \n  \nCurrently I would say my comparative advantages are understanding the business domain, architecture, and security. If you notice, none of these are junior attributes. As an OG AI hater, it pains me to say that if you gave me a list of projects to complete, and I had a choice between an intern or Claude Code and an intern's salary of tokens, for effectiveness I would take Claude Code every time hands down. (I'm not saying that the AI is better in every way, rather there's no comparative advantage).\n  \nAnd I don't even know how long my moats will hold. There is a degree of skill involved with using agents, which can explain why some people are getting poor results, but tbh the skill curve is not that high, and any clever tricks people think of to squeeze more performance out of the agents (e.g. chain-of-thought, ralph-loop, sub-agents) just get baked into the harness and democratized. I think the only reason people don't get better at it is a psychological aversion to AI.\n\nAgainst all odds, my company managed to assemble a competent team of AI engineers that built an in-house agentic framework that _actually delivered business value_...for about 12 months. Now I wonder what was the whole point if every analyst will be rolling with Claude Cowork on their laptop in 6-12 months. If you look at where we were just 24 months ago vs now, it's hard to imagine where we'll be 24 months from now.  \n  \nSo all that to say, we're all guessing for the next 5 years. My hope is a small business renaissance and a democratization of knowledge, but I fear we just end up in a world where 12 rich guys own everything until one day they betray each other. But I wanted to give a deep and nuanced answer, and get my own thoughts out. We can only nudge the world to the better scenario if we're honest to ourselves.",
          "score": 2,
          "created_utc": "2026-02-14 20:00:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fb1vx",
          "author": "rambouhh",
          "text": "Data is the backbone of ai, and ai is worthless without good data. Worry about learning principals, the business meaning of data, and you are going to be better off than 95% of people even if the job itself is gone or looks completely different in 5 years",
          "score": 2,
          "created_utc": "2026-02-14 23:49:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fikc4",
          "author": "CluckingLucky",
          "text": "Ok, so there are two ways to answer your question.\n\nFirst off, as u/ProfessorNoPuede pointed out, no one knows what will happen five or ten years into the future in terms of tooling or capability. No one knows what the industry is going to be calling for.\n\nHowever, and this leads to the second way to answer your question--- look at the fundamentals of what drives demand for data, and, consequently, what drives demand for data engineers.\n\nIt's these things that suck up a lot of data to deliver correlative approximations that can never be accurately used in the exact processes  data engineers excel in. LLMs.\n\nSo, synthesising this: yes, the tooling will change, but data engineers will only be riding this AI bubble as much as they have been already. Maybe more of them will be data scientists. Maybe more of them will be AI, but I doubt it.\n\nThere might be less of them. If that's the case, those that remain (or enterprise) will be very highly paid.\n\nSincerely,\n\nNo one smarter than your average CEO thought leader",
          "score": 2,
          "created_utc": "2026-02-15 00:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d60ua",
          "author": "SRMPDX",
          "text": "AI is already heavily affecting DE. There would be no AI without DE. Figure out how to use it, and how to market your knowledge to people who need it in their organization. \n\nIn AI we're seeing huge changes in 5-10 months, nobody knows what's coming in 5-10 years.",
          "score": 2,
          "created_utc": "2026-02-14 16:55:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cu6y6",
          "author": "shittyfuckdick",
          "text": "Bro im losing my mind you realize this thing is just autocomplete right? The only people saying its gonna replace devs are the CEOs of AI companies who are trying to pump the stock.¬†",
          "score": 3,
          "created_utc": "2026-02-14 15:56:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5drn5a",
              "author": "alexlazar98",
              "text": "Looking at the rest of the comments, this is really not your audience, lol. The blinders are on.",
              "score": 0,
              "created_utc": "2026-02-14 18:43:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fcifx",
                  "author": "shittyfuckdick",
                  "text": "Are you saying the blinders are on me? I could be totally wrong and im preparing for worst case scenario. but i really think most of AI is driven by hype.¬†",
                  "score": 1,
                  "created_utc": "2026-02-14 23:58:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5c7tx6",
          "author": "andrew2018022",
          "text": "I think it‚Äôs the most ai proof of the ‚Äúdata‚Äù domains because there is so much analytics slop on social media nowadays, the barrier to entry of analytics and BI is so low. DE requires more technical knowledge you can‚Äôt just vibe code solutions",
          "score": 1,
          "created_utc": "2026-02-14 13:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cbu19",
          "author": "hibe1010",
          "text": "Of course it will be affected- a job like that will always and was always heavily affected by latest emerging technology¬†\n\nYou will need to stay on top of those then I am very sure it is still an interesting and challenging field to choose - just don‚Äôt expect that your skill set will not have to change over time¬†",
          "score": 1,
          "created_utc": "2026-02-14 14:15:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ci7g4",
          "author": "DenselyRanked",
          "text": "It's a difficult question for anyone to answer. Data engineering as a practice will still exist but the methods, tools, and skill set needed will evolve. \n\nThere are smart people putting a lot of thought into this and I tend to agree with much of [this presentation](https://www.youtube.com/watch?v=Fu6JBodxqGQ), where there is not going to be a Data/Analytics Engineer title in the near future for data teams, opting instead for titles like Data Product Owners and Data Domain Experts. AI can help close the technical gap between product management and engineering, so DE's will need more emphasis on stakeholder communication and requirements gathering. ",
          "score": 1,
          "created_utc": "2026-02-14 14:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ct19x",
          "author": "Lastrevio",
          "text": "Between \"it will replace me\" and \"it's not helping me at all\" there will have to be a transitory phase where it will increase your productivity. So far, AI is barely helping in making me 5% more productive, and I'm doing quite simple things. I'll worry that it's gonna replace me when it will double my productivity, until then it's next to useless.\n\nMoreover, there are academic studies showing that programmers are 20% *less* productive using AI.",
          "score": 1,
          "created_utc": "2026-02-14 15:50:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cu9nk",
          "author": "rajekum512",
          "text": "If data engineering is falls under \"white collar\" jobs then it would be safe to be classified under endanger threat",
          "score": 1,
          "created_utc": "2026-02-14 15:56:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cx82c",
          "author": "billysacco",
          "text": "My opinion it‚Äôs already starting to be affected with many employers going to the offshore combined with AI route. It will probably take a few years to come back around, the off shoring stuff always seems to go in a circular cycle. Or the AI is as good as they say (I am still skeptical on that) and things don‚Äôt come back around. Hard to say.",
          "score": 1,
          "created_utc": "2026-02-14 16:11:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dvtkm",
          "author": "melvinroest",
          "text": "Yea, I think doing data engineering is AI engineering-lite in some cases",
          "score": 1,
          "created_utc": "2026-02-14 19:04:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dy6d5",
          "author": "frozengrandmatetris",
          "text": "in 5 to 10 years, the number of orgs locked into legacy low-code ETL tools like ODI and SSIS will be almost exactly the same",
          "score": 1,
          "created_utc": "2026-02-14 19:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e0ku1",
          "author": "Least-Possession-163",
          "text": "DE or anything code driven will be impacted. Companies might ask you to do everything end to end. AI augmentation will lead to fewer jobs (leaner team) but more work. 5 years is too long. 5 years back NLP was a big deal and now people have AI girlfriends. So take your bet.",
          "score": 1,
          "created_utc": "2026-02-14 19:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e656w",
          "author": "ImpressiveProgress43",
          "text": "AI is heavily affecting data engineering. It is still stable and good. I havent seen a single year where de's werent over demanded in my area.",
          "score": 1,
          "created_utc": "2026-02-14 19:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e7ksf",
          "author": "decrementsf",
          "text": "<< any professional role >> Will be affected in the next 5-10 years.\n\nFor << any professional role >> study the profession, and practice applying the current AI trends to it.",
          "score": 1,
          "created_utc": "2026-02-14 20:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h911d",
          "author": "po1k",
          "text": "It you want smth stable go with c/c++. DE is tough and far away from being stable. At this point I realized c++ might easier",
          "score": 1,
          "created_utc": "2026-02-15 08:43:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c6g4r",
          "author": "Nekobul",
          "text": "I think when the LLMs start to perform brain surgeries on their own, you will have to start worrying if your job is next. Until then, breathe! Everything will be fine.",
          "score": 1,
          "created_utc": "2026-02-14 13:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cjs47",
          "author": "itsMineDK",
          "text": "I wouldn‚Äôt do it, but that‚Äôs just me.. a lot of tech folks were making bank on software engineering when i started uni, I wanted to go there but I didn‚Äôt and did finance instead.. that was in 2008 l, always regretted UNTIL NOW.. \n\nit‚Äôs been rough for SEs in general but things might turn around nobody has a crystal ball",
          "score": 0,
          "created_utc": "2026-02-14 15:01:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5guc05",
          "author": "Faintly_glowing_fish",
          "text": "Yes it will be very heavily affected in the future.  By 5 years much of what DE does today will be completely gone.  Some of the skills will be useless, and some of the things will be as pointless as formatting the code with your hand. But that is fine really.\n\nThe distinction between different kinds of engineering will break down a lot and you will be doing very different things",
          "score": 0,
          "created_utc": "2026-02-15 06:23:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gv5hw",
              "author": "False_Square1734",
              "text": "Then do u think IT jobs will be gone?",
              "score": 1,
              "created_utc": "2026-02-15 06:31:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gviy2",
                  "author": "Faintly_glowing_fish",
                  "text": "You will have a job that will still involve computer programs and data but I honestly don‚Äôt know what you really call it.   You will be doing a mixture of what DE DS and product engineer do today and some infra too",
                  "score": 1,
                  "created_utc": "2026-02-15 06:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2wvxg",
      "title": "Being pushed out of job, trying to plan next steps",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/",
      "author": "octacon100",
      "created_utc": "2026-02-12 15:46:59",
      "score": 87,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "First post for a while, hope this is ok. Spent roughly 5 years at my current job, all with excellent reviews each year, survived the last round of layoffs, had my performance review which basically said don't make any thing and start putting process in place while the ceo just looked at me in disgust. So I'm thinking I'm pretty much on the way out as the company is planning to buy software that makes what I'm doing irrelevant (Has its own data warehouse, it's own way of loading data, etc).\n\nOur company is currently all on prem for work, so a big shared drive is our datalake, sql server is our database, and the best I've been able to do to improve/modernize things was to introduce Prefect for our orchestration, make my own libraries in python to make loading data easier, show the usages of PowerBI and Tableau and create a data warehouse that did what the company wanted to do, but now has decided was a waste of time.\n\nI've started go through the AWS Data Engineering Exam and Snowflake exams, and I have projects on Github that show the use of Amazon S3, Athena, and Glue, so I can at least point to those and say I have cloud experience that I've set up myself. I've been applying to jobs, but I usually get stopped where they are looking for cloud experience. \n\nI've been working with data for almost 20 years now, so I'm hoping my experience can help in terms of getting a job. Does anyone have any advice out there for how to get an in on cloud experience or what places look for with cloud experience? Would the certifications be enough?\n\nAny help is greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o50agi9",
          "author": "snarleyWhisper",
          "text": "Hey there , I had a similar thing happen to me a few years ago. Our whole department was outsourced and then fired unceremoniously. I did some courses on snowflake , databricks and fabric they all have free training and tiers. I would get to the end of the rounds with an interview and usually they would go with someone who had more direct experience with their exact stack. It‚Äôs frustrating but ultimately I ended up landing somewhere that the tooling was less important since I‚Äôm setting a lot of it up. But generally if you focus on one of the top data platforms that‚Äôs your main top of funnel filter.",
          "score": 27,
          "created_utc": "2026-02-12 16:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52k3oh",
              "author": "octacon100",
              "text": "Yeah I‚Äôve had the same experience, make it to interviews do ok, get rejected for someone that has the experience. Good idea to go for jobs where there are starting to make the move.",
              "score": 3,
              "created_utc": "2026-02-12 23:20:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54je8a",
          "author": "viniciusjooj",
          "text": "You don‚Äôt sound underqualified, you sound mis-positioned. 20 years in data isn‚Äôt erased because the stack changed. Before stacking more certs, I‚Äôd get really clear on how you want to position yourself. A structured strengths/work-style assessment (CareerExplorer, Pigment, etc.) can actually help frame your narrative. Are you a systems architect? a data reliability person? a modernization lead? That clarity matters in interviews. Certs help, but story + positioning is what gets you past the cloud filter.",
          "score": 23,
          "created_utc": "2026-02-13 07:16:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b8ibb",
              "author": "valentin-orlovs2c99",
              "text": "Yeah this is super on point. OP, the way you described what you‚Äôve done already actually screams ‚Äúmodernization / enablement‚Äù more than ‚Äújust a data engineer.‚Äù\n\nYou took an on‚Äëprem mess, brought in orchestration, built libraries, introduced BI tools, and designed a warehouse. That‚Äôs literally the narrative a lot of companies want for ‚Äúwe‚Äôre stuck on old infra and need someone to drag us into 2025 without breaking everything.‚Äù\n\nIf you frame yourself as ‚ÄúI specialize in taking legacy on‚Äëprem setups and moving them toward cloud‚Äëready practices‚Äù then the lack of BigCorp‚ÄëAWS‚Ñ¢ experience hurts less. Pair that story with 1‚Äì2 focused certs and a couple of small, very concrete cloud projects and you‚Äôre in a much better spot than trying to look like a generic cloud data engineer.\n\nAlso, target places that are mid‚Äëtransition, not already full cloud. Those folks value someone who actually knows the ugly side of on‚Äëprem.",
              "score": 4,
              "created_utc": "2026-02-14 08:47:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o511l9o",
          "author": "Historical-Fudge6991",
          "text": "Honestly the cloud transition isn‚Äôt too bad. I‚Äôd recommend John Savill on YT. I find the biggest hurdles are RBAC (thankful for a good IT team) and understanding solutions. There‚Äôs 50 ways to make a record with cloud but the core DE principles will always apply. You could checkout Databricks if you want to leverage your python xp",
          "score": 8,
          "created_utc": "2026-02-12 18:53:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52ksfi",
              "author": "octacon100",
              "text": "Haven‚Äôt heard of John Savill, I‚Äôll have to check him out. Thanks. Yeah IAM has been a whole thing for sure. Even close code has issues with that.",
              "score": 1,
              "created_utc": "2026-02-12 23:24:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50c5ss",
          "author": "rotr0102",
          "text": "If you decide to spend some time learning cloud keep in mind some vendors offer free trials, and then you can restart them with different throw away email accounts. So, you could start up a snowflake trial with dbt trial and build some models. Just save everything locally so you can rebuild the environment quickly when you need to restart your free trial.",
          "score": 15,
          "created_utc": "2026-02-12 16:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52o7qk",
          "author": "DoomsdayMcDoom",
          "text": "Job market is tough for us older guys.  I was in your shoes at a point in time.  That‚Äôs when I started my own consulting company and haven‚Äôt looked back since.  I gained the cloud experience I was lacking as I gained more clients.  Now I just run the business and let the younger guys worry about learning the next and greatest tech.",
          "score": 6,
          "created_utc": "2026-02-12 23:43:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52pnjp",
              "author": "octacon100",
              "text": "I did try a bit of that, getting the first clients is tough. Tried linked in with local firms, have previous people I‚Äôve worked with that might be looking for people. Any hints on how to get your first client? That‚Äôs where I‚Äôm getting stuck. Upwork seems like spending money on a slot machine.",
              "score": 1,
              "created_utc": "2026-02-12 23:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o53c81g",
                  "author": "DoomsdayMcDoom",
                  "text": "My first gig I worked with a local recruiter and asked to go corp to corp.  Then I started to attend start-up networking events and started getting word of mouth from clients.",
                  "score": 2,
                  "created_utc": "2026-02-13 02:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51z5w1",
          "author": "cky_stew",
          "text": "I mean dude you‚Äôve kinda been playing on hard mode - as long as you don‚Äôt get a case of ‚Äúwe‚Äôve-always-done-it-this-way-itis‚Äù then you will understand the concepts, problems, and risks of bad system design as it all applies in the cloud too. I‚Äôd focus on the concepts of orchestrators, transformation tools, and OLAP dbs, rather than try to get direct experience with any particular set of tools - stuff tends to be a bit mix and match sometimes. If you need to do something to get it on your resume just to get past the recruiters then go for some certifications - but you should be fine in an interview with someone who knows the deal - you‚Äôll probably enjoy the cloud compared to on prem - you get so much more done it‚Äôs great!",
          "score": 3,
          "created_utc": "2026-02-12 21:33:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52jtuf",
              "author": "octacon100",
              "text": "Thanks, it helps to read this.",
              "score": 1,
              "created_utc": "2026-02-12 23:19:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56ab3v",
          "author": "aMare83",
          "text": "I genuinly think cloud experience is a bit overrated. √Åt the end of the day there you also have storage, computation, databases and semi or unstructured file formats.\n\nI don't think you could not pick up the level of cloud knowledge you need in 2 weeks. Your database design, pipeline creation and orchestration experience worth gold comparing to young guys who can click here and there on AWS or Azure UI.\n\nIf I was an employer I would give you the chance for sure.",
          "score": 3,
          "created_utc": "2026-02-13 15:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5g3ll5",
          "author": "peeyushu",
          "text": "I am also in a similar stage in my life/career and it almost feels like jobs are not for me. I have a long term contract with a public sector client but it can go away anytime and then, will have to hunt for new work. \n\nThe general advice about sharing stories and experiences of taking client(s) from on-prem to cloud is very sound, try publishing something on linkedin/medium or substack or do your own website. Good luck out there.  ",
          "score": 2,
          "created_utc": "2026-02-15 02:55:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5262qu",
          "author": "redditreader2020",
          "text": "Snowflake has great free training and docs.",
          "score": 1,
          "created_utc": "2026-02-12 22:06:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55hig1",
          "author": "Sufficient_Example30",
          "text": "The best thing to do is ,\nStudy while on the job and do the minimum to get by and keep applying.\nJump at the first chance",
          "score": 1,
          "created_utc": "2026-02-13 12:20:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1yl3v",
      "title": "Hired as a data engineer in a startup but being used only for building analytics dashboards, how do i pivot",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r1yl3v/hired_as_a_data_engineer_in_a_startup_but_being/",
      "author": "aks-786",
      "created_utc": "2026-02-11 14:18:44",
      "score": 80,
      "num_comments": 36,
      "upvote_ratio": 0.92,
      "text": "Am a solo Data Engineer at a startup. I was hired to build infrastructure and pipelines, but leadership doesn't value anything they can't \"see.\"\n\nI spend 100% of my time churning out ad-hoc dashboards that get used once and forgotten. Meanwhile, the AI team is getting all the praise and attention, even though my work supports them. Also, i think they can now build rdbms in such a way that DE work would not be required in sometime\n\nRight now, I feel like a glorified Excel support desk. How do I convince leadership to let me actually do Engineering work, or is this a lost cause and look for switch?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r1yl3v/hired_as_a_data_engineer_in_a_startup_but_being/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4symz4",
          "author": "codykonior",
          "text": "> Also, i think they can now build rdbms in such a way that DE work would not be required in sometime. \n\nUhhh. Yeah. Sure. Any day now for the past 3 decades.",
          "score": 70,
          "created_utc": "2026-02-11 14:29:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4t9ipo",
              "author": "aks-786",
              "text": "But what if data size is low ü•≤. Do they need columnar database for this?",
              "score": -13,
              "created_utc": "2026-02-11 15:25:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4u97sy",
                  "author": "Subject_Fix2471",
                  "text": "data size and data complexity are separate, can have a \"low\" amount of data that's complex enough to greatly benefit from a relational db. ",
                  "score": 26,
                  "created_utc": "2026-02-11 18:12:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4x2k24",
                  "author": "IndependentTrouble62",
                  "text": "A well modeled database is like a tailored tux. Its never out of style.",
                  "score": 12,
                  "created_utc": "2026-02-12 03:04:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4xxmaj",
                  "author": "sib_n",
                  "text": "The explanation is that this AI team is doing the data engineering for their need, not that there is no DE. It's possible that they would be thankful for someone to do it for them, maybe you can try asking them. If this doesn't work and you can't find DE work, go somewhere else. But don't neglect the fact that building analytics dashboard is great experience for a DE. It is usually the main downstream usage of a DE's work and it is common to ask DEs to do dashboarding, especially in small structures.",
                  "score": 3,
                  "created_utc": "2026-02-12 06:59:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tlske",
          "author": "ianitic",
          "text": "Doing more of the infrastructure and pipelines would likely make the praise and attention situation worse. That was how it was when I was at a small company.\n\nReport builder got the largest raises due to visibility. I built the pipelines, infrastructure, and ml models. I could build reports too but just didn't have the time. The discrepancy got so bad that by the time I left the report builder had double my salary.\n\nThe only way to fix it is to leave.",
          "score": 50,
          "created_utc": "2026-02-11 16:22:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tuano",
              "author": "aks-786",
              "text": "Okay I see. Thanks for the feedback",
              "score": 6,
              "created_utc": "2026-02-11 17:02:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4ymuu1",
              "author": "bamboo-farm",
              "text": "No. The way to fix is to do less. \n\nDE is the sweetest job. \n\nIt‚Äôs only the worst because we do too much",
              "score": 4,
              "created_utc": "2026-02-12 11:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54om3d",
                  "author": "Afedzi",
                  "text": "I agree",
                  "score": 1,
                  "created_utc": "2026-02-13 08:04:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wsnbn",
              "author": "OkMaize9773",
              "text": "Stop giving quality data to the report builder and enjoy the show",
              "score": 8,
              "created_utc": "2026-02-12 02:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4x00ou",
                  "author": "ianitic",
                  "text": "I'm not there anymore, left for greener pastures lol",
                  "score": 3,
                  "created_utc": "2026-02-12 02:48:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sxzha",
          "author": "Key_Post9255",
          "text": "Look somewhere else",
          "score": 91,
          "created_utc": "2026-02-11 14:26:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tkpu0",
              "author": "mrbartuss",
              "text": "Easier said than done in the current job market",
              "score": 49,
              "created_utc": "2026-02-11 16:17:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w7lov",
                  "author": "Key_Post9255",
                  "text": "He's tagged as the plumber. \n\nUnluckily for him there is no one above him to give him value or \"defend\" him in front of the leadership.\n\nHe would have to find someone to sponsor him or put him in a different situation, but it seems no one is going to do that. It doesn't matter how beautiful the data will be, what tools he uses, whatever else..he is just the nerd in the back doing the PC stuff. No one understands what he's doing. Other people get the merit. Get out if you can because you'll get nothing from this company üòÄ",
                  "score": 15,
                  "created_utc": "2026-02-11 23:59:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ue5s9",
          "author": "randomuser1231234",
          "text": "If you‚Äôre writing dashboards that aren‚Äôt being used, that means you‚Äôre working on someone‚Äôs ‚Äúoh I‚Äôm curious‚Äù questions and not addressing the actual business needs.\n\nLearn how your company works, how it actually makes money, what the market differentiators are. Get cozy with the engineering managers and the finance manager and learn what they give a shit about. Make data artifacts and dashboards that answer THOSE questions.",
          "score": 22,
          "created_utc": "2026-02-11 18:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4to7na",
          "author": "Yuki100Percent",
          "text": "I'm also a solo data person at a startup. First off, if you're at a decent company/team, when your work is related to somebody's that got attention, they would or should mention your work too. What you just said kinda indicates a not-so-good-culture.   \n  \nAnyway, I don't know how much tasks or tickets you get a day/week, you should start asking project priorities or start building one yourself and propose it to the exec team. And how they impact the business for the better. This also helps you become more visible if you've been just working with other folks in the company, but not with exec team. Not sure who you report to, but asking these kinds of questions to your direct report also helps. If not, I might start looking for a new place to work for.   \n  \nAlso, realistically what you can do is to start building infra and pipelines along with what you're doing. You satisfy the current needs and start working on things you think are important or necessary to do what they ask you to do (reporting, dashboarding). \n\nOverall, I'd try to communicate a lot more and see what they say. And depending on that, you either start looking for a new job or decide to do things differently going forward. \n\nDM me if you have any other questions! ",
          "score": 15,
          "created_utc": "2026-02-11 16:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vosuz",
              "author": "blue_leader27",
              "text": "hey I‚Äôm also in your position can I dm you",
              "score": 2,
              "created_utc": "2026-02-11 22:18:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w3m7n",
                  "author": "Yuki100Percent",
                  "text": "Yup feel free",
                  "score": 1,
                  "created_utc": "2026-02-11 23:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tui8g",
              "author": "aks-786",
              "text": "Thanks for the feedback. I will reach out to you in DM once I think about this",
              "score": 1,
              "created_utc": "2026-02-11 17:03:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uz1fb",
          "author": "cky_stew",
          "text": "When you say the AI team is piggybacking on your work, that would sound like you‚Äôre doing some data engineering in order to feed their models/LLMs, but if that‚Äôs not the case- then how does that situation look?\n\nHow do these ad-hoc dashboards look? Are you just writing one off queries on raw data to populate them? In my experience ad hoc reports are a symptom of a lack of good models (and pipelines that build those models) going into your BI tools. If that‚Äôs the case it‚Äôs a pretty easy sell to your line manager that building a proper pipeline would mean dashboards get built faster and have way more reusability.\n\nIf you‚Äôre the solo data engineer, then you‚Äôre the sole authority and the only one who can explain why it is a problem. Have you raised this with them? If you‚Äôre just complying with the requests of things they think they want to see, then they‚Äôre gunna be stuck in a loop of forcing through ad hoc things - because it‚Äôs all they know.\n\nYou mention being an excel support desk too - this definitely shouldn‚Äôt be happening - spreadsheets can be avoided in almost all cases these days (with some exceptions). I LOVE it when someone requests a spreadsheet because it‚Äôs an opportunity for me to ask them ‚Äúout of curiosity‚Äù what are they doing with the spreadsheet - then you almost always get given an opportunity to solve a problem that they didn‚Äôt even know existed, this can make people very happy and that‚Äôs the most satisfying part of this job, I find.\n\nMaybe I‚Äôve got the completely wrong take here but it sounds like the company hasn‚Äôt been exposed to a modern data stack before and are doing things the old way, if you‚Äôve already shown them how it could be better (time optimisation, data reliability, and undiscovered insights being the selling points that execs hear) - then fair enough leave - if you haven‚Äôt you‚Äôre sitting on a golden opportunity, cause it sounds like you‚Äôre the only authority. Best wishes going forward - but this sounds like you‚Äôre not doing data engineering at all and would be a red flag to me if I were to interview you for an engineering role and you spoke about this sort of setup.",
          "score": 6,
          "created_utc": "2026-02-11 20:13:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w1f65",
          "author": "SaintTimothy",
          "text": "My job since forever has been to try and automate myself out of a job. I keep creating automation, pr enhancing old ones, and they keep bringing up more and more stuff that needs it.\n\n\nYou making the donuts every day manually is the opposite of automation. Taking what, of that, can be automated is your job. Either they recognize you did a good job, or you still do a good job and only work one hour a day and surf reddit the other 7. OR you keep manually making the donuts every day because that's fun.",
          "score": 6,
          "created_utc": "2026-02-11 23:24:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4up0n4",
          "author": "sciencewarrior",
          "text": "One possible opportunity is tooling. See where the AI team is spending time with manual tasks and propose ways to simplify their workflow.",
          "score": 3,
          "created_utc": "2026-02-11 19:25:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xdtj7",
          "author": "MindlessTime",
          "text": "Find a way to be necessary. By necessary I mean without your knowledge something important would fail in a costly way. Maybe you simplify data pulls for the accounting team and without you they can‚Äôt create financial statements. Maybe you create and maintain the data that goes into the CRM, without which all the marketing campaigns would fail. Even if leadership doesn‚Äôt ‚Äúsee‚Äù the work, someone will say ‚ÄúI‚Äôm screwed without this person‚Äù and you‚Äôll be fine.",
          "score": 3,
          "created_utc": "2026-02-12 04:18:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yfw11",
          "author": "oscarmch",
          "text": "What you're lacking is marketing.\n\nData Engineering is always backend, it always goes unnoticed. My only advice is that you don't use tools to get a CV. \n\nPlan to use tools according to Business' needs",
          "score": 3,
          "created_utc": "2026-02-12 09:57:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4won63",
          "author": "rdmcoloring",
          "text": "Transition to AI engineer, just do whatever you were doing and just add a chatgpt API call in between",
          "score": 2,
          "created_utc": "2026-02-12 01:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ymo9f",
          "author": "bamboo-farm",
          "text": "This is easy. \n\nThey won‚Äôt see you unless you‚Äôre solving pain. \n\nYou need to find the balance between them feeling some pain and doing your job. \n\nIf they don‚Äôt feel any pain, you‚Äôre doing too much as you‚Äôre not solving anything. \n\nIf above doesn‚Äôt work, coast, learn some shitz and move. \n\nWorlds your oyster.",
          "score": 2,
          "created_utc": "2026-02-12 11:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z4kql",
          "author": "Oddly_Energy",
          "text": "> How do I pivot?\n\nPun intended?",
          "score": 2,
          "created_utc": "2026-02-12 13:14:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u3x8p",
          "author": "Firm_Communication99",
          "text": "Do data science stuff‚Äî r2 , basic stats",
          "score": 1,
          "created_utc": "2026-02-11 17:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vsm0s",
          "author": "Eleventhousand",
          "text": "Talk to your boss in the one in one and ask to be given more DE work .\n\n\nBe nice about it though, don't go in there trashing other data related work like you did in here.",
          "score": 1,
          "created_utc": "2026-02-11 22:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vxfsc",
          "author": "HeyNiceOneGuy",
          "text": "How are ad-hoc dashboards that only get used once supporting your AI team?",
          "score": 1,
          "created_utc": "2026-02-11 23:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51exyl",
          "author": "shropshireladwales",
          "text": "I would say for now our jobs as data engineers if to get info to people, if you can do that with what you have happy days otherwise make the case and link the fact that you can‚Äôt make the dashboard without the good pipeline",
          "score": 1,
          "created_utc": "2026-02-12 19:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54oz5u",
          "author": "Afedzi",
          "text": "Sad to hear‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-13 08:07:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ekkpf",
          "author": "OkDirection3438",
          "text": "If you get a new spot, can I have your current spot?",
          "score": 1,
          "created_utc": "2026-02-14 21:17:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5johuy",
          "author": "Head-Environment6554",
          "text": "Same here hired for de positon but i am also doing ad hoc queries where some bi engineees will created and my duty is received reuquest check wether requester raise ad hoc rightly if right pass report creare perons then check it against live system and delivered to request person. I m 10-20 per day some day onyl just 1 (due to complexity) i m feeling so so so miserable. Cant find any job quickly due to bad job markret.",
          "score": 1,
          "created_utc": "2026-02-15 18:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v3iia",
          "author": "Accomplished-Row7524",
          "text": "SELECT ...\nFROM ...\n   PIVOT ( <aggregate_function> ( <pivot_column> ) [ [ AS ] <alias> ]\n            FOR <value_column> IN (\n              <pivot_value_1> [ [ AS ] <alias> ] [ , <pivot_value_2> [ [ AS ] <alias> ] ... ]\n              | ANY [ ORDER BY ... ]\n              | <subquery>\n            )\n            [ DEFAULT ON NULL (<value>) ]\n         )\n\n[ ... ]",
          "score": 0,
          "created_utc": "2026-02-11 20:35:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0rt35",
      "title": "Are people actually use AI in data ingestions? Looking for practical ideas",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r0rt35/are_people_actually_use_ai_in_data_ingestions/",
      "author": "[deleted]",
      "created_utc": "2026-02-10 05:03:57",
      "score": 62,
      "num_comments": 27,
      "upvote_ratio": 0.92,
      "text": "Hi All,  \n\n\nI have a degree in Data Science and am working as a Data Engineer (Azure Databricks)  \n\n\nI was wondering if there are any practical use cases for me to implement AI in my day to day tasks. My degree taught us mostly ML, since it was a few years ago. I am new to AI and was wondering how I should go about this? Happy to answer any questions that'll help you guys guide me better. \n\nThank you redditors :)",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0rt35/are_people_actually_use_ai_in_data_ingestions/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4kv87x",
          "author": "SharpRule4025",
          "text": "The biggest practical win right now is using LLMs to extract structured data from unstructured web sources. Scrape a product page, get back clean JSON with price, description, specs fields instead of maintaining brittle CSS selector pipelines that break every time the source site changes a div class.\n\nAlso useful for classifying and routing incoming data during ingestion - deciding which pipeline a document goes through based on content type rather than hardcoded rules.\n\nFor Databricks specifically, you could experiment with running smaller models to do schema inference on messy source data before it hits your bronze layer. Saves a lot of manual mapping work.",
          "score": 82,
          "created_utc": "2026-02-10 07:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l6x25",
              "author": "pceimpulsive",
              "text": "I would use the AI to generate the CSS selector pipeline.\n\nOnce you get an error reading you can re-run the CSS selector generator.\n\nThis way you don't burn tokens like crazy, and you get higher performance too!",
              "score": 30,
              "created_utc": "2026-02-10 09:38:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lfyi5",
              "author": "Ultimate_Foreigner",
              "text": "For data ingestion with AI, getting back clean JSON from a web page can be tricky and easily break but using [Pydantic AI](https://ai.pydantic.dev/) would likely help here - basically data validation for LLM responses with auto retries etc.\n\nFor any use case other than web scraping, I don‚Äôt really think it is worth trying to wedge in any LLM steps here. Data integration is really a solved problem that would only be hindered by adding in superfluous AI tooling.",
              "score": 6,
              "created_utc": "2026-02-10 11:02:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lpq9e",
              "author": "tadtoad",
              "text": "This is brilliant! I need to crawl a page where the html changes frequently enough to make traversing the page a nightmare because of the daily monitoring. I think this LLM JSON output would work for me. Thanks for sharing!",
              "score": 3,
              "created_utc": "2026-02-10 12:20:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kyet9",
          "author": "drag8800",
          "text": "honestly the biggest win for us has been using LLMs during validation. not type checking, but catching semantic weirdness that rules miss. like when a field is technically valid but contains \"N/A\" or \"TBD\" or \"pending\" and those all mean different things downstream. having an LLM tag those during ingestion saves so much debugging later.\n\nother thing that's been useful is throwing sample records at an LLM when you inherit a data source with garbage documentation. \"what do these fields probably mean and what types should they be\" gets you 80% there way faster than playing detective.\n\nfor actual pipeline dev i've been using claude code to scaffold ingestion jobs. not shipping the code directly but it's good at recognizing patterns for common sources like REST APIs or SFTP drops. still review everything but cuts initial dev time.\n\nwhat hasn't worked: trying to be clever with dynamic schema evolution. sometimes you want the pipeline to fail loudly when something breaks, not silently adapt and cause problems downstream.\n\nif you're on databricks, check out unity catalog's AI stuff for metadata enrichment. more governance side but still useful.",
          "score": 16,
          "created_utc": "2026-02-10 08:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lefc5",
          "author": "Which_Roof5176",
          "text": "Yep, people use ‚ÄúAI‚Äù in ingestion, but mostly around the pipeline, not inside it: schema mapping, data quality checks, log/alert summarization, and writing connector/ETL code faster.",
          "score": 3,
          "created_utc": "2026-02-10 10:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qjm4c",
              "author": "GAZ082",
              "text": "mmmh, how you would use it for data quality without sharing the actual data?",
              "score": 1,
              "created_utc": "2026-02-11 03:19:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lqe14",
          "author": "tadtoad",
          "text": "I use LLMs for classification/tagging. A stage in my pipeline requires classification of the ingested data into one of 100 categories. I send the category list and the content and get by the right category. It barely costs anything.",
          "score": 4,
          "created_utc": "2026-02-10 12:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r4ayg",
              "author": "Desperate_Pumpkin168",
              "text": "Could you please elaborate on how you have set up llm to do this",
              "score": 1,
              "created_utc": "2026-02-11 05:46:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tl98i",
                  "author": "tadtoad",
                  "text": "It‚Äôs pretty straightforward. I have a huge list of product names in my database that are not categorized. I pull each product name, add it to my prompt (along with a list of categories), then send it to OpenAI‚Äôs api. It then returns the right category from my list, which I then store in my database.",
                  "score": 2,
                  "created_utc": "2026-02-11 16:20:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l72ii",
          "author": "pceimpulsive",
          "text": "Just hell naww to me.\n\nI want my data ingestions to be very fast and have as little dependencies as possible, I also don't want to them to change when openAI changes their guardrails or guts their model a little more to save costs ....",
          "score": 5,
          "created_utc": "2026-02-10 09:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lzgx5",
              "author": "Skullclownlol",
              "text": "> I want my data ingestions to be very fast and have as little dependencies as possible, I also don't want to them to change when openAI changes their guardrails or guts their model a little more to save costs ....\n\nExactly the same here. Ingestion = source copy, no transformations.",
              "score": 1,
              "created_utc": "2026-02-10 13:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4odhna",
                  "author": "pceimpulsive",
                  "text": "I do ELT,\n\nSmall transforms via uoserts.\n\nE.g. my source system stores timestamps as epoch and a few fields are ints that I want as enumerated strings. I achieve this via a view in a staging layer in the destination DB.\n\nOutside that though... It's copy copy",
                  "score": 1,
                  "created_utc": "2026-02-10 20:16:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n1hn0",
          "author": "mckey86",
          "text": "I guess U can use automation",
          "score": 2,
          "created_utc": "2026-02-10 16:36:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l984h",
          "author": "DungKhuc",
          "text": "I'm using AI to ingest news that's relevant to the user profile from different news feeds. LLM is used to transform the news into signals (in JSON format) for UI to consume.",
          "score": 1,
          "created_utc": "2026-02-10 10:01:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lijg7",
          "author": "Nearby_Fix_8613",
          "text": "Heading our data science and ml dept\n\nIts a blessing and a curse for us",
          "score": 1,
          "created_utc": "2026-02-10 11:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lsw2m",
          "author": "reditandfirgetit",
          "text": "Data analysis. Using AI to find fast answers or confirm your theories. For example, a properly trained model could help catch fraud",
          "score": 1,
          "created_utc": "2026-02-10 12:42:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m96v8",
          "author": "ppsaoda",
          "text": "I'm working on medical datasets. And it's messy with clinical notes, so we have developed in-house LLM model to classify diagnosis. Other than that, not much except helping to write code based on my ideas.",
          "score": 1,
          "created_utc": "2026-02-10 14:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rtwi9",
              "author": "dillanthumous",
              "text": "How do you deal with data loss and hallucinations. Sounds extremely high risk.",
              "score": 1,
              "created_utc": "2026-02-11 09:40:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4satzc",
                  "author": "ppsaoda",
                  "text": "We have dedicated staffs to validate.",
                  "score": 1,
                  "created_utc": "2026-02-11 12:05:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mcjuk",
          "author": "share_insights",
          "text": "Great conversation. For those training models (even toy models) and looking for ways to make money off of their hard work, we'd love to chat. We believe (read: know) there is a market for the intelligence encapsulated in the code.",
          "score": 1,
          "created_utc": "2026-02-10 14:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kn3yl",
          "author": "Prestigious-Bath8022",
          "text": "Depends what you call AI.\n\n",
          "score": 1,
          "created_utc": "2026-02-10 06:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ldkj9",
          "author": "Reach_Reclaimer",
          "text": "Unless it's for actually scraping data, there's no reason to use it over a traditional source as far as I'm aware. Would be more expensive for little gain and no ability to troubleshoot",
          "score": 1,
          "created_utc": "2026-02-10 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ki7ze",
          "author": "Thinker_Assignment",
          "text": "I'm co-founder of an oss ingestion library so I can give you some community observations \n\nFirst, everyone uses LLMs for coding at this time, some do it completely by chat interface. We support them with tools to do so with less bad consequences, and faster.\n\nSecond, there's a small group of people that does a lot of ingestion from unstructured sources like multimodal and social media, or in document heavy industries. Those folks do an order of magnitude more ingestion than the rest of the community combined - so the LLM data processing use cases far outweigh normal data engineering in data engineering work at this time.\n\nOn the other hand we're moving towards complete agentic coding, Wes recently said python is going to no longer be coded by humans but agents. So maybe learn in that direction. Check out skills, they are the latest thing that works well.",
          "score": -7,
          "created_utc": "2026-02-10 05:50:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2nrv3",
      "title": "Am I cooked?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/",
      "author": "Slik350",
      "created_utc": "2026-02-12 08:08:21",
      "score": 59,
      "num_comments": 39,
      "upvote_ratio": 0.78,
      "text": "Will keep this as short and sweet as possible.\n\nJoined current company as an intern gave it 1000% got offered full time under the title of:\n\nJunior Data Engineer.\n\nDespite this being my title the nature of the company allowed me work with basic ETL, dash boarding, SQL and Python. I also developed some internal streamit applications for teams to input information directly into the database using a user friendly UI.\n\nWhy am I potentially cooked?\n\nData stack consists of Snowflake, Tableau and and Snaplogic (a low code drag and drop etl tool). I realised early that this low code tool would hinder me in the future so I worked on using it as a place to experiment with metadata based ingestion and create fast solutions. \n\nNow that I‚Äôve been placed on work for a year that is 80% non DE related aka SQL copying/report bug fixing Whilst initially I‚Äôd go above and beyond to build additional pipelines and solutions I feel as though I‚Äôve burnt out.\n\nI asked to alter this work flow to something more aligned with my role this time last year. I was told I‚Äôd finally be moving onto data product development this year April (in effect I‚Äôve been begging to just do what I should have been doing) and I‚Äôve realised even if I begin this work in April I‚Äôm still at almost three years experience with the same salary I was offered when I went full time and no mention or promise of an increase.\n\nI know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesn‚Äôt interest me whatsoever. At this rate my performance will continue to drop for lack of any incentive to continue besides collecting this current pay check.\n\nI‚Äôve had some interviews which are offering 20-25% more than my current role, interpersonally I succeed and am able to progress but in the technical sections I struggle without resources. I‚Äôd say I‚Äôm a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwers‚Ä¶\n\nHas anyone been in a similar position and what did you do to move on from it?\n\nTldr:\nAlmost at 3 years experience, level of experience technically lagging behind timeframe due to exposure at work being limited and lack of personal growth. Getting interviews but struggling with answering without resources.",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4y4w9w",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-12 08:08:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y5h9v",
          "author": "Typical_Priority3319",
          "text": "Ridiculously far off from cooked. Create an itemized list of the things you don‚Äôt know that you either\n a) have been asked in interviews already \n B) think u might get in future interviews based off of research\n\nStart looking at videos on YouTube to understand those concepts. Find excuses to learn those concepts at work whenever possible , but u might just have to do lil mini side projects to crystallize the concepts",
          "score": 136,
          "created_utc": "2026-02-12 08:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ymu1l",
              "author": "Slik350",
              "text": "Thank you for this; will do this asap. I‚Äôve been doing some side projects but can definitely up the effort and make it along with practice more of my focus instead of my current day to day tasks.",
              "score": 9,
              "created_utc": "2026-02-12 11:02:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ztity",
                  "author": "SRMPDX",
                  "text": "Set up a personal GitHub. Work on side projects that are interesting and fill knowledge gaps. Document what you did, why you did it (be honest about upskilling), what issues you had in doing this the first time, maybe even \"what if do differently next time\", and make the repos public. Put a link on your resume. \n\nPotential hiring managers would love to see someone with initiative that can self learn and solve problems. Instead of answering random questions about syntax (15+ YoE and I still suck at syntax sometimes) they can talk to you about your code. Don't fall into the trap of letting a chat bot write all the code though.",
                  "score": 11,
                  "created_utc": "2026-02-12 15:27:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5oyvlq",
                  "author": "Reddit_User_654",
                  "text": "Have you looked at the job market in general but also in this specific field? You should feel really lucky with what you have as the situation \"out there\" is much worse. Don't be so harsh on yourself.ok...it could be better but also quite much worse so yea, thank the Lord for the stable situation you have.",
                  "score": 1,
                  "created_utc": "2026-02-16 15:05:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y5yzc",
          "author": "mh2sae",
          "text": "Snowflake and Tableau are used at Big Tech. Your tasks look to me aligned with what I would expect of a Junior DE. \n\nSnowflake itself is less infra than the counterparts at AWS/GCP but still quite complex and with plenty of options to optimize, do ML, ETL pipes, orchestrate scripts...\n\n  \nThere is plenty you can automate in Snowflake to either do more technical work or sell at interviews. Look into cost optimization, infra as code, documentation, optimizing queries...",
          "score": 45,
          "created_utc": "2026-02-12 08:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yn08p",
              "author": "Slik350",
              "text": "Cost ptimisation is something I‚Äôve not had much time to look into sounds valuable and interesting will give it a look, thanks",
              "score": 5,
              "created_utc": "2026-02-12 11:03:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zzcqq",
                  "author": "randomuser1231234",
                  "text": "Re: cost optimization‚Äîlook into predicate pushdown, and how this affects SQL query costs. Learn how to read query explains if you don‚Äôt already know. Use that to make the queries you‚Äôre copy/pasting BETTER.",
                  "score": 4,
                  "created_utc": "2026-02-12 15:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y83wn",
          "author": "Sensitive-Sugar-3894",
          "text": "DE is boring. After you suffer to make it all work, it becomes boring as it should.\n\nSnowflake, Databricks whatever, are just another thing in the jungle. The good jobs are over MySQL, very old Psql... In old Perl or Bash scripts with horrible embedded SQL. Your dream is to move to dbt and if you do, it will be boring again.\n\nData Engineering is not Systems Engineering. Want excitement, move out from DE.",
          "score": 31,
          "created_utc": "2026-02-12 08:40:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwzdc",
              "author": "xean333",
              "text": "I mean this earnestly: data modeling and building OLAP/OLTP systems has never gotten old for me after 11 yoe. Though I do agree - when done right, the only people that should find it thrilling are the freaks (myself included) that get off to the beautiful machinery of a well oiled platform. In other words, it‚Äôs usually a sign you‚Äôve done something right if the build is boring to basically everyone who looks at it",
              "score": 16,
              "created_utc": "2026-02-12 15:44:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z146v",
              "author": "mcgrst",
              "text": "There is a reason interesting times is a curse!¬†",
              "score": 7,
              "created_utc": "2026-02-12 12:52:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59kapu",
              "author": "under_stroke",
              "text": "My experience is that the market is expading the role to Analytics Engineer, where some level of data intelligence is done whislt implementing most of the DE technical attributes. I assume the reason why Snowflake and low/no code is getting more popular is to remove technical barriers so data professionals can potentially spend more time shipping business value. In the last 7-8 years I saw a great increase in end-to-end more or so fullstack data professional. ",
              "score": 2,
              "created_utc": "2026-02-14 01:05:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ypj1a",
          "author": "domscatterbrain",
          "text": ">I'd say I'm a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwers...\n\nYou have a strong base, mate. \n\nDon't worry, Google is your friend. And now AI will get you the answers faster than reading the entire forum discussion. Well, as long as you ask the right questions.",
          "score": 8,
          "created_utc": "2026-02-12 11:25:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yxo8t",
          "author": "tasker2020",
          "text": "3 years in is a good time for your first job hop.  You‚Äôll get a raise and broaden your experience.",
          "score": 6,
          "created_utc": "2026-02-12 12:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9tva",
          "author": "LeaveTheWorldBehind",
          "text": "Great DE advice in here. Speaking strictly from career perspective, it's typical to feel that boredom/itch around 2-3 years and it's always good to make your needs/expectations relatively clear. If you want to keep growing, don't wait 3 years to share that or 1 year. Keep at it consistently, talk with your manager or your manager+1 about the things you're interested in, ideas you have for other things.\n\nIf you want more, push for more. You're most useful when you're engaged and that'll often show. It doesn't always mean staying with the same company, but often times it does. I've made business cases while in low paid roles that turned into better work.",
          "score": 5,
          "created_utc": "2026-02-12 13:44:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zicj5",
          "author": "Apprehensive-Ad-80",
          "text": "3 years in, DEFINITELY not cooked. Hell even if this was 5 years in I‚Äôd say you‚Äôre fine. If you‚Äôre struggling on the technical evaluations during interviews make that a priority in your current job‚Ä¶ instead of finding an online example or previous work to build from, do it from scratch and only use references when you fail",
          "score": 5,
          "created_utc": "2026-02-12 14:31:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z2i9k",
          "author": "WallyMetropolis",
          "text": "You joined as an intern. They know you need to learn. They offerered you the job because they believe you can.¬†",
          "score": 5,
          "created_utc": "2026-02-12 13:01:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zl8z8",
          "author": "Leather-Replacement7",
          "text": "Low code wrangling skills mean you will still have intuition. Learning syntax takes practice. Practice leetcodes, stratascratch. Get yourself an AWS account, or play with some tech via docker compose. I bet you know more than you think. Sadly imposter syndrome in data engineering doesn‚Äôt ever go away but it gets better. I have 10 years experience, I‚Äôve hardly used pyspark, because everywhere I‚Äôve worked prefers an elt approach to transformation or the data simply isn‚Äôt big enough. There‚Äôs just so many ways to skin a cat in our field, one way isn‚Äôt necessarily better than another.",
          "score": 4,
          "created_utc": "2026-02-12 14:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51hmjg",
          "author": "Specific-Sandwich627",
          "text": "You should take time for your rest. Rest is part of work. The fact that you‚Äôre burning out is already a serious sign. You need to try to address this as soon as possible. Try to relieve yourself mentally and shift the focus of your body and mind to different activities for a few hours before sleep. You could change your diet, try new foods‚Äîmaybe cooking or going for walks somewhere that feels closer to your soul. This is very important.",
          "score": 3,
          "created_utc": "2026-02-12 20:10:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53w94w",
          "author": "DiscussionCritical77",
          "text": "'I know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesn‚Äôt interest me whatsoever.'\n\nI'm 46 and that has been like 30% of my working career. Jobs naturally conclude when they are no longer useful to your career progression. What you do now, is you figure out what the next move to your career is, you train up for it with certs and side projects, you level up, and you change jobs and get a fat raise in the process.",
          "score": 3,
          "created_utc": "2026-02-13 04:14:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54ceo3",
          "author": "Dense___",
          "text": "I‚Äôm in the same boat too, my tech stack is limited to SAP enterprise software that came out in 2008 that crashes every other time I use it... I am 2 years in at my first job in DE and work with so many legacy tools but I know I‚Äôm at least understanding the concepts. Currently spending all my spare time to learn more modern tools and technologies so I can get an easy 20-30% bump at the next place lol",
          "score": 3,
          "created_utc": "2026-02-13 06:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nmosu",
              "author": "The_g0d_f4ther",
              "text": "since you mentionned it, is SAP something useful for a DE ? For some reason i've been asked to learn it a bit for a project at my job, and at this point i fail to see how this helps my career too. Do you have any ideas on how to sell it in interviews if that's the case ?",
              "score": 2,
              "created_utc": "2026-02-16 09:25:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5otaih",
                  "author": "Dense___",
                  "text": "I think it‚Äôs still kinda valuable experience due to it still being an ETL software, some companies are probably in the process (hopefully) of transferring to better platforms and might need help migrating. My manager told me to spend my free time learning SAP stuff for my role as it‚Äôs a popular tool as well, but honestly I haven‚Äôt put much time into it since I feel like learning newer stuff is a better use of my time. I also work in the public sector currently so that probably contributes to the older tech stack",
                  "score": 1,
                  "created_utc": "2026-02-16 14:36:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o561n9f",
          "author": "FantasticEquipment69",
          "text": "I believe what you're looking for is working for an IT solution/consultancy company (of course under the data team)\n\nBeing on the vendor side will give you many of what you're looking for like: \n1- being exposed to different problems, different technologies, different businesses (banking, telecom, health care,...) \n2- avoiding the part where \"whenever the job is done it become boring\", aka avoiding working in operation and waiting for something to fail just to find something to work on.",
          "score": 3,
          "created_utc": "2026-02-13 14:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ac546",
          "author": "ehulchdjhnceudcccbku",
          "text": ">I asked to alter this work flow to something more aligned with my role this time last year.¬†\n\nThat's not the right way to approach this conversation. You need to tell them the business benefit of this change e.g. does your solution save company money (usually the best way to quantify impact), improve user experience etc.",
          "score": 3,
          "created_utc": "2026-02-14 04:07:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z14cv",
          "author": "OhNo171",
          "text": "I once thought that too, but no/low code tools won‚Äôt hinder you, specially in your early years. On the contrary, Id say it makes you focus on what really matters - how to better optimize your pipeline and think more about the end product instead of worrying about language/semantics. I wont say its not important to learn to code, but in the future, regardless if you use spark, pandas, scala, python, ruby, the core etl development skillsets are still there.",
          "score": 2,
          "created_utc": "2026-02-12 12:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53tjub",
          "author": "Icy_Clench",
          "text": "My advice to you is if you think your current company can be doing their data engineering better, lead the way forward. You will move up in no time if you can show value in doing things differently.\n\nWhen I was promoted to DE at my company (analyst for 3 months prior), they also used a drag and drop GUI hooked up to an OLTP database, and frankly the datasets sucked. They had no mechanism to retain historical data for SCDs so there was no historical accuracy. They had created like 6 surviving OBT tables after 3 years and they spent all of their time fixing constant bugs. The data analysts had created a shadow data warehouse that actually ran 95% of the reports.\n\nSo, in my first 6 months I learned about the company and processes and pushed for better practices: git, OLAP database (Snowflake), and proper modeling. I produced 4 datasets that were correctly modeled. I also optimized the daily runtime of the warehouse from 3 hours to 1 hour, and reduced a weekly pipeline from 10 hours to 1 minute, so I built a lot of cred as an expert.\n\nSecond year the company was ready and budgeted for a migration to Snowflake and git. The team was doing PRs and I was setting the standards for code reviews. We set up the platform with basic tooling to deploy, ingest, and transform data, including CDC and SCDs. I introduced some project management frameworks to my manager as well.\n\nThis year I‚Äôm hammering on modeling and tooling. I have shown them the value of conformed dimensions and I do the data modeling. I basically mandated no more unmodeled data and no more shadow warehouse moving forward. We also have big capability gaps between ingestion, transformation, automated testing (unit/data quality), orchestration, and more. I‚Äôve put together a roadmap to address these for this year.",
          "score": 2,
          "created_utc": "2026-02-13 03:55:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58up94",
          "author": "Equivalent_Effect_93",
          "text": "Absolutely not, the hardest part is getting through the door. Learn on your own, build portfolio don't wait for your employer to train you on the toys you wanna play with. 5 years ago I started automating unversionned SAS files on a local Cron server, almost a laptop. Now I'm a senior data engineer playing with a massive databricks systems, elt, streaming and batch, debezium+kafka ingestion, mlops and model serving. Build stuff you wanna try, then when a good opportunity arise you have proof you can deliver in an enterprise setting (even low code tool), and a portfolio with a few project, properly versionned, even maybe deployable through CI/CD. Keep up the good work.",
          "score": 2,
          "created_utc": "2026-02-13 22:34:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ddepf",
          "author": "Vardonius",
          "text": "Get deep into the dbt world and using AI to build models using semantic models to answer business questions.\n\nConsider connecting LLMs with your bi layer via MCP servers and APIs.",
          "score": 2,
          "created_utc": "2026-02-14 17:32:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5g8irk",
          "author": "superjisan",
          "text": "Nope, you're not cooked. I keep a Fail Log of all the technical interviews I've failed over the years in software engineering and data engineering over the years. It has over a 100 interviews I've failed at (that I remembered to update) in the last 12+ years.\n\nBuild on your failures because that will make your great.",
          "score": 2,
          "created_utc": "2026-02-15 03:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y6fnq",
          "author": "Thinker_Assignment",
          "text": "been there, moved jobs. different times\n\nMaybe you can start working on changing the situation by looking for data sources your saas does not provide and creating pipelies for those, run them on snowpark or gh actions if you have nothing else.  \nor consider if the saas is worth replacing with code\n\nalso why so much report bugfixing? look into dimensional modeling for self service, canonical models, maybe if you have better architecture you don't have so much ad hoc work. tableau is not great for self service, it operates under the paradigm that the analyst spoon feeds most things, its both too complex and too weak to be powerful for business user for self service. Most engineers see tableau as a \"busy tool\" and prefer things like metabase etc for this reason",
          "score": 2,
          "created_utc": "2026-02-12 08:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o510r5l",
          "author": "RelevantScience4271",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-02-12 18:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i27k2",
          "author": "Ill_Negotiation3078",
          "text": "Heyz. I have just got trained in Snowflake and informatica idmc.\nGot some tips nd tricks? Goona be deployed in project pretty soon have no idea what work am goona get tho. Not goona use idmc most likely",
          "score": 1,
          "created_utc": "2026-02-15 13:06:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iv4d7",
          "author": "Certain_Leader9946",
          "text": "When I was a junior engineer I was up to 4k commits a year for multiple years. Your goal is learning on the job and working hard until we can actually trust you enough to pay you properly. This post screams you aren't ready for that.",
          "score": 1,
          "created_utc": "2026-02-15 15:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9jd6",
          "author": "tsk93",
          "text": "Short answer: No\n\nI'm pretty much in a similar situation as u are. Just that i'm a data analyst hoping to move into DE one day. Current job feels kinda mundane and u are looking for smth else to grow. Personally I use certifications to build foundational knowledge and move to projects later on. Believe in yourself, u got this.",
          "score": 1,
          "created_utc": "2026-02-12 13:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zutox",
          "author": "starrorange",
          "text": "Your dumb if you think this is bad for a fresh grad",
          "score": 1,
          "created_utc": "2026-02-12 15:33:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r12ckn",
      "title": "How do you justify confluent cloud costs to leadership when the bill keeps climbing?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r12ckn/how_do_you_justify_confluent_cloud_costs_to/",
      "author": "Funny-Affect-8718",
      "created_utc": "2026-02-10 14:34:12",
      "score": 54,
      "num_comments": 71,
      "upvote_ratio": 0.88,
      "text": "Our confluent bill just hit $18k this month and my manager is freaking out. We're processing around 2 million events daily, but between cluster costs, connector fees, and moving data around we're burning through money.\n\n\n\nI tried explaining that kafka needs this setup, showed him what competitors charge, but he keeps asking why we can't use something cheaper, and honestly starting to wonder the same thing. We're paying top dollar and I still spend half my time fixing cluster issues.\n\n\n\nHow do you prove it's worth it when your boss sees the bill and goes pale, we're a series b startup so every dollar counts, what are teams using these days that won't drain your budget but also won't wake you up with alerts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r12ckn/how_do_you_justify_confluent_cloud_costs_to/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4mqup0",
          "author": "DungKhuc",
          "text": "Without knowing your requirements, that sounds like a very small amount of data to move for 18k / month",
          "score": 93,
          "created_utc": "2026-02-10 15:46:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4oem9n",
              "author": "TA_poly_sci",
              "text": "<30 events per second, <60 if we say most are concentrated during the day. Not nothing, but sure as hell not something that requires spending 18k a month.",
              "score": 20,
              "created_utc": "2026-02-10 20:22:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ofl0j",
              "author": "shoppedpixels",
              "text": "It is likely the connectors, I do t believe you can sleep them for lower environments or cost savings. Maybe over those to a self managed connect cluster or just a container environment?",
              "score": 5,
              "created_utc": "2026-02-10 20:26:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mxg9w",
          "author": "Skullclownlol",
          "text": "> How do you prove it's worth it when your boss sees the bill and goes pale, we're a series b startup so every dollar counts, what are teams using these days that won't drain your budget but also won't wake you up with alerts?\n\nEverything non-realtime/non-streaming. Literally almost everything else will be cheaper.",
          "score": 35,
          "created_utc": "2026-02-10 16:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nhcqa",
          "author": "vish4life",
          "text": "2 million events for $18k on Kafka? We process 1 billion+ events on AWS MSK Kafka < 15k. A single Kafka node on basic cloud machines can easily handle 100-500 mil events per day. You are struggling to handle 2 mil events. \n\nSomething has gone terribly wrong with your setup.",
          "score": 35,
          "created_utc": "2026-02-10 17:49:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mykmx",
          "author": "Prinzka",
          "text": "In fact I do the opposite.  \nI use what would be high cloud cost to justify keeping our Kafka on-prem.  \nWe stream trillions of events per day for basically the same cost as your confluent cloud setup by just running Kafka on-prem.",
          "score": 34,
          "created_utc": "2026-02-10 16:22:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ol59v",
              "author": "yesoknowhymayb",
              "text": "Just curious, are you including extra labour cost? If there is any.",
              "score": 6,
              "created_utc": "2026-02-10 20:52:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4oqrql",
                  "author": "Prinzka",
                  "text": "In my internal calculations yes, labour and on prem compute cost. \nI meant our licensing is the same cost as OP's confluent cloud cost.  \nIncluding all costs and comparing to confluent cloud for our volume it was more than an order of magnitude more expensive to stay on-prem.   \n\nAlso keep in mind that to run 24/7 without outages you still need at least 2 people (realistically 3) to maintain things even if you're using confluent cloud.  \nWe're expected to provide real time security feeds without downtime, so we can't just switch off after hours.",
                  "score": 5,
                  "created_utc": "2026-02-10 21:18:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4nmvti",
          "author": "sleeper_must_awaken",
          "text": "Talk to Confluent sales rep: either consult to bring the cost down by 50% or we‚Äôre out.\n\nBut 2M events per day is literally only 23 events per second. That‚Äôs nothing. An old Raspberry Pi could process at least 2000 events per second with two fingers in its nose. You‚Äôre being screwed, either by Confluent or by incompetence in configuration, probably both.",
          "score": 20,
          "created_utc": "2026-02-10 18:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mpqgv",
          "author": "sazed33",
          "text": "Your manager is right, you don't need Kafka",
          "score": 43,
          "created_utc": "2026-02-10 15:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mzd0s",
          "author": "ReporterNervous6822",
          "text": "2 million daily and using Kafka üò≠üò≠",
          "score": 14,
          "created_utc": "2026-02-10 16:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n36oc",
              "author": "Sex4Vespene",
              "text": "I mean if they need streaming, it‚Äôs a decent enough reason to go with Kafka is it not? Although I have to wonder if they really need streaming",
              "score": 6,
              "created_utc": "2026-02-10 16:43:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mr22r",
          "author": "tedward27",
          "text": "You should be embarrassed at spending so much for so little¬†",
          "score": 47,
          "created_utc": "2026-02-10 15:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mot9n",
          "author": "No_Lifeguard_64",
          "text": "If you can't afford streaming data then don't do streaming. That's just the reality of it. Whats the business difference between streaming and microbatch? If you aren't actually actioning on that real-time data then just move to airflow or something.",
          "score": 42,
          "created_utc": "2026-02-10 15:37:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mjaol",
          "author": "TheOverzealousEngie",
          "text": "why do you need streaming? Why not data replication?",
          "score": 23,
          "created_utc": "2026-02-10 15:10:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ngr6f",
          "author": "dfwtjms",
          "text": "That's cloud. In reality a smartwatch and sqlite could handle that workload.  \n  \nThe truth is that nothing justifies those bills. Don't fall for the sunk cost fallacy and build something better.",
          "score": 8,
          "created_utc": "2026-02-10 17:46:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mv223",
          "author": "wqrahd",
          "text": "If you are on aws, why not kinesis?",
          "score": 8,
          "created_utc": "2026-02-10 16:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n39mj",
          "author": "Truth-and-Power",
          "text": "What about microbatch instead of streaming?",
          "score": 3,
          "created_utc": "2026-02-10 16:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pmq6n",
          "author": "DarthCalumnious",
          "text": "Get a VM with fast nvme and just throw your junk into clickhouse.",
          "score": 3,
          "created_utc": "2026-02-11 00:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mrh6o",
          "author": "Training_Refuse7745",
          "text": "You can try serverless approach if cluster keeps on going down. That is more reliable. I have 1.5yoe but recently in my current project we replaced things with serverless and it is not failing and also the costs are low. We have batch ingestion so I am not 100% sure about streaming.",
          "score": 2,
          "created_utc": "2026-02-10 15:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qll3n",
              "author": "kingglocks",
              "text": "Lambda + step functions?",
              "score": 1,
              "created_utc": "2026-02-11 03:31:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4myi9v",
          "author": "ThroughTheWire",
          "text": "Would need to know/understand why you need Kafka in the first place to give you a real answer tbh",
          "score": 2,
          "created_utc": "2026-02-10 16:22:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p5ugs",
          "author": "zikawtf",
          "text": "OP could you bring us more context, I am really curious about the stack and business context to justify using Kafka (not just as tool, but as a business requirement)",
          "score": 2,
          "created_utc": "2026-02-10 22:30:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4morxd",
          "author": "Nekobul",
          "text": "That's ridiculous. You can process that amount of data easily on a single machine using SSIS and it will be dirt cheap.",
          "score": 7,
          "created_utc": "2026-02-10 15:36:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nh8sa",
              "author": "dfwtjms",
              "text": "Or even better, open source stack without Microslop.",
              "score": 7,
              "created_utc": "2026-02-10 17:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pxemt",
                  "author": "wubalubadubdub55",
                  "text": "What‚Äôs up with Microsoft hate for everything? \n\nLike what have the engineers who develop SSIS done to you that‚Äôs so bad that you have to ridicule it? \n\nMan you people are pathetic and full of hate.",
                  "score": -1,
                  "created_utc": "2026-02-11 01:04:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nrqru",
                  "author": "Nekobul",
                  "text": "How is that better if you have to pay consultants top dollars to create and maintain that solution?",
                  "score": -2,
                  "created_utc": "2026-02-10 18:36:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4negya",
              "author": "IndependentTrouble62",
              "text": "Truth...",
              "score": 1,
              "created_utc": "2026-02-10 17:35:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nnb1u",
          "author": "Flacracker_173",
          "text": "You need to self host a Kafka connect cluster and probably lower your data retention period and topic replicas.",
          "score": 1,
          "created_utc": "2026-02-10 18:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nncn0",
          "author": "baby-wall-e",
          "text": "2 millions per day is to tiny. You should consider refactoring your system, moving away from confluent.",
          "score": 1,
          "created_utc": "2026-02-10 18:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nt44o",
          "author": "Sad_Monk_",
          "text": "for 2 million events what is stopping you from micro batching?",
          "score": 1,
          "created_utc": "2026-02-10 18:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nvfjm",
          "author": "Brave_Affect_298",
          "text": "We will soon have to choose between Google Pub/Sub and Confluent Kafka. Is Pub/Sub cheaper?",
          "score": 1,
          "created_utc": "2026-02-10 18:53:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ocfl9",
          "author": "dan_the_lion",
          "text": "Why do you feel like you need to justify the bill instead of exploring alternatives which might be better fit for your use case? \n\nWhat are you using Kafka for exactly? Is it just data replication from databases to warehouses or is it actually a queue/streaming backbone of your application? \n\nIf it‚Äôs just to enable analytics there are many options you can consider. Do you need log-based CDC?",
          "score": 1,
          "created_utc": "2026-02-10 20:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p45u0",
          "author": "Hackerjurassicpark",
          "text": "You don‚Äôt. We‚Äôre moving to pub sub",
          "score": 1,
          "created_utc": "2026-02-10 22:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pwavn",
          "author": "wubalubadubdub55",
          "text": "You‚Äôre wasting so much money for so little. \n\nA .NET worker service + Postgres db can do that in a small VM without breaking a sweat for a tiny fraction of that cost.",
          "score": 1,
          "created_utc": "2026-02-11 00:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q6iol",
          "author": "thisfunnieguy",
          "text": ">kafka needs this setup\n\nThere are things like MSK via AWS\n\n  \nim not saying thats right for you, but there are a number of other options.",
          "score": 1,
          "created_utc": "2026-02-11 01:59:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qpr8j",
          "author": "codykonior",
          "text": "Replace it with a bash script on a VM.\n\nPro: $18k cheaper\n\nCon: Does not look fancy on the resume and HA DR are extra work.\n\n/s but ... üòÄ",
          "score": 1,
          "created_utc": "2026-02-11 03:59:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r817v",
          "author": "Front-Ambition1110",
          "text": "I think you are overprovisioning. 2 million events is not that huge.¬†",
          "score": 1,
          "created_utc": "2026-02-11 06:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rs3km",
          "author": "calimovetips",
          "text": "at 2m events a day that bill sounds more like overprovisioning or connector sprawl than pure volume, i‚Äôd break down cost per million events and map it to actual business use cases so leadership sees value per stream, not just a lump sum.\n\nhave you modeled what self managed or a lighter managed kafka setup would cost once you factor in ops time and on call?",
          "score": 1,
          "created_utc": "2026-02-11 09:23:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sbdnm",
          "author": "observability_geek",
          "text": "You don't because you can run enterprise kafka without confluent.. I really don't understand why orgs pay for kafka if they can use Strimzi. [https://github.com/strimzi/strimzi-kafka-operator](https://github.com/strimzi/strimzi-kafka-operator) if you are in the EU you can use [axual.com](http://axual.com) for the governance.",
          "score": 1,
          "created_utc": "2026-02-11 12:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sdjz9",
          "author": "Los_Cairos",
          "text": "As others in this thread have said, your costs are crazy given the volume.\n\nI work at a data streaming company (not Confluent) and we have customers doing 25x your daily events for 1-1.5x the cost.\n\nSo either something is really off in your setup (e.g. something about your connector setup, or you've got some super high custom data retention duration), or you're paying wayyyy too much and you need to talk to your rep.\n\nEdit: better cost estimate",
          "score": 1,
          "created_utc": "2026-02-11 12:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sixbi",
          "author": "trentsiggy",
          "text": "Are you processing them realtime?  If so, do you *have* to process them realtime?",
          "score": 1,
          "created_utc": "2026-02-11 13:00:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o569hmw",
          "author": "Lingonberry_Feeling",
          "text": "I‚Äôm generally all for the working man, but your manager is right.\n\nSomething is very wrong with your setup, not knowing anything about it, 2 million events for 18k way too much.",
          "score": 1,
          "created_utc": "2026-02-13 14:59:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m9u8e",
          "author": "StubYourToeAt2am",
          "text": "Your manager is right to question it. 2 million events per day is  what 23 events per second? An $18k Confluent bill at that volume usually means over provisioned clusters or connector sprawl. My first question. Do you actually need sub second streaming? If the business is not triggering automated actions in milliseconds then you are paying for guarantees you do not use. Moving most flows to micro-batch (5-15 minute windows) instantly cuts always on cluster and connector costs.\n\nMany keep Kafka for the truly real time edge cases and replace the rest with scheduled ELT pipelines. Stitch, Integrateio, Airbyte can all pull incrementally from your sources and load in controlled batches without running 24/7 streaming infrastructure. That gives you near real time analytics without cluster babysitting and without that high bill every month.",
          "score": 1,
          "created_utc": "2026-02-16 02:48:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3vgbd",
      "title": "How MinIO went from open source darling to cautionary tale",
      "subreddit": "dataengineering",
      "url": "https://news.reading.sh/2026/02/14/how-minio-went-from-open-source-darling-to-cautionary-tale/",
      "author": "jpcaparas",
      "created_utc": "2026-02-13 17:31:12",
      "score": 45,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Blog",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r3vgbd/how_minio_went_from_open_source_darling_to/",
      "domain": "news.reading.sh",
      "is_self": false,
      "comments": [
        {
          "id": "o58hbde",
          "author": "VEMODMASKINEN",
          "text": "Replaced it with Garage in my homelab. Was easy to setup in my K8s cluster and I haven't really had any issues one month in so far.\n\n\nhttps://garagehq.deuxfleurs.fr/",
          "score": 11,
          "created_utc": "2026-02-13 21:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59jas6",
              "author": "PencilBoy99",
              "text": "does garage let you create/setup s3 storage on an on premises box? that's what minio did think ",
              "score": 3,
              "created_utc": "2026-02-14 00:58:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5boktq",
              "author": "current_thread",
              "text": "I've been using Rook/Ceph for the last year and had no issues whatsoever.",
              "score": 1,
              "created_utc": "2026-02-14 11:24:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b4i20",
          "author": "every_other_freackle",
          "text": "We migrated to SeaweedFS and it is perfect!",
          "score": 5,
          "created_utc": "2026-02-14 08:08:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0wwrn",
      "title": "Our company successfully built an on-prem \"Lakehouse\" with Spark on K8s, Hive, Minio. What are Day 2 data engineering challenges that we will inevitably face?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r0wwrn/our_company_successfully_built_an_onprem/",
      "author": "seaborn_as_sns",
      "created_utc": "2026-02-10 10:08:31",
      "score": 43,
      "num_comments": 50,
      "upvote_ratio": 0.91,
      "text": "I'm thinking \n\n\\- schema evolution for iceberg/delta lake  \n\\- small file performance issues, compaction\n\nWhat else? \n\nAny resources and best practices for on-prem Lakehouse management?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r0wwrn/our_company_successfully_built_an_onprem/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4la04a",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-10 10:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lccan",
          "author": "liprais",
          "text": "minio will be your biggest pain of ass",
          "score": 53,
          "created_utc": "2026-02-10 10:30:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lde2s",
              "author": "jupacaluba",
              "text": "I second that. Just reading gave me the itch",
              "score": 6,
              "created_utc": "2026-02-10 10:39:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ldpak",
              "author": "seaborn_as_sns",
              "text": "is it because they abandoned foss? what else is there for on-prem? ceph?",
              "score": 4,
              "created_utc": "2026-02-10 10:42:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4lkli5",
                  "author": "rmoff",
                  "text": "garage, seaweedfs, apache ozone, and several others. depends what you need. I wrote about it here (although from a PoC/demo perspective, not production usage): https://rmoff.net/2026/01/14/alternatives-to-minio-for-single-node-local-s3/",
                  "score": 6,
                  "created_utc": "2026-02-10 11:42:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ljukg",
                  "author": "liprais",
                  "text": "i am running hdfs ,works smooth",
                  "score": 2,
                  "created_utc": "2026-02-10 11:36:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nln7l",
                  "author": "Colafusion",
                  "text": "It‚Äôs also AGPL, which depending on what you‚Äôre doing can be a massive issue.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m127x",
              "author": "543254447",
              "text": "Can't agree with you more. Literally cannot delete some files for no reason.......\n\nAlways run into weird error with spark due to it.....",
              "score": 2,
              "created_utc": "2026-02-10 13:32:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4met9m",
                  "author": "seaborn_as_sns",
                  "text": "how big is your dataeng/dataops team?",
                  "score": 1,
                  "created_utc": "2026-02-10 14:47:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4p6daw",
              "author": "zikawtf",
              "text": "What justify the MinIO as a storage tool in production environment? I mean, store data is cheap, so why not S3?",
              "score": 1,
              "created_utc": "2026-02-10 22:33:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4sx6sn",
                  "author": "seaborn_as_sns",
                  "text": "airgapped environment, data residency regulations, etc",
                  "score": 1,
                  "created_utc": "2026-02-11 14:22:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4t1sjq",
              "author": "ludflu",
              "text": "wait, people use minio in production?!",
              "score": 1,
              "created_utc": "2026-02-11 14:46:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lh0gn",
          "author": "Gold_Ad_2201",
          "text": "it sounds like you buit now a 20 year old architecture.\n1. is spark the only access to data? what about lower latency? trino, duckdb?\n2. hive partitioning will only delay your problems. you def need to look into table formats (iceberg, delta). and more importantly - they are also designed badly. you need to look into having catalog with them to have the good speed\n3. I assume minio and k8s are because you have some requirement to have air gapped env? if not, do consider S3/blob to save your maintenance team",
          "score": 18,
          "created_utc": "2026-02-10 11:12:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mf2uk",
              "author": "seaborn_as_sns",
              "text": "1. experimenting on trino too  \n2. we have iceberg and delta too, unified hive catalog. should we adopt polaris or something else do you think?  \n3. yes we need airgapped. i think ceph is better option but no experience to advocate for it.  ",
              "score": 4,
              "created_utc": "2026-02-10 14:48:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4mwg0o",
              "author": "Doto_bird",
              "text": "Do you have any experience with MotherDuck (from DuckDB)? They critized iceberg and delta quite harshly in their announcement video and they addressed those issues (in their opinion), but I've never talked with anyone who's actually used it for big data workloads yet.",
              "score": 2,
              "created_utc": "2026-02-10 16:12:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mxgfv",
                  "author": "Gold_Ad_2201",
                  "text": "didn't use it in production, no. their comments are fair. but let's see if this tech becomes adopted and supported. their idea of DuckLake sounds pretty logical but other than MotherDuck I didn't hear of any commercial implementation.\nbut duckDB itself is awesome engine!",
                  "score": 2,
                  "created_utc": "2026-02-10 16:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4le43p",
          "author": "dragonnfr",
          "text": "Run aggressive compaction (bin-packing, 128MB targets). For schema evolution, only add fields. Check Delta docs for OPTIMIZE + ZORDER BY on small files.",
          "score": 5,
          "created_utc": "2026-02-10 10:46:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4leirv",
              "author": "seaborn_as_sns",
              "text": "any tool to monitor general health of delta tables or do teams build inhouse monitoring scripts?",
              "score": 1,
              "created_utc": "2026-02-10 10:50:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltl6j",
          "author": "Hackerjurassicpark",
          "text": "Upgrading your K8S, Hive and Minio when your current versions go EOL",
          "score": 5,
          "created_utc": "2026-02-10 12:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfa2j",
              "author": "seaborn_as_sns",
              "text": "you think thats near-term (2yrs) or bit later? ",
              "score": 2,
              "created_utc": "2026-02-10 14:49:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4p5ima",
                  "author": "Hackerjurassicpark",
                  "text": "Depends on the version you‚Äôre using. Go check the EOL dates for the exact version you‚Äôre using",
                  "score": 1,
                  "created_utc": "2026-02-10 22:28:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ldmck",
          "author": "FunAd6672",
          "text": "Data quality checks become your real Day 2 job not pipelines.",
          "score": 3,
          "created_utc": "2026-02-10 10:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4le9qw",
              "author": "seaborn_as_sns",
              "text": "how do you manage them? via dbt or inhouse tools via great expectations or something? ",
              "score": 2,
              "created_utc": "2026-02-10 10:47:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lsdb6",
          "author": "Eitamr",
          "text": "Minio is for testing, avoid on prod if you can",
          "score": 3,
          "created_utc": "2026-02-10 12:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfh1k",
              "author": "seaborn_as_sns",
              "text": "even enterprise minio? the \"aistor: Exabyte-Scale Storage Engineered for the AI Era\"",
              "score": 1,
              "created_utc": "2026-02-10 14:50:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltxgx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 3,
          "created_utc": "2026-02-10 12:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfyxl",
              "author": "seaborn_as_sns",
              "text": "thanks so much! what was the decision-making process that you guys arrived to that stack? followed some tried-and-tested blueprint from some similar company's experience or arrived purely based on internal discussions and evaluations?",
              "score": 1,
              "created_utc": "2026-02-10 14:53:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtd7n",
          "author": "ShanghaiBebop",
          "text": "Governance and access management will be a PITA.¬†",
          "score": 3,
          "created_utc": "2026-02-10 15:58:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4syj5q",
              "author": "seaborn_as_sns",
              "text": "any limitations to apache ranger?",
              "score": 1,
              "created_utc": "2026-02-11 14:29:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5d2wxu",
                  "author": "vik-kes",
                  "text": "Check more modern concepts such as openFGA or Cedar Policy. This is what we implemented in Lakekeeper",
                  "score": 2,
                  "created_utc": "2026-02-14 16:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n92oq",
          "author": "SuperTangelo1898",
          "text": "Ghost objects that exist in the backend but don't exist in Minio's front end UI object manager",
          "score": 2,
          "created_utc": "2026-02-10 17:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nbpub",
          "author": "swapripper",
          "text": "Tenancy/Cost attribution \nGovernance/PII masking / RLS\nLogs/Lineage/Observability/Performance monitoring\nSemantic layer possibly\nCDC if you need it\nEasy abstractions for backfills/backups/compaction/cleanup",
          "score": 2,
          "created_utc": "2026-02-10 17:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nw2e9",
          "author": "efxhoy",
          "text": "Just curious, how much data do you have? 1TB? 100TB?¬†",
          "score": 2,
          "created_utc": "2026-02-10 18:56:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sxtzp",
              "author": "seaborn_as_sns",
              "text": "around 10TB in now legacy data warehouse in total",
              "score": 1,
              "created_utc": "2026-02-11 14:25:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pn6dc",
          "author": "Due_Carrot_3544",
          "text": "Whats your total data volume stored right now?",
          "score": 2,
          "created_utc": "2026-02-11 00:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sxv14",
              "author": "seaborn_as_sns",
              "text": "around 10TB in now legacy data warehouse in total",
              "score": 1,
              "created_utc": "2026-02-11 14:25:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lg82w",
          "author": "reallyserious",
          "text": "How can one handle access control like row level security and table level security?",
          "score": 1,
          "created_utc": "2026-02-10 11:05:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mfc28",
              "author": "seaborn_as_sns",
              "text": "experimenting with ranger now",
              "score": 3,
              "created_utc": "2026-02-10 14:49:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4psyls",
          "author": "Rich-Ad5460",
          "text": "May I ask how long does it take to build this? And with how many people?",
          "score": 1,
          "created_utc": "2026-02-11 00:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sy3ft",
              "author": "seaborn_as_sns",
              "text": "total 10\\~ people built this as poc, ops + engineering",
              "score": 1,
              "created_utc": "2026-02-11 14:26:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5h78i0",
          "author": "vik-kes",
          "text": "Governance. Now you have a challenge to manage access to your data",
          "score": 1,
          "created_utc": "2026-02-15 08:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n14a3",
          "author": "ChinoGitano",
          "text": "Why use Hive when Unity Catalog is now open-source?  Governance and performance may be your biggest headache ‚Ä¶ assuming you actually have the component integration licked. üòÖ",
          "score": 1,
          "created_utc": "2026-02-10 16:34:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2gu5c",
      "title": "Should I prioritize easy/medium or hard questions from DataLemur as a new graduate?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/",
      "author": "SIumped",
      "created_utc": "2026-02-12 02:03:26",
      "score": 43,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Hi all, I'll be graduating June so I'm currently applying to data roles with previous data engineering internships at a T100 company. I've picked up DataLemur and I'm somewhat comfortable with all easy/medium questions listed. Should I walk through these again to ensure I am 100% confident in answering these, or should I move onto hard questions?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o4xwal1",
          "author": "Specific-Mechanic273",
          "text": "You can move to hard as they'll strengthen the skills required in medium. Most DataLemur hard questions feel like they're combining a lot of concepts. Tbh in most interviews you're asked medium level questions. \n\nJust be sure you're able to answer these question patterns (copy pasted from my Notion, let me know if i should clarify something):\n\n\\- Ordinal & Ranking Patterns (first, second, third, latest X per group) -> row\\_number() + dense\\_rank() + rank()\n\n\\- Rolling / Sliding Aggregations (rolling x-day average, running total etc.) -> sum/avg/count window function + \"ROWS BETWEEN N PRECEDING AND CURRENT ROW)\n\n\\- LAG / LEAD Window Functions (year-over-year changes)\n\n\\- Metric by Dimension (e.g. revenue by department) -> GROUP BY + join\n\n\\- Self Joins (often used in hierarchies)\n\n\\- Anti joins (find what's missing)\n\n\\- Conditional aggregation (count(case when x = y then 1 end))\n\n\\- CTEs\n\n\\- Knowing functions to manipulate dates (get month/year from timestamp, date diff, add time, ...)\n\n  \nWith this you'll be able to answer 99% of all interview questions",
          "score": 9,
          "created_utc": "2026-02-12 06:47:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z8xef",
              "author": "NickSinghTechCareers",
              "text": "good overview of skills here!",
              "score": 1,
              "created_utc": "2026-02-12 13:39:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xbnxt",
          "author": "NickSinghTechCareers",
          "text": "Hi! DataLemur founder here ‚Äì glad to hear you've been grinding SQL & Python on the site. I think moving onto the hard questions is good, if you've already done the easy/medium problems. You can always re-visit the Mediums again, and try to speed through them, after going through a few dozen hard problems. You just might be surprised how much faster you can go, after practicing on harder problems, and getting better at pattern recognition. \n\nBesides DataLemur, I think having a proper project to talk about is also super important for Data Engineering interviews. Hopefully, this can be sourced from a past internship ‚Äì but if not, go make a real portfolio project that's end-to-end, deployed (with a live link), that's also key-word rich (so use AWS, PostgreSQL, Airflow, Python, etc.). ",
          "score": 22,
          "created_utc": "2026-02-12 04:03:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xfgzz",
              "author": "WildLandShark",
              "text": "Hey Nick! I recently went through some of your hard SQL questions on DataLemur in preparation for BI Engineer interview. The questions were super helpful for refreshing myself on some querying techniques that I don't use all that often. I ended up receiving an offer, so thank you for creating such a helpful resource.\n\nI'm wondering though, how do you source your questions? I'm especially curious as there are a wide variety of companies that are listed as question sources.",
              "score": 7,
              "created_utc": "2026-02-12 04:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y71z8",
                  "author": "dyogenys",
                  "text": "Is this whole thing an ad?",
                  "score": 11,
                  "created_utc": "2026-02-12 08:29:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4z8p24",
                  "author": "NickSinghTechCareers",
                  "text": "glad it was helpful. for question sources ‚Äì many people tell me them, and then I change up the details slightly to go around NDA / maintain privacy. with like 175k+ followers on linkedin, and 50k copies sold of my book, enough people just LinkedIn DM me or email me their interview experience, ask for advice, feedback, etc. I also do a ton of 1:1 coaching, where we also go through past interviews they've had, and seen where they struggled or could improve. \n\nfinally ‚Äì i got a ton of it from Glassdoor, Reddit, Blind, and Medium back when I started DataLemur a few years ago. ",
                  "score": 5,
                  "created_utc": "2026-02-12 13:38:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50zyev",
              "author": "w_savage",
              "text": "Is DataLemur a free site? I've never ran across it before.",
              "score": 1,
              "created_utc": "2026-02-12 18:45:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4kugz",
      "title": "‚ÄúWhat are the best resources to learn Docker from scratch?‚Äù",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r4kugz/what_are_the_best_resources_to_learn_docker_from/",
      "author": "Effective_Bluebird19",
      "created_utc": "2026-02-14 13:38:31",
      "score": 42,
      "num_comments": 20,
      "upvote_ratio": 0.94,
      "text": "I‚Äôm a Data Engineer with around 2 years of experience and I‚Äôm trying to properly learn Docker so I can use it in real-world data pipelines.",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r4kugz/what_are_the_best_resources_to_learn_docker_from/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5c5ttz",
          "author": "AutoModerator",
          "text": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-14 13:38:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cfnfb",
          "author": "ThroughTheWire",
          "text": "learning docker from scratch is pretty pointless imo. learn what you need to know as you have to deal with it. in practice for most data engineers this would be running your own airflow instance locally or perhaps running a tool like Airbyte locally. both situations have tutorials that you can follow along that will essentially teach you how docker is working with a practical use case in mind. I wouldn't waste time learning how docker otherwise works.",
          "score": 31,
          "created_utc": "2026-02-14 14:37:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e5xds",
              "author": "VEMODMASKINEN",
              "text": "It's hardly a waste knowing how container internals work with namespaces and cgroups along with security and modern building best practices... We're Data *Engineers* after all.¬†\n\n\nAnyone can follow a tutorial that spells out how to set something up, it won't teach you much about the tool itself though...\n¬†\n\nAnyways, here OP:\n\n\nhttps://courses.mooc.fi/org/uh-cs/courses/devops-with-docker\n\n\nAnd:\n\n\nhttps://m.youtube.com/watch?v=Utf-A4rODH8",
              "score": 12,
              "created_utc": "2026-02-14 19:57:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dqi71",
          "author": "Cloudskipper92",
          "text": "Since you (and a couple others) seem to be more or less asking for some structure to learn against, here's what I use or have used in my day-to-day.\n\n- How to install docker, and the follow-ups needed, on your Distro/OS. Windows/Mac are pretty straight-forward. Linux has some steps after install that you need to do.\n\n- Get used to navigating dockerhub, finding official image builds, and how to pull specific versions. Much like Python version pinning, you _certainly_ want to pin versions of infra.\n\n- Read the docs on the most important docker cli commands. Non-exhaustive: `docker build`, `docker run`, `docker pull`, `docker exec`, `docker container cp`.\n\n- Learn and practice making [Dockerfiles](https://docs.docker.com/reference/dockerfile/). Learn the subtle differences between `ADD` and `COPY`. How Layering works. Learn the differences between `CMD` and `ENTRYPOINT`, `ARG` and `ENV`. Learn how to expose a port on a container to the host. HINT: it isn't with the `EXPOSE` instruction and if you made it this far without being able to ping your container from your host you should go back one step ;) . Make a `.dockerignore` so you don't put anything you don't want in the container. You can ignore these instructions for now: `HEALTHCHECK`, `LABEL`, `MAINTAINER`, `ONBUILD`, `SHELL`, `STOPSIGNAL`.\n\n- Learn how networking works for Docker. Networking generally is a weak point for most SWEs, and seems to often be doubly so for DEs. In the same vein, read the docs on how Docker Volumes work and how to attach them.\n\nNow, you came to the DE Subreddit to ask this and mention you have 2 YOE already, so I'm going to also mention a couple of more specific things.\n\n1. [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html) is D E N S E but obfuscated heavily. As in, it has a lot of levers and knobs, but it mostly assumes the defaults are good enough for this. It also assumes you know docker-compose which I _did_ leave out of the top. The justification I'll give for that is that Docker Compose is great... but you should be using the dockerized airflow mentioned here as a TESTING system ONLY. Thus, get it going following thier instructions, do what you need to do, but don't assume it matches production-grade Airflow deployments.\n\n2. You could (and perhaps should) use [Astronomer's CLI](https://www.astronomer.io/docs/astro/cli/overview). I don't work for them or anything, but I have used their managed service in the past. The fact that the CLI exists for free is great and should be taken advantage of for local testing.\n\n3. Now that you've seen those two and understand how docker works and have played with the ins and outs, contrary to what others may say, I would then _AND ONLY THEN_ look into Kubernetes. No matter the system, managed or self-hosted, Airflow and it's pipelines ALWAYS run on Kubernetes behind the scenes. The way you build the image for [Airflow will change](https://gist.github.com/wesh92/4ce0634e61949a1679e2ae5a1788cb18) and, thus, how you manage dependancies and the way you need to understand how Kubernetes sees Containers versus how you've seen them thus far at this point. I cannot stress this enough though, DO NOT jump straight to this point. Everything above here should be _weeks_ of testing, toiling, and troubleshooting at a minimum before you try to introduce K8s. When you do, start with a local manager like `k3s`. I would recommend not using `minikube` or `kind` as those are \"k8s in docker\" which is a whole extra layer you don't need. The justification I'll give for including this: I like my local testing env for pipelines to be as close, if not exact, to what I will deploy. For me this means _in kubernetes using exactly what I will deploy_ with as much of the kubernetes weirdness as I can account for. If this doesn't feel important to you, please feel free to ignore!\n\nHope this helps! But please, just start with the Docker basics. You can search up a youtube if your more visually-inclined. Read through the docs and try implementing things if you're more of the experiential kind. Nothing is going to be a cheat code because these are kind of foundational tools for SWE and DE.\n\nEDIT: formatting. reddit pls",
          "score": 5,
          "created_utc": "2026-02-14 18:37:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f7qct",
              "author": "wetnmoist",
              "text": "I learned how to use it for a home server / networking. \n\nImo you‚Äôre on point - the only thing I‚Äôd add is finding a project where it‚Äôs actually necessary to use it rather than playing around with everything it can do.",
              "score": 2,
              "created_utc": "2026-02-14 23:29:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5c77q2",
          "author": "Outside-Storage-1523",
          "text": "I think finding a use case in your personal project is useful.",
          "score": 12,
          "created_utc": "2026-02-14 13:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cj7g0",
          "author": "gn-musa",
          "text": "Ship one thing,not tutorials.",
          "score": 3,
          "created_utc": "2026-02-14 14:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5caa8q",
          "author": "Prestigious_Radio582",
          "text": "+1 I also want to learn docker from a structured resource.",
          "score": 3,
          "created_utc": "2026-02-14 14:05:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5choaf",
          "author": "xean333",
          "text": "What do you mean learn it from scratch? Are you just saying you don‚Äôt currently know anything about docker? Start with a super high level overview of the problem(s) docker solves. Then run airflow locally with it. Start now and you‚Äôll be done by the afternoon. If you want to learn it deeper, this is probably the wrong sub to ask",
          "score": 2,
          "created_utc": "2026-02-14 14:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d25t0",
          "author": "meiousei2",
          "text": "All you need to understand is how containerized networking works and basic syntax to be able to audit AI generated Dockerfiles, they're not that complex",
          "score": 1,
          "created_utc": "2026-02-14 16:36:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dti1w",
          "author": "ludflu",
          "text": "If you want to make it specific to data engineering: \n\n1. run airflow using `docker compose` to get a sense for docker. (https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)\n2. Then create a DAG that does its work by running a docker image using the DockerOperator\n3. In production, its very similar, but you'll likely run your DAG task in ECS on AWS or Cloud Run on GCP",
          "score": 1,
          "created_utc": "2026-02-14 18:52:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5duvjt",
          "author": "dumb_user_404",
          "text": "there is a course on docker by javascript mastery, its a pretty new course and will teach you about docker from basics up. \n\nWatch half of that video and then move on to video teaching you about running your own airflow or spark or what ever you need. \n\nwith the foundational knowledge from the first video, everything will fall in place. \n\nyoutube : [https://youtu.be/GFgJkfScVNU?si=8jdbZsjlI4axyuzg](https://youtu.be/GFgJkfScVNU?si=8jdbZsjlI4axyuzg)\n\n",
          "score": 1,
          "created_utc": "2026-02-14 18:59:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ekxzk",
          "author": "JBalloonist",
          "text": "I'm just good enough to be dangerous with Docker, and frankly that was all I needed. I was using it to create containers running in AWS ECS (Elastic Container Service) and Lambda. If I ran into trouble enough searching or AI questions would usually give me the answer. Occasionally I'd reach out to a coworker who had more experience with builds (but not much Python experience). ",
          "score": 1,
          "created_utc": "2026-02-14 21:19:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ey0va",
          "author": "Dawido090",
          "text": "Book \"Learn Docker in a Month of Lunches\" it's great as whole series, its bit outdated but it's great.",
          "score": 1,
          "created_utc": "2026-02-14 22:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gqrk4",
          "author": "tbot888",
          "text": "YouTube and A.I. when you need to solve a problem.\n\nReview it.\n\nAsk someone else to have a look at it too.\n\nHow I‚Äôm learning most of my stuff now.\n\nAnd the blog /youtube/stackechange etc back to the world about it.\n\nI mean that‚Äôs what computer science is all about. ¬†Helping a brother out working on each others stuff.\n\nThen hiring each other.",
          "score": 1,
          "created_utc": "2026-02-15 05:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cvd92",
          "author": "DoomsdayMcDoom",
          "text": "Better off learning K8/Kubernettes since that‚Äôs what‚Äôs replacing docker in every company with a hybrid or cloud infrastructure.",
          "score": -3,
          "created_utc": "2026-02-14 16:02:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dao2a",
              "author": "Black_Magic100",
              "text": "Kubernetes does not replace Docker. \n\nRecommending somebody jump straight to orchestration when they want to learn containerization is also a crazy idea.",
              "score": 8,
              "created_utc": "2026-02-14 17:19:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eesh4",
                  "author": "DoomsdayMcDoom",
                  "text": "Docker never kept up and unfortunately that‚Äôs their fault.  It could of been a better platform but technology advanced and they stayed stagnant.\n\ncontainerd is what Docker uses under the hood to manage containers. Kubernetes was like, ‚Äúwhy not just talk to that directly?‚Äù",
                  "score": 0,
                  "created_utc": "2026-02-14 20:45:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3u0hi",
      "title": "For those who write data pipeline apps using Python (or any other language), at what point do you make a package instead of copying the same code for new pipelines?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r3u0hi/for_those_who_write_data_pipeline_apps_using/",
      "author": "opabm",
      "created_utc": "2026-02-13 16:37:35",
      "score": 40,
      "num_comments": 33,
      "upvote_ratio": 0.94,
      "text": "I'm building out a Python app to ingest some data from an API. The last part of the app is a pretty straightforward class and function to upload the data into S3.\n\nI can see future projects that I would work on where I'm doing very similar work - querying an API and then uploading the data onto S3. For parts of the app that would likely be copied onto next projects like the upload to S3, would it make more sense to write a separate package to do the work? Or do you all usually just copy + paste code and just tweak it as necessary? When does it make sense to do the package? The only trade-off I can think of is managing a separate repository for the reusable package",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r3u0hi/for_those_who_write_data_pipeline_apps_using/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o56ziav",
          "author": "dev81808",
          "text": "Immediately.",
          "score": 45,
          "created_utc": "2026-02-13 17:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57unhq",
              "author": "popopopopopopopopoop",
              "text": "Premature optimisation can be counter productive.",
              "score": 14,
              "created_utc": "2026-02-13 19:34:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59hiie",
                  "author": "MonochromeDinosaur",
                  "text": "Code organization is not premature optimization.",
                  "score": 17,
                  "created_utc": "2026-02-14 00:48:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o58q1zu",
                  "author": "dev81808",
                  "text": "Sure, but I've found that thoughtful early optimization is usually net positive. \n\nWith enough experience, it becomes easier to judge where early effort is worthwhile and where it isn‚Äôt.",
                  "score": 4,
                  "created_utc": "2026-02-13 22:10:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bs7nx",
                  "author": "ZirePhiinix",
                  "text": "So instead of changing one package, you'll now be changing X number of files. This isn't optimization, this is making sure you're actually deploying the same thing across your system.",
                  "score": 1,
                  "created_utc": "2026-02-14 11:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o570s2o",
          "author": "Atticus_Taintwater",
          "text": "For utility stuff that often fits well in a package\n\n\nIt's a loaded question for transformation reuse. But I swear people have forgotten views exist now that python is in the mix.¬†\n\n\nI see so much python module hullabaloo that could just be \"reused\" by way of a regular ass view.",
          "score": 19,
          "created_utc": "2026-02-13 17:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57aclz",
          "author": "Skullclownlol",
          "text": "Write Everything Twice\n\nUsually for deduplication, but it also works for generalization.",
          "score": 16,
          "created_utc": "2026-02-13 17:56:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58m9rr",
              "author": "opabm",
              "text": "I'm not following completely, can you explain what you mean?",
              "score": 9,
              "created_utc": "2026-02-13 21:51:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fyjif",
                  "author": "azirale",
                  "text": "Never write directly to a library/module -- make that the second write. \n\nFirst time using some specific function? Just leave it in the script? Second time writing the exact same thing for the exact same use? Write it into a module/library for the second write. \n\nLater you'll get an eye for things you want to write directly to a module, but if you're not sure just start with local only",
                  "score": 2,
                  "created_utc": "2026-02-15 02:21:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o59953d",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -9,
                  "created_utc": "2026-02-13 23:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57fs8z",
              "author": "Oct8-Danger",
              "text": "This is the way. Good balance of reusing code and having it fit your needs at a time",
              "score": 2,
              "created_utc": "2026-02-13 18:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57jor3",
          "author": "Atmosck",
          "text": "I got there recently. I wrote an internal python package that handles all the boilerplate that gets used by multiple python automations - credential management, logging configuration, s3 operations, redshift and MySQL helpers, API clients with pydantic. Published internally to CodeArtifact.\n\nThe thing that got me to actually do it and made it an easy sell as a project, was an upstream API change we weren't informed about that broke and required updating a whole bunch of things. Now that would just be a matter of updating the package and bumping the version in the projects that use it.",
          "score": 4,
          "created_utc": "2026-02-13 18:41:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56ywp6",
          "author": "davrax",
          "text": "Take a look at dlt(hub)",
          "score": 9,
          "created_utc": "2026-02-13 17:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58q490",
              "author": "toabear",
              "text": "Second this. You will still write some code, but I handles a lot for you.",
              "score": 1,
              "created_utc": "2026-02-13 22:10:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dxbnq",
              "author": "opabm",
              "text": "Looks promising but seems like another package to rely on, no? Would this help much with avoiding have to copy+paste code?",
              "score": 1,
              "created_utc": "2026-02-14 19:12:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jwtsz",
                  "author": "davrax",
                  "text": "Eh, it might remove the need to build most of what you are building.",
                  "score": 1,
                  "created_utc": "2026-02-15 18:53:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57bedk",
          "author": "Tomaxto_",
          "text": "It depends, how many other jobs  in you pipeline share the same data extraction and writing?\n\nIn my case is 90%, hence I build a ‚Äútoolkit‚Äù package and put the reading and writing logic there, add robust tests to it, and CD with uv + S3. On my pipeline repo each jobs share them and only implement the transformations unique to each one.",
          "score": 3,
          "created_utc": "2026-02-13 18:01:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c0l94",
          "author": "umognog",
          "text": "Make a package? Hell, make a container image and use the entry point.",
          "score": 3,
          "created_utc": "2026-02-14 13:03:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5708vg",
          "author": "Big-Touch-9293",
          "text": "I have all of my cloud code hosted on a GitHub, when I push to main it gets versioned and deployed automatically to cloud. \n\nThat being said, I almost exclusively write helper functions and hardly copy paste code, if I do it‚Äôs minimal. I‚Äôll have helpers for normalization, outbound, ingestion etc and just call. By versioning I know that the best, most up to date helper is used and working. That way I know all my code is using the most up to date and nothing is obsolete/unsupported.",
          "score": 1,
          "created_utc": "2026-02-13 17:07:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57abxc",
          "author": "Clever_Username69",
          "text": "Anytime I expect to be use the code more than once I'll make it into a function (or anything at work tbh, with personal projects that can be overkill and I usually write it once messily then rewrite if I feel like it). In your case it seems worth it to have an upload to s3 function within a larger AWS class, if you're starting out and don't see the need for an entire class you can add on later. Either way think of the components that are reusable and define those somewhere to avoid repeating yourself as much as possible. Definitely dont copy/paste the same code (or try not to), it's a bad habit ",
          "score": 1,
          "created_utc": "2026-02-13 17:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57xmpu",
          "author": "dans_clam_pie",
          "text": "Fairly early but contingent on having a reasonably fast dev experience for making quick changes to the util package (eg. Not having to create a PR, wait for ci/cd pipeline publish a version etc‚Ä¶)\n\nInstalling the utils package as an editable python package is sometimes nice, eg:\n\ncreate your utils package and install into your dependent dev repos with ‚Äòuv add ‚Äîeditable /path/to/utils‚Äô (or ‚Äòpip install -e ‚Ä¶‚Äô)",
          "score": 1,
          "created_utc": "2026-02-13 19:48:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57xrcp",
          "author": "Efficient_Sun_4155",
          "text": "If you have a coherent purpose and you know it will be used a few times in different places. Then I‚Äôd make it a package that you can maintain in one place and rely on elsewhere. \n\nFollow decent practices, git tag your versions and automate the build test and publishing of your package. Use auto doc to keep docs up to date automatically and publish them in your CI pipeline",
          "score": 1,
          "created_utc": "2026-02-13 19:49:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o588z8i",
          "author": "BihariGuy",
          "text": "From the get go. As much as it's a pain to keep things modular and super organized in the beginning, it usually pays off pretty well later.",
          "score": 1,
          "created_utc": "2026-02-13 20:45:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58yxq7",
          "author": "tecedu",
          "text": "All the time, any new repo gets pyproject.toml, a runner to build a publish to internal pypi\n\nCode goes into your package, have another folder called runscripts which calls those packages.\n\nIts helps out a lot for a lot of things, you can just pip install again when needed, even when you dont need it you can use paths using library name instead of relative or absolute paths",
          "score": 1,
          "created_utc": "2026-02-13 22:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5955z7",
          "author": "Oldmanbabydog",
          "text": "For me it‚Äôs less about duplication and more about change management. If I have a code that is reused a bunch of places and I need to update it I‚Äôd rather make the update in one place than the same update in 8 different places",
          "score": 1,
          "created_utc": "2026-02-13 23:34:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bfskb",
              "author": "lightnegative",
              "text": "The downside of that of course is that (particularly with Python) you now have to test those 8 pipelines to check that they're not broken, vs just 1",
              "score": 1,
              "created_utc": "2026-02-14 09:59:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5968da",
          "author": "Adrien0623",
          "text": "I try to make my code as generic as possible to have as little work as possible in case we want to duplicate the logic for another topic or if we need to swap a source, destination or logic element",
          "score": 1,
          "created_utc": "2026-02-13 23:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o598hvl",
          "author": "skatastic57",
          "text": "I just made one package, put it on pypi and if there's some function I need a lot then I'll put it in that package. When I make a new venv, script, pipeline, etc then I always know I can just install it and use it regardless of where it will be run from.",
          "score": 1,
          "created_utc": "2026-02-13 23:54:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c5no5",
          "author": "Alonlon79",
          "text": "As best practice - always parameterize your notebooks, your pipelines etc. \nthis is programming 101 that gives you the option to reuse any code you produce by pushing different parameters through an orchestration tool (like ADF or Datafactory in Fabric).\nIf you ingestion patterns are similar this will save a bunch of time.",
          "score": 1,
          "created_utc": "2026-02-14 13:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57j14k",
          "author": "reditandfirgetit",
          "text": "If you have to write the same code more than once, make a package",
          "score": 1,
          "created_utc": "2026-02-13 18:38:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r383ef",
      "title": "Is Microsoft OneLake the new lock-in?",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r383ef/is_microsoft_onelake_the_new_lockin/",
      "author": "AwayCommercial4639",
      "created_utc": "2026-02-12 22:49:03",
      "score": 38,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "I was running some tests on OneLake the other day and I noticed that its performance is 20-30% worse than ADLS. \n\nThey have these 2 weird APIs under the hood: Redirect and Proxy. Redirect is only available to Fabric engines and likely is some internal library for translating OneLake paths to ADLS paths. Proxy is for everything else (including 3rd party engines) and is probably just as it sounds some additional compute layer to hide direct access to ADLS.\n\nI also think that there may be some caching on Fabric side which is only working for Fabric engines...\n\nMy scenario - run a query from Snowflake or Spark k8s against an Iceberg table on ADLS and on OneLake. The performance is not the same! OneLake is always worse especially for tables with lots of files...\n\nSo here is my fear - OneLake is not ADLS. It is NOT operating as open storage. It is operating as a premium storage for Fabric and a sub optimal storage for everything else...\n\nJust use ADLS then.. Yes, we do. But every time I chat with our Microsoft reps they are pushing and pushing me to use OneLake. I am concerned that one day they will just deprecate ADLS in favour of OneLake.\n\nLook Fabric might be decent if you love Power BI, but our business runs on 2 clouds. We have transactional workloads on both, and no way are we going to egress all that data to one cloud or another for analytics. Hence we primarily run an open stack and some multi cloud software like Snowflake.\n\nWhat is wrong with ADLS? Why. do they keep pushing to OneLake? Is this is the next lock-in?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r383ef/is_microsoft_onelake_the_new_lockin/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o54w8tm",
          "author": "Tribaal",
          "text": "Yes, it‚Äôs the next lock in. That‚Äôs it.",
          "score": 33,
          "created_utc": "2026-02-13 09:16:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56693j",
              "author": "Brilliant-Gur9384",
              "text": ">Just use ADLS then.. Yes, we do. But every time I chat with our Microsoft reps they are pushing and pushing me to use OneLake. I am concerned that one day they will just deprecate ADLS in favour of OneLake.\n\nYes, this mirrored the OneDrive fiasco. Remember how it would re-install it every time you deleted it? I guess they finally stopped pushing it because it stayed off, but for a good 2 years, it was a battle to keep OneDrive off an ms machine.\n\n>Yes, it‚Äôs the next lock in. That‚Äôs it.\n\nBingo. If you know AI, then you know these big tech companies want control of data. That's the real value and they know it. The big winners are going to be people/companies who \"get\" this early and choose the right platforms because if you get stuck on a platform that changes agreements after the fact, that could be a major ouch.",
              "score": 5,
              "created_utc": "2026-02-13 14:43:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55onta",
          "author": "TowerOutrageous5939",
          "text": "I‚Äôm hope they push us. My CIO is already very much over MS I would have for this to be the tipping point",
          "score": 6,
          "created_utc": "2026-02-13 13:06:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57m5em",
          "author": "m1nkeh",
          "text": "Yes, it‚Äôs massive lock in.. it‚Äôs ADLS with shackles",
          "score": 3,
          "created_utc": "2026-02-13 18:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55y9yx",
          "author": "thecoller",
          "text": "Not long ago, reading OneLake from non-Fabric computer was 3x more expensive than reading from Fabric. Hopefully customers keep up the pressure and keep choosing ADLS so the rest of the barriers come down.",
          "score": 2,
          "created_utc": "2026-02-13 14:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57m9pc",
              "author": "m1nkeh",
              "text": "Oh, is this no longer the case, the 3x thing?",
              "score": 1,
              "created_utc": "2026-02-13 18:53:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57u3qw",
                  "author": "thecoller",
                  "text": "Fortunately not, as of a couple of months ago",
                  "score": 1,
                  "created_utc": "2026-02-13 19:31:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o563he7",
          "author": "TheRealStepBot",
          "text": "Fabric and one lake is crap. üìé",
          "score": 6,
          "created_utc": "2026-02-13 14:28:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57g6dz",
          "author": "Sea-Meringue4956",
          "text": "Onelake is on top of ADLS and yet costs 10x more the last time I checked. I dont think though that ADLS will stop to exist.",
          "score": 3,
          "created_utc": "2026-02-13 18:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56ues6",
          "author": "Low_Second9833",
          "text": "Trust your instincts.",
          "score": 1,
          "created_utc": "2026-02-13 16:39:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57to2a",
          "author": "Frosty-Practice-5416",
          "text": "So happy I don't have to think about that crap anymore",
          "score": 1,
          "created_utc": "2026-02-13 19:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57fqgh",
          "author": "thpeps",
          "text": "Hey - I'm a PM with OneLake, the performance you're experiencing is not expected, can you send me a DM so we can get in touch and I can help debug what you're seeing. Thanks!",
          "score": -3,
          "created_utc": "2026-02-13 18:22:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57mclg",
              "author": "m1nkeh",
              "text": "DM me if you need a referral out of MS ‚úåÔ∏è",
              "score": 6,
              "created_utc": "2026-02-13 18:53:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r49mr4",
      "title": "Is my ETL project at work using Python + SQL well designed? Or am I just being nitpicky",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r49mr4/is_my_etl_project_at_work_using_python_sql_well/",
      "author": "masterhoovy",
      "created_utc": "2026-02-14 03:15:59",
      "score": 38,
      "num_comments": 28,
      "upvote_ratio": 0.84,
      "text": "Hey all,\n\nI'm a fairly new software engineer who's graduated school recently. I have about \\~2.5YOE including internships and a year at my current job. I've been working on an ETL project at work that involves moving data from one platform via an API to a SQL database using Python. I work on this project with a senior dev with 10+YOE.\n\nA lot of my work on this project feels like I'm reinventing the wheel. My senior dev strives for minimizing dependencies to not be tied to any package which makes sense to some extent, but we are only really using a standard API library and pyodbc. I don't really deal with any business logic and have been basically recreating an ORM from the ground up. And at times I feel like I'm writing C code, like checking for return codes and validating errors at the start of every single method and not utilizing exceptions. \n\nI don't mean to knock this senior dev in any way, he has a ton of experience and I have learned a lot about writing clean code, but there are some things that throw me off from what I read online about Python best practices. From what I read, it seems like SQLAlchemy, Pydantic, and Prefect are popular frameworks for creating ETL solutions in Python. \n\nFrom experienced Python developers: is this approach ‚Äî sticking to vanilla Python, minimizing dependencies, and using very defensive coding patterns ‚Äî considered reasonable for ETL work? Or would adopting some standard frameworks be more typical in professional projects?",
      "is_original_content": false,
      "link_flair_text": "Help",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r49mr4/is_my_etl_project_at_work_using_python_sql_well/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5a5utd",
          "author": "reditandfirgetit",
          "text": "Rewriting things that are already written is insane. The packages exist, and the ones you'll need are well maintained. It also becomes a maintenance nightmare. I suspect the sr dev does this for some kind of idiotic job security",
          "score": 74,
          "created_utc": "2026-02-14 03:23:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5btu3u",
              "author": "SoggyGrayDuck",
              "text": "I've made this mistake. Shit like that is miserable",
              "score": 7,
              "created_utc": "2026-02-14 12:10:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5a62cy",
              "author": "masterhoovy",
              "text": "I get the impression he's concerned with performance and wants a small footprint. It seems he has a strong background in low-level and C programming. But yeah it's fairly strange to me",
              "score": 7,
              "created_utc": "2026-02-14 03:25:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5au524",
                  "author": "VipeholmsCola",
                  "text": "he should write in c++ then or rust. Python way is to use whatever tools you need to ship stuff.",
                  "score": 29,
                  "created_utc": "2026-02-14 06:32:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5btkju",
                  "author": "reditandfirgetit",
                  "text": "Then you find a package and run a timing test, you don't assume you can write cleaner more optimal code. That's not a senior mentality.\n\nIf the package is way slower than you want you can roll your own or get into the package code and optimize there.",
                  "score": 9,
                  "created_utc": "2026-02-14 12:08:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5e5fbz",
                  "author": "runawayasfastasucan",
                  "text": "Doesn't matter, he wont make it more performant.",
                  "score": 1,
                  "created_utc": "2026-02-14 19:54:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5a5iem",
          "author": "Peppper",
          "text": "Yeah, seems like a bad idea. What‚Äôs the value add for not using libraries. Are SQLAlchemy or Polars going away anytime soon? Better value add work would be to concentrate resources on delivering analytics and data products.",
          "score": 13,
          "created_utc": "2026-02-14 03:21:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ae6ng",
              "author": "Thadrea",
              "text": "Using off the shelf libraries, yes, no good reason not to.\n\nThere is definite value to avoiding unnecessary internal dependencies on stuff managed by other teams if the application is business or mission critical.",
              "score": 7,
              "created_utc": "2026-02-14 04:22:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5af4hx",
                  "author": "Peppper",
                  "text": "Totally fair. It‚Äôs definitely a balance, but seems like OP‚Äôs dev is too dogmatic.",
                  "score": 2,
                  "created_utc": "2026-02-14 04:29:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5abq3d",
          "author": "cmcclu5",
          "text": "I‚Äôve done it. You‚Äôll learn a lot of excellent Python mechanics, but you‚Äôre taking the first step to the next level by being aware of other packages that can take the place of your ‚Äúlow-level‚Äù work. SQLAlchemy has limitations, so a lot of groups default back to pyodbc or psycopg2 anyway. Polars is excellent, but still locks you into a core set of functions (pandas is even worse). Sometimes, it‚Äôs best to start at the standard library, depending on your use case. Your senior is giving you a ton of experience a lot of seniors I‚Äôve met don‚Äôt have. Just be aware that it won‚Äôt always be applicable.",
          "score": 5,
          "created_utc": "2026-02-14 04:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b4ofe",
          "author": "drag8800",
          "text": "Both approaches have their place but the context matters a lot. I've built ETL pipelines both ways over 12+ years.\n\nMinimizing dependencies makes sense when you're building something simple and stable that won't need much maintenance. We had a batch job that ran for 8 years with zero external packages beyond requests and psycopg2. Worked great.\n\nBut rebuilding ORM patterns from scratch is different from just not using one. If you're writing your own query builders, connection pooling, retry logic, and type coercion, you're now maintaining all that code forever. Every bug fix, edge case, and security patch is on you. SQLAlchemy has had hundreds of contributors finding problems you'll never think of.\n\nThe return code pattern concerns me more than the dependency choice honestly. Python's exception model exists for a reason and fighting it creates code that's harder to read and debug. If your senior prefers explicit error handling, that's fine, but there are Pythonic ways to do it without making every function look like C.\n\nThe real question is what's the maintenance horizon here. If this is a one time data migration that runs and gets archived, vanilla Python is fine. If this is production infrastructure your team will maintain for years, the cost of rolling your own ORM will compound. Every new team member has to learn your custom patterns instead of reaching for documentation.\n\nWorth having a direct conversation about it. Ask what specific concern drives the no dependencies stance. Sometimes it's a bad experience with a package breaking, which is valid but solvable with pinned versions and testing.",
          "score": 10,
          "created_utc": "2026-02-14 08:10:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cydbo",
              "author": "masterhoovy",
              "text": ">The return code pattern concerns me more than the dependency choice honestly. Python's exception model exists for a reason and fighting it creates code that's harder to read and debug. If your senior prefers explicit error handling, that's fine, but there are Pythonic ways to do it without making every function look like C.\n\nYeah I'm still trying to wrap my head around using return codes in Python. Could you provide some examples of how this can be done in a pythonic way? I feel like it would be way easier to just log exceptions but he is pretty adamantly opposed to exceptions and stack traces and would rather log all our messages manually.",
              "score": 1,
              "created_utc": "2026-02-14 16:17:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g440c",
                  "author": "drag8800",
                  "text": "Few patterns that work without going full C:\n\nSimple tuple returns:\n```python\ndef fetch_user(id):\n    if not id:\n        return None, \"missing id\"\n    user = db.get(id)\n    if not user:\n        return None, \"not found\"\n    return user, None\n\nuser, err = fetch_user(123)\nif err:\n    logger.error(err)\n    return\n```\n\nDataclass Result type:\n```python\n@dataclass\nclass Result:\n    value: Any = None\n    error: str = None\n    \n    @property\n    def ok(self):\n        return self.error is None\n```\n\nThen your code reads `if not result.ok: handle_error()` which is cleaner than tuple unpacking everywhere.\n\nThe `returns` library does this more formally if you want railway-oriented programming, but that might be overkill for ETL.\n\nHonestly though - the real question is why stack traces are the enemy. They're the single best debugging tool when something breaks in prod. Manual logging means you're recreating what the runtime gives you for free, except worse. Might be worth understanding what burned him before. Sometimes it's \"stack traces leaked to users\" which is a presentation problem, not an exception problem.",
                  "score": 1,
                  "created_utc": "2026-02-15 02:59:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5a80sj",
          "author": "Treemosher",
          "text": "I wish I had a senior dev to work under.  I basically did what you're doing but flying solo.  Had to teach myself everything.  It was pretty rough.\n\nI doubt any of this will be useful, all I really have is color commentary here.\n\nI did use SQLAlchemy.  I had nobody telling me no, I just communicated to IT what I was doing and got their blessing in email to cover my ass.\n\nAside from that, I think I was also pretty strict about choosing libraries.  I try to stick to stuff that's in the Anaconda suite since it seems to get a buttload of community and support.\n\nIf a vendor has their own libraries that they actively maintain, I'll use those.  \n\nIf it's a real niche thing by some dude and I can't find a way to feel good about it, hell no.  I've never had anything bad happen, but it only takes one time.  \n\nI don't think my situation is a very good comparison.  Nobody was really supporting me or looking at my work.  I just packaged code where I could reference a config file like a control table.  Send queries, save them as flat files, do the things like package them into this or that or upload them here or there.\n\nI was never given the greenlight for an actual database, but it got us by in the interim as we grew.  \n\nAgain, my situation was very specific with weird constraints.\n\nIf your senior dev dude is having you basically write your own stuff instead of using a library, take that is a chance to reverse engineer a process that many might take for granted.  A few years from now you'll be that much more versed in things I guess?  \n\nLike, I had nobody to review my code.  If anyone had to go back and look at it, it was me.  So that forced me to write in a way that didn't force me to curse my own name.  What feels like a setback can sometimes make you stronger in the end.",
          "score": 5,
          "created_utc": "2026-02-14 03:38:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bczyd",
          "author": "Outside-Storage-1523",
          "text": "My understanding is that you pull data from API to a DB so most likely you don‚Äôt need to do much, if any SQL transformation. Maybe SQLAlchemy is a bit overkill? Do you insert the rows one by one? I think as long as you can insert them in batch that should be fine.",
          "score": 3,
          "created_utc": "2026-02-14 09:32:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ago78",
          "author": "calimovetips",
          "text": "keeping deps light is fine for simple pipelines, but rebuilding orm logic and avoiding exceptions is usually unnecessary overhead.\n\ni‚Äôd ask what real constraint justifies it, long term maintainability usually favors proven libraries.",
          "score": 2,
          "created_utc": "2026-02-14 04:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bbd32",
          "author": "Sensitive-Sugar-3894",
          "text": "Detach your SQL from the code. I don't use ORMs for ETL. System Engineering, maybe. Data Engineering, no.\n\nAre you using Windows for the DB? Why?",
          "score": 2,
          "created_utc": "2026-02-14 09:15:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bhdo9",
          "author": "LookAtTheHat",
          "text": "Is there any company regulations stating you cannot use 3rd party libraries unless they have been approved?",
          "score": 1,
          "created_utc": "2026-02-14 10:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5btok5",
          "author": "AcanthisittaEarly983",
          "text": "To maintain or not to maintain.¬†",
          "score": 1,
          "created_utc": "2026-02-14 12:09:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5caypy",
          "author": "PeterDowdy",
          "text": "You should probably not homebrew these tools if at all possible. Have you considered a framework like dbt?",
          "score": 1,
          "created_utc": "2026-02-14 14:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f2v9b",
          "author": "prehensilemullet",
          "text": "I don‚Äôt know that much about python orms, but how good are they at doing bulk inserts with associations efficiently? ¬†Because I‚Äôve seen Typescript orms do bulk inserts in Postgres in really sub-par ways compared to the queries i can write by hand",
          "score": 1,
          "created_utc": "2026-02-14 22:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5g4pty",
          "author": "WhiteWillow132",
          "text": "The only reason you do what this dev is doing is for security concerns or because he wants the installed package to be size constrained, likely due to where it‚Äôs being deployed. Otherwise it‚Äôs pointless. \n\nPulling data from an API is also to obscure of a comment. Is it batch, transactional, is there latency, is it paginated? Too many unknowns for anyone here to give you real advice.",
          "score": 1,
          "created_utc": "2026-02-15 03:03:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gkbjo",
          "author": "FORCEUPDATEALL",
          "text": "You are leaving a lot of context unclear. How big is your company and what is the data volume? Any data confidentiality? How important is data quality (this might justify expanding the work on the return codes). \n\nI‚Äôve been doing ETL, data integrations, and API extractions for a while now, and one thing that‚Äôs universally true is this: APIs are not designed to be persistent or cheap. You want to hit them as few times as possible, and you want to do it quickly and predictably. That alone can justify a more defensive, dependency‚Äëlight approach depending on the environment. What i am saying is you generally don‚Äôt want your transform step or load step to depend on live API calls. It‚Äôs better to extract once, stage the raw data, and transform or load from the staged files.",
          "score": 1,
          "created_utc": "2026-02-15 04:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h0bug",
          "author": "acana95",
          "text": "Your senior is making up work so both of you wont be fired haha. Cleaning tech debts is only worth doing whenever it they slow down the data products otherwise as long as they meet sla and dont have critical bugs or security issues, its better for you to learn new tech or work on developing pipelines for business cases. In the end, a best pipeline isnt the best pipeline for a business if it is not generating money",
          "score": 1,
          "created_utc": "2026-02-15 07:19:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lbi7c",
          "author": "Certain_Leader9946",
          "text": "No he's not insane but doing this in a language like python is orthogonal to the reason why it exists. If you're doing that at least use Go and SQLC. Which is broadly what all my stuff (Databricks included) is based on.",
          "score": 1,
          "created_utc": "2026-02-15 23:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c5mxq",
          "author": "Nekobul",
          "text": "If you are using SQL Server, it is crazy not to take advantage of the SSIS and the available third-party extensions. You will save tons of time and have a more stable solution.",
          "score": 0,
          "created_utc": "2026-02-14 13:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hqk2i",
              "author": "NoleMercy05",
              "text": "While true, this crowd doesn't want to hear it.",
              "score": 2,
              "created_utc": "2026-02-15 11:31:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b5pt7",
          "author": "i_make_maps_0",
          "text": "Getting requirements, reading API docs and usage rules, setting up python process (25 lines max) with exception-handling, logging, monitoring, alerting, and auto-restart-on-death, creating indexed raw/staging tables, should take no more than 10-20 minutes. Will you be onboarding dozens of disparate sources API/scraping? Or is this a one-off for which you need data refreshed constantly for the one source?",
          "score": 0,
          "created_utc": "2026-02-14 08:20:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5p40y",
      "title": "Started a new DE job and a little overwhelmed with the amount of networking knowledge it requires",
      "subreddit": "dataengineering",
      "url": "https://www.reddit.com/r/dataengineering/comments/1r5p40y/started_a_new_de_job_and_a_little_overwhelmed/",
      "author": "starrorange",
      "created_utc": "2026-02-15 20:29:15",
      "score": 37,
      "num_comments": 41,
      "upvote_ratio": 0.86,
      "text": "Maybe I was naive to think it was mainly pipelining on top of a platform like azure or databricks but I‚Äôm in the middle of figuring out how to ping and turn on servers etc. I‚Äôm going to read up on Linux and some other recommended textbooks but just overwhelmed I guess. I did math in undergrad and did cs for my masters so I opted out of the networking classes thinking I would never need it. ",
      "is_original_content": false,
      "link_flair_text": "Career",
      "permalink": "https://reddit.com/r/dataengineering/comments/1r5p40y/started_a_new_de_job_and_a_little_overwhelmed/",
      "domain": "self.dataengineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5l2r2z",
          "author": "masapadre",
          "text": "DE means a different thing in each company.\nI had to learn too a lot of networking for my current DE position. Vnets, peerings, private endpoints, dns resolution and so on but what my company calls data engineer to me is more like a software & platform engineer",
          "score": 37,
          "created_utc": "2026-02-15 22:27:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lnlf7",
              "author": "IAMHideoKojimaAMA",
              "text": "So what data are you actually engineering üòÇ",
              "score": 4,
              "created_utc": "2026-02-16 00:28:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5oharx",
                  "author": "masapadre",
                  "text": "I extract data from text logs and then I save to MongoDB. We can call that ETL and I did a little bit of data modeling (how am I saving this to mongo, how are we going to query this data, and so on). Nothing fancy.\nI look forward to start doing things with Databrick, have a proper lakehouse, work with tables, that kind of thing. We have that in the roadmap",
                  "score": 1,
                  "created_utc": "2026-02-16 13:31:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5os6n2",
              "author": "king_booker",
              "text": "Any good resources for this? \n\n",
              "score": 1,
              "created_utc": "2026-02-16 14:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pfbq7",
                  "author": "masapadre",
                  "text": "I followed ‚ÄòGetting Started with Azure Networking Services‚Äô by Houssem Dellai. Recommended if you work with Azure.",
                  "score": 1,
                  "created_utc": "2026-02-16 16:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kgjl9",
          "author": "UhhSamuel",
          "text": "Any entry into DE right now is a good opportunity, but in my 7.5 years as a DE I've never pinged or turned on a server.",
          "score": 91,
          "created_utc": "2026-02-15 20:33:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l1aer",
              "author": "Mindless_Let1",
              "text": "You've never set up an ec2 instance or nothing? Wild",
              "score": 36,
              "created_utc": "2026-02-15 22:20:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ljphy",
                  "author": "Typical_Priority3319",
                  "text": "Just copy that one value with the command and paste it into the terminal‚Ä¶ no not that one the other one",
                  "score": 13,
                  "created_utc": "2026-02-16 00:05:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5oe300",
                  "author": "Inner-Block8845",
                  "text": "I think you can do that through Terraform or Cloud Functions so I am pretty sure all is programming and reading documentation and stakeholders requirements.",
                  "score": 2,
                  "created_utc": "2026-02-16 13:12:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5lt8ah",
              "author": "SalamanderPop",
              "text": "7.5 years and you've never done any infrastructure work? That's crazy in this day and age.",
              "score": 12,
              "created_utc": "2026-02-16 01:02:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5m0vor",
                  "author": "UhhSamuel",
                  "text": "I've done plenty of infra as code, if that helps. But networking is almost completely a black box to me. ",
                  "score": 5,
                  "created_utc": "2026-02-16 01:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5m6azb",
              "author": "No_Airline_8073",
              "text": "We own a Kubernetes cluster, minus the networking part",
              "score": 3,
              "created_utc": "2026-02-16 02:25:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l68rs",
          "author": "DenselyRanked",
          "text": "Are you working with a cloud provider? If so, then refer to their training modules. If not, then take this as an opportunity to create a run book for your team.",
          "score": 7,
          "created_utc": "2026-02-15 22:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kgnjo",
          "author": "xean333",
          "text": "Hmm networking is usually out of scope for a data engineer. Sounds like you‚Äôve got yourself a fuzzy role my friend. Make the best of it. You studied math - networking isn‚Äôt harder than abstract algebra or complex analysis. Good luck",
          "score": 31,
          "created_utc": "2026-02-15 20:33:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kkuqt",
              "author": "VEMODMASKINEN",
              "text": "Err, networking is never out of scope for anyone working in IT.¬†\n\n\nIt's basically one of the few things everyone should know at least the basics of because whatever you do will involve transmitting data over networks.¬†",
              "score": 43,
              "created_utc": "2026-02-15 20:55:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5njn0g",
                  "author": "sib_n",
                  "text": "You can say this about a lot of things in IT. IT professions have a core and then depending on companies and projects you may explore different things: streaming, ML, CICD, frontend, security etc. It's common to have an infrastructure team that abstract networking for you so you can focus on the data instead, so it's normal to not have experience with it as a DE.",
                  "score": 4,
                  "created_utc": "2026-02-16 08:56:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5knjir",
                  "author": "Plus-Willingness-324",
                  "text": "This.",
                  "score": 10,
                  "created_utc": "2026-02-15 21:09:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5kqpir",
                  "author": "xean333",
                  "text": "I agree we should all know the basics of networking. I suspect OP is being held to higher expectations than day 1 networking. Maybe I‚Äôm wrong!",
                  "score": 3,
                  "created_utc": "2026-02-15 21:25:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5n7xxa",
                  "author": "zangler",
                  "text": "This",
                  "score": 1,
                  "created_utc": "2026-02-16 07:06:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5l5q0m",
                  "author": "Dry_Philosophy7927",
                  "text": "I'm a data scientist doing research and contemplating a future move to.... somewhere else. I'm scared for exactly this reason. Where/how should i plug my knowledge? Just a starter for 10 pls",
                  "score": 1,
                  "created_utc": "2026-02-15 22:44:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5mylgu",
              "author": "Fit_Highway5925",
              "text": "If you're a data engineer who's focused mainly on platform engineering & infrastructure, then I don't think it's out of scope. You have to really know how networks work. \n\nThe thing about DE is that it's so broad and has many flavors that what one DE is doing may be totally different from another. \n\nI've experienced this firsthand. In my previous DE role, I did mostly ETL development and I never dealt with networks. Now I'm working mostly in data infra & platform engineering, and networks are suddenly everywhere. Good thing I have somewhat of a background from my college degree so it came in handy.",
              "score": 5,
              "created_utc": "2026-02-16 05:46:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5l7edv",
              "author": "greenestgreen",
              "text": "what? Is very much involved. How would you set up a spark cluster if it was needed on premise?\nCollecting data from system in different networks, crossing firewalls, etc.",
              "score": 5,
              "created_utc": "2026-02-15 22:53:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5lkjqt",
                  "author": "xean333",
                  "text": "I‚Äôve always had admin/infra guys that are responsible for setting me up with what I need",
                  "score": 9,
                  "created_utc": "2026-02-16 00:10:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5lcwsd",
                  "author": "BobBarkerIsTheKey",
                  "text": "I've only worked with spark in AWS glue jobs. I've set up small spark clusters at home with maybe 3 machines before but it would be incredibly easy to not know anything about it  because it's been abstracted away for me. ",
                  "score": 1,
                  "created_utc": "2026-02-15 23:24:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kinvv",
              "author": "PhraatesIV",
              "text": "Some of the math guys I know would certainly disagree with your last sentence :)",
              "score": 1,
              "created_utc": "2026-02-15 20:44:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l98g7",
          "author": "peterxsyd",
          "text": "Time to get the books out",
          "score": 3,
          "created_utc": "2026-02-15 23:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpxag",
          "author": "confusing-world",
          "text": "How many years of experience do you have?",
          "score": 2,
          "created_utc": "2026-02-15 21:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5krrtw",
          "author": "reditandfirgetit",
          "text": "Ping as in check if it's up or ping as in the network pinging an IP address?\n\nAlso what exactly do you mean turn on a server? Do you mean setup/standup a server?",
          "score": 2,
          "created_utc": "2026-02-15 21:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5letzx",
          "author": "Only-Succotash-8829",
          "text": "Having worked across all clouds, I've found Databricks and Azure to be one with the most convoluted networking, LZ architecture and RBAC setup.¬†\n\n\nThat said, what kind of servers are you maintaining?¬†\n\n\nTry to speak with your IT department as they may already have ownership of this. Often times theres duplication of effort happening in orgs and there may already be processes in place, just not readily triggered by the teams needing them - and instead we end up doing it ourselves as we feel capable enough, even at our own detriment long term.",
          "score": 1,
          "created_utc": "2026-02-15 23:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oajry",
          "author": "Ok-Sentence-8542",
          "text": "Well you skipped networking thats on you. It all comes back to basics but its pretty easy to learn all about networking on the internet. Just do it.",
          "score": 1,
          "created_utc": "2026-02-16 12:48:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n877t",
          "author": "zangler",
          "text": "Some people need to understand that NOTHING you do matters without network... fucking learn some basics...like out of pure intellectual curiosity",
          "score": 1,
          "created_utc": "2026-02-16 07:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oal34",
              "author": "mcgrst",
              "text": "Yeah... I work in a very large Corp, I have people who set that up. A DE in my company wouldn't be allowed anywhere near networking, we're given APIs or database locations all the underlying stuff is managed for us.¬†",
              "score": 2,
              "created_utc": "2026-02-16 12:49:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5od6sq",
                  "author": "zangler",
                  "text": "All the more reason to understand it. It will help",
                  "score": 1,
                  "created_utc": "2026-02-16 13:06:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ks7j3",
          "author": "Mefsha5",
          "text": "Some networking is required at the DE level, enough to build and test connections and through-put, and knowing how to expose your systems to others.\n\nMostly, its about understanding the existing network design and working within them.",
          "score": 0,
          "created_utc": "2026-02-15 21:33:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}