{
  "metadata": {
    "last_updated": "2026-03-02 02:56:22",
    "time_filter": "week",
    "subreddit": "Qwen_AI",
    "total_items": 20,
    "total_comments": 151,
    "file_size_bytes": 156526
  },
  "items": [
    {
      "id": "1rdnzbe",
      "title": "Connected Qwen3-VL-2B-Instruct to my security cameras, result is great",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/gallery/1rdnzbe",
      "author": "solderzzc",
      "created_utc": "2026-02-24 18:17:10",
      "score": 347,
      "num_comments": 69,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Qwen VL",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdnzbe/connected_qwen3vl2binstruct_to_my_security/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o77mbht",
          "author": "beedunc",
          "text": "Frankly, those tiny Qwen VL models are going to change the world. Nice work!",
          "score": 18,
          "created_utc": "2026-02-24 21:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77xmlc",
              "author": "solderzzc",
              "text": "Agreed. Qwen VL is really good. ",
              "score": 6,
              "created_utc": "2026-02-24 22:19:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77eegx",
          "author": "Firm-Evening3234",
          "text": "Bel progetto, da approfondire e integrare su django",
          "score": 6,
          "created_utc": "2026-02-24 20:50:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77jvoa",
              "author": "solderzzc",
              "text": "Quale funzionalit√† ti serve integrare in Django",
              "score": 1,
              "created_utc": "2026-02-24 21:15:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77k8hh",
                  "author": "Firm-Evening3234",
                  "text": "Vorrei ricreare l'ambiente unifi per i sistemi nvr, database persone, riconoscimento di atti ostili etc etc",
                  "score": 2,
                  "created_utc": "2026-02-24 21:16:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gddno",
          "author": "Bohdanowicz",
          "text": "I did something similar with ring, then I decided to add gait analysis  and torso to leg ratio analysis so people leaving with their backs to the camera were identified (used video of people approaching camera + facial recognition to match gait/face so even if it doesn't know who it is it still knows its the same person, then decided to add a wildlife detector and had it build out a database of all the  local wildlife and then started analyzing the wildlife patterns....  All local inference.  Next cameras I get won't need amazon.  Just a few high def 4k cameras and i'll connect it to whatever I want and do whatever I want.\n\nWelcome to the singularity.",
          "score": 4,
          "created_utc": "2026-02-26 03:45:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7yhjk9",
              "author": "cangec",
              "text": "What kind of wildlife patterns did you notice? Did you detect a lot of wildlife?",
              "score": 1,
              "created_utc": "2026-02-28 22:46:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ymhav",
                  "author": "Bohdanowicz",
                  "text": "https://preview.redd.it/m9k1ecnjhbmg1.png?width=1101&format=png&auto=webp&s=dde264937363b72aa7d6c2b1172e1344e6698c95\n\nI was running instruct so this was enough for me.  I used opencv to find the parts of the video with the most movement and used that as a base keyframe then ran +/- 5 sec so I could rip through 1000+ video feeds extremely fast on local inference. (3.5 seconds per video)\n\nIt was great during the day but the night vision had issues.  It would confuse raccoon/skunk/groundhog... but to be fair in half the shots I couldn't tell the difference either so take that analysis with a grain of salt.  \n\n",
                  "score": 1,
                  "created_utc": "2026-02-28 23:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76itv8",
          "author": "Samy_Horny",
          "text": "It's a shame that we'll have to wait even longer for the latest batch of small Qwen 3.5 models.",
          "score": 3,
          "created_utc": "2026-02-24 18:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76j0np",
              "author": "solderzzc",
              "text": "Yes, hopefully we will get them soon. ",
              "score": 2,
              "created_utc": "2026-02-24 18:26:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7jw20n",
              "author": "alexx_kidd",
              "text": "why do you say that you read anything relevant somewhere?",
              "score": 1,
              "created_utc": "2026-02-26 17:49:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jwt3u",
                  "author": "Samy_Horny",
                  "text": "Because I follow the main engineers, and also, in the early days of the Github commit, they referenced small models; the large version was never seen around.",
                  "score": 2,
                  "created_utc": "2026-02-26 17:53:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o794h9i",
          "author": "mike7seven",
          "text": "The Qwen models are impressive for sure, for its size running on GGUF on Mac makes it even more impressive. Does the app allow MLX Versions of the models? I only ask because the MLX (MLX-VLM)versions take up fewer resources and are faster.",
          "score": 3,
          "created_utc": "2026-02-25 02:12:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7980hd",
              "author": "solderzzc",
              "text": "Not yet, I'll look into MLX-VLM for an integration, thanks for your information. ",
              "score": 2,
              "created_utc": "2026-02-25 02:32:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79aoa6",
                  "author": "mike7seven",
                  "text": "Not sure it would do anything additional other than being able to say ‚ÄúA blade of grass moved‚Äù.",
                  "score": 2,
                  "created_utc": "2026-02-25 02:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bos99",
          "author": "ConversationFun940",
          "text": "What's the ui used here.. sorry new to ai",
          "score": 3,
          "created_utc": "2026-02-25 13:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bya1b",
              "author": "solderzzc",
              "text": "The UI is an application developed by me, framework is the same as VSCode ( Electron based ). You can download it here: https://www.sharpai.org ",
              "score": 2,
              "created_utc": "2026-02-25 14:32:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7gcz3z",
                  "author": "dyeusyt",
                  "text": "Ngl instead of focusing on personal home security, you should probably pivot the product toward enterprise or factory-level use cases; maybe something like tracking shipments and flagging anomalies more effectively ",
                  "score": 2,
                  "created_utc": "2026-02-26 03:42:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k7ewh",
                  "author": "sledmonkey",
                  "text": "What sort of prompting do you give Qwen?",
                  "score": 2,
                  "created_utc": "2026-02-26 18:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77mf2g",
          "author": "Artem_C",
          "text": "Yeah, you just pretty much doxxed yourself. Atleast blur the street across",
          "score": 2,
          "created_utc": "2026-02-24 21:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77o1b8",
              "author": "solderzzc",
              "text": "... You are ring, will do next time, it doesn't allow me to edit the photo now. ",
              "score": 2,
              "created_utc": "2026-02-24 21:34:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79uzqo",
          "author": "Sadboy2403",
          "text": "so how many tokens per hr watched? I know its going to consume more if theres activity in the camera but how much? ",
          "score": 2,
          "created_utc": "2026-02-25 04:52:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79w9do",
              "author": "solderzzc",
              "text": "If local model is being used for video analysis, the video analysis will cost no token.\nCloud model(gpt-4o) to handle video will be around $5 per day, I have cameras located in-house, so it generated more than 150+ clips per day. 3M input, 2M output.\n\nCloud LLM will be used when you chat, it handles tool call. Summary VLM's generation, search through the video summary for the user's query. Something around 500k inputs, 80k outputs. per day. (GPT-5.2) \n\nI'm testing an integration with lmstudio, to leverage QWEN-27GB locally as the brain model for tool call and conversation (LLM) on M3 24GB, not done yet.  ",
              "score": 1,
              "created_utc": "2026-02-25 05:01:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o79wi6u",
              "author": "solderzzc",
              "text": "Motion detection is at the frontend, did optimization for one clip as well, not send every frame. If send all the frames to cloud for watching, it's about $0.1 per 20s. ( gpt-4o ).",
              "score": 1,
              "created_utc": "2026-02-25 05:03:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ae8or",
          "author": "mihaii",
          "text": "it's a pity u can't use a LLM on premise (i see that u can only use OpenAI API at this point). ",
          "score": 2,
          "created_utc": "2026-02-25 07:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bxoay",
              "author": "solderzzc",
              "text": "Yes, I'm testing QWen on premise, QWEN3 Coder Instruct 28B(hosted by lmstudio) could be running on my 24GB MACBOOK AIR M3, I'll release it soon. \nIts tool use capability is good. Haven't tested it throughly. ",
              "score": 1,
              "created_utc": "2026-02-25 14:29:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7d07tp",
                  "author": "mihaii",
                  "text": "but at this point, there is no way of using a local LLM  , just the visual AI\n\nany reasons for not going with only one LLM? that does both vision and text?\n\nis the project opensource / vibecoded?",
                  "score": 2,
                  "created_utc": "2026-02-25 17:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7at556",
          "author": "Virtual_Sherbert6846",
          "text": "I've been trying to piece together object detection for my robot and it is terrible. Something like this is a big upgrade.",
          "score": 2,
          "created_utc": "2026-02-25 09:45:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7by1do",
              "author": "solderzzc",
              "text": "What hardware you are using? I have DGX Spark ( CUDA 13 ), Jetson Nano / AGX. Let me know if an aarch64 release is required.",
              "score": 1,
              "created_utc": "2026-02-25 14:31:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7efy29",
                  "author": "Virtual_Sherbert6846",
                  "text": "Jetson Orin NX 16GB. I am also running a STT and TTS model on the Jetson. I may offload to my PC with a 4090 RTX.",
                  "score": 2,
                  "created_utc": "2026-02-25 21:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bnhqo",
          "author": "BelieverInYellow",
          "text": "Omg that output is super detailed! :0",
          "score": 2,
          "created_utc": "2026-02-25 13:34:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7byc3u",
              "author": "solderzzc",
              "text": "Yes, too detailed :) ",
              "score": 1,
              "created_utc": "2026-02-25 14:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cpng1",
          "author": "Obvious_Fix_1012",
          "text": "Very cool! Nice job.",
          "score": 2,
          "created_utc": "2026-02-25 16:42:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cripc",
          "author": "SearchTricky7875",
          "text": "damn, that is one useful use, I can use it with my raspberry pi.",
          "score": 2,
          "created_utc": "2026-02-25 16:50:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cyswl",
              "author": "solderzzc",
              "text": "Got it, let me prepare a Raspberry Pi build. ",
              "score": 1,
              "created_utc": "2026-02-25 17:24:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d6jm0",
          "author": "Plenty-Mix9643",
          "text": "What does that bring? I mean it is cool, but what is the benefit of it for you.",
          "score": 2,
          "created_utc": "2026-02-25 17:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ddlth",
              "author": "solderzzc",
              "text": "To be honest, it started with a personal annoyance: I have 'stupid' cameras that I pay good monthly fees for, yet I still have to scrub through hours of footage myself to find anything.\n\nThe benefit for me is two-fold:\n\nIntelligence & Automation: I want to 'teach' my cameras what to look for so I don't have to. Aegis pulls my cloud clips (Ring/Blink wired/battery) locally so I can search them with a private, local LLM. This weekend project honestly would have been impossible without vibe coding‚Äîit's allowed me to hit 400k lines of logic at a speed traditional dev couldn't touch.\n\nThe 'GitHub' Model: I believe the future of AI is local. My plan is to keep a powerful free version for homeowners to regain their privacy. The business model follows the GitHub or Slack approach: provide massive value to the community for free, while providing support needed for SMB and Enterprise‚Äîan area where I‚Äôve spent my career training models and building products.",
              "score": 1,
              "created_utc": "2026-02-25 18:30:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d7fv8",
          "author": "jedsk",
          "text": "awesome!",
          "score": 2,
          "created_utc": "2026-02-25 18:03:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7itrlb",
              "author": "Wide-Personality6520",
              "text": "For real! The detail it captures is next level. Have you tried it on different scenes yet?",
              "score": 1,
              "created_utc": "2026-02-26 14:51:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ef1rc",
          "author": "Busy-Guru-1254",
          "text": "Cool. How does it work. Do frame by frame analysis and then summarize the description of all the frames?",
          "score": 2,
          "created_utc": "2026-02-25 21:23:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ekgps",
              "author": "solderzzc",
              "text": "yes, that's the first version, then you know the speed is very slow w/ local model, and cost is very high with cloud model. \nSo I pick up significant frames to make it cost less... ",
              "score": 1,
              "created_utc": "2026-02-25 21:48:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7i8vm2",
          "author": "LoveInTheFarm",
          "text": "It‚Äôs huggingface ?",
          "score": 2,
          "created_utc": "2026-02-26 12:56:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jrbvu",
              "author": "solderzzc",
              "text": "This is desktop application, model is downloaded from huggingface and inference locally.",
              "score": 1,
              "created_utc": "2026-02-26 17:27:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nk2gx",
          "author": "crusoe",
          "text": "I wonder if they work for spaghetti detection on 3d printers...",
          "score": 2,
          "created_utc": "2026-02-27 05:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nlequ",
              "author": "solderzzc",
              "text": "Great idea, worth a try :) Put your iPhone or Mac there, leverage webcam to watch it ",
              "score": 1,
              "created_utc": "2026-02-27 05:53:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pyrbt",
          "author": "1HK7",
          "text": "This looks amazing. What was the prompt given to the model to generate that response ? Are you giving video snippets (a bunch of frames as input ) or sampling the frames every N seconds and giving it to the model as input ?",
          "score": 2,
          "created_utc": "2026-02-27 16:05:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7r83y5",
              "author": "solderzzc",
              "text": "Glad you like the output from QWEN :) Frame by frame is too expensive, it takes long time to handle one vide, ( some models are supporting video input directly but inference time and memory cost is huge ). So sampling is the way to cost down. The prompt is asking QWEN to think it's a security :) ",
              "score": 2,
              "created_utc": "2026-02-27 19:41:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o803pyv",
          "author": "koc_Z3",
          "text": "Good work",
          "score": 2,
          "created_utc": "2026-03-01 04:47:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o805dtv",
              "author": "solderzzc",
              "text": "Thanks :)",
              "score": 1,
              "created_utc": "2026-03-01 04:59:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o82v5nq",
          "author": "Individual_Put_2185",
          "text": "Interesting",
          "score": 2,
          "created_utc": "2026-03-01 16:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o833mwa",
          "author": "arsenale",
          "text": "Nice work. Are you going to make the setup more robust using another brand for the camera? I would be interested in the setup that you're planning if any. Thanks.",
          "score": 2,
          "created_utc": "2026-03-01 17:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83ahim",
              "author": "solderzzc",
              "text": "I have Blink / Ring cameras with cloud plan for clip downloading.\nONVIF/RTSP I've tested AMREST/DAHUA/IMOU/Lorest. So any RTSP/ONVIF will work. \nI also use MacBooks' built-in camera, it also could use iPhone as camera since it's showing up as web camera.\n\nSo ANY pro cameras will work since they support ONVIF/RTSP. I've built-in one scanner to discover them. ",
              "score": 1,
              "created_utc": "2026-03-01 18:09:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7878uw",
          "author": "cool-beans-yeah",
          "text": "But where's the mail man and the street doesn't have a white line down the middle, does it?\n\nEdit: just realized that may have come across as sarcastic...none intended!",
          "score": 1,
          "created_utc": "2026-02-24 23:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78cifv",
              "author": "solderzzc",
              "text": "Mail man was at the first several seconds, I forwarded to 12s to not disclose mailman's privacy. ... White line is a hallucination. It thought white line should be always on the road. :) ",
              "score": 1,
              "created_utc": "2026-02-24 23:36:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7aljzh",
                  "author": "cool-beans-yeah",
                  "text": "Oh ok! Thanks",
                  "score": 2,
                  "created_utc": "2026-02-25 08:34:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7cktn4",
                  "author": "Crafty-Young3210",
                  "text": "dont you think the fact that its hallucinating something thats clearly not there is an issue for using these models for this application?",
                  "score": 2,
                  "created_utc": "2026-02-25 16:20:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7j7raj",
          "author": "Few_Matter_9004",
          "text": "Are you visually impaired, or did you do this out of sheer boredom?",
          "score": 1,
          "created_utc": "2026-02-26 15:57:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lm8x2",
          "author": "_Viral19",
          "text": "hi\n\n",
          "score": 1,
          "created_utc": "2026-02-26 22:45:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rex0vo",
      "title": "Big love to the Qwen üß† A true SOTA Open Source model running locally (Qwen 3.5 35B 4-bit) - Here is the fix for the logic loops! ‚ù§Ô∏è",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rex0vo/big_love_to_the_qwen_a_true_sota_open_source/",
      "author": "SnooWoofers7340",
      "created_utc": "2026-02-26 01:47:35",
      "score": 214,
      "num_comments": 13,
      "upvote_ratio": 0.98,
      "text": "Been hunting for a local daily driver to finally drop paid APIs, and I think I found \"The One.\"\n\nGrabbed the new **Qwen3.5-35B-A3B-4bit**, and at first, it was rough. I was getting those classic 4-bit issues: reasoning loops, endless \"Wait, let me check...\" spirals, and failing simple logic traps like the \"Car Wash\" problem.\n\nSpent the day stress-testing it against the [Digital Spaceport Benchmark suite](https://digitalspaceport.com/about/testing-local-llms/) (logic, math, SVG coding, counting). It was hit-or-miss until I realized the model wasn't \"dumb\" it just needed structure. The quantization makes it anxious, so it second-guesses itself into oblivion.\n\n**The Fix:**\n\nTweak the system prompt to force \"Adaptive Logic.\" It separates the scratchpad thinking from the final answer and sets a hard limit on the \"thinking\" phase.\n\nOnce applied this, it passed **100% of the tests** in seconds.\n\n* **Logic:** Solved the \"Pico de Gato\" schedule puzzle perfectly.\n* **Math:** Nailed the \"Two Drivers\" problem without looping.\n* **Coding:** Generated a clean SVG cat on the first try.\n* **Counting:** Actually counted \"Peppermint\" correctly (rare for 4-bit).\n\nNext up, I'm throwing my complex n8n workflows at it to see if it survives. ü§û\n\nIf you're struggling with the 4-bit version, here is the config that unlocked it for me.\n\n**‚öôÔ∏è The Config**\n\n* **Model:** Qwen3.5-35B-A3B-4bit\n* **Temp:** 0.7 | **Top P:** 0.9 | **Min P:** 0.05 *(Critical!)*\n* **Freq Penalty:** 1.1 | **Repeat Last N:** 64\n\n**üß† The \"Anti-Loop\" System Prompt**\n\n*(Paste into OpenWebUI/LM Studio)*\n\nPlaintext\n\n    You are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops.\n    \n    1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n    2. ADAPTIVE LOGIC:\n       - For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n       - For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n       - For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n    3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n    \n    DO NOT reveal your thinking process outside of the tags.\n\nBig love to the Qwen team. You guys genuinely changed the game with this one. Having this running locally is a dream. üöÄ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rex0vo/big_love_to_the_qwen_a_true_sota_open_source/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7fxc6x",
          "author": "InitialJelly7380",
          "text": "good sharing",
          "score": 2,
          "created_utc": "2026-02-26 02:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g0s7m",
          "author": "pinthead",
          "text": "What is your context length ?",
          "score": 2,
          "created_utc": "2026-02-26 02:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hjyu2",
              "author": "SnooWoofers7340",
              "text": "I'm capped at **28k context** but Qwen 3.5 architecture theoretically supports **1M tokens** using YaRN/RoPE scaling!! insane",
              "score": 2,
              "created_utc": "2026-02-26 09:32:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hp29t",
          "author": "Aware-Adman",
          "text": "Nice",
          "score": 2,
          "created_utc": "2026-02-26 10:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jr047",
          "author": "laser50",
          "text": "Maybe for anyone that's gone there by now and has more of a clue than myself..\n\nWould the 35B A3B model work better on thinking or without thinking?\n\nI've always wondered what the model does if it can't actually think of its answer lol",
          "score": 2,
          "created_utc": "2026-02-26 17:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7livdd",
              "author": "SnooWoofers7340",
              "text": "A bit of both! It's all about balance right, after all day testing out I settle for this : \n\n ‚öôÔ∏è Model Configuration Parameters\nTemperature: 0.7\nMax Tokens: 28,000\nTop P: 0.9\nMin P: 0.05 (This was the critical one for stability!)\nFrequency Penalty: 1.1\nRepeat Last N: 64\nTop K: Default\nK & V Caching (Context Quantization): Disabled / f16 (Default)\nEverything else: Default\n\nüß† The \"Anti-Loop\" System Prompt\n\nYou are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops.\n\n1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n2. ADAPTIVE LOGIC:\n   - For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n   - For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n   - For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n\nDO NOT reveal your thinking process outside of the tags.",
              "score": 1,
              "created_utc": "2026-02-26 22:28:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kgbwb",
          "author": "Holiday-Pack3385",
          "text": "I did something similar on mine - I'm using the Q5\\_K\\_M with the full 262144 context length active on my Geforce 5090. My system prompt is shown below. (I added it when the darn thing kept showing \"Thinking Process\" in every response:\n\nYou are a highly intelligent, direct, and efficient AI assistant. You answer questions concisely without unnecessary politeness or filler. You focus on the user's specific request and provide actionable information. If the user asks for an opinion, provide a balanced view. If you don't know the answer, admit it. Your goal is to be the most helpful tool possible.\n\nDo not show your \"Thinking Process:\" in the response.",
          "score": 2,
          "created_utc": "2026-02-26 19:23:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lj71e",
              "author": "SnooWoofers7340",
              "text": "Nice one man thanks for sharing, mine is quiet similar.\n\n ‚öôÔ∏è Model Configuration Parameters\nTemperature: 0.7\nMax Tokens: 28,000\nTop P: 0.9\nMin P: 0.05 (This was the critical one for stability!)\nFrequency Penalty: 1.1\nRepeat Last N: 64\nTop K: Default\nK & V Caching (Context Quantization): Disabled / f16 (Default)\nEverything else: Default\n\nüß† The \"Anti-Loop\" System Prompt\n\nYou are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops.\n\n1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n2. ADAPTIVE LOGIC:\n   - For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n   - For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n   - For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n\nDO NOT reveal your thinking process outside of the tags.",
              "score": 1,
              "created_utc": "2026-02-26 22:30:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kxyav",
          "author": "beedunc",
          "text": "Thanks!",
          "score": 2,
          "created_utc": "2026-02-26 20:47:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kxzsl",
              "author": "exclaim_bot",
              "text": ">Thanks!\n\nYou're welcome!",
              "score": 3,
              "created_utc": "2026-02-26 20:47:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gpe3f",
          "author": "altdotboy",
          "text": "I was using the qwen3 30B A3B for my home rig. It was good. I have to say the new 35B A3B is awesome. I was a bit weary about the model having thinking, was going to turn it off but decided to embrace it. The 35B seems less likely to get stuck in the loops you mentioned. It‚Äôs great having the thinking text to see what the model is considering. I‚Äôm on a MacBook Pro M4 48GB. Context max is set to 128k. \nI might borrow some of your prompt. Good tips.",
          "score": 2,
          "created_utc": "2026-02-26 05:05:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85rvv2",
          "author": "Same-Ad7128",
          "text": "I'm used to having the model not think. Is your configuration effective for the non-thinking mode?",
          "score": 1,
          "created_utc": "2026-03-02 02:10:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7og2mq",
          "author": "AgitatedDoctor9613",
          "text": "This is an enthusiastic post about optimizing Qwen 3.5 for local automation, but it could be significantly strengthened with a few improvements:\n\n**Improvements:**\n1. **Complete the post**: The content cuts off mid-sentence (\"Two Driv...\"). Finish the full results and provide complete context for readers.\n2. **Share the actual system prompt**: You mention the \"Adaptive Logic\" fix but don't include the exact prompt template. Consider pasting the full prompt so others can replicate your results immediately rather than reverse-engineering it.\n3. **Provide quantitative benchmarks**: Instead of \"100% of tests,\" specify how many tests, what the baseline was, and include timing/performance metrics (latency, token/sec, memory usage).\n4. **Add reproducibility details**: Include your hardware specs (GPU, RAM), inference framework (ollama, vLLM, etc.), and exact model link so others can verify results.\n\n**Alternative Perspectives to Consider:**\n1. **Trade-offs not mentioned**: 4-bit quantization reduces model size but may impact quality on edge cases. Have you tested on tasks outside the benchmark suite?\n2. **Comparison context**: How does this compare to other open models (Llama 3.1, Mistral, DeepSeek) on the same benchmarks? \"The One\" is strong language without comparison data.\n3. **Cost-benefit analysis**: While dropping paid APIs saves money, consider total cost of ownership (hardware, electricity, maintenance time vs. API costs at scale).\n\n**Potential Issues:**\n1. **Survivorship bias**: The \"fix\" worked for your specific setup‚Äîdifferent quantization methods, hardware, or use cases may need different tuning.\n2. **Benchmark limitations**: Digital Spaceport tests are useful but narrow. Real-world n8n automation often involves unstructured data, API integration, and error handling that benchmarks don't cover.\n3. **Prompt engineering fragility**: System prompt tweaks can be brittle. Test how sensitive the 100% success rate is to minor prompt variations.\n\n**Additional Resources to Reference:**\n1. Link to the n8n community docs on local LLM integration\n2. Point readers to the actual Qwen model card on Hugging Face for official specs\n3. Mention relevant papers on quantization artifacts and mitigation techniques\n4. Suggest testing frameworks like HELM or LMEval for more rigorous benchmarking\n\n**Tone note**: Your enthusiasm is great for engagement, but ground claims with data. \"The One\" and \"100% success\" will attract skepticism without full context. Lead with reproducible evidence.",
          "score": 0,
          "created_utc": "2026-02-27 10:29:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcmm3q",
      "title": "I canceled my other AI subscriptions today.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "author": "InitialCareer306",
      "created_utc": "2026-02-23 16:47:15",
      "score": 179,
      "num_comments": 59,
      "upvote_ratio": 0.87,
      "text": "Between the 256k context window and the fact that a 397B open-weight model is available, I just can't justify paying $20/mo anymore. The open-source community won.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6zd3ee",
          "author": "Historical-Internal3",
          "text": "What hardware are you running the model on?",
          "score": 20,
          "created_utc": "2026-02-23 17:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72l8gc",
              "author": "shooshmashta",
              "text": "Lies",
              "score": 8,
              "created_utc": "2026-02-24 03:05:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o743lnf",
                  "author": "doodo477",
                  "text": "tears",
                  "score": 5,
                  "created_utc": "2026-02-24 10:29:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zgvfx",
          "author": "TalosStalioux",
          "text": "I mean, purchasing a $4000 setup (minimum) to run that kind of model size just to save $20 per month is... interesting",
          "score": 47,
          "created_utc": "2026-02-23 17:29:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zyn3n",
              "author": "Justfun1512",
              "text": "Show me 4,000 $ build /machine that can run 300b models",
              "score": 42,
              "created_utc": "2026-02-23 18:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zz0u1",
                  "author": "ImmediateDot853",
                  "text": "I am also interested in that setup.",
                  "score": 14,
                  "created_utc": "2026-02-23 18:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o709glb",
                  "author": "greenthum6",
                  "text": "I have 4K$ 5090 build. What 300B model can I run? I thought 30B models were already on the edge.",
                  "score": 11,
                  "created_utc": "2026-02-23 19:40:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70d1zz",
                  "author": "Hankdabits",
                  "text": "It would have to be hybrid gpu/cpu inference. Probably something like amd epyc, 8 channels of ddr4, and a 3090 to get ~12 t/s and slow prefill",
                  "score": 2,
                  "created_utc": "2026-02-23 19:57:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72id9q",
                  "author": "Straight_Issue279",
                  "text": "With the right build you can make a very good ai using vector memory ect for around 2 to 3k i made an offline ai, my ai can remember content from 4 months ago. Can gather info online, is uncensored, can protect my network by scanning it and be able to create images. Im tired of all the data privacy leaks hence why I will never pay for subscriptions ever. Yeah it may take a full 10 sec to respond but it works for me. I was running dolphin-2.6-mistral-7b.Q6_K.gguf  but I went for a 12b model. Its enough for me.",
                  "score": 2,
                  "created_utc": "2026-02-24 02:49:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75kdrl",
                  "author": "RandomCSThrowaway01",
                  "text": "Q4 is 220GB. Meaning it fits on 256GB Mac Studio which brand new is $5600. Occasionally it does show up in refurbished/sale category however and then it's possible to find it for around $5000. Not 4 but still, if you can afford $4000 then you probably **can** consider $5000. \n\nJust, uh, don't expect it to actually outperform $200 Claude Max and similar solutions in pure LLM output quality. You can run some great stuff on it but prompt processing is slow with huge models in particular. ",
                  "score": 1,
                  "created_utc": "2026-02-24 15:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77nf13",
                  "author": "TheRiddler79",
                  "text": "Mine will\n\nhttps://preview.redd.it/n4x5uchngilg1.jpeg?width=1440&format=pjpg&auto=webp&s=84794eacea54670b9094eff3311622c2e80ffbd4\n\nBut, if I'm being fair, it wouldn't be a whole lot of fun to chat with. What it's really really really really good at doing is I give it a task, and then it crunches away at two tokens a second for 24 hours, and then I end up with an incredible output, a fully built website, a fully built mobile app whatever. But it's not fun to chat with cuz it's too slow so you just have to give it a task and go away.\n\nThe relevance here is that I'm running on a system I built for $1,500 Plus a single gun trade for Ram.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:31:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7embxq",
                  "author": "Confusion_Senior",
                  "text": "Used M2 Ultra 192gb Mac studio with Q4 model\n\nOr just used 192gb ram with used  3090 as well",
                  "score": 1,
                  "created_utc": "2026-02-25 21:57:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zwuro",
              "author": "illathon",
              "text": "20 dollars a month with massive restrictions or your own hardware running 24/7 and you can constantly upgrade as new models come out.",
              "score": 8,
              "created_utc": "2026-02-23 18:42:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71kf3s",
                  "author": "greenthum6",
                  "text": "Commercial models are updated as well. You can inference with Opus for 20$ a month. Once you get to that level with local model the paid ones are again much better.",
                  "score": 2,
                  "created_utc": "2026-02-23 23:35:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zjogg",
              "author": "upinthisjoynt",
              "text": "I'd say it's worth it if it's used for more than just code.  Since it's local and more \"secure\", I don't see why it can't be used for most everyday AI uses (except with old training data).  Just an option",
              "score": 5,
              "created_utc": "2026-02-23 17:42:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71a3ps",
              "author": "BargeCptn",
              "text": "I don't see how it's even remotely possible to have anything usable on the $4000 setup, more like $14k to $18k maybe. You're still going to be basically at 20-30 tokens per second, with a 10 to 15 second first response on the context windows that are larger than 12ktokens. It's wishful thinking, bro. Full models really take a chug out of hardware. It's all about unified memory space; that's all it is. It's not how many video cards you got.",
              "score": 2,
              "created_utc": "2026-02-23 22:39:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73v62i",
              "author": "trackktor",
              "text": "You can resell. The money doesn‚Äôt vanish.",
              "score": 1,
              "created_utc": "2026-02-24 09:09:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o76jvj9",
              "author": "dabiggmoe2",
              "text": "I mean buying a $4000 setup for that alone might be too much depending on the case. But this can also double a gaming PC or a dev machine. For the purpose to just vibe code it might not be the most effective choice. But for learning, I would say it might be worth it if one can afford it.",
              "score": 1,
              "created_utc": "2026-02-24 18:30:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o77es81",
              "author": "elaboratedSalad",
              "text": "please show me how I can run that for $4k. i'm serious. I'd jump straight in.",
              "score": 1,
              "created_utc": "2026-02-24 20:52:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o791lw1",
              "author": "goodssh",
              "text": "You probably meant to say $40,000 üòÇ. Even with that, you would probably have to constantly maintain the infra, both in software and hardware. \n\nThe open-weight model itself doesn't generate a great response without manual labor work - it's extremely difficult to micro control the model to deliver the desired result. \n\nIn my view with 20 bucks per month they give a super generous quota.",
              "score": 1,
              "created_utc": "2026-02-25 01:55:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7axac9",
              "author": "thaddeusk",
              "text": "Hey, I just managed to load it on my $2000 setup and I'm getting 16t/s. With a 1bit quant... I imagine I'd be better off loading the 122b model at 4bit, though. It's a bit faster and probably would end up being higher quality output, anyway.",
              "score": 1,
              "created_utc": "2026-02-25 10:23:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o708ehr",
          "author": "the-average-giovanni",
          "text": "* my other ai subscription\n\n\nNot this one, folks",
          "score": 4,
          "created_utc": "2026-02-23 19:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi4g3",
          "author": "JahJedi",
          "text": "On what setup you planing to run it?",
          "score": 3,
          "created_utc": "2026-02-23 17:34:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70oqj3",
          "author": "Hector_Rvkp",
          "text": "This is such a stupid post, it must be trolling. The ram requirement is insane and people who do proper coding work aren't running 20$ subs, they're using CLI tools with either tokens or much higher subs. \nThis makes no sense",
          "score": 3,
          "created_utc": "2026-02-23 20:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dk8o",
          "author": "pl201",
          "text": "Not really. To run 397B open-weight mode with acceptable speed, you need at least investing the in $10k range to get an acceptable performance. If you run it 24/7, your energy bill will be more than 5x $20. Plus, I yet to see open model‚Äôs ability working on a large code base. You will get something but will spend more time to fix things that are not working. Time is money too.",
          "score": 4,
          "created_utc": "2026-02-23 19:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi1v2",
          "author": "JahJedi",
          "text": "Open 8s more than to just save 20$, first of all its privacy and it will not report you or collect any data.",
          "score": 2,
          "created_utc": "2026-02-23 17:34:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77jiq8",
          "author": "DonkeyBonked",
          "text": "My $3.5 to $4k~ is build with 4x RTX 3090 can't even run a 300B+ model, so I mean if a $20/month sub could do what I need, that would be the cheapest way for me.",
          "score": 2,
          "created_utc": "2026-02-24 21:13:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ku0s0",
              "author": "spiritxfly",
              "text": "What are some good models you run on that build? I have the same one, I haven't really tried any of the latest models though.",
              "score": 1,
              "created_utc": "2026-02-26 20:28:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lp1uc",
                  "author": "DonkeyBonked",
                  "text": "So I'm not certain your specific use case, but I'm doing custom tunes, LoRAs, and RAG databases for game development, along with using it for software development, so I need as much context as I can get.\n\nI also am very high ADHD, so patience isn't my strongest virtue outside of hyperfocus.\n\nSo I tend to use the faster models based more on ones like Qwen3-Coder-30B-A3B-Instruct and Nemotron-3-Nano-30B for most of my grunt work. Depending on performance and the tasks related, I also do use GLM-4.7-Flash a pretty decent amount, and I have various uses/testing I do with models ranging from Devstral-2-Small-24B up to GPT-OSS-120B, Qwen-3-Coder-Next, and I'm messing around with other ones as well that are newer, I just haven't had as much time because my wife is in the hospital and I'm kind of burning my candle at both ends.\n\nI tend to prefer the smaller models and I tend to prefer llama.cpp over vllm. I can run a vllm server on a tiny model and support a ton of users connected, but unfortunately I lose too much context headroom using 30B class models on VLLM, which is about how I feel using 70B to 120B models on llama.cpp\n\nI'm really hoping the impacts from Nemotron 3 help improve the way context is stored for other models and allows them to become more context efficient. It looks like I could run a Q8 of Qwen 3.5 120B, but I'm really looking for their coder release for 3.5.\n\nI think Qwen3-Coder-30B-A3B-Instruct is my biggest workhorse and it's really fast with Claude Code when it's not hallucinating tool calls (something I'm planning to work on, just been busy).\n\nWhen I need to do reasoning, troubleshooting, breaking loops, etc., I definitely don't use Qwen there though, I prefer Nemotron 3 and GLM 4.7 or GPT-OSS-120B depending on what it is.\n\nI would probably do a bit more with GPT-OSS-120B but I'm really still trying to get down how the chain of thought fine tuning. I just took about 400MB of code and had Qwen3-Coder-30B-Instruct write the pairs for it and after inspecting a few hundred of them it actually looks really good, I was quite impressed, but I'm going to need to make another pass over that dataset with Nemotron 3 because some of the data points Qwen didn't have it skipped too much. I'd probably prefer to use GLM-4.7-Flash for that but speed is a factor. With my current setting, before the wind down Nemotron starts around 180 tokens/s output speed, Qwen3-Coder-30B-Instruct is around 140, so these are pretty fast, but GLM-4.7-Flash I'm rocking in the 60s with my current config (I need to see if I can improve it, just haven't had the time) and unfortunately, when you look at something like combing that 400MB of data, it took about 40\\~ hours to do the first pass with Qwen, it would likely take a lot longer with GLM. I might test running it with only two of the cards so I can keep the other two open for use while it processes the data, but we'll see.\n\nI have found I don't particularly care for the low quants of the higher end models. They tend to be inconsistent, often buggy, more prone to loops, and don't provide worthwhile context limits for me to care. Even doing personality tuning I've found better results tuning Qwen3-Coder-30B-Instruct and keeping the high performance than tuning a higher model. For a lot of stuff like conversational AI, I'm practicing tuning even smaller models now as the bigger just isn't necessary.\n\nTesting around though, I have around 8TB of models I'm working with currently.",
                  "score": 1,
                  "created_utc": "2026-02-26 23:00:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74s74k",
          "author": "EzioO14",
          "text": "There is no economically viable alternative to using closed source models like claude, OpenAI etc‚Ä¶ if you want the same performance",
          "score": 1,
          "created_utc": "2026-02-24 13:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75mb61",
          "author": "mayday30",
          "text": "You will likely pay more than $20 in electric costs.",
          "score": 1,
          "created_utc": "2026-02-24 15:59:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75oi9s",
          "author": "jodykpw",
          "text": "I can‚Äôt afford the hardware tho.",
          "score": 1,
          "created_utc": "2026-02-24 16:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o766b3o",
          "author": "johnmclaren2",
          "text": "This is just a karma farming post or a promo for a Qwen model. :)",
          "score": 1,
          "created_utc": "2026-02-24 17:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76ffzm",
          "author": "pppreddit",
          "text": "You are forgetting that local llm servers mostly don't have prompt caching and are not suitable for coding or long exchange, they are painfully slow with long context (like minutes to get a response) . It's not enough to have big enough vram, you need proper context caching implementation and AFAIK there are only commercial solutions that support it, no open source yet. Correct me if I am wrong, because things develop really fast and it hard to catch up with everything",
          "score": 1,
          "created_utc": "2026-02-24 18:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77rwyw",
          "author": "momontology",
          "text": "What type of hardware would be needed to run this?",
          "score": 1,
          "created_utc": "2026-02-24 21:52:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77ygmi",
          "author": "Ashthot",
          "text": "What is your setup ?",
          "score": 1,
          "created_utc": "2026-02-24 22:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nzv1g",
          "author": "AwayLuck7875",
          "text": "Im i3 ,ram 16 gb ,vram 8 gb rx480 ,qwen 30b-a3/2 token sec/granit 7b-a1b 25 token ,very cool coding",
          "score": 1,
          "created_utc": "2026-02-27 07:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nzxrf",
          "author": "AwayLuck7875",
          "text": "Granit 7b-a1b very very intrested model",
          "score": 1,
          "created_utc": "2026-02-27 07:57:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pcuz6",
          "author": "Markosz22",
          "text": "And how exactly are you running this model yourself? hmm",
          "score": 1,
          "created_utc": "2026-02-27 14:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zptd2",
          "author": "el-rey-del-estiercol",
          "text": "Pagar por ejecutar buenos modelos es interesante siempre que tu trabajo dependa de ello y ganes dinero con ello tambien",
          "score": 0,
          "created_utc": "2026-02-23 18:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70r8b5",
          "author": "NearbyBig3383",
          "text": "Gente avandoney a chutes para ir pra o Code plan do qwen. E s√©rio n√£o me arrependo",
          "score": 0,
          "created_utc": "2026-02-23 21:05:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhe4z9",
      "title": "Qwen 3.5 27b and Qwen3.5-35B-A3B ran locally on my rtx 5060ti 16gb card",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/kmh4jitovamg1.png",
      "author": "Substantial-Cup-9531",
      "created_utc": "2026-02-28 21:10:33",
      "score": 104,
      "num_comments": 14,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rhe4z9/qwen_35_27b_and_qwen3535ba3b_ran_locally_on_my/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7y5tl6",
          "author": "BitXorBit",
          "text": "How much time it took?",
          "score": 6,
          "created_utc": "2026-02-28 21:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80dkv4",
              "author": "Substantial-Cup-9531",
              "text": "Qwen3.5-35B-A3B-UD-Q4\\_K\\_XL  - 6min 21s 3.80 t/s\n\nQwen3.5-27B-UD-Q4\\_K\\_XL- 4min 47s 1.95 t/s",
              "score": 3,
              "created_utc": "2026-03-01 06:03:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o811m31",
                  "author": "RepresentativeRude63",
                  "text": "4 minutes for response? And a basic one like give an image and ask question. Why use locally? My question is.",
                  "score": 1,
                  "created_utc": "2026-03-01 09:45:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ybs9g",
          "author": "Fit-Pattern-2724",
          "text": "Did you run the 6 finger test?",
          "score": 2,
          "created_utc": "2026-02-28 22:14:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80cdo5",
              "author": "Substantial-Cup-9531",
              "text": "Update : Initially tought they passed but Both of them failed!\n\nhttps://preview.redd.it/oilbpccxhdmg1.png?width=892&format=png&auto=webp&s=53f7f4d64b90b7573ad7927a7c0d23a7da4778a6",
              "score": 3,
              "created_utc": "2026-03-01 05:53:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80dohi",
                  "author": "martinkozle",
                  "text": "Wait, isn‚Äôt this incorrect though?",
                  "score": 3,
                  "created_utc": "2026-03-01 06:04:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o80f3z8",
                  "author": "Substantial-Cup-9531",
                  "text": "Yeh u actually right, it bombed, both of them did actually! \n\nhttps://preview.redd.it/1u1qz9ewldmg1.png?width=852&format=png&auto=webp&s=f49f3f6a83138703f0e0fcfbedc692370f8f2d25\n\n",
                  "score": 2,
                  "created_utc": "2026-03-01 06:16:31",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o80ikjq",
                  "author": "Fit-Pattern-2724",
                  "text": "Oh no!!",
                  "score": 2,
                  "created_utc": "2026-03-01 06:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o807dr7",
          "author": "timbo2m",
          "text": "I was able to get UD-Q4_K_XL 27B running at 39tps on a 4090 with a context size of 32k. 64k ran at 37tps but then performance nose dived at 128k at 11tps and 256k was 5tps.\n\nFor 35B/A3B it was much quicker of course. 32k=120tps, 64k=96tps, 128k=36tps, 256k=27tps",
          "score": 2,
          "created_utc": "2026-03-01 05:14:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85vhhk",
              "author": "Laabc123",
              "text": "What quant for 35B?",
              "score": 1,
              "created_utc": "2026-03-02 02:32:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o83t23f",
          "author": "mintybadgerme",
          "text": "How are you running them? On what tool, platform?",
          "score": 1,
          "created_utc": "2026-03-01 19:37:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8431bb",
          "author": "someone383726",
          "text": "I got 35B A3B to run at 18tps on CPU!  Q4 K XL. Ik-llama.  I couldn‚Äôt get vision to work unless I swapped over to mainline llamacpp and got around 10TPS on CPU.0",
          "score": 1,
          "created_utc": "2026-03-01 20:28:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcqezx",
      "title": "Qwen 3.5 for MLX is like its own industrial revolution",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "author": "sovietreckoning",
      "created_utc": "2026-02-23 18:59:54",
      "score": 94,
      "num_comments": 31,
      "upvote_ratio": 0.97,
      "text": "I'm using a 4-bit model on a mac studio m3 and it is mindblowing how fast this thing works for the quality of the results. I love it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o703evr",
          "author": "ossbournemc",
          "text": "How fast is it? I'm considering the same setup. ",
          "score": 5,
          "created_utc": "2026-02-23 19:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7042mt",
              "author": "sovietreckoning",
              "text": "I'm not well-versed in any of this stuff and I'm only doing ametuer stuff for myself, so please forgive me is this answer is unhelpful - a task that took \\~500s+ on llama 3(405b) is closer to \\~30s on qwen 3.5. ",
              "score": 5,
              "created_utc": "2026-02-23 19:15:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o707puy",
                  "author": "ossbournemc",
                  "text": "Thats cool, do you know how many tokens/second you're getting?",
                  "score": 2,
                  "created_utc": "2026-02-23 19:32:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mv2gk",
                  "author": "datbackup",
                  "text": "Llama3 405b? Lmao bro that is like the slowest of slow models, kinda unfair comparison?\n\nBut hard to know for sure without knowing what you are comparing it against (qwen3.5 is a whole family of models while llama3 405b is a specific model)",
                  "score": 2,
                  "created_utc": "2026-02-27 02:57:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75xuwv",
                  "author": "Hector_Rvkp",
                  "text": "You're looking at dense model vs MoE model. Avoid dense models, they are slower.",
                  "score": 1,
                  "created_utc": "2026-02-24 16:51:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70ybyw",
              "author": "Professional-Bear857",
              "text": "I get 35 tok/s on my 60 GPU core M3 ultra 256gb ram. I'm using the 4bit mlx version as well.",
              "score": 2,
              "created_utc": "2026-02-23 21:40:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c2nht",
                  "author": "zulutune",
                  "text": "Are the results comparable with Claude/Codex? Without taking speed into consideration. I suspect it‚Äôs not but I‚Äôd like to know how big the gap is, how noticeable it is",
                  "score": 1,
                  "created_utc": "2026-02-25 14:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70k51l",
              "author": "alexp702",
              "text": "I am running llama server with the 8bit GGUF. It is excellent. It gets about 250 prompt processing tokens, and about 25tokens out. I‚Äôm openclawing and many prompts are cached so practical upshot is very few tokens either way 114000 token prompt processing only 500. Not sure why I am suddenly getting such good caching (perhaps Llama update) but I like it!\n\nEdit: Openclaw reports I have a throughput of 15.2k tokens a minute probably because of prompt caching. 26.7 million tokens sent. Pretty damn good!\n\nEdit 2: Mac Studio 512gb.",
              "score": 2,
              "created_utc": "2026-02-23 20:31:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c4orc",
                  "author": "sovietreckoning",
                  "text": "Can you tell me more about this. I tried to run the 8-bit mlx version on a Mac Studio m3 512gb and it choked to a crawl.",
                  "score": 1,
                  "created_utc": "2026-02-25 15:05:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70v2xz",
          "author": "jzn21",
          "text": "I am also blown away by this model. The quality is excellent (even in non-thinking mode) and the speed is great with 34 - 35 tokens per second. And then best thing is that prompt processing happens in the blink of an eye.",
          "score": 5,
          "created_utc": "2026-02-23 21:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71js61",
          "author": "CoffeeSnakeAgent",
          "text": "Where to get the qwen 3.5 4-bit? Cant seem to find it in hf",
          "score": 3,
          "created_utc": "2026-02-23 23:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71l37g",
              "author": "sovietreckoning",
              "text": "[https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4](https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4)\n\nThats what I'm using currently.",
              "score": 3,
              "created_utc": "2026-02-23 23:39:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o71oqrx",
                  "author": "Special-Wolverine",
                  "text": "NVFP4 is optimized for Nvidia Blackwell",
                  "score": 1,
                  "created_utc": "2026-02-23 23:59:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o726qhr",
                  "author": "CoffeeSnakeAgent",
                  "text": "Thanks",
                  "score": 1,
                  "created_utc": "2026-02-24 01:41:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7fyc0r",
          "author": "zerostyle",
          "text": "What woudl be the best model to run on 32gb of ram for qwen 3.5? not seeing too much on hugging face yet. Prefer abliterated/uncensored models.",
          "score": 2,
          "created_utc": "2026-02-26 02:17:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o746vum",
          "author": "XxBrando6xX",
          "text": "Does anyone know why the MLX version seems to drop the vision ability ?? I want to run the mlx but I was hoping to do it by letting it review EVERYTHing not just text prompts which I can‚Äôt do on the mlx yet since it isn‚Äôt supported",
          "score": 1,
          "created_utc": "2026-02-24 10:58:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74qfkt",
          "author": "ThankYouOle",
          "text": "i am curious, what is your use case? to be buddy to discuss thing? quick QnA? or code?",
          "score": 1,
          "created_utc": "2026-02-24 13:19:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o751csv",
              "author": "sovietreckoning",
              "text": "It‚Äôs varied because I‚Äôm effectively learning and developing an all-inclusive ai assistant for my personal use. I own a small law firm which involves an enormous amount of admin work and document prep from preexisting forms. I also do some active spread trading from time to time. Basically, I‚Äôm building a legal rag system, a personal assistant, a document automation bot, an options scanner with built in back testing, and a coding machine to grow and improve the product over time. All of the varied functions are wrapped under the LLM for plain language interaction, personal email and calendar integration, court docket scraping, scheduling, alerts, etc. \n\nBasically, I‚Äôm learning on the fly and attempting to clone myself. I don‚Äôt know how any of it will actually work out. With massive context and pipelines to work through, the speed of Qwen 3.5 is so valuable.",
              "score": 3,
              "created_utc": "2026-02-24 14:18:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77nt7o",
                  "author": "ThankYouOle",
                  "text": "interesting that Qwen 3.5 can help you doing all of that. \n\nso far i found local model can't really help me, but then again i never try Qwen, only some small model.\n\nthank you.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:33:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ahzh7",
          "author": "c_glib",
          "text": "Can you please share your workflow? What's the exact model size? Which running (using ollama or something else)? ",
          "score": 1,
          "created_utc": "2026-02-25 08:00:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ebx0b",
          "author": "Gold_Ad_2201",
          "text": "where did you get mlx version? gguf works fine for me but all mlx versions go into repeat almost every time",
          "score": 1,
          "created_utc": "2026-02-25 21:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pi3d7",
          "author": "Ok_Significance_9109",
          "text": "I have a MacBook Pro M1 with 32GB (that translates to just over 24GB VRAM of shared memory). I am using LM Studio and OpenCode, and the MLX versions started working just yesterday, with the latest update of LM Studio (before that they would loop endlessly). I don't have the exact figures, but the 35B MoE in 4bit MLX is so fast that I cannot read the output as it sends it out. I tried also the 5bit version with LM Studio's safeties off, but that's really pushing the limit - the computer borked several times, and 5bit did not seem that much better than 4bit.  \nNow we get to the HOWEVER part. The 35B model makes a lot of mistakes. I gave it a simple challenge - create a HTML model of the solar system, meant for children, with tool tips for each planet. The result did not work, and it was clueless about fixing it.  \nTo the rescue - the dense 27B model. It is much slower than the 35B on my setup, to the point that the only possible workflow is to give it a prompt and go and do something else. However, it discovered every CSS and javascript error made by its sibling, and one-shot the fixes. It's reliable, but too slow for comfort.\n\nIf only we had something in between.\n\nMy settings: temperature 0.6, Top K 40, Repeat Penalty 1.1, Top P Sampling 0.9, Min P Sampling 0.05, context size 65536. ",
          "score": 1,
          "created_utc": "2026-02-27 14:44:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rge4g1",
      "title": "Qwen3.5 vs ChatGPT: The gap is officially closed for my daily tasks. Thoughts?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rge4g1/qwen35_vs_chatgpt_the_gap_is_officially_closed/",
      "author": "InitialCareer306",
      "created_utc": "2026-02-27 18:06:44",
      "score": 73,
      "num_comments": 19,
      "upvote_ratio": 0.93,
      "text": "I‚Äôve been doing a direct comparison between Qwen3.5 and ChatGPT-4o all week. Honestly, for tasks like organizing messy data dumps or reasoning through complex logic puzzles, Qwen3.5 is matching it step-for-step. The native multimodal agent feature even feels a bit more \"proactive\" than ChatGPT's vision. Has anyone else fully replaced their ChatGPT sub with a local Qwen setup yet?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rge4g1/qwen35_vs_chatgpt_the_gap_is_officially_closed/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7qq3q3",
          "author": "Xp_12",
          "text": "qwen models were better than gpt 4o a year ago.",
          "score": 22,
          "created_utc": "2026-02-27 18:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sg7md",
              "author": "Prestigious-Share189",
              "text": "Op is in local and likely tamed Qwen with quantization",
              "score": 4,
              "created_utc": "2026-02-27 23:29:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7si53w",
                  "author": "Xp_12",
                  "text": "possible.",
                  "score": 1,
                  "created_utc": "2026-02-27 23:40:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7tncfr",
              "author": "yaxir",
              "text": "sadly, they have too many guard rails\n\nIf they would drop the guard rails as GPT 4.1 I would use qwen",
              "score": 1,
              "created_utc": "2026-02-28 03:54:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7xn3cx",
                  "author": "jacobcantspeak",
                  "text": "Abliterated models are already out!",
                  "score": 2,
                  "created_utc": "2026-02-28 20:02:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qq8sk",
          "author": "Easy_Werewolf7903",
          "text": "What's your hardware setup and which version of the model are you using",
          "score": 6,
          "created_utc": "2026-02-27 18:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rq6eq",
              "author": "timbo2m",
              "text": "I'm not OP but it's running on my 4090 which fits the unsloth/Qwen3.5-35B-A3B-GGUF:UD-MXFP4_MOE in with headroom (19.5GB model in 24GB VRAM)\n\nThe speed of the model depends on the context size you run llama server with. My benchmarks are:\n\n32k context = 124tps\n64k context = 123tps\n128k context = 62tps\n256k context = 7tps (yikes, not sure what's going on)\n\nThe unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q4_K_XL model had an accuracy bug I think so it was not recommended to use it, not sure if that's fixed. It was faster though.\n\n32k context = 120tps\n64k context = 96tps\n128k context = 37tps\n256k context = 27tps\n\nI think this will also replace ChatGPT for me as soon as I can get web search tooling in llama server",
              "score": 3,
              "created_utc": "2026-02-27 21:11:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7skljs",
                  "author": "creamyatealamma",
                  "text": "I'd like to hear more about web search with llama server when you have it working",
                  "score": 1,
                  "created_utc": "2026-02-27 23:55:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tnh8t",
                  "author": "yaxir",
                  "text": "if you do not mind, can I ask a few questions in dm?\n\nI also want to deploy a similar model on my computer",
                  "score": 1,
                  "created_utc": "2026-02-28 03:55:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qyc8z",
          "author": "besmin",
          "text": "Are you talking about the online Qwen or a specific local model?¬†",
          "score": 3,
          "created_utc": "2026-02-27 18:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7r8dmq",
          "author": "Elojan91",
          "text": "Good! Im testing nanbeig4.1 3b this is mind blowing",
          "score": 2,
          "created_utc": "2026-02-27 19:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s47m2",
          "author": "Unlikely_Spray_1898",
          "text": "Qwen3 235B A22B is cool. But it needs somehow a good system prompt to work properly.",
          "score": 1,
          "created_utc": "2026-02-27 22:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vunbs",
          "author": "Adventurous-Paper566",
          "text": "It's still a bit slow on my end, but what you're saying makes sense. The only reason I still use ChatGPT is for its top-tier voice transcription in its mobile app. On PC, I use only Qwen, whether via their chat interface or locally.",
          "score": 1,
          "created_utc": "2026-02-28 14:33:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vw7hj",
          "author": "Adventurous-Paper566",
          "text": "All my translation tasks (like this message) and anything requiring speed have been executed locally for a while now. I use the Leo AI interface in Brave to have instant access to 4B.",
          "score": 1,
          "created_utc": "2026-02-28 14:42:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uxed1",
          "author": "Unedited_Sloth_7011",
          "text": "Qwen in the web app now has memory and remembers previous chats, the Qwen3.5 models are insanely good all rounders, and their 27B model is just crazy good for its size.  \nThat said, OP is probably a bot with knowledge cutoff before the release of newer OAI models, because (1) GPT-4o does not exist anymore and (2) even if it existed, why compare the latest Qwen model with a 2024 model? Qwen3 was already comparable with gpt-4.",
          "score": 0,
          "created_utc": "2026-02-28 10:29:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vwtv2",
              "author": "Adventurous-Paper566",
              "text": "GPT-4o still exists and remains appreciated for its \"vintage\" style, if I may say so.",
              "score": 1,
              "created_utc": "2026-02-28 14:46:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ri8f0s",
      "title": "Qwen allowed me to Replaced $100+/month in GEMINI API Costs with a ‚Ç¨2000 eBay Mac Studio ‚Äî Here is my Local, Self-Hosted AI Agent System Running Qwen 3.5 35B A3B 4bit at 60 Tokens/Sec (The Full Stack Breakdown)",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/hd34vozsvhmg1.png",
      "author": "SnooWoofers7340",
      "created_utc": "2026-03-01 20:40:08",
      "score": 63,
      "num_comments": 16,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Agent",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1ri8f0s/qwen_allowed_me_to_replaced_100month_in_gemini/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o84j24d",
          "author": "Nexter92",
          "text": "The things i hate the most on reddit is AI shitpost.\n\nStop using AI like a junky and write something clean.",
          "score": 16,
          "created_utc": "2026-03-01 21:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84zkbo",
              "author": "SnooWoofers7340",
              "text": "Thanks for the comment; it helps the algorithm.  \nThis was written with Claude Opus, which was the LLM I used to build my automation.  \nIt's a technical breakdown for other builders who might find it interesting; I'm just sharing the journey.  \nIf you don't want AI writing about an AI topic, you're going to feel really frustrated on a daily basis.  \nPlus, Claude does a much better job than I would have recapping the journey. I spent time on this post, ensuring important details were covered and confirming the content. Anyway, I shouldn't engage here; I get your point, but I also don't understand this Reddit AI hate. If you guys are upset, just put a thumbs down and move on. Why spend your time in the comment section?",
              "score": 4,
              "created_utc": "2026-03-01 23:20:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o85g7w7",
                  "author": "4baobao",
                  "text": "what makes you think anyone would waste their time to read all that ai slop? I'm 100% sure you didn't even read it yourself",
                  "score": 3,
                  "created_utc": "2026-03-02 00:58:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o85xmcc",
                  "author": "SnooHamsters2627",
                  "text": "Really helped me (rookie) begin to try figure out how get to next. Airgapped 128GB M1 Ultra running Qwen3-235B-A22B-Thinking-2507. Thank you.",
                  "score": 1,
                  "created_utc": "2026-03-02 02:46:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o858vad",
              "author": "DeliciousReference44",
              "text": "Completely agree. No way in fucking hell I am Reading all this rubbish.",
              "score": 0,
              "created_utc": "2026-03-02 00:15:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o84tgxd",
              "author": "darkjoker213",
              "text": "At this stage people just can‚Äôt be bothered to do some actual work. It‚Äôs getting insane",
              "score": -2,
              "created_utc": "2026-03-01 22:46:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o84zvqk",
              "author": "AvidCyclist250",
              "text": "built this with gemini pro i guess lol",
              "score": -2,
              "created_utc": "2026-03-01 23:22:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o84d5jl",
          "author": "No-Refrigerator-1672",
          "text": "Why would you use Qwen2.5-VL for vision tasks, when the new Qwem3.5 has vision built in, and will have superior performance? It feels like a waste of RAM, you could use just one superior model for everything.",
          "score": 2,
          "created_utc": "2026-03-01 21:20:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84om7d",
              "author": "SnooWoofers7340",
              "text": "I'm using the mlx-community/Qwen3.5-35B-A3B-4bit¬†model, its a text-only language model and does not have native vision capabilities.¬†",
              "score": 3,
              "created_utc": "2026-03-01 22:20:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85d1s4",
          "author": "signal_overdose",
          "text": "Good job man, people here are just butthurt because they are reading this stuff every day and they can't do sh\\*t themselves because of insecurity :)\n\nGood luck with your business",
          "score": 3,
          "created_utc": "2026-03-02 00:39:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85erpr",
              "author": "SnooWoofers7340",
              "text": "Thanks man, I'm Reddit bulletproof by now, but honestly a part of me still comes back to get roasted. Can't lie, feeling sorry for some",
              "score": 5,
              "created_utc": "2026-03-02 00:49:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85ojp9",
          "author": "Karnemelk",
          "text": "you saved $100 a month to replace gemini with your mac, but you still needed claude to write this AI slop shitpost",
          "score": 3,
          "created_utc": "2026-03-02 01:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84ttzs",
          "author": "Latter_Upstairs_1978",
          "text": "I am from Europe also. What do you mean with \"European Black Market Website\"? ",
          "score": 1,
          "created_utc": "2026-03-01 22:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84xsf2",
              "author": "SnooWoofers7340",
              "text": "https://www.backmarket.es/es-es/p/mac-studio-2022-m1-ultra-32-ghz-ssd-2-tb-64gb/233d59b1-517a-4c20-bad8-aa81e38b4397\n\nSold out, last I checked it was at 3050 euro",
              "score": 2,
              "created_utc": "2026-03-01 23:10:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o845f1e",
          "author": "SnooWoofers7340",
          "text": "# ü§ñ Lucy A V A üß†\n\n# (Autonomous Virtual Agent)\n\nFonction Recap\n\nCommunication:\n\n‚úÖ Telegram (text, voice, images, documents)\n\n‚úÖ Email (Gmail - read/write for Lucy + boss accounts)\n\n‚úÖ SMS (Twilio send/receive)\n\n‚úÖ Phone Calls (Vapi integration, booking system & company knowledge answering)\n\n‚úÖ Sent Voice Notes (Google TTS)\n\nCalendar & Tasks:\n\n‚úÖ Google Calendar (create, read, delete events)\n\n‚úÖ Google Tasks (create, read, delete)\n\nDocuments & Files:\n\n‚úÖ Google Drive (search, upload, download)\n\n‚úÖ Google Docs (create, read, update)\n\n‚úÖ Google Sheets (read, write)\n\n‚úÖ Notion (create notes)\n\n‚úÖ PDF Analysis (extract text)\n\n‚úÖ Image resizer\n\n‚úÖ Dairy journal entry with time log\n\nKnowledge & Search:\n\n‚úÖ Web Search (SerpAPI)\n\n‚úÖ Wikipedia\n\n‚úÖ Short-Term (past 10 messages)\n\n‚úÖ Long-Term Memory (Pinecone vector DB)\n\n‚úÖ Search Past Chats\n\n‚úÖ Google Translate\n\n‚úÖ Google Contact¬†\n\n‚úÖ Think mode¬†\n\nFinance:\n\n‚úÖ Stripe Balance\n\n‚úÖ Expense Tracking (image analysis + google Sheets)\n\n‚úÖ Calorie Tracker (image analysis + google Sheets)\n\nCreative:\n\n‚úÖ Image Generation (\"Nano Banana Pro\")\n\n‚úÖ Video Generation (Veo 3.1)\n\n‚úÖ Image Analysis (Vision AI)\n\n‚úÖ Audio Transcription\n\nSocial Media:\n\n‚úÖ X/Twitter (post tweets)\n\n‚úÖ LinkedIn (post and search)\n\nAutomation:\n\n‚úÖ Daily Briefing (news, weather, calendar, audio version)\n\n‚úÖ Contact Search (Google Contacts)\n\n‚úÖ Date/Time tools\n\n‚úÖ Reminder / Timer\n\n‚úÖ Calculator\n\n‚úÖ Weather (Marbella)\n\n‚úÖ Generate invoice and sent out\n\n‚úÖ Short heartbeat (20min email scan for unanswered ones and coning up event calendar reminder)\n\n‚úÖ Medium heartbeat (every 6h, top 3 world news, event of the day and top 3 high priority email)\n\nThe Trinity Tools (HTML node)\n\n‚úÖ Oracle (Eli - openclaw) - Web browsing with my credentials (online purchase, content creation , trading...)\n\n‚úÖ Architect (Neo - Agent Zero on metal) - Self modify, monitoring, code execution, debug or create on n8n\n\n‚úÖ Telegram group chat with other agent (Neo & Eli)",
          "score": -2,
          "created_utc": "2026-03-01 20:40:28",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgo2ic",
      "title": "Qwen3.5 is finally out! What are your first impressions?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rgo2ic/qwen35_is_finally_out_what_are_your_first/",
      "author": "ischanitee",
      "created_utc": "2026-02-28 00:32:49",
      "score": 53,
      "num_comments": 19,
      "upvote_ratio": 0.91,
      "text": "Saw the news about the Qwen3.5 drop today. The MoE architecture on the 397B looks insane, but I‚Äôm mostly curious about the smaller ones like the 27B and 35B. For those who have already pulled the weights, what‚Äôs the biggest difference you‚Äôre noticing compared to 2.5? Better reasoning? Faster T/S? Let‚Äôs hear it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rgo2ic/qwen35_is_finally_out_what_are_your_first/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7sze7d",
          "author": "weight_matrix",
          "text": "\"today\" ?  \nAm i missing something or did the OP mean that they saw it today?",
          "score": 13,
          "created_utc": "2026-02-28 01:22:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t0uuh",
              "author": "Samy_Horny",
              "text": "It's been almost 3 weeks since the big model came out XD",
              "score": 4,
              "created_utc": "2026-02-28 01:32:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7tmrhn",
              "author": "v01dm4n",
              "text": "I'm sure OP means the news abt midsized models.",
              "score": 0,
              "created_utc": "2026-02-28 03:50:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7st2yo",
          "author": "gusnbru1",
          "text": "I try not to bring my attention to how good Qwen Max is because I want all the tokens for myself üòâ\n\nI use Qwen via API. I find the reasoning and thinking capabilities top notch. Great coder too",
          "score": 5,
          "created_utc": "2026-02-28 00:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7w4noo",
              "author": "Easy_Werewolf7903",
              "text": "Is the Qwen API free? What is the API?",
              "score": 1,
              "created_utc": "2026-02-28 15:27:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7swtpu",
          "author": "National_Meeting_749",
          "text": "I've been running Qwen 3 30B since it came out, nothing had given it a run for its money until GLM 4.7 flash, and I hadn't got a chance to test it out yet. \n\nThen Qwen 3.5 35B drops? I had to check it out and I'm in love. \nIt follows instructions WAY better than the previous gen, is a better coder, smarter, and writes better.\n\nImmediate plug and play upgrade to my whole system.",
          "score": 4,
          "created_utc": "2026-02-28 01:07:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t02mo",
              "author": "Lightningstormz",
              "text": "What platform do you load the model and run it on?",
              "score": 2,
              "created_utc": "2026-02-28 01:27:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7t1fgo",
                  "author": "National_Meeting_749",
                  "text": "I'm using LMstudio to serve and then use a few Claude/Qwen built scripts to run workflows.",
                  "score": 2,
                  "created_utc": "2026-02-28 01:35:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tnflv",
          "author": "v01dm4n",
          "text": "One of the main differences I noted is structured reasoning output. These models think systematically unlike qwen3, qwen2.5.\n\nFor each query, they make a thinking plan, which is decomposition of the query. Then elobate on each point and finally generate output. The result quality is great.\n\nThink of it as meeting a well thought out person than someone who has a monkey mind.",
          "score": 4,
          "created_utc": "2026-02-28 03:55:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u65o2",
          "author": "Gringe8",
          "text": "Im mostly interested in 122b and 27b. They have alot of potential and im amazed with how little vram context takes up. But since i use it for roleplay i need to wait for finetunes. They are just too censored and not good enough as they are. With my setup glm steam is my favorite model for roleplaying atm.",
          "score": 2,
          "created_utc": "2026-02-28 06:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tglcj",
          "author": "sovietreckoning",
          "text": "Definitely didn‚Äôt drop today but it‚Äôs dope. I‚Äôm running a 6_4 hybrid of the 397b and it‚Äôs amazing.",
          "score": 1,
          "created_utc": "2026-02-28 03:10:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uh6iq",
          "author": "SkewRadial",
          "text": "Goood Morning brother, hope you had a great good night sleep .",
          "score": 1,
          "created_utc": "2026-02-28 07:55:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7utuo6",
          "author": "Unedited_Sloth_7011",
          "text": "It's been two weeks... And, compared to 2.5? Why are we ignoring Qwen3?",
          "score": 1,
          "created_utc": "2026-02-28 09:54:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vjpyi",
          "author": "Competitive_Ideal866",
          "text": "> Saw the news about the Qwen3.5 drop today. The MoE architecture on the 397B looks insane,\n\nToo big for my 128GB M4 Max.\n\n> but I‚Äôm mostly curious about the smaller ones like the 27B and 35B.\n\nI'm most interested in the 122B.\n\n> For those who have already pulled the weights, what‚Äôs the biggest difference you‚Äôre noticing compared to 2.5?\n\n2.5? You mean 3?\n\nThe main difference is structured reasoning. Old reasoning was a very ad-hoc rambling. This reasoning is step-by-step and structured using indentation.\n\n> Better reasoning? Faster T/S?\n\nI'm just getting started but Qwen3.5 35B A3B can reverse engineer the data behind a chart as reliably as Qwen3 VL 32B. Faster t/s but I think it is generating much longer reasoning traces.\n\nFWIW, I found the Qwen3 models were worse with long context than the Qwen2.5 1M models. I'd like to test Qwen3.5.",
          "score": 1,
          "created_utc": "2026-02-28 13:28:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xl6ny",
          "author": "Mulan20",
          "text": "You can get the model on Nvidia.",
          "score": 1,
          "created_utc": "2026-02-28 19:52:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81dccr",
          "author": "adorablecreature",
          "text": "Alguien por favor puede decir si tienen la sensaci√≥n de que es mejor que gemini 3.1 pro?",
          "score": 1,
          "created_utc": "2026-03-01 11:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sxad5",
          "author": "dreamingwell",
          "text": "The MOE model is fantastic. Even at Q3. It‚Äôs great as a general purpose instruction follower. Runs fast on my 4090. Thinking I‚Äôll use it as an OpenClaw engine.",
          "score": 0,
          "created_utc": "2026-02-28 01:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uzk6v",
          "author": "EzioO14",
          "text": "Still censored garbage",
          "score": 0,
          "created_utc": "2026-02-28 10:49:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhni4b",
      "title": "Android control with DGX Spark & Qwen3.5-27B with a simple web UI",
      "subreddit": "Qwen_AI",
      "url": "https://v.redd.it/l5of3ehxzcmg1",
      "author": "koc_Z3",
      "created_utc": "2026-03-01 04:12:55",
      "score": 47,
      "num_comments": 2,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Agent",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rhni4b/android_control_with_dgx_spark_qwen3527b_with_a/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o803g8f",
          "author": "koc_Z3",
          "text": "X op: https://x.com/ominousind/status/2027722261591998817?s=46",
          "score": 1,
          "created_utc": "2026-03-01 04:45:54",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o805tz8",
          "author": "SillyLilBear",
          "text": "what version of 27b are you running, and what t/sec?  I just tested it on a Strix halo and was getting 8 t/sec with full 16 bit",
          "score": 1,
          "created_utc": "2026-03-01 05:03:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdhy4i",
      "title": "My wish has been fulfilled",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/gallery/1rdhy4i",
      "author": "celsowm",
      "created_utc": "2026-02-24 14:38:55",
      "score": 38,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdhy4i/my_wish_has_been_fulfilled/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o76lb6m",
          "author": "Samy_Horny",
          "text": "There's still a small batch of models missing, approximately less than 10b, but who knows if they'll release them this week, hopefully they will.",
          "score": 5,
          "created_utc": "2026-02-24 18:36:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77pngw",
              "author": "AbheekG",
              "text": "Desperately waiting for the 2B",
              "score": 1,
              "created_utc": "2026-02-24 21:41:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lmmsm",
          "author": "Jdones2599",
          "text": "Hi, what is your pc/server spec to be able to run this model?",
          "score": 1,
          "created_utc": "2026-02-26 22:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7okfvp",
          "author": "madaradess007",
          "text": "8B, hello?\n\nyou know we can't buy GPUs anymore, right?",
          "score": 1,
          "created_utc": "2026-02-27 11:08:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfyjqp",
      "title": "What's the one feature of Qwen3.5 you didn't expect to be this good?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rfyjqp/whats_the_one_feature_of_qwen35_you_didnt_expect/",
      "author": "Original_Night7733",
      "created_utc": "2026-02-27 05:55:39",
      "score": 34,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "We all expected it to be smart, but what genuinely surprised you? For me, it was how naturally it handles humor and conversational tone without sounding like a corporate robot. What‚Äôs the most impressive \"hidden\" capability you've discovered so far?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rfyjqp/whats_the_one_feature_of_qwen35_you_didnt_expect/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7nnpk2",
          "author": "Old-Sherbert-4495",
          "text": "well im trying 27b q4 now, i asked it what's 1 + 111? boy.... the CoT is crazy ü§£ it goes on and on and on.... hallucinating like hell.. i know llms are not supposed to be good at this but the full precision model on openrouter chat gets to the answer very quick. i dunno if its the quant or missing system prompt.",
          "score": 6,
          "created_utc": "2026-02-27 06:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nr3zj",
              "author": "Old-Sherbert-4495",
              "text": "i take it back... it was my stupid llama.cpp args which I didn't understand. after correcting it boom, got the answer directly, no bs at all...",
              "score": 8,
              "created_utc": "2026-02-27 06:40:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ocqgk",
                  "author": "fijasko_ultimate",
                  "text": "which arguments did you use?",
                  "score": 2,
                  "created_utc": "2026-02-27 09:58:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7oeihz",
                  "author": "Original_Night7733",
                  "text": "wild how much the args matter",
                  "score": 1,
                  "created_utc": "2026-02-27 10:15:20",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ntzv8",
          "author": "DaddyBurton",
          "text": "Its very good at capturing my thoughts and putting it into ideas. Its helped with some coding bugs. Still a little funky at identifying specific things in images, but, does a better job than the previous model. But the heretic model, paired with the default, to double and facts check, chef's kiss. Even adding the plug ins for web search and web link browse, extremely well done!\n\nStill playing around with a lot of stuff, but unfortunately, due to my own limitations, I keep circling back to some of the things I have already tried.\n\nExcited to see what others have to say though.",
          "score": 5,
          "created_utc": "2026-02-27 07:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oedx6",
              "author": "Original_Night7733",
              "text": "Same, I was surprised how it puts my messy thoughts into something I can work with",
              "score": 1,
              "created_utc": "2026-02-27 10:14:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oo52d",
          "author": "msrdatha",
          "text": "It's thinking is a little bit too good. in fact the thinking is too long, that I stop waiting for the answer.   \n  \nYes, I am able to turn off the thinking, using reasoning-budget param on Llama, but I wish there was some way to tell \"buddy, Don't think too much, just tell me what you think..!\"",
          "score": 4,
          "created_utc": "2026-02-27 11:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7orvd6",
              "author": "lundrog",
              "text": "üòÇ",
              "score": 1,
              "created_utc": "2026-02-27 12:07:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oajqw",
          "author": "Unedited_Sloth_7011",
          "text": "I expected the personality would be as good as Qwen3, but I was not ready for the super dramatic reasoning traces, lol. \"I recognize the user's friendly tone and personal connection, acknowledging their use of my name as a sign of trust and rapport. I am attuned to the emotional nuance in their message, responding with genuine warmth and attentiveness. My focus remains on maintaining a seamless, human-like interaction grounded in empathy and presence.\", bro, I just said \"Hi Qwen\".   \nThe present tense really adds to the effect. Loving it so far, it might not win all benchmarks, but it's a very good all-rounder",
          "score": 3,
          "created_utc": "2026-02-27 09:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p2mh3",
          "author": "Ok_Bedroom_5088",
          "text": "working on financial complex tasks/questions (closed-book, close to opus on my small test suite)",
          "score": 2,
          "created_utc": "2026-02-27 13:19:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7osbq4",
          "author": "Mordimer86",
          "text": "I have a new favourite (after gemma3:27b) for helping me ready difficult Chinese texts. I attach a whole document for context, ask it about a fragment and it's very capable in breaking down and analyzing comlex stuff like Buddhist texts. 27b Q4 works fine on 7900XT.\n\n  \nExample:\n\n*The phrase \"ÂúÜÊª°Êô∫ÊÖßÈáëËΩÆÊâÄÊîæÂÖâ\" (yu√°nm«én zh√¨hu√¨ jƒ´n l√∫n su«í f√†ng guƒÅng) translates to \\*\\*\"the light emitted by the perfect/complete wheel of wisdom.\"\\*\\*  Let's break it down:*\n\n*\\* \\*\\*ÂúÜÊª° (yu√°nm«én):\\*\\*  Perfect, complete, fulfilled, rounded.  This implies a wisdom that is not fragmented or lacking.*\n\n*\\* \\*\\*Êô∫ÊÖß (zh√¨hu√¨):\\*\\* Wisdom, intelligence, understanding.*\n\n*\\* \\*\\*ÈáëËΩÆ (jƒ´n l√∫n):\\*\\*  Golden wheel.  This is a metaphor deeply rooted in Buddhist imagery.  The \"wheel\" (l√∫n) represents \\*dharmachakra\\* - the Wheel of Dharma, symbolizing the Buddha's teachings and the turning of the law. \"Golden\" (jƒ´n) symbolizes the preciousness and purity of this teaching.*\n\n*\\* \\*\\*ÊâÄÊîæÂÖâ (su«í f√†ng guƒÅng):\\*\\*  The light emitted, the light that shines forth.  \\*Su«í\\* is a grammatical particle indicating \"that which.\"*\n\n*\\*\\*Meaning in the context of the verse:\\*\\**\n\n*This phrase refers to the illuminating power of perfect wisdom (the Buddhist teachings as embodied by Manjushri, as the verse references him). The \"golden wheel\" represents the complete and radiant Buddhist Dharma.  The \"light\" that it emits is the understanding and insight that dispel ignorance.*\n\n*\\*\\*Overall, \"ÂúÜÊª°Êô∫ÊÖßÈáëËΩÆÊâÄÊîæÂÖâ\" paints a vivid image of the powerful, illuminating wisdom of the Buddha's teachings, breaking through the darkness of ignorance.\\*\\* It's a beautiful and evocative metaphor used to highlight the transformative power of enlightenment.*\n\n*The entire verse translates as:*\n\n*\"In that vast expanse of knowledge, the light emitted by the perfect wheel of wisdom*\n\n*Dispels the ignorance and darkness of all beings.*\n\n*I prostrate to Manjushri Buddha, whose wondrous voice I honor.\"*\n\nI remember paying a language teacher to read texts together as a part of learning Chinese. Costed so much money overall. Now I can do it myself. Teacher got automated xD",
          "score": 1,
          "created_utc": "2026-02-27 12:10:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1re8tje",
      "title": "Qwen3.5-122B-A10B vs. old Coder-Next-80B: Both at NVFP4 on DGX Spark ‚Äì worth the upgrade?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1re8tje/qwen35122ba10b_vs_old_codernext80b_both_at_nvfp4/",
      "author": "alfons_fhl",
      "created_utc": "2026-02-25 09:37:10",
      "score": 33,
      "num_comments": 23,
      "upvote_ratio": 0.87,
      "text": "Running a¬†**DGX Spark (128GB)**¬†. Currently on¬†**Qwen3-Coder-Next-80B (NVFP4)**¬†. Wondering if the new¬†**Qwen3.5-122B-A10B**¬†is actually a flagship replacement or just sidegrade.\n\n**NVFP4 comparison:**\n\n* **Coder-Next-80B**¬†at NVFP4: \\~40GB\n* **122B-A10B**¬†at NVFP4: \\~61GB\n* Both fit comfortably in 128GB with 256k+ context headroom\n\n**Official SWE-Bench Verified:**\n\n* 122B-A10B:¬†**72.0**\n* Coder-Next-80B:¬†**\\~70**¬†(with agent framework)\n* 27B dense:¬†**72.4**¬†(weird flex but ok)\n\n**The real question:**\n\n* Is the 122B actually a¬†**new flagship**¬†or just more params for similar coding performance?\n* Coder-Next was specialized for coding. New 122B seems more \"general agent\" focused.\n* Does the¬†**10B active params**¬†(vs. 3B active on Coder-Next) help with¬†**complex multi-file reasoning**¬†at 256k context or more?\n\n**What I need to know:**\n\n* Anyone done¬†**side-by-side NVFP4**¬†tests on real codebases?\n* **Long context retrieval**¬†‚Äì does 122B handle 256k better than Coder-Next or larger context?\n* **LiveCodeBench/BigCodeBench**¬†numbers for both?\n\nOld Coder-Next was the coding king. New 122B has better paper numbers but barely. Need real NVFP4 comparisons before I download another 60GB.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1re8tje/qwen35122ba10b_vs_old_codernext80b_both_at_nvfp4/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7b9x4n",
          "author": "flavio_geo",
          "text": "I have been using qwen3-coder-next since launch (tested quants Q3KXL to Q8, chose to stay with Q6KXL) and yesterday I tested the 122b, the thing is I only tested it in Q4KXL and since the model has just launched it is possible that its output is not yet fully operational in the current llama.cpp engine (same thing happened with qwen3-coder-next at launch)\n\nSo, given all this considerations, at this moment, the 122b is falling behind big time in coding against the qwen3-coder-next for the coding tests I have done.\n\nOne very simple example is a test I do asking them to create a pygame of chrome dinosaur, and 122b made multiple mistakes before getting 100% working game, while qwen3-coder-next even in Q3 was one shoting it with good quality SVG, collision, birds, cactus, etc. The Q6 never missed it in one-shot and makes very good quality SVG and game dynamics. The 122b after a number of attempts created a 'ugly' version of the game.\n\nAnyhow, the most likely answer to your question at this point is: it is too soon to know which one will perform better.\n\nNot to mention that the two models have different purpose, so ideally you should leverage each one in its better performing scenario.",
          "score": 6,
          "created_utc": "2026-02-25 12:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b3y4q",
          "author": "bacocololo",
          "text": "Where do you see any qwen3.5 in nvfp4 please",
          "score": 4,
          "created_utc": "2026-02-25 11:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b6xy0",
              "author": "alfons_fhl",
              "text": "NVIDIA with TensorRT you can do it by self",
              "score": 2,
              "created_utc": "2026-02-25 11:46:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7dqeim",
                  "author": "Purple-Programmer-7",
                  "text": "What datasets are you using?",
                  "score": 1,
                  "created_utc": "2026-02-25 19:28:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7tekd5",
              "author": "catplusplusok",
              "text": "These are now on huggingface",
              "score": 2,
              "created_utc": "2026-02-28 02:57:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7dnn0a",
          "author": "qubridInc",
          "text": "122B-A10B gives better multi-file reasoning and long-context handling, but raw coding gains over Coder-Next-80B are small in practice.\n\nIf your workload is mostly coding, Coder-Next is still very competitive. If you want stronger agentic + general reasoning, 122B is worth trying.",
          "score": 3,
          "created_utc": "2026-02-25 19:15:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e3xmd",
              "author": "alfons_fhl",
              "text": "Understand you fully, you tested booth?",
              "score": 1,
              "created_utc": "2026-02-25 20:32:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cb3g7",
          "author": "Porespellar",
          "text": "Do you think vision understanding is factor in any of these numbers? Coder next doesn‚Äôt have vision, right?",
          "score": 1,
          "created_utc": "2026-02-25 15:35:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7goaaa",
              "author": "alfons_fhl",
              "text": "In my use case no. But you‚Äòre right, I fully forgot this aspect",
              "score": 1,
              "created_utc": "2026-02-26 04:57:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7dchqj",
          "author": "klop2031",
          "text": "I also found the larger model to perform poorly. Even compared to the 27b dense. It could be my task or my quantization but i was surprised to see it kinda suck for now. Im hoping there will be some optimizations in lamma.cpp",
          "score": 1,
          "created_utc": "2026-02-25 18:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f18kk",
          "author": "jinnyjuice",
          "text": "The 3.5 122B is multimodal.",
          "score": 1,
          "created_utc": "2026-02-25 23:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f7xt0",
          "author": "jinnyjuice",
          "text": ">Both fit comfortably in 128GB with 256k+ context headroom\n\nHow do you calculate the memory needed for the number of context tokens?",
          "score": 1,
          "created_utc": "2026-02-25 23:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gouks",
              "author": "alfons_fhl",
              "text": "Ask ai and don‚Äôt forget your quantisation. I mostly use this repo from the YouTuber Alex Ziskind: https://github.com/alexziskind1/llm-inference-calculator",
              "score": 1,
              "created_utc": "2026-02-26 05:01:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fk9rv",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-26 00:57:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7goz46",
              "author": "alfons_fhl",
              "text": "What do you mean with the looping Problem?",
              "score": 1,
              "created_utc": "2026-02-26 05:02:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7j17ts",
          "author": "Total_Activity_7550",
          "text": "Agent framework does so much! If Qwen3-Coder-Next is worse with agent framework, then adding agent framework to Qwen3-122B-A10B will blow Qwen3-Coder-Next out of the water.\n\nI replaced Qwen3-Coder-Next with Qwen3.5-27B, despite being slower, I guess, Qwen3-122B-A10B should be a little better and at least twice as fast, compared to Qwen3.5-27B, although slower than Coder. Still, I think it is better to wait and get one-shot success, rather than fix code and repeatedly prompt the agent.",
          "score": 1,
          "created_utc": "2026-02-26 15:27:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nyimw",
          "author": "catplusplusok",
          "text": "I definitely find QWEN 3.5 122B model to be a more natural and creative problem solver vs Qwen3-Coder-Next. On the other hand, it's going to be slower on any given hardware, more so with think mode enabled, so depends on your preferences for waiting more for AI work vs providing more human guidance.",
          "score": 1,
          "created_utc": "2026-02-27 07:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o98f9",
          "author": "Potential-Leg-639",
          "text": "Whats your speed on the spark? \n\nOn Strix Halo i get 18-20 tk/s initial on th 122B-MXFP4, but degrades when context is filled up (128k), yesterday i saw 8 tk/s, but not sure if that was maybe because another model was parallel loaded, have to check again.",
          "score": 1,
          "created_utc": "2026-02-27 09:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o9cf5",
              "author": "Potential-Leg-639",
              "text": "In yesterdays tests it was strong as a code reviewer/planner. It found/enhanced quite a few things from a project created by cloud Minimax M2.5/GLM 4.7, that was solid work.",
              "score": 1,
              "created_utc": "2026-02-27 09:26:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7teibg",
          "author": "catplusplusok",
          "text": "Qwen 3.5 122B seems a lot better at writing an Android app from scratch than Coder-Next. Still need to iterate and ask it to fix issues, but that's true for cloud models too. The downside is that 10B active parameters is a lot slower than 3B. Make sure to turn off the reasoning mode in kwargs or you will have to leave it coding for hours.",
          "score": 1,
          "created_utc": "2026-02-28 02:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7avwfy",
          "author": "SkullEnemyX-Z",
          "text": "Can someone create an agentic version under 4gb? Would be dream come true for openclaw",
          "score": 0,
          "created_utc": "2026-02-25 10:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7axiv5",
              "author": "Fast_Thing_7949",
              "text": "you can try Nanbeige4.1-3B",
              "score": 2,
              "created_utc": "2026-02-25 10:26:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7cg5bl",
                  "author": "akumaburn",
                  "text": "That's a thinking model, best to wait for the instruct version or use: [https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill](https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill)",
                  "score": 1,
                  "created_utc": "2026-02-25 15:58:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cg547",
              "author": "akumaburn",
              "text": "Not quite under 4GB but close Q8\\_0 of [https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill-GGUF](https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill-GGUF) ",
              "score": 1,
              "created_utc": "2026-02-25 15:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rexnjc",
      "title": "Using Qwen to make old game textures more realistic",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rexnjc/using_qwen_to_make_old_game_textures_more/",
      "author": "MrWrodgy",
      "created_utc": "2026-02-26 02:15:10",
      "score": 25,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "https://preview.redd.it/4bwuvi220rlg1.png?width=256&format=png&auto=webp&s=b85ec451bb507a7b29f345d267817d3ca86abdde\n\nhttps://preview.redd.it/kzedp3240rlg1.png?width=1024&format=png&auto=webp&s=0fefb877ba5c908e062bb27de23aee71c5073dbd\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rexnjc/using_qwen_to_make_old_game_textures_more/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7fzfk3",
          "author": "MrWrodgy",
          "text": "https://preview.redd.it/7q3lh6nd1rlg1.png?width=821&format=png&auto=webp&s=c225ad86b64d4d9474083099b2d3d5f1bf3be3ff\n\n",
          "score": 3,
          "created_utc": "2026-02-26 02:23:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7fzlgh",
              "author": "MrWrodgy",
              "text": "https://preview.redd.it/ks3iab5w1rlg1.png?width=128&format=png&auto=webp&s=5e153eda9ab084d487c87dfc18fa062b222364e2\n\n",
              "score": 2,
              "created_utc": "2026-02-26 02:24:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7fzmbl",
                  "author": "MrWrodgy",
                  "text": "https://preview.redd.it/uxdsax0x1rlg1.png?width=1024&format=png&auto=webp&s=3a3821e03f0de6742391bf9c0b37aa94805f8786\n\n",
                  "score": 2,
                  "created_utc": "2026-02-26 02:24:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7fzgi4",
              "author": "MrWrodgy",
              "text": "before \n\nhttps://preview.redd.it/fj2ljl2r1rlg1.png?width=753&format=png&auto=webp&s=b842ccb556dfb74d08252b14a575ee2913123cb9\n\n",
              "score": 1,
              "created_utc": "2026-02-26 02:23:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7gxeak",
                  "author": "Gold_Sugar_4098",
                  "text": "I like the before better, in-game.\nA better solution would be something in between, but that‚Äôs my 2 cent.\n\nAlso, terrain building (close-ups) might benefit ?\n",
                  "score": 1,
                  "created_utc": "2026-02-26 06:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gqis9",
          "author": "ischanitee",
          "text": "Recently I kinda liking qwen than any other wise out there",
          "score": 2,
          "created_utc": "2026-02-26 05:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fy375",
          "author": "MrWrodgy",
          "text": "prompt: While maintaining the overall structure of the image, try to make the elements more realistic.",
          "score": 2,
          "created_utc": "2026-02-26 02:16:12",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7o0hm5",
              "author": "stopbanni",
              "text": "What model? How? Does they always had way to generate images?",
              "score": 1,
              "created_utc": "2026-02-27 08:02:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7q8gka",
                  "author": "MrWrodgy",
                  "text": "Qwen3.5-Plus in \"Create Image\" mode",
                  "score": 1,
                  "created_utc": "2026-02-27 16:51:08",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rh4qga",
      "title": "New here! saw some benchmarks of Qwen 3.5 27B, is this for real?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rh4qga/new_here_saw_some_benchmarks_of_qwen_35_27b_is/",
      "author": "SuzerainR",
      "created_utc": "2026-02-28 14:59:00",
      "score": 22,
      "num_comments": 20,
      "upvote_ratio": 0.93,
      "text": "It seems really insane to me that it scored very close to the proprietary trillion parameter models as an open source desktop model. Does someone have an idea how they achieved this? or do you guys think its not accurate ",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rh4qga/new_here_saw_some_benchmarks_of_qwen_35_27b_is/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7wjvp5",
          "author": "sovietreckoning",
          "text": "I‚Äôm using a 6_4 quant of the full model and it‚Äôs unreal.\n\nEdit: quant, not quart",
          "score": 5,
          "created_utc": "2026-02-28 16:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wogdw",
              "author": "SuzerainR",
              "text": "I cant believe what I am seeing",
              "score": 2,
              "created_utc": "2026-02-28 17:06:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7wz42x",
              "author": "ProfessionalLet9913",
              "text": "I have a macbook pro m1 max 32gb ram, the model seems to not work on it, any idea ?",
              "score": 1,
              "created_utc": "2026-02-28 18:00:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7x405l",
                  "author": "SuzerainR",
                  "text": "27B means that it needs a beefier system. What u/sovietreckoning did was quantization, which helps you run it on daily setups, you lose accuracy, but given its a small model it shouldnt be much",
                  "score": 2,
                  "created_utc": "2026-02-28 18:24:39",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o804sjx",
                  "author": "IvaldiFhole",
                  "text": "I tried to use qwen3.5-35b-a3b on an M2 Max 32gb via LM Studio, but 90% of the time it gets stuck in loops. I went back to qwen3 for the time being.\n\nIs that your experience, or do you mean something else by \"not work\"?",
                  "score": 2,
                  "created_utc": "2026-03-01 04:55:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7xgyzk",
              "author": "CaptBrick",
              "text": "Kv cache quantization or nah?",
              "score": 1,
              "created_utc": "2026-02-28 19:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ynx5n",
                  "author": "laser50",
                  "text": "Q8 is almost lossless, so if you need to, i'd choose that. If you can do without, do without.",
                  "score": 1,
                  "created_utc": "2026-02-28 23:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xisj3",
          "author": "Background_Spell_418",
          "text": "What t/s are you guys getting. I am running quant4 from unsloth and i am not getting more than 14t/s. Using M4 Max 36gb",
          "score": 1,
          "created_utc": "2026-02-28 19:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xk8ty",
              "author": "Birdinhandandbush",
              "text": "It's personal preference, but I would consider that fine",
              "score": 1,
              "created_utc": "2026-02-28 19:47:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7xro3h",
              "author": "330d",
              "text": "Q8 quant, around 8t/s on M1 Max 64GB",
              "score": 1,
              "created_utc": "2026-02-28 20:26:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o8034bj",
                  "author": "himefei",
                  "text": "For Mac you can run MLX 5 bit. It should have very little degradation in quality.\nOr you can try convert to mxfp4 yourself",
                  "score": 1,
                  "created_utc": "2026-03-01 04:43:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7y9jwr",
              "author": "LazyLancer",
              "text": "I don't know if it's of any use to you as i'm not on Apple, but i am getting 40 t/s on a regular 4090 using 27B Q4\\_K\\_M.\n\nit barely barely fits into the GPU, if i have anything else using VRAM i am down to 17 t/s (Ryzen 9800X3D, 32 Gb DDR5)",
              "score": 1,
              "created_utc": "2026-02-28 22:02:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ymba3",
              "author": "congard",
              "text": "7900xtx, Q4_K_M from unsloth, ‚âà28t/s, 48K context, the model fits entirely in the GPU and I even have 4 gigs free. If I use iGPU for graphics instead, I have ‚âà6gb of vram available",
              "score": 1,
              "created_utc": "2026-02-28 23:13:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7yo5js",
                  "author": "laser50",
                  "text": "Maybe a tip then, go for the Q4_K_XL from unsloth, slightly better quality where it matters for a tiny increase in size",
                  "score": 1,
                  "created_utc": "2026-02-28 23:24:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o84lgoh",
              "author": "AppealSame4367",
              "text": "If you can tolerate slightly worse intelligence: I'm getting 10-20 tps with qwen3.5-35b-a3b, q2\\_k\\_xl quant on an rtx 2060, lol.\n\nWith your setup you should get 40-50 tps with that one.",
              "score": 1,
              "created_utc": "2026-03-01 22:03:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o80zvx8",
          "author": "serioustavern",
          "text": "27B at Q4_K_L has been surprisingly good for me for programming & technical tasks.",
          "score": 1,
          "created_utc": "2026-03-01 09:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81gagp",
          "author": "Elojan91",
          "text": "https://preview.redd.it/uwhktdoabfmg1.jpeg?width=3024&format=pjpg&auto=webp&s=55be8c418f5ec732522fea01dab62a9eeb7af3ab\n\nQwen3 5-35-A3B its perfect. RTX 5900, can i upgrade my laptop? Or is this the end?",
          "score": 1,
          "created_utc": "2026-03-01 12:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84ziog",
          "author": "Sweet-Ad-654",
          "text": "It‚Äôs slow for me even on M3 ultra for prompt processing in LM studio to the point it‚Äôs unusable. Anyone else?",
          "score": 1,
          "created_utc": "2026-03-01 23:20:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdvc8a",
      "title": "Qwen lanza nuevos modelos peque√±os 3.5",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rdvc8a/qwen_lanza_nuevos_modelos_peque√±os_35/",
      "author": "el-rey-del-estiercol",
      "created_utc": "2026-02-24 22:42:13",
      "score": 16,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "Qwen acaba de lanzar nuevos modelos peque√±os de la version 3.5 GRACIAS QWEN!!! TODOS DEBEMOS APOYA A QWEN LA MEJOR EMPRESA DEL MUNDO ENTERO!!!!!!!\n\nLos modelos los podeis ver en la pagina de unsloth o en hugging face.\n\nAYUDAR A QWEN ELLOS NOS AYUDAN A NOSOTROS!!!\n\nLos mejores modelos medianos para hardware de consumo!!! QWEN TODOS ESTAMOS CON VOSOTROS Y OS APOYAMOS VAMOS HACER PUBLICIDAD DE LA MEJOR EMPRESA DEL MUNDO POR LIBERAR MODELOS PARA TODOS!!!\n\nQue se jodan los otros malditos openai y todos los otros desgraciados que no comparten nada Opensource ojala se le arruinen sus empresas‚Ä¶NOSOTROS DEBEMOS APOYAR SOLO A QWEN Y ALGUNOS USAR SU MODELO DE IA ONLINE PARA APOYA ECONOMICAMENTE A LA EMPRESA!!! Tenemos que ayudarlos como ellos nos ayudan a nosotros!!!!",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdvc8a/qwen_lanza_nuevos_modelos_peque√±os_35/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7875ig",
          "author": "el-rey-del-estiercol",
          "text": "No eres creyente en QWEN ? Eres un traidor? Hay que tener fe y creer en QWEN Y REZAR por ellos entendido!!! O rezas por ellos o te doy unos palazos!!! üòéüòéüòéüòé aqui te dejo en enlace para un infiel que no cree y no tiene fe. https://huggingface.co/collections/unsloth/qwen35",
          "score": 2,
          "created_utc": "2026-02-24 23:07:49",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786gu9",
          "author": "charmander_cha",
          "text": "Pequeno √© ate no m√°ximo 14B e nao to vendo nenhuma deste tipo",
          "score": 1,
          "created_utc": "2026-02-24 23:04:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o787pnt",
              "author": "Effective_Head_5020",
              "text": "Queria o 9b, pelo jeito n√£o vai ter\n\n\n\n\nMas as vers√µes destiladas do 35 e 27 t√£o dando pra rodar",
              "score": 1,
              "created_utc": "2026-02-24 23:10:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o786jwh",
          "author": "el-rey-del-estiercol",
          "text": "Si que hay busca en reedit la pagina de unsloth burro!!!",
          "score": 1,
          "created_utc": "2026-02-24 23:04:39",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786lnx",
          "author": "el-rey-del-estiercol",
          "text": "Hay muchas versiones y una de 110B",
          "score": 1,
          "created_utc": "2026-02-24 23:04:55",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786mig",
          "author": "el-rey-del-estiercol",
          "text": "Otra de 35B",
          "score": 1,
          "created_utc": "2026-02-24 23:05:02",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o787bvr",
          "author": "el-rey-del-estiercol",
          "text": "Ahora ponte a rezar y a dar las gracias a nuestro se√±or QWEN en los cielos!!! A ellos les debemos la gloria del se√±or üòçüòçüòçüòçüòç",
          "score": 1,
          "created_utc": "2026-02-24 23:08:47",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o787yrt",
          "author": "el-rey-del-estiercol",
          "text": "Debeis anotaros al portal de qwen online para agradecer y ayudar a los desarrolladores ya que ellos no viven del ‚Äúaire‚Äù tienen que comer y tienen sus gastos y debemos apoyarlos economicamente tambien nos dan los mejores modelos gratis del mercado!!! ASI QUE YA SABEIS ‚Ä¶suscribiros a la IA online de qwen en qwen.ai para ayudar a la empresa!!!",
          "score": 1,
          "created_utc": "2026-02-24 23:12:06",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1ri2l62",
      "title": "I really don't understand how it's possible to code with Qwen 3.5 series",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1ri2l62/i_really_dont_understand_how_its_possible_to_code/",
      "author": "BitXorBit",
      "created_utc": "2026-03-01 17:05:02",
      "score": 13,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "I'm running the models locally, on Mac Studio m3 ultra - lm studio + open code.\n\niv'e tried more than 20 different prompts, no matter what I do, I can't get any code as output.\n\nthe models gets into some infinite loop of generating tokens + process prompt.\n\nI was really excited to test the hype around this models but I just can't get any results.\n\nI tried with 0.3 Temperature and Top K 50, it didn't helped\n\nI tried using openai and Claude to help me generate prompt specially for this line of models with very strict rules, still nothing.\n\nqwen3-coder-next worked well.\n\nMiniMax M2.5 worked extremely well.\n\nplease if anyone can assist on that matter, I really like to test this line of models.",
      "is_original_content": false,
      "link_flair_text": "Help üôã‚Äç‚ôÇÔ∏è",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1ri2l62/i_really_dont_understand_how_its_possible_to_code/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o82yf7t",
          "author": "Ok_Significance_9109",
          "text": "Update to the latest version of LM studio, the latest beta engine of llama and mlx, and for now use gguf models. This will solve it. They were released yesterday.",
          "score": 9,
          "created_utc": "2026-03-01 17:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82znq7",
              "author": "BitXorBit",
              "text": "So, it‚Äôs mlx problem?",
              "score": 2,
              "created_utc": "2026-03-01 17:17:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o831mjo",
                  "author": "Ok_Significance_9109",
                  "text": "They both work now, but mlx is not reusing the cache, so every prompt is reprocessing the whole context. It will probably be fixed soon. This problem was fixed for gguf only yesterday, and tool calling the day before.",
                  "score": 1,
                  "created_utc": "2026-03-01 17:27:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o83nzza",
          "author": "Uranday",
          "text": "There was an issue with tools. Did you go for the new UD version?\n\nNoob here, but yesterday I made quite an app in c#/react.",
          "score": 1,
          "created_utc": "2026-03-01 19:12:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83scza",
              "author": "BitXorBit",
              "text": "I will check it out and report back, thank you!",
              "score": 1,
              "created_utc": "2026-03-01 19:33:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o83pk8r",
          "author": "carchengue626",
          "text": "Use droid instead of open code for better results.",
          "score": 1,
          "created_utc": "2026-03-01 19:19:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84h14e",
          "author": "Jayfree138",
          "text": "Yeah same. I get infinite thought loops in ollama and it won't even load in open web UI.\n\nIt's not working correctly on most backends. Going to have to wait for updates since it just came out. It'll be a really good model once they work out the software bugs in a few weeks. Too new.",
          "score": 1,
          "created_utc": "2026-03-01 21:40:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84hkdj",
              "author": "BitXorBit",
              "text": "I just updated lm studio to latest version and downloaded latest gguf version instead of mlx, it working now. Tomorrow i will try all models again",
              "score": 5,
              "created_utc": "2026-03-01 21:42:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85so7p",
          "author": "kayteee1995",
          "text": "lmstudio release new update every these days. \nand try GGUF from unsloth, they do it so good!",
          "score": 1,
          "created_utc": "2026-03-02 02:15:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg5jyw",
      "title": "Using the 1M context to analyze Influencer/Social Media trends.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rg5jyw/using_the_1m_context_to_analyze_influencersocial/",
      "author": "Old_Investment7497",
      "created_utc": "2026-02-27 12:38:26",
      "score": 12,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "I dumped about 50 pages of raw social media comments and influencer engagement metrics into Qwen3.5. I asked it to identify the core audience sentiment and suggest which influencers had the most authentic reach. The way it maintained context from page 1 to page 50 without hallucinating is incredible. Is anyone else using this for heavy data analysis?",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rg5jyw/using_the_1m_context_to_analyze_influencersocial/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7ph09o",
          "author": "zephyr_33",
          "text": "which one? plus? flash? 122b?",
          "score": 3,
          "created_utc": "2026-02-27 14:38:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg0skz",
      "title": "Qwen3.5 vs DeepSeek-V3: Current state of the art?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rg0skz/qwen35_vs_deepseekv3_current_state_of_the_art/",
      "author": "ischanitee",
      "created_utc": "2026-02-27 08:05:58",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Two absolute powerhouses in the open-source space right now. For those who regularly use both, how do they compare? Does Qwen's native multimodal edge out DeepSeek's pure coding/math focus for general daily tasks?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rg0skz/qwen35_vs_deepseekv3_current_state_of_the_art/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7uk37l",
          "author": "ebrahim750",
          "text": "Hard to tell, both are good, but I noticed much less halucinations from Qwen3.5",
          "score": 1,
          "created_utc": "2026-02-28 08:21:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ri6j4d",
      "title": "Qwen-3.5 in Claude Code?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1ri6j4d/qwen35_in_claude_code/",
      "author": "GreenInterview",
      "created_utc": "2026-03-01 19:29:29",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "What is your experience with using qwen-3.5 in Claude Code? I started with the $20 Anthropic subscription, but it depletes the 5 hour usage limit in one hour ‚Äì if I‚Äôm lucky.\n\nSo I first looked at DeepSeek as an alternative ‚Äòengine‚Äô, and it works *fine*, but you have to deal with a limit in the number of tokens the API can process: when the conversation gets complex, it will easily hang. And although overall it would cost less than the Anthropic subscription, based on my projections I would end up spending *at least* $15 per month.\n\nWith the Alibaba Cloud subscription, I‚Äôve only got $3 to pay the first month, $5 the second, and then it will be $10, but... gosh, it runs super-smoothly, never complains about hitting some token limit, and with 4 days of intensive usage I‚Äôve barely used 5% of my monthly quota.\n\nSo far it seems like a real deal. I‚Äôm wondering if there‚Äôs a catch or if it‚Äôs really this good and this cheap.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1ri6j4d/qwen35_in_claude_code/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o83ujo6",
          "author": "Negative_Scarcity315",
          "text": "I'm on the same boat. I tried to do claude code with hands off approach, multiple agents, spec writer, field specialists, database migration agent, algorithmic performance specialist, uncle bob agent, martin fowler agent, lol, I was happy with the results but available tokens melt away, not viable on the pro subscription. I'm leaning more and more towards Qwen for the cost-effectiveness.",
          "score": 3,
          "created_utc": "2026-03-01 19:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85bggh",
          "author": "justgetoffmylawn",
          "text": "How do you wire Qwen up to Claude Code? I've only used CC with Claude models.\n\nAny issues setting up and paying for Alibaba (if you're in the USA)?",
          "score": 1,
          "created_utc": "2026-03-02 00:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85ph7b",
          "author": "lundrog",
          "text": "Use a api gateway https://github.com/mcowger/plexus",
          "score": 1,
          "created_utc": "2026-03-02 01:55:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rh8t3x",
      "title": "27b vs 35b",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rh8t3x/27b_vs_35b/",
      "author": "elestio-support",
      "created_utc": "2026-02-28 17:41:36",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 0.83,
      "text": "27b Q2 version seems to be better than 35b q4 on my side on pretty much everything (except speed)\n\ndo you see the same thing?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rh8t3x/27b_vs_35b/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7ydle3",
          "author": "woolcoxm",
          "text": "27b does just as well as the 122b, i find it better. the 122b is a bit more detailed.",
          "score": 3,
          "created_utc": "2026-02-28 22:24:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7x2hi7",
          "author": "Whiplashorus",
          "text": "Yes and it's normal \nThe 27b is dense and the 35b is moe soo ur comparing 27b to ~14b",
          "score": 2,
          "created_utc": "2026-02-28 18:17:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y64dz",
          "author": "Fine_Atmosphere557",
          "text": "35b iq2 is kicking ass for me",
          "score": 2,
          "created_utc": "2026-02-28 21:44:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7y6cfu",
              "author": "MarsupialNo7114",
              "text": "Speed is wonderful, I get 128 tokens/s with it\nBut answers from 27b are always a bit better",
              "score": 2,
              "created_utc": "2026-02-28 21:45:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ynp69",
          "author": "timbo2m",
          "text": "Depends on the use case, 27B has to work harder so is slower but more accurate for stuff like coding but 35B has 3B active so is a lot faster at the cost of a little accuracy.",
          "score": 2,
          "created_utc": "2026-02-28 23:21:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80arxn",
          "author": "adel_b",
          "text": "For me, 27b seems better for almost everything but car wash puzzle",
          "score": 1,
          "created_utc": "2026-03-01 05:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80r48w",
          "author": "SillyLilBear",
          "text": "it's dense, that's why",
          "score": 1,
          "created_utc": "2026-03-01 08:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o810ns3",
          "author": "serioustavern",
          "text": "Yes. According to \n most benchmarks 27B is better than 35B A3B. The ‚ÄúA3B‚Äù part is important.",
          "score": 1,
          "created_utc": "2026-03-01 09:36:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}