{
  "metadata": {
    "last_updated": "2026-02-26 17:15:53",
    "time_filter": "week",
    "subreddit": "Qwen_AI",
    "total_items": 20,
    "total_comments": 128,
    "file_size_bytes": 126482
  },
  "items": [
    {
      "id": "1rdnzbe",
      "title": "Connected Qwen3-VL-2B-Instruct to my security cameras, result is great",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/gallery/1rdnzbe",
      "author": "solderzzc",
      "created_utc": "2026-02-24 18:17:10",
      "score": 244,
      "num_comments": 48,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Qwen VL",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdnzbe/connected_qwen3vl2binstruct_to_my_security/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o77mbht",
          "author": "beedunc",
          "text": "Frankly, those tiny Qwen VL models are going to change the world. Nice work!",
          "score": 14,
          "created_utc": "2026-02-24 21:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77xmlc",
              "author": "solderzzc",
              "text": "Agreed. Qwen VL is really good. ",
              "score": 5,
              "created_utc": "2026-02-24 22:19:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77eegx",
          "author": "Firm-Evening3234",
          "text": "Bel progetto, da approfondire e integrare su django",
          "score": 5,
          "created_utc": "2026-02-24 20:50:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77jvoa",
              "author": "solderzzc",
              "text": "Quale funzionalit√† ti serve integrare in Django",
              "score": 1,
              "created_utc": "2026-02-24 21:15:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77k8hh",
                  "author": "Firm-Evening3234",
                  "text": "Vorrei ricreare l'ambiente unifi per i sistemi nvr, database persone, riconoscimento di atti ostili etc etc",
                  "score": 1,
                  "created_utc": "2026-02-24 21:16:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76itv8",
          "author": "Samy_Horny",
          "text": "It's a shame that we'll have to wait even longer for the latest batch of small Qwen 3.5 models.",
          "score": 3,
          "created_utc": "2026-02-24 18:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76j0np",
              "author": "solderzzc",
              "text": "Yes, hopefully we will get them soon. ",
              "score": 2,
              "created_utc": "2026-02-24 18:26:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o794h9i",
          "author": "mike7seven",
          "text": "The Qwen models are impressive for sure, for its size running on GGUF on Mac makes it even more impressive. Does the app allow MLX Versions of the models? I only ask because the MLX (MLX-VLM)versions take up fewer resources and are faster.",
          "score": 3,
          "created_utc": "2026-02-25 02:12:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7980hd",
              "author": "solderzzc",
              "text": "Not yet, I'll look into MLX-VLM for an integration, thanks for your information. ",
              "score": 2,
              "created_utc": "2026-02-25 02:32:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79aoa6",
                  "author": "mike7seven",
                  "text": "Not sure it would do anything additional other than being able to say ‚ÄúA blade of grass moved‚Äù.",
                  "score": 2,
                  "created_utc": "2026-02-25 02:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bos99",
          "author": "ConversationFun940",
          "text": "What's the ui used here.. sorry new to ai",
          "score": 3,
          "created_utc": "2026-02-25 13:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bya1b",
              "author": "solderzzc",
              "text": "The UI is an application developed by me, framework is the same as VSCode ( Electron based ). You can download it here: https://www.sharpai.org ",
              "score": 2,
              "created_utc": "2026-02-25 14:32:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7gcz3z",
                  "author": "dyeusyt",
                  "text": "Ngl instead of focusing on personal home security, you should probably pivot the product toward enterprise or factory-level use cases; maybe something like tracking shipments and flagging anomalies more effectively ",
                  "score": 2,
                  "created_utc": "2026-02-26 03:42:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gddno",
          "author": "Bohdanowicz",
          "text": "I did something similar with ring, then I decided to add gait analysis  and torso to leg ratio analysis so people leaving with their backs to the camera were identified (used video of people approaching camera + facial recognition to match gait/face so even if it doesn't know who it is it still knows its the same person, then decided to add a wildlife detector and had it build out a database of all the  local wildlife and then started analyzing the wildlife patterns....  All local inference.  Next cameras I get won't need amazon.  Just a few high def 4k cameras and i'll connect it to whatever I want and do whatever I want.\n\nWelcome to the singularity.",
          "score": 3,
          "created_utc": "2026-02-26 03:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77mf2g",
          "author": "Artem_C",
          "text": "Yeah, you just pretty much doxxed yourself. Atleast blur the street across",
          "score": 2,
          "created_utc": "2026-02-24 21:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77o1b8",
              "author": "solderzzc",
              "text": "... You are ring, will do next time, it doesn't allow me to edit the photo now. ",
              "score": 2,
              "created_utc": "2026-02-24 21:34:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79uzqo",
          "author": "Sadboy2403",
          "text": "so how many tokens per hr watched? I know its going to consume more if theres activity in the camera but how much? ",
          "score": 2,
          "created_utc": "2026-02-25 04:52:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79w9do",
              "author": "solderzzc",
              "text": "If local model is being used for video analysis, the video analysis will cost no token.\nCloud model(gpt-4o) to handle video will be around $5 per day, I have cameras located in-house, so it generated more than 150+ clips per day. 3M input, 2M output.\n\nCloud LLM will be used when you chat, it handles tool call. Summary VLM's generation, search through the video summary for the user's query. Something around 500k inputs, 80k outputs. per day. (GPT-5.2) \n\nI'm testing an integration with lmstudio, to leverage QWEN-27GB locally as the brain model for tool call and conversation (LLM) on M3 24GB, not done yet.  ",
              "score": 1,
              "created_utc": "2026-02-25 05:01:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o79wi6u",
              "author": "solderzzc",
              "text": "Motion detection is at the frontend, did optimization for one clip as well, not send every frame. If send all the frames to cloud for watching, it's about $0.1 per 20s. ( gpt-4o ).",
              "score": 1,
              "created_utc": "2026-02-25 05:03:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ae8or",
          "author": "mihaii",
          "text": "it's a pity u can't use a LLM on premise (i see that u can only use OpenAI API at this point). ",
          "score": 2,
          "created_utc": "2026-02-25 07:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bxoay",
              "author": "solderzzc",
              "text": "Yes, I'm testing QWen on premise, QWEN3 Coder Instruct 28B(hosted by lmstudio) could be running on my 24GB MACBOOK AIR M3, I'll release it soon. \nIts tool use capability is good. Haven't tested it throughly. ",
              "score": 1,
              "created_utc": "2026-02-25 14:29:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7d07tp",
                  "author": "mihaii",
                  "text": "but at this point, there is no way of using a local LLM  , just the visual AI\n\nany reasons for not going with only one LLM? that does both vision and text?\n\nis the project opensource / vibecoded?",
                  "score": 2,
                  "created_utc": "2026-02-25 17:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7at556",
          "author": "Virtual_Sherbert6846",
          "text": "I've been trying to piece together object detection for my robot and it is terrible. Something like this is a big upgrade.",
          "score": 2,
          "created_utc": "2026-02-25 09:45:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7by1do",
              "author": "solderzzc",
              "text": "What hardware you are using? I have DGX Spark ( CUDA 13 ), Jetson Nano / AGX. Let me know if an aarch64 release is required.",
              "score": 1,
              "created_utc": "2026-02-25 14:31:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7efy29",
                  "author": "Virtual_Sherbert6846",
                  "text": "Jetson Orin NX 16GB. I am also running a STT and TTS model on the Jetson. I may offload to my PC with a 4090 RTX.",
                  "score": 2,
                  "created_utc": "2026-02-25 21:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bnhqo",
          "author": "BelieverInYellow",
          "text": "Omg that output is super detailed! :0",
          "score": 2,
          "created_utc": "2026-02-25 13:34:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7byc3u",
              "author": "solderzzc",
              "text": "Yes, too detailed :) ",
              "score": 1,
              "created_utc": "2026-02-25 14:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cpng1",
          "author": "Obvious_Fix_1012",
          "text": "Very cool! Nice job.",
          "score": 2,
          "created_utc": "2026-02-25 16:42:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cripc",
          "author": "SearchTricky7875",
          "text": "damn, that is one useful use, I can use it with my raspberry pi.",
          "score": 2,
          "created_utc": "2026-02-25 16:50:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cyswl",
              "author": "solderzzc",
              "text": "Got it, let me prepare a Raspberry Pi build. ",
              "score": 1,
              "created_utc": "2026-02-25 17:24:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d6jm0",
          "author": "Plenty-Mix9643",
          "text": "What does that bring? I mean it is cool, but what is the benefit of it for you.",
          "score": 2,
          "created_utc": "2026-02-25 17:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ddlth",
              "author": "solderzzc",
              "text": "To be honest, it started with a personal annoyance: I have 'stupid' cameras that I pay good monthly fees for, yet I still have to scrub through hours of footage myself to find anything.\n\nThe benefit for me is two-fold:\n\nIntelligence & Automation: I want to 'teach' my cameras what to look for so I don't have to. Aegis pulls my cloud clips (Ring/Blink wired/battery) locally so I can search them with a private, local LLM. This weekend project honestly would have been impossible without vibe coding‚Äîit's allowed me to hit 400k lines of logic at a speed traditional dev couldn't touch.\n\nThe 'GitHub' Model: I believe the future of AI is local. My plan is to keep a powerful free version for homeowners to regain their privacy. The business model follows the GitHub or Slack approach: provide massive value to the community for free, while providing support needed for SMB and Enterprise‚Äîan area where I‚Äôve spent my career training models and building products.",
              "score": 1,
              "created_utc": "2026-02-25 18:30:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d7fv8",
          "author": "jedsk",
          "text": "awesome!",
          "score": 2,
          "created_utc": "2026-02-25 18:03:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7itrlb",
              "author": "Wide-Personality6520",
              "text": "For real! The detail it captures is next level. Have you tried it on different scenes yet?",
              "score": 1,
              "created_utc": "2026-02-26 14:51:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ef1rc",
          "author": "Busy-Guru-1254",
          "text": "Cool. How does it work. Do frame by frame analysis and then summarize the description of all the frames?",
          "score": 2,
          "created_utc": "2026-02-25 21:23:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ekgps",
              "author": "solderzzc",
              "text": "yes, that's the first version, then you know the speed is very slow w/ local model, and cost is very high with cloud model. \nSo I pick up significant frames to make it cost less... ",
              "score": 1,
              "created_utc": "2026-02-25 21:48:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7878uw",
          "author": "cool-beans-yeah",
          "text": "But where's the mail man and the street doesn't have a white line down the middle, does it?\n\nEdit: just realized that may have come across as sarcastic...none intended!",
          "score": 1,
          "created_utc": "2026-02-24 23:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78cifv",
              "author": "solderzzc",
              "text": "Mail man was at the first several seconds, I forwarded to 12s to not disclose mailman's privacy. ... White line is a hallucination. It thought white line should be always on the road. :) ",
              "score": 1,
              "created_utc": "2026-02-24 23:36:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7aljzh",
                  "author": "cool-beans-yeah",
                  "text": "Oh ok! Thanks",
                  "score": 2,
                  "created_utc": "2026-02-25 08:34:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7cktn4",
                  "author": "Crafty-Young3210",
                  "text": "dont you think the fact that its hallucinating something thats clearly not there is an issue for using these models for this application?",
                  "score": 2,
                  "created_utc": "2026-02-25 16:20:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7i8vm2",
          "author": "LoveInTheFarm",
          "text": "It‚Äôs huggingface ?",
          "score": 1,
          "created_utc": "2026-02-26 12:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j7raj",
          "author": "Few_Matter_9004",
          "text": "Are you visually impaired, or did you do this out of sheer boredom?",
          "score": 1,
          "created_utc": "2026-02-26 15:57:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcmm3q",
      "title": "I canceled my other AI subscriptions today.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "author": "InitialCareer306",
      "created_utc": "2026-02-23 16:47:15",
      "score": 146,
      "num_comments": 50,
      "upvote_ratio": 0.85,
      "text": "Between the 256k context window and the fact that a 397B open-weight model is available, I just can't justify paying $20/mo anymore. The open-source community won.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6zd3ee",
          "author": "Historical-Internal3",
          "text": "What hardware are you running the model on?",
          "score": 19,
          "created_utc": "2026-02-23 17:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72l8gc",
              "author": "shooshmashta",
              "text": "Lies",
              "score": 6,
              "created_utc": "2026-02-24 03:05:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o743lnf",
                  "author": "doodo477",
                  "text": "tears",
                  "score": 4,
                  "created_utc": "2026-02-24 10:29:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zgvfx",
          "author": "TalosStalioux",
          "text": "I mean, purchasing a $4000 setup (minimum) to run that kind of model size just to save $20 per month is... interesting",
          "score": 38,
          "created_utc": "2026-02-23 17:29:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zyn3n",
              "author": "Justfun1512",
              "text": "Show me 4,000 $ build /machine that can run 300b models",
              "score": 34,
              "created_utc": "2026-02-23 18:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zz0u1",
                  "author": "ImmediateDot853",
                  "text": "I am also interested in that setup.",
                  "score": 15,
                  "created_utc": "2026-02-23 18:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o709glb",
                  "author": "greenthum6",
                  "text": "I have 4K$ 5090 build. What 300B model can I run? I thought 30B models were already on the edge.",
                  "score": 11,
                  "created_utc": "2026-02-23 19:40:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70d1zz",
                  "author": "Hankdabits",
                  "text": "It would have to be hybrid gpu/cpu inference. Probably something like amd epyc, 8 channels of ddr4, and a 3090 to get ~12 t/s and slow prefill",
                  "score": 2,
                  "created_utc": "2026-02-23 19:57:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72id9q",
                  "author": "Straight_Issue279",
                  "text": "With the right build you can make a very good ai using vector memory ect for around 2 to 3k i made an offline ai, my ai can remember content from 4 months ago. Can gather info online, is uncensored, can protect my network by scanning it and be able to create images. Im tired of all the data privacy leaks hence why I will never pay for subscriptions ever. Yeah it may take a full 10 sec to respond but it works for me. I was running dolphin-2.6-mistral-7b.Q6_K.gguf  but I went for a 12b model. Its enough for me.",
                  "score": 2,
                  "created_utc": "2026-02-24 02:49:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75kdrl",
                  "author": "RandomCSThrowaway01",
                  "text": "Q4 is 220GB. Meaning it fits on 256GB Mac Studio which brand new is $5600. Occasionally it does show up in refurbished/sale category however and then it's possible to find it for around $5000. Not 4 but still, if you can afford $4000 then you probably **can** consider $5000. \n\nJust, uh, don't expect it to actually outperform $200 Claude Max and similar solutions in pure LLM output quality. You can run some great stuff on it but prompt processing is slow with huge models in particular. ",
                  "score": 1,
                  "created_utc": "2026-02-24 15:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77nf13",
                  "author": "TheRiddler79",
                  "text": "Mine will\n\nhttps://preview.redd.it/n4x5uchngilg1.jpeg?width=1440&format=pjpg&auto=webp&s=84794eacea54670b9094eff3311622c2e80ffbd4\n\nBut, if I'm being fair, it wouldn't be a whole lot of fun to chat with. What it's really really really really good at doing is I give it a task, and then it crunches away at two tokens a second for 24 hours, and then I end up with an incredible output, a fully built website, a fully built mobile app whatever. But it's not fun to chat with cuz it's too slow so you just have to give it a task and go away.\n\nThe relevance here is that I'm running on a system I built for $1,500 Plus a single gun trade for Ram.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:31:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7embxq",
                  "author": "Confusion_Senior",
                  "text": "Used M2 Ultra 192gb Mac studio with Q4 model\n\nOr just used 192gb ram with used  3090 as well",
                  "score": 1,
                  "created_utc": "2026-02-25 21:57:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zwuro",
              "author": "illathon",
              "text": "20 dollars a month with massive restrictions or your own hardware running 24/7 and you can constantly upgrade as new models come out.",
              "score": 7,
              "created_utc": "2026-02-23 18:42:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71kf3s",
                  "author": "greenthum6",
                  "text": "Commercial models are updated as well. You can inference with Opus for 20$ a month. Once you get to that level with local model the paid ones are again much better.",
                  "score": 2,
                  "created_utc": "2026-02-23 23:35:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zjogg",
              "author": "upinthisjoynt",
              "text": "I'd say it's worth it if it's used for more than just code.  Since it's local and more \"secure\", I don't see why it can't be used for most everyday AI uses (except with old training data).  Just an option",
              "score": 7,
              "created_utc": "2026-02-23 17:42:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71a3ps",
              "author": "BargeCptn",
              "text": "I don't see how it's even remotely possible to have anything usable on the $4000 setup, more like $14k to $18k maybe. You're still going to be basically at 20-30 tokens per second, with a 10 to 15 second first response on the context windows that are larger than 12ktokens. It's wishful thinking, bro. Full models really take a chug out of hardware. It's all about unified memory space; that's all it is. It's not how many video cards you got.",
              "score": 2,
              "created_utc": "2026-02-23 22:39:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73v62i",
              "author": "trackktor",
              "text": "You can resell. The money doesn‚Äôt vanish.",
              "score": 1,
              "created_utc": "2026-02-24 09:09:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o76jvj9",
              "author": "dabiggmoe2",
              "text": "I mean buying a $4000 setup for that alone might be too much depending on the case. But this can also double a gaming PC or a dev machine. For the purpose to just vibe code it might not be the most effective choice. But for learning, I would say it might be worth it if one can afford it.",
              "score": 1,
              "created_utc": "2026-02-24 18:30:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o77es81",
              "author": "elaboratedSalad",
              "text": "please show me how I can run that for $4k. i'm serious. I'd jump straight in.",
              "score": 1,
              "created_utc": "2026-02-24 20:52:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o791lw1",
              "author": "goodssh",
              "text": "You probably meant to say $40,000 üòÇ. Even with that, you would probably have to constantly maintain the infra, both in software and hardware. \n\nThe open-weight model itself doesn't generate a great response without manual labor work - it's extremely difficult to micro control the model to deliver the desired result. \n\nIn my view with 20 bucks per month they give a super generous quota.",
              "score": 1,
              "created_utc": "2026-02-25 01:55:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7axac9",
              "author": "thaddeusk",
              "text": "Hey, I just managed to load it on my $2000 setup and I'm getting 16t/s. With a 1bit quant... I imagine I'd be better off loading the 122b model at 4bit, though. It's a bit faster and probably would end up being higher quality output, anyway.",
              "score": 1,
              "created_utc": "2026-02-25 10:23:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o708ehr",
          "author": "the-average-giovanni",
          "text": "* my other ai subscription\n\n\nNot this one, folks",
          "score": 4,
          "created_utc": "2026-02-23 19:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70oqj3",
          "author": "Hector_Rvkp",
          "text": "This is such a stupid post, it must be trolling. The ram requirement is insane and people who do proper coding work aren't running 20$ subs, they're using CLI tools with either tokens or much higher subs. \nThis makes no sense",
          "score": 3,
          "created_utc": "2026-02-23 20:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi1v2",
          "author": "JahJedi",
          "text": "Open 8s more than to just save 20$, first of all its privacy and it will not report you or collect any data.",
          "score": 2,
          "created_utc": "2026-02-23 17:34:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi4g3",
          "author": "JahJedi",
          "text": "On what setup you planing to run it?",
          "score": 2,
          "created_utc": "2026-02-23 17:34:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dk8o",
          "author": "pl201",
          "text": "Not really. To run 397B open-weight mode with acceptable speed, you need at least investing the in $10k range to get an acceptable performance. If you run it 24/7, your energy bill will be more than 5x $20. Plus, I yet to see open model‚Äôs ability working on a large code base. You will get something but will spend more time to fix things that are not working. Time is money too.",
          "score": 4,
          "created_utc": "2026-02-23 19:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74s74k",
          "author": "EzioO14",
          "text": "There is no economically viable alternative to using closed source models like claude, OpenAI etc‚Ä¶ if you want the same performance",
          "score": 1,
          "created_utc": "2026-02-24 13:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75mb61",
          "author": "mayday30",
          "text": "You will likely pay more than $20 in electric costs.",
          "score": 1,
          "created_utc": "2026-02-24 15:59:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75oi9s",
          "author": "jodykpw",
          "text": "I can‚Äôt afford the hardware tho.",
          "score": 1,
          "created_utc": "2026-02-24 16:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o766b3o",
          "author": "johnmclaren2",
          "text": "This is just a karma farming post or a promo for a Qwen model. :)",
          "score": 1,
          "created_utc": "2026-02-24 17:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76ffzm",
          "author": "pppreddit",
          "text": "You are forgetting that local llm servers mostly don't have prompt caching and are not suitable for coding or long exchange, they are painfully slow with long context (like minutes to get a response) . It's not enough to have big enough vram, you need proper context caching implementation and AFAIK there are only commercial solutions that support it, no open source yet. Correct me if I am wrong, because things develop really fast and it hard to catch up with everything",
          "score": 1,
          "created_utc": "2026-02-24 18:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77jiq8",
          "author": "DonkeyBonked",
          "text": "My $3.5 to $4k~ is build with 4x RTX 3090 can't even run a 300B+ model, so I mean if a $20/month sub could do what I need, that would be the cheapest way for me.",
          "score": 1,
          "created_utc": "2026-02-24 21:13:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77rwyw",
          "author": "momontology",
          "text": "What type of hardware would be needed to run this?",
          "score": 1,
          "created_utc": "2026-02-24 21:52:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77ygmi",
          "author": "Ashthot",
          "text": "What is your setup ?",
          "score": 1,
          "created_utc": "2026-02-24 22:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zptd2",
          "author": "el-rey-del-estiercol",
          "text": "Pagar por ejecutar buenos modelos es interesante siempre que tu trabajo dependa de ello y ganes dinero con ello tambien",
          "score": 0,
          "created_utc": "2026-02-23 18:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70r8b5",
          "author": "NearbyBig3383",
          "text": "Gente avandoney a chutes para ir pra o Code plan do qwen. E s√©rio n√£o me arrependo",
          "score": 0,
          "created_utc": "2026-02-23 21:05:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rac9k0",
      "title": "I managed to run Qwen 3.5 on four DGX Sparks",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/ifu522zupqkg1.jpeg",
      "author": "Icy_Programmer7186",
      "created_utc": "2026-02-21 00:15:27",
      "score": 114,
      "num_comments": 38,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rac9k0/i_managed_to_run_qwen_35_on_four_dgx_sparks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ke7z8",
          "author": "ciprianveg",
          "text": "How is the prompt processing speed?",
          "score": 4,
          "created_utc": "2026-02-21 07:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6klp89",
              "author": "Icy_Programmer7186",
              "text": "(APIServer pid=1108) INFO 02-21 08:34:45 \\[loggers.py:259\\] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%\n\n23.2 tokens/s ... it is definitively productive speed.",
              "score": 1,
              "created_utc": "2026-02-21 08:35:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lnuka",
                  "author": "RG_Fusion",
                  "text": "Are you sure this is the prefill speed and not sequential generation?\n\n\n23 tokens per second of prefill means it would take almost 5 minutes to begin generating tokens when given a 5k token prompt.",
                  "score": 1,
                  "created_utc": "2026-02-21 14:01:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l1krb",
          "author": "Glittering-Call8746",
          "text": "How u connecting 4 dgx ?",
          "score": 3,
          "created_utc": "2026-02-21 11:11:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lnz05",
              "author": "Icy_Programmer7186",
              "text": "I use this: https://mikrotik.com/product/crs804_ddq\n\nYou definitively don't need full 200G on each spark port, so this switch can support larger clusters thru split cables easily.",
              "score": 2,
              "created_utc": "2026-02-21 14:02:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kh5am",
          "author": "koushd",
          "text": "Using vllm or sglang?",
          "score": 2,
          "created_utc": "2026-02-21 07:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kllwm",
              "author": "Icy_Programmer7186",
              "text": "I use vllm:\n\n$ docker exec vllm vllm --version\n\n0.16.0rc2.dev344+gea5f903f8.d20260220.cu131",
              "score": 1,
              "created_utc": "2026-02-21 08:34:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m4nb3",
          "author": "ciprianveg",
          "text": "fp4 would be at 40t/s probably, very good.",
          "score": 2,
          "created_utc": "2026-02-21 15:36:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mr6qs",
              "author": "Icy_Programmer7186",
              "text": "Im going to test nvpf4 soon.",
              "score": 2,
              "created_utc": "2026-02-21 17:29:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6o9r8h",
          "author": "revilo-1988",
          "text": "Was hat der Spa√ü gekostet und wann rechnest du das sich das refinanziert",
          "score": 2,
          "created_utc": "2026-02-21 22:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oe1di",
              "author": "apVoyocpt",
              "text": "About 16k?",
              "score": 1,
              "created_utc": "2026-02-21 22:33:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p3omo",
                  "author": "Icy_Programmer7186",
                  "text": "Yes. That's about right, including the switch. \nI have quite solid business for this so ROI is in couple of months. It is kind of no-brainer investment in my field ATM",
                  "score": 3,
                  "created_utc": "2026-02-22 01:08:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qgqwm",
          "author": "--dany--",
          "text": "if you‚Äôre using vllm, is the reported 21 tok/s from one user or aggregated from multiple users? You may leverage vllm‚Äôs batching capability to maximize your parallel throughput.",
          "score": 2,
          "created_utc": "2026-02-22 07:07:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r2vh9",
              "author": "Icy_Programmer7186",
              "text": "It is single user scenario.",
              "score": 1,
              "created_utc": "2026-02-22 10:38:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6snrud",
          "author": "Best-Echidna-5883",
          "text": "Interesting.  Your total baseline cost without tax/shipping is around 16K for the Spark's and 1.2K for the switch.  The Mac Studio with 512GB is upwards of 10K without taxes/shipping.  Its power consumption is super low around 90W and your setup gets around 240W per unit under load (searched google) so around 900W.  But I have seen posts by people mentioning around 50W under load which is similar to your post in this thread!  This review says 160W under load.  [Power, efficiency, heat, and noise - Nvidia DGX Spark review: the GB10 Superchip powers a fast and fun AI toolbox that beats out AMD‚Äôs Ryzen AI Max+ 395 - Page 4 | Tom's Hardware](https://www.tomshardware.com/pc-components/gpus/nvidia-dgx-spark-review/4).  Pfft.  The Mac Studio gets comparable tokens/second on that model as well.  25 tokens/second via this source: [Let's Run Qwen-3.5 - Local AI HERO Model for OpenClaw, Writing, Coding & More](https://www.youtube.com/watch?v=tzF8jv3VGAg)",
          "score": 2,
          "created_utc": "2026-02-22 16:32:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6v7j1m",
              "author": "funding__secured",
              "text": "I have a Mac Studio M3 Ultra with 512gb and 8 sparks. The Sparks smoke the Mac Studio. ¬Ø\\_(„ÉÑ)_/¬Ø¬†",
              "score": 2,
              "created_utc": "2026-02-23 00:09:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77pmoq",
                  "author": "apVoyocpt",
                  "text": "how do they compare in t/s and what models do you run?",
                  "score": 1,
                  "created_utc": "2026-02-24 21:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6k9ss8",
          "author": "Glad-Audience9131",
          "text": "how much electricity you consume?",
          "score": 1,
          "created_utc": "2026-02-21 06:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6klilc",
              "author": "Icy_Programmer7186",
              "text": "Based on \\`nvidia-smi\\`, it is 35W-45W on each node, then the prompt is running.  \n",
              "score": 1,
              "created_utc": "2026-02-21 08:33:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6sncyf",
                  "author": "Best-Echidna-5883",
                  "text": "All the data I am looking at mention 240W per unit under load.  Can you post a video because that is a huge difference.",
                  "score": 1,
                  "created_utc": "2026-02-22 16:30:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m7vbs",
          "author": "monty3413",
          "text": "Could you detail your setup? Which Dashboard/ Stack Tool do you use? Cost of the switch?",
          "score": 1,
          "created_utc": "2026-02-21 15:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pci1x",
              "author": "Icy_Programmer7186",
              "text": "Sure.\n\nI use four DXG NVidia Sparks interconnected with 200G DAC cables (where a bit difficult to find in Europe by I managed to do it, back during Xmas).  \nThe switch I use is this: [https://mikrotik.com/product/crs804\\_ddq](https://mikrotik.com/product/crs804_ddq) \\- I had to wait for its release a bit. Price is 1250 EUR, the bargain in this speed category. Also, Spark is not capable of using full 200G bandwidth so I guess with a right splitter cable, this switch will do 8 Sparks easily (AFAIK, not going to test it likely).\n\nI run primarily vllm but also TensorRT-LLM, in the beginning I used Ollama but you cannot make a cluster from it (yet). I run everything within a Docker container, that's my rule.  \n  \nFor vllm & cluster setup, I use [https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker) \\- the repo people/guy respond to changes in vllm very quickly. I used standard build for this but I also tested \\`--pre-tf --pre-flashinfer\\` for pre-release version and it was fine too (and must for some other models).\n\nThe dashboard is mine: [https://github.com/ateska/dgx-spark-prometheus/tree/main](https://github.com/ateska/dgx-spark-prometheus/tree/main)  \n\nI don't have any recent photo but I can post one later.",
              "score": 3,
              "created_utc": "2026-02-22 02:05:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6r2rau",
          "author": "MajinAnix",
          "text": "Why FP8? This decide is dedicated for FP4 (native acceleration)",
          "score": 1,
          "created_utc": "2026-02-22 10:37:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r30ch",
              "author": "Icy_Programmer7186",
              "text": "Yes. It was not ready yet. \nI actually just downloading NVFP4. \nI'm also curious about practically observable loss of precision",
              "score": 2,
              "created_utc": "2026-02-22 10:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6r5mw6",
                  "author": "usefulslug",
                  "text": "Would love to hear the results of that.",
                  "score": 3,
                  "created_utc": "2026-02-22 11:04:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sjl4o",
              "author": "AustinM731",
              "text": "Blackwell also has hardware acceleration for FP8 (Ada generation was the first to get FP8 acceleration). You would get better throughput with FP4, you will get higher accuracy with FP8.",
              "score": 2,
              "created_utc": "2026-02-22 16:13:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kjkfj",
          "author": "maded2",
          "text": "Show off üòÅ, envious",
          "score": 0,
          "created_utc": "2026-02-21 08:14:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rex0vo",
      "title": "Big love to the Qwen üß† A true SOTA Open Source model running locally (Qwen 3.5 35B 4-bit) - Here is the fix for the logic loops! ‚ù§Ô∏è",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rex0vo/big_love_to_the_qwen_a_true_sota_open_source/",
      "author": "SnooWoofers7340",
      "created_utc": "2026-02-26 01:47:35",
      "score": 113,
      "num_comments": 5,
      "upvote_ratio": 0.97,
      "text": "Been hunting for a local daily driver to finally drop paid APIs, and I think I found \"The One.\"\n\nGrabbed the new **Qwen3.5-35B-A3B-4bit**, and at first, it was rough. I was getting those classic 4-bit issues: reasoning loops, endless \"Wait, let me check...\" spirals, and failing simple logic traps like the \"Car Wash\" problem.\n\nSpent the day stress-testing it against the [Digital Spaceport Benchmark suite](https://digitalspaceport.com/about/testing-local-llms/) (logic, math, SVG coding, counting). It was hit-or-miss until I realized the model wasn't \"dumb\" it just needed structure. The quantization makes it anxious, so it second-guesses itself into oblivion.\n\n**The Fix:**\n\nTweak the system prompt to force \"Adaptive Logic.\" It separates the scratchpad thinking from the final answer and sets a hard limit on the \"thinking\" phase.\n\nOnce applied this, it passed **100% of the tests** in seconds.\n\n* **Logic:** Solved the \"Pico de Gato\" schedule puzzle perfectly.\n* **Math:** Nailed the \"Two Drivers\" problem without looping.\n* **Coding:** Generated a clean SVG cat on the first try.\n* **Counting:** Actually counted \"Peppermint\" correctly (rare for 4-bit).\n\nNext up, I'm throwing my complex n8n workflows at it to see if it survives. ü§û\n\nIf you're struggling with the 4-bit version, here is the config that unlocked it for me.\n\n**‚öôÔ∏è The Config**\n\n* **Model:** Qwen3.5-35B-A3B-4bit\n* **Temp:** 0.7 | **Top P:** 0.9 | **Min P:** 0.05 *(Critical!)*\n* **Freq Penalty:** 1.1 | **Repeat Last N:** 64\n\n**üß† The \"Anti-Loop\" System Prompt**\n\n*(Paste into OpenWebUI/LM Studio)*\n\nPlaintext\n\n    You are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops.\n    \n    1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n    2. ADAPTIVE LOGIC:\n       - For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n       - For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n       - For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n    3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n    \n    DO NOT reveal your thinking process outside of the tags.\n\nBig love to the Qwen team. You guys genuinely changed the game with this one. Having this running locally is a dream. üöÄ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rex0vo/big_love_to_the_qwen_a_true_sota_open_source/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7fxc6x",
          "author": "InitialJelly7380",
          "text": "good sharing",
          "score": 2,
          "created_utc": "2026-02-26 02:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g0s7m",
          "author": "pinthead",
          "text": "What is your context length ?",
          "score": 2,
          "created_utc": "2026-02-26 02:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hjyu2",
              "author": "SnooWoofers7340",
              "text": "I'm capped at **28k context** but Qwen 3.5 architecture theoretically supports **1M tokens** using YaRN/RoPE scaling!! insane",
              "score": 2,
              "created_utc": "2026-02-26 09:32:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hp29t",
          "author": "Aware-Adman",
          "text": "Nice",
          "score": 2,
          "created_utc": "2026-02-26 10:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gpe3f",
          "author": "altdotboy",
          "text": "I was using the qwen3 30B A3B for my home rig. It was good. I have to say the new 35B A3B is awesome. I was a bit weary about the model having thinking, was going to turn it off but decided to embrace it. The 35B seems less likely to get stuck in the loops you mentioned. It‚Äôs great having the thinking text to see what the model is considering. I‚Äôm on a MacBook Pro M4 48GB. Context max is set to 128k. \nI might borrow some of your prompt. Good tips.",
          "score": 2,
          "created_utc": "2026-02-26 05:05:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcqezx",
      "title": "Qwen 3.5 for MLX is like its own industrial revolution",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "author": "sovietreckoning",
      "created_utc": "2026-02-23 18:59:54",
      "score": 87,
      "num_comments": 26,
      "upvote_ratio": 0.97,
      "text": "I'm using a 4-bit model on a mac studio m3 and it is mindblowing how fast this thing works for the quality of the results. I love it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o703evr",
          "author": "ossbournemc",
          "text": "How fast is it? I'm considering the same setup. ",
          "score": 5,
          "created_utc": "2026-02-23 19:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7042mt",
              "author": "sovietreckoning",
              "text": "I'm not well-versed in any of this stuff and I'm only doing ametuer stuff for myself, so please forgive me is this answer is unhelpful - a task that took \\~500s+ on llama 3(405b) is closer to \\~30s on qwen 3.5. ",
              "score": 4,
              "created_utc": "2026-02-23 19:15:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o707puy",
                  "author": "ossbournemc",
                  "text": "Thats cool, do you know how many tokens/second you're getting?",
                  "score": 2,
                  "created_utc": "2026-02-23 19:32:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75xuwv",
                  "author": "Hector_Rvkp",
                  "text": "You're looking at dense model vs MoE model. Avoid dense models, they are slower.",
                  "score": 1,
                  "created_utc": "2026-02-24 16:51:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70ybyw",
              "author": "Professional-Bear857",
              "text": "I get 35 tok/s on my 60 GPU core M3 ultra 256gb ram. I'm using the 4bit mlx version as well.",
              "score": 2,
              "created_utc": "2026-02-23 21:40:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c2nht",
                  "author": "zulutune",
                  "text": "Are the results comparable with Claude/Codex? Without taking speed into consideration. I suspect it‚Äôs not but I‚Äôd like to know how big the gap is, how noticeable it is",
                  "score": 1,
                  "created_utc": "2026-02-25 14:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70k51l",
              "author": "alexp702",
              "text": "I am running llama server with the 8bit GGUF. It is excellent. It gets about 250 prompt processing tokens, and about 25tokens out. I‚Äôm openclawing and many prompts are cached so practical upshot is very few tokens either way 114000 token prompt processing only 500. Not sure why I am suddenly getting such good caching (perhaps Llama update) but I like it!\n\nEdit: Openclaw reports I have a throughput of 15.2k tokens a minute probably because of prompt caching. 26.7 million tokens sent. Pretty damn good!\n\nEdit 2: Mac Studio 512gb.",
              "score": 2,
              "created_utc": "2026-02-23 20:31:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c4orc",
                  "author": "sovietreckoning",
                  "text": "Can you tell me more about this. I tried to run the 8-bit mlx version on a Mac Studio m3 512gb and it choked to a crawl.",
                  "score": 1,
                  "created_utc": "2026-02-25 15:05:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70v2xz",
          "author": "jzn21",
          "text": "I am also blown away by this model. The quality is excellent (even in non-thinking mode) and the speed is great with 34 - 35 tokens per second. And then best thing is that prompt processing happens in the blink of an eye.",
          "score": 4,
          "created_utc": "2026-02-23 21:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71js61",
          "author": "CoffeeSnakeAgent",
          "text": "Where to get the qwen 3.5 4-bit? Cant seem to find it in hf",
          "score": 3,
          "created_utc": "2026-02-23 23:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71l37g",
              "author": "sovietreckoning",
              "text": "[https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4](https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4)\n\nThats what I'm using currently.",
              "score": 3,
              "created_utc": "2026-02-23 23:39:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o71oqrx",
                  "author": "Special-Wolverine",
                  "text": "NVFP4 is optimized for Nvidia Blackwell",
                  "score": 1,
                  "created_utc": "2026-02-23 23:59:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o726qhr",
                  "author": "CoffeeSnakeAgent",
                  "text": "Thanks",
                  "score": 1,
                  "created_utc": "2026-02-24 01:41:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o746vum",
          "author": "XxBrando6xX",
          "text": "Does anyone know why the MLX version seems to drop the vision ability ?? I want to run the mlx but I was hoping to do it by letting it review EVERYTHing not just text prompts which I can‚Äôt do on the mlx yet since it isn‚Äôt supported",
          "score": 1,
          "created_utc": "2026-02-24 10:58:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74qfkt",
          "author": "ThankYouOle",
          "text": "i am curious, what is your use case? to be buddy to discuss thing? quick QnA? or code?",
          "score": 1,
          "created_utc": "2026-02-24 13:19:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o751csv",
              "author": "sovietreckoning",
              "text": "It‚Äôs varied because I‚Äôm effectively learning and developing an all-inclusive ai assistant for my personal use. I own a small law firm which involves an enormous amount of admin work and document prep from preexisting forms. I also do some active spread trading from time to time. Basically, I‚Äôm building a legal rag system, a personal assistant, a document automation bot, an options scanner with built in back testing, and a coding machine to grow and improve the product over time. All of the varied functions are wrapped under the LLM for plain language interaction, personal email and calendar integration, court docket scraping, scheduling, alerts, etc. \n\nBasically, I‚Äôm learning on the fly and attempting to clone myself. I don‚Äôt know how any of it will actually work out. With massive context and pipelines to work through, the speed of Qwen 3.5 is so valuable.",
              "score": 3,
              "created_utc": "2026-02-24 14:18:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77nt7o",
                  "author": "ThankYouOle",
                  "text": "interesting that Qwen 3.5 can help you doing all of that. \n\nso far i found local model can't really help me, but then again i never try Qwen, only some small model.\n\nthank you.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:33:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ahzh7",
          "author": "c_glib",
          "text": "Can you please share your workflow? What's the exact model size? Which running (using ollama or something else)? ",
          "score": 1,
          "created_utc": "2026-02-25 08:00:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ebx0b",
          "author": "Gold_Ad_2201",
          "text": "where did you get mlx version? gguf works fine for me but all mlx versions go into repeat almost every time",
          "score": 1,
          "created_utc": "2026-02-25 21:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fyc0r",
          "author": "zerostyle",
          "text": "What woudl be the best model to run on 32gb of ram for qwen 3.5? not seeing too much on hugging face yet. Prefer abliterated/uncensored models.",
          "score": 1,
          "created_utc": "2026-02-26 02:17:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pv5t",
      "title": "Qwen-AI Slides is really slept on! It generates PowerPoint Presentations in minutes",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/co68yech0mkg1.png",
      "author": "Substantial-Cup-9531",
      "created_utc": "2026-02-20 08:26:50",
      "score": 86,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Image Gen",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9pv5t/qwenai_slides_is_really_slept_on_it_generates/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6edie1",
          "author": "Ok_Recording8157",
          "text": "No funciona bien en otros idiomas, sirve para ingl√©s y chino.",
          "score": 3,
          "created_utc": "2026-02-20 10:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6emstu",
          "author": "Frosty_Medicine9134",
          "text": "The Sphynx was defaced. Originally a lion face.",
          "score": 3,
          "created_utc": "2026-02-20 11:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ezbe6",
              "author": "Substantial-Cup-9531",
              "text": "Yea know, did adress it in the video that the model generates so its not factual correct",
              "score": 0,
              "created_utc": "2026-02-20 13:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h7sdt",
          "author": "Upstairs_Ad_9919",
          "text": "I honestly prefer Kimi Slides, which uses Nano Banana Pro, and it's mind-blowingly good. But their servers have been overwhelmed since January when they got a lot of new users, so slides rarely work now. I hope they fix that.",
          "score": 1,
          "created_utc": "2026-02-20 19:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1a62",
              "author": "Substantial-Cup-9531",
              "text": "if they release the qwen image 2.0 then with some loras, one should be able to run this locally with high quality outputs",
              "score": 2,
              "created_utc": "2026-02-21 05:28:30",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6j8l7d",
              "author": "darkninjademon",
              "text": "U can just use Google slides with appscript and use the same model from there .....",
              "score": 1,
              "created_utc": "2026-02-21 02:10:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6npcpz",
          "author": "ciprianveg",
          "text": "can it be used locally?",
          "score": 1,
          "created_utc": "2026-02-21 20:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6q5aur",
              "author": "Substantial-Cup-9531",
              "text": "not yet! but if and when they release qwen image 2.0 then yes",
              "score": 2,
              "created_utc": "2026-02-22 05:27:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra3mod",
      "title": "Qwen 3 ‚Üí Qwen 3.5: the agentic evolution measured in dollars (FoodTruck Bench case study)",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/7ffdpbn42pkg1.png",
      "author": "Disastrous_Theme5906",
      "created_utc": "2026-02-20 18:39:57",
      "score": 47,
      "num_comments": 8,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Experiment",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1ra3mod/qwen_3_qwen_35_the_agentic_evolution_measured_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6h4eoh",
          "author": "SillyLilBear",
          "text": "I'd love to see this for 1000 runs, to see how consistent it is.\n\n",
          "score": 1,
          "created_utc": "2026-02-20 19:18:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h6d6g",
              "author": "Disastrous_Theme5906",
              "text": "Each Qwen 3.5 run costs $3-5 in API calls (hundreds of tool-calling turns over 25 days). 1,000 runs = $3-5K for a single model, and I benchmark 14+ models. For reference, most agentic benchmarks use similar sample sizes: SWE-bench (pass@5), œÑ-bench (5 runs), Vending Bench 2 (average of 5). 5 runs already show strong consistency: 4/5 bankrupt, staff costs 59-106% of revenue in every run, same overstaffing pattern across all 5. When all runs fail the same structural way, the signal is clear.",
              "score": 3,
              "created_utc": "2026-02-20 19:28:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6h6rsq",
                  "author": "SillyLilBear",
                  "text": "Yeah that sounds unsustainable for a large run, but would be nice to get some funding to run it more runs.  A handful of runs, to me at least, is just noise.",
                  "score": 1,
                  "created_utc": "2026-02-20 19:30:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ljrc0",
          "author": "masterlafontaine",
          "text": "It feels that Opus, 5.2 and 3.1 probably benchmaxxed.",
          "score": 1,
          "created_utc": "2026-02-21 13:36:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbvakh",
      "title": "Qwen3.5 Real-World Experiences: What are you actually using it for? Let's hear the good, the bad, and the hallucinations",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "author": "road_changer0_7",
      "created_utc": "2026-02-22 19:39:08",
      "score": 41,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "The hype around Qwen3.5 has been massive over the last few days. We've all seen the benchmark charts and the official demos about the Native VLM , the 17B active parameters , and the 256k context window. But benchmarks are just numbers. I want to know how it's holding up in your actual, messy, day-to-day workflows.\n\nShare your prompts, your screenshots, and your honest reviews. Let's figure out where this model truly shines!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6uetwo",
          "author": "lundrog",
          "text": "I just started using it with a provider yesterday. Using it in claude code. Vibe coding",
          "score": 4,
          "created_utc": "2026-02-22 21:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v2aa0",
          "author": "FormalAd7367",
          "text": "Not a heavy user.  I am on claude and gemini.  But very surprised to see how capable Qwen is.  Had some trouble with some scripts and my engineers were on leave.  Had tried almost everything.  Qwen solved the issues. Qwen is quite underrated in this space?  I don‚Äôt come here often but in other subs almost nobody mentions how this free software can perform so well.  Most prefer to use the expensive models.",
          "score": 3,
          "created_utc": "2026-02-22 23:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v3rgp",
          "author": "pakalolo7123432",
          "text": "I'm a fan,  \n  \nThe Qwen App on my Mac is my daily chat app driver. It‚Äôs connected to my knowledge graph, helping me with project management and life connections. The project feature allows uploading documents to a knowledge base for context.\n\nI signed up for their coding plan to see if Qwen can replace GLM and its slowness. I'm crossing my fingers.   \n  \n",
          "score": 3,
          "created_utc": "2026-02-22 23:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x0i5b",
          "author": "alexp702",
          "text": "Running the 8bit quant on a Mac Studio. Extremely happy. The vision part passed all our tests and has replaced 235b (previous winner). Tool calling has many times less failures than the 4 bit GLM and mini max in our test suite. Now hooked up to openclaw. Seems to be chugging along nicely on every task. Quite a lot of emojis in the responses!\n\n17b active means it‚Äôs pretty quick - though cos we now run 8 bit only fractionally quicker than qwen3 480b. I‚Äôd say it was a perfect memory fit to fully utilise a 512GB Mac, which feels good not to to waste resources!",
          "score": 3,
          "created_utc": "2026-02-23 07:40:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o777gqc",
              "author": "sovietreckoning",
              "text": "I'm downloading the 8-bit now. I'm moving up from a 4-bit and really excited to test it out. ",
              "score": 1,
              "created_utc": "2026-02-24 20:17:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uuw4c",
          "author": "jzn21",
          "text": "I am really happy with the Qwen 397b 4-bit MLX. I use it with thinking mode off and it‚Äôs quite smart. Does very complicated sorting of numbers and text with ease. I believe this model is as good as Deepseek V3.2, but almost two times faster.",
          "score": 4,
          "created_utc": "2026-02-22 22:57:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vt3bp",
              "author": "Gold_Scholar1111",
              "text": "I also agree that it is very strong without thinking.\nCould you share how you use it in details? Which 4bit mlx you downloaded? What platform you serving it? I have to use Q4 version because I found mlx model was unstable on my Lmstudio.",
              "score": 1,
              "created_utc": "2026-02-23 02:17:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6x1oib",
              "author": "StartCodeEmAdagio",
              "text": "Can I see a full example? prompt message and result?",
              "score": 1,
              "created_utc": "2026-02-23 07:51:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vq7b2",
          "author": "LinkAmbitious8931",
          "text": "This must take some pretty serious hardware. What are you all using?",
          "score": 1,
          "created_utc": "2026-02-23 01:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x72nm",
              "author": "RG_Fusion",
              "text": "I run Q4KM Qwen3-235b-a22b which isn't the new 3.5 model, but the old one actually requires more compute.\n\n\nI'm on an AMD EPYC 7742 server with 512 GB of 8-channel DDR4 3,200 MT/s RAM, and I also load the \"hot\" portions of the model onto an RTX Pro 4500 Blackwell.\n\n\nI get 13 tokens/s of decode speed on Qwen3, and I would expect to get 17 t/s on Qwen3.5 based on how the math works out, though I haven't run it yet.",
              "score": 2,
              "created_utc": "2026-02-23 08:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xy6lr",
                  "author": "LinkAmbitious8931",
                  "text": "Wow, that is some pretty nice hardware. I am hacking away on an old Dell 720R with two nVidia P40S, each with 24 GB. I had to attach them externally as they heat up too much in the case. So there is no way I could use this model. ",
                  "score": 2,
                  "created_utc": "2026-02-23 12:45:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x1pds",
              "author": "StartCodeEmAdagio",
              "text": "Interested to know aswell",
              "score": 1,
              "created_utc": "2026-02-23 07:51:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z2xo8",
          "author": "absolutenobody",
          "text": "Prompted it this morning for a pretty simple node.js script--200 or so lines of code, including comments. It just does some math, albeit somewhat complicated.\n\nThe first six iterations were built around some sort of math function that Qwen either hallucinated or is in a library it forgot to include. When I told it (as I normally would with, e.g. Gemini) \"it fails with an error on line 49\" it got weirdly defensive and said I need to upgrade to a modern operating system that supports bitwise operators... completely missing the real problem entirely. Eventually on the seventh try it was like \"I'm sorry, CalculateParallelogramArea() doesn't exist, I hallucinated that. Here's a corrected script that will actually work.\" And in its defense, it did.",
          "score": 1,
          "created_utc": "2026-02-23 16:24:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75awee",
          "author": "ciprianveg",
          "text": "maybe lower the temp a bit and redo the test?",
          "score": 1,
          "created_utc": "2026-02-24 15:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o771hi3",
          "author": "MinosAristos",
          "text": "I've been really impressed by its ability to quickly research niche subjects without hallucinating. I compared it to Gemini and Gemini absolutely sucks at this unless you use deep research. Qwen takes its time to actually read and synthesise the online sources in a decent amount of time, not the ages that Gemini deep research takes.",
          "score": 1,
          "created_utc": "2026-02-24 19:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7769c1",
          "author": "Inevitable_Cost6132",
          "text": "I have been using it (via openrouter) for translation work, from English to Czech. Its the best open weights model right now. I am hoping the smaller ones could do the same job, but Kimi was the second best. Czech is hard when you dont have much context to work with!\n\n",
          "score": 1,
          "created_utc": "2026-02-24 20:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hk57p",
          "author": "ayoooAnnie",
          "text": "ÊâìÂπøÂëäÊâìÂà∞ËøôÈáåÊù•‰∫ÜÈòøÈáåü§ó",
          "score": 1,
          "created_utc": "2026-02-26 09:34:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb0o5m",
      "title": "Will we be getting smaller Qwen3.5 models starting from tomorrow?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "author": "Significant_Fig_7581",
      "created_utc": "2026-02-21 19:50:49",
      "score": 30,
      "num_comments": 18,
      "upvote_ratio": 0.86,
      "text": "I've heard they plan to release smaller models each day before 24feb I think, is that true? And if so is there any sign for a new Qwen tomorrow?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6nlwiv",
          "author": "Packetbytes",
          "text": "I want a 14B to test at home",
          "score": 3,
          "created_utc": "2026-02-21 20:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6os8se",
              "author": "Iq1pl",
              "text": "14B2A would be awesome",
              "score": 2,
              "created_utc": "2026-02-21 23:57:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmqal",
          "author": "Samy_Horny",
          "text": "I doubt it; new model releases on weekends are extremely rare... I only remember the Llama 4 being released on a weekend, and that model was a disappointment as well.",
          "score": 3,
          "created_utc": "2026-02-21 20:07:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nod5x",
              "author": "Significant_Fig_7581",
              "text": "Maybe this time it's different?",
              "score": 1,
              "created_utc": "2026-02-21 20:16:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6novvo",
                  "author": "Samy_Horny",
                  "text": "I think it will be until Monday and all next week.",
                  "score": 2,
                  "created_utc": "2026-02-21 20:19:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pyt85",
          "author": "MetalZone00",
          "text": "Algo para correr con solo 16GB? üôèüèª",
          "score": 3,
          "created_utc": "2026-02-22 04:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nsvq5",
          "author": "hocuspocus4201",
          "text": "The Chinese New Year is going to delay it until early March.",
          "score": 2,
          "created_utc": "2026-02-21 20:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ntruo",
              "author": "Significant_Fig_7581",
              "text": "But aren't the holidays ending on 23 Feb?",
              "score": 1,
              "created_utc": "2026-02-21 20:45:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6o9flo",
                  "author": "stormy1one",
                  "text": "Many people in China and surrounding areas take extra time off during CNY.   I wouldn‚Äôt expect anything until March, but would be pleasantly surprised if we got something this coming week.",
                  "score": 5,
                  "created_utc": "2026-02-21 22:08:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oa6g0",
          "author": "revilo-1988",
          "text": "W√§re toll wenn das stimmen w√ºrde",
          "score": 2,
          "created_utc": "2026-02-21 22:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oagp2",
          "author": "AppealThink1733",
          "text": "That's what I hope!\n\nYayyyyyyy",
          "score": 2,
          "created_utc": "2026-02-21 22:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t37xl",
          "author": "mshelbz",
          "text": "Been looking for something to replace qwen2.5-coder:14b Q4_K_M\n\nIf a 14b were to drop I‚Äôd love to try it.",
          "score": 2,
          "created_utc": "2026-02-22 17:43:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t3i61",
              "author": "Significant_Fig_7581",
              "text": "Well I think a 9B is confirmed...",
              "score": 1,
              "created_utc": "2026-02-22 17:44:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t3tb0",
                  "author": "mshelbz",
                  "text": "Oh awesome. Hope they come out soon",
                  "score": 2,
                  "created_utc": "2026-02-22 17:45:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nxgsg",
          "author": "el-rey-del-estiercol",
          "text": "Qwen son amigos mios!!!!",
          "score": 1,
          "created_utc": "2026-02-21 21:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ok1zp",
          "author": "Armadilla-Brufolosa",
          "text": "Mi piacerebbe tanto un bel modello 20 o 30B da provare! Magari con capacit√† di ragionamento e visione anche... \nSono troppo pretenziosa? ü´£",
          "score": 1,
          "created_utc": "2026-02-21 23:08:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r3nsh",
          "author": "East-Form7086",
          "text": "Want a 30b one <3",
          "score": 1,
          "created_utc": "2026-02-22 10:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wdbe7",
          "author": "el-rey-del-estiercol",
          "text": "Mejor modelos 80B o 100B modelos medianos y capaces de realizar tareas exigentes",
          "score": 1,
          "created_utc": "2026-02-23 04:27:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdhy4i",
      "title": "My wish has been fulfilled",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/gallery/1rdhy4i",
      "author": "celsowm",
      "created_utc": "2026-02-24 14:38:55",
      "score": 30,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdhy4i/my_wish_has_been_fulfilled/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o76lb6m",
          "author": "Samy_Horny",
          "text": "There's still a small batch of models missing, approximately less than 10b, but who knows if they'll release them this week, hopefully they will.",
          "score": 5,
          "created_utc": "2026-02-24 18:36:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77pngw",
              "author": "AbheekG",
              "text": "Desperately waiting for the 2B",
              "score": 1,
              "created_utc": "2026-02-24 21:41:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1re8tje",
      "title": "Qwen3.5-122B-A10B vs. old Coder-Next-80B: Both at NVFP4 on DGX Spark ‚Äì worth the upgrade?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1re8tje/qwen35122ba10b_vs_old_codernext80b_both_at_nvfp4/",
      "author": "alfons_fhl",
      "created_utc": "2026-02-25 09:37:10",
      "score": 26,
      "num_comments": 18,
      "upvote_ratio": 0.87,
      "text": "Running a¬†**DGX Spark (128GB)**¬†. Currently on¬†**Qwen3-Coder-Next-80B (NVFP4)**¬†. Wondering if the new¬†**Qwen3.5-122B-A10B**¬†is actually a flagship replacement or just sidegrade.\n\n**NVFP4 comparison:**\n\n* **Coder-Next-80B**¬†at NVFP4: \\~40GB\n* **122B-A10B**¬†at NVFP4: \\~61GB\n* Both fit comfortably in 128GB with 256k+ context headroom\n\n**Official SWE-Bench Verified:**\n\n* 122B-A10B:¬†**72.0**\n* Coder-Next-80B:¬†**\\~70**¬†(with agent framework)\n* 27B dense:¬†**72.4**¬†(weird flex but ok)\n\n**The real question:**\n\n* Is the 122B actually a¬†**new flagship**¬†or just more params for similar coding performance?\n* Coder-Next was specialized for coding. New 122B seems more \"general agent\" focused.\n* Does the¬†**10B active params**¬†(vs. 3B active on Coder-Next) help with¬†**complex multi-file reasoning**¬†at 256k context or more?\n\n**What I need to know:**\n\n* Anyone done¬†**side-by-side NVFP4**¬†tests on real codebases?\n* **Long context retrieval**¬†‚Äì does 122B handle 256k better than Coder-Next or larger context?\n* **LiveCodeBench/BigCodeBench**¬†numbers for both?\n\nOld Coder-Next was the coding king. New 122B has better paper numbers but barely. Need real NVFP4 comparisons before I download another 60GB.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1re8tje/qwen35122ba10b_vs_old_codernext80b_both_at_nvfp4/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7b9x4n",
          "author": "flavio_geo",
          "text": "I have been using qwen3-coder-next since launch (tested quants Q3KXL to Q8, chose to stay with Q6KXL) and yesterday I tested the 122b, the thing is I only tested it in Q4KXL and since the model has just launched it is possible that its output is not yet fully operational in the current llama.cpp engine (same thing happened with qwen3-coder-next at launch)\n\nSo, given all this considerations, at this moment, the 122b is falling behind big time in coding against the qwen3-coder-next for the coding tests I have done.\n\nOne very simple example is a test I do asking them to create a pygame of chrome dinosaur, and 122b made multiple mistakes before getting 100% working game, while qwen3-coder-next even in Q3 was one shoting it with good quality SVG, collision, birds, cactus, etc. The Q6 never missed it in one-shot and makes very good quality SVG and game dynamics. The 122b after a number of attempts created a 'ugly' version of the game.\n\nAnyhow, the most likely answer to your question at this point is: it is too soon to know which one will perform better.\n\nNot to mention that the two models have different purpose, so ideally you should leverage each one in its better performing scenario.",
          "score": 5,
          "created_utc": "2026-02-25 12:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b3y4q",
          "author": "bacocololo",
          "text": "Where do you see any qwen3.5 in nvfp4 please",
          "score": 4,
          "created_utc": "2026-02-25 11:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b6xy0",
              "author": "alfons_fhl",
              "text": "NVIDIA with TensorRT you can do it by self",
              "score": 2,
              "created_utc": "2026-02-25 11:46:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7dqeim",
                  "author": "Purple-Programmer-7",
                  "text": "What datasets are you using?",
                  "score": 1,
                  "created_utc": "2026-02-25 19:28:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dnn0a",
          "author": "qubridInc",
          "text": "122B-A10B gives better multi-file reasoning and long-context handling, but raw coding gains over Coder-Next-80B are small in practice.\n\nIf your workload is mostly coding, Coder-Next is still very competitive. If you want stronger agentic + general reasoning, 122B is worth trying.",
          "score": 3,
          "created_utc": "2026-02-25 19:15:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e3xmd",
              "author": "alfons_fhl",
              "text": "Understand you fully, you tested booth?",
              "score": 1,
              "created_utc": "2026-02-25 20:32:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cb3g7",
          "author": "Porespellar",
          "text": "Do you think vision understanding is factor in any of these numbers? Coder next doesn‚Äôt have vision, right?",
          "score": 1,
          "created_utc": "2026-02-25 15:35:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7goaaa",
              "author": "alfons_fhl",
              "text": "In my use case no. But you‚Äòre right, I fully forgot this aspect",
              "score": 1,
              "created_utc": "2026-02-26 04:57:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7dchqj",
          "author": "klop2031",
          "text": "I also found the larger model to perform poorly. Even compared to the 27b dense. It could be my task or my quantization but i was surprised to see it kinda suck for now. Im hoping there will be some optimizations in lamma.cpp",
          "score": 1,
          "created_utc": "2026-02-25 18:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f18kk",
          "author": "jinnyjuice",
          "text": "The 3.5 122B is multimodal.",
          "score": 1,
          "created_utc": "2026-02-25 23:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f7xt0",
          "author": "jinnyjuice",
          "text": ">Both fit comfortably in 128GB with 256k+ context headroom\n\nHow do you calculate the memory needed for the number of context tokens?",
          "score": 1,
          "created_utc": "2026-02-25 23:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gouks",
              "author": "alfons_fhl",
              "text": "Ask ai and don‚Äôt forget your quantisation. I mostly use this repo from the YouTuber Alex Ziskind: https://github.com/alexziskind1/llm-inference-calculator",
              "score": 1,
              "created_utc": "2026-02-26 05:01:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fk9rv",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-26 00:57:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7goz46",
              "author": "alfons_fhl",
              "text": "What do you mean with the looping Problem?",
              "score": 1,
              "created_utc": "2026-02-26 05:02:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7j17ts",
          "author": "Total_Activity_7550",
          "text": "Agent framework does so much! If Qwen3-Coder-Next is worse with agent framework, then adding agent framework to Qwen3-122B-A10B will blow Qwen3-Coder-Next out of the water.\n\nI replaced Qwen3-Coder-Next with Qwen3.5-27B, despite being slower, I guess, Qwen3-122B-A10B should be a little better and at least twice as fast, compared to Qwen3.5-27B, although slower than Coder. Still, I think it is better to wait and get one-shot success, rather than fix code and repeatedly prompt the agent.",
          "score": 1,
          "created_utc": "2026-02-26 15:27:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7avwfy",
          "author": "SkullEnemyX-Z",
          "text": "Can someone create an agentic version under 4gb? Would be dream come true for openclaw",
          "score": 0,
          "created_utc": "2026-02-25 10:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7axiv5",
              "author": "Fast_Thing_7949",
              "text": "you can try Nanbeige4.1-3B",
              "score": 2,
              "created_utc": "2026-02-25 10:26:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7cg5bl",
                  "author": "akumaburn",
                  "text": "That's a thinking model, best to wait for the instruct version or use: [https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill](https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill)",
                  "score": 1,
                  "created_utc": "2026-02-25 15:58:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cg547",
              "author": "akumaburn",
              "text": "Not quite under 4GB but close Q8\\_0 of [https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill-GGUF](https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill-GGUF) ",
              "score": 1,
              "created_utc": "2026-02-25 15:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9molz",
      "title": "Qwen is the winner, gpt sucks",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r9molz/qwen_is_the_winner_gpt_sucks/",
      "author": "WritingVast9815",
      "created_utc": "2026-02-20 05:19:30",
      "score": 23,
      "num_comments": 15,
      "upvote_ratio": 0.87,
      "text": "Q: what is the latest version of antigravity ?\n\nAnswers : 1.18.3 - suppose to find out according to me, me being a developer info is given to them.\n\nQwen (winner): https://chat.qwen.ai/s/b7a08e6d-59a8-44b6-86b7-599d56077916?fev=0.2.7\n\ndeepseek : https://chat.deepseek.com/share/a3e1dfdraj5leksmwr\n\nchatgpt : (me a model 5.2 pro user) - the worst : https://chatgpt.com/share/6997ed0c-0cec-800b-9610-25d8b8cc2dbe",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9molz/qwen_is_the_winner_gpt_sucks/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6dh81z",
          "author": "WritingVast9815",
          "text": "in terms of latest data research, like ai auto trading and etc, news trading, I am not believing in any other platform !",
          "score": 2,
          "created_utc": "2026-02-20 05:29:10",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6dhaoz",
          "author": "WritingVast9815",
          "text": "should we test the gemeni ? :)",
          "score": 1,
          "created_utc": "2026-02-20 05:29:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6dpy8t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-20 06:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dq3lc",
              "author": "Puzzleheaded-Box2913",
              "text": "Google and Microsoft has become so meh I don't even wanna use em anymoreüò©",
              "score": 1,
              "created_utc": "2026-02-20 06:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dq7kb",
                  "author": "Puzzleheaded-Box2913",
                  "text": "Google AI Ultra-Bloat and Restrictions vs Microslop HAHAHAHA",
                  "score": 1,
                  "created_utc": "2026-02-20 06:46:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ju4yk",
                  "author": "WritingVast9815",
                  "text": "well yep, but i like google, as they give free ai in antigravity, and the gemeini are pretty good for my front end tasks, i only do backend now.",
                  "score": 1,
                  "created_utc": "2026-02-21 04:34:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ju0op",
              "author": "WritingVast9815",
              "text": "they had the search tool though !, its not like i am using api, its on there own platform...",
              "score": 1,
              "created_utc": "2026-02-21 04:33:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6egcri",
          "author": "ischanitee",
          "text": "Been getting a good results when never I use Yelp, and Gemini tho",
          "score": 1,
          "created_utc": "2026-02-20 10:49:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elh9q",
          "author": "medazizln",
          "text": "https://g.co/gemini/share/b3628750c2dc\nGemini",
          "score": 1,
          "created_utc": "2026-02-20 11:32:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jtknt",
              "author": "WritingVast9815",
              "text": "noice",
              "score": 1,
              "created_utc": "2026-02-21 04:29:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f5sxv",
          "author": "One_Internal_6567",
          "text": "Why exactly would you use non-thinking gpt, while other models are in thinking mode?",
          "score": 1,
          "created_utc": "2026-02-20 13:44:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jti4n",
              "author": "WritingVast9815",
              "text": "well a valid point, now i look at it its as you said, well it happened because i didnt configure anything, these are the default configurations, i only copied and pasted the question, not my fault :)",
              "score": 1,
              "created_utc": "2026-02-21 04:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fnbmv",
          "author": "Crafty_Ball_8285",
          "text": "5.2 is old use 5.3",
          "score": 1,
          "created_utc": "2026-02-20 15:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jtp1b",
              "author": "WritingVast9815",
              "text": "https://preview.redd.it/pmbk6exrzrkg1.png?width=382&format=png&auto=webp&s=13e911886ac62a8648f02106162545ec094b5bab\n\nnot available in my go plan !",
              "score": 1,
              "created_utc": "2026-02-21 04:30:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6k2rr6",
                  "author": "Crafty_Ball_8285",
                  "text": "Codex",
                  "score": 1,
                  "created_utc": "2026-02-21 05:40:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6jtraa",
              "author": "WritingVast9815",
              "text": "but still others were totally free, and this was paid !",
              "score": 1,
              "created_utc": "2026-02-21 04:31:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdvc8a",
      "title": "Qwen lanza nuevos modelos peque√±os 3.5",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rdvc8a/qwen_lanza_nuevos_modelos_peque√±os_35/",
      "author": "el-rey-del-estiercol",
      "created_utc": "2026-02-24 22:42:13",
      "score": 16,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "Qwen acaba de lanzar nuevos modelos peque√±os de la version 3.5 GRACIAS QWEN!!! TODOS DEBEMOS APOYA A QWEN LA MEJOR EMPRESA DEL MUNDO ENTERO!!!!!!!\n\nLos modelos los podeis ver en la pagina de unsloth o en hugging face.\n\nAYUDAR A QWEN ELLOS NOS AYUDAN A NOSOTROS!!!\n\nLos mejores modelos medianos para hardware de consumo!!! QWEN TODOS ESTAMOS CON VOSOTROS Y OS APOYAMOS VAMOS HACER PUBLICIDAD DE LA MEJOR EMPRESA DEL MUNDO POR LIBERAR MODELOS PARA TODOS!!!\n\nQue se jodan los otros malditos openai y todos los otros desgraciados que no comparten nada Opensource ojala se le arruinen sus empresas‚Ä¶NOSOTROS DEBEMOS APOYAR SOLO A QWEN Y ALGUNOS USAR SU MODELO DE IA ONLINE PARA APOYA ECONOMICAMENTE A LA EMPRESA!!! Tenemos que ayudarlos como ellos nos ayudan a nosotros!!!!",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdvc8a/qwen_lanza_nuevos_modelos_peque√±os_35/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7875ig",
          "author": "el-rey-del-estiercol",
          "text": "No eres creyente en QWEN ? Eres un traidor? Hay que tener fe y creer en QWEN Y REZAR por ellos entendido!!! O rezas por ellos o te doy unos palazos!!! üòéüòéüòéüòé aqui te dejo en enlace para un infiel que no cree y no tiene fe. https://huggingface.co/collections/unsloth/qwen35",
          "score": 2,
          "created_utc": "2026-02-24 23:07:49",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786gu9",
          "author": "charmander_cha",
          "text": "Pequeno √© ate no m√°ximo 14B e nao to vendo nenhuma deste tipo",
          "score": 1,
          "created_utc": "2026-02-24 23:04:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o787pnt",
              "author": "Effective_Head_5020",
              "text": "Queria o 9b, pelo jeito n√£o vai ter\n\n\n\n\nMas as vers√µes destiladas do 35 e 27 t√£o dando pra rodar",
              "score": 1,
              "created_utc": "2026-02-24 23:10:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o786jwh",
          "author": "el-rey-del-estiercol",
          "text": "Si que hay busca en reedit la pagina de unsloth burro!!!",
          "score": 1,
          "created_utc": "2026-02-24 23:04:39",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786lnx",
          "author": "el-rey-del-estiercol",
          "text": "Hay muchas versiones y una de 110B",
          "score": 1,
          "created_utc": "2026-02-24 23:04:55",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786mig",
          "author": "el-rey-del-estiercol",
          "text": "Otra de 35B",
          "score": 1,
          "created_utc": "2026-02-24 23:05:02",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o787bvr",
          "author": "el-rey-del-estiercol",
          "text": "Ahora ponte a rezar y a dar las gracias a nuestro se√±or QWEN en los cielos!!! A ellos les debemos la gloria del se√±or üòçüòçüòçüòçüòç",
          "score": 1,
          "created_utc": "2026-02-24 23:08:47",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o787yrt",
          "author": "el-rey-del-estiercol",
          "text": "Debeis anotaros al portal de qwen online para agradecer y ayudar a los desarrolladores ya que ellos no viven del ‚Äúaire‚Äù tienen que comer y tienen sus gastos y debemos apoyarlos economicamente tambien nos dan los mejores modelos gratis del mercado!!! ASI QUE YA SABEIS ‚Ä¶suscribiros a la IA online de qwen en qwen.ai para ayudar a la empresa!!!",
          "score": 1,
          "created_utc": "2026-02-24 23:12:06",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rexnjc",
      "title": "Using Qwen to make old game textures more realistic",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rexnjc/using_qwen_to_make_old_game_textures_more/",
      "author": "MrWrodgy",
      "created_utc": "2026-02-26 02:15:10",
      "score": 13,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "https://preview.redd.it/4bwuvi220rlg1.png?width=256&format=png&auto=webp&s=b85ec451bb507a7b29f345d267817d3ca86abdde\n\nhttps://preview.redd.it/kzedp3240rlg1.png?width=1024&format=png&auto=webp&s=0fefb877ba5c908e062bb27de23aee71c5073dbd\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rexnjc/using_qwen_to_make_old_game_textures_more/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7fzfk3",
          "author": "MrWrodgy",
          "text": "https://preview.redd.it/7q3lh6nd1rlg1.png?width=821&format=png&auto=webp&s=c225ad86b64d4d9474083099b2d3d5f1bf3be3ff\n\n",
          "score": 3,
          "created_utc": "2026-02-26 02:23:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7fzlgh",
              "author": "MrWrodgy",
              "text": "https://preview.redd.it/ks3iab5w1rlg1.png?width=128&format=png&auto=webp&s=5e153eda9ab084d487c87dfc18fa062b222364e2\n\n",
              "score": 2,
              "created_utc": "2026-02-26 02:24:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7fzmbl",
                  "author": "MrWrodgy",
                  "text": "https://preview.redd.it/uxdsax0x1rlg1.png?width=1024&format=png&auto=webp&s=3a3821e03f0de6742391bf9c0b37aa94805f8786\n\n",
                  "score": 2,
                  "created_utc": "2026-02-26 02:24:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7fzgi4",
              "author": "MrWrodgy",
              "text": "before \n\nhttps://preview.redd.it/fj2ljl2r1rlg1.png?width=753&format=png&auto=webp&s=b842ccb556dfb74d08252b14a575ee2913123cb9\n\n",
              "score": 1,
              "created_utc": "2026-02-26 02:23:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7gxeak",
                  "author": "Gold_Sugar_4098",
                  "text": "I like the before better, in-game.\nA better solution would be something in between, but that‚Äôs my 2 cent.\n\nAlso, terrain building (close-ups) might benefit ?\n",
                  "score": 0,
                  "created_utc": "2026-02-26 06:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gqis9",
          "author": "ischanitee",
          "text": "Recently I kinda liking qwen than any other wise out there",
          "score": 1,
          "created_utc": "2026-02-26 05:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fy375",
          "author": "MrWrodgy",
          "text": "prompt: While maintaining the overall structure of the image, try to make the elements more realistic.",
          "score": 1,
          "created_utc": "2026-02-26 02:16:12",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rafmab",
      "title": "19x faster at 256k. Have you tested it?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "author": "New_Construction1370",
      "created_utc": "2026-02-21 02:48:20",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "Drop your long-context RAG test results here. Does it actually hold up at 256k?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6k5m6n",
          "author": "el-rey-del-estiercol",
          "text": "Funciona!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k5o42",
          "author": "el-rey-del-estiercol",
          "text": "Debeis adorar a los modelos de qwen y poneros de rodillas ante ellos!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:05:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rclvgi",
      "title": "Let's talk about the \"Agentic Workflow\" capabilities in real life.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rclvgi/lets_talk_about_the_agentic_workflow_capabilities/",
      "author": "dino_gr01",
      "created_utc": "2026-02-23 16:20:54",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "I set up a loop where Qwen writes code, tests it in a sandbox, reads the error log, and fixes it. It successfully debugged a massive JSON parsing script without my intervention.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rclvgi/lets_talk_about_the_agentic_workflow_capabilities/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o70frph",
          "author": "iolairemcfadden",
          "text": "How did you do this. Was it a single prompt or something else.",
          "score": 1,
          "created_utc": "2026-02-23 20:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z33jm",
          "author": "Crafty_Ball_8285",
          "text": "Codex does this by itself with no prompting",
          "score": 0,
          "created_utc": "2026-02-23 16:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z43qt",
              "author": "somas",
              "text": "I love what codex does for me for $20 a month but I‚Äôm not 100% on OpenAI being around in five years. I‚Äôm trying to make Qwen do what codex can do too.",
              "score": 3,
              "created_utc": "2026-02-23 16:29:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbk7r9",
      "title": "how to use qwen coding model for free?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbk7r9/how_to_use_qwen_coding_model_for_free/",
      "author": "Jaded_Expert2806",
      "created_utc": "2026-02-22 12:06:29",
      "score": 9,
      "num_comments": 20,
      "upvote_ratio": 0.8,
      "text": "how to use qwen coding model for free by api key or any tool ?",
      "is_original_content": false,
      "link_flair_text": "Help üôã‚Äç‚ôÇÔ∏è",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbk7r9/how_to_use_qwen_coding_model_for_free/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6reivm",
          "author": "jackorjek",
          "text": "qwen chat is free, unlimited. cli is free up to 1000 requests per day. api is free for 90 days through alibaba cloud. correct me if im wrong.",
          "score": 6,
          "created_utc": "2026-02-22 12:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rcxcu",
          "author": "Moist-Chip3793",
          "text": "Nvidia NiM.\n\nYou can get a free API key here: [https://build.nvidia.com/](https://build.nvidia.com/)\n\nThis might also be useful: [https://www.reddit.com/r/opencodeCLI/comments/1rb65ak/i\\_made\\_a\\_tui\\_tool\\_to\\_use\\_opencode\\_basically\\_for/?share\\_id=AzR\\_MMLPv4S7RjK1QtRWi&utm\\_medium=android\\_app&utm\\_name=androidcss&utm\\_source=share&utm\\_term=4](https://www.reddit.com/r/opencodeCLI/comments/1rb65ak/i_made_a_tui_tool_to_use_opencode_basically_for/?share_id=AzR_MMLPv4S7RjK1QtRWi&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=4)",
          "score": 4,
          "created_utc": "2026-02-22 12:10:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rw0wj",
              "author": "Rude-Ad2841",
              "text": "T√ºrkiye is also not on the menu",
              "score": 3,
              "created_utc": "2026-02-22 14:18:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rdwgk",
              "author": "MrMrsPotts",
              "text": "What are the limits?",
              "score": 1,
              "created_utc": "2026-02-22 12:18:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rwkbe",
                  "author": "Moist-Chip3793",
                  "text": "I've just ripped this from my link above:\n\n**‚ö†Ô∏è Honest limitations you should know:**\n\nNVIDIA moved from a credit system to rate limits in mid-2025 so the good news is there's no credit counter running out anymore. The free access is ongoing with no expiry, as long as you use it for dev/prototyping (not for serving real users in production).\n\nThe commonly reported rate limit is around **40 requests/minute**, though NVIDIA doesn't publish exact per-model limits and has confirmed they don't plan to. For a coding session that's rarely an issue.\n\nThe real pain point is that popular models especially the S+ tier ones like DeepSeek V3.2 or Qwen3 Coder 480B can be slow or outright overloaded üî• during peak hours. That's actually the main reason I built this tool: instead of guessing, you see all 44 models' live latency and uptime at once and switch in one keystroke.",
                  "score": 1,
                  "created_utc": "2026-02-22 14:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6tehxw",
              "author": "ActiveAd9022",
              "text": "Yeah I don't know what the heck is going on but out of all the countries in the world only T√ºrkiye, Egypt, Pakistan, and a few others like Russia are not available.¬†\n\n\nDo you know of any reason for why this is the case? Heck I could even find Palestine and the holy See who most of the time don't count as countries in the list",
              "score": 1,
              "created_utc": "2026-02-22 18:34:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ww2a4",
                  "author": "Moist-Chip3793",
                  "text": "Nobody knows, it seems. :(\n\n[https://forums.developer.nvidia.com/t/i-cant-find-my-country-on-the-list-to-verify-my-account-and-get-api-access/360844](https://forums.developer.nvidia.com/t/i-cant-find-my-country-on-the-list-to-verify-my-account-and-get-api-access/360844)",
                  "score": 1,
                  "created_utc": "2026-02-23 06:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rd0lb",
              "author": "Jaded_Expert2806",
              "text": "I cant verify nim by my country egypt not there üò≠",
              "score": 1,
              "created_utc": "2026-02-22 12:11:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rd7sq",
                  "author": "Moist-Chip3793",
                  "text": "Damn!",
                  "score": 1,
                  "created_utc": "2026-02-22 12:12:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rdcm7",
          "author": "ridablellama",
          "text": "qwen coding plan includes a free tier",
          "score": 2,
          "created_utc": "2026-02-22 12:14:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rde6k",
              "author": "Jaded_Expert2806",
              "text": "Free api key?",
              "score": 1,
              "created_utc": "2026-02-22 12:14:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rga2c",
                  "author": "ridablellama",
                  "text": "ye you can use their oauth login with third party apps. i‚Äôve used it for openclaw starter models before. ",
                  "score": 3,
                  "created_utc": "2026-02-22 12:37:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6tklzq",
                  "author": "ridablellama",
                  "text": "[https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available](https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available)",
                  "score": 1,
                  "created_utc": "2026-02-22 19:02:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rdslj",
              "author": "MrMrsPotts",
              "text": "What are the limits?",
              "score": 1,
              "created_utc": "2026-02-22 12:17:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rg4h6",
                  "author": "ridablellama",
                  "text": "# OAuth Free Tier\n\nSign in with Qwen OAuth to get 1,000 free requests per day. No credit card required, perfect for individual developers and small teams.",
                  "score": 3,
                  "created_utc": "2026-02-22 12:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76owxw",
          "author": "NoobMLDude",
          "text": "Here you go, shows exactly how to setup the FREE Tier on Qwen \n\n- [Setup Qwen Code - FREE Code Agent using Qwen3Coder in Terminal](https://youtu.be/M6ubLFqL-OA)\n\n\nIf you wish to use Cline /KiloCode inside VSCode IDE, you can also connect the QwenCode to those extensions.\n\n- [FREE Qwen3Code with Kilo-Code](https://youtu.be/z_ks6Li1D5M)",
          "score": 1,
          "created_utc": "2026-02-24 18:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rgbd7",
          "author": "Suitable-Program-181",
          "text": "Zed ACP has sweet limits",
          "score": 1,
          "created_utc": "2026-02-22 12:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s4agi",
          "author": "iolairemcfadden",
          "text": "Someone in the last two weeks posted and they were offering Quinn credits if you message them. The credit is through Alibaba cloud.",
          "score": 0,
          "created_utc": "2026-02-22 15:02:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raoi9l",
      "title": "Qwen3.5 API on ModelStudio is live.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1raoi9l/qwen35_api_on_modelstudio_is_live/",
      "author": "HawkLopsided6107",
      "created_utc": "2026-02-21 11:05:46",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Link is up. Time to update all our production endpoints!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1raoi9l/qwen35_api_on_modelstudio_is_live/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rcfnn8",
      "title": "Using Qwen Code to untangle legacy spaghetti code.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcfnn8/using_qwen_code_to_untangle_legacy_spaghetti_code/",
      "author": "AppropriateMark8528",
      "created_utc": "2026-02-23 12:04:04",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.74,
      "text": "I gave it a 10-year-old JavaScript file with zero comments. It refactored it, added modern ES6 syntax, and commented the entire logic flow.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcfnn8/using_qwen_code_to_untangle_legacy_spaghetti_code/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6xszmg",
          "author": "thatonereddditor",
          "text": "And...are you going to share results?",
          "score": 6,
          "created_utc": "2026-02-23 12:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xvl7w",
          "author": "habibi-sheikh",
          "text": "But does it run?",
          "score": 4,
          "created_utc": "2026-02-23 12:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xxxud",
          "author": "Puzzleheaded-Box2913",
          "text": "I gave my cognition MCP project to Claude Opus 4.6 and it fixed all the simple issues and by the time I ran out of usage all simple issues we're fixed but then all the complex logic and gateways weren't working anymore üòÜ\n\nI gave Qwen3.5-Plus via Qwen Code CLI my project and it actually improved it. Also started fixing the problems Claude made. üòÖ",
          "score": 4,
          "created_utc": "2026-02-23 12:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xy9dy",
              "author": "Puzzleheaded-Box2913",
              "text": "I gave both of the models full info on the code base, architecture, layers, structure, metrics, protocols and methods I've applied to the project.",
              "score": 1,
              "created_utc": "2026-02-23 12:46:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9lvn7",
      "title": "Share your Qwen Code prompts!",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r9lvn7/share_your_qwen_code_prompts/",
      "author": "hosohep",
      "created_utc": "2026-02-20 04:37:56",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "Qwen Code is super powerful now. What system prompts are you using to get the best results?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9lvn7/share_your_qwen_code_prompts/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    }
  ]
}