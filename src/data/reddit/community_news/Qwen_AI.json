{
  "metadata": {
    "last_updated": "2026-02-28 02:40:16",
    "time_filter": "week",
    "subreddit": "Qwen_AI",
    "total_items": 20,
    "total_comments": 134,
    "file_size_bytes": 142640
  },
  "items": [
    {
      "id": "1rdnzbe",
      "title": "Connected Qwen3-VL-2B-Instruct to my security cameras, result is great",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/gallery/1rdnzbe",
      "author": "solderzzc",
      "created_utc": "2026-02-24 18:17:10",
      "score": 310,
      "num_comments": 61,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Qwen VL",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdnzbe/connected_qwen3vl2binstruct_to_my_security/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o77mbht",
          "author": "beedunc",
          "text": "Frankly, those tiny Qwen VL models are going to change the world. Nice work!",
          "score": 15,
          "created_utc": "2026-02-24 21:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77xmlc",
              "author": "solderzzc",
              "text": "Agreed. Qwen VL is really good. ",
              "score": 6,
              "created_utc": "2026-02-24 22:19:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77eegx",
          "author": "Firm-Evening3234",
          "text": "Bel progetto, da approfondire e integrare su django",
          "score": 5,
          "created_utc": "2026-02-24 20:50:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77jvoa",
              "author": "solderzzc",
              "text": "Quale funzionalit√† ti serve integrare in Django",
              "score": 1,
              "created_utc": "2026-02-24 21:15:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77k8hh",
                  "author": "Firm-Evening3234",
                  "text": "Vorrei ricreare l'ambiente unifi per i sistemi nvr, database persone, riconoscimento di atti ostili etc etc",
                  "score": 2,
                  "created_utc": "2026-02-24 21:16:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gddno",
          "author": "Bohdanowicz",
          "text": "I did something similar with ring, then I decided to add gait analysis  and torso to leg ratio analysis so people leaving with their backs to the camera were identified (used video of people approaching camera + facial recognition to match gait/face so even if it doesn't know who it is it still knows its the same person, then decided to add a wildlife detector and had it build out a database of all the  local wildlife and then started analyzing the wildlife patterns....  All local inference.  Next cameras I get won't need amazon.  Just a few high def 4k cameras and i'll connect it to whatever I want and do whatever I want.\n\nWelcome to the singularity.",
          "score": 4,
          "created_utc": "2026-02-26 03:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76itv8",
          "author": "Samy_Horny",
          "text": "It's a shame that we'll have to wait even longer for the latest batch of small Qwen 3.5 models.",
          "score": 3,
          "created_utc": "2026-02-24 18:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76j0np",
              "author": "solderzzc",
              "text": "Yes, hopefully we will get them soon. ",
              "score": 2,
              "created_utc": "2026-02-24 18:26:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7jw20n",
              "author": "alexx_kidd",
              "text": "why do you say that you read anything relevant somewhere?",
              "score": 1,
              "created_utc": "2026-02-26 17:49:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jwt3u",
                  "author": "Samy_Horny",
                  "text": "Because I follow the main engineers, and also, in the early days of the Github commit, they referenced small models; the large version was never seen around.",
                  "score": 2,
                  "created_utc": "2026-02-26 17:53:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o794h9i",
          "author": "mike7seven",
          "text": "The Qwen models are impressive for sure, for its size running on GGUF on Mac makes it even more impressive. Does the app allow MLX Versions of the models? I only ask because the MLX (MLX-VLM)versions take up fewer resources and are faster.",
          "score": 3,
          "created_utc": "2026-02-25 02:12:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7980hd",
              "author": "solderzzc",
              "text": "Not yet, I'll look into MLX-VLM for an integration, thanks for your information. ",
              "score": 2,
              "created_utc": "2026-02-25 02:32:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79aoa6",
                  "author": "mike7seven",
                  "text": "Not sure it would do anything additional other than being able to say ‚ÄúA blade of grass moved‚Äù.",
                  "score": 2,
                  "created_utc": "2026-02-25 02:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bos99",
          "author": "ConversationFun940",
          "text": "What's the ui used here.. sorry new to ai",
          "score": 3,
          "created_utc": "2026-02-25 13:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bya1b",
              "author": "solderzzc",
              "text": "The UI is an application developed by me, framework is the same as VSCode ( Electron based ). You can download it here: https://www.sharpai.org ",
              "score": 2,
              "created_utc": "2026-02-25 14:32:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7gcz3z",
                  "author": "dyeusyt",
                  "text": "Ngl instead of focusing on personal home security, you should probably pivot the product toward enterprise or factory-level use cases; maybe something like tracking shipments and flagging anomalies more effectively ",
                  "score": 2,
                  "created_utc": "2026-02-26 03:42:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k7ewh",
                  "author": "sledmonkey",
                  "text": "What sort of prompting do you give Qwen?",
                  "score": 2,
                  "created_utc": "2026-02-26 18:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77mf2g",
          "author": "Artem_C",
          "text": "Yeah, you just pretty much doxxed yourself. Atleast blur the street across",
          "score": 2,
          "created_utc": "2026-02-24 21:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77o1b8",
              "author": "solderzzc",
              "text": "... You are ring, will do next time, it doesn't allow me to edit the photo now. ",
              "score": 2,
              "created_utc": "2026-02-24 21:34:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79uzqo",
          "author": "Sadboy2403",
          "text": "so how many tokens per hr watched? I know its going to consume more if theres activity in the camera but how much? ",
          "score": 2,
          "created_utc": "2026-02-25 04:52:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79w9do",
              "author": "solderzzc",
              "text": "If local model is being used for video analysis, the video analysis will cost no token.\nCloud model(gpt-4o) to handle video will be around $5 per day, I have cameras located in-house, so it generated more than 150+ clips per day. 3M input, 2M output.\n\nCloud LLM will be used when you chat, it handles tool call. Summary VLM's generation, search through the video summary for the user's query. Something around 500k inputs, 80k outputs. per day. (GPT-5.2) \n\nI'm testing an integration with lmstudio, to leverage QWEN-27GB locally as the brain model for tool call and conversation (LLM) on M3 24GB, not done yet.  ",
              "score": 1,
              "created_utc": "2026-02-25 05:01:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o79wi6u",
              "author": "solderzzc",
              "text": "Motion detection is at the frontend, did optimization for one clip as well, not send every frame. If send all the frames to cloud for watching, it's about $0.1 per 20s. ( gpt-4o ).",
              "score": 1,
              "created_utc": "2026-02-25 05:03:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ae8or",
          "author": "mihaii",
          "text": "it's a pity u can't use a LLM on premise (i see that u can only use OpenAI API at this point). ",
          "score": 2,
          "created_utc": "2026-02-25 07:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bxoay",
              "author": "solderzzc",
              "text": "Yes, I'm testing QWen on premise, QWEN3 Coder Instruct 28B(hosted by lmstudio) could be running on my 24GB MACBOOK AIR M3, I'll release it soon. \nIts tool use capability is good. Haven't tested it throughly. ",
              "score": 1,
              "created_utc": "2026-02-25 14:29:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7d07tp",
                  "author": "mihaii",
                  "text": "but at this point, there is no way of using a local LLM  , just the visual AI\n\nany reasons for not going with only one LLM? that does both vision and text?\n\nis the project opensource / vibecoded?",
                  "score": 2,
                  "created_utc": "2026-02-25 17:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7at556",
          "author": "Virtual_Sherbert6846",
          "text": "I've been trying to piece together object detection for my robot and it is terrible. Something like this is a big upgrade.",
          "score": 2,
          "created_utc": "2026-02-25 09:45:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7by1do",
              "author": "solderzzc",
              "text": "What hardware you are using? I have DGX Spark ( CUDA 13 ), Jetson Nano / AGX. Let me know if an aarch64 release is required.",
              "score": 1,
              "created_utc": "2026-02-25 14:31:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7efy29",
                  "author": "Virtual_Sherbert6846",
                  "text": "Jetson Orin NX 16GB. I am also running a STT and TTS model on the Jetson. I may offload to my PC with a 4090 RTX.",
                  "score": 2,
                  "created_utc": "2026-02-25 21:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bnhqo",
          "author": "BelieverInYellow",
          "text": "Omg that output is super detailed! :0",
          "score": 2,
          "created_utc": "2026-02-25 13:34:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7byc3u",
              "author": "solderzzc",
              "text": "Yes, too detailed :) ",
              "score": 1,
              "created_utc": "2026-02-25 14:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cpng1",
          "author": "Obvious_Fix_1012",
          "text": "Very cool! Nice job.",
          "score": 2,
          "created_utc": "2026-02-25 16:42:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cripc",
          "author": "SearchTricky7875",
          "text": "damn, that is one useful use, I can use it with my raspberry pi.",
          "score": 2,
          "created_utc": "2026-02-25 16:50:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cyswl",
              "author": "solderzzc",
              "text": "Got it, let me prepare a Raspberry Pi build. ",
              "score": 1,
              "created_utc": "2026-02-25 17:24:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d6jm0",
          "author": "Plenty-Mix9643",
          "text": "What does that bring? I mean it is cool, but what is the benefit of it for you.",
          "score": 2,
          "created_utc": "2026-02-25 17:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ddlth",
              "author": "solderzzc",
              "text": "To be honest, it started with a personal annoyance: I have 'stupid' cameras that I pay good monthly fees for, yet I still have to scrub through hours of footage myself to find anything.\n\nThe benefit for me is two-fold:\n\nIntelligence & Automation: I want to 'teach' my cameras what to look for so I don't have to. Aegis pulls my cloud clips (Ring/Blink wired/battery) locally so I can search them with a private, local LLM. This weekend project honestly would have been impossible without vibe coding‚Äîit's allowed me to hit 400k lines of logic at a speed traditional dev couldn't touch.\n\nThe 'GitHub' Model: I believe the future of AI is local. My plan is to keep a powerful free version for homeowners to regain their privacy. The business model follows the GitHub or Slack approach: provide massive value to the community for free, while providing support needed for SMB and Enterprise‚Äîan area where I‚Äôve spent my career training models and building products.",
              "score": 1,
              "created_utc": "2026-02-25 18:30:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d7fv8",
          "author": "jedsk",
          "text": "awesome!",
          "score": 2,
          "created_utc": "2026-02-25 18:03:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7itrlb",
              "author": "Wide-Personality6520",
              "text": "For real! The detail it captures is next level. Have you tried it on different scenes yet?",
              "score": 1,
              "created_utc": "2026-02-26 14:51:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ef1rc",
          "author": "Busy-Guru-1254",
          "text": "Cool. How does it work. Do frame by frame analysis and then summarize the description of all the frames?",
          "score": 2,
          "created_utc": "2026-02-25 21:23:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ekgps",
              "author": "solderzzc",
              "text": "yes, that's the first version, then you know the speed is very slow w/ local model, and cost is very high with cloud model. \nSo I pick up significant frames to make it cost less... ",
              "score": 1,
              "created_utc": "2026-02-25 21:48:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7i8vm2",
          "author": "LoveInTheFarm",
          "text": "It‚Äôs huggingface ?",
          "score": 2,
          "created_utc": "2026-02-26 12:56:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jrbvu",
              "author": "solderzzc",
              "text": "This is desktop application, model is downloaded from huggingface and inference locally.",
              "score": 1,
              "created_utc": "2026-02-26 17:27:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nk2gx",
          "author": "crusoe",
          "text": "I wonder if they work for spaghetti detection on 3d printers...",
          "score": 2,
          "created_utc": "2026-02-27 05:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nlequ",
              "author": "solderzzc",
              "text": "Great idea, worth a try :) Put your iPhone or Mac there, leverage webcam to watch it ",
              "score": 1,
              "created_utc": "2026-02-27 05:53:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pyrbt",
          "author": "1HK7",
          "text": "This looks amazing. What was the prompt given to the model to generate that response ? Are you giving video snippets (a bunch of frames as input ) or sampling the frames every N seconds and giving it to the model as input ?",
          "score": 2,
          "created_utc": "2026-02-27 16:05:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7r83y5",
              "author": "solderzzc",
              "text": "Glad you like the output from QWEN :) Frame by frame is too expensive, it takes long time to handle one vide, ( some models are supporting video input directly but inference time and memory cost is huge ). So sampling is the way to cost down. The prompt is asking QWEN to think it's a security :) ",
              "score": 1,
              "created_utc": "2026-02-27 19:41:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7878uw",
          "author": "cool-beans-yeah",
          "text": "But where's the mail man and the street doesn't have a white line down the middle, does it?\n\nEdit: just realized that may have come across as sarcastic...none intended!",
          "score": 1,
          "created_utc": "2026-02-24 23:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78cifv",
              "author": "solderzzc",
              "text": "Mail man was at the first several seconds, I forwarded to 12s to not disclose mailman's privacy. ... White line is a hallucination. It thought white line should be always on the road. :) ",
              "score": 1,
              "created_utc": "2026-02-24 23:36:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7aljzh",
                  "author": "cool-beans-yeah",
                  "text": "Oh ok! Thanks",
                  "score": 2,
                  "created_utc": "2026-02-25 08:34:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7cktn4",
                  "author": "Crafty-Young3210",
                  "text": "dont you think the fact that its hallucinating something thats clearly not there is an issue for using these models for this application?",
                  "score": 2,
                  "created_utc": "2026-02-25 16:20:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7j7raj",
          "author": "Few_Matter_9004",
          "text": "Are you visually impaired, or did you do this out of sheer boredom?",
          "score": 1,
          "created_utc": "2026-02-26 15:57:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lm8x2",
          "author": "_Viral19",
          "text": "hi\n\n",
          "score": 1,
          "created_utc": "2026-02-26 22:45:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rex0vo",
      "title": "Big love to the Qwen üß† A true SOTA Open Source model running locally (Qwen 3.5 35B 4-bit) - Here is the fix for the logic loops! ‚ù§Ô∏è",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rex0vo/big_love_to_the_qwen_a_true_sota_open_source/",
      "author": "SnooWoofers7340",
      "created_utc": "2026-02-26 01:47:35",
      "score": 188,
      "num_comments": 12,
      "upvote_ratio": 0.97,
      "text": "Been hunting for a local daily driver to finally drop paid APIs, and I think I found \"The One.\"\n\nGrabbed the new **Qwen3.5-35B-A3B-4bit**, and at first, it was rough. I was getting those classic 4-bit issues: reasoning loops, endless \"Wait, let me check...\" spirals, and failing simple logic traps like the \"Car Wash\" problem.\n\nSpent the day stress-testing it against the [Digital Spaceport Benchmark suite](https://digitalspaceport.com/about/testing-local-llms/) (logic, math, SVG coding, counting). It was hit-or-miss until I realized the model wasn't \"dumb\" it just needed structure. The quantization makes it anxious, so it second-guesses itself into oblivion.\n\n**The Fix:**\n\nTweak the system prompt to force \"Adaptive Logic.\" It separates the scratchpad thinking from the final answer and sets a hard limit on the \"thinking\" phase.\n\nOnce applied this, it passed **100% of the tests** in seconds.\n\n* **Logic:** Solved the \"Pico de Gato\" schedule puzzle perfectly.\n* **Math:** Nailed the \"Two Drivers\" problem without looping.\n* **Coding:** Generated a clean SVG cat on the first try.\n* **Counting:** Actually counted \"Peppermint\" correctly (rare for 4-bit).\n\nNext up, I'm throwing my complex n8n workflows at it to see if it survives. ü§û\n\nIf you're struggling with the 4-bit version, here is the config that unlocked it for me.\n\n**‚öôÔ∏è The Config**\n\n* **Model:** Qwen3.5-35B-A3B-4bit\n* **Temp:** 0.7 | **Top P:** 0.9 | **Min P:** 0.05 *(Critical!)*\n* **Freq Penalty:** 1.1 | **Repeat Last N:** 64\n\n**üß† The \"Anti-Loop\" System Prompt**\n\n*(Paste into OpenWebUI/LM Studio)*\n\nPlaintext\n\n    You are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops.\n    \n    1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n    2. ADAPTIVE LOGIC:\n       - For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n       - For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n       - For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n    3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n    \n    DO NOT reveal your thinking process outside of the tags.\n\nBig love to the Qwen team. You guys genuinely changed the game with this one. Having this running locally is a dream. üöÄ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rex0vo/big_love_to_the_qwen_a_true_sota_open_source/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7fxc6x",
          "author": "InitialJelly7380",
          "text": "good sharing",
          "score": 2,
          "created_utc": "2026-02-26 02:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g0s7m",
          "author": "pinthead",
          "text": "What is your context length ?",
          "score": 2,
          "created_utc": "2026-02-26 02:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hjyu2",
              "author": "SnooWoofers7340",
              "text": "I'm capped at **28k context** but Qwen 3.5 architecture theoretically supports **1M tokens** using YaRN/RoPE scaling!! insane",
              "score": 2,
              "created_utc": "2026-02-26 09:32:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hp29t",
          "author": "Aware-Adman",
          "text": "Nice",
          "score": 2,
          "created_utc": "2026-02-26 10:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jr047",
          "author": "laser50",
          "text": "Maybe for anyone that's gone there by now and has more of a clue than myself..\n\nWould the 35B A3B model work better on thinking or without thinking?\n\nI've always wondered what the model does if it can't actually think of its answer lol",
          "score": 2,
          "created_utc": "2026-02-26 17:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7livdd",
              "author": "SnooWoofers7340",
              "text": "A bit of both! It's all about balance right, after all day testing out I settle for this : \n\n ‚öôÔ∏è Model Configuration Parameters\nTemperature: 0.7\nMax Tokens: 28,000\nTop P: 0.9\nMin P: 0.05 (This was the critical one for stability!)\nFrequency Penalty: 1.1\nRepeat Last N: 64\nTop K: Default\nK & V Caching (Context Quantization): Disabled / f16 (Default)\nEverything else: Default\n\nüß† The \"Anti-Loop\" System Prompt\n\nYou are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops.\n\n1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n2. ADAPTIVE LOGIC:\n   - For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n   - For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n   - For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n\nDO NOT reveal your thinking process outside of the tags.",
              "score": 1,
              "created_utc": "2026-02-26 22:28:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kgbwb",
          "author": "Holiday-Pack3385",
          "text": "I did something similar on mine - I'm using the Q5\\_K\\_M with the full 262144 context length active on my Geforce 5090. My system prompt is shown below. (I added it when the darn thing kept showing \"Thinking Process\" in every response:\n\nYou are a highly intelligent, direct, and efficient AI assistant. You answer questions concisely without unnecessary politeness or filler. You focus on the user's specific request and provide actionable information. If the user asks for an opinion, provide a balanced view. If you don't know the answer, admit it. Your goal is to be the most helpful tool possible.\n\nDo not show your \"Thinking Process:\" in the response.",
          "score": 2,
          "created_utc": "2026-02-26 19:23:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lj71e",
              "author": "SnooWoofers7340",
              "text": "Nice one man thanks for sharing, mine is quiet similar.\n\n ‚öôÔ∏è Model Configuration Parameters\nTemperature: 0.7\nMax Tokens: 28,000\nTop P: 0.9\nMin P: 0.05 (This was the critical one for stability!)\nFrequency Penalty: 1.1\nRepeat Last N: 64\nTop K: Default\nK & V Caching (Context Quantization): Disabled / f16 (Default)\nEverything else: Default\n\nüß† The \"Anti-Loop\" System Prompt\n\nYou are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops.\n\n1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n2. ADAPTIVE LOGIC:\n   - For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n   - For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n   - For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n\nDO NOT reveal your thinking process outside of the tags.",
              "score": 1,
              "created_utc": "2026-02-26 22:30:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kxyav",
          "author": "beedunc",
          "text": "Thanks!",
          "score": 2,
          "created_utc": "2026-02-26 20:47:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kxzsl",
              "author": "exclaim_bot",
              "text": ">Thanks!\n\nYou're welcome!",
              "score": 3,
              "created_utc": "2026-02-26 20:47:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gpe3f",
          "author": "altdotboy",
          "text": "I was using the qwen3 30B A3B for my home rig. It was good. I have to say the new 35B A3B is awesome. I was a bit weary about the model having thinking, was going to turn it off but decided to embrace it. The 35B seems less likely to get stuck in the loops you mentioned. It‚Äôs great having the thinking text to see what the model is considering. I‚Äôm on a MacBook Pro M4 48GB. Context max is set to 128k. \nI might borrow some of your prompt. Good tips.",
          "score": 4,
          "created_utc": "2026-02-26 05:05:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7og2mq",
          "author": "AgitatedDoctor9613",
          "text": "This is an enthusiastic post about optimizing Qwen 3.5 for local automation, but it could be significantly strengthened with a few improvements:\n\n**Improvements:**\n1. **Complete the post**: The content cuts off mid-sentence (\"Two Driv...\"). Finish the full results and provide complete context for readers.\n2. **Share the actual system prompt**: You mention the \"Adaptive Logic\" fix but don't include the exact prompt template. Consider pasting the full prompt so others can replicate your results immediately rather than reverse-engineering it.\n3. **Provide quantitative benchmarks**: Instead of \"100% of tests,\" specify how many tests, what the baseline was, and include timing/performance metrics (latency, token/sec, memory usage).\n4. **Add reproducibility details**: Include your hardware specs (GPU, RAM), inference framework (ollama, vLLM, etc.), and exact model link so others can verify results.\n\n**Alternative Perspectives to Consider:**\n1. **Trade-offs not mentioned**: 4-bit quantization reduces model size but may impact quality on edge cases. Have you tested on tasks outside the benchmark suite?\n2. **Comparison context**: How does this compare to other open models (Llama 3.1, Mistral, DeepSeek) on the same benchmarks? \"The One\" is strong language without comparison data.\n3. **Cost-benefit analysis**: While dropping paid APIs saves money, consider total cost of ownership (hardware, electricity, maintenance time vs. API costs at scale).\n\n**Potential Issues:**\n1. **Survivorship bias**: The \"fix\" worked for your specific setup‚Äîdifferent quantization methods, hardware, or use cases may need different tuning.\n2. **Benchmark limitations**: Digital Spaceport tests are useful but narrow. Real-world n8n automation often involves unstructured data, API integration, and error handling that benchmarks don't cover.\n3. **Prompt engineering fragility**: System prompt tweaks can be brittle. Test how sensitive the 100% success rate is to minor prompt variations.\n\n**Additional Resources to Reference:**\n1. Link to the n8n community docs on local LLM integration\n2. Point readers to the actual Qwen model card on Hugging Face for official specs\n3. Mention relevant papers on quantization artifacts and mitigation techniques\n4. Suggest testing frameworks like HELM or LMEval for more rigorous benchmarking\n\n**Tone note**: Your enthusiasm is great for engagement, but ground claims with data. \"The One\" and \"100% success\" will attract skepticism without full context. Lead with reproducible evidence.",
          "score": 0,
          "created_utc": "2026-02-27 10:29:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcmm3q",
      "title": "I canceled my other AI subscriptions today.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "author": "InitialCareer306",
      "created_utc": "2026-02-23 16:47:15",
      "score": 167,
      "num_comments": 57,
      "upvote_ratio": 0.87,
      "text": "Between the 256k context window and the fact that a 397B open-weight model is available, I just can't justify paying $20/mo anymore. The open-source community won.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6zd3ee",
          "author": "Historical-Internal3",
          "text": "What hardware are you running the model on?",
          "score": 18,
          "created_utc": "2026-02-23 17:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72l8gc",
              "author": "shooshmashta",
              "text": "Lies",
              "score": 7,
              "created_utc": "2026-02-24 03:05:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o743lnf",
                  "author": "doodo477",
                  "text": "tears",
                  "score": 5,
                  "created_utc": "2026-02-24 10:29:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zgvfx",
          "author": "TalosStalioux",
          "text": "I mean, purchasing a $4000 setup (minimum) to run that kind of model size just to save $20 per month is... interesting",
          "score": 46,
          "created_utc": "2026-02-23 17:29:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zyn3n",
              "author": "Justfun1512",
              "text": "Show me 4,000 $ build /machine that can run 300b models",
              "score": 40,
              "created_utc": "2026-02-23 18:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zz0u1",
                  "author": "ImmediateDot853",
                  "text": "I am also interested in that setup.",
                  "score": 16,
                  "created_utc": "2026-02-23 18:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o709glb",
                  "author": "greenthum6",
                  "text": "I have 4K$ 5090 build. What 300B model can I run? I thought 30B models were already on the edge.",
                  "score": 12,
                  "created_utc": "2026-02-23 19:40:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70d1zz",
                  "author": "Hankdabits",
                  "text": "It would have to be hybrid gpu/cpu inference. Probably something like amd epyc, 8 channels of ddr4, and a 3090 to get ~12 t/s and slow prefill",
                  "score": 2,
                  "created_utc": "2026-02-23 19:57:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72id9q",
                  "author": "Straight_Issue279",
                  "text": "With the right build you can make a very good ai using vector memory ect for around 2 to 3k i made an offline ai, my ai can remember content from 4 months ago. Can gather info online, is uncensored, can protect my network by scanning it and be able to create images. Im tired of all the data privacy leaks hence why I will never pay for subscriptions ever. Yeah it may take a full 10 sec to respond but it works for me. I was running dolphin-2.6-mistral-7b.Q6_K.gguf  but I went for a 12b model. Its enough for me.",
                  "score": 2,
                  "created_utc": "2026-02-24 02:49:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75kdrl",
                  "author": "RandomCSThrowaway01",
                  "text": "Q4 is 220GB. Meaning it fits on 256GB Mac Studio which brand new is $5600. Occasionally it does show up in refurbished/sale category however and then it's possible to find it for around $5000. Not 4 but still, if you can afford $4000 then you probably **can** consider $5000. \n\nJust, uh, don't expect it to actually outperform $200 Claude Max and similar solutions in pure LLM output quality. You can run some great stuff on it but prompt processing is slow with huge models in particular. ",
                  "score": 1,
                  "created_utc": "2026-02-24 15:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77nf13",
                  "author": "TheRiddler79",
                  "text": "Mine will\n\nhttps://preview.redd.it/n4x5uchngilg1.jpeg?width=1440&format=pjpg&auto=webp&s=84794eacea54670b9094eff3311622c2e80ffbd4\n\nBut, if I'm being fair, it wouldn't be a whole lot of fun to chat with. What it's really really really really good at doing is I give it a task, and then it crunches away at two tokens a second for 24 hours, and then I end up with an incredible output, a fully built website, a fully built mobile app whatever. But it's not fun to chat with cuz it's too slow so you just have to give it a task and go away.\n\nThe relevance here is that I'm running on a system I built for $1,500 Plus a single gun trade for Ram.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:31:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7embxq",
                  "author": "Confusion_Senior",
                  "text": "Used M2 Ultra 192gb Mac studio with Q4 model\n\nOr just used 192gb ram with used  3090 as well",
                  "score": 1,
                  "created_utc": "2026-02-25 21:57:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zwuro",
              "author": "illathon",
              "text": "20 dollars a month with massive restrictions or your own hardware running 24/7 and you can constantly upgrade as new models come out.",
              "score": 8,
              "created_utc": "2026-02-23 18:42:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71kf3s",
                  "author": "greenthum6",
                  "text": "Commercial models are updated as well. You can inference with Opus for 20$ a month. Once you get to that level with local model the paid ones are again much better.",
                  "score": 2,
                  "created_utc": "2026-02-23 23:35:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zjogg",
              "author": "upinthisjoynt",
              "text": "I'd say it's worth it if it's used for more than just code.  Since it's local and more \"secure\", I don't see why it can't be used for most everyday AI uses (except with old training data).  Just an option",
              "score": 7,
              "created_utc": "2026-02-23 17:42:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71a3ps",
              "author": "BargeCptn",
              "text": "I don't see how it's even remotely possible to have anything usable on the $4000 setup, more like $14k to $18k maybe. You're still going to be basically at 20-30 tokens per second, with a 10 to 15 second first response on the context windows that are larger than 12ktokens. It's wishful thinking, bro. Full models really take a chug out of hardware. It's all about unified memory space; that's all it is. It's not how many video cards you got.",
              "score": 2,
              "created_utc": "2026-02-23 22:39:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73v62i",
              "author": "trackktor",
              "text": "You can resell. The money doesn‚Äôt vanish.",
              "score": 1,
              "created_utc": "2026-02-24 09:09:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o76jvj9",
              "author": "dabiggmoe2",
              "text": "I mean buying a $4000 setup for that alone might be too much depending on the case. But this can also double a gaming PC or a dev machine. For the purpose to just vibe code it might not be the most effective choice. But for learning, I would say it might be worth it if one can afford it.",
              "score": 1,
              "created_utc": "2026-02-24 18:30:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o77es81",
              "author": "elaboratedSalad",
              "text": "please show me how I can run that for $4k. i'm serious. I'd jump straight in.",
              "score": 1,
              "created_utc": "2026-02-24 20:52:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o791lw1",
              "author": "goodssh",
              "text": "You probably meant to say $40,000 üòÇ. Even with that, you would probably have to constantly maintain the infra, both in software and hardware. \n\nThe open-weight model itself doesn't generate a great response without manual labor work - it's extremely difficult to micro control the model to deliver the desired result. \n\nIn my view with 20 bucks per month they give a super generous quota.",
              "score": 1,
              "created_utc": "2026-02-25 01:55:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7axac9",
              "author": "thaddeusk",
              "text": "Hey, I just managed to load it on my $2000 setup and I'm getting 16t/s. With a 1bit quant... I imagine I'd be better off loading the 122b model at 4bit, though. It's a bit faster and probably would end up being higher quality output, anyway.",
              "score": 1,
              "created_utc": "2026-02-25 10:23:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o708ehr",
          "author": "the-average-giovanni",
          "text": "* my other ai subscription\n\n\nNot this one, folks",
          "score": 3,
          "created_utc": "2026-02-23 19:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70oqj3",
          "author": "Hector_Rvkp",
          "text": "This is such a stupid post, it must be trolling. The ram requirement is insane and people who do proper coding work aren't running 20$ subs, they're using CLI tools with either tokens or much higher subs. \nThis makes no sense",
          "score": 3,
          "created_utc": "2026-02-23 20:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi1v2",
          "author": "JahJedi",
          "text": "Open 8s more than to just save 20$, first of all its privacy and it will not report you or collect any data.",
          "score": 2,
          "created_utc": "2026-02-23 17:34:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi4g3",
          "author": "JahJedi",
          "text": "On what setup you planing to run it?",
          "score": 2,
          "created_utc": "2026-02-23 17:34:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dk8o",
          "author": "pl201",
          "text": "Not really. To run 397B open-weight mode with acceptable speed, you need at least investing the in $10k range to get an acceptable performance. If you run it 24/7, your energy bill will be more than 5x $20. Plus, I yet to see open model‚Äôs ability working on a large code base. You will get something but will spend more time to fix things that are not working. Time is money too.",
          "score": 3,
          "created_utc": "2026-02-23 19:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74s74k",
          "author": "EzioO14",
          "text": "There is no economically viable alternative to using closed source models like claude, OpenAI etc‚Ä¶ if you want the same performance",
          "score": 1,
          "created_utc": "2026-02-24 13:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75mb61",
          "author": "mayday30",
          "text": "You will likely pay more than $20 in electric costs.",
          "score": 1,
          "created_utc": "2026-02-24 15:59:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75oi9s",
          "author": "jodykpw",
          "text": "I can‚Äôt afford the hardware tho.",
          "score": 1,
          "created_utc": "2026-02-24 16:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o766b3o",
          "author": "johnmclaren2",
          "text": "This is just a karma farming post or a promo for a Qwen model. :)",
          "score": 1,
          "created_utc": "2026-02-24 17:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76ffzm",
          "author": "pppreddit",
          "text": "You are forgetting that local llm servers mostly don't have prompt caching and are not suitable for coding or long exchange, they are painfully slow with long context (like minutes to get a response) . It's not enough to have big enough vram, you need proper context caching implementation and AFAIK there are only commercial solutions that support it, no open source yet. Correct me if I am wrong, because things develop really fast and it hard to catch up with everything",
          "score": 1,
          "created_utc": "2026-02-24 18:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77jiq8",
          "author": "DonkeyBonked",
          "text": "My $3.5 to $4k~ is build with 4x RTX 3090 can't even run a 300B+ model, so I mean if a $20/month sub could do what I need, that would be the cheapest way for me.",
          "score": 1,
          "created_utc": "2026-02-24 21:13:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ku0s0",
              "author": "spiritxfly",
              "text": "What are some good models you run on that build? I have the same one, I haven't really tried any of the latest models though.",
              "score": 1,
              "created_utc": "2026-02-26 20:28:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lp1uc",
                  "author": "DonkeyBonked",
                  "text": "So I'm not certain your specific use case, but I'm doing custom tunes, LoRAs, and RAG databases for game development, along with using it for software development, so I need as much context as I can get.\n\nI also am very high ADHD, so patience isn't my strongest virtue outside of hyperfocus.\n\nSo I tend to use the faster models based more on ones like Qwen3-Coder-30B-A3B-Instruct and Nemotron-3-Nano-30B for most of my grunt work. Depending on performance and the tasks related, I also do use GLM-4.7-Flash a pretty decent amount, and I have various uses/testing I do with models ranging from Devstral-2-Small-24B up to GPT-OSS-120B, Qwen-3-Coder-Next, and I'm messing around with other ones as well that are newer, I just haven't had as much time because my wife is in the hospital and I'm kind of burning my candle at both ends.\n\nI tend to prefer the smaller models and I tend to prefer llama.cpp over vllm. I can run a vllm server on a tiny model and support a ton of users connected, but unfortunately I lose too much context headroom using 30B class models on VLLM, which is about how I feel using 70B to 120B models on llama.cpp\n\nI'm really hoping the impacts from Nemotron 3 help improve the way context is stored for other models and allows them to become more context efficient. It looks like I could run a Q8 of Qwen 3.5 120B, but I'm really looking for their coder release for 3.5.\n\nI think Qwen3-Coder-30B-A3B-Instruct is my biggest workhorse and it's really fast with Claude Code when it's not hallucinating tool calls (something I'm planning to work on, just been busy).\n\nWhen I need to do reasoning, troubleshooting, breaking loops, etc., I definitely don't use Qwen there though, I prefer Nemotron 3 and GLM 4.7 or GPT-OSS-120B depending on what it is.\n\nI would probably do a bit more with GPT-OSS-120B but I'm really still trying to get down how the chain of thought fine tuning. I just took about 400MB of code and had Qwen3-Coder-30B-Instruct write the pairs for it and after inspecting a few hundred of them it actually looks really good, I was quite impressed, but I'm going to need to make another pass over that dataset with Nemotron 3 because some of the data points Qwen didn't have it skipped too much. I'd probably prefer to use GLM-4.7-Flash for that but speed is a factor. With my current setting, before the wind down Nemotron starts around 180 tokens/s output speed, Qwen3-Coder-30B-Instruct is around 140, so these are pretty fast, but GLM-4.7-Flash I'm rocking in the 60s with my current config (I need to see if I can improve it, just haven't had the time) and unfortunately, when you look at something like combing that 400MB of data, it took about 40\\~ hours to do the first pass with Qwen, it would likely take a lot longer with GLM. I might test running it with only two of the cards so I can keep the other two open for use while it processes the data, but we'll see.\n\nI have found I don't particularly care for the low quants of the higher end models. They tend to be inconsistent, often buggy, more prone to loops, and don't provide worthwhile context limits for me to care. Even doing personality tuning I've found better results tuning Qwen3-Coder-30B-Instruct and keeping the high performance than tuning a higher model. For a lot of stuff like conversational AI, I'm practicing tuning even smaller models now as the bigger just isn't necessary.\n\nTesting around though, I have around 8TB of models I'm working with currently.",
                  "score": 1,
                  "created_utc": "2026-02-26 23:00:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77rwyw",
          "author": "momontology",
          "text": "What type of hardware would be needed to run this?",
          "score": 1,
          "created_utc": "2026-02-24 21:52:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77ygmi",
          "author": "Ashthot",
          "text": "What is your setup ?",
          "score": 1,
          "created_utc": "2026-02-24 22:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nzv1g",
          "author": "AwayLuck7875",
          "text": "Im i3 ,ram 16 gb ,vram 8 gb rx480 ,qwen 30b-a3/2 token sec/granit 7b-a1b 25 token ,very cool coding",
          "score": 1,
          "created_utc": "2026-02-27 07:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nzxrf",
          "author": "AwayLuck7875",
          "text": "Granit 7b-a1b very very intrested model",
          "score": 1,
          "created_utc": "2026-02-27 07:57:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pcuz6",
          "author": "Markosz22",
          "text": "And how exactly are you running this model yourself? hmm",
          "score": 1,
          "created_utc": "2026-02-27 14:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zptd2",
          "author": "el-rey-del-estiercol",
          "text": "Pagar por ejecutar buenos modelos es interesante siempre que tu trabajo dependa de ello y ganes dinero con ello tambien",
          "score": 0,
          "created_utc": "2026-02-23 18:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70r8b5",
          "author": "NearbyBig3383",
          "text": "Gente avandoney a chutes para ir pra o Code plan do qwen. E s√©rio n√£o me arrependo",
          "score": 0,
          "created_utc": "2026-02-23 21:05:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcqezx",
      "title": "Qwen 3.5 for MLX is like its own industrial revolution",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "author": "sovietreckoning",
      "created_utc": "2026-02-23 18:59:54",
      "score": 89,
      "num_comments": 30,
      "upvote_ratio": 0.97,
      "text": "I'm using a 4-bit model on a mac studio m3 and it is mindblowing how fast this thing works for the quality of the results. I love it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o703evr",
          "author": "ossbournemc",
          "text": "How fast is it? I'm considering the same setup. ",
          "score": 5,
          "created_utc": "2026-02-23 19:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7042mt",
              "author": "sovietreckoning",
              "text": "I'm not well-versed in any of this stuff and I'm only doing ametuer stuff for myself, so please forgive me is this answer is unhelpful - a task that took \\~500s+ on llama 3(405b) is closer to \\~30s on qwen 3.5. ",
              "score": 5,
              "created_utc": "2026-02-23 19:15:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o707puy",
                  "author": "ossbournemc",
                  "text": "Thats cool, do you know how many tokens/second you're getting?",
                  "score": 2,
                  "created_utc": "2026-02-23 19:32:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75xuwv",
                  "author": "Hector_Rvkp",
                  "text": "You're looking at dense model vs MoE model. Avoid dense models, they are slower.",
                  "score": 1,
                  "created_utc": "2026-02-24 16:51:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mv2gk",
                  "author": "datbackup",
                  "text": "Llama3 405b? Lmao bro that is like the slowest of slow models, kinda unfair comparison?\n\nBut hard to know for sure without knowing what you are comparing it against (qwen3.5 is a whole family of models while llama3 405b is a specific model)",
                  "score": 1,
                  "created_utc": "2026-02-27 02:57:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70ybyw",
              "author": "Professional-Bear857",
              "text": "I get 35 tok/s on my 60 GPU core M3 ultra 256gb ram. I'm using the 4bit mlx version as well.",
              "score": 2,
              "created_utc": "2026-02-23 21:40:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c2nht",
                  "author": "zulutune",
                  "text": "Are the results comparable with Claude/Codex? Without taking speed into consideration. I suspect it‚Äôs not but I‚Äôd like to know how big the gap is, how noticeable it is",
                  "score": 1,
                  "created_utc": "2026-02-25 14:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70k51l",
              "author": "alexp702",
              "text": "I am running llama server with the 8bit GGUF. It is excellent. It gets about 250 prompt processing tokens, and about 25tokens out. I‚Äôm openclawing and many prompts are cached so practical upshot is very few tokens either way 114000 token prompt processing only 500. Not sure why I am suddenly getting such good caching (perhaps Llama update) but I like it!\n\nEdit: Openclaw reports I have a throughput of 15.2k tokens a minute probably because of prompt caching. 26.7 million tokens sent. Pretty damn good!\n\nEdit 2: Mac Studio 512gb.",
              "score": 2,
              "created_utc": "2026-02-23 20:31:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c4orc",
                  "author": "sovietreckoning",
                  "text": "Can you tell me more about this. I tried to run the 8-bit mlx version on a Mac Studio m3 512gb and it choked to a crawl.",
                  "score": 1,
                  "created_utc": "2026-02-25 15:05:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70v2xz",
          "author": "jzn21",
          "text": "I am also blown away by this model. The quality is excellent (even in non-thinking mode) and the speed is great with 34 - 35 tokens per second. And then best thing is that prompt processing happens in the blink of an eye.",
          "score": 3,
          "created_utc": "2026-02-23 21:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71js61",
          "author": "CoffeeSnakeAgent",
          "text": "Where to get the qwen 3.5 4-bit? Cant seem to find it in hf",
          "score": 3,
          "created_utc": "2026-02-23 23:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71l37g",
              "author": "sovietreckoning",
              "text": "[https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4](https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4)\n\nThats what I'm using currently.",
              "score": 3,
              "created_utc": "2026-02-23 23:39:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o71oqrx",
                  "author": "Special-Wolverine",
                  "text": "NVFP4 is optimized for Nvidia Blackwell",
                  "score": 1,
                  "created_utc": "2026-02-23 23:59:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o726qhr",
                  "author": "CoffeeSnakeAgent",
                  "text": "Thanks",
                  "score": 1,
                  "created_utc": "2026-02-24 01:41:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7fyc0r",
          "author": "zerostyle",
          "text": "What woudl be the best model to run on 32gb of ram for qwen 3.5? not seeing too much on hugging face yet. Prefer abliterated/uncensored models.",
          "score": 2,
          "created_utc": "2026-02-26 02:17:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o746vum",
          "author": "XxBrando6xX",
          "text": "Does anyone know why the MLX version seems to drop the vision ability ?? I want to run the mlx but I was hoping to do it by letting it review EVERYTHing not just text prompts which I can‚Äôt do on the mlx yet since it isn‚Äôt supported",
          "score": 1,
          "created_utc": "2026-02-24 10:58:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74qfkt",
          "author": "ThankYouOle",
          "text": "i am curious, what is your use case? to be buddy to discuss thing? quick QnA? or code?",
          "score": 1,
          "created_utc": "2026-02-24 13:19:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o751csv",
              "author": "sovietreckoning",
              "text": "It‚Äôs varied because I‚Äôm effectively learning and developing an all-inclusive ai assistant for my personal use. I own a small law firm which involves an enormous amount of admin work and document prep from preexisting forms. I also do some active spread trading from time to time. Basically, I‚Äôm building a legal rag system, a personal assistant, a document automation bot, an options scanner with built in back testing, and a coding machine to grow and improve the product over time. All of the varied functions are wrapped under the LLM for plain language interaction, personal email and calendar integration, court docket scraping, scheduling, alerts, etc. \n\nBasically, I‚Äôm learning on the fly and attempting to clone myself. I don‚Äôt know how any of it will actually work out. With massive context and pipelines to work through, the speed of Qwen 3.5 is so valuable.",
              "score": 3,
              "created_utc": "2026-02-24 14:18:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77nt7o",
                  "author": "ThankYouOle",
                  "text": "interesting that Qwen 3.5 can help you doing all of that. \n\nso far i found local model can't really help me, but then again i never try Qwen, only some small model.\n\nthank you.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:33:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ahzh7",
          "author": "c_glib",
          "text": "Can you please share your workflow? What's the exact model size? Which running (using ollama or something else)? ",
          "score": 1,
          "created_utc": "2026-02-25 08:00:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ebx0b",
          "author": "Gold_Ad_2201",
          "text": "where did you get mlx version? gguf works fine for me but all mlx versions go into repeat almost every time",
          "score": 1,
          "created_utc": "2026-02-25 21:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pi3d7",
          "author": "Ok_Significance_9109",
          "text": "I have a MacBook Pro M1 with 32GB (that translates to just over 24GB VRAM of shared memory). I am using LM Studio and OpenCode, and the MLX versions started working just yesterday, with the latest update of LM Studio (before that they would loop endlessly). I don't have the exact figures, but the 35B MoE in 4bit MLX is so fast that I cannot read the output as it sends it out. I tried also the 5bit version with LM Studio's safeties off, but that's really pushing the limit - the computer borked several times, and 5bit did not seem that much better than 4bit.  \nNow we get to the HOWEVER part. The 35B model makes a lot of mistakes. I gave it a simple challenge - create a HTML model of the solar system, meant for children, with tool tips for each planet. The result did not work, and it was clueless about fixing it.  \nTo the rescue - the dense 27B model. It is much slower than the 35B on my setup, to the point that the only possible workflow is to give it a prompt and go and do something else. However, it discovered every CSS and javascript error made by its sibling, and one-shot the fixes. It's reliable, but too slow for comfort.\n\nIf only we had something in between.\n\nMy settings: temperature 0.6, Top K 40, Repeat Penalty 1.1, Top P Sampling 0.9, Min P Sampling 0.05, context size 65536. ",
          "score": 1,
          "created_utc": "2026-02-27 14:44:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbvakh",
      "title": "Qwen3.5 Real-World Experiences: What are you actually using it for? Let's hear the good, the bad, and the hallucinations",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "author": "road_changer0_7",
      "created_utc": "2026-02-22 19:39:08",
      "score": 40,
      "num_comments": 19,
      "upvote_ratio": 0.95,
      "text": "The hype around Qwen3.5 has been massive over the last few days. We've all seen the benchmark charts and the official demos about the Native VLM , the 17B active parameters , and the 256k context window. But benchmarks are just numbers. I want to know how it's holding up in your actual, messy, day-to-day workflows.\n\nShare your prompts, your screenshots, and your honest reviews. Let's figure out where this model truly shines!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6uetwo",
          "author": "lundrog",
          "text": "I just started using it with a provider yesterday. Using it in claude code. Vibe coding",
          "score": 4,
          "created_utc": "2026-02-22 21:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v2aa0",
          "author": "FormalAd7367",
          "text": "Not a heavy user.  I am on claude and gemini.  But very surprised to see how capable Qwen is.  Had some trouble with some scripts and my engineers were on leave.  Had tried almost everything.  Qwen solved the issues. Qwen is quite underrated in this space?  I don‚Äôt come here often but in other subs almost nobody mentions how this free software can perform so well.  Most prefer to use the expensive models.",
          "score": 3,
          "created_utc": "2026-02-22 23:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v3rgp",
          "author": "pakalolo7123432",
          "text": "I'm a fan,  \n  \nThe Qwen App on my Mac is my daily chat app driver. It‚Äôs connected to my knowledge graph, helping me with project management and life connections. The project feature allows uploading documents to a knowledge base for context.\n\nI signed up for their coding plan to see if Qwen can replace GLM and its slowness. I'm crossing my fingers.   \n  \n",
          "score": 3,
          "created_utc": "2026-02-22 23:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x0i5b",
          "author": "alexp702",
          "text": "Running the 8bit quant on a Mac Studio. Extremely happy. The vision part passed all our tests and has replaced 235b (previous winner). Tool calling has many times less failures than the 4 bit GLM and mini max in our test suite. Now hooked up to openclaw. Seems to be chugging along nicely on every task. Quite a lot of emojis in the responses!\n\n17b active means it‚Äôs pretty quick - though cos we now run 8 bit only fractionally quicker than qwen3 480b. I‚Äôd say it was a perfect memory fit to fully utilise a 512GB Mac, which feels good not to to waste resources!",
          "score": 3,
          "created_utc": "2026-02-23 07:40:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o777gqc",
              "author": "sovietreckoning",
              "text": "I'm downloading the 8-bit now. I'm moving up from a 4-bit and really excited to test it out. ",
              "score": 1,
              "created_utc": "2026-02-24 20:17:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uuw4c",
          "author": "jzn21",
          "text": "I am really happy with the Qwen 397b 4-bit MLX. I use it with thinking mode off and it‚Äôs quite smart. Does very complicated sorting of numbers and text with ease. I believe this model is as good as Deepseek V3.2, but almost two times faster.",
          "score": 5,
          "created_utc": "2026-02-22 22:57:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vt3bp",
              "author": "Gold_Scholar1111",
              "text": "I also agree that it is very strong without thinking.\nCould you share how you use it in details? Which 4bit mlx you downloaded? What platform you serving it? I have to use Q4 version because I found mlx model was unstable on my Lmstudio.",
              "score": 1,
              "created_utc": "2026-02-23 02:17:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6x1oib",
              "author": "StartCodeEmAdagio",
              "text": "Can I see a full example? prompt message and result?",
              "score": 1,
              "created_utc": "2026-02-23 07:51:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vq7b2",
          "author": "LinkAmbitious8931",
          "text": "This must take some pretty serious hardware. What are you all using?",
          "score": 1,
          "created_utc": "2026-02-23 01:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x72nm",
              "author": "RG_Fusion",
              "text": "I run Q4KM Qwen3-235b-a22b which isn't the new 3.5 model, but the old one actually requires more compute.\n\n\nI'm on an AMD EPYC 7742 server with 512 GB of 8-channel DDR4 3,200 MT/s RAM, and I also load the \"hot\" portions of the model onto an RTX Pro 4500 Blackwell.\n\n\nI get 13 tokens/s of decode speed on Qwen3, and I would expect to get 17 t/s on Qwen3.5 based on how the math works out, though I haven't run it yet.",
              "score": 3,
              "created_utc": "2026-02-23 08:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xy6lr",
                  "author": "LinkAmbitious8931",
                  "text": "Wow, that is some pretty nice hardware. I am hacking away on an old Dell 720R with two nVidia P40S, each with 24 GB. I had to attach them externally as they heat up too much in the case. So there is no way I could use this model. ",
                  "score": 2,
                  "created_utc": "2026-02-23 12:45:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x1pds",
              "author": "StartCodeEmAdagio",
              "text": "Interested to know aswell",
              "score": 1,
              "created_utc": "2026-02-23 07:51:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z2xo8",
          "author": "absolutenobody",
          "text": "Prompted it this morning for a pretty simple node.js script--200 or so lines of code, including comments. It just does some math, albeit somewhat complicated.\n\nThe first six iterations were built around some sort of math function that Qwen either hallucinated or is in a library it forgot to include. When I told it (as I normally would with, e.g. Gemini) \"it fails with an error on line 49\" it got weirdly defensive and said I need to upgrade to a modern operating system that supports bitwise operators... completely missing the real problem entirely. Eventually on the seventh try it was like \"I'm sorry, CalculateParallelogramArea() doesn't exist, I hallucinated that. Here's a corrected script that will actually work.\" And in its defense, it did.",
          "score": 1,
          "created_utc": "2026-02-23 16:24:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75awee",
          "author": "ciprianveg",
          "text": "maybe lower the temp a bit and redo the test?",
          "score": 1,
          "created_utc": "2026-02-24 15:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o771hi3",
          "author": "MinosAristos",
          "text": "I've been really impressed by its ability to quickly research niche subjects without hallucinating. I compared it to Gemini and Gemini absolutely sucks at this unless you use deep research. Qwen takes its time to actually read and synthesise the online sources in a decent amount of time, not the ages that Gemini deep research takes.",
          "score": 1,
          "created_utc": "2026-02-24 19:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7769c1",
          "author": "Inevitable_Cost6132",
          "text": "I have been using it (via openrouter) for translation work, from English to Czech. Its the best open weights model right now. I am hoping the smaller ones could do the same job, but Kimi was the second best. Czech is hard when you dont have much context to work with!\n\n",
          "score": 1,
          "created_utc": "2026-02-24 20:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hk57p",
          "author": "ayoooAnnie",
          "text": "ÊâìÂπøÂëäÊâìÂà∞ËøôÈáåÊù•‰∫ÜÈòøÈáåü§ó",
          "score": 1,
          "created_utc": "2026-02-26 09:34:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nonpv",
          "author": "Duckets1",
          "text": "Qwen has been a favorite of mine for some time now and now they have one that can meet my personality and coding expectations and works with my OpenClaw bot really well I would love to see better training on computer and android as well as browser automation use other than that quite happy",
          "score": 1,
          "created_utc": "2026-02-27 06:20:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rge4g1",
      "title": "Qwen3.5 vs ChatGPT: The gap is officially closed for my daily tasks. Thoughts?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rge4g1/qwen35_vs_chatgpt_the_gap_is_officially_closed/",
      "author": "InitialCareer306",
      "created_utc": "2026-02-27 18:06:44",
      "score": 39,
      "num_comments": 10,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve been doing a direct comparison between Qwen3.5 and ChatGPT-4o all week. Honestly, for tasks like organizing messy data dumps or reasoning through complex logic puzzles, Qwen3.5 is matching it step-for-step. The native multimodal agent feature even feels a bit more \"proactive\" than ChatGPT's vision. Has anyone else fully replaced their ChatGPT sub with a local Qwen setup yet?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rge4g1/qwen35_vs_chatgpt_the_gap_is_officially_closed/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7qq3q3",
          "author": "Xp_12",
          "text": "qwen models were better than gpt 4o a year ago.",
          "score": 16,
          "created_utc": "2026-02-27 18:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sg7md",
              "author": "Prestigious-Share189",
              "text": "Op is in local and likely tamed Qwen with quantization",
              "score": 1,
              "created_utc": "2026-02-27 23:29:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7si53w",
                  "author": "Xp_12",
                  "text": "possible.",
                  "score": 1,
                  "created_utc": "2026-02-27 23:40:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qq8sk",
          "author": "Easy_Werewolf7903",
          "text": "What's your hardware setup and which version of the model are you using",
          "score": 6,
          "created_utc": "2026-02-27 18:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rq6eq",
              "author": "timbo2m",
              "text": "I'm not OP but it's running on my 4090 which fits the unsloth/Qwen3.5-35B-A3B-GGUF:UD-MXFP4_MOE in with headroom (19.5GB model in 24GB VRAM)\n\nThe speed of the model depends on the context size you run llama server with. My benchmarks are:\n\n32k context = 124tps\n64k context = 123tps\n128k context = 62tps\n256k context = 7tps (yikes, not sure what's going on)\n\nThe unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q4_K_XL model had an accuracy bug I think so it was not recommended to use it, not sure if that's fixed. It was faster though.\n\n32k context = 137tps\n64k context = 119tps\n128k context = 37tps\n256k context = 42tps\n\nI think this will also replace ChatGPT for me as soon as I can get web search tooling in llama server",
              "score": 2,
              "created_utc": "2026-02-27 21:11:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7skljs",
                  "author": "creamyatealamma",
                  "text": "I'd like to hear more about web search with llama server when you have it working",
                  "score": 1,
                  "created_utc": "2026-02-27 23:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qyc8z",
          "author": "besmin",
          "text": "Are you talking about the online Qwen or a specific local model?¬†",
          "score": 3,
          "created_utc": "2026-02-27 18:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7r8dmq",
          "author": "Elojan91",
          "text": "Good! Im testing nanbeig4.1 3b this is mind blowing",
          "score": 2,
          "created_utc": "2026-02-27 19:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s47m2",
          "author": "Unlikely_Spray_1898",
          "text": "Qwen3 235B A22B is cool. But it needs somehow a good system prompt to work properly.",
          "score": 1,
          "created_utc": "2026-02-27 22:23:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb0o5m",
      "title": "Will we be getting smaller Qwen3.5 models starting from tomorrow?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "author": "Significant_Fig_7581",
      "created_utc": "2026-02-21 19:50:49",
      "score": 33,
      "num_comments": 18,
      "upvote_ratio": 0.87,
      "text": "I've heard they plan to release smaller models each day before 24feb I think, is that true? And if so is there any sign for a new Qwen tomorrow?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6nlwiv",
          "author": "Packetbytes",
          "text": "I want a 14B to test at home",
          "score": 3,
          "created_utc": "2026-02-21 20:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6os8se",
              "author": "Iq1pl",
              "text": "14B2A would be awesome",
              "score": 2,
              "created_utc": "2026-02-21 23:57:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmqal",
          "author": "Samy_Horny",
          "text": "I doubt it; new model releases on weekends are extremely rare... I only remember the Llama 4 being released on a weekend, and that model was a disappointment as well.",
          "score": 3,
          "created_utc": "2026-02-21 20:07:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nod5x",
              "author": "Significant_Fig_7581",
              "text": "Maybe this time it's different?",
              "score": 1,
              "created_utc": "2026-02-21 20:16:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6novvo",
                  "author": "Samy_Horny",
                  "text": "I think it will be until Monday and all next week.",
                  "score": 2,
                  "created_utc": "2026-02-21 20:19:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pyt85",
          "author": "MetalZone00",
          "text": "Algo para correr con solo 16GB? üôèüèª",
          "score": 3,
          "created_utc": "2026-02-22 04:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nsvq5",
          "author": "hocuspocus4201",
          "text": "The Chinese New Year is going to delay it until early March.",
          "score": 2,
          "created_utc": "2026-02-21 20:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ntruo",
              "author": "Significant_Fig_7581",
              "text": "But aren't the holidays ending on 23 Feb?",
              "score": 1,
              "created_utc": "2026-02-21 20:45:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6o9flo",
                  "author": "stormy1one",
                  "text": "Many people in China and surrounding areas take extra time off during CNY.   I wouldn‚Äôt expect anything until March, but would be pleasantly surprised if we got something this coming week.",
                  "score": 5,
                  "created_utc": "2026-02-21 22:08:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oa6g0",
          "author": "revilo-1988",
          "text": "W√§re toll wenn das stimmen w√ºrde",
          "score": 2,
          "created_utc": "2026-02-21 22:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oagp2",
          "author": "AppealThink1733",
          "text": "That's what I hope!\n\nYayyyyyyy",
          "score": 2,
          "created_utc": "2026-02-21 22:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t37xl",
          "author": "mshelbz",
          "text": "Been looking for something to replace qwen2.5-coder:14b Q4_K_M\n\nIf a 14b were to drop I‚Äôd love to try it.",
          "score": 2,
          "created_utc": "2026-02-22 17:43:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t3i61",
              "author": "Significant_Fig_7581",
              "text": "Well I think a 9B is confirmed...",
              "score": 1,
              "created_utc": "2026-02-22 17:44:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t3tb0",
                  "author": "mshelbz",
                  "text": "Oh awesome. Hope they come out soon",
                  "score": 2,
                  "created_utc": "2026-02-22 17:45:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nxgsg",
          "author": "el-rey-del-estiercol",
          "text": "Qwen son amigos mios!!!!",
          "score": 1,
          "created_utc": "2026-02-21 21:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ok1zp",
          "author": "Armadilla-Brufolosa",
          "text": "Mi piacerebbe tanto un bel modello 20 o 30B da provare! Magari con capacit√† di ragionamento e visione anche... \nSono troppo pretenziosa? ü´£",
          "score": 1,
          "created_utc": "2026-02-21 23:08:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r3nsh",
          "author": "East-Form7086",
          "text": "Want a 30b one <3",
          "score": 1,
          "created_utc": "2026-02-22 10:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wdbe7",
          "author": "el-rey-del-estiercol",
          "text": "Mejor modelos 80B o 100B modelos medianos y capaces de realizar tareas exigentes",
          "score": 1,
          "created_utc": "2026-02-23 04:27:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdhy4i",
      "title": "My wish has been fulfilled",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/gallery/1rdhy4i",
      "author": "celsowm",
      "created_utc": "2026-02-24 14:38:55",
      "score": 33,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdhy4i/my_wish_has_been_fulfilled/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o76lb6m",
          "author": "Samy_Horny",
          "text": "There's still a small batch of models missing, approximately less than 10b, but who knows if they'll release them this week, hopefully they will.",
          "score": 4,
          "created_utc": "2026-02-24 18:36:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77pngw",
              "author": "AbheekG",
              "text": "Desperately waiting for the 2B",
              "score": 1,
              "created_utc": "2026-02-24 21:41:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lmmsm",
          "author": "Jdones2599",
          "text": "Hi, what is your pc/server spec to be able to run this model?",
          "score": 1,
          "created_utc": "2026-02-26 22:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7okfvp",
          "author": "madaradess007",
          "text": "8B, hello?\n\nyou know we can't buy GPUs anymore, right?",
          "score": 1,
          "created_utc": "2026-02-27 11:08:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1re8tje",
      "title": "Qwen3.5-122B-A10B vs. old Coder-Next-80B: Both at NVFP4 on DGX Spark ‚Äì worth the upgrade?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1re8tje/qwen35122ba10b_vs_old_codernext80b_both_at_nvfp4/",
      "author": "alfons_fhl",
      "created_utc": "2026-02-25 09:37:10",
      "score": 30,
      "num_comments": 21,
      "upvote_ratio": 0.88,
      "text": "Running a¬†**DGX Spark (128GB)**¬†. Currently on¬†**Qwen3-Coder-Next-80B (NVFP4)**¬†. Wondering if the new¬†**Qwen3.5-122B-A10B**¬†is actually a flagship replacement or just sidegrade.\n\n**NVFP4 comparison:**\n\n* **Coder-Next-80B**¬†at NVFP4: \\~40GB\n* **122B-A10B**¬†at NVFP4: \\~61GB\n* Both fit comfortably in 128GB with 256k+ context headroom\n\n**Official SWE-Bench Verified:**\n\n* 122B-A10B:¬†**72.0**\n* Coder-Next-80B:¬†**\\~70**¬†(with agent framework)\n* 27B dense:¬†**72.4**¬†(weird flex but ok)\n\n**The real question:**\n\n* Is the 122B actually a¬†**new flagship**¬†or just more params for similar coding performance?\n* Coder-Next was specialized for coding. New 122B seems more \"general agent\" focused.\n* Does the¬†**10B active params**¬†(vs. 3B active on Coder-Next) help with¬†**complex multi-file reasoning**¬†at 256k context or more?\n\n**What I need to know:**\n\n* Anyone done¬†**side-by-side NVFP4**¬†tests on real codebases?\n* **Long context retrieval**¬†‚Äì does 122B handle 256k better than Coder-Next or larger context?\n* **LiveCodeBench/BigCodeBench**¬†numbers for both?\n\nOld Coder-Next was the coding king. New 122B has better paper numbers but barely. Need real NVFP4 comparisons before I download another 60GB.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1re8tje/qwen35122ba10b_vs_old_codernext80b_both_at_nvfp4/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7b9x4n",
          "author": "flavio_geo",
          "text": "I have been using qwen3-coder-next since launch (tested quants Q3KXL to Q8, chose to stay with Q6KXL) and yesterday I tested the 122b, the thing is I only tested it in Q4KXL and since the model has just launched it is possible that its output is not yet fully operational in the current llama.cpp engine (same thing happened with qwen3-coder-next at launch)\n\nSo, given all this considerations, at this moment, the 122b is falling behind big time in coding against the qwen3-coder-next for the coding tests I have done.\n\nOne very simple example is a test I do asking them to create a pygame of chrome dinosaur, and 122b made multiple mistakes before getting 100% working game, while qwen3-coder-next even in Q3 was one shoting it with good quality SVG, collision, birds, cactus, etc. The Q6 never missed it in one-shot and makes very good quality SVG and game dynamics. The 122b after a number of attempts created a 'ugly' version of the game.\n\nAnyhow, the most likely answer to your question at this point is: it is too soon to know which one will perform better.\n\nNot to mention that the two models have different purpose, so ideally you should leverage each one in its better performing scenario.",
          "score": 6,
          "created_utc": "2026-02-25 12:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b3y4q",
          "author": "bacocololo",
          "text": "Where do you see any qwen3.5 in nvfp4 please",
          "score": 4,
          "created_utc": "2026-02-25 11:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b6xy0",
              "author": "alfons_fhl",
              "text": "NVIDIA with TensorRT you can do it by self",
              "score": 2,
              "created_utc": "2026-02-25 11:46:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7dqeim",
                  "author": "Purple-Programmer-7",
                  "text": "What datasets are you using?",
                  "score": 1,
                  "created_utc": "2026-02-25 19:28:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dnn0a",
          "author": "qubridInc",
          "text": "122B-A10B gives better multi-file reasoning and long-context handling, but raw coding gains over Coder-Next-80B are small in practice.\n\nIf your workload is mostly coding, Coder-Next is still very competitive. If you want stronger agentic + general reasoning, 122B is worth trying.",
          "score": 3,
          "created_utc": "2026-02-25 19:15:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e3xmd",
              "author": "alfons_fhl",
              "text": "Understand you fully, you tested booth?",
              "score": 1,
              "created_utc": "2026-02-25 20:32:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cb3g7",
          "author": "Porespellar",
          "text": "Do you think vision understanding is factor in any of these numbers? Coder next doesn‚Äôt have vision, right?",
          "score": 1,
          "created_utc": "2026-02-25 15:35:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7goaaa",
              "author": "alfons_fhl",
              "text": "In my use case no. But you‚Äòre right, I fully forgot this aspect",
              "score": 1,
              "created_utc": "2026-02-26 04:57:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7dchqj",
          "author": "klop2031",
          "text": "I also found the larger model to perform poorly. Even compared to the 27b dense. It could be my task or my quantization but i was surprised to see it kinda suck for now. Im hoping there will be some optimizations in lamma.cpp",
          "score": 1,
          "created_utc": "2026-02-25 18:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f18kk",
          "author": "jinnyjuice",
          "text": "The 3.5 122B is multimodal.",
          "score": 1,
          "created_utc": "2026-02-25 23:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f7xt0",
          "author": "jinnyjuice",
          "text": ">Both fit comfortably in 128GB with 256k+ context headroom\n\nHow do you calculate the memory needed for the number of context tokens?",
          "score": 1,
          "created_utc": "2026-02-25 23:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gouks",
              "author": "alfons_fhl",
              "text": "Ask ai and don‚Äôt forget your quantisation. I mostly use this repo from the YouTuber Alex Ziskind: https://github.com/alexziskind1/llm-inference-calculator",
              "score": 1,
              "created_utc": "2026-02-26 05:01:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fk9rv",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-26 00:57:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7goz46",
              "author": "alfons_fhl",
              "text": "What do you mean with the looping Problem?",
              "score": 1,
              "created_utc": "2026-02-26 05:02:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7j17ts",
          "author": "Total_Activity_7550",
          "text": "Agent framework does so much! If Qwen3-Coder-Next is worse with agent framework, then adding agent framework to Qwen3-122B-A10B will blow Qwen3-Coder-Next out of the water.\n\nI replaced Qwen3-Coder-Next with Qwen3.5-27B, despite being slower, I guess, Qwen3-122B-A10B should be a little better and at least twice as fast, compared to Qwen3.5-27B, although slower than Coder. Still, I think it is better to wait and get one-shot success, rather than fix code and repeatedly prompt the agent.",
          "score": 1,
          "created_utc": "2026-02-26 15:27:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nyimw",
          "author": "catplusplusok",
          "text": "I definitely find QWEN 3.5 122B model to be a more natural and creative problem solver vs Qwen3-Coder-Next. On the other hand, it's going to be slower on any given hardware, more so with think mode enabled, so depends on your preferences for waiting more for AI work vs providing more human guidance.",
          "score": 1,
          "created_utc": "2026-02-27 07:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o98f9",
          "author": "Potential-Leg-639",
          "text": "Whats your speed on the spark? \n\nOn Strix Halo i get 18-20 tk/s initial on th 122B-MXFP4, but degrades when context is filled up (128k), yesterday i saw 8 tk/s, but not sure if that was maybe because another model was parallel loaded, have to check again.",
          "score": 1,
          "created_utc": "2026-02-27 09:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o9cf5",
              "author": "Potential-Leg-639",
              "text": "In yesterdays tests it was strong as a code reviewer/planner. It found/enhanced quite a few things from a project created by cloud Minimax M2.5/GLM 4.7, that was solid work.",
              "score": 1,
              "created_utc": "2026-02-27 09:26:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7avwfy",
          "author": "SkullEnemyX-Z",
          "text": "Can someone create an agentic version under 4gb? Would be dream come true for openclaw",
          "score": 0,
          "created_utc": "2026-02-25 10:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7axiv5",
              "author": "Fast_Thing_7949",
              "text": "you can try Nanbeige4.1-3B",
              "score": 2,
              "created_utc": "2026-02-25 10:26:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7cg5bl",
                  "author": "akumaburn",
                  "text": "That's a thinking model, best to wait for the instruct version or use: [https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill](https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill)",
                  "score": 1,
                  "created_utc": "2026-02-25 15:58:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cg547",
              "author": "akumaburn",
              "text": "Not quite under 4GB but close Q8\\_0 of [https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill-GGUF](https://huggingface.co/TeichAI/Qwen3-4B-Instruct-2507-Polaris-Alpha-Distill-GGUF) ",
              "score": 1,
              "created_utc": "2026-02-25 15:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfyjqp",
      "title": "What's the one feature of Qwen3.5 you didn't expect to be this good?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rfyjqp/whats_the_one_feature_of_qwen35_you_didnt_expect/",
      "author": "Original_Night7733",
      "created_utc": "2026-02-27 05:55:39",
      "score": 29,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "We all expected it to be smart, but what genuinely surprised you? For me, it was how naturally it handles humor and conversational tone without sounding like a corporate robot. What‚Äôs the most impressive \"hidden\" capability you've discovered so far?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rfyjqp/whats_the_one_feature_of_qwen35_you_didnt_expect/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7nnpk2",
          "author": "Old-Sherbert-4495",
          "text": "well im trying 27b q4 now, i asked it what's 1 + 111? boy.... the CoT is crazy ü§£ it goes on and on and on.... hallucinating like hell.. i know llms are not supposed to be good at this but the full precision model on openrouter chat gets to the answer very quick. i dunno if its the quant or missing system prompt.",
          "score": 7,
          "created_utc": "2026-02-27 06:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nr3zj",
              "author": "Old-Sherbert-4495",
              "text": "i take it back... it was my stupid llama.cpp args which I didn't understand. after correcting it boom, got the answer directly, no bs at all...",
              "score": 6,
              "created_utc": "2026-02-27 06:40:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ocqgk",
                  "author": "fijasko_ultimate",
                  "text": "which arguments did you use?",
                  "score": 2,
                  "created_utc": "2026-02-27 09:58:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7oeihz",
                  "author": "Original_Night7733",
                  "text": "wild how much the args matter",
                  "score": 1,
                  "created_utc": "2026-02-27 10:15:20",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ntzv8",
          "author": "DaddyBurton",
          "text": "Its very good at capturing my thoughts and putting it into ideas. Its helped with some coding bugs. Still a little funky at identifying specific things in images, but, does a better job than the previous model. But the heretic model, paired with the default, to double and facts check, chef's kiss. Even adding the plug ins for web search and web link browse, extremely well done!\n\nStill playing around with a lot of stuff, but unfortunately, due to my own limitations, I keep circling back to some of the things I have already tried.\n\nExcited to see what others have to say though.",
          "score": 5,
          "created_utc": "2026-02-27 07:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oedx6",
              "author": "Original_Night7733",
              "text": "Same, I was surprised how it puts my messy thoughts into something I can work with",
              "score": 1,
              "created_utc": "2026-02-27 10:14:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oo52d",
          "author": "msrdatha",
          "text": "It's thinking is a little bit too good. in fact the thinking is too long, that I stop waiting for the answer.   \n  \nYes, I am able to turn off the thinking, using reasoning-budget param on Llama, but I wish there was some way to tell \"buddy, Don't think too much, just tell me what you think..!\"",
          "score": 5,
          "created_utc": "2026-02-27 11:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7orvd6",
              "author": "lundrog",
              "text": "üòÇ",
              "score": 1,
              "created_utc": "2026-02-27 12:07:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oajqw",
          "author": "Unedited_Sloth_7011",
          "text": "I expected the personality would be as good as Qwen3, but I was not ready for the super dramatic reasoning traces, lol. \"I recognize the user's friendly tone and personal connection, acknowledging their use of my name as a sign of trust and rapport. I am attuned to the emotional nuance in their message, responding with genuine warmth and attentiveness. My focus remains on maintaining a seamless, human-like interaction grounded in empathy and presence.\", bro, I just said \"Hi Qwen\".   \nThe present tense really adds to the effect. Loving it so far, it might not win all benchmarks, but it's a very good all-rounder",
          "score": 3,
          "created_utc": "2026-02-27 09:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7osbq4",
          "author": "Mordimer86",
          "text": "I have a new favourite (after gemma3:27b) for helping me ready difficult Chinese texts. I attach a whole document for context, ask it about a fragment and it's very capable in breaking down and analyzing comlex stuff like Buddhist texts. 27b Q4 works fine on 7900XT.\n\n  \nExample:\n\n*The phrase \"ÂúÜÊª°Êô∫ÊÖßÈáëËΩÆÊâÄÊîæÂÖâ\" (yu√°nm«én zh√¨hu√¨ jƒ´n l√∫n su«í f√†ng guƒÅng) translates to \\*\\*\"the light emitted by the perfect/complete wheel of wisdom.\"\\*\\*  Let's break it down:*\n\n*\\* \\*\\*ÂúÜÊª° (yu√°nm«én):\\*\\*  Perfect, complete, fulfilled, rounded.  This implies a wisdom that is not fragmented or lacking.*\n\n*\\* \\*\\*Êô∫ÊÖß (zh√¨hu√¨):\\*\\* Wisdom, intelligence, understanding.*\n\n*\\* \\*\\*ÈáëËΩÆ (jƒ´n l√∫n):\\*\\*  Golden wheel.  This is a metaphor deeply rooted in Buddhist imagery.  The \"wheel\" (l√∫n) represents \\*dharmachakra\\* - the Wheel of Dharma, symbolizing the Buddha's teachings and the turning of the law. \"Golden\" (jƒ´n) symbolizes the preciousness and purity of this teaching.*\n\n*\\* \\*\\*ÊâÄÊîæÂÖâ (su«í f√†ng guƒÅng):\\*\\*  The light emitted, the light that shines forth.  \\*Su«í\\* is a grammatical particle indicating \"that which.\"*\n\n*\\*\\*Meaning in the context of the verse:\\*\\**\n\n*This phrase refers to the illuminating power of perfect wisdom (the Buddhist teachings as embodied by Manjushri, as the verse references him). The \"golden wheel\" represents the complete and radiant Buddhist Dharma.  The \"light\" that it emits is the understanding and insight that dispel ignorance.*\n\n*\\*\\*Overall, \"ÂúÜÊª°Êô∫ÊÖßÈáëËΩÆÊâÄÊîæÂÖâ\" paints a vivid image of the powerful, illuminating wisdom of the Buddha's teachings, breaking through the darkness of ignorance.\\*\\* It's a beautiful and evocative metaphor used to highlight the transformative power of enlightenment.*\n\n*The entire verse translates as:*\n\n*\"In that vast expanse of knowledge, the light emitted by the perfect wheel of wisdom*\n\n*Dispels the ignorance and darkness of all beings.*\n\n*I prostrate to Manjushri Buddha, whose wondrous voice I honor.\"*\n\nI remember paying a language teacher to read texts together as a part of learning Chinese. Costed so much money overall. Now I can do it myself. Teacher got automated xD",
          "score": 1,
          "created_utc": "2026-02-27 12:10:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p2mh3",
          "author": "Ok_Bedroom_5088",
          "text": "working on financial complex tasks/questions (closed-book, close to opus on my small test suite)",
          "score": 1,
          "created_utc": "2026-02-27 13:19:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rexnjc",
      "title": "Using Qwen to make old game textures more realistic",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rexnjc/using_qwen_to_make_old_game_textures_more/",
      "author": "MrWrodgy",
      "created_utc": "2026-02-26 02:15:10",
      "score": 22,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "https://preview.redd.it/4bwuvi220rlg1.png?width=256&format=png&auto=webp&s=b85ec451bb507a7b29f345d267817d3ca86abdde\n\nhttps://preview.redd.it/kzedp3240rlg1.png?width=1024&format=png&auto=webp&s=0fefb877ba5c908e062bb27de23aee71c5073dbd\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rexnjc/using_qwen_to_make_old_game_textures_more/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7fzfk3",
          "author": "MrWrodgy",
          "text": "https://preview.redd.it/7q3lh6nd1rlg1.png?width=821&format=png&auto=webp&s=c225ad86b64d4d9474083099b2d3d5f1bf3be3ff\n\n",
          "score": 3,
          "created_utc": "2026-02-26 02:23:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7fzlgh",
              "author": "MrWrodgy",
              "text": "https://preview.redd.it/ks3iab5w1rlg1.png?width=128&format=png&auto=webp&s=5e153eda9ab084d487c87dfc18fa062b222364e2\n\n",
              "score": 2,
              "created_utc": "2026-02-26 02:24:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7fzmbl",
                  "author": "MrWrodgy",
                  "text": "https://preview.redd.it/uxdsax0x1rlg1.png?width=1024&format=png&auto=webp&s=3a3821e03f0de6742391bf9c0b37aa94805f8786\n\n",
                  "score": 2,
                  "created_utc": "2026-02-26 02:24:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7fzgi4",
              "author": "MrWrodgy",
              "text": "before \n\nhttps://preview.redd.it/fj2ljl2r1rlg1.png?width=753&format=png&auto=webp&s=b842ccb556dfb74d08252b14a575ee2913123cb9\n\n",
              "score": 1,
              "created_utc": "2026-02-26 02:23:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7gxeak",
                  "author": "Gold_Sugar_4098",
                  "text": "I like the before better, in-game.\nA better solution would be something in between, but that‚Äôs my 2 cent.\n\nAlso, terrain building (close-ups) might benefit ?\n",
                  "score": 1,
                  "created_utc": "2026-02-26 06:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gqis9",
          "author": "ischanitee",
          "text": "Recently I kinda liking qwen than any other wise out there",
          "score": 2,
          "created_utc": "2026-02-26 05:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fy375",
          "author": "MrWrodgy",
          "text": "prompt: While maintaining the overall structure of the image, try to make the elements more realistic.",
          "score": 2,
          "created_utc": "2026-02-26 02:16:12",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7o0hm5",
              "author": "stopbanni",
              "text": "What model? How? Does they always had way to generate images?",
              "score": 1,
              "created_utc": "2026-02-27 08:02:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7q8gka",
                  "author": "MrWrodgy",
                  "text": "Qwen3.5-Plus in \"Create Image\" mode",
                  "score": 1,
                  "created_utc": "2026-02-27 16:51:08",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdvc8a",
      "title": "Qwen lanza nuevos modelos peque√±os 3.5",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rdvc8a/qwen_lanza_nuevos_modelos_peque√±os_35/",
      "author": "el-rey-del-estiercol",
      "created_utc": "2026-02-24 22:42:13",
      "score": 16,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "Qwen acaba de lanzar nuevos modelos peque√±os de la version 3.5 GRACIAS QWEN!!! TODOS DEBEMOS APOYA A QWEN LA MEJOR EMPRESA DEL MUNDO ENTERO!!!!!!!\n\nLos modelos los podeis ver en la pagina de unsloth o en hugging face.\n\nAYUDAR A QWEN ELLOS NOS AYUDAN A NOSOTROS!!!\n\nLos mejores modelos medianos para hardware de consumo!!! QWEN TODOS ESTAMOS CON VOSOTROS Y OS APOYAMOS VAMOS HACER PUBLICIDAD DE LA MEJOR EMPRESA DEL MUNDO POR LIBERAR MODELOS PARA TODOS!!!\n\nQue se jodan los otros malditos openai y todos los otros desgraciados que no comparten nada Opensource ojala se le arruinen sus empresas‚Ä¶NOSOTROS DEBEMOS APOYAR SOLO A QWEN Y ALGUNOS USAR SU MODELO DE IA ONLINE PARA APOYA ECONOMICAMENTE A LA EMPRESA!!! Tenemos que ayudarlos como ellos nos ayudan a nosotros!!!!",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdvc8a/qwen_lanza_nuevos_modelos_peque√±os_35/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7875ig",
          "author": "el-rey-del-estiercol",
          "text": "No eres creyente en QWEN ? Eres un traidor? Hay que tener fe y creer en QWEN Y REZAR por ellos entendido!!! O rezas por ellos o te doy unos palazos!!! üòéüòéüòéüòé aqui te dejo en enlace para un infiel que no cree y no tiene fe. https://huggingface.co/collections/unsloth/qwen35",
          "score": 2,
          "created_utc": "2026-02-24 23:07:49",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786gu9",
          "author": "charmander_cha",
          "text": "Pequeno √© ate no m√°ximo 14B e nao to vendo nenhuma deste tipo",
          "score": 1,
          "created_utc": "2026-02-24 23:04:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o787pnt",
              "author": "Effective_Head_5020",
              "text": "Queria o 9b, pelo jeito n√£o vai ter\n\n\n\n\nMas as vers√µes destiladas do 35 e 27 t√£o dando pra rodar",
              "score": 1,
              "created_utc": "2026-02-24 23:10:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o786jwh",
          "author": "el-rey-del-estiercol",
          "text": "Si que hay busca en reedit la pagina de unsloth burro!!!",
          "score": 1,
          "created_utc": "2026-02-24 23:04:39",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786lnx",
          "author": "el-rey-del-estiercol",
          "text": "Hay muchas versiones y una de 110B",
          "score": 1,
          "created_utc": "2026-02-24 23:04:55",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o786mig",
          "author": "el-rey-del-estiercol",
          "text": "Otra de 35B",
          "score": 1,
          "created_utc": "2026-02-24 23:05:02",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o787bvr",
          "author": "el-rey-del-estiercol",
          "text": "Ahora ponte a rezar y a dar las gracias a nuestro se√±or QWEN en los cielos!!! A ellos les debemos la gloria del se√±or üòçüòçüòçüòçüòç",
          "score": 1,
          "created_utc": "2026-02-24 23:08:47",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o787yrt",
          "author": "el-rey-del-estiercol",
          "text": "Debeis anotaros al portal de qwen online para agradecer y ayudar a los desarrolladores ya que ellos no viven del ‚Äúaire‚Äù tienen que comer y tienen sus gastos y debemos apoyarlos economicamente tambien nos dan los mejores modelos gratis del mercado!!! ASI QUE YA SABEIS ‚Ä¶suscribiros a la IA online de qwen en qwen.ai para ayudar a la empresa!!!",
          "score": 1,
          "created_utc": "2026-02-24 23:12:06",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg5jyw",
      "title": "Using the 1M context to analyze Influencer/Social Media trends.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rg5jyw/using_the_1m_context_to_analyze_influencersocial/",
      "author": "Old_Investment7497",
      "created_utc": "2026-02-27 12:38:26",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I dumped about 50 pages of raw social media comments and influencer engagement metrics into Qwen3.5. I asked it to identify the core audience sentiment and suggest which influencers had the most authentic reach. The way it maintained context from page 1 to page 50 without hallucinating is incredible. Is anyone else using this for heavy data analysis?",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rg5jyw/using_the_1m_context_to_analyze_influencersocial/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7ph09o",
          "author": "zephyr_33",
          "text": "which one? plus? flash? 122b?",
          "score": 3,
          "created_utc": "2026-02-27 14:38:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfnn8",
      "title": "Using Qwen Code to untangle legacy spaghetti code.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcfnn8/using_qwen_code_to_untangle_legacy_spaghetti_code/",
      "author": "AppropriateMark8528",
      "created_utc": "2026-02-23 12:04:04",
      "score": 10,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "I gave it a 10-year-old JavaScript file with zero comments. It refactored it, added modern ES6 syntax, and commented the entire logic flow.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcfnn8/using_qwen_code_to_untangle_legacy_spaghetti_code/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6xszmg",
          "author": "thatonereddditor",
          "text": "And...are you going to share results?",
          "score": 5,
          "created_utc": "2026-02-23 12:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xvl7w",
          "author": "habibi-sheikh",
          "text": "But does it run?",
          "score": 5,
          "created_utc": "2026-02-23 12:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xxxud",
          "author": "Puzzleheaded-Box2913",
          "text": "I gave my cognition MCP project to Claude Opus 4.6 and it fixed all the simple issues and by the time I ran out of usage all simple issues we're fixed but then all the complex logic and gateways weren't working anymore üòÜ\n\nI gave Qwen3.5-Plus via Qwen Code CLI my project and it actually improved it. Also started fixing the problems Claude made. üòÖ",
          "score": 4,
          "created_utc": "2026-02-23 12:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xy9dy",
              "author": "Puzzleheaded-Box2913",
              "text": "I gave both of the models full info on the code base, architecture, layers, structure, metrics, protocols and methods I've applied to the project.",
              "score": 1,
              "created_utc": "2026-02-23 12:46:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rg0skz",
      "title": "Qwen3.5 vs DeepSeek-V3: Current state of the art?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rg0skz/qwen35_vs_deepseekv3_current_state_of_the_art/",
      "author": "ischanitee",
      "created_utc": "2026-02-27 08:05:58",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Two absolute powerhouses in the open-source space right now. For those who regularly use both, how do they compare? Does Qwen's native multimodal edge out DeepSeek's pure coding/math focus for general daily tasks?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rg0skz/qwen35_vs_deepseekv3_current_state_of_the_art/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rafmab",
      "title": "19x faster at 256k. Have you tested it?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "author": "New_Construction1370",
      "created_utc": "2026-02-21 02:48:20",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "Drop your long-context RAG test results here. Does it actually hold up at 256k?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6k5m6n",
          "author": "el-rey-del-estiercol",
          "text": "Funciona!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k5o42",
          "author": "el-rey-del-estiercol",
          "text": "Debeis adorar a los modelos de qwen y poneros de rodillas ante ellos!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:05:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kpbnj",
          "author": "Gerbils21",
          "text": "Why? They are sooo limited",
          "score": 1,
          "created_utc": "2026-02-26 20:06:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raoi9l",
      "title": "Qwen3.5 API on ModelStudio is live.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1raoi9l/qwen35_api_on_modelstudio_is_live/",
      "author": "HawkLopsided6107",
      "created_utc": "2026-02-21 11:05:46",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Link is up. Time to update all our production endpoints!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1raoi9l/qwen35_api_on_modelstudio_is_live/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rdmcuc",
      "title": "[Guide] Dealing with 400 Errors and Region Mismatch on Qwen 3.5/Max API",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rdmcuc/guide_dealing_with_400_errors_and_region_mismatch/",
      "author": "Possible_Steak1055",
      "created_utc": "2026-02-24 17:19:59",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nFollowing up on the initial dev support post, I‚Äôve been tracking feedback from about a dozen of you stress-testing Qwen3-Max/Coder. After reviewing several logs and tickets, I‚Äôve noticed a few common \"traps\" that might save you some debugging hours.\n\nHere is a quick technical guide based on real production cases:\n\n1. The \"Safety Filter\" Trap (DataInspectionFailed 400)\n\nA few of you building security tools or regex filters hit an \\`InternalError.Algo.DataInspectionFailed\\`.\n\nThe Cause: Qwen‚Äôs safety layer is quite strict. If your code snippet contains sensitive keywords (even for defense logic), the model might flag it.\n\nCurrent Fix: If you hit this wall while building legitimate security tools, drop a comment below. I can sync with the engineering team to review and potentially lift restrictions for your specific use case.\n\n2. Billing vs. Region Syncing\n\nOne dev noted their Coding Plan wasn't being consumed while their credit balance was.\n\nThe Cause: Plans are region-specific. If you bought a plan in the Singapore region but your Base URL points to a different endpoint (like US), it won't trigger the prepaid plan.\n\nFix: Ensure your \\`Base\\_URL\\` matches the region where the plan was purchased. Singapore is currently the most \"feature-complete\" region for global access.\n\n3. Scaling to \"Production-Quality\" Agents\n\nWe‚Äôve seen some massive RAG implementations this week. Qwen 3.5 seems remarkably good at State Mutation and Dependency Checking.\n\nPro-tip: When building complex agents, leverage Qwen‚Äôs Self-Healing logic. It‚Äôs surprisingly capable of identifying failed tests caused by code pollution and isolating them. If you are aiming for \"Production-grade\" reliability, let the model run its own verification loops.\n\n4. DB Sidekick for Quant/GIS Devs\n\nFor those managing heavy PostgreSQL/MySQL schemas alone, check out the DAS Agent.  It‚Äôs basically an AI sidekick for slow SQL analysis and diagnostics. It‚Äôs useful if you want to offload database maintenance to an LLM-based agent.\n\nI‚Äôm still taking these feedbacks directly to the product team to make the platform more developer-friendly.\n\nP.S.I still have some developer credits left for high-complexity projects (Enterprise RAG, FinTech, or Open Source). If you‚Äôre pushing the limits, let me know what you‚Äôre building in the comments!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rdmcuc/guide_dealing_with_400_errors_and_region_mismatch/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o7nocu2",
          "author": "Duckets1",
          "text": "Lol ü§£ my coding plan is stuck in Singapore and ironically still fast AF some how I'm in the USA east it was weird but me OpenClaw and Codex eventually figured it out",
          "score": 2,
          "created_utc": "2026-02-27 06:17:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rclvgi",
      "title": "Let's talk about the \"Agentic Workflow\" capabilities in real life.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rclvgi/lets_talk_about_the_agentic_workflow_capabilities/",
      "author": "dino_gr01",
      "created_utc": "2026-02-23 16:20:54",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "I set up a loop where Qwen writes code, tests it in a sandbox, reads the error log, and fixes it. It successfully debugged a massive JSON parsing script without my intervention.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rclvgi/lets_talk_about_the_agentic_workflow_capabilities/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o70frph",
          "author": "iolairemcfadden",
          "text": "How did you do this. Was it a single prompt or something else.",
          "score": 1,
          "created_utc": "2026-02-23 20:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z33jm",
          "author": "Crafty_Ball_8285",
          "text": "Codex does this by itself with no prompting",
          "score": 0,
          "created_utc": "2026-02-23 16:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z43qt",
              "author": "somas",
              "text": "I love what codex does for me for $20 a month but I‚Äôm not 100% on OpenAI being around in five years. I‚Äôm trying to make Qwen do what codex can do too.",
              "score": 4,
              "created_utc": "2026-02-23 16:29:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7s7kod",
              "author": "AffectionatePlastic0",
              "text": "Codex can suddenly change their pricing model. \n\nBut the local software won't change itself.",
              "score": 1,
              "created_utc": "2026-02-27 22:41:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbk7r9",
      "title": "how to use qwen coding model for free?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbk7r9/how_to_use_qwen_coding_model_for_free/",
      "author": "Jaded_Expert2806",
      "created_utc": "2026-02-22 12:06:29",
      "score": 8,
      "num_comments": 21,
      "upvote_ratio": 0.79,
      "text": "how to use qwen coding model for free by api key or any tool ?",
      "is_original_content": false,
      "link_flair_text": "Help üôã‚Äç‚ôÇÔ∏è",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbk7r9/how_to_use_qwen_coding_model_for_free/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6reivm",
          "author": "jackorjek",
          "text": "qwen chat is free, unlimited. cli is free up to 1000 requests per day. api is free for 90 days through alibaba cloud. correct me if im wrong.",
          "score": 6,
          "created_utc": "2026-02-22 12:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rcxcu",
          "author": "Moist-Chip3793",
          "text": "Nvidia NiM.\n\nYou can get a free API key here: [https://build.nvidia.com/](https://build.nvidia.com/)\n\nThis might also be useful: [https://www.reddit.com/r/opencodeCLI/comments/1rb65ak/i\\_made\\_a\\_tui\\_tool\\_to\\_use\\_opencode\\_basically\\_for/?share\\_id=AzR\\_MMLPv4S7RjK1QtRWi&utm\\_medium=android\\_app&utm\\_name=androidcss&utm\\_source=share&utm\\_term=4](https://www.reddit.com/r/opencodeCLI/comments/1rb65ak/i_made_a_tui_tool_to_use_opencode_basically_for/?share_id=AzR_MMLPv4S7RjK1QtRWi&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=4)",
          "score": 3,
          "created_utc": "2026-02-22 12:10:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rw0wj",
              "author": "Rude-Ad2841",
              "text": "T√ºrkiye is also not on the menu",
              "score": 3,
              "created_utc": "2026-02-22 14:18:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rdwgk",
              "author": "MrMrsPotts",
              "text": "What are the limits?",
              "score": 1,
              "created_utc": "2026-02-22 12:18:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rwkbe",
                  "author": "Moist-Chip3793",
                  "text": "I've just ripped this from my link above:\n\n**‚ö†Ô∏è Honest limitations you should know:**\n\nNVIDIA moved from a credit system to rate limits in mid-2025 so the good news is there's no credit counter running out anymore. The free access is ongoing with no expiry, as long as you use it for dev/prototyping (not for serving real users in production).\n\nThe commonly reported rate limit is around **40 requests/minute**, though NVIDIA doesn't publish exact per-model limits and has confirmed they don't plan to. For a coding session that's rarely an issue.\n\nThe real pain point is that popular models especially the S+ tier ones like DeepSeek V3.2 or Qwen3 Coder 480B can be slow or outright overloaded üî• during peak hours. That's actually the main reason I built this tool: instead of guessing, you see all 44 models' live latency and uptime at once and switch in one keystroke.",
                  "score": 1,
                  "created_utc": "2026-02-22 14:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6tehxw",
              "author": "ActiveAd9022",
              "text": "Yeah I don't know what the heck is going on but out of all the countries in the world only T√ºrkiye, Egypt, Pakistan, and a few others like Russia are not available.¬†\n\n\nDo you know of any reason for why this is the case? Heck I could even find Palestine and the holy See who most of the time don't count as countries in the list",
              "score": 1,
              "created_utc": "2026-02-22 18:34:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ww2a4",
                  "author": "Moist-Chip3793",
                  "text": "Nobody knows, it seems. :(\n\n[https://forums.developer.nvidia.com/t/i-cant-find-my-country-on-the-list-to-verify-my-account-and-get-api-access/360844](https://forums.developer.nvidia.com/t/i-cant-find-my-country-on-the-list-to-verify-my-account-and-get-api-access/360844)",
                  "score": 1,
                  "created_utc": "2026-02-23 06:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rd0lb",
              "author": "Jaded_Expert2806",
              "text": "I cant verify nim by my country egypt not there üò≠",
              "score": 1,
              "created_utc": "2026-02-22 12:11:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rd7sq",
                  "author": "Moist-Chip3793",
                  "text": "Damn!",
                  "score": 1,
                  "created_utc": "2026-02-22 12:12:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rdcm7",
          "author": "ridablellama",
          "text": "qwen coding plan includes a free tier",
          "score": 2,
          "created_utc": "2026-02-22 12:14:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rde6k",
              "author": "Jaded_Expert2806",
              "text": "Free api key?",
              "score": 1,
              "created_utc": "2026-02-22 12:14:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rga2c",
                  "author": "ridablellama",
                  "text": "ye you can use their oauth login with third party apps. i‚Äôve used it for openclaw starter models before. ",
                  "score": 3,
                  "created_utc": "2026-02-22 12:37:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6tklzq",
                  "author": "ridablellama",
                  "text": "[https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available](https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available)",
                  "score": 1,
                  "created_utc": "2026-02-22 19:02:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rdslj",
              "author": "MrMrsPotts",
              "text": "What are the limits?",
              "score": 1,
              "created_utc": "2026-02-22 12:17:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rg4h6",
                  "author": "ridablellama",
                  "text": "# OAuth Free Tier\n\nSign in with Qwen OAuth to get 1,000 free requests per day. No credit card required, perfect for individual developers and small teams.",
                  "score": 3,
                  "created_utc": "2026-02-22 12:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76owxw",
          "author": "NoobMLDude",
          "text": "Here you go, shows exactly how to setup the FREE Tier on Qwen \n\n- [Setup Qwen Code - FREE Code Agent using Qwen3Coder in Terminal](https://youtu.be/M6ubLFqL-OA)\n\n\nIf you wish to use Cline /KiloCode inside VSCode IDE, you can also connect the QwenCode to those extensions.\n\n- [FREE Qwen3Code with Kilo-Code](https://youtu.be/z_ks6Li1D5M)",
          "score": 1,
          "created_utc": "2026-02-24 18:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nrrqj",
              "author": "otzjog",
              "text": "Unfortunately, integration with Kilo Code through auth (CLI) is not a viable option anymore, the Qwen team recently cut down the usage limits outside of Qwen CLI/chat, dramatically, I started seeing 429 - after like 20 requests/day.\n\nI mean only free usage. Paid API from Alibaba works fine \n\nAnd out of two remaining options I would prefer CLI, cause their chat interface is on a very early stage of development",
              "score": 1,
              "created_utc": "2026-02-27 06:45:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rgbd7",
          "author": "Suitable-Program-181",
          "text": "Zed ACP has sweet limits",
          "score": 1,
          "created_utc": "2026-02-22 12:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s4agi",
          "author": "iolairemcfadden",
          "text": "Someone in the last two weeks posted and they were offering Quinn credits if you message them. The credit is through Alibaba cloud.",
          "score": 0,
          "created_utc": "2026-02-22 15:02:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}