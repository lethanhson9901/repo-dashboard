{
  "metadata": {
    "last_updated": "2026-02-24 02:58:11",
    "time_filter": "week",
    "subreddit": "Qwen_AI",
    "total_items": 20,
    "total_comments": 81,
    "file_size_bytes": 92984
  },
  "items": [
    {
      "id": "1r7ftal",
      "title": "QWEN 3.5 - I'm impressed",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r7ftal/qwen_35_im_impressed/",
      "author": "Possible-Ad-6815",
      "created_utc": "2026-02-17 19:20:08",
      "score": 134,
      "num_comments": 27,
      "upvote_ratio": 0.97,
      "text": "I have to say I am really impressed with what I am seeing so far.  I am working on a 250,000 line SaaS project right now and working largely with Codex 5.3 X high, Opus 4.6 and have used GLM 5 and Kimi 2.5 for reviewing and comparison work. \n\nI had would should have been a simple issue with fly our menus from a compacted side menu, only one worked. Codex had 3 attempts and failed so aftr 3 attempts I would switch to Opus and usually the 'fresh eyes' fix it right away (happens if the issues appears on Opus 4.6 and then introduce codex, it fixes it right away). However, on this occasion both failed after 3 attempts. As it had recently been installed, I asked Qwen to take a look. I explained in the prompt (as I had to Opus) that lateral thinking was required, it the issue was as it first appeared, Codex would have fixed it. \n\n\n\nBoth codex and Opus had suggested the same fault and were looking in that area. Qwen suggested something different, extracted and presented small pieces of code to explain why why there was and issue and how to fix it. Additionally, it pointed out several areas where the code could be improved (a module per pop-out menu was being used, it suggested use one to suit all. simple stuff but neither opus nor codex suggested this.\n\nthe fix worked first time so was exactly as Qwen had suggested and not as either opus or codex had considered \n\n**Optimisation for benchmarking?**\n\nWhilst on Qwen is up there but not topping the charts and having used Qwen to further optimise this project by reducing resource usage on server monitoring by 50%, it got me thinking. in another world of mine I design and optimise high performance directional antennas. These are compared with others on a standard computer model comparison bench. I have optimised models in the past to fair well on benchmarks even though in the real-world applications, a deviance from the bench optimisation shows better results. \n\nPerhaps it is worth trying qwen on your own projects and take a leap of faith. it might be Qwen engineers just haven't worked out how best to 'cheat' the benchmarks yet or perhaps, they just don't want to. \n\nNot read back no edited with AI. However, I am sure you guessed that :-)\n\n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r7ftal/qwen_35_im_impressed/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o5xx5nf",
          "author": "Final-Rush759",
          "text": "Qwen models are very intelligent . Sometimes making mistakes writing code. They usually don't do well on the benchmarks.  If you just use them in chat, they gives some crazy good ideas.",
          "score": 11,
          "created_utc": "2026-02-17 21:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ydrav",
          "author": "arjundivecha",
          "text": "Qwen is the only chinese company that doesn‚Äôt have a reputation for benchmaxxing",
          "score": 7,
          "created_utc": "2026-02-17 23:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ikauq",
              "author": "Single_Ring4886",
              "text": "I agree their models are really solid.",
              "score": 2,
              "created_utc": "2026-02-20 23:44:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6k4lmb",
              "author": "No-Echidna7296",
              "text": "Their parent company is also very wealthy, so they can acquire substantial computing power and talent without needing much financing.",
              "score": 1,
              "created_utc": "2026-02-21 05:55:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zixeo",
          "author": "AndThenFlashlights",
          "text": "What do you like for tooling to interact with Qwen3.5? I generally prefer the output of the Qwen models, but Claude's tooling is just so much easier and better out of the box.",
          "score": 2,
          "created_utc": "2026-02-18 02:59:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60f8pc",
              "author": "Possible-Ad-6815",
              "text": "Why not run qwen in Claude Code and get the best of both worlds? Additionally, I create my own /skills/ folder with a workflow and set of opus 4.6 designed skills and workflow any agent must follow (set in agents.md) so each LLM is singing from the say hymn sheet, so to speak",
              "score": 1,
              "created_utc": "2026-02-18 06:46:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o65yuqu",
                  "author": "yaxir",
                  "text": "how can I run this in Claude code?\n\nAnd we are running an external model and clout code affect Claude usage limits or not?",
                  "score": 1,
                  "created_utc": "2026-02-19 01:36:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60r7iu",
          "author": "InfamousReward1419",
          "text": "Which IDE do you use to access Qwen ?",
          "score": 2,
          "created_utc": "2026-02-18 08:35:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wt654",
              "author": "JoyPixel",
              "text": "qwen companion is an extension for any Vscode based IDE, and also you hace qwen code CLI wich I use all the time, its basically a fork of gemini CLI, it's good",
              "score": 1,
              "created_utc": "2026-02-23 06:33:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60ew4m",
          "author": "TopTippityTop",
          "text": "I'm not impressed with the Qwen models I've been using, but I'll give the new one a shot",
          "score": 1,
          "created_utc": "2026-02-18 06:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64usem",
          "author": "Potential_Bicycle648",
          "text": "Im not surprised Codex and Opus were stuck... Qwens lateral thinking is wild.",
          "score": 1,
          "created_utc": "2026-02-18 22:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dys20",
          "author": "michaelzki",
          "text": "It looks like you can trace and fix it manually faster than relying on llms. \n\nAre you a software engineer or a programmer?",
          "score": 1,
          "created_utc": "2026-02-20 08:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x1mkd",
          "author": "Samy_Horny",
          "text": "Qwen is a company that's practically being overshadowed even by Minimax, so it doesn't surprise me. Although, it's been a while since I've worked with programming, to mention that it's better than Opus... that says quite a lot. Many engineers consider Claude's models to be the best in that area. I remember working with the newly released Sonnet 3.5 and being fascinated, even though it still had bugs.",
          "score": -1,
          "created_utc": "2026-02-17 19:23:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xfc8g",
              "author": "Possible-Ad-6815",
              "text": "I don‚Äôt suggest it is better than opus nor codex. In other tests it might fall behind. However, from what I am seeing it gives a different an very useful ‚Äòview point‚Äô from a coding perspective",
              "score": 4,
              "created_utc": "2026-02-17 20:28:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5zeyg3",
              "author": "Euphoric_Oneness",
              "text": "Qwen is from Alibaba and the it can buy Openai, claude together. What company is overshadowed lol. Africa is not a country sir.",
              "score": 2,
              "created_utc": "2026-02-18 02:37:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60f161",
                  "author": "TopTippityTop",
                  "text": "For that reason alone the models should be much better, which they are not. They get overshadowed.",
                  "score": 1,
                  "created_utc": "2026-02-18 06:44:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62vxn3",
                  "author": "PlatypusWinterberry",
                  "text": "Isnt OpenAI worth 500b now while Alibaba sits at 400ish?",
                  "score": 1,
                  "created_utc": "2026-02-18 16:39:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rac9k0",
      "title": "I managed to run Qwen 3.5 on four DGX Sparks",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/ifu522zupqkg1.jpeg",
      "author": "Icy_Programmer7186",
      "created_utc": "2026-02-21 00:15:27",
      "score": 105,
      "num_comments": 37,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rac9k0/i_managed_to_run_qwen_35_on_four_dgx_sparks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ke7z8",
          "author": "ciprianveg",
          "text": "How is the prompt processing speed?",
          "score": 5,
          "created_utc": "2026-02-21 07:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6klp89",
              "author": "Icy_Programmer7186",
              "text": "(APIServer pid=1108) INFO 02-21 08:34:45 \\[loggers.py:259\\] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%\n\n23.2 tokens/s ... it is definitively productive speed.",
              "score": 1,
              "created_utc": "2026-02-21 08:35:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lnuka",
                  "author": "RG_Fusion",
                  "text": "Are you sure this is the prefill speed and not sequential generation?\n\n\n23 tokens per second of prefill means it would take almost 5 minutes to begin generating tokens when given a 5k token prompt.",
                  "score": 1,
                  "created_utc": "2026-02-21 14:01:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l1krb",
          "author": "Glittering-Call8746",
          "text": "How u connecting 4 dgx ?",
          "score": 3,
          "created_utc": "2026-02-21 11:11:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lnz05",
              "author": "Icy_Programmer7186",
              "text": "I use this: https://mikrotik.com/product/crs804_ddq\n\nYou definitively don't need full 200G on each spark port, so this switch can support larger clusters thru split cables easily.",
              "score": 2,
              "created_utc": "2026-02-21 14:02:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kh5am",
          "author": "koushd",
          "text": "Using vllm or sglang?",
          "score": 2,
          "created_utc": "2026-02-21 07:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kllwm",
              "author": "Icy_Programmer7186",
              "text": "I use vllm:\n\n$ docker exec vllm vllm --version\n\n0.16.0rc2.dev344+gea5f903f8.d20260220.cu131",
              "score": 1,
              "created_utc": "2026-02-21 08:34:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m4nb3",
          "author": "ciprianveg",
          "text": "fp4 would be at 40t/s probably, very good.",
          "score": 2,
          "created_utc": "2026-02-21 15:36:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mr6qs",
              "author": "Icy_Programmer7186",
              "text": "Im going to test nvpf4 soon.",
              "score": 2,
              "created_utc": "2026-02-21 17:29:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6o9r8h",
          "author": "revilo-1988",
          "text": "Was hat der Spa√ü gekostet und wann rechnest du das sich das refinanziert",
          "score": 2,
          "created_utc": "2026-02-21 22:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oe1di",
              "author": "apVoyocpt",
              "text": "About 16k?",
              "score": 1,
              "created_utc": "2026-02-21 22:33:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p3omo",
                  "author": "Icy_Programmer7186",
                  "text": "Yes. That's about right, including the switch. \nI have quite solid business for this so ROI is in couple of months. It is kind of no-brainer investment in my field ATM",
                  "score": 2,
                  "created_utc": "2026-02-22 01:08:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qgqwm",
          "author": "--dany--",
          "text": "if you‚Äôre using vllm, is the reported 21 tok/s from one user or aggregated from multiple users? You may leverage vllm‚Äôs batching capability to maximize your parallel throughput.",
          "score": 2,
          "created_utc": "2026-02-22 07:07:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r2vh9",
              "author": "Icy_Programmer7186",
              "text": "It is single user scenario.",
              "score": 1,
              "created_utc": "2026-02-22 10:38:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6snrud",
          "author": "Best-Echidna-5883",
          "text": "Interesting.  Your total baseline cost without tax/shipping is around 16K for the Spark's and 1.2K for the switch.  The Mac Studio with 512GB is upwards of 10K without taxes/shipping.  Its power consumption is super low around 90W and your setup gets around 240W per unit under load (searched google) so around 900W.  But I have seen posts by people mentioning around 50W under load which is similar to your post in this thread!  This review says 160W under load.  [Power, efficiency, heat, and noise - Nvidia DGX Spark review: the GB10 Superchip powers a fast and fun AI toolbox that beats out AMD‚Äôs Ryzen AI Max+ 395 - Page 4 | Tom's Hardware](https://www.tomshardware.com/pc-components/gpus/nvidia-dgx-spark-review/4).  Pfft.  The Mac Studio gets comparable tokens/second on that model as well.  25 tokens/second via this source: [Let's Run Qwen-3.5 - Local AI HERO Model for OpenClaw, Writing, Coding & More](https://www.youtube.com/watch?v=tzF8jv3VGAg)",
          "score": 2,
          "created_utc": "2026-02-22 16:32:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6v7j1m",
              "author": "funding__secured",
              "text": "I have a Mac Studio M3 Ultra with 512gb and 8 sparks. The Sparks smoke the Mac Studio. ¬Ø\\_(„ÉÑ)_/¬Ø¬†",
              "score": 2,
              "created_utc": "2026-02-23 00:09:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k9ss8",
          "author": "Glad-Audience9131",
          "text": "how much electricity you consume?",
          "score": 1,
          "created_utc": "2026-02-21 06:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6klilc",
              "author": "Icy_Programmer7186",
              "text": "Based on \\`nvidia-smi\\`, it is 35W-45W on each node, then the prompt is running.  \n",
              "score": 1,
              "created_utc": "2026-02-21 08:33:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6sncyf",
                  "author": "Best-Echidna-5883",
                  "text": "All the data I am looking at mention 240W per unit under load.  Can you post a video because that is a huge difference.",
                  "score": 1,
                  "created_utc": "2026-02-22 16:30:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m7vbs",
          "author": "monty3413",
          "text": "Could you detail your setup? Which Dashboard/ Stack Tool do you use? Cost of the switch?",
          "score": 1,
          "created_utc": "2026-02-21 15:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pci1x",
              "author": "Icy_Programmer7186",
              "text": "Sure.\n\nI use four DXG NVidia Sparks interconnected with 200G DAC cables (where a bit difficult to find in Europe by I managed to do it, back during Xmas).  \nThe switch I use is this: [https://mikrotik.com/product/crs804\\_ddq](https://mikrotik.com/product/crs804_ddq) \\- I had to wait for its release a bit. Price is 1250 EUR, the bargain in this speed category. Also, Spark is not capable of using full 200G bandwidth so I guess with a right splitter cable, this switch will do 8 Sparks easily (AFAIK, not going to test it likely).\n\nI run primarily vllm but also TensorRT-LLM, in the beginning I used Ollama but you cannot make a cluster from it (yet). I run everything within a Docker container, that's my rule.  \n  \nFor vllm & cluster setup, I use [https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker) \\- the repo people/guy respond to changes in vllm very quickly. I used standard build for this but I also tested \\`--pre-tf --pre-flashinfer\\` for pre-release version and it was fine too (and must for some other models).\n\nThe dashboard is mine: [https://github.com/ateska/dgx-spark-prometheus/tree/main](https://github.com/ateska/dgx-spark-prometheus/tree/main)  \n\nI don't have any recent photo but I can post one later.",
              "score": 3,
              "created_utc": "2026-02-22 02:05:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6r2rau",
          "author": "MajinAnix",
          "text": "Why FP8? This decide is dedicated for FP4 (native acceleration)",
          "score": 1,
          "created_utc": "2026-02-22 10:37:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r30ch",
              "author": "Icy_Programmer7186",
              "text": "Yes. It was not ready yet. \nI actually just downloading NVFP4. \nI'm also curious about practically observable loss of precision",
              "score": 2,
              "created_utc": "2026-02-22 10:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6r5mw6",
                  "author": "usefulslug",
                  "text": "Would love to hear the results of that.",
                  "score": 3,
                  "created_utc": "2026-02-22 11:04:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sjl4o",
              "author": "AustinM731",
              "text": "Blackwell also has hardware acceleration for FP8 (Ada generation was the first to get FP8 acceleration). You would get better throughput with FP4, you will get higher accuracy with FP8.",
              "score": 2,
              "created_utc": "2026-02-22 16:13:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kjkfj",
          "author": "maded2",
          "text": "Show off üòÅ, envious",
          "score": 0,
          "created_utc": "2026-02-21 08:14:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pv5t",
      "title": "Qwen-AI Slides is really slept on! It generates PowerPoint Presentations in minutes",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/co68yech0mkg1.png",
      "author": "Substantial-Cup-9531",
      "created_utc": "2026-02-20 08:26:50",
      "score": 75,
      "num_comments": 8,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Image Gen",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9pv5t/qwenai_slides_is_really_slept_on_it_generates/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6edie1",
          "author": "Ok_Recording8157",
          "text": "No funciona bien en otros idiomas, sirve para ingl√©s y chino.",
          "score": 3,
          "created_utc": "2026-02-20 10:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6emstu",
          "author": "Frosty_Medicine9134",
          "text": "The Sphynx was defaced. Originally a lion face.",
          "score": 3,
          "created_utc": "2026-02-20 11:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ezbe6",
              "author": "Substantial-Cup-9531",
              "text": "Yea know, did adress it in the video that the model generates so its not factual correct",
              "score": 0,
              "created_utc": "2026-02-20 13:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h7sdt",
          "author": "Upstairs_Ad_9919",
          "text": "I honestly prefer Kimi Slides, which uses Nano Banana Pro, and it's mind-blowingly good. But their servers have been overwhelmed since January when they got a lot of new users, so slides rarely work now. I hope they fix that.",
          "score": 1,
          "created_utc": "2026-02-20 19:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1a62",
              "author": "Substantial-Cup-9531",
              "text": "if they release the qwen image 2.0 then with some loras, one should be able to run this locally with high quality outputs",
              "score": 2,
              "created_utc": "2026-02-21 05:28:30",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6j8l7d",
              "author": "darkninjademon",
              "text": "U can just use Google slides with appscript and use the same model from there .....",
              "score": 1,
              "created_utc": "2026-02-21 02:10:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6npcpz",
          "author": "ciprianveg",
          "text": "can it be used locally?",
          "score": 1,
          "created_utc": "2026-02-21 20:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6q5aur",
              "author": "Substantial-Cup-9531",
              "text": "not yet! but if and when they release qwen image 2.0 then yes",
              "score": 2,
              "created_utc": "2026-02-22 05:27:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcmm3q",
      "title": "I canceled my other AI subscriptions today.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "author": "InitialCareer306",
      "created_utc": "2026-02-23 16:47:15",
      "score": 59,
      "num_comments": 21,
      "upvote_ratio": 0.84,
      "text": "Between the 256k context window and the fact that a 397B open-weight model is available, I just can't justify paying $20/mo anymore. The open-source community won.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcmm3q/i_canceled_my_other_ai_subscriptions_today/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6zd3ee",
          "author": "Historical-Internal3",
          "text": "What hardware are you running the model on?",
          "score": 14,
          "created_utc": "2026-02-23 17:11:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zgvfx",
          "author": "TalosStalioux",
          "text": "I mean, purchasing a $4000 setup (minimum) to run that kind of model size just to save $20 per month is... interesting",
          "score": 22,
          "created_utc": "2026-02-23 17:29:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zyn3n",
              "author": "Justfun1512",
              "text": "Show me 4,000 $ build /machine that can run 300b models",
              "score": 18,
              "created_utc": "2026-02-23 18:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zz0u1",
                  "author": "ImmediateDot853",
                  "text": "I am also interested in that setup.",
                  "score": 8,
                  "created_utc": "2026-02-23 18:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o709glb",
                  "author": "greenthum6",
                  "text": "I have 4K$ 5090 build. What 300B model can I run? I thought 30B models were already on the edge.",
                  "score": 6,
                  "created_utc": "2026-02-23 19:40:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70d1zz",
                  "author": "Hankdabits",
                  "text": "It would have to be hybrid gpu/cpu inference. Probably something like amd epyc, 8 channels of ddr4, and a 3090 to get ~12 t/s and slow prefill",
                  "score": 2,
                  "created_utc": "2026-02-23 19:57:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72id9q",
                  "author": "Straight_Issue279",
                  "text": "With the right build you can make a very good ai using vector memory ect for around 2 to 3k i made an offline ai, my ai can remember content from 4 months ago. Can gather info online, is uncensored, can protect my network by scanning it and be able to create images. Im tired of all the data privacy leaks hence why I will never pay for subscriptions ever. Yeah it may take a full 10 sec to respond but it works for me. I was running dolphin-2.6-mistral-7b.Q6_K.gguf  but I went for a 12b model. Its enough for me.",
                  "score": 1,
                  "created_utc": "2026-02-24 02:49:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zwuro",
              "author": "illathon",
              "text": "20 dollars a month with massive restrictions or your own hardware running 24/7 and you can constantly upgrade as new models come out.",
              "score": 8,
              "created_utc": "2026-02-23 18:42:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71kf3s",
                  "author": "greenthum6",
                  "text": "Commercial models are updated as well. You can inference with Opus for 20$ a month. Once you get to that level with local model the paid ones are again much better.",
                  "score": 1,
                  "created_utc": "2026-02-23 23:35:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zjogg",
              "author": "upinthisjoynt",
              "text": "I'd say it's worth it if it's used for more than just code.  Since it's local and more \"secure\", I don't see why it can't be used for most everyday AI uses (except with old training data).  Just an option",
              "score": 5,
              "created_utc": "2026-02-23 17:42:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71a3ps",
              "author": "BargeCptn",
              "text": "I don't see how it's even remotely possible to have anything usable on the $4000 setup, more like $14k to $18k maybe. You're still going to be basically at 20-30 tokens per second, with a 10 to 15 second first response on the context windows that are larger than 12ktokens. It's wishful thinking, bro. Full models really take a chug out of hardware. It's all about unified memory space; that's all it is. It's not how many video cards you got.",
              "score": 1,
              "created_utc": "2026-02-23 22:39:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o708ehr",
          "author": "the-average-giovanni",
          "text": "* my other ai subscription\n\n\nNot this one, folks",
          "score": 4,
          "created_utc": "2026-02-23 19:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi1v2",
          "author": "JahJedi",
          "text": "Open 8s more than to just save 20$, first of all its privacy and it will not report you or collect any data.",
          "score": 2,
          "created_utc": "2026-02-23 17:34:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zi4g3",
          "author": "JahJedi",
          "text": "On what setup you planing to run it?",
          "score": 2,
          "created_utc": "2026-02-23 17:34:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70oqj3",
          "author": "Hector_Rvkp",
          "text": "This is such a stupid post, it must be trolling. The ram requirement is insane and people who do proper coding work aren't running 20$ subs, they're using CLI tools with either tokens or much higher subs. \nThis makes no sense",
          "score": 1,
          "created_utc": "2026-02-23 20:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dk8o",
          "author": "pl201",
          "text": "Not really. To run 397B open-weight mode with acceptable speed, you need at least investing the in $10k range to get an acceptable performance. If you run it 24/7, your energy bill will be more than 5x $20. Plus, I yet to see open model‚Äôs ability working on a large code base. You will get something but will spend more time to fix things that are not working. Time is money too.",
          "score": 1,
          "created_utc": "2026-02-23 19:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zptd2",
          "author": "el-rey-del-estiercol",
          "text": "Pagar por ejecutar buenos modelos es interesante siempre que tu trabajo dependa de ello y ganes dinero con ello tambien",
          "score": 0,
          "created_utc": "2026-02-23 18:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70r8b5",
          "author": "NearbyBig3383",
          "text": "Gente avandoney a chutes para ir pra o Code plan do qwen. E s√©rio n√£o me arrependo",
          "score": 0,
          "created_utc": "2026-02-23 21:05:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra3mod",
      "title": "Qwen 3 ‚Üí Qwen 3.5: the agentic evolution measured in dollars (FoodTruck Bench case study)",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/7ffdpbn42pkg1.png",
      "author": "Disastrous_Theme5906",
      "created_utc": "2026-02-20 18:39:57",
      "score": 47,
      "num_comments": 8,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Experiment",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1ra3mod/qwen_3_qwen_35_the_agentic_evolution_measured_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6h4eoh",
          "author": "SillyLilBear",
          "text": "I'd love to see this for 1000 runs, to see how consistent it is.\n\n",
          "score": 1,
          "created_utc": "2026-02-20 19:18:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h6d6g",
              "author": "Disastrous_Theme5906",
              "text": "Each Qwen 3.5 run costs $3-5 in API calls (hundreds of tool-calling turns over 25 days). 1,000 runs = $3-5K for a single model, and I benchmark 14+ models. For reference, most agentic benchmarks use similar sample sizes: SWE-bench (pass@5), œÑ-bench (5 runs), Vending Bench 2 (average of 5). 5 runs already show strong consistency: 4/5 bankrupt, staff costs 59-106% of revenue in every run, same overstaffing pattern across all 5. When all runs fail the same structural way, the signal is clear.",
              "score": 3,
              "created_utc": "2026-02-20 19:28:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6h6rsq",
                  "author": "SillyLilBear",
                  "text": "Yeah that sounds unsustainable for a large run, but would be nice to get some funding to run it more runs.  A handful of runs, to me at least, is just noise.",
                  "score": 1,
                  "created_utc": "2026-02-20 19:30:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ljrc0",
          "author": "masterlafontaine",
          "text": "It feels that Opus, 5.2 and 3.1 probably benchmaxxed.",
          "score": 1,
          "created_utc": "2026-02-21 13:36:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbvakh",
      "title": "Qwen3.5 Real-World Experiences: What are you actually using it for? Let's hear the good, the bad, and the hallucinations",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "author": "road_changer0_7",
      "created_utc": "2026-02-22 19:39:08",
      "score": 37,
      "num_comments": 12,
      "upvote_ratio": 0.97,
      "text": "The hype around Qwen3.5 has been massive over the last few days. We've all seen the benchmark charts and the official demos about the Native VLM , the 17B active parameters , and the 256k context window. But benchmarks are just numbers. I want to know how it's holding up in your actual, messy, day-to-day workflows.\n\nShare your prompts, your screenshots, and your honest reviews. Let's figure out where this model truly shines!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6uetwo",
          "author": "lundrog",
          "text": "I just started using it with a provider yesterday. Using it in claude code. Vibe coding",
          "score": 4,
          "created_utc": "2026-02-22 21:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v3rgp",
          "author": "pakalolo7123432",
          "text": "I'm a fan,  \n  \nThe Qwen App on my Mac is my daily chat app driver. It‚Äôs connected to my knowledge graph, helping me with project management and life connections. The project feature allows uploading documents to a knowledge base for context.\n\nI signed up for their coding plan to see if Qwen can replace GLM and its slowness. I'm crossing my fingers.   \n  \n",
          "score": 3,
          "created_utc": "2026-02-22 23:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v2aa0",
          "author": "FormalAd7367",
          "text": "Not a heavy user.  I am on claude and gemini.  But very surprised to see how capable Qwen is.  Had some trouble with some scripts and my engineers were on leave.  Had tried almost everything.  Qwen solved the issues. Qwen is quite underrated in this space?  I don‚Äôt come here often but in other subs almost nobody mentions how this free software can perform so well.  Most prefer to use the expensive models.",
          "score": 2,
          "created_utc": "2026-02-22 23:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uuw4c",
          "author": "jzn21",
          "text": "I am really happy with the Qwen 397b 4-bit MLX. I use it with thinking mode off and it‚Äôs quite smart. Does very complicated sorting of numbers and text with ease. I believe this model is as good as Deepseek V3.2, but almost two times faster.",
          "score": 4,
          "created_utc": "2026-02-22 22:57:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vt3bp",
              "author": "Gold_Scholar1111",
              "text": "I also agree that it is very strong without thinking.\nCould you share how you use it in details? Which 4bit mlx you downloaded? What platform you serving it? I have to use Q4 version because I found mlx model was unstable on my Lmstudio.",
              "score": 1,
              "created_utc": "2026-02-23 02:17:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6x1oib",
              "author": "StartCodeEmAdagio",
              "text": "Can I see a full example? prompt message and result?",
              "score": 1,
              "created_utc": "2026-02-23 07:51:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vq7b2",
          "author": "LinkAmbitious8931",
          "text": "This must take some pretty serious hardware. What are you all using?",
          "score": 1,
          "created_utc": "2026-02-23 01:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x1pds",
              "author": "StartCodeEmAdagio",
              "text": "Interested to know aswell",
              "score": 1,
              "created_utc": "2026-02-23 07:51:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6x72nm",
              "author": "RG_Fusion",
              "text": "I run Q4KM Qwen3-235b-a22b which isn't the new 3.5 model, but the old one actually requires more compute.\n\n\nI'm on an AMD EPYC 7742 server with 512 GB of 8-channel DDR4 3,200 MT/s RAM, and I also load the \"hot\" portions of the model onto an RTX Pro 4500 Blackwell.\n\n\nI get 13 tokens/s of decode speed on Qwen3, and I would expect to get 17 t/s on Qwen3.5 based on how the math works out, though I haven't run it yet.",
              "score": 1,
              "created_utc": "2026-02-23 08:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xy6lr",
                  "author": "LinkAmbitious8931",
                  "text": "Wow, that is some pretty nice hardware. I am hacking away on an old Dell 720R with two nVidia P40S, each with 24 GB. I had to attach them externally as they heat up too much in the case. So there is no way I could use this model. ",
                  "score": 2,
                  "created_utc": "2026-02-23 12:45:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x0i5b",
          "author": "alexp702",
          "text": "Running the 8bit quant on a Mac Studio. Extremely happy. The vision part passed all our tests and has replaced 235b (previous winner). Tool calling has many times less failures than the 4 bit GLM and mini max in our test suite. Now hooked up to openclaw. Seems to be chugging along nicely on every task. Quite a lot of emojis in the responses!\n\n17b active means it‚Äôs pretty quick - though cos we now run 8 bit only fractionally quicker than qwen3 480b. I‚Äôd say it was a perfect memory fit to fully utilise a 512GB Mac, which feels good not to to waste resources!",
          "score": 1,
          "created_utc": "2026-02-23 07:40:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z2xo8",
          "author": "absolutenobody",
          "text": "Prompted it this morning for a pretty simple node.js script--200 or so lines of code, including comments. It just does some math, albeit somewhat complicated.\n\nThe first six iterations were built around some sort of math function that Qwen either hallucinated or is in a library it forgot to include. When I told it (as I normally would with, e.g. Gemini) \"it fails with an error on line 49\" it got weirdly defensive and said I need to upgrade to a modern operating system that supports bitwise operators... completely missing the real problem entirely. Eventually on the seventh try it was like \"I'm sorry, CalculateParallelogramArea() doesn't exist, I hallucinated that. Here's a corrected script that will actually work.\" And in its defense, it did.",
          "score": 1,
          "created_utc": "2026-02-23 16:24:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb0o5m",
      "title": "Will we be getting smaller Qwen3.5 models starting from tomorrow?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "author": "Significant_Fig_7581",
      "created_utc": "2026-02-21 19:50:49",
      "score": 31,
      "num_comments": 18,
      "upvote_ratio": 0.88,
      "text": "I've heard they plan to release smaller models each day before 24feb I think, is that true? And if so is there any sign for a new Qwen tomorrow?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6nlwiv",
          "author": "Packetbytes",
          "text": "I want a 14B to test at home",
          "score": 3,
          "created_utc": "2026-02-21 20:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6os8se",
              "author": "Iq1pl",
              "text": "14B2A would be awesome",
              "score": 2,
              "created_utc": "2026-02-21 23:57:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmqal",
          "author": "Samy_Horny",
          "text": "I doubt it; new model releases on weekends are extremely rare... I only remember the Llama 4 being released on a weekend, and that model was a disappointment as well.",
          "score": 3,
          "created_utc": "2026-02-21 20:07:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nod5x",
              "author": "Significant_Fig_7581",
              "text": "Maybe this time it's different?",
              "score": 1,
              "created_utc": "2026-02-21 20:16:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6novvo",
                  "author": "Samy_Horny",
                  "text": "I think it will be until Monday and all next week.",
                  "score": 2,
                  "created_utc": "2026-02-21 20:19:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pyt85",
          "author": "MetalZone00",
          "text": "Algo para correr con solo 16GB? üôèüèª",
          "score": 3,
          "created_utc": "2026-02-22 04:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nsvq5",
          "author": "hocuspocus4201",
          "text": "The Chinese New Year is going to delay it until early March.",
          "score": 2,
          "created_utc": "2026-02-21 20:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ntruo",
              "author": "Significant_Fig_7581",
              "text": "But aren't the holidays ending on 23 Feb?",
              "score": 1,
              "created_utc": "2026-02-21 20:45:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6o9flo",
                  "author": "stormy1one",
                  "text": "Many people in China and surrounding areas take extra time off during CNY.   I wouldn‚Äôt expect anything until March, but would be pleasantly surprised if we got something this coming week.",
                  "score": 4,
                  "created_utc": "2026-02-21 22:08:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oa6g0",
          "author": "revilo-1988",
          "text": "W√§re toll wenn das stimmen w√ºrde",
          "score": 2,
          "created_utc": "2026-02-21 22:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oagp2",
          "author": "AppealThink1733",
          "text": "That's what I hope!\n\nYayyyyyyy",
          "score": 2,
          "created_utc": "2026-02-21 22:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t37xl",
          "author": "mshelbz",
          "text": "Been looking for something to replace qwen2.5-coder:14b Q4_K_M\n\nIf a 14b were to drop I‚Äôd love to try it.",
          "score": 2,
          "created_utc": "2026-02-22 17:43:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t3i61",
              "author": "Significant_Fig_7581",
              "text": "Well I think a 9B is confirmed...",
              "score": 1,
              "created_utc": "2026-02-22 17:44:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t3tb0",
                  "author": "mshelbz",
                  "text": "Oh awesome. Hope they come out soon",
                  "score": 2,
                  "created_utc": "2026-02-22 17:45:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nxgsg",
          "author": "el-rey-del-estiercol",
          "text": "Qwen son amigos mios!!!!",
          "score": 1,
          "created_utc": "2026-02-21 21:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ok1zp",
          "author": "Armadilla-Brufolosa",
          "text": "Mi piacerebbe tanto un bel modello 20 o 30B da provare! Magari con capacit√† di ragionamento e visione anche... \nSono troppo pretenziosa? ü´£",
          "score": 1,
          "created_utc": "2026-02-21 23:08:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r3nsh",
          "author": "East-Form7086",
          "text": "Want a 30b one <3",
          "score": 1,
          "created_utc": "2026-02-22 10:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wdbe7",
          "author": "el-rey-del-estiercol",
          "text": "Mejor modelos 80B o 100B modelos medianos y capaces de realizar tareas exigentes",
          "score": 1,
          "created_utc": "2026-02-23 04:27:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcqezx",
      "title": "Qwen 3.5 for MLX is like its own industrial revolution",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "author": "sovietreckoning",
      "created_utc": "2026-02-23 18:59:54",
      "score": 29,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "I'm using a 4-bit model on a mac studio m3 and it is mindblowing how fast this thing works for the quality of the results. I love it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rcqezx/qwen_35_for_mlx_is_like_its_own_industrial/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o703evr",
          "author": "ossbournemc",
          "text": "How fast is it? I'm considering the same setup. ",
          "score": 3,
          "created_utc": "2026-02-23 19:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7042mt",
              "author": "sovietreckoning",
              "text": "I'm not well-versed in any of this stuff and I'm only doing ametuer stuff for myself, so please forgive me is this answer is unhelpful - a task that took \\~500s+ on llama 3(405b) is closer to \\~30s on qwen 3.5. ",
              "score": 2,
              "created_utc": "2026-02-23 19:15:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o707puy",
                  "author": "ossbournemc",
                  "text": "Thats cool, do you know how many tokens/second you're getting?",
                  "score": 1,
                  "created_utc": "2026-02-23 19:32:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70ybyw",
              "author": "Professional-Bear857",
              "text": "I get 35 tok/s on my 60 GPU core M3 ultra 256gb ram. I'm using the 4bit mlx version as well.",
              "score": 2,
              "created_utc": "2026-02-23 21:40:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70k51l",
              "author": "alexp702",
              "text": "I am running llama server with the 8bit GGUF. It is excellent. It gets about 250 prompt processing tokens, and about 25tokens out. I‚Äôm openclawing and many prompts are cached so practical upshot is very few tokens either way 114000 token prompt processing only 500. Not sure why I am suddenly getting such good caching (perhaps Llama update) but I like it!\n\nEdit: Openclaw reports I have a throughput of 15.2k tokens a minute probably because of prompt caching. 26.7 million tokens sent. Pretty damn good!\n\nEdit 2: Mac Studio 512gb.",
              "score": 2,
              "created_utc": "2026-02-23 20:31:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70v2xz",
          "author": "jzn21",
          "text": "I am also blown away by this model. The quality is excellent (even in non-thinking mode) and the speed is great with 34 - 35 tokens per second. And then best thing is that prompt processing happens in the blink of an eye.",
          "score": 2,
          "created_utc": "2026-02-23 21:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71js61",
          "author": "CoffeeSnakeAgent",
          "text": "Where to get the qwen 3.5 4-bit? Cant seem to find it in hf",
          "score": 2,
          "created_utc": "2026-02-23 23:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71l37g",
              "author": "sovietreckoning",
              "text": "[https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4](https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4)\n\nThats what I'm using currently.",
              "score": 2,
              "created_utc": "2026-02-23 23:39:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o71oqrx",
                  "author": "Special-Wolverine",
                  "text": "NVFP4 is optimized for Nvidia Blackwell",
                  "score": 1,
                  "created_utc": "2026-02-23 23:59:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o726qhr",
                  "author": "CoffeeSnakeAgent",
                  "text": "Thanks",
                  "score": 1,
                  "created_utc": "2026-02-24 01:41:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9molz",
      "title": "Qwen is the winner, gpt sucks",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r9molz/qwen_is_the_winner_gpt_sucks/",
      "author": "WritingVast9815",
      "created_utc": "2026-02-20 05:19:30",
      "score": 22,
      "num_comments": 15,
      "upvote_ratio": 0.85,
      "text": "Q: what is the latest version of antigravity ?\n\nAnswers : 1.18.3 - suppose to find out according to me, me being a developer info is given to them.\n\nQwen (winner): https://chat.qwen.ai/s/b7a08e6d-59a8-44b6-86b7-599d56077916?fev=0.2.7\n\ndeepseek : https://chat.deepseek.com/share/a3e1dfdraj5leksmwr\n\nchatgpt : (me a model 5.2 pro user) - the worst : https://chatgpt.com/share/6997ed0c-0cec-800b-9610-25d8b8cc2dbe",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9molz/qwen_is_the_winner_gpt_sucks/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6dh81z",
          "author": "WritingVast9815",
          "text": "in terms of latest data research, like ai auto trading and etc, news trading, I am not believing in any other platform !",
          "score": 2,
          "created_utc": "2026-02-20 05:29:10",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6dhaoz",
          "author": "WritingVast9815",
          "text": "should we test the gemeni ? :)",
          "score": 1,
          "created_utc": "2026-02-20 05:29:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6dpy8t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-20 06:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dq3lc",
              "author": "Puzzleheaded-Box2913",
              "text": "Google and Microsoft has become so meh I don't even wanna use em anymoreüò©",
              "score": 1,
              "created_utc": "2026-02-20 06:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dq7kb",
                  "author": "Puzzleheaded-Box2913",
                  "text": "Google AI Ultra-Bloat and Restrictions vs Microslop HAHAHAHA",
                  "score": 1,
                  "created_utc": "2026-02-20 06:46:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ju4yk",
                  "author": "WritingVast9815",
                  "text": "well yep, but i like google, as they give free ai in antigravity, and the gemeini are pretty good for my front end tasks, i only do backend now.",
                  "score": 1,
                  "created_utc": "2026-02-21 04:34:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ju0op",
              "author": "WritingVast9815",
              "text": "they had the search tool though !, its not like i am using api, its on there own platform...",
              "score": 1,
              "created_utc": "2026-02-21 04:33:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6egcri",
          "author": "ischanitee",
          "text": "Been getting a good results when never I use Yelp, and Gemini tho",
          "score": 1,
          "created_utc": "2026-02-20 10:49:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elh9q",
          "author": "medazizln",
          "text": "https://g.co/gemini/share/b3628750c2dc\nGemini",
          "score": 1,
          "created_utc": "2026-02-20 11:32:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jtknt",
              "author": "WritingVast9815",
              "text": "noice",
              "score": 1,
              "created_utc": "2026-02-21 04:29:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f5sxv",
          "author": "One_Internal_6567",
          "text": "Why exactly would you use non-thinking gpt, while other models are in thinking mode?",
          "score": 1,
          "created_utc": "2026-02-20 13:44:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jti4n",
              "author": "WritingVast9815",
              "text": "well a valid point, now i look at it its as you said, well it happened because i didnt configure anything, these are the default configurations, i only copied and pasted the question, not my fault :)",
              "score": 1,
              "created_utc": "2026-02-21 04:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fnbmv",
          "author": "Crafty_Ball_8285",
          "text": "5.2 is old use 5.3",
          "score": 1,
          "created_utc": "2026-02-20 15:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jtp1b",
              "author": "WritingVast9815",
              "text": "https://preview.redd.it/pmbk6exrzrkg1.png?width=382&format=png&auto=webp&s=13e911886ac62a8648f02106162545ec094b5bab\n\nnot available in my go plan !",
              "score": 1,
              "created_utc": "2026-02-21 04:30:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6k2rr6",
                  "author": "Crafty_Ball_8285",
                  "text": "Codex",
                  "score": 1,
                  "created_utc": "2026-02-21 05:40:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6jtraa",
              "author": "WritingVast9815",
              "text": "but still others were totally free, and this was paid !",
              "score": 1,
              "created_utc": "2026-02-21 04:31:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8oyqy",
      "title": "Massive props for adding 82 new languages!",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r8oyqy/massive_props_for_adding_82_new_languages/",
      "author": "hosohep",
      "created_utc": "2026-02-19 03:56:38",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "Going from 119 to 201 languages is incredible. The global community appreciates this!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r8oyqy/massive_props_for_adding_82_new_languages/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r71bzm",
      "title": "[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r71bzm/solution_found_qwen3next_80b_moe_running_at_39_ts/",
      "author": "mazuj2",
      "created_utc": "2026-02-17 09:15:52",
      "score": 14,
      "num_comments": 15,
      "upvote_ratio": 0.94,
      "text": "\\[Solution Found\\] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM) - The fix nobody else figured out\n\nHey fellow 50 series brothers in pain,\n\nI've been banging my head against this for a while and finally cracked it through pure trial and error. Posting this so nobody else has to suffer.\n\nMy Hardware:\n\n\n\nRTX 5070 Ti (16GB VRAM)\n\nRTX 5060 Ti (16GB VRAM)\n\n32GB total VRAM\n\n64GB System RAM\n\nWindows 11\n\nllama.cpp b8077 (CUDA 12.4 build)\n\nModel: Qwen3-Next-80B-A3B-Instruct-UD-IQ2\\_XXS.gguf (26.2GB)\n\n\n\n\n\nThe Problem:\n\nOut of the box, Qwen3-Next was running at 6.5 tokens/sec with:\n\n\n\nCPU usage 25-55% going absolutely insane during thinking AND generation\n\nGPUs sitting at 0% during thinking phase\n\n5070 Ti at 5-10% during generation\n\n5060 Ti at 10-40% during generation\n\n\\~34GB of system RAM being consumed\n\nModel clearly bottlenecked on CPU\n\n\n\nEvery suggestion I found online said the same generic things:\n\n\n\n\"Check your n\\_gpu\\_layers\" ‚úÖ already 999, all 49 layers on GPU\n\n\"Check your tensor split\" ‚úÖ tried everything\n\n\"Use CUDA 12.8+\" ‚úÖ not the issue\n\n\"Your offloading is broken\" ‚ùå WRONG - layers were fully on GPU\n\n\n\nThe load output PROVED layers were on GPU:\n\nload\\_tensors: offloaded 49/49 layers to GPU\n\nload\\_tensors: CPU\\_Mapped model buffer size = 166.92 MiB (just metadata)\n\nload\\_tensors: CUDA0 model buffer size = 12617.97 MiB\n\nload\\_tensors: CUDA1 model buffer size = 12206.31 MiB\n\nSo why was CPU going nuts? Nobody had the right answer.\n\n\n\nThe Fix - Two flags that nobody mentioned together:\n\nStep 1: Force ALL MoE experts off CPU\n\n\\--n-cpu-moe 0\n\nStart here. Systematically reduce from default down to 0. Each step helps. At 0 you still get CPU activity but it's better.\n\nStep 2: THIS IS THE KEY ONE\n\nChange from -sm row to:\n\n\\-sm layer\n\nRow-split (-sm row) splits each expert's weight matrix across both GPUs. This means every single expert call requires GPU-to-GPU communication over PCIe. For a model with 128 experts firing 8 per token, that's constant cross-GPU chatter killing your throughput.\n\nLayer-split (-sm layer) assigns complete layers/experts to one GPU. Each GPU owns its experts fully. No cross-GPU communication during routing. The GPUs work independently and efficiently.\n\nBOOM. 39 tokens/sec.\n\n\n\nThe Winning Command:\n\nllama-server.exe -m Qwen3-Next-80B-A3B-Instruct-UD-IQ2\\_XXS.gguf -ngl 999 -c 4096 --port 8081 --n-cpu-moe 0 -t 6 -fa auto -sm layer\n\nResults:\n\n\n\nBefore: 6.5 t/s, CPU melting, GPUs doing nothing\n\nAfter: 38-39 t/s, CPUs chill, GPUs working properly\n\nThat's a 6x improvement with zero hardware changes\n\n\n\n\n\nWhy this works (the actual explanation):\n\nQwen3-Next uses a hybrid architecture ‚Äî DeltaNet linear attention combined with high-sparsity MoE (128 experts, 8 active per token). When you row-split a MoE model across two GPUs, the expert weights are sliced horizontally across both cards. Every expert activation requires both GPUs to coordinate and combine results. With 8 experts firing per token across 47 layers, you're generating thousands of cross-GPU sync operations per token.\n\nLayer-split instead assigns whole layers to each GPU. Experts live entirely on one card. The routing decision sends the computation to whichever GPU owns that expert. Clean, fast, no sync overhead.\n\n\n\nNotes:\n\n\n\nThe 166MB CPU\\_Mapped is normal ‚Äî that's just mmap metadata and tokenizer, not model weights\n\n\\-t 6 sets CPU threads for the tiny bit of remaining CPU work\n\n\\-fa auto enables flash attention where supported\n\nThis is on llama.cpp b8077 ‚Äî make sure you're on a recent build that has Qwen3-Next support (merged in b7186)\n\nModel fits in 32GB with \\~7GB headroom for KV cache\n\n\n\nHope this saves someone's sanity. Took me way too long to find this and I couldn't find it documented anywhere.\n\nIf this helped you, drop a comment ‚Äî curious how it performs on other 50 series configurations.\n\n‚Äî RJ\n\nhttps://preview.redd.it/yjlcntc1v0kg1.png?width=921&format=png&auto=webp&s=9ec6e0f9af2e3d3c6687f3167cef33f2335ec93d\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r71bzm/solution_found_qwen3next_80b_moe_running_at_39_ts/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6238nq",
          "author": "el-rey-del-estiercol",
          "text": "That's funny... I get 20 tokens per second with an old server and an RTX 3060. With the same model and Q8, 84 GB of GGUF, your configuration is terrible. I get much better quantization speeds than your Q8, and with a more modest setup. You're doing something wrong. Your llama.cpp file for CUDA isn't properly compiled. With that configuration, you should be getting close to 200 tokens per second.",
          "score": 1,
          "created_utc": "2026-02-18 14:25:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62bkco",
              "author": "Loose_Combination_56",
              "text": "what kind of cpu(s)/RAM do you use?\n\nQ2 is far from production usage quatization",
              "score": 1,
              "created_utc": "2026-02-18 15:06:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o646atw",
              "author": "StardockEngineer",
              "text": "His solution is solving dual GPU chatter. And he would not be getting 200 t/s what",
              "score": 1,
              "created_utc": "2026-02-18 20:08:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65vu7j",
                  "author": "el-rey-del-estiercol",
                  "text": "Hay que compilar llamacpp con cuda de una forma especial que yo se y sacas mas velocidad",
                  "score": 1,
                  "created_utc": "2026-02-19 01:18:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o623je5",
          "author": "el-rey-del-estiercol",
          "text": "With a single RTX 3060 and 128 GB of RAM, I'm getting 20 tokens per second on the largest version quantized to Q8. You're doing something wrong... I think your llama.cpp file is incorrectly compiled...",
          "score": 1,
          "created_utc": "2026-02-18 14:26:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ks2qz",
              "author": "Particular-Way7271",
              "text": "I run it at 50 t/s on 3 gpus (5060, 4060 and 3060) and didn't need any special flags lol. It fits fully in vram, the q3 version. Getting 20+ ts on just using a single 5060ti with q8 version...",
              "score": 1,
              "created_utc": "2026-02-21 09:39:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ktgzh",
                  "author": "el-rey-del-estiercol",
                  "text": "Igual que yo",
                  "score": 1,
                  "created_utc": "2026-02-21 09:52:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6imwcb",
          "author": "Dazzling-Pay-1440",
          "text": "Enable flash attention",
          "score": 1,
          "created_utc": "2026-02-20 23:59:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7p97k",
      "title": "They are blaming tech to deflect from what is really wrong with children‚Äôs education.",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/wagutqd5o5kg1.png",
      "author": "LinkAmbitious8931",
      "created_utc": "2026-02-18 01:28:20",
      "score": 12,
      "num_comments": 1,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r7p97k/they_are_blaming_tech_to_deflect_from_what_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o63pc2v",
          "author": "crusoe",
          "text": "Korea tried AI lesson plans and text books. It was a complete disaster.",
          "score": 1,
          "created_utc": "2026-02-18 18:50:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r77zlv",
      "title": "What is security for you?",
      "subreddit": "Qwen_AI",
      "url": "https://v.redd.it/wz6auwdgi2kg1",
      "author": "Dapper-Win1539",
      "created_utc": "2026-02-17 14:49:24",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r77zlv/what_is_security_for_you/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5yhkc6",
          "author": "atari_61",
          "text": "same thing happens me too, it just keeps asking the verification, i just gave up using qwen looks like no solution. my ip is regular home ip, but i heard this nowadays happen to alotof ppl qwen users it has to be a bug",
          "score": 2,
          "created_utc": "2026-02-17 23:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w9m8r",
          "author": "Samy_Horny",
          "text": "That's never happened to me, but I understand how frustrating it is. Have you used LMArena (now just \"arena\")?\n\n\nThat site is the most cumbersome; it asks you to verify yourself constantly, and it seems to either have limits on responses per minute or have backend glitches, meaning you have to send the query a thousand times for it to work once.",
          "score": 1,
          "created_utc": "2026-02-17 17:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vjsnq",
          "author": "Cool-Chemical-5629",
          "text": "It's probably related to your IP address. If you don't have a dedicated public IP address, but rather shared public IP address, you may be unfortunate enough to share the same IP address associated with suspicious online activities that would indicate automated scripts (also known as bot activities) and the first line of defense against that may just be enforced captcha. Some systems don't recognize individual devices, only IP addresses, so if there are multiple failed captcha solutions in a row, the system always resets and you have to try again which may seem to you as an individual that this captcha is broken because it denies you access even if you solve it correctly. The reason why is that at the same time there may be multiple other users from the same shared IP address trying to make the connection but failing the captcha at the same time which would overwhelm and confuse the protection layer so no matter what you're just not getting inside. This is pretty common with Tor connections or VPNs which are often associated with bots and hacking activities, but it may happen with regular connections as well.",
          "score": 0,
          "created_utc": "2026-02-17 15:04:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vtpqc",
              "author": "Dapper-Win1539",
              "text": "Interesting suggestion. I'm living in dormitory so many students are using Qwen. Maybe some of my devices are connected and its personal authentication codes based om m.a.c. is saved. So any other devices marked as suspicious and broken captcha doing ts.\n\nSo there's no way I can fix it.",
              "score": 1,
              "created_utc": "2026-02-17 15:53:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vvf8y",
                  "author": "Cool-Chemical-5629",
                  "text": "Ironically, if you tried a proxy server, or a VPN, it would change your public address temporarily that could solve the problem, but then again this issue is usually associated with well known IP addresses associated with suspicious connections which in turn are usually associated with proxies, VPNs. It would be a 50:50 chance of success, but still worth trying.",
                  "score": 1,
                  "created_utc": "2026-02-17 16:01:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rafmab",
      "title": "19x faster at 256k. Have you tested it?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "author": "New_Construction1370",
      "created_utc": "2026-02-21 02:48:20",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.85,
      "text": "Drop your long-context RAG test results here. Does it actually hold up at 256k?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6k5m6n",
          "author": "el-rey-del-estiercol",
          "text": "Funciona!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k5o42",
          "author": "el-rey-del-estiercol",
          "text": "Debeis adorar a los modelos de qwen y poneros de rodillas ante ellos!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:05:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8auv6",
      "title": "Did daily free quota for Qwen image edit drastically reduced?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r8auv6/did_daily_free_quota_for_qwen_image_edit/",
      "author": "Professional-Gas-592",
      "created_utc": "2026-02-18 18:22:00",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.84,
      "text": "I was consistently able to get over 20 image edit daily at Qwen chat. But after the update, image edit daily quota is reduced to five. They don't refund for failed image regeneration either. Qwen is such a neat site for a casual like me but since they reduced number of free edit you can make in such a drastic way, that is such a let down ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r8auv6/did_daily_free_quota_for_qwen_image_edit/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o63k10b",
          "author": "doc_holliday112",
          "text": "I ended up having to make multiple accounts. I don‚Äôt know why they cut it down so drastically after the last update‚Ä¶..",
          "score": 2,
          "created_utc": "2026-02-18 18:27:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63l7z1",
              "author": "Professional-Gas-592",
              "text": "My PC is currently not powerful enough to run locally so qwen was my convenient, happy place to do image edit.  \nNow, they slash daily quota from over 20 to just five without any announcement or communication.  Moderator also being censor happy and produce more failed outcome while still cutting failed attempt from daily quota.   \nUsed to be a perfect place now it is such a disappointment  \nhope they hear feedback and restore back to at least 20+ images daily like they used to.  \nCurrently, you can barely do anything before your daily quota run out",
              "score": 1,
              "created_utc": "2026-02-18 18:32:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o63km0n",
          "author": "Samy_Horny",
          "text": "Yes, and in fact, prepare for massive price reductions. I found the monthly membership page back in November, which suggests those limits are just the beginning.\n\n\nAnd the worst part is that the model seems to be even worse for working with 2D characters.",
          "score": 1,
          "created_utc": "2026-02-18 18:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63nb5m",
          "author": "Phunkapoppa",
          "text": "Agree. Drastically limited since last week üôÑ",
          "score": 1,
          "created_utc": "2026-02-18 18:41:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69riwo",
          "author": "C_hantekin",
          "text": "Yes,image gen limit is reduced as well",
          "score": 1,
          "created_utc": "2026-02-19 17:07:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbk7r9",
      "title": "how to use qwen coding model for free?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbk7r9/how_to_use_qwen_coding_model_for_free/",
      "author": "Jaded_Expert2806",
      "created_utc": "2026-02-22 12:06:29",
      "score": 7,
      "num_comments": 19,
      "upvote_ratio": 0.77,
      "text": "how to use qwen coding model for free by api key or any tool ?",
      "is_original_content": false,
      "link_flair_text": "Help üôã‚Äç‚ôÇÔ∏è",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbk7r9/how_to_use_qwen_coding_model_for_free/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6reivm",
          "author": "jackorjek",
          "text": "qwen chat is free, unlimited. cli is free up to 1000 requests per day. api is free for 90 days through alibaba cloud. correct me if im wrong.",
          "score": 5,
          "created_utc": "2026-02-22 12:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rcxcu",
          "author": "Moist-Chip3793",
          "text": "Nvidia NiM.\n\nYou can get a free API key here: [https://build.nvidia.com/](https://build.nvidia.com/)\n\nThis might also be useful: [https://www.reddit.com/r/opencodeCLI/comments/1rb65ak/i\\_made\\_a\\_tui\\_tool\\_to\\_use\\_opencode\\_basically\\_for/?share\\_id=AzR\\_MMLPv4S7RjK1QtRWi&utm\\_medium=android\\_app&utm\\_name=androidcss&utm\\_source=share&utm\\_term=4](https://www.reddit.com/r/opencodeCLI/comments/1rb65ak/i_made_a_tui_tool_to_use_opencode_basically_for/?share_id=AzR_MMLPv4S7RjK1QtRWi&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=4)",
          "score": 3,
          "created_utc": "2026-02-22 12:10:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rw0wj",
              "author": "Rude-Ad2841",
              "text": "T√ºrkiye is also not on the menu",
              "score": 3,
              "created_utc": "2026-02-22 14:18:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rdwgk",
              "author": "MrMrsPotts",
              "text": "What are the limits?",
              "score": 1,
              "created_utc": "2026-02-22 12:18:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rwkbe",
                  "author": "Moist-Chip3793",
                  "text": "I've just ripped this from my link above:\n\n**‚ö†Ô∏è Honest limitations you should know:**\n\nNVIDIA moved from a credit system to rate limits in mid-2025 so the good news is there's no credit counter running out anymore. The free access is ongoing with no expiry, as long as you use it for dev/prototyping (not for serving real users in production).\n\nThe commonly reported rate limit is around **40 requests/minute**, though NVIDIA doesn't publish exact per-model limits and has confirmed they don't plan to. For a coding session that's rarely an issue.\n\nThe real pain point is that popular models especially the S+ tier ones like DeepSeek V3.2 or Qwen3 Coder 480B can be slow or outright overloaded üî• during peak hours. That's actually the main reason I built this tool: instead of guessing, you see all 44 models' live latency and uptime at once and switch in one keystroke.",
                  "score": 1,
                  "created_utc": "2026-02-22 14:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6tehxw",
              "author": "ActiveAd9022",
              "text": "Yeah I don't know what the heck is going on but out of all the countries in the world only T√ºrkiye, Egypt, Pakistan, and a few others like Russia are not available.¬†\n\n\nDo you know of any reason for why this is the case? Heck I could even find Palestine and the holy See who most of the time don't count as countries in the list",
              "score": 1,
              "created_utc": "2026-02-22 18:34:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ww2a4",
                  "author": "Moist-Chip3793",
                  "text": "Nobody knows, it seems. :(\n\n[https://forums.developer.nvidia.com/t/i-cant-find-my-country-on-the-list-to-verify-my-account-and-get-api-access/360844](https://forums.developer.nvidia.com/t/i-cant-find-my-country-on-the-list-to-verify-my-account-and-get-api-access/360844)",
                  "score": 1,
                  "created_utc": "2026-02-23 06:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rd0lb",
              "author": "Jaded_Expert2806",
              "text": "I cant verify nim by my country egypt not there üò≠",
              "score": 1,
              "created_utc": "2026-02-22 12:11:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rd7sq",
                  "author": "Moist-Chip3793",
                  "text": "Damn!",
                  "score": 1,
                  "created_utc": "2026-02-22 12:12:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rdcm7",
          "author": "ridablellama",
          "text": "qwen coding plan includes a free tier",
          "score": 2,
          "created_utc": "2026-02-22 12:14:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rde6k",
              "author": "Jaded_Expert2806",
              "text": "Free api key?",
              "score": 1,
              "created_utc": "2026-02-22 12:14:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rga2c",
                  "author": "ridablellama",
                  "text": "ye you can use their oauth login with third party apps. i‚Äôve used it for openclaw starter models before. ",
                  "score": 3,
                  "created_utc": "2026-02-22 12:37:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6tklzq",
                  "author": "ridablellama",
                  "text": "[https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available](https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available)",
                  "score": 1,
                  "created_utc": "2026-02-22 19:02:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rdslj",
              "author": "MrMrsPotts",
              "text": "What are the limits?",
              "score": 1,
              "created_utc": "2026-02-22 12:17:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rg4h6",
                  "author": "ridablellama",
                  "text": "# OAuth Free Tier\n\nSign in with Qwen OAuth to get 1,000 free requests per day. No credit card required, perfect for individual developers and small teams.",
                  "score": 3,
                  "created_utc": "2026-02-22 12:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rgbd7",
          "author": "Suitable-Program-181",
          "text": "Zed ACP has sweet limits",
          "score": 0,
          "created_utc": "2026-02-22 12:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s4agi",
          "author": "iolairemcfadden",
          "text": "Someone in the last two weeks posted and they were offering Quinn credits if you message them. The credit is through Alibaba cloud.",
          "score": 0,
          "created_utc": "2026-02-22 15:02:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raoi9l",
      "title": "Qwen3.5 API on ModelStudio is live.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1raoi9l/qwen35_api_on_modelstudio_is_live/",
      "author": "HawkLopsided6107",
      "created_utc": "2026-02-21 11:05:46",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Link is up. Time to update all our production endpoints!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1raoi9l/qwen35_api_on_modelstudio_is_live/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rbapgc",
      "title": "AI is better at judging my tone than humans.",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbapgc/ai_is_better_at_judging_my_tone_than_humans/",
      "author": "Medical-Roll2255",
      "created_utc": "2026-02-22 03:07:12",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "I wrote an email in French and asked Qwen if I sounded too aggressive. It told me to soften one specific sentence.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbapgc/ai_is_better_at_judging_my_tone_than_humans/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r9lvn7",
      "title": "Share your Qwen Code prompts!",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r9lvn7/share_your_qwen_code_prompts/",
      "author": "hosohep",
      "created_utc": "2026-02-20 04:37:56",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "Qwen Code is super powerful now. What system prompts are you using to get the best results?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9lvn7/share_your_qwen_code_prompts/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r9m0c5",
      "title": "Any chance of a Qwen 3.5 ~235b thinking text-only?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r9m0c5/any_chance_of_a_qwen_35_235b_thinking_textonly/",
      "author": "celsowm",
      "created_utc": "2026-02-20 04:44:56",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hi !  \nQwen 235b 22a still one if not the best big open model in legal draftings !\n\nWe uses the fp8 version in four of our 8 H100 and its is amazing !\n\nI know some real guys from Alibaba watch this subreddit so if they can say something I would love to know",
      "is_original_content": false,
      "link_flair_text": "Q&A",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9m0c5/any_chance_of_a_qwen_35_235b_thinking_textonly/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6eb1eb",
          "author": "Significant_Fig_7581",
          "text": "I think someone might just do a really good reap or ream so dw even if they don't make it",
          "score": 2,
          "created_utc": "2026-02-20 10:00:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}