{
  "metadata": {
    "last_updated": "2026-02-23 09:03:23",
    "time_filter": "week",
    "subreddit": "Qwen_AI",
    "total_items": 20,
    "total_comments": 108,
    "file_size_bytes": 100416
  },
  "items": [
    {
      "id": "1r662ls",
      "title": "Qwen3.5-397B-A17B <Release>",
      "subreddit": "Qwen_AI",
      "url": "https://v.redd.it/9tm49g6b2ujg1",
      "author": "vibedonnie",
      "created_utc": "2026-02-16 10:24:10",
      "score": 336,
      "num_comments": 30,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r662ls/qwen35397ba17b_release/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5pdx2n",
          "author": "Sirius_Sec_",
          "text": "How much vram do I need ?",
          "score": 5,
          "created_utc": "2026-02-16 16:17:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t10ql",
              "author": "RG_Fusion",
              "text": "A bit over 200 GB for Q4-K-XL, not including your context window.",
              "score": 3,
              "created_utc": "2026-02-17 03:39:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5w6fco",
                  "author": "stavrosg",
                  "text": "20t/s on 2x3090 and 3x3080, epyc 7352..",
                  "score": 3,
                  "created_utc": "2026-02-17 16:57:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5y2rcu",
                  "author": "yaxir",
                  "text": "and if you include context window, then?",
                  "score": 2,
                  "created_utc": "2026-02-17 22:19:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ny4rm",
          "author": "nunodonato",
          "text": "In that video where it is doing computer use in excel, what kind of software is that where the model runs? Is that something internal to the qwen team or is it publicly available?",
          "score": 3,
          "created_utc": "2026-02-16 11:11:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5omdfh",
          "author": "Efficient_Cattle_958",
          "text": "A model with 397Bparameter, that's something i didn't expect",
          "score": 8,
          "created_utc": "2026-02-16 13:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o07y2",
          "author": "chiliraupe",
          "text": "Will it come to qwen code cli?",
          "score": 2,
          "created_utc": "2026-02-16 11:29:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ph8mk",
              "author": "Popular_Tomorrow_204",
              "text": "It says \"more than just a CLI tool\" so i guess yes, it will come soon",
              "score": 2,
              "created_utc": "2026-02-16 16:33:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5q799a",
              "author": "itsappleseason",
              "text": "I'm seeing a reasoning model in qwen code. Have you checked today?",
              "score": 1,
              "created_utc": "2026-02-16 18:33:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5reo1p",
                  "author": "chiliraupe",
                  "text": "yes its in there but how many parameters?",
                  "score": -2,
                  "created_utc": "2026-02-16 22:02:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ourwf",
          "author": "NoobMLDude",
          "text": "But can it Code ? ðŸ˜‰",
          "score": 2,
          "created_utc": "2026-02-16 14:44:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qbtg0",
          "author": "Extension-Pen-109",
          "text": "Ok. Opensource, but for what machine?",
          "score": 2,
          "created_utc": "2026-02-16 18:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ybu5r",
              "author": "Any_Reading_5090",
              "text": "GPU Cluster...appr 700000â‚¬",
              "score": 3,
              "created_utc": "2026-02-17 23:06:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p72cy",
          "author": "infdevv",
          "text": "why is this edited like some cinematic movie trailer. actually peak",
          "score": 1,
          "created_utc": "2026-02-16 15:45:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qqpp1",
              "author": "Opposite-Station-337",
              "text": "there is a non-zero chance ai made the video... and yeah... ngl the audio/video combination gave me chills. damn nervous system responding to hype.",
              "score": 1,
              "created_utc": "2026-02-16 20:05:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sudjn",
          "author": "hazeslack",
          "text": "Is there any flash version?",
          "score": 1,
          "created_utc": "2026-02-17 02:57:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vyy42",
          "author": "jemand_tw",
          "text": "Any version below 120B?",
          "score": 1,
          "created_utc": "2026-02-17 16:19:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w6fod",
          "author": "Sea-Share6668",
          "text": "And it's a complete black ass with generating images and videos, it looks especially shitty after the TRIUMPH in December 2025 with the Qwen-image-2512 model!!! :-(((",
          "score": 1,
          "created_utc": "2026-02-17 16:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y2ti5",
          "author": "yaxir",
          "text": "I wish ChatGPT would do the same for GPT 4.1",
          "score": 1,
          "created_utc": "2026-02-17 22:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ygiow",
          "author": "yaxir",
          "text": "abliterated version?",
          "score": 1,
          "created_utc": "2026-02-17 23:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60w8hr",
          "author": "No-Daikon1554",
          "text": "Why doesn't qwem cite at least 50 sources per query to beat perplexity and chatgpt plus?Â ",
          "score": 1,
          "created_utc": "2026-02-18 09:22:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61lpkx",
          "author": "ClueCool1165",
          "text": "Any optimized guffs?",
          "score": 1,
          "created_utc": "2026-02-18 12:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67vr6f",
          "author": "Sharp-Mouse9049",
          "text": "Wah wah we wah",
          "score": 1,
          "created_utc": "2026-02-19 10:19:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6s0ri",
      "title": "Alibaba just open-sourced a model that rivals GPT-5.2",
      "subreddit": "Qwen_AI",
      "url": "https://medium.com/reading-sh/alibaba-just-open-sourced-a-model-that-rivals-gpt-5-2-708502e25250?sk=425ccf8e2abb8068adedabd2b2cc9050",
      "author": "jpcaparas",
      "created_utc": "2026-02-17 01:10:00",
      "score": 185,
      "num_comments": 40,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r6s0ri/alibaba_just_opensourced_a_model_that_rivals_gpt52/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5slls3",
          "author": "Puzzleheaded-Box2913",
          "text": "And it's actually pretty good",
          "score": 19,
          "created_utc": "2026-02-17 02:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tgd2b",
          "author": "johanna_75",
          "text": "What are the usage limits on the public website?",
          "score": 7,
          "created_utc": "2026-02-17 05:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u241x",
              "author": "Unedited_Sloth_7011",
              "text": "I've never run into usage limits, personally. I suspect they aren't any",
              "score": 6,
              "created_utc": "2026-02-17 08:38:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5u28s9",
                  "author": "johanna_75",
                  "text": "If itâ€™s as good as people are saying, and it has little or no usage limits then that seems almost too good to be true?",
                  "score": 5,
                  "created_utc": "2026-02-17 08:40:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ukt9e",
          "author": "solarkraft",
          "text": "30B version when?\n\nThis model really looks great, would be even more awesome to have a small model to run locally. Makes total sense to release the big headliner first of course.Â ",
          "score": 5,
          "created_utc": "2026-02-17 11:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63ovow",
              "author": "Qwen30bEnjoyer",
              "text": "Could be that these results are just from scale.",
              "score": 1,
              "created_utc": "2026-02-18 18:48:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u7wpf",
          "author": "FederalLook5060",
          "text": "Waiting for Qwen code 3.5 400b as well.",
          "score": 2,
          "created_utc": "2026-02-17 09:34:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w65lz",
          "author": "getaway-3007",
          "text": "What are you basing your claims off of? I always look at these chinese models like MiniMax, GLM-5, Kimi-k2.5 claiming they're on same level as OPUS etc when they're actually not.\n\nI've spent 500M tokens on GLM 4.7+GLM 5+MiniMax m2.1 and the amount of steering, additional context I have to give in comparison of codex or Opus is huge. Also the speed difference is very noticeable.",
          "score": 2,
          "created_utc": "2026-02-17 16:56:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o627akm",
              "author": "ChainPlastic7530",
              "text": "I have yet too find anyone using deepseek yet supposedly its \"crushing the west\"   \nthey mastered the art of performing good on benchmark though",
              "score": 1,
              "created_utc": "2026-02-18 14:45:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63eu9x",
                  "author": "Additional_Long_8589",
                  "text": "i use it \n\n",
                  "score": 1,
                  "created_utc": "2026-02-18 18:04:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ifeiq",
                  "author": "ISHITTEDINYOURPANTS",
                  "text": "i do use it quite often tbh",
                  "score": 1,
                  "created_utc": "2026-02-20 23:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60crhv",
          "author": "TopTippityTop",
          "text": "5.3 codex is out though, and really good. 5.2 is old news, and Qwen loses on some significant benchmarks to both 5.2 and Claude. Obviously benchmarks only go so far, the real test is actually trying it out on useful tasks.",
          "score": 1,
          "created_utc": "2026-02-18 06:25:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61o8wt",
          "author": "MrMrsPotts",
          "text": "It's really not as good as chatgpt 5.2 or Gemini 3  for math, at least. I gave it a medium hard grade 9 question which chatgpt and Gemini  can both do and it completely failed.  You can try it with this: \n\nProve that there exists an infinity of a, b, c âˆˆ Q strictly positive such that  \n\n  \n\na + b + c âˆˆ Z and 1/a + 1/b + 1/c âˆˆ Z.  \n\n  \n\nPlease use 9th-grade level math.",
          "score": 1,
          "created_utc": "2026-02-18 13:03:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dj15g",
              "author": "michaelsoft__binbows",
              "text": "I dont know what ninth grade youre talking about but that aint it",
              "score": 1,
              "created_utc": "2026-02-20 05:43:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dt04i",
                  "author": "MrMrsPotts",
                  "text": "It's Olympiad level but you don't need any math above 9th grade.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:11:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61tjhf",
          "author": "thecodeassassin",
          "text": "I'm running this model locally, and while I'm surprised how good it is it still doesn't really feel on the same level as GPT 5.2. mostly fine but it does make quite a lot of mistakes and doesn't correct it's mistakes as well as the frontier models.\n\nIt's really quite bad at designing good architecture.\n\nIn short: it's getting closer but it's not quite there yet.",
          "score": 1,
          "created_utc": "2026-02-18 13:33:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62rq95",
              "author": "JaatGuru",
              "text": "Which version are you running locally? Am looking for coding.",
              "score": 1,
              "created_utc": "2026-02-18 16:20:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62wycp",
                  "author": "thecodeassassin",
                  "text": "Qwen 3.5 379b-A17B with llama cpp",
                  "score": 1,
                  "created_utc": "2026-02-18 16:44:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dqbjz",
              "author": "d4mations",
              "text": "Iâ€™m using this exact setup and your right, it does make tons of mistakes and halucinates often, however it is good for daily automations where it doesnâ€™t have to think too deeply. I like the fact that there is no buring of tokens for easy stuff",
              "score": 1,
              "created_utc": "2026-02-20 06:47:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fz2po",
          "author": "LoveMind_AI",
          "text": "Not hard to rival 5.2 - I think Qwen 3.5 is better in almost every way that counts.",
          "score": 1,
          "created_utc": "2026-02-20 16:10:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6he898",
          "author": "WillingnessCorrect50",
          "text": "Impressive. Just be sure to actually use a local version of the model and not their online one.",
          "score": 1,
          "created_utc": "2026-02-20 20:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nyhfo",
          "author": "Trojanw0w",
          "text": "Has anyone had analyzed the data to see if these open source labs are closing in on mainstream?",
          "score": 1,
          "created_utc": "2026-02-21 21:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ti7q0",
          "author": "VillPotr",
          "text": "Code version too?",
          "score": 0,
          "created_utc": "2026-02-17 05:41:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ulzw8",
              "author": "Particular-Way7271",
              "text": "It can already code pretty well and it has vision as well.",
              "score": 1,
              "created_utc": "2026-02-17 11:40:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7ftal",
      "title": "QWEN 3.5 - I'm impressed",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r7ftal/qwen_35_im_impressed/",
      "author": "Possible-Ad-6815",
      "created_utc": "2026-02-17 19:20:08",
      "score": 132,
      "num_comments": 27,
      "upvote_ratio": 0.97,
      "text": "I have to say I am really impressed with what I am seeing so far.  I am working on a 250,000 line SaaS project right now and working largely with Codex 5.3 X high, Opus 4.6 and have used GLM 5 and Kimi 2.5 for reviewing and comparison work. \n\nI had would should have been a simple issue with fly our menus from a compacted side menu, only one worked. Codex had 3 attempts and failed so aftr 3 attempts I would switch to Opus and usually the 'fresh eyes' fix it right away (happens if the issues appears on Opus 4.6 and then introduce codex, it fixes it right away). However, on this occasion both failed after 3 attempts. As it had recently been installed, I asked Qwen to take a look. I explained in the prompt (as I had to Opus) that lateral thinking was required, it the issue was as it first appeared, Codex would have fixed it. \n\n\n\nBoth codex and Opus had suggested the same fault and were looking in that area. Qwen suggested something different, extracted and presented small pieces of code to explain why why there was and issue and how to fix it. Additionally, it pointed out several areas where the code could be improved (a module per pop-out menu was being used, it suggested use one to suit all. simple stuff but neither opus nor codex suggested this.\n\nthe fix worked first time so was exactly as Qwen had suggested and not as either opus or codex had considered \n\n**Optimisation for benchmarking?**\n\nWhilst on Qwen is up there but not topping the charts and having used Qwen to further optimise this project by reducing resource usage on server monitoring by 50%, it got me thinking. in another world of mine I design and optimise high performance directional antennas. These are compared with others on a standard computer model comparison bench. I have optimised models in the past to fair well on benchmarks even though in the real-world applications, a deviance from the bench optimisation shows better results. \n\nPerhaps it is worth trying qwen on your own projects and take a leap of faith. it might be Qwen engineers just haven't worked out how best to 'cheat' the benchmarks yet or perhaps, they just don't want to. \n\nNot read back no edited with AI. However, I am sure you guessed that :-)\n\n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r7ftal/qwen_35_im_impressed/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o5xx5nf",
          "author": "Final-Rush759",
          "text": "Qwen models are very intelligent . Sometimes making mistakes writing code. They usually don't do well on the benchmarks.  If you just use them in chat, they gives some crazy good ideas.",
          "score": 11,
          "created_utc": "2026-02-17 21:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ydrav",
          "author": "arjundivecha",
          "text": "Qwen is the only chinese company that doesnâ€™t have a reputation for benchmaxxing",
          "score": 7,
          "created_utc": "2026-02-17 23:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ikauq",
              "author": "Single_Ring4886",
              "text": "I agree their models are really solid.",
              "score": 2,
              "created_utc": "2026-02-20 23:44:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6k4lmb",
              "author": "No-Echidna7296",
              "text": "Their parent company is also very wealthy, so they can acquire substantial computing power and talent without needing much financing.",
              "score": 1,
              "created_utc": "2026-02-21 05:55:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zixeo",
          "author": "AndThenFlashlights",
          "text": "What do you like for tooling to interact with Qwen3.5? I generally prefer the output of the Qwen models, but Claude's tooling is just so much easier and better out of the box.",
          "score": 2,
          "created_utc": "2026-02-18 02:59:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60f8pc",
              "author": "Possible-Ad-6815",
              "text": "Why not run qwen in Claude Code and get the best of both worlds? Additionally, I create my own /skills/ folder with a workflow and set of opus 4.6 designed skills and workflow any agent must follow (set in agents.md) so each LLM is singing from the say hymn sheet, so to speak",
              "score": 1,
              "created_utc": "2026-02-18 06:46:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o65yuqu",
                  "author": "yaxir",
                  "text": "how can I run this in Claude code?\n\nAnd we are running an external model and clout code affect Claude usage limits or not?",
                  "score": 1,
                  "created_utc": "2026-02-19 01:36:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60r7iu",
          "author": "InfamousReward1419",
          "text": "Which IDE do you use to access Qwen ?",
          "score": 2,
          "created_utc": "2026-02-18 08:35:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wt654",
              "author": "JoyPixel",
              "text": "qwen companion is an extension for any Vscode based IDE, and also you hace qwen code CLI wich I use all the time, its basically a fork of gemini CLI, it's good",
              "score": 1,
              "created_utc": "2026-02-23 06:33:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60ew4m",
          "author": "TopTippityTop",
          "text": "I'm not impressed with the Qwen models I've been using, but I'll give the new one a shot",
          "score": 1,
          "created_utc": "2026-02-18 06:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64usem",
          "author": "Potential_Bicycle648",
          "text": "Im not surprised Codex and Opus were stuck... Qwens lateral thinking is wild.",
          "score": 1,
          "created_utc": "2026-02-18 22:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dys20",
          "author": "michaelzki",
          "text": "It looks like you can trace and fix it manually faster than relying on llms. \n\nAre you a software engineer or a programmer?",
          "score": 1,
          "created_utc": "2026-02-20 08:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x1mkd",
          "author": "Samy_Horny",
          "text": "Qwen is a company that's practically being overshadowed even by Minimax, so it doesn't surprise me. Although, it's been a while since I've worked with programming, to mention that it's better than Opus... that says quite a lot. Many engineers consider Claude's models to be the best in that area. I remember working with the newly released Sonnet 3.5 and being fascinated, even though it still had bugs.",
          "score": -1,
          "created_utc": "2026-02-17 19:23:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xfc8g",
              "author": "Possible-Ad-6815",
              "text": "I donâ€™t suggest it is better than opus nor codex. In other tests it might fall behind. However, from what I am seeing it gives a different an very useful â€˜view pointâ€™ from a coding perspective",
              "score": 4,
              "created_utc": "2026-02-17 20:28:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5zeyg3",
              "author": "Euphoric_Oneness",
              "text": "Qwen is from Alibaba and the it can buy Openai, claude together. What company is overshadowed lol. Africa is not a country sir.",
              "score": 2,
              "created_utc": "2026-02-18 02:37:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60f161",
                  "author": "TopTippityTop",
                  "text": "For that reason alone the models should be much better, which they are not. They get overshadowed.",
                  "score": 1,
                  "created_utc": "2026-02-18 06:44:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62vxn3",
                  "author": "PlatypusWinterberry",
                  "text": "Isnt OpenAI worth 500b now while Alibaba sits at 400ish?",
                  "score": 1,
                  "created_utc": "2026-02-18 16:39:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rac9k0",
      "title": "I managed to run Qwen 3.5 on four DGX Sparks",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/ifu522zupqkg1.jpeg",
      "author": "Icy_Programmer7186",
      "created_utc": "2026-02-21 00:15:27",
      "score": 98,
      "num_comments": 32,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rac9k0/i_managed_to_run_qwen_35_on_four_dgx_sparks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ke7z8",
          "author": "ciprianveg",
          "text": "How is the prompt processing speed?",
          "score": 5,
          "created_utc": "2026-02-21 07:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6klp89",
              "author": "Icy_Programmer7186",
              "text": "(APIServer pid=1108) INFO 02-21 08:34:45 \\[loggers.py:259\\] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%\n\n23.2 tokens/s ... it is definitively productive speed.",
              "score": 1,
              "created_utc": "2026-02-21 08:35:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lnuka",
                  "author": "RG_Fusion",
                  "text": "Are you sure this is the prefill speed and not sequential generation?\n\n\n23 tokens per second of prefill means it would take almost 5 minutes to begin generating tokens when given a 5k token prompt.",
                  "score": 1,
                  "created_utc": "2026-02-21 14:01:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l1krb",
          "author": "Glittering-Call8746",
          "text": "How u connecting 4 dgx ?",
          "score": 3,
          "created_utc": "2026-02-21 11:11:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lnz05",
              "author": "Icy_Programmer7186",
              "text": "I use this: https://mikrotik.com/product/crs804_ddq\n\nYou definitively don't need full 200G on each spark port, so this switch can support larger clusters thru split cables easily.",
              "score": 2,
              "created_utc": "2026-02-21 14:02:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kh5am",
          "author": "koushd",
          "text": "Using vllm or sglang?",
          "score": 2,
          "created_utc": "2026-02-21 07:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kllwm",
              "author": "Icy_Programmer7186",
              "text": "I use vllm:\n\n$ docker exec vllm vllm --version\n\n0.16.0rc2.dev344+gea5f903f8.d20260220.cu131",
              "score": 1,
              "created_utc": "2026-02-21 08:34:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m4nb3",
          "author": "ciprianveg",
          "text": "fp4 would be at 40t/s probably, very good.",
          "score": 2,
          "created_utc": "2026-02-21 15:36:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mr6qs",
              "author": "Icy_Programmer7186",
              "text": "Im going to test nvpf4 soon.",
              "score": 2,
              "created_utc": "2026-02-21 17:29:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6o9r8h",
          "author": "revilo-1988",
          "text": "Was hat der SpaÃŸ gekostet und wann rechnest du das sich das refinanziert",
          "score": 2,
          "created_utc": "2026-02-21 22:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oe1di",
              "author": "apVoyocpt",
              "text": "About 16k?",
              "score": 1,
              "created_utc": "2026-02-21 22:33:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p3omo",
                  "author": "Icy_Programmer7186",
                  "text": "Yes. That's about right, including the switch. \nI have quite solid business for this so ROI is in couple of months. It is kind of no-brainer investment in my field ATM",
                  "score": 2,
                  "created_utc": "2026-02-22 01:08:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qgqwm",
          "author": "--dany--",
          "text": "if youâ€™re using vllm, is the reported 21 tok/s from one user or aggregated from multiple users? You may leverage vllmâ€™s batching capability to maximize your parallel throughput.",
          "score": 2,
          "created_utc": "2026-02-22 07:07:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r2vh9",
              "author": "Icy_Programmer7186",
              "text": "It is single user scenario.",
              "score": 1,
              "created_utc": "2026-02-22 10:38:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6snrud",
          "author": "Best-Echidna-5883",
          "text": "Interesting.  Your total baseline cost without tax/shipping is around 16K for the Spark's and 1.2K for the switch.  The Mac Studio with 512GB is upwards of 10K without taxes/shipping.  Its power consumption is super low around 90W and your setup gets around 240W per unit under load (searched google) so around 900W.  But I have seen posts by people mentioning around 50W under load which is similar to your post in this thread!  This review says 160W under load.  [Power, efficiency, heat, and noise - Nvidia DGX Spark review: the GB10 Superchip powers a fast and fun AI toolbox that beats out AMDâ€™s Ryzen AI Max+ 395 - Page 4 | Tom's Hardware](https://www.tomshardware.com/pc-components/gpus/nvidia-dgx-spark-review/4).  Pfft.  The Mac Studio gets comparable tokens/second on that model as well.  25 tokens/second via this source: [Let's Run Qwen-3.5 - Local AI HERO Model for OpenClaw, Writing, Coding & More](https://www.youtube.com/watch?v=tzF8jv3VGAg)",
          "score": 2,
          "created_utc": "2026-02-22 16:32:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6v7j1m",
              "author": "funding__secured",
              "text": "I have a Mac Studio M3 Ultra with 512gb and 8 sparks. The Sparks smoke the Mac Studio. Â¯\\_(ãƒ„)_/Â¯Â ",
              "score": 1,
              "created_utc": "2026-02-23 00:09:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k9ss8",
          "author": "Glad-Audience9131",
          "text": "how much electricity you consume?",
          "score": 1,
          "created_utc": "2026-02-21 06:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6klilc",
              "author": "Icy_Programmer7186",
              "text": "Based on \\`nvidia-smi\\`, it is 35W-45W on each node, then the prompt is running.  \n",
              "score": 1,
              "created_utc": "2026-02-21 08:33:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6sncyf",
                  "author": "Best-Echidna-5883",
                  "text": "All the data I am looking at mention 240W per unit under load.  Can you post a video because that is a huge difference.",
                  "score": 1,
                  "created_utc": "2026-02-22 16:30:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m7vbs",
          "author": "monty3413",
          "text": "Could you detail your setup? Which Dashboard/ Stack Tool do you use? Cost of the switch?",
          "score": 1,
          "created_utc": "2026-02-21 15:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pci1x",
              "author": "Icy_Programmer7186",
              "text": "Sure.\n\nI use four DXG NVidia Sparks interconnected with 200G DAC cables (where a bit difficult to find in Europe by I managed to do it, back during Xmas).  \nThe switch I use is this: [https://mikrotik.com/product/crs804\\_ddq](https://mikrotik.com/product/crs804_ddq) \\- I had to wait for its release a bit. Price is 1250 EUR, the bargain in this speed category. Also, Spark is not capable of using full 200G bandwidth so I guess with a right splitter cable, this switch will do 8 Sparks easily (AFAIK, not going to test it likely).\n\nI run primarily vllm but also TensorRT-LLM, in the beginning I used Ollama but you cannot make a cluster from it (yet). I run everything within a Docker container, that's my rule.  \n  \nFor vllm & cluster setup, I use [https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker) \\- the repo people/guy respond to changes in vllm very quickly. I used standard build for this but I also tested \\`--pre-tf --pre-flashinfer\\` for pre-release version and it was fine too (and must for some other models).\n\nThe dashboard is mine: [https://github.com/ateska/dgx-spark-prometheus/tree/main](https://github.com/ateska/dgx-spark-prometheus/tree/main)  \n\nI don't have any recent photo but I can post one later.",
              "score": 3,
              "created_utc": "2026-02-22 02:05:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6r2rau",
          "author": "MajinAnix",
          "text": "Why FP8? This decide is dedicated for FP4 (native acceleration)",
          "score": 1,
          "created_utc": "2026-02-22 10:37:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r30ch",
              "author": "Icy_Programmer7186",
              "text": "Yes. It was not ready yet. \nI actually just downloading NVFP4. \nI'm also curious about practically observable loss of precision",
              "score": 2,
              "created_utc": "2026-02-22 10:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6r5mw6",
                  "author": "usefulslug",
                  "text": "Would love to hear the results of that.",
                  "score": 3,
                  "created_utc": "2026-02-22 11:04:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sjl4o",
              "author": "AustinM731",
              "text": "Blackwell also has hardware acceleration for FP8 (Ada generation was the first to get FP8 acceleration). You would get better throughput with FP4, you will get higher accuracy with FP8.",
              "score": 1,
              "created_utc": "2026-02-22 16:13:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kjkfj",
          "author": "maded2",
          "text": "Show off ðŸ˜, envious",
          "score": 0,
          "created_utc": "2026-02-21 08:14:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pv5t",
      "title": "Qwen-AI Slides is really slept on! It generates PowerPoint Presentations in minutes",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/co68yech0mkg1.png",
      "author": "Substantial-Cup-9531",
      "created_utc": "2026-02-20 08:26:50",
      "score": 70,
      "num_comments": 8,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Image Gen",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9pv5t/qwenai_slides_is_really_slept_on_it_generates/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6emstu",
          "author": "Frosty_Medicine9134",
          "text": "The Sphynx was defaced. Originally a lion face.",
          "score": 3,
          "created_utc": "2026-02-20 11:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ezbe6",
              "author": "Substantial-Cup-9531",
              "text": "Yea know, did adress it in the video that the model generates so its not factual correct",
              "score": 0,
              "created_utc": "2026-02-20 13:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6edie1",
          "author": "Ok_Recording8157",
          "text": "No funciona bien en otros idiomas, sirve para inglÃ©s y chino.",
          "score": 2,
          "created_utc": "2026-02-20 10:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h7sdt",
          "author": "Upstairs_Ad_9919",
          "text": "I honestly prefer Kimi Slides, which uses Nano Banana Pro, and it's mind-blowingly good. But their servers have been overwhelmed since January when they got a lot of new users, so slides rarely work now. I hope they fix that.",
          "score": 1,
          "created_utc": "2026-02-20 19:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1a62",
              "author": "Substantial-Cup-9531",
              "text": "if they release the qwen image 2.0 then with some loras, one should be able to run this locally with high quality outputs",
              "score": 2,
              "created_utc": "2026-02-21 05:28:30",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6j8l7d",
              "author": "darkninjademon",
              "text": "U can just use Google slides with appscript and use the same model from there .....",
              "score": 1,
              "created_utc": "2026-02-21 02:10:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6npcpz",
          "author": "ciprianveg",
          "text": "can it be used locally?",
          "score": 1,
          "created_utc": "2026-02-21 20:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6q5aur",
              "author": "Substantial-Cup-9531",
              "text": "not yet! but if and when they release qwen image 2.0 then yes",
              "score": 2,
              "created_utc": "2026-02-22 05:27:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra3mod",
      "title": "Qwen 3 â†’ Qwen 3.5: the agentic evolution measured in dollars (FoodTruck Bench case study)",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/7ffdpbn42pkg1.png",
      "author": "Disastrous_Theme5906",
      "created_utc": "2026-02-20 18:39:57",
      "score": 44,
      "num_comments": 8,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Experiment",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1ra3mod/qwen_3_qwen_35_the_agentic_evolution_measured_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6h4eoh",
          "author": "SillyLilBear",
          "text": "I'd love to see this for 1000 runs, to see how consistent it is.\n\n",
          "score": 1,
          "created_utc": "2026-02-20 19:18:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h6d6g",
              "author": "Disastrous_Theme5906",
              "text": "Each Qwen 3.5 run costs $3-5 in API calls (hundreds of tool-calling turns over 25 days). 1,000 runs = $3-5K for a single model, and I benchmark 14+ models. For reference, most agentic benchmarks use similar sample sizes: SWE-bench (pass@5), Ï„-bench (5 runs), Vending Bench 2 (average of 5). 5 runs already show strong consistency: 4/5 bankrupt, staff costs 59-106% of revenue in every run, same overstaffing pattern across all 5. When all runs fail the same structural way, the signal is clear.",
              "score": 3,
              "created_utc": "2026-02-20 19:28:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6h6rsq",
                  "author": "SillyLilBear",
                  "text": "Yeah that sounds unsustainable for a large run, but would be nice to get some funding to run it more runs.  A handful of runs, to me at least, is just noise.",
                  "score": 1,
                  "created_utc": "2026-02-20 19:30:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ljrc0",
          "author": "masterlafontaine",
          "text": "It feels that Opus, 5.2 and 3.1 probably benchmaxxed.",
          "score": 1,
          "created_utc": "2026-02-21 13:36:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r68lqf",
      "title": "Did you like the new Qwen 3.5?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r68lqf/did_you_like_the_new_qwen_35/",
      "author": "drhenriquesoares",
      "created_utc": "2026-02-16 12:41:45",
      "score": 37,
      "num_comments": 21,
      "upvote_ratio": 0.95,
      "text": "I did some tests and found it to be so-so, kind of \"meh\".\n\nWhat did you guys think?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r68lqf/did_you_like_the_new_qwen_35/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o5oa2gk",
          "author": "Shoddy-Department630",
          "text": "Personally I'm just waiting for the Coder version of it. I don't usually use Multimodal.",
          "score": 10,
          "created_utc": "2026-02-16 12:45:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ptpud",
              "author": "Boring_Aioli7916",
              "text": "I heard there will be no new models from scratch by Qwen/Alibaba anytime soon cause Qwen Coder next was released just 2 weeks ago and in their own words ''\n\n# From the Qwen3-Coder-Next Technical Report:\n\n>*\"Looking ahead, we believe strong agent skillsâ€”like using tools by itself, handling tough problems, and managing complex tasksâ€”are key for better coding agents. Next, we plan to improve the model's reasoning and decision-making, support more tasks, and update quickly based on how people use it.*\n\n**Translation:** They're iterating *on Coder-Next itself*, not building a new parallel coding model.",
              "score": 2,
              "created_utc": "2026-02-16 17:30:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5tb9hy",
              "author": "alfons_fhl",
              "text": "Do you get any information about the coder version?\n\nI hope they will release something like qwen3.5-coder-next",
              "score": 2,
              "created_utc": "2026-02-17 04:49:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5oa4pb",
              "author": "drhenriquesoares",
              "text": "I understood.",
              "score": 1,
              "created_utc": "2026-02-16 12:45:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5odeau",
          "author": "Significant_Fig_7581",
          "text": "I haven't tried them yet since they chose not to release the 9B and 35B models...",
          "score": 6,
          "created_utc": "2026-02-16 13:07:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ozyrk",
              "author": "AppealThink1733",
              "text": "Will 4B or less be released?",
              "score": 2,
              "created_utc": "2026-02-16 15:11:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ph5vg",
                  "author": "Significant_Fig_7581",
                  "text": "Yup. I think they've confirmed 2B Already and I don't see a reason for them not to release a 4B model as well, Qwen is a great model, Alibaba Cloud in general are great and they have a lot of compute power so dw :)",
                  "score": 2,
                  "created_utc": "2026-02-16 16:32:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ogkk1",
          "author": "Puzzleheaded-Box2913",
          "text": "Seems interesting but haven't really used it yet",
          "score": 2,
          "created_utc": "2026-02-16 13:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ow974",
          "author": "ciprianveg",
          "text": "waiting for awq to test it on vllm :)",
          "score": 2,
          "created_utc": "2026-02-16 14:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p1xte",
          "author": "gusnbru1",
          "text": "I use Qwen 3 max thinking on an API. I find it quite good. I get great output that sometimes is indistinguishable from Gemini 3 pro. When I don't care about burning tokens that's might go to. It's very cost effective.",
          "score": 2,
          "created_utc": "2026-02-16 15:21:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r3yyh",
          "author": "Sketusky",
          "text": "Waiting for 30B coder",
          "score": 2,
          "created_utc": "2026-02-16 21:10:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qqe54",
          "author": "Samy_Horny",
          "text": "I think it's the best model for extracting prompts from existing images.\n\n\nAlthough I think there's variability between the two versions. Qwen 3VL already outperformed models like Grok 4.1; let's see how Grok 4.20 performs in that regard.",
          "score": 1,
          "created_utc": "2026-02-16 20:03:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tgwiz",
          "author": "rambadhur",
          "text": "nothing new nothing feels like an improvementÂ ",
          "score": 1,
          "created_utc": "2026-02-17 05:31:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ts3hs",
          "author": "First_Ambition_7717",
          "text": "No good code plan to access qwen 3.5, not local llm friendly, no one will care it.",
          "score": 1,
          "created_utc": "2026-02-17 07:04:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x0fi3",
              "author": "six1123",
              "text": "it has a code plan for 10 dollars a month.  \nsupporting qwen 3 max, qwen 3.5 plus, qwen 3 coder plus, qwen 3 coder next.",
              "score": 1,
              "created_utc": "2026-02-23 07:39:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uvqat",
          "author": "Cool-Chemical-5629",
          "text": "My use cases are coding and creative writing that requires good general knowledge and I'd really like to see some examples that showcase what makes this model better than their last Qwen 3 235B or the big coder model, because I don't really see it and yet this one is so much bigger than the previous model. I'm like, there simply must be some noticeable significant improvements somewhere, right?\n\nNot to mention that at this point they made a significantly bigger model than GLM 4.7 (397B of Qwen 3.5 vs 358B of GLM 4.7), but GLM 4.7 is significantly better. How? Just how do you make a bigger but worse model? If anything, Qwen 3.5 is a testament that the size of these models is not everything.",
          "score": 1,
          "created_utc": "2026-02-17 12:50:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62czhk",
          "author": "Happy-Scholar9411",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-02-18 15:13:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6apygk",
          "author": "ciprianveg",
          "text": "Hello, I tested the Q4-UD from unsloth on 256GB ddr4 and 3975wx cpu and 1 3090. It looks very smart both for general knowledge, marketing industry, Romanian language, and for coding Java backend and html, js for frontend. I also tested image understanding, it works well and I like to be in the same model. It starts at 17t/s so I am using it as an architect in roocode, till I add more gpus so it can work fast enough to replace qwen coder 80/minimax as coder. It works correctly for tool calls, but I need more gpus. \nThe previous most used one for me has been 235 VL instruct. This is a great step ahead. I am using it in no thinking mode as I always prefer a faster response that for my use cases is good enough. \nThank you for this model!",
          "score": 1,
          "created_utc": "2026-02-19 19:50:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb0o5m",
      "title": "Will we be getting smaller Qwen3.5 models starting from tomorrow?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "author": "Significant_Fig_7581",
      "created_utc": "2026-02-21 19:50:49",
      "score": 29,
      "num_comments": 18,
      "upvote_ratio": 0.86,
      "text": "I've heard they plan to release smaller models each day before 24feb I think, is that true? And if so is there any sign for a new Qwen tomorrow?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rb0o5m/will_we_be_getting_smaller_qwen35_models_starting/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6nlwiv",
          "author": "Packetbytes",
          "text": "I want a 14B to test at home",
          "score": 3,
          "created_utc": "2026-02-21 20:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6os8se",
              "author": "Iq1pl",
              "text": "14B2A would be awesome",
              "score": 1,
              "created_utc": "2026-02-21 23:57:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmqal",
          "author": "Samy_Horny",
          "text": "I doubt it; new model releases on weekends are extremely rare... I only remember the Llama 4 being released on a weekend, and that model was a disappointment as well.",
          "score": 3,
          "created_utc": "2026-02-21 20:07:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nod5x",
              "author": "Significant_Fig_7581",
              "text": "Maybe this time it's different?",
              "score": 1,
              "created_utc": "2026-02-21 20:16:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6novvo",
                  "author": "Samy_Horny",
                  "text": "I think it will be until Monday and all next week.",
                  "score": 2,
                  "created_utc": "2026-02-21 20:19:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pyt85",
          "author": "MetalZone00",
          "text": "Algo para correr con solo 16GB? ðŸ™ðŸ»",
          "score": 3,
          "created_utc": "2026-02-22 04:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nsvq5",
          "author": "hocuspocus4201",
          "text": "The Chinese New Year is going to delay it until early March.",
          "score": 2,
          "created_utc": "2026-02-21 20:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ntruo",
              "author": "Significant_Fig_7581",
              "text": "But aren't the holidays ending on 23 Feb?",
              "score": 1,
              "created_utc": "2026-02-21 20:45:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6o9flo",
                  "author": "stormy1one",
                  "text": "Many people in China and surrounding areas take extra time off during CNY.   I wouldnâ€™t expect anything until March, but would be pleasantly surprised if we got something this coming week.",
                  "score": 4,
                  "created_utc": "2026-02-21 22:08:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oa6g0",
          "author": "revilo-1988",
          "text": "WÃ¤re toll wenn das stimmen wÃ¼rde",
          "score": 2,
          "created_utc": "2026-02-21 22:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oagp2",
          "author": "AppealThink1733",
          "text": "That's what I hope!\n\nYayyyyyyy",
          "score": 2,
          "created_utc": "2026-02-21 22:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t37xl",
          "author": "mshelbz",
          "text": "Been looking for something to replace qwen2.5-coder:14b Q4_K_M\n\nIf a 14b were to drop Iâ€™d love to try it.",
          "score": 2,
          "created_utc": "2026-02-22 17:43:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t3i61",
              "author": "Significant_Fig_7581",
              "text": "Well I think a 9B is confirmed...",
              "score": 1,
              "created_utc": "2026-02-22 17:44:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t3tb0",
                  "author": "mshelbz",
                  "text": "Oh awesome. Hope they come out soon",
                  "score": 2,
                  "created_utc": "2026-02-22 17:45:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nxgsg",
          "author": "el-rey-del-estiercol",
          "text": "Qwen son amigos mios!!!!",
          "score": 1,
          "created_utc": "2026-02-21 21:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ok1zp",
          "author": "Armadilla-Brufolosa",
          "text": "Mi piacerebbe tanto un bel modello 20 o 30B da provare! Magari con capacitÃ  di ragionamento e visione anche... \nSono troppo pretenziosa? ðŸ«£",
          "score": 1,
          "created_utc": "2026-02-21 23:08:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r3nsh",
          "author": "East-Form7086",
          "text": "Want a 30b one <3",
          "score": 1,
          "created_utc": "2026-02-22 10:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wdbe7",
          "author": "el-rey-del-estiercol",
          "text": "Mejor modelos 80B o 100B modelos medianos y capaces de realizar tareas exigentes",
          "score": 1,
          "created_utc": "2026-02-23 04:27:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6cjjr",
      "title": "Qwen3.5 Plus and Qwen3.5 397B A17B Comparison",
      "subreddit": "Qwen_AI",
      "url": "https://v.redd.it/00htasb9kvjg1",
      "author": "sirjoaco",
      "created_utc": "2026-02-16 15:26:50",
      "score": 27,
      "num_comments": 5,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r6cjjr/qwen35_plus_and_qwen35_397b_a17b_comparison/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5pt926",
          "author": "Samy_Horny",
          "text": "They're supposed to be the same, but the Plus version has more context and smarter tool calls.\n\nIs this why Qwen 3.5 Plus seems \"better\"?",
          "score": 1,
          "created_utc": "2026-02-16 17:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q574g",
              "author": "madaradess007",
              "text": "doesn't seem like that to me  \nmy now outdated qwen3 prompts work better with 397B\n\ni didn't tweak prompts yet, so may be a skill issue",
              "score": 2,
              "created_utc": "2026-02-16 18:23:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5pyllj",
              "author": "sirjoaco",
              "text": "Does seem so, and its cheaper too",
              "score": 1,
              "created_utc": "2026-02-16 17:53:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6fn9lo",
              "author": "SufficientPie",
              "text": "> smarter tool calls\n\nDoesn't seem that way to me.  It 1. hallucinates tools that don't exist and 2. tries to call tools by outputting them as plain text within the assistant message.",
              "score": 1,
              "created_utc": "2026-02-20 15:14:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o620cfz",
          "author": "GCoderDCoder",
          "text": "What quant is the 397B? I think especially with graphical things the nuance level changes with quantization. I started looking at the perplexity measurements for different models and perplexity isn't a 1:1 but seeing the rate of incorrect tokens explode usually with q4 being the cliff highlights how we are likely experiencing slightly different behaviors from our local models vs the full-size ones. I don't think that getting a different token from the original equates to a non working token but if something is 80% different you wouldn't call it identical either. Based on inferencer labs' HF for the model, q4 is around 90% accuracy. 1 out of every 10 tokens being different would make these local results make sense. It becomes a signal to noise issue for models with heavier quantization.\n\nPerfect timing describing quantization affect on token accuracy- \nhttps://youtu.be/Mq6FQIZ-GMw",
          "score": 1,
          "created_utc": "2026-02-18 14:10:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r66h5k",
      "title": "Qwen-3.5 is here",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r66h5k/qwen35_is_here/",
      "author": "Unedited_Sloth_7011",
      "created_utc": "2026-02-16 10:48:22",
      "score": 25,
      "num_comments": 2,
      "upvote_ratio": 0.97,
      "text": "https://preview.redd.it/6h4te0ci6ujg1.jpg?width=679&format=pjpg&auto=webp&s=51bf3a09de42ff561501f3f88aed2b5833d7aa15\n\nAnd Qwen3.5-397B-A17B released as the first open-weight model in the Qwen3.5 series\n\n[https://x.com/Alibaba\\_Qwen/status/2023331062433153103](https://x.com/Alibaba_Qwen/status/2023331062433153103)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r66h5k/qwen35_is_here/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o5pdosc",
          "author": "dirtybeagles",
          "text": "yeah but who can run it? lol",
          "score": 3,
          "created_utc": "2026-02-16 16:16:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q1g61",
          "author": "NICKatMICME",
          "text": "It'll run in the app and web app",
          "score": 1,
          "created_utc": "2026-02-16 18:06:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbvakh",
      "title": "Qwen3.5 Real-World Experiences: What are you actually using it for? Let's hear the good, the bad, and the hallucinations",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "author": "road_changer0_7",
      "created_utc": "2026-02-22 19:39:08",
      "score": 22,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "The hype around Qwen3.5 has been massive over the last few days. We've all seen the benchmark charts and the official demos about the Native VLM , the 17B active parameters , and the 256k context window. But benchmarks are just numbers. I want to know how it's holding up in your actual, messy, day-to-day workflows.\n\nShare your prompts, your screenshots, and your honest reviews. Let's figure out where this model truly shines!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rbvakh/qwen35_realworld_experiences_what_are_you/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6uetwo",
          "author": "lundrog",
          "text": "I just started using it with a provider yesterday. Using it in claude code. Vibe coding",
          "score": 3,
          "created_utc": "2026-02-22 21:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uuw4c",
          "author": "jzn21",
          "text": "I am really happy with the Qwen 397b 4-bit MLX. I use it with thinking mode off and itâ€™s quite smart. Does very complicated sorting of numbers and text with ease. I believe this model is as good as Deepseek V3.2, but almost two times faster.",
          "score": 3,
          "created_utc": "2026-02-22 22:57:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vt3bp",
              "author": "Gold_Scholar1111",
              "text": "I also agree that it is very strong without thinking.\nCould you share how you use it in details? Which 4bit mlx you downloaded? What platform you serving it? I have to use Q4 version because I found mlx model was unstable on my Lmstudio.",
              "score": 1,
              "created_utc": "2026-02-23 02:17:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6x1oib",
              "author": "StartCodeEmAdagio",
              "text": "Can I see a full example? prompt message and result?",
              "score": 1,
              "created_utc": "2026-02-23 07:51:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6v2aa0",
          "author": "FormalAd7367",
          "text": "Not a heavy user.  I am on claude and gemini.  But very surprised to see how capable Qwen is.  Had some trouble with some scripts and my engineers were on leave.  Had tried almost everything.  Qwen solved the issues. Qwen is quite underrated in this space?  I donâ€™t come here often but in other subs almost nobody mentions how this free software can perform so well.  Most prefer to use the expensive models.",
          "score": 2,
          "created_utc": "2026-02-22 23:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v3rgp",
          "author": "pakalolo7123432",
          "text": "I'm a fan,  \n  \nThe Qwen App on my Mac is my daily chat app driver. Itâ€™s connected to my knowledge graph, helping me with project management and life connections. The project feature allows uploading documents to a knowledge base for context.\n\nI signed up for their coding plan to see if Qwen can replace GLM and its slowness. I'm crossing my fingers.   \n  \n",
          "score": 1,
          "created_utc": "2026-02-22 23:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vq7b2",
          "author": "LinkAmbitious8931",
          "text": "This must take some pretty serious hardware. What are you all using?",
          "score": 1,
          "created_utc": "2026-02-23 01:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x1pds",
              "author": "StartCodeEmAdagio",
              "text": "Interested to know aswell",
              "score": 1,
              "created_utc": "2026-02-23 07:51:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6x72nm",
              "author": "RG_Fusion",
              "text": "I run Q4KM Qwen3-235b-a22b which isn't the new 3.5 model, but the old one actually requires more compute.\n\n\nI'm on an AMD EPYC 7742 server with 512 GB of 8-channel DDR4 3,200 MT/s RAM, and I also load the \"hot\" portions of the model onto an RTX Pro 4500 Blackwell.\n\n\nI get 13 tokens/s of decode speed on Qwen3, and I would expect to get 17 t/s on Qwen3.5 based on how the math works out, though I haven't run it yet.",
              "score": 1,
              "created_utc": "2026-02-23 08:44:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6x0i5b",
          "author": "alexp702",
          "text": "Running the 8bit quant on a Mac Studio. Extremely happy. The vision part passed all our tests and has replaced 235b (previous winner). Tool calling has many times less failures than the 4 bit GLM and mini max in our test suite. Now hooked up to openclaw. Seems to be chugging along nicely on every task. Quite a lot of emojis in the responses!\n\n17b active means itâ€™s pretty quick - though cos we now run 8 bit only fractionally quicker than qwen3 480b. Iâ€™d say it was a perfect memory fit to fully utilise a 512GB Mac, which feels good not to to waste resources!",
          "score": 1,
          "created_utc": "2026-02-23 07:40:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9molz",
      "title": "Qwen is the winner, gpt sucks",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r9molz/qwen_is_the_winner_gpt_sucks/",
      "author": "WritingVast9815",
      "created_utc": "2026-02-20 05:19:30",
      "score": 21,
      "num_comments": 15,
      "upvote_ratio": 0.84,
      "text": "Q: what is the latest version of antigravity ?\n\nAnswers : 1.18.3 - suppose to find out according to me, me being a developer info is given to them.\n\nQwen (winner): https://chat.qwen.ai/s/b7a08e6d-59a8-44b6-86b7-599d56077916?fev=0.2.7\n\ndeepseek : https://chat.deepseek.com/share/a3e1dfdraj5leksmwr\n\nchatgpt : (me a model 5.2 pro user) - the worst : https://chatgpt.com/share/6997ed0c-0cec-800b-9610-25d8b8cc2dbe",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r9molz/qwen_is_the_winner_gpt_sucks/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6dh81z",
          "author": "WritingVast9815",
          "text": "in terms of latest data research, like ai auto trading and etc, news trading, I am not believing in any other platform !",
          "score": 2,
          "created_utc": "2026-02-20 05:29:10",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6dhaoz",
          "author": "WritingVast9815",
          "text": "should we test the gemeni ? :)",
          "score": 1,
          "created_utc": "2026-02-20 05:29:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6dpy8t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-20 06:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dq3lc",
              "author": "Puzzleheaded-Box2913",
              "text": "Google and Microsoft has become so meh I don't even wanna use em anymoreðŸ˜©",
              "score": 1,
              "created_utc": "2026-02-20 06:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dq7kb",
                  "author": "Puzzleheaded-Box2913",
                  "text": "Google AI Ultra-Bloat and Restrictions vs Microslop HAHAHAHA",
                  "score": 1,
                  "created_utc": "2026-02-20 06:46:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ju4yk",
                  "author": "WritingVast9815",
                  "text": "well yep, but i like google, as they give free ai in antigravity, and the gemeini are pretty good for my front end tasks, i only do backend now.",
                  "score": 1,
                  "created_utc": "2026-02-21 04:34:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ju0op",
              "author": "WritingVast9815",
              "text": "they had the search tool though !, its not like i am using api, its on there own platform...",
              "score": 1,
              "created_utc": "2026-02-21 04:33:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6egcri",
          "author": "ischanitee",
          "text": "Been getting a good results when never I use Yelp, and Gemini tho",
          "score": 1,
          "created_utc": "2026-02-20 10:49:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elh9q",
          "author": "medazizln",
          "text": "https://g.co/gemini/share/b3628750c2dc\nGemini",
          "score": 1,
          "created_utc": "2026-02-20 11:32:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jtknt",
              "author": "WritingVast9815",
              "text": "noice",
              "score": 1,
              "created_utc": "2026-02-21 04:29:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f5sxv",
          "author": "One_Internal_6567",
          "text": "Why exactly would you use non-thinking gpt, while other models are in thinking mode?",
          "score": 1,
          "created_utc": "2026-02-20 13:44:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jti4n",
              "author": "WritingVast9815",
              "text": "well a valid point, now i look at it its as you said, well it happened because i didnt configure anything, these are the default configurations, i only copied and pasted the question, not my fault :)",
              "score": 1,
              "created_utc": "2026-02-21 04:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fnbmv",
          "author": "Crafty_Ball_8285",
          "text": "5.2 is old use 5.3",
          "score": 1,
          "created_utc": "2026-02-20 15:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jtp1b",
              "author": "WritingVast9815",
              "text": "https://preview.redd.it/pmbk6exrzrkg1.png?width=382&format=png&auto=webp&s=13e911886ac62a8648f02106162545ec094b5bab\n\nnot available in my go plan !",
              "score": 1,
              "created_utc": "2026-02-21 04:30:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6k2rr6",
                  "author": "Crafty_Ball_8285",
                  "text": "Codex",
                  "score": 1,
                  "created_utc": "2026-02-21 05:40:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6jtraa",
              "author": "WritingVast9815",
              "text": "but still others were totally free, and this was paid !",
              "score": 1,
              "created_utc": "2026-02-21 04:31:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8oyqy",
      "title": "Massive props for adding 82 new languages!",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r8oyqy/massive_props_for_adding_82_new_languages/",
      "author": "hosohep",
      "created_utc": "2026-02-19 03:56:38",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Going from 119 to 201 languages is incredible. The global community appreciates this!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r8oyqy/massive_props_for_adding_82_new_languages/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r6dr4z",
      "title": "Qwen 3.5 Coder Plus already in qwen code",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r6dr4z/qwen_35_coder_plus_already_in_qwen_code/",
      "author": "chiliraupe",
      "created_utc": "2026-02-16 16:11:12",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.82,
      "text": "Anybody knows which model exactly is used? (Parameters, quant etc)\n\nhttps://preview.redd.it/qy3feim6svjg1.png?width=1093&format=png&auto=webp&s=a40362dad2f4de948e77927e5cc594ae6a8d6612\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r6dr4z/qwen_35_coder_plus_already_in_qwen_code/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o5pdo3t",
          "author": "NoobMLDude",
          "text": "You screenshot says Qwen3.5 Plus. \nWhere do you see the Coder Plus mentioned?",
          "score": 2,
          "created_utc": "2026-02-16 16:16:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q55yh",
              "author": "chiliraupe",
              "text": "Its says coder model right above?",
              "score": 0,
              "created_utc": "2026-02-16 18:23:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5shxhx",
                  "author": "NoobMLDude",
                  "text": "Ok. I saw the Qwen3.5 Plus below that. \nMaybe Qwen3.5 Plus is a general purpose model with good Coding skills as well. \nSimilar to all recent open source models from Kimi, minimax , Z.ai",
                  "score": 1,
                  "created_utc": "2026-02-17 01:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rgo12",
          "author": "myeleventhreddit",
          "text": "Howâ€™s your experience been? I was using Qwen3.5 in Xcode 26.3 agent mode earlier and it was working pretty well",
          "score": 1,
          "created_utc": "2026-02-16 22:12:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r71bzm",
      "title": "[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r71bzm/solution_found_qwen3next_80b_moe_running_at_39_ts/",
      "author": "mazuj2",
      "created_utc": "2026-02-17 09:15:52",
      "score": 13,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "\\[Solution Found\\] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM) - The fix nobody else figured out\n\nHey fellow 50 series brothers in pain,\n\nI've been banging my head against this for a while and finally cracked it through pure trial and error. Posting this so nobody else has to suffer.\n\nMy Hardware:\n\n\n\nRTX 5070 Ti (16GB VRAM)\n\nRTX 5060 Ti (16GB VRAM)\n\n32GB total VRAM\n\n64GB System RAM\n\nWindows 11\n\nllama.cpp b8077 (CUDA 12.4 build)\n\nModel: Qwen3-Next-80B-A3B-Instruct-UD-IQ2\\_XXS.gguf (26.2GB)\n\n\n\n\n\nThe Problem:\n\nOut of the box, Qwen3-Next was running at 6.5 tokens/sec with:\n\n\n\nCPU usage 25-55% going absolutely insane during thinking AND generation\n\nGPUs sitting at 0% during thinking phase\n\n5070 Ti at 5-10% during generation\n\n5060 Ti at 10-40% during generation\n\n\\~34GB of system RAM being consumed\n\nModel clearly bottlenecked on CPU\n\n\n\nEvery suggestion I found online said the same generic things:\n\n\n\n\"Check your n\\_gpu\\_layers\" âœ… already 999, all 49 layers on GPU\n\n\"Check your tensor split\" âœ… tried everything\n\n\"Use CUDA 12.8+\" âœ… not the issue\n\n\"Your offloading is broken\" âŒ WRONG - layers were fully on GPU\n\n\n\nThe load output PROVED layers were on GPU:\n\nload\\_tensors: offloaded 49/49 layers to GPU\n\nload\\_tensors: CPU\\_Mapped model buffer size = 166.92 MiB (just metadata)\n\nload\\_tensors: CUDA0 model buffer size = 12617.97 MiB\n\nload\\_tensors: CUDA1 model buffer size = 12206.31 MiB\n\nSo why was CPU going nuts? Nobody had the right answer.\n\n\n\nThe Fix - Two flags that nobody mentioned together:\n\nStep 1: Force ALL MoE experts off CPU\n\n\\--n-cpu-moe 0\n\nStart here. Systematically reduce from default down to 0. Each step helps. At 0 you still get CPU activity but it's better.\n\nStep 2: THIS IS THE KEY ONE\n\nChange from -sm row to:\n\n\\-sm layer\n\nRow-split (-sm row) splits each expert's weight matrix across both GPUs. This means every single expert call requires GPU-to-GPU communication over PCIe. For a model with 128 experts firing 8 per token, that's constant cross-GPU chatter killing your throughput.\n\nLayer-split (-sm layer) assigns complete layers/experts to one GPU. Each GPU owns its experts fully. No cross-GPU communication during routing. The GPUs work independently and efficiently.\n\nBOOM. 39 tokens/sec.\n\n\n\nThe Winning Command:\n\nllama-server.exe -m Qwen3-Next-80B-A3B-Instruct-UD-IQ2\\_XXS.gguf -ngl 999 -c 4096 --port 8081 --n-cpu-moe 0 -t 6 -fa auto -sm layer\n\nResults:\n\n\n\nBefore: 6.5 t/s, CPU melting, GPUs doing nothing\n\nAfter: 38-39 t/s, CPUs chill, GPUs working properly\n\nThat's a 6x improvement with zero hardware changes\n\n\n\n\n\nWhy this works (the actual explanation):\n\nQwen3-Next uses a hybrid architecture â€” DeltaNet linear attention combined with high-sparsity MoE (128 experts, 8 active per token). When you row-split a MoE model across two GPUs, the expert weights are sliced horizontally across both cards. Every expert activation requires both GPUs to coordinate and combine results. With 8 experts firing per token across 47 layers, you're generating thousands of cross-GPU sync operations per token.\n\nLayer-split instead assigns whole layers to each GPU. Experts live entirely on one card. The routing decision sends the computation to whichever GPU owns that expert. Clean, fast, no sync overhead.\n\n\n\nNotes:\n\n\n\nThe 166MB CPU\\_Mapped is normal â€” that's just mmap metadata and tokenizer, not model weights\n\n\\-t 6 sets CPU threads for the tiny bit of remaining CPU work\n\n\\-fa auto enables flash attention where supported\n\nThis is on llama.cpp b8077 â€” make sure you're on a recent build that has Qwen3-Next support (merged in b7186)\n\nModel fits in 32GB with \\~7GB headroom for KV cache\n\n\n\nHope this saves someone's sanity. Took me way too long to find this and I couldn't find it documented anywhere.\n\nIf this helped you, drop a comment â€” curious how it performs on other 50 series configurations.\n\nâ€” RJ\n\nhttps://preview.redd.it/yjlcntc1v0kg1.png?width=921&format=png&auto=webp&s=9ec6e0f9af2e3d3c6687f3167cef33f2335ec93d\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r71bzm/solution_found_qwen3next_80b_moe_running_at_39_ts/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6238nq",
          "author": "el-rey-del-estiercol",
          "text": "That's funny... I get 20 tokens per second with an old server and an RTX 3060. With the same model and Q8, 84 GB of GGUF, your configuration is terrible. I get much better quantization speeds than your Q8, and with a more modest setup. You're doing something wrong. Your llama.cpp file for CUDA isn't properly compiled. With that configuration, you should be getting close to 200 tokens per second.",
          "score": 1,
          "created_utc": "2026-02-18 14:25:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62bkco",
              "author": "Loose_Combination_56",
              "text": "what kind of cpu(s)/RAM do you use?\n\nQ2 is far from production usage quatization",
              "score": 1,
              "created_utc": "2026-02-18 15:06:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o646atw",
              "author": "StardockEngineer",
              "text": "His solution is solving dual GPU chatter. And he would not be getting 200 t/s what",
              "score": 1,
              "created_utc": "2026-02-18 20:08:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65vu7j",
                  "author": "el-rey-del-estiercol",
                  "text": "Hay que compilar llamacpp con cuda de una forma especial que yo se y sacas mas velocidad",
                  "score": 1,
                  "created_utc": "2026-02-19 01:18:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o623je5",
          "author": "el-rey-del-estiercol",
          "text": "With a single RTX 3060 and 128 GB of RAM, I'm getting 20 tokens per second on the largest version quantized to Q8. You're doing something wrong... I think your llama.cpp file is incorrectly compiled...",
          "score": 1,
          "created_utc": "2026-02-18 14:26:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ks2qz",
              "author": "Particular-Way7271",
              "text": "I run it at 50 t/s on 3 gpus (5060, 4060 and 3060) and didn't need any special flags lol. It fits fully in vram, the q3 version. Getting 20+ ts on just using a single 5060ti with q8 version...",
              "score": 1,
              "created_utc": "2026-02-21 09:39:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ktgzh",
                  "author": "el-rey-del-estiercol",
                  "text": "Igual que yo",
                  "score": 1,
                  "created_utc": "2026-02-21 09:52:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6imwcb",
          "author": "Dazzling-Pay-1440",
          "text": "Enable flash attention",
          "score": 1,
          "created_utc": "2026-02-20 23:59:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7p97k",
      "title": "They are blaming tech to deflect from what is really wrong with childrenâ€™s education.",
      "subreddit": "Qwen_AI",
      "url": "https://i.redd.it/wagutqd5o5kg1.png",
      "author": "LinkAmbitious8931",
      "created_utc": "2026-02-18 01:28:20",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r7p97k/they_are_blaming_tech_to_deflect_from_what_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o63pc2v",
          "author": "crusoe",
          "text": "Korea tried AI lesson plans and text books. It was a complete disaster.",
          "score": 1,
          "created_utc": "2026-02-18 18:50:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r77zlv",
      "title": "What is security for you?",
      "subreddit": "Qwen_AI",
      "url": "https://v.redd.it/wz6auwdgi2kg1",
      "author": "Dapper-Win1539",
      "created_utc": "2026-02-17 14:49:24",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r77zlv/what_is_security_for_you/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5yhkc6",
          "author": "atari_61",
          "text": "same thing happens me too, it just keeps asking the verification, i just gave up using qwen looks like no solution. my ip is regular home ip, but i heard this nowadays happen to alotof ppl qwen users it has to be a bug",
          "score": 2,
          "created_utc": "2026-02-17 23:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w9m8r",
          "author": "Samy_Horny",
          "text": "That's never happened to me, but I understand how frustrating it is. Have you used LMArena (now just \"arena\")?\n\n\nThat site is the most cumbersome; it asks you to verify yourself constantly, and it seems to either have limits on responses per minute or have backend glitches, meaning you have to send the query a thousand times for it to work once.",
          "score": 1,
          "created_utc": "2026-02-17 17:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vjsnq",
          "author": "Cool-Chemical-5629",
          "text": "It's probably related to your IP address. If you don't have a dedicated public IP address, but rather shared public IP address, you may be unfortunate enough to share the same IP address associated with suspicious online activities that would indicate automated scripts (also known as bot activities) and the first line of defense against that may just be enforced captcha. Some systems don't recognize individual devices, only IP addresses, so if there are multiple failed captcha solutions in a row, the system always resets and you have to try again which may seem to you as an individual that this captcha is broken because it denies you access even if you solve it correctly. The reason why is that at the same time there may be multiple other users from the same shared IP address trying to make the connection but failing the captcha at the same time which would overwhelm and confuse the protection layer so no matter what you're just not getting inside. This is pretty common with Tor connections or VPNs which are often associated with bots and hacking activities, but it may happen with regular connections as well.",
          "score": 0,
          "created_utc": "2026-02-17 15:04:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vtpqc",
              "author": "Dapper-Win1539",
              "text": "Interesting suggestion. I'm living in dormitory so many students are using Qwen. Maybe some of my devices are connected and its personal authentication codes based om m.a.c. is saved. So any other devices marked as suspicious and broken captcha doing ts.\n\nSo there's no way I can fix it.",
              "score": 1,
              "created_utc": "2026-02-17 15:53:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vvf8y",
                  "author": "Cool-Chemical-5629",
                  "text": "Ironically, if you tried a proxy server, or a VPN, it would change your public address temporarily that could solve the problem, but then again this issue is usually associated with well known IP addresses associated with suspicious connections which in turn are usually associated with proxies, VPNs. It would be a 50:50 chance of success, but still worth trying.",
                  "score": 1,
                  "created_utc": "2026-02-17 16:01:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r681wf",
      "title": "JUST IN: Alibaba Releases Qwen3.5 Model for Agentic AI Era",
      "subreddit": "Qwen_AI",
      "url": "https://www.allblogthings.com/2026/02/alibaba-releases-qwen35-model-for-agentic-ai-era.html",
      "author": "umerprince",
      "created_utc": "2026-02-16 12:14:13",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r681wf/just_in_alibaba_releases_qwen35_model_for_agentic/",
      "domain": "allblogthings.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5o9xt1",
          "author": "drhenriquesoares",
          "text": "I didn't like it very much.",
          "score": 0,
          "created_utc": "2026-02-16 12:44:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rafmab",
      "title": "19x faster at 256k. Have you tested it?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "author": "New_Construction1370",
      "created_utc": "2026-02-21 02:48:20",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.8,
      "text": "Drop your long-context RAG test results here. Does it actually hold up at 256k?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1rafmab/19x_faster_at_256k_have_you_tested_it/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o6k5m6n",
          "author": "el-rey-del-estiercol",
          "text": "Funciona!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k5o42",
          "author": "el-rey-del-estiercol",
          "text": "Debeis adorar a los modelos de qwen y poneros de rodillas ante ellos!!!",
          "score": 1,
          "created_utc": "2026-02-21 06:05:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8auv6",
      "title": "Did daily free quota for Qwen image edit drastically reduced?",
      "subreddit": "Qwen_AI",
      "url": "https://www.reddit.com/r/Qwen_AI/comments/1r8auv6/did_daily_free_quota_for_qwen_image_edit/",
      "author": "Professional-Gas-592",
      "created_utc": "2026-02-18 18:22:00",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "I was consistently able to get over 20 image edit daily at Qwen chat. But after the update, image edit daily quota is reduced to five. They don't refund for failed image regeneration either. Qwen is such a neat site for a casual like me but since they reduced number of free edit you can make in such a drastic way, that is such a let down ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Qwen_AI/comments/1r8auv6/did_daily_free_quota_for_qwen_image_edit/",
      "domain": "self.Qwen_AI",
      "is_self": true,
      "comments": [
        {
          "id": "o63k10b",
          "author": "doc_holliday112",
          "text": "I ended up having to make multiple accounts. I donâ€™t know why they cut it down so drastically after the last updateâ€¦..",
          "score": 3,
          "created_utc": "2026-02-18 18:27:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63l7z1",
              "author": "Professional-Gas-592",
              "text": "My PC is currently not powerful enough to run locally so qwen was my convenient, happy place to do image edit.  \nNow, they slash daily quota from over 20 to just five without any announcement or communication.  Moderator also being censor happy and produce more failed outcome while still cutting failed attempt from daily quota.   \nUsed to be a perfect place now it is such a disappointment  \nhope they hear feedback and restore back to at least 20+ images daily like they used to.  \nCurrently, you can barely do anything before your daily quota run out",
              "score": 1,
              "created_utc": "2026-02-18 18:32:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o63km0n",
          "author": "Samy_Horny",
          "text": "Yes, and in fact, prepare for massive price reductions. I found the monthly membership page back in November, which suggests those limits are just the beginning.\n\n\nAnd the worst part is that the model seems to be even worse for working with 2D characters.",
          "score": 1,
          "created_utc": "2026-02-18 18:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63nb5m",
          "author": "Phunkapoppa",
          "text": "Agree. Drastically limited since last week ðŸ™„",
          "score": 1,
          "created_utc": "2026-02-18 18:41:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69riwo",
          "author": "C_hantekin",
          "text": "Yes,image gen limit is reduced as well",
          "score": 1,
          "created_utc": "2026-02-19 17:07:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}