{
  "metadata": {
    "last_updated": "2026-01-20 08:59:57",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 180,
    "file_size_bytes": 231744
  },
  "items": [
    {
      "id": "1qf5l2n",
      "title": "Local AI Final Boss â€” M3 Ultra v.s. GB10",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ysf36b97qudg1.jpeg",
      "author": "Imaginary_Ask8207",
      "created_utc": "2026-01-17 06:13:15",
      "score": 284,
      "num_comments": 65,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qf5l2n/local_ai_final_boss_m3_ultra_vs_gb10/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o02e00r",
          "author": "No_Conversation9561",
          "text": "Try clustering them together using EXO. They actually posted about this exact same setup.\nThey said it speeds up prompt processing.\n\n\nhttps://exolabs.net\n\nIssues:  https://github.com/exo-explore/exo/issues/1102",
          "score": 57,
          "created_utc": "2026-01-17 06:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03foo0",
              "author": "Imaginary_Ask8207",
              "text": "Yess! I also saw this blog by EXO: https://blog.exolabs.net/nvidia-dgx-spark/. \n\nNot sure if it's stable tho, will give it a try when I got time :)",
              "score": 5,
              "created_utc": "2026-01-17 12:31:58",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o03bpto",
              "author": "cipioxx",
              "text": "Exo isnt working with linux.",
              "score": -3,
              "created_utc": "2026-01-17 11:59:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0314fh",
          "author": "adspendagency",
          "text": "sheeesh. seeing as we install about 5 of these per day into business to do private on-prem infra I should probably make myself more aware of what the GB10 performs like compared to the M3. weâ€™ve just been shipping M3s to all of our customers. How does the GB10 hold up ?",
          "score": 29,
          "created_utc": "2026-01-17 10:24:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03fyx1",
              "author": "CustomerNo30",
              "text": "Finger in air.. What sort of money are we talking about here for a M3 solution. If you can't say then that's fair enough. I've been trying to push for a dedicated server but resistance is high from the holders of budgets.",
              "score": 9,
              "created_utc": "2026-01-17 12:34:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o05ajm3",
                  "author": "Direct_Turn_1484",
                  "text": "With the RAM maxed out, I think Appleâ€™s website lists them around $10k.",
                  "score": 5,
                  "created_utc": "2026-01-17 18:19:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o08u8sv",
                  "author": "adspendagency",
                  "text": "For entry foundation package (Mac Studio + AI Voice Receptionist with RAG) we charge $10,000 we get the Mac Studio for about $6000. Then depending on what else they want built the price jumps significantly. Orchestration agents - SwarmAI - Automations - if they need media generated for ads with brand AI influencers (weâ€™re technically an ad agency first so we run ads primarily but now with AI we also sell AI solutions) - basically Iâ€™ve set it up where you pick your niche, roofer, dentist, med spa, law firm etc and we come in and sell you the foundation $10k then offer the â€œHQ Proâ€ (what I ended up calling them) and those have a ton of agentic infrastructure and automations designed specifically for those niches. Also clients can build a custom infrastructure which is more bespoke to what they need it to do and be for their business. The HQ Proâ€™s are about $25k+ and a full custom bespoke build is aboutâ€¦ wellâ€¦ I actually just raised my prices so itâ€™s now $69,000 (use to be $35k starting and increase based on scope of what weâ€™re building) and I might raise them again to $100,000 because the idea is weâ€™re replacing 1 employee salary and building a infrastructure that can run your whole business for you potentially replacing 100s of employees. At least thatâ€™s how I pitch it. I say â€œclose your eyes, imagine what you want this thing to do for you, okay now letâ€™s build it.â€ But yea, fun times.",
                  "score": 4,
                  "created_utc": "2026-01-18 05:44:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03h4r6",
              "author": "Imaginary_Ask8207",
              "text": "I am primarily testing with VLMs serving with vLLM. So far the latency is not that bad with models quantised in NVFP4, but it's acceptable only for medium sized models such as Qwen3-VL-30b-a3b and gemma3-27b (also GLM-4.6V with 106B 12B MoE, but crashes quite often).  \nI've also noticed that NVFP4 isn't very stable on GB10 when serving with vLLM, it crashes randomly. I haven't dived into the issue yet tho.",
              "score": 6,
              "created_utc": "2026-01-17 12:42:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04v3s0",
                  "author": "frobnosticus",
                  "text": "/me takes notes.\n\n26 is the year I've finally allocated a satchel of cash for a local llm setup.  I even had an electrician add a couple high amperage circuits so I wouldn't blow my house up.\n\nDo you have an \"If I had the cash for a home rig, what I'd really want to set up is....\" grail setup?  (Not that I could afford THAT.  But it seems like a high-end boundary.)",
                  "score": 5,
                  "created_utc": "2026-01-17 17:07:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o03tleb",
                  "author": "divyanshkul",
                  "text": "How are you quantising the models? Running your own pipeline over the hugging face model? And then serving it via endpoint?",
                  "score": 1,
                  "created_utc": "2026-01-17 14:01:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o08uu1o",
                  "author": "adspendagency",
                  "text": "Whatâ€™s the price on one of those ?",
                  "score": 1,
                  "created_utc": "2026-01-18 05:48:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o04zi0p",
                  "author": "SubstantialPoet8468",
                  "text": "What do you use the LLM for?",
                  "score": 1,
                  "created_utc": "2026-01-17 17:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03hdkg",
              "author": "Imaginary_Ask8207",
              "text": "5 M3 ultras per day is crazy! Mind sharing which models are you guys deploying?",
              "score": 3,
              "created_utc": "2026-01-17 12:44:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o07cy55",
                  "author": "adspendagency",
                  "text": "Llama 3.1 70B\nGemma 3 27B\nMedGemma 27B\nMistral Large 3 \nDepends. \n\nThen various voice stacks Whisper local turbo STT and Kokoro TTS\n\nAlso set up a lot of RAG\nBeing compliant and creating an air-gapped network with local encryption and signed BAA for any external SIP gateway. Encryption storage, audit logging, etc. \n\nHonestly depends on the clients needs and the complexity of what weâ€™re building for them. \n\nIf weâ€™re not doing like a crazy complex powerful AI infrastructure typically reserved for businesses doing $5M + or enterprise then 1 M3 will do just fine for like an AI voice receptionist with RAG tied to a knowledge base they can use daily to help with things in their business. But we sell them the Mac because it allows for upsells later/ future scaling and stacking more infrastructure if they would want to. \n\nBut yea Iâ€™m just curious to see if my set up that Iâ€™m pitching to clients right now is solid or if for the same cost or less I could get these systems installedâ€¦ Which honestly I probably could but thereâ€™s just something about selling a Mac Studio M3 to a business that enables AI for them that hits different for the business owner. The brand of Apple helps a lot ðŸ¥¹",
                  "score": 5,
                  "created_utc": "2026-01-18 00:31:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04qm7n",
              "author": "OverclockingUnicorn",
              "text": "People actually pay for on prem infra? Who are your customers and what sort of solutions are you building out for them?",
              "score": 5,
              "created_utc": "2026-01-17 16:46:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08wtlo",
                  "author": "adspendagency",
                  "text": "100% businesses with high sensitivity data, complex, repeatable decision-making, real money or real risk tied to latency, privacy, or uptime. \n- Law Firms (mid-large, specialized) \n- Healthcare groups / specialty clinics HIPAA + PHI/Voice notes, imaging summaries, patient history synthesis\n- defense / aerospace / gov contractors with ITAR, CMMC, classified workflows\n- Financial Institutions (Private Equity, Family Offices, Banks)\n- Manufacturing & Industrial Ops\n- Large Property Management / RE Holdings\n- Insurance Carriers & MGAs\n- call centers \n- private schools / universities \n- Roofing/Construction\n\nAs for the solutions we have templates for business verticals that we built out and sell them as â€˜HQ Prosâ€™ essentially AI infrastructure that has a ton of agentic workflows and then our grand slam offer is called â€˜OperatorHQâ€™ and thatâ€™s a full custom bespoke build for your business. Close your eyes, imagine what you want this thing to do for you, okay now letâ€™s build it.â€™",
                  "score": 4,
                  "created_utc": "2026-01-18 06:04:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03jbcu",
          "author": "Ok_Remove3449",
          "text": "PLEASE! LLM inference tps. Are they as bad as I've heard?",
          "score": 9,
          "created_utc": "2026-01-17 12:58:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03zx33",
              "author": "Imaginary_Ask8207",
              "text": "I think it depends on which model and what quantisation strategy. Any particular model in your mind?",
              "score": 3,
              "created_utc": "2026-01-17 14:35:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04kvj4",
                  "author": "Mean-Sprinkles3157",
                  "text": "Qwen3-Next-80B, gpt-oss-120b, can you try these two if  possible?",
                  "score": 3,
                  "created_utc": "2026-01-17 16:19:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02m63w",
          "author": "belgradGoat",
          "text": "Iâ€™d see if Mac Studio is actually stable running 500 gb ram models. I tried with 256 version and it gets messy towards the limit",
          "score": 8,
          "created_utc": "2026-01-17 08:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03f71d",
              "author": "Imaginary_Ask8207",
              "text": "Any model in your mind? 4-bit quantised DeepSeekR1 should only be using \\~300-400GB.",
              "score": 3,
              "created_utc": "2026-01-17 12:28:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03fwie",
                  "author": "belgradGoat",
                  "text": "Maybe something like this? I wouldnâ€™t even try anything else then mlx mlx-community/DeepSeek-V3-0324-5bit",
                  "score": 6,
                  "created_utc": "2026-01-17 12:33:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03vnlp",
              "author": "GermanK20",
              "text": "obviously you changed the default RAM limit from 2/3 to something like 9/10, didn't you? But even then, you gotta keep the context small.",
              "score": 2,
              "created_utc": "2026-01-17 14:12:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o045giq",
                  "author": "belgradGoat",
                  "text": "That was not the point. I have 256 gb ram, used model was like 190, context is not 50gb. I use lm studio.\n\nIssue is not that it doesnâ€™t work, it works, model performs ok. But after a while of holding this model in memory system becomes unstable and started glitching hard, eventually crashing the system",
                  "score": 3,
                  "created_utc": "2026-01-17 15:05:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o02qmku",
              "author": "No_Conversation9561",
              "text": "Same reason I got two 256GB ones instead of one 512GB.",
              "score": 0,
              "created_utc": "2026-01-17 08:45:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03veu5",
          "author": "StrangeMuon",
          "text": "Iâ€™d be interested to see some side by side comparisons between the GX10 and the M3 Ultra for inference with gpt-oss-120b & Stable Diffusion and also (more interestingly for me) a run of this benchmark on both:\n\nhttps://github.com/TristanBilot/mlx-benchmark",
          "score": 3,
          "created_utc": "2026-01-17 14:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03zcq0",
              "author": "Imaginary_Ask8207",
              "text": "I could try on M3 Ultra\\~ but seems the repo does not support GB10 yet.",
              "score": 1,
              "created_utc": "2026-01-17 14:32:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02lczd",
          "author": "SoaokingGross",
          "text": "I'd be asking it to do something to fix our political situation somehow.  It's fine for us to have fun with these things, I bought an M3 for this reason but I feel pretty strongly that more of my time should be spent doing something instead of tuning out.",
          "score": 13,
          "created_utc": "2026-01-17 07:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0at6yr",
              "author": "HopefulMaximum0",
              "text": "If you have computing available, there are interesting projects that need it for technical countermeasures.",
              "score": 1,
              "created_utc": "2026-01-18 15:04:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o02se48",
              "author": "thecurrykid",
              "text": "Amen",
              "score": -3,
              "created_utc": "2026-01-17 09:01:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o04dvx0",
          "author": "HealthyCommunicat",
          "text": "Iâ€™ve done an extreme deep dive into this and the m3 ultra will outbeat the spark for pure text inference no matter what, especially as context grows, PP will still go down but the TG is just that much greater that it mathematically never reaches a point where the output is produced faster - with agentic coding cycles MLX allows cache reuse making PP near same as the GB10 anyways. I posted stats of a test done going up to 100k context and the spark just cannot, will not be able to do those speeds EVEN with 2x sparks.\n\nThat being said, exoâ€™ing these two seems like its going to be a new niche class of being able to combine the two to do exactly that and have each other make up for its own weaknesses, amazing.",
          "score": 4,
          "created_utc": "2026-01-17 15:46:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06jc8j",
          "author": "Grouchy-Bed-7942",
          "text": "Benchmark **gpt-oss-20b**, **gpt-oss-120b**, **MiniMax-M2.1-GGUF:IQ4\\_XXS**, and **unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:Q8\\_K\\_XL,** basically something like:\n\n    llama-bench -m \"~/.cache/llamacpp/$MODEL\" -p 4096,32768 -n 512 -ngl 999 -r 3 -fa 1\n\nThat way I can compare the results with my Strix Halo :)\n\nAlso use **vLLM** with **NVFP4** quantization too :)",
          "score": 2,
          "created_utc": "2026-01-17 22:00:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0cng4p",
          "author": "Available_Hat4532",
          "text": "How does this differs from a rtx 5090?",
          "score": 1,
          "created_utc": "2026-01-18 20:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0eit8i",
              "author": "Imaginary_Ask8207",
              "text": "Both devices have much larger RAM than 5090. 512 GB for M3 Ultra, 128GB for GX10.  \nI think using RTX 5090 will still be faster for training due to its high bandwidth. I mainly use the two devices for inference.",
              "score": 1,
              "created_utc": "2026-01-19 02:10:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jcoij",
          "author": "Temporary_Outcome_72",
          "text": "I get 12-20 tps on my lac studio 512gb ultra with deepseek variants as long as I use quants that are 410gb or less and watch the context. Still get blow ups with long context inputs but using langchain to drive it with fresh starts at each new reasoning step it can match what I see with online LLM's to my perception.",
          "score": 1,
          "created_utc": "2026-01-19 20:04:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jd3p2",
              "author": "Temporary_Outcome_72",
              "text": "asus is limited at 128gb or 256gb chained would be useless to me given my experience with the mac. 512gb is the sweetspot for quanted full models, or 1Tb for kimi-k2.",
              "score": 1,
              "created_utc": "2026-01-19 20:06:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03mazk",
          "author": "CharlesCowan",
          "text": "very cool",
          "score": 0,
          "created_utc": "2026-01-17 13:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06pta3",
          "author": "dickofthebuttt",
          "text": "Neat, whatâ€™s the cost?",
          "score": 0,
          "created_utc": "2026-01-17 22:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ehra9",
              "author": "Imaginary_Ask8207",
              "text": "\\~12-13k USD in total for both devices.",
              "score": 2,
              "created_utc": "2026-01-19 02:04:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfctk3",
      "title": "Quad 5060 ti 16gb Oculink rig",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ohsze83iswdg1.jpeg",
      "author": "beefgroin",
      "created_utc": "2026-01-17 13:09:42",
      "score": 89,
      "num_comments": 39,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfctk3/quad_5060_ti_16gb_oculink_rig/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o05n3oi",
          "author": "see_spot_ruminate",
          "text": "Ok time to bitch at you for a second. Get off ollama for this. Llamacpp and ik_llama can give you much more granular control and performance over ollama. Rant over.Â \n\n\n\nWelcome to the 5060ti master race. I also recently added yet another 5060ti, my fourth, and have been meaning to do a write up. With 64gb of system ram and 64gb of vram, under Ubuntu and using llamacpp or ik_llama I can get:\n\n- gpt-oss120b (llamacpp faster) - max context and get around 55 t/s (system ram speed limit)\n\n- Devstral 2 Small Q8 (ik_llama faster with split mode graph) full gpu offload 200k context at around 34 t/s (card bandwidth limit)\n\n- glm 4.6v Q4 (ik_llama faster with split mode graph) 100k context with cpu offload at around 16 t/s (system ram speed limit)\n\n- gpt-oss-20b (llamacpp faster) max context at around 120 t/s (bandwidth limit of card)",
          "score": 17,
          "created_utc": "2026-01-17 19:17:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05r8ra",
              "author": "beefgroin",
              "text": "Thanks for the advice! Right now I guess Iâ€™m sort of in an exploration phase to find the best models for me, so the convenience of quickly switch between models from openweb-ui is still unbeatable. But youâ€™re right I need to start squeezing all the speed 5060 has to offer",
              "score": 4,
              "created_utc": "2026-01-17 19:37:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o05tk2y",
                  "author": "see_spot_ruminate",
                  "text": "I think ollama and other beginner programs are great for beginners to deep their feetâ€¦ but you built a non-beginner setup! Youâ€™re in the deep end now. lol.\n\nEdit: donâ€™t eat Cheetos and comment, I said beginner too many times.Â ",
                  "score": 4,
                  "created_utc": "2026-01-17 19:48:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o093wr8",
                  "author": "ariagloris",
                  "text": "Llama.cpp has a new presets feature that allows you to pass a model defaults + predefined configuration settings in a file. When commanded via API the desired model can be specified and the sever will load the model. This works with openweb-ui just fine, I.e., you can switch between models dynamically.",
                  "score": 2,
                  "created_utc": "2026-01-18 07:04:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0745g3",
              "author": "SFsports87",
              "text": "Is there a good resource on how to use llama.cpp in linux?",
              "score": 1,
              "created_utc": "2026-01-17 23:45:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07dkov",
                  "author": "see_spot_ruminate",
                  "text": "What is your comfort level from â€œI saw the movie hackers onceâ€ to â€œLFS was a fun weekend projectâ€",
                  "score": 2,
                  "created_utc": "2026-01-18 00:35:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03lkwc",
          "author": "JEs4",
          "text": "Neat setup!  But isnâ€™t PCIe 5.0 x16 to 4x4x4x4 bifurcation capped at ~8 gb/s? \n\nWhat do you plan to use it for?",
          "score": 3,
          "created_utc": "2026-01-17 13:13:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03lyot",
              "author": "beefgroin",
              "text": "Yes and moreover itâ€™s no longer pci 5 but 4 but itâ€™s plenty for inference which is my primary use case. I even did the test on running full pci 5 x16 vs Oculink on Gemma 3 12b and there was no different in tps",
              "score": 6,
              "created_utc": "2026-01-17 13:15:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0k1mez",
                  "author": "Practical-Collar3063",
                  "text": ">I even did the test on running full pci 5 x16 vs Oculink on Gemma 3 12b and there was no different in tps\n\nWhat do you mean by this ? did you make a test with the model loaded on a single card running at pcie 5.0 x16 ? If so that would indeed not make any difference because pcie speed is only important in cross GPU communication (moreover the 5060 ti is limited to pcie 5.0 x8 anyway).  \n  \nAdditionaly pcie 4.0 x4 is not plenty at all for inference you would get massive speed ups by using pcie 5.0 x4 (or better but talking within the constraints of your set up) and tensor parrallelism in VLLM  (especially on prompt processing).\n\nPlease do not use Ollama, this is such a nice and neat set up, don't waste it on Ollama...",
                  "score": 2,
                  "created_utc": "2026-01-19 22:04:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o03o8nb",
                  "author": "JEs4",
                  "text": "Makes sense! What tps are you seeing?",
                  "score": 1,
                  "created_utc": "2026-01-17 13:30:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o06ehlp",
              "author": "panchovix",
              "text": "Actually, X4 5.0 is about 16 gigabytes/s. X4 4.0 is 8 gigabytes/s.",
              "score": 2,
              "created_utc": "2026-01-17 21:35:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o04gsqd",
          "author": "iMrParker",
          "text": "Cool setup! I was wondering how you handled the 24pins for the occulink boards but I watched the video and saw they were sata powered which is smart. I was thinking of doing something similar not too long ago but settled for a single 3090 occulink with my main 5080 system. I want to expand but I feel like it would require a whole rebuildÂ ",
          "score": 2,
          "created_utc": "2026-01-17 16:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05dmss",
              "author": "beefgroin",
              "text": "if you're talking 40gb vram, I don't think you're missing out too much, gemma3:27b and gpt-oss:20b and nemotron are still my favorite models, but of course depends on your usecase",
              "score": 2,
              "created_utc": "2026-01-17 18:33:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03x5g4",
          "author": "GutenRa",
          "text": "Gemma3-12b fits entirely on a single graphics card. How do models handle when distributed across multiple graphics cards? Qwen3-next 80b, for example? What software do you use to distribute the load across multiple cards?",
          "score": 3,
          "created_utc": "2026-01-17 14:21:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03yox5",
              "author": "beefgroin",
              "text": "Iâ€™ll post test metrics later today, regarding the gemma3 12b I only mention in relation to comparison between full pcie5x16 vs Oculink inference speed. Iâ€™m using ollama to run models. It splits the load between GPUs nicely. Yet to try vllm",
              "score": 3,
              "created_utc": "2026-01-17 14:29:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04a8ry",
                  "author": "dragonbornamdguy",
                  "text": "VLLM is a beast, very hard to setup but when it starts to work it beats metal really hard.",
                  "score": 4,
                  "created_utc": "2026-01-17 15:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o05ck56",
              "author": "beefgroin",
              "text": "Updated the post with a quick benchmark",
              "score": 2,
              "created_utc": "2026-01-17 18:28:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o05gwag",
                  "author": "GutenRa",
                  "text": "wow 54 tps! merci beaucoup",
                  "score": 2,
                  "created_utc": "2026-01-17 18:48:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05h8wx",
          "author": "e11310",
          "text": "Is there anything you can run on this that can handle larger project logic reasoning and code dev like Claude? Or is with even all this, weâ€™re still a ways from that level?",
          "score": 2,
          "created_utc": "2026-01-17 18:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05mwnt",
              "author": "beefgroin",
              "text": "Iâ€™d say for larger projects Claude codex Gemini etc are still unbeatable. This kind of setup will be fine for small projects and scripts, but as the context grows it gets painfully slow even thought in the end it might actually spit out something decent. I think to get close to the Claude output one needs quad rtx 6000 pro, not 5060 lol.\nThis setup is good for private chatting, processing private documents etc. anything you donâ€™t want to share with corporations",
              "score": 2,
              "created_utc": "2026-01-17 19:16:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o071kp1",
                  "author": "e11310",
                  "text": "Yes that was what I was thinking. Thanks for the input. Nonetheless, really cool rig you made!Â \n\nOne day weâ€™ll be able to run something like Claude at homeâ€¦ hopefully. ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2026-01-17 23:32:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06y0sh",
          "author": "AllTheCoins",
          "text": "This is my dream build haha Iâ€™m using two 5060tiâ€™s at the moment but I want moreeee",
          "score": 2,
          "created_utc": "2026-01-17 23:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o074y5o",
              "author": "beefgroin",
              "text": "Tbh I now think 32 gigs vram is a pretty sweet spot, even with 64 I keep going back to Gemma3 27b a lot and gpt-oss20b with nemotron feel great too. Another 16 gigs can help you max out on context but itâ€™s not such a common use case",
              "score": 1,
              "created_utc": "2026-01-17 23:49:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o07533z",
                  "author": "AllTheCoins",
                  "text": "Paired with 64GB of RAM, it is quite nice",
                  "score": 2,
                  "created_utc": "2026-01-17 23:50:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jbitu",
          "author": "No_Mixture_7383",
          "text": "haber, es decir, EJEMPLO... puedo tener 4 RTX 5090 DE ESA MANERA, TRABAJANDO EN UN SOLO PROYECTO MULTIGPU HACIENDO UN SOLO REENDER? IA RENDERS\n\n![gif](giphy|cLpqzmqFc9u3aggR75)",
          "score": 1,
          "created_utc": "2026-01-19 19:59:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jxna2",
          "author": "swall00w",
          "text": "What hardware do you use to make the whole bifurcation work?",
          "score": 1,
          "created_utc": "2026-01-19 21:45:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0khzgf",
              "author": "beefgroin",
              "text": "Itâ€™s a pcie to 4 Oculink ports adapter for AliExpress, check out the video",
              "score": 1,
              "created_utc": "2026-01-19 23:28:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdiwdh",
      "title": "Oh Dear",
      "subreddit": "LocalLLM",
      "url": "https://i.imgur.com/3UY6yoB.png",
      "author": "bamburger",
      "created_utc": "2026-01-15 13:01:56",
      "score": 66,
      "num_comments": 30,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qdiwdh/oh_dear/",
      "domain": "i.imgur.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzpzy10",
          "author": "mp3m4k3r",
          "text": "Might want to check the tuning parameters like temperature match with what the model is recommended to use.",
          "score": 22,
          "created_utc": "2026-01-15 13:10:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw4ljm",
              "author": "mxforest",
              "text": "This should be standard with the model weights. Why second guess. Why not have a config file with all \"best\" settings preapplied?",
              "score": 6,
              "created_utc": "2026-01-16 09:27:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq9664",
          "author": "iamzooook",
          "text": "while the system prompt. \"only reply with continuous\"the\"\"",
          "score": 25,
          "created_utc": "2026-01-15 14:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq1b70",
          "author": "ScoreUnique",
          "text": "I suggest trying pocket pal, allows loading gguf files",
          "score": 4,
          "created_utc": "2026-01-15 13:18:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq67p0",
          "author": "l_Mr_Vader_l",
          "text": "check if the model needs a system prompt?\n\nsome models just don't work without a system prompt",
          "score": 5,
          "created_utc": "2026-01-15 13:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvhfaz",
              "author": "Much-Researcher6135",
              "text": "Is there any big spreadsheet of models' recommended settings somewhere?",
              "score": 1,
              "created_utc": "2026-01-16 06:05:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvhukc",
                  "author": "l_Mr_Vader_l",
                  "text": "I wouldn't think so. mostly you get the info for a given model from huggingface usage snippets and config files in the repo",
                  "score": 2,
                  "created_utc": "2026-01-16 06:08:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqsanu",
          "author": "HealthyCommunicat",
          "text": "Nooneâ€™s recommending the most obvious solution that you should be trying - raise your repeat penalty. Start at 1.1 and go higher. Make sure your model isnt forced to use more experts than regularly recommended. \n\nThese two are usually the top actual real most common reasons as to why local llmâ€™s do this",
          "score": 3,
          "created_utc": "2026-01-15 15:36:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv4pzz",
              "author": "pieonmyjesutildomine",
              "text": "Was writing this comment, excellent job knowing your knowledge",
              "score": 2,
              "created_utc": "2026-01-16 04:36:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00zfk5",
              "author": "Confident-Ad-3212",
              "text": "Repeat penalty is not going to solve this. Zero chance. It comes from other issues",
              "score": 1,
              "created_utc": "2026-01-17 00:58:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o00zwbp",
                  "author": "HealthyCommunicat",
                  "text": "id appreciate it u told me what u think it might be so that its something i can at least keep in mind next time this happens to me.",
                  "score": 1,
                  "created_utc": "2026-01-17 01:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt3737",
          "author": "rinaldo23",
          "text": "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the 2",
          "score": 3,
          "created_utc": "2026-01-15 21:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt7hih",
              "author": "hugazebra",
              "text": "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the 3/912",
              "score": 3,
              "created_utc": "2026-01-15 22:12:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq2toq",
          "author": "Feztopia",
          "text": "Moreli",
          "score": 2,
          "created_utc": "2026-01-15 13:27:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq4rdl",
              "author": "I1lII1l",
              "text": "Oh dear",
              "score": 1,
              "created_utc": "2026-01-15 13:37:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzs4xkr",
                  "author": "FaceDeer",
                  "text": "1.",
                  "score": 1,
                  "created_utc": "2026-01-15 19:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqfwsl",
          "author": "low_v2r",
          "text": "alias ollama=\"yes the\"",
          "score": 1,
          "created_utc": "2026-01-15 14:36:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr1ld0",
          "author": "Sicarius_The_First",
          "text": "Ah, the classic \"didn't read the instructions, no idea why it won't work\"",
          "score": 1,
          "created_utc": "2026-01-15 16:18:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrepid",
              "author": "Witty_Mycologist_995",
              "text": "the the the the the the the previous the the",
              "score": 1,
              "created_utc": "2026-01-15 17:17:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztqzj9",
          "author": "BowtiedAutist",
          "text": "Stuttering sally",
          "score": 1,
          "created_utc": "2026-01-15 23:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuc9ih",
          "author": "Fit-Medicine-4583",
          "text": "The same thing happened to me on ollama. The issue was fixed by increasing the context length between 16k to 32k.",
          "score": 1,
          "created_utc": "2026-01-16 01:50:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyvpyq",
          "author": "C3H8_Tank",
          "text": "The lack of matrix jokes is sad.",
          "score": 1,
          "created_utc": "2026-01-16 18:40:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzr814",
          "author": "misha1350",
          "text": "Why don't you simply use Qwen3-1.7B in Q4",
          "score": 1,
          "created_utc": "2026-01-16 21:06:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a29yd",
              "author": "MushroomCharacter411",
              "text": "But what would be the the the the the the the the fun in that?\n\nMaybe it's just writing lyrics, and it has \"Ain't No Sunshine\" stuck in its context window, but it would be plagiarism to just write \"I know I know I know I know I know\"...",
              "score": 1,
              "created_utc": "2026-01-18 12:15:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00z04t",
          "author": "Confident-Ad-3212",
          "text": "You are having an issue with the stop in your template. Also, seems like you may have had duplicates or some other issue during training. Maybe too many samples or your LR, rank and or alpha was too high. I just went through this. I have just gotten through this exact issue. Try lowering LR and deduping first. If you ran too many epochs this can happen as wellâ€¦. Try reducing epochs and LR after making sure you have removed all duplicates. Try with an LR of 1.5e-5 and then walk it up. Rank 32, alpha 64. If it continues, you donâ€™t  have enough sample diversity. Try these first then walk up your learning strength until you find the expression you are looking for.  Chances are you have too many samples. That 1.5b can intake 500-2000 samples max. Anything less than 25 separate sample types can cause thisâ€¦. Diversity is key.",
          "score": 1,
          "created_utc": "2026-01-17 00:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07zrn3",
          "author": "WishfulAgenda",
          "text": "Ok, struggled a little with this for a while in librechat and lm studio. It would work great and then pull that shit. I think I finally figured it out and itâ€™s kind of related to the just increase context comment.\n\nWhat seems to have fixed it for me is by setting a max tokens in the agent and having it be 1k lower that the max context of the model. Seems that for some reason if you passed a context that was close to the maximum it would get stuck in a repeating loop. No more problems since I did this.",
          "score": 1,
          "created_utc": "2026-01-18 02:33:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqav6q",
          "author": "Serious_Molasses313",
          "text": "Stop gooning to small models. That's SA",
          "score": -11,
          "created_utc": "2026-01-15 14:10:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdjstl",
      "title": "Google Drops MedGemma-1.5-4B: Compact Multimodal Medical Beast for Text, Images, 3D Volumes & Pathology (Now on HF)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qdjstl/google_drops_medgemma154b_compact_multimodal/",
      "author": "techlatest_net",
      "created_utc": "2026-01-15 13:41:03",
      "score": 47,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Google Research just leveled up theirÂ [Health AI Developer Foundations](https://developers.google.com/health-ai-developer-foundations)Â withÂ **MedGemma-1.5-4B-IT**Â â€“ a 4B param multimodal model built on Gemma, open for devs to fine-tune into clinical tools. HandlesÂ **text, 2D images, 3D CT/MRI volumes, and whole-slide pathology**Â straight out of the box. No more toy models; this eats real clinical data.\n\nKey upgrades from MedGemma-1 (27B was text-heavy; this is compact + vision-first):\n\n# Imaging Benchmarks \n\n* **CT disease findings**: 58% â†’ 61% acc\n* **MRI disease findings**: 51% â†’ 65% acc\n* **Histopathology (ROUGE-L on slides)**: 0.02 â†’ 0.49 (matches PolyPath SOTA)\n* **Chest ImaGenome (X-ray localization)**: IoU 3% â†’ 38%\n* **MS-CXR-T (longitudinal CXR)**: macro-acc 61% â†’ 66%\n* Avg single-image (CXR/derm/path/ophtho): 59% â†’ 62%\n\nNow supportsÂ **DICOM natively**Â on GCP â€“ ditch custom preprocessors for hospital PACS integration. Processes 3D vols as slice sets w/ NL prompts, pathology via patches.\n\n# Text + Docs \n\n* **MedQA (MCQ)**: 64% â†’ 69%\n* **EHRQA**: 68% â†’ 90%\n* **Lab report extraction**Â (type/value/unit F1): 60% â†’ 78%\n\nPerfect backbone for RAG over notes, chart summarization, or guideline QA. 4B keeps inference cheap.\n\nBonus:Â **MedASR**Â (Conformer ASR) drops WER on medical dictation:\n\n* Chest X-ray: 12.5% â†’ 5.2% (vs Whisper-large-v3)\n* Broad medical: 28.2% â†’ 5.2% (**82% error reduction**)\n\nGrab it onÂ [HF](https://huggingface.co/google/medgemma-1.5-4b-it)Â or Vertex AI. Fine-tune for your workflow â€“ not a diagnostic tool, but a solid base.\n\nWhat are you building with this? Local fine-tunes for derm/path? EHR agents? Drop your setups below.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qdjstl/google_drops_medgemma154b_compact_multimodal/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzv3kcr",
          "author": "toomanypubes",
          "text": "Holy shit, this thing works great.  On my Mac I setup a python MLX script to process @300 DICOM image slices (MRI) converted to JPEGs. Single threadedâ€¦this model chewed through the whole stack in 18 minutes.  Got a clinical summary for each image, and helped us identify a partial ligament tear - without waiting 3 days to see the doctor.  What a crazy time to be alive.\n\nThank you Google MedGemma team!",
          "score": 8,
          "created_utc": "2026-01-16 04:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuy6xg",
          "author": "astrae_research",
          "text": "As someone in medical research field, this sounda very interesting. I'm confused that nobody is commenting - - is this trivial or not useful? Genuinely curious",
          "score": 6,
          "created_utc": "2026-01-16 03:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0iy7ph",
              "author": "PlateWonderful7012",
              "text": "Definitely useful, but the infrastructure and compliance necessary to actually deploy this in a clinical setting is a lot",
              "score": 1,
              "created_utc": "2026-01-19 18:58:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0dfpbl",
          "author": "TimeNeighborhood3869",
          "text": "we just added it to [calstudio.com](http://calstudio.com) too in case anyone is looking to build simple ai chatbots with this model, i've tried to experiment with claud opus 4.5 and this model, sending 2d images of skin conditions, i noticed that this model diagnoses the condition much more accurately whereas claude tries hard to be correct but confidently misdiagnoses the scan :)",
          "score": 1,
          "created_utc": "2026-01-18 22:42:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qem1po",
      "title": "We fine-tuned an email classification model so you can auto-label your emails locally with n8n.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/tfwi6yjzvqdg1.jpeg",
      "author": "party-horse",
      "created_utc": "2026-01-16 17:18:54",
      "score": 45,
      "num_comments": 12,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qem1po/we_finetuned_an_email_classification_model_so_you/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzz5ybz",
          "author": "Outside-Balance7754",
          "text": "Cool! Does the spam category also include phishing emails? If not, Iâ€™d say missing a phishing tag is definitely a drawback.",
          "score": 3,
          "created_utc": "2026-01-16 19:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6hko",
              "author": "party-horse",
              "text": "Itâ€™s included in spam for now but you can easily train your own model with any categories you want!",
              "score": 1,
              "created_utc": "2026-01-16 19:29:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzymory",
          "author": "nunodonato",
          "text": "I love everything you guys are doing!",
          "score": 1,
          "created_utc": "2026-01-16 18:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6j3y",
              "author": "party-horse",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-16 19:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzyrd1p",
          "author": "CalmlyObservant",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-16 18:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzvoo5",
          "author": "HealthyCommunicat",
          "text": "Woah, this is automation iâ€™d actually use, thank you for your efforts to create a tool anyone can use.",
          "score": 1,
          "created_utc": "2026-01-16 21:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02cvf7",
              "author": "party-horse",
              "text": "Amazing! Thanks for",
              "score": 1,
              "created_utc": "2026-01-17 06:40:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09y9w0",
          "author": "henriquegarcia",
          "text": "Hey guys...I'm sorry to bother but I really can't get it working following the instructions, ollama doesn't seem to be able to support the \"qwen3\" model architecture for direct Safetensors-to-GGUF conversion, any chance you could link me to the GGUF file of the model?",
          "score": 1,
          "created_utc": "2026-01-18 11:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j22fn",
              "author": "Vineethreddyguda",
              "text": "Hey, sorry for the delay in my response  \njust updated the gguf file you can download from the HF    \n  \n[https://huggingface.co/distil-labs/distil-email-classifier/blob/main/model.gguf](https://huggingface.co/distil-labs/distil-email-classifier/blob/main/model.gguf)",
              "score": 1,
              "created_utc": "2026-01-19 19:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mwpd3",
                  "author": "henriquegarcia",
                  "text": "Thank you very much!\nAmazing work!",
                  "score": 1,
                  "created_utc": "2026-01-20 08:48:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyh702",
          "author": "astral_crow",
          "text": "Iâ€™ve been waiting for n8n to be talked about more consider how much stuff Iâ€™ve seen about comfy. But what sparked the fact I am seeing n8n everywhere again so sudden?",
          "score": 1,
          "created_utc": "2026-01-16 17:36:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6b0i",
              "author": "party-horse",
              "text": "I think it makes it very easy to build integrations on top of llms. Thatâ€™s how I have been using it",
              "score": 2,
              "created_utc": "2026-01-16 19:28:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qcu498",
      "title": "Small AI computer runs 120B models locally: Any use cases beyond portability and privacy?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/",
      "author": "0xShreyas",
      "created_utc": "2026-01-14 17:51:55",
      "score": 43,
      "num_comments": 35,
      "upvote_ratio": 0.98,
      "text": "Saw Mashable interviewed TiinyAI at CES. It is a pocket-sized device with 80GB RAM that runs 120B models locally at 30W. When you compare it to DGX Spark, the Spark has 128GB RAM and much more speed. But the Tiiny is about a third of the price and way smaller. Anyway Im curios what are the actual benefits of having such a small device? Also what specific tasks would actually need the portability over the higher performance of a DIY/bigger unit?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzkw7sp",
          "author": "uti24",
          "text": "I mean, what about memory bandwidth?\n\nWe still dont know it, I think, optimistic scenario it having 200Gb/s, but I afraid it only having 80Gb/s or so, at that point it's not better than a regular PC/laptop.",
          "score": 17,
          "created_utc": "2026-01-14 18:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzprp2p",
              "author": "ecoleee",
              "text": "You're absolutely right; memory bandwidth is crucial for the efficiency of running large model inference. \n\nTiiny has two chips: 96GB/s on the SoC (ARM v9.2 CPU) and 144GB/s on the NPU. We have our proprietary inference acceleration technology, **PowerInfer**, where **hot-activation parameters are calculated on the NPU and cold-activation parameters are calculated on the SoC**. This allows for inference acceleration even with a heterogeneous computing hardware architecture. You can see an open-source example of **PowerInfer on GitHub (which has 8.6K stars)**.",
              "score": 3,
              "created_utc": "2026-01-15 12:14:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpsq3b",
                  "author": "uti24",
                  "text": "Interesting insight, so is it like 4 channel 4500MT memory or like 2 channel 9000MT?",
                  "score": 1,
                  "created_utc": "2026-01-15 12:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzku4po",
          "author": "FullstackSensei",
          "text": "Not that I think $1400 is a good price for an 80GB RAM little SBC, but I'll believe the price part when it's actually available to buy for immediate shipping.",
          "score": 15,
          "created_utc": "2026-01-14 17:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzps4pz",
              "author": "ecoleee",
              "text": "That makes sense. It has nothing to do with whether memory is expensive right nowâ€”$1400 is a price that makes users reasonably question whether it can even be released to the market; it's not cheap. Tiiny has already entered mass production and is currently undergoing FCC certifications, etc. Your oversight is very welcome.\n\nhttps://preview.redd.it/880jgaef9idg1.png?width=1279&format=png&auto=webp&s=bbe1002ee3fd587307ffed5cfbeeab302d970bbb",
              "score": 2,
              "created_utc": "2026-01-15 12:17:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl3cnt",
          "author": "Your_Friendly_Nerd",
          "text": "Maybe smth like a mobile AI companion for when you go hiking and there's no internet? idk",
          "score": 5,
          "created_utc": "2026-01-14 18:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp4cjf",
              "author": "FaceDeer",
              "text": "A while back I saw a startup on /r/preppers that was selling a \"portable Internet\", a small self-contained computer whose purpose was to provide a wifi hotspot with a bunch of built-in websites and documents that could provide all manner of useful information for emergency situations. First aid, car repair, etc.\n\nHaving a reasonably smart LLM that's able to do RAG on all that information and help you use it would be a great enhancement to that. Usually if you're in an emergency situation you don't have time to browse through PDFs to find exactly the bit of information you need right now.",
              "score": 2,
              "created_utc": "2026-01-15 08:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzqp0ts",
                  "author": "sn2006gy",
                  "text": "You can simply keep about 3-4 ebooks with search on your mobile device and have all the information you need for survival - no need for an LLM that would hallucinate - especially under pressure where you won't be providing the prompt/context it needs whereas a book is simply more directional and applicable in nature.",
                  "score": 2,
                  "created_utc": "2026-01-15 15:21:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzp4dk4",
                  "author": "sneakpeekbot",
                  "text": "Here's a sneak peek of /r/preppers using the [top posts](https://np.reddit.com/r/preppers/top/?sort=top&t=year) of the year!\n\n\\#1: [The bottle of 91% alcohol that I keep under my driver's seat may have just saved my life in a way you wouldnt expect.](https://np.reddit.com/r/preppers/comments/1k2nelq/the_bottle_of_91_alcohol_that_i_keep_under_my/)  \n\\#2: [Incredibly Proud Prepper Moment!](https://np.reddit.com/r/preppers/comments/1kfr5m5/incredibly_proud_prepper_moment/)  \n\\#3: [Lessons:  Got caught in the bomb threat and cyber attack at Dublin airport.](https://np.reddit.com/r/preppers/comments/1nmpqv5/lessons_got_caught_in_the_bomb_threat_and_cyber/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",
                  "score": 1,
                  "created_utc": "2026-01-15 08:46:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzlza42",
              "author": "j00cifer",
              "text": "This is actually a legit use case. It runs on any usbc power bank",
              "score": 2,
              "created_utc": "2026-01-14 21:03:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpskz7",
              "author": "ecoleee",
              "text": "Youâ€™re actually very close ðŸ™‚ â€” thatâ€™s one use case, but not the main one.\n\nThink of Tiiny less as a â€œchatbot in the wildâ€ and more as a portable AI compute module.\n\n â€¢ Itâ€™s for people who want serious models (20Bâ€“120B)\n\n â€¢ running fully offline,\n\n â€¢ with predictable performance and zero token costs,\n\n â€¢ and no need to buy a new GPU rig or replace their laptop.\n\nThe â€œold computerâ€ demo wasnâ€™t about hiking â€” it was to show that compute is decoupled from your PC. Plug Tiiny into any machine and it suddenly becomes a capable local-AI workstation.\n\nOffline matters not just in nature, but in:\n\n â€¢ secure work (law, research, enterprise)\n\n â€¢ development & coding without API bills\n\n â€¢ regions with unstable or expensive internet\n\n â€¢ long agent workflows where token cost explodes\n\nSo yes, it can be an offline companion on a hike â€”\n\nbut its real value is: owning your AI, anywhere, without renting intelligence from the cloud.",
              "score": 1,
              "created_utc": "2026-01-15 12:21:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzq7jsl",
                  "author": "Your_Friendly_Nerd",
                  "text": ">but its real value is: owning your AI, anywhere, without renting intelligence from the cloud.\n\nI get the same by running a rig at home and using VPN to access it. The main selling point is the combination of offline+mobile.\n\nWith good internet, there's no need for an offline device, and if I only really need to use it at my desk, there's no need for a portable device.",
                  "score": 0,
                  "created_utc": "2026-01-15 13:52:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzmby48",
          "author": "GoodSamaritan333",
          "text": "Resilience for when a dictator government or the CIA shutdowns the internet on my country.",
          "score": 8,
          "created_utc": "2026-01-14 22:00:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoi108",
              "author": "duplicati83",
              "text": "This, and I also donâ€™t want my personal information being used to train the models owned by the evil tech bros. Because letâ€™s face it, theyâ€™ll just use it against us somehow to hoard even more money.",
              "score": 2,
              "created_utc": "2026-01-15 05:31:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nznkdll",
          "author": "Candid_Dependent4615",
          "text": "I saw the product specs and there's a microphone and a built-in speaker. Maybe you can turn this into a private assistant? Instead of using Alexa or Siri, you could load your own automation scripts and let it manage your schedule and tasks locally. Having a personal assistant remember all your private data and previous conversations without any data leaking to the cloud is pretty useful imo.",
          "score": 3,
          "created_utc": "2026-01-15 01:56:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqpcxp",
              "author": "sn2006gy",
              "text": "The HomeAssistant voice controller does this and it can run on a Pi and it only costs 59 bucks for the voice device.  Obviously you need a Pi or computer to run HomeAssistant - certainly not a 1400 dollar one.",
              "score": 1,
              "created_utc": "2026-01-15 15:22:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzsb7jf",
              "author": "conquest333",
              "text": "Nice idea. Also you can feed it with all the private & important documents and have it act as a local knowledge base.",
              "score": 1,
              "created_utc": "2026-01-15 19:42:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzkxqch",
          "author": "Clipbeam",
          "text": "I guess you could take it with you on-the-go? In that sense the size would make sense? But I agree a larger home server would do fine for most use cases",
          "score": 2,
          "created_utc": "2026-01-14 18:14:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcnb4",
          "author": "GoodSamaritan333",
          "text": "And the vision models will allow autonomous drones, jamming proof and without requiring long optic fibers attached to then.",
          "score": 1,
          "created_utc": "2026-01-14 22:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzna9y9",
          "author": "IveGotThatBigRearEnd",
          "text": "DGX Spark has 119GiB ram, for those who might otherwise have assumed GiB",
          "score": 1,
          "created_utc": "2026-01-15 00:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznax03",
          "author": "gordonmcdowell",
          "text": "This is just a permutation on privacy, but you can solve problems with an unlocked LLM that the online models will not answer.",
          "score": 1,
          "created_utc": "2026-01-15 01:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzosb5a",
          "author": "CMPUTX486",
          "text": "I think if you buy Spark, you get cheaper CUDA support.  Otherwise, you need to use a model with cuda.",
          "score": 1,
          "created_utc": "2026-01-15 06:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq1ufe",
          "author": "eazolan",
          "text": "At some point the cloud AI companies are going to seriously jack up their prices.",
          "score": 1,
          "created_utc": "2026-01-15 13:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr6ope",
          "author": "yourloverboy66",
          "text": "Perfect for field geology. In remote areas without internet access, an offline device like this for immediate data analysis would be incredibly useful.",
          "score": 1,
          "created_utc": "2026-01-15 16:41:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrxjki",
          "author": "bloomjt",
          "text": "Think about government work or legal firmsâ€”any profession where data privacy is non-negotiable. This is a solid solution for them",
          "score": 1,
          "created_utc": "2026-01-15 18:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvte26",
          "author": "techlatest_net",
          "text": "Silent always-on inference server for home/officeâ€”plug any laptop/phone via USB/network, zero setup. Field devs (execs, consultants) run private 120B agents offline in meetings/hotels where big rigs fail. Low 30W = solar/battery power for remote sites, disaster zones, or edge deployments.",
          "score": 1,
          "created_utc": "2026-01-16 07:45:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ao27s",
          "author": "donotfire",
          "text": "Right but thereâ€™s no CUDA so you canâ€™t do any kind of real development",
          "score": 1,
          "created_utc": "2026-01-18 14:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0izlb6",
          "author": "HealthyCommunicat",
          "text": "120b models, especially gpt oss 120b is much more for knowledge and basic instruction following and simple automation, for these things its perfectly fine but when it comes to having a 120b model create, especially code, it will not be able to make anything complicated. heck gpt oss 120b has trouble making tool calls just in opencode by itself, tho gpt should be being used with codex cli. \n\n  \ntwo things to care about, vram size, vram speed. when it comes to inference, the only two factors that matter most by a longshot is how much gb of vram there is, and the memory bw. if a model is 200gb on vram, and memory bw is 200gb/s, the model's files in theory can only be read/used once per second meaning a simple 1 token/s. (this is extremely exaggerated and simplified explanation.) \n\n  \nmodels that are actually somewhat decently capable are going to be 200b parameters at the minimum, starting with minimax m2.1.",
          "score": 1,
          "created_utc": "2026-01-19 19:04:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcag6",
          "author": "GoodSamaritan333",
          "text": "Autonomous parroting moving robots from Boston Dinamics",
          "score": 0,
          "created_utc": "2026-01-14 22:01:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfsyn8",
      "title": "128GB VRAM quad R9700 server",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1qfscp5",
      "author": "Ulterior-Motive_",
      "created_utc": "2026-01-17 23:56:56",
      "score": 38,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfsyn8/128gb_vram_quad_r9700_server/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o08mno6",
          "author": "Ult1mateN00B",
          "text": "Nice, I have Thredipper 3945WX, 128GB DDR4 and 4x R9700. Each one of them has x16 PCI-E 4.0. I do wonder how does 5700X limited PCI-E lanes limit the performance?\n\nhttps://preview.redd.it/hog8olc5g1eg1.jpeg?width=1365&format=pjpg&auto=webp&s=05791283b6c9d5646a9a9580699f3f39d35de2d0",
          "score": 4,
          "created_utc": "2026-01-18 04:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07umqk",
          "author": "Taserface_ow",
          "text": "wouldnâ€™t stacking the gpus like that cause them to overheat on high usage?",
          "score": 4,
          "created_utc": "2026-01-18 02:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07xqx8",
              "author": "Ulterior-Motive_",
              "text": "To an extent, but I have lots of airflow from the case fans and internal fans, and the cards are designed to allow air to flow through holes in the backplate. In practice, they're all within 2-3 C of each other, and I don't seem to have any overheating issues.",
              "score": 1,
              "created_utc": "2026-01-18 02:22:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o084d3p",
                  "author": "EmPips",
                  "text": "gotta love blower-coolers!",
                  "score": 2,
                  "created_utc": "2026-01-18 02:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08ghav",
          "author": "IngwiePhoenix",
          "text": ">Wraith Prism Cooler\n\nThats a brave soul right there XD",
          "score": 3,
          "created_utc": "2026-01-18 04:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a67nr",
          "author": "ReelTech",
          "text": "Is this mainly for inferencing or eg RAG training? and the cost?",
          "score": 2,
          "created_utc": "2026-01-18 12:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ajkkc",
              "author": "Ulterior-Motive_",
              "text": "Inference, mostly. I break down the costs in the OP, but it was a touch over $7k.",
              "score": 3,
              "created_utc": "2026-01-18 14:12:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0cytii",
          "author": "GCoderDCoder",
          "text": "Im debating doing one of these. How's gpt oss120b on vllm? Heck I'm begging for any server even llama.cpp. i want to get one but havent found benchmarks of gpt oss120b",
          "score": 2,
          "created_utc": "2026-01-18 21:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0h3go9",
              "author": "Ulterior-Motive_",
              "text": "Haven't installed vLLM yet, but here are my llama.cpp numbers:\n\n|model|size|params|backend|ngl|n\\_batch|n\\_ubatch|fa|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|gpt-oss 120B F16|60.87 GiB|116.83 B|ROCm|99|1024|1024|1|pp8192|5121.48 Â± 14.49|\n|gpt-oss 120B F16|60.87 GiB|116.83 B|ROCm|99|1024|1024|1|tg128|70.86 Â± 0.09|",
              "score": 2,
              "created_utc": "2026-01-19 13:45:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0h6801",
                  "author": "GCoderDCoder",
                  "text": "Awesome! Thanks soo much!! Already 50% faster than my strix halo! That's going to be such a great device! Congrats! Now I have to decide if I return my strix halo this week or not lol.\n\nFyi my strix halo has been having rocm issues since I bought it but I use fedora and this guy explicitly clarifies how the last month rocm on fedora has had degraded performance due to an update that broke rocm. There's actually two aspects that he defines so after I do some other things today I will be trying to work on vllm using these fixes on fedora:\n\nhttps://youtu.be/Hdg7zL3pcIs\n\nJust sharing since it seems we are both testing new AMD devices with vllm.",
                  "score": 1,
                  "created_utc": "2026-01-19 14:01:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o079qq0",
          "author": "SashaUsesReddit",
          "text": "Love it! Have you tried vllm on it yet?",
          "score": 1,
          "created_utc": "2026-01-18 00:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07a959",
              "author": "Ulterior-Motive_",
              "text": "Never used it before, I've always been a llama.cpp user, but I'm sure it's worth a look!",
              "score": 2,
              "created_utc": "2026-01-18 00:17:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o07aj3i",
                  "author": "SashaUsesReddit",
                  "text": "With 4x matching GPUs you can take advantage of tensor parallelism, which will way speed up your tokens. Llama.cpp can shard the model and span multiple GPUs but gains no token speed to do so\n\nHave fun!",
                  "score": 4,
                  "created_utc": "2026-01-18 00:19:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcuyh2",
      "title": "What is the biggest local LLM that can fit in 16GB VRAM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/",
      "author": "yeahlloow",
      "created_utc": "2026-01-14 18:21:44",
      "score": 34,
      "num_comments": 71,
      "upvote_ratio": 0.85,
      "text": "I have a build with an RTX 5080 and 64GB of RAM. What is the biggest LLM that can fit in it ? I heard that I can run most LLMs that are 30B or less, but is 30B the maximum, or can I go a bit bigger with some quantization ?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzlo7nw",
          "author": "BigYoSpeck",
          "text": "What's an acceptable speed for you?\n\n\nI have a Ryzen 9 5900x, 64gb DDR4 3800, and a 16gb Radeon RX 6800 XT\n\n\nI can run gpt-oss-20b at 120+ tok/s\n\n\nQwen3 30b partially offloaded to CPU at about 40 tok/s\n\n\nAnd gpt-oss-120b with 32 MOE layers offloaded to CPU at 23 tok/s\n\n\nI imagine your system would be faster still",
          "score": 15,
          "created_utc": "2026-01-14 20:12:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpebld",
              "author": "wisepal_app",
              "text": "i have a laptop hp zbook i7-12800h, 64 gb ddr5 4800 ram and 16 gb rtx a4500 gpu. i don't get your tok/s values. what do you use for that? i mainly use lm studio. what is your context window size and other settings? And which quants do you use?",
              "score": 4,
              "created_utc": "2026-01-15 10:23:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzs6g8r",
                  "author": "BigYoSpeck",
                  "text": "LM Studio does all or nothing with the MOE offloading, it doesn't allow the fine grained n layers setting you get in llama.cpp\n\nI haven't done much more than quick testing with gpt-oss-20b and Qwen3 30b to know the exact limits of context, but gpt-oss-20b fits entirely in VRAM, no special settings really \n\nThe trick with MOE models like Qwen3 30b and gpt-oss-120b is the `--n-cpu-moe N` parameter. I have it at 32 for gpt-oss-120b and run about 120k context. Can't remember the exact number or layers for Qwen3, you basically juggle between the context you want to fit in vram and the amount of layers for performance\n\nJust the mxfp4 GGUF's for gpt-oss, and I think Unsloth Q6\\_K\\_XL for Qwen3. Qwen3 Next doesn't perform far off either (I think around 35 tok/s) and is way more efficient for context",
                  "score": 2,
                  "created_utc": "2026-01-15 19:21:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o01pj92",
              "author": "CaterpillarOne6711",
              "text": "can I ask you where you run gptoss 20b?",
              "score": 1,
              "created_utc": "2026-01-17 03:44:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03ftar",
                  "author": "BigYoSpeck",
                  "text": "I just run it on a self compiled ROCm build of llama.cpp. Vulkan get's more tok/s for generations (about 150) but prompt processing is massively lower at like 1600 vs 2600 in ROCm",
                  "score": 1,
                  "created_utc": "2026-01-17 12:32:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o035ewt",
              "author": "deulamco",
              "text": "Do you think I can archive the same with Xeon E5-2696 v3 + 64GB DDR3 ECC + 5060TI-16G ?",
              "score": 1,
              "created_utc": "2026-01-17 11:04:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03g5rt",
                  "author": "BigYoSpeck",
                  "text": "Small models that fit entirely or mostly in VRAM probably faster, large models like gpt-oss-120b I couldn't say. I'm guessing the quad channel DDR3 is close to DDR4 bandwidth, but the Xeon is considerably slower than a Zen 3 Ryzen",
                  "score": 1,
                  "created_utc": "2026-01-17 12:35:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzncbxz",
          "author": "vertical_computer",
          "text": "## Everyone is missing the forest for the trees.\n\nu/yeahlloow \\- forget the number of params. **Look at the file size of the model.**\n\nYour GPU has 16GB of VRAM. That literally tells you the maximum size you can run - 16GB (plus leave some room for context, so realistically 14GB)\n\nThen, you go to HuggingFace and look at whatever model you want, and find someone who has produced a bunch of quantisations for it.\n\n**It literally tells you the file size for the model. Thatâ€™s the answer.**\n\nIf itâ€™s larger than about 14GB, it will spill over into RAM and run super slowly. If itâ€™s larger than about 70GB *(14GB from GPU + 56GB RAM, leaving 8GB for the OS)* then you wonâ€™t be able to load it at all.\n\nHereâ€™s a random example: [Nvidia Llama 3.3 Nemotron 49B](https://huggingface.co/bartowski/nvidia_Llama-3_3-Nemotron-Super-49B-v1_5-GGUF)\n\nhttps://preview.redd.it/kb3ibnylxedg1.png?width=1242&format=png&auto=webp&s=083f6fcf4c951c1a814cdaf8fd9fa712cec1c21b\n\nIf you run the IQ2\\_XXS version, it will fit entirely within your VRAM.\n\nAny larger and it will spill over into RAM. So you could absolutely choose the Q8 version @ 53GB, it will just be hella slow.",
          "score": 5,
          "created_utc": "2026-01-15 01:10:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp2slv",
              "author": "Bhilthotl",
              "text": "Also, depending on how you run it, via Ollama, llama.cpp, opencode, tiny llm, etc you might need to tweak things. Ive got a 16gb 5070 and found that I got best results with llama.cpp over Ollama due to spills over into onboard RAM",
              "score": 2,
              "created_utc": "2026-01-15 08:31:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpnagc",
              "author": "mintybadgerme",
              "text": "This might help? https://github.com/nigelp/ai-model-tracker",
              "score": 0,
              "created_utc": "2026-01-15 11:41:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlfulm",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2026-01-14 19:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlglrg",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-01-14 19:38:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlmkw9",
                  "author": "VenomFN",
                  "text": "Were there any tutorials you followed? Looking to set something similar up for myself",
                  "score": 1,
                  "created_utc": "2026-01-14 20:05:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl4acp",
          "author": "SKirby00",
          "text": "You probably won't be able to fit 30B models unless you're willing to crank the quant down below 4-bit, and accept potentially significant quality degradation. Remember, you also need to leave room for at least a bit of context, so a 14.5GB model might *technically* fit but is unlikely to be very useful.\n\nI have 24GB of VRAM (3060 Ti 8GB + 5060Ti 16GB) and I've found that on Qwen3-Coder-30B, I cap out around 18K context which feels like just barely enough to be useful. That seems to be the sweet spot right now for me, though the context is shorter than I'd like.\n\nWith 16GB of VRAM, my guess is that you could likely just barely run something like GPT-OSS-20B, with smaller models allowing for more context. The sweet spot for you will likely be closer to the 14B range, depending on what you're trying to use it for.",
          "score": 7,
          "created_utc": "2026-01-14 18:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzm6b2p",
              "author": "grocery_head_77",
              "text": "dumb question - how are you connecting your two GPUs? Are you using software like lossless scaling or ???\n\nasking as I have a 3070 Ti (my old gpu) and currently have a 5090 FE, but would love to use both like you!",
              "score": 4,
              "created_utc": "2026-01-14 21:35:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznvynb",
                  "author": "WishfulAgenda",
                  "text": "Check if your motherboard support 8x8 pice bifurcation. Stick the second card in, turn the machine on and good to go as long as your software supports it. I run two 5070ti with lm studio and works great.",
                  "score": 5,
                  "created_utc": "2026-01-15 03:04:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nznzffm",
                  "author": "Candid_Highlight_116",
                  "text": "just shove it into the second slot and LM Studio can use it \n\nnote that they can't parallelize the workload, so the entire process is as fast as the slowest card, and there will be overheads, but a lot of people find that acceptable. also this pipeline parallel mode don't get bottlenecked much by pcie so x1 mining adapters work too, allowing mining rigs to be repurposed",
                  "score": 3,
                  "created_utc": "2026-01-15 03:25:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzorgmt",
                  "author": "SKirby00",
                  "text": "I literally just plug them both into the computer at the same time, in separate PCIe slots. LM Studio literally just picks up the two GPUs and if your drivers are up to date, you're good to go. Just use a frontier model to help you figure it out if you run into problems.\n\nIt's not like using GPUs for gaming back in the day where you had to set up SLI and basically needed matching GPUs. Any two reasonably modern cards from the same brand (both Nvidia or both AMD) will be fine together.",
                  "score": 2,
                  "created_utc": "2026-01-15 06:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl0uq3",
          "author": "sam7oon",
          "text": "i would suggest for you to read more into it, there is a lot of types of LLMs, if you need it to invoke tools, code, write emails and so on,\n\nyou are most probably are gonna max at 14B models, however if you are planning to run a long context, you may need to downsize to 8B, \n\nBut again depending on what you are gonna use it for, its different, it would be nice if you can specify your target,",
          "score": 5,
          "created_utc": "2026-01-14 18:27:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzljv0p",
              "author": "mckirkus",
              "text": "Totally depends on how it's quantized.  80 billion parameters at Q4 is smaller than 80 billion at Q16",
              "score": 2,
              "created_utc": "2026-01-14 19:53:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm22vm",
                  "author": "sam7oon",
                  "text": "Yes i see more informed answers after mine, am afraid to touch quantized models , do you think a 80B Model quantized would produce better result than a non quant 8B models ? \n\nAssuming we are talking about generating email as an example.",
                  "score": 1,
                  "created_utc": "2026-01-14 21:16:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl3l8a",
          "author": "PermanentLiminality",
          "text": "If you are doing anything serious, you need space for context.  You can shoehorn a model that fills your VRAM.  It will work great for a simple one sentence question.  you will not be able to feed it a lot of data though.  Try and keep the model under 80% of your VRAM. so say 13Gb in size.  Not a hard rule, but it is a good starting place.  If you want to feed a big input, you may need something smaller.  \n\nYou can spill over into system RAM, but your speed will drop big time.  The Qwen 3 30B A3B actaully does OK even if it does spill over a bit.   It may be one of the largest models you can run effectively.",
          "score": 2,
          "created_utc": "2026-01-14 18:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw00oh",
          "author": "cibernox",
          "text": "The best you can fit in vram alone, probably gpt-oss 20B. If you use system ram, y comes down to the speed you want. But you can run 80B MoE models at reasonable speed",
          "score": 2,
          "created_utc": "2026-01-16 08:45:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzllh0o",
          "author": "karmakaze1",
          "text": "I'm running [qwen3-30b-a3b with IQ4_XS quant](https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF?show_file_info=Qwen_Qwen3-30B-A3B-IQ4_XS.gguf) which doesn't quite fit. I let some of the layers spill over to CPU but it still gets a decent number of tokens/sec.",
          "score": 1,
          "created_utc": "2026-01-14 20:00:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzncn80",
              "author": "vertical_computer",
              "text": "When itâ€™s so close to fitting, why not drop to something like Q3_K_M and let it entirely fit on your GPU?",
              "score": 3,
              "created_utc": "2026-01-15 01:12:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzo7j3b",
                  "author": "karmakaze1",
                  "text": "Great advice. By default I've been using q4 quants. The other way to go is to use a dense model with fewer parameters which can be a bit slower with higher accuracy.",
                  "score": 1,
                  "created_utc": "2026-01-15 04:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlp63d",
          "author": "PM_ME_COOL_SCIENCE",
          "text": "Iâ€™m using a 5060 ti with 16gb vram, and with llama.cpp server Iâ€™m getting gpt oss 20b mxfp4 with 120k context. I can share the exact command, but nothing too crazy. Fully on gpu, 120 tk/s generation. If youâ€™re willing to go slower, you can fit qwen 3 next or gpt oss 120b with MOE expert offloading. Those are the biggest yet somewhat performant models for mixed cpu/gpu.\n\nWhatâ€™s your use case? How much context?",
          "score": 1,
          "created_utc": "2026-01-14 20:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlq28j",
              "author": "ristlincin",
              "text": "Jesus 120k? What's the model's size?",
              "score": 1,
              "created_utc": "2026-01-14 20:21:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlrv5j",
                  "author": "PM_ME_COOL_SCIENCE",
                  "text": "20b, 3.5b active. Gpt oss 20b",
                  "score": 1,
                  "created_utc": "2026-01-14 20:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzm7j0i",
          "author": "GutenRa",
          "text": "I discovered the qwen3-next 80b q4, which was recently updated by unsloth. \n\nA significant improvement in quality compared to the 30b and lower models.\n\n10t/s at 5900x & 5060ti16gb vram & 64gb ram.",
          "score": 1,
          "created_utc": "2026-01-14 21:40:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmd790",
          "author": "metamec",
          "text": "Qwen3-Next-80B-A3B (Thinking or Instruct) by Unsloth. Use the Q4 K M quant.\n\nI have an RTX 5080 too with 128 GB RAM, and that model only uses around 40 GB of RAM when configured optimally. Unfortunately I'm a long way from home until next week and unable to access my exact settings, but you basically want to offload 99 layers to the GPU and then force 36 MoE layers to the CPU.",
          "score": 1,
          "created_utc": "2026-01-14 22:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmh1dj",
          "author": "National_Cod9546",
          "text": "You can fit a 24b model at q4s and 16k context. More then that and it will become painfully slow as it spills into ram. Generally you want to use the biggest model you can at q4. At lower quants, the model becomes too stupid. Very large 70b+ can handle going to q3 or even q2 before the stops gets to bad. But you are usually better using a smaller model. With less than 16k context, it gets hard to give the model enough information to do anything useful.Â ",
          "score": 1,
          "created_utc": "2026-01-14 22:24:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmnqy0",
          "author": "Loud_Communication68",
          "text": "Check out the phi series",
          "score": 1,
          "created_utc": "2026-01-14 22:57:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq6ixt",
          "author": "ProfessionalYear5755",
          "text": "I have just built my rig B550, 5600X, 9060XT 16GB, (only 16GB 3200mhz at the moment) BUT what surprised me was when a model spills over in RAM it does NOT crawl. It is slower yes but is totally useable. I believed from insane amounts of research that RAM = crawl, It demonstrably does not. Happy days!",
          "score": 1,
          "created_utc": "2026-01-15 13:47:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvtjvy",
          "author": "techlatest_net",
          "text": "24B Q4\\_K\\_M fits clean on 16GB (RTX 5080), \\~140t/s with your 64GB RAM soaking KV cache. Push 30B Q3 or 32B Q2\\_K if you squash context to 2k, but quality dips. 30B isn't maxâ€”quantization gets you there, offload handles overflow.â€‹",
          "score": 1,
          "created_utc": "2026-01-16 07:46:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwensw",
          "author": "mistrjirka",
          "text": "I would say one of the best ones is ministral 3 14B. Depends on what you want.",
          "score": 1,
          "created_utc": "2026-01-16 10:58:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyiopq",
          "author": "TheRiddler79",
          "text": "You could download gptoss120b offload most of it to ram and you'd still get good speed and a very intelligent ai. You should try",
          "score": 1,
          "created_utc": "2026-01-16 17:43:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00w93n",
          "author": "beryugyo619",
          "text": "Theoretically 8B fp16, 16B q8, 32B q4, 64B q2, 128B q1, so on. See the pattern?  \n\nAnd there comes overheads and context caches",
          "score": 1,
          "created_utc": "2026-01-17 00:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03a4k2",
          "author": "Mad_Bark00",
          "text": "Give me a good llm for 8gb ram",
          "score": 1,
          "created_utc": "2026-01-17 11:46:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlenzq",
          "author": "ZoSoPa",
          "text": "http://www.canirunthisllm.net/",
          "score": 1,
          "created_utc": "2026-01-14 19:29:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzm2mm7",
              "author": "Dan_Wood_",
              "text": "SSL has expired and itâ€™s a coming soon page hosted by PythonAnywhere â€¦",
              "score": 2,
              "created_utc": "2026-01-14 21:18:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm4q8q",
                  "author": "Caltaire",
                  "text": "Donâ€™t know how accurate this is but saw it posted elsewhere in response to the previous link.\n\nhttps://apxml.com/tools/vram-calculator",
                  "score": 1,
                  "created_utc": "2026-01-14 21:27:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlf0hm",
          "author": "Turbulent_Dot3764",
          "text": "16vram, I5 10Â°, 32gb ram\n\nGPS oss 20b 48k context, full gpu, ollama.\n\nRunning this plus my docker and others apps for development.\n\n\n24B models fit wit less context.\n\n\nBigger than 24B, only with smaller quantization",
          "score": 1,
          "created_utc": "2026-01-14 19:31:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoia8v",
              "author": "SexMedGPT",
              "text": "Which quant do you run?",
              "score": 1,
              "created_utc": "2026-01-15 05:33:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04ctzj",
                  "author": "Turbulent_Dot3764",
                  "text": "I pull from ollama, it's use the open aÃ­ gpt. MXFP4",
                  "score": 1,
                  "created_utc": "2026-01-17 15:41:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlmmoj",
          "author": "forthejungle",
          "text": "Only VRAM matters. RAM is irrelevant, especially when it comes to speed.",
          "score": 1,
          "created_utc": "2026-01-14 20:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzleapx",
          "author": "HealthyCommunicat",
          "text": "Why not just copy paste this exact question into gemini and get a really specific exact answer that is near instant, actually informational, and can give you even further specific info related to your setup? Do you actually care to learn?",
          "score": -4,
          "created_utc": "2026-01-14 19:27:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlk574",
              "author": "true-though",
              "text": "change your username.",
              "score": 9,
              "created_utc": "2026-01-14 19:54:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm6ssx",
                  "author": "HealthyCommunicat",
                  "text": "nawh. the basis of healthy communication is being upfront and blunt about things that would actually in long run benefit the other person. you don't think it would be good for OP to know to ask gemini? they want to get into llm's but dont realize that all the answers and all the information in the world is more at your fingertips than ever? go read all my comments, they're all extremely blunt - yet not one single person is able to refute what i say because every single person knows in the back of their head what im saying is right. tell me, doesn't it take much more time and effort and energy to make a post on reddit and wait for comments and reply and read them when you can do that all in one interaction with gemini?",
                  "score": 2,
                  "created_utc": "2026-01-14 21:37:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzm35ov",
                  "author": "Condomphobic",
                  "text": "He has a genuine point that I actually wonder about daily. \n\nA LLM like Gemini can give a **much** better and nuanced answer than humans can.",
                  "score": 2,
                  "created_utc": "2026-01-14 21:20:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qery3t",
      "title": "Training ideas with 900Gb of vram",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qery3t/training_ideas_with_900gb_of_vram/",
      "author": "soppapoju",
      "created_utc": "2026-01-16 20:56:10",
      "score": 33,
      "num_comments": 22,
      "upvote_ratio": 0.9,
      "text": "Hello, i have an opportunity to train something and use a \"supercomputer\".\n\nWhat would you do with this amount of vram available? About 10x H100\n\nThinking of training something and bringing it to personal use or to be used publicly on a website.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qery3t/training_ideas_with_900gb_of_vram/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o006wdx",
          "author": "FamiliarRice",
          "text": "I have an open source research company that uses around this much compute. Let me know if youâ€™re interested in collaborating and we will get you on some top ml conference papers !",
          "score": 25,
          "created_utc": "2026-01-16 22:21:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01x6cd",
              "author": "NoobMLDude",
              "text": "I would be interested in collaborating. \nWhat topics , domains , problems do you work on?\nCan I Dm you?",
              "score": 1,
              "created_utc": "2026-01-17 04:37:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzy2ga",
          "author": "WolfeheartGames",
          "text": "If you don't already have an important research idea to follow, distilling a larger model down to a non quadratic architecture is probably the highest impact you can have. \n\nIf you have an experiment to run, make sure it's solid first.\n\nIf you will allow me a moment to self promote, I have this https://github.com/bigwolfeman/TitanMAC-Standalone an implementation from a series of Google papers. I haven't worked out all the kinks and performance elements yet, but you can take the neural memory module and use it on any other architecture as a replacement for quadratic attention. The kernels are there for forward and back passes. I'd prefer if you contributed to this project though =) you'd be working through issues either way if you took the neural memory just as a factor of writing your own architecture. \n\nThere is a potential issue with the sliding window required for the neural memory, you may have chunk boundary issues. From my testing I have this resolved, but I haven't gone past 50k steps on it yet to make sure it doesn't crop up again later.\n\nI also submitted this to the hackathon for this subreddit https://github.com/bigwolfeman/Retnet-Distillation \n\nI do not recommend using retnet for this. It is extremely unstable and slow. I had to patch torchscale just to make it work at all and kept finding issues in torchscale. But there is a nice reference implementation of keeping the teacher in vram beside the student, caching it's logits to disk, or inferencing from a vllm server and exposing top k logits.\n\nEven with how unstable retention is, it may be worth implementing to resolve chunk boundary problems. I am currently not maintaining retention in TitanMAC, but if further testing shows issues I'm planning to use it.\n\nAlso, my repo has the nested learning module based on SGD. It is a memory efficient optimizer. Allowing for more parameters with less vram. \n\nIf you're interested in using this repo, reach out to me. I have a branch I am doing a lot of development work on. I have found that feeding in hyper connections directly to the nested optimizer gives it richer signals for convergence and is basically free to do. It may be possible to average gradient heuristics to 16x16 tiles using my development branch, but I haven't had time to test this yet. It is the last step of the dynamic sparse implementation I'm working on in that branch. \n\n---\n\nOther great use for so much vram: distill a VLM to V-JEPA.\nDistill to HOPE https://github.com/Sk16er/hope_nano HOPE is a vram monster, I don't think this architecture is ready to scale yet, but it's research value is tremendous.\n\nEdit: the custom kernels are targeting blackwell, but they're written in Triton. Getting them ready for an h100 shouldn't be too bad.\n\nIf this deployment you want to do is far enough out in the future, I am planning on implementing the entire architecture in mojo for further performance gains. I'm still ablating several takes on the sparsity before I do this. The current code isn't designed to shard across gpus, though pytorch makes it fairly easy. Mojo is very good at sharing, though I haven't tested that myself.",
          "score": 16,
          "created_utc": "2026-01-16 21:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o009vgj",
          "author": "RoyalCities",
          "text": "Download everything you can find from Annas archive and make your own Chatgpt.",
          "score": 12,
          "created_utc": "2026-01-16 22:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00unc0",
              "author": "No-Acanthaceae-5979",
              "text": "This",
              "score": 2,
              "created_utc": "2026-01-17 00:30:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o000pn1",
          "author": "amooz",
          "text": "Whereâ€™s the dude whoâ€™s training a model using only text from the 1800â€™s London?",
          "score": 9,
          "created_utc": "2026-01-16 21:50:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzzwf8",
          "author": "K33P4D",
          "text": "this is just too much  \nI'm here whipping 12B and 30B models, waiting 30 mins for an answer and this dude here with 900GB vRAM smh",
          "score": 6,
          "created_utc": "2026-01-16 21:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0065y8",
              "author": "much_longer_username",
              "text": "It sounds like they're just a guest on this system, probably through work or school. \n\nBut my dad always said \"it's only a lot of money if you don't have it\".",
              "score": 9,
              "created_utc": "2026-01-16 22:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o009vrn",
                  "author": "Large-Excitement777",
                  "text": "Still trying to wrap my head around how gullible everyone is",
                  "score": -2,
                  "created_utc": "2026-01-16 22:36:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o004vsj",
              "author": "Osprey6767",
              "text": "yeah lol same",
              "score": 1,
              "created_utc": "2026-01-16 22:11:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzxnub",
          "author": "Aromatic-Low-4578",
          "text": "I'd pretrain an LLM",
          "score": 2,
          "created_utc": "2026-01-16 21:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzzptm",
          "author": "Sicarius_The_First",
          "text": "If I'd write all I would do with it, it would be several pages long.",
          "score": 2,
          "created_utc": "2026-01-16 21:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzxqua",
          "author": "Hoak-em",
          "text": "\"planning\"-optimized glm4.7 would be interesting, though I'm not sure how much work that would take.",
          "score": 1,
          "created_utc": "2026-01-16 21:36:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o033n3l",
          "author": "soppapoju",
          "text": "900gb is an estimate. It might be more might be less, when i get to know the exact specs of the machine.\n\nAs person new to LLM training, is why I ask.\n\nGot some ideas with gpt but human answers always better!\n\nI can que to use it, but it needs to be something worthwhile to me or to a lot of people.\n\nBest Regards all!",
          "score": 1,
          "created_utc": "2026-01-17 10:48:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o03b3e0",
          "author": "alexbarbershop",
          "text": "I would set it loose on a very high quality curated historical dataset or historical imagery datasets as that is a very weak spot in all AI models currently.  Coincidentally, I do have over 1000 negative scans of public domain trolley images from the NEHR trolley museum collection on DigitalMaine and a massive spreadsheet of metadata I compiled manually.",
          "score": 1,
          "created_utc": "2026-01-17 11:54:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03p8fb",
          "author": "cjstoddard",
          "text": "Depends, do you want to get rich or do you want to do legitimate research? If you want to get rich, I'd say you can't go wrong with porn, if you want to do legitimate research, I got nothing.",
          "score": 1,
          "created_utc": "2026-01-17 13:36:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gz5yd",
          "author": "CurtissYT",
          "text": "Id train an ai on purely on cia conspiracy theories and something like the epstein files, etc. Something funny. Maybe something about HolyC and other stuff like that",
          "score": 1,
          "created_utc": "2026-01-19 13:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004io8",
          "author": "UseMoreBandwith",
          "text": "I would start with   \nwriting better reddit posts",
          "score": -3,
          "created_utc": "2026-01-16 22:09:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhg3pm",
      "title": "GLM-4.7-Flash-NVFP4 (20.5GB) is on huggingface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qhg3pm/glm47flashnvfp4_205gb_is_on_huggingface/",
      "author": "DataGOGO",
      "created_utc": "2026-01-19 20:42:39",
      "score": 29,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I published a mixed precision NVFP4 quantized version of the new GLM-4.7-FLASH model on huggingface. \n\n  \nCan any of you test it out and let me know how it works for you?\n\n  \n[GadflyII/GLM-4.7-Flash-NVFP4 Â· Hugging Face](https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qhg3pm/glm47flashnvfp4_205gb_is_on_huggingface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qg403g",
      "title": "Claude Code and local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qg403g/claude_code_and_local_llms/",
      "author": "rivsters",
      "created_utc": "2026-01-18 09:13:25",
      "score": 27,
      "num_comments": 20,
      "upvote_ratio": 0.89,
      "text": "This looks promising - will be trying later today [https://ollama.com/blog/claude](https://ollama.com/blog/claude) \\- although  blog says \"It is recommended to run a model with at least 64k tokens context length.\" Share if you are having success using it for your local LLM. \n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qg403g/claude_code_and_local_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0e71ur",
          "author": "lol-its-funny",
          "text": "EDIT: I tested that even newer llama-server can do this natively without requiring litellm in the middle. See [https://www.reddit.com/r/LocalLLaMA/comments/1qhaq21/comment/o0jtqr0/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qhaq21/comment/o0jtqr0/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\n\\----  \n  \nFYI, LiteLLM can already do this. It can connect with both Anthropic (Claude Code) or OpenAI clients (ChatGPT desktop) and then connect those to Anthropic or OpenAI or llama-cpp (openai compatible) or other native providers.\n\nNot a fan of Ollama with their cloud direction. Anyone interested in cloud will go to the native providers or aggregators like openrouter. Local LLM folks donâ€™t need another cloud focused service.",
          "score": 17,
          "created_utc": "2026-01-19 01:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gkgim",
          "author": "SatoshiNotMe",
          "text": "As others said, months ago llama.cpp already added anthropic messages API compatibility for some popular open-weight LLMs. This makes it easy to hook up these LLMs to work with CC. I had to hunt around for the specific llama-server flag settings for these models and I gathered these into a little guide on setting up these models to work with CC and Codex CLI:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nOne subtle thing to note is that you have to set \n\nCLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC\": \"1\"\n\nin your CC settings, to avoid total network failure due to CCâ€™s logging pings.",
          "score": 7,
          "created_utc": "2026-01-19 11:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hqlpq",
              "author": "Tema_Art_7777",
              "text": "    Â  Â  \"claudeCode.environmentVariables\": [\n    Â  Â  Â  Â  {\n    Â  Â  Â  Â  Â  Â  \"name\": \"ANTHROPIC_BASE_URL\",\n    Â  Â  Â  Â  Â  Â  \"value\": \"http://192.168.10.65:8282\"\n    Â  Â  Â  Â  },\n    Â  Â  Â  Â  {\n    Â  Â  Â  Â  Â  Â  \"name\": \"CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC\",\n    Â  Â  Â  Â  Â  Â  \"value\": \"1\"\n    Â  Â      }\n    Â  Â  ]\n\nThanks for the URL. I am using claudecode in visual studio code. I added to my settings these lines, however, instead of invoking the llama.cpp server, it is asking me to login with anthropic keys. it actually never calls my server. Did I miss a step?",
              "score": 1,
              "created_utc": "2026-01-19 15:43:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0hzz86",
                  "author": "eli_pizza",
                  "text": "You need to set an API key env var too (set it to anything) so it doesnâ€™t try to use oauth",
                  "score": 1,
                  "created_utc": "2026-01-19 16:25:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0eqz5y",
          "author": "Tuned3f",
          "text": "Llama.cpp had this months ago",
          "score": 3,
          "created_utc": "2026-01-19 02:53:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0euind",
              "author": "Tema_Art_7777",
              "text": "How do you hookup claude code to llama.cpp??",
              "score": 2,
              "created_utc": "2026-01-19 03:12:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0evhe8",
                  "author": "Tuned3f",
                  "text": "Set ANTHROPIC_BASE_URL to the llama.cpp endpoint",
                  "score": 4,
                  "created_utc": "2026-01-19 03:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0dy6x3",
          "author": "cars_and_computers",
          "text": "If this is what I think. This is kind of a game changer. The power of Claude code cli but with the privacy of your local models is awesome. Assuming it's actually private. If not it would be amazing to have llamacpp have something like this working and have it be a truly private session",
          "score": 3,
          "created_utc": "2026-01-19 00:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fb0fq",
              "author": "Big-Masterpiece-9581",
              "text": "Claude code router and lite-llm already exist. But I donâ€™t think this is legal. Theyâ€™ll probably sue.",
              "score": 3,
              "created_utc": "2026-01-19 04:57:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0i09me",
                  "author": "eli_pizza",
                  "text": "Sue under DMCA anti-circumvention provision? Seems like a stretch.",
                  "score": 1,
                  "created_utc": "2026-01-19 16:26:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0fawba",
          "author": "Big-Masterpiece-9581",
          "text": "How is this kosher with the proprietary Claude license? They donâ€™t want you using other tools with their models. I have to assume they donâ€™t want you using their tool with other models.",
          "score": 1,
          "created_utc": "2026-01-19 04:56:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i0uuq",
              "author": "eli_pizza",
              "text": "I imagine theyâ€™re quite happy to have the whole industry standardize on Claude code as the best agent, especially local LLM enthusiasts.",
              "score": 1,
              "created_utc": "2026-01-19 16:29:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0m79ph",
              "author": "Internal_Werewolf_48",
              "text": "Claude Code has about a half dozen competing tools on the market. Qwen Code CLI, Mistral Vibe, Charm Crush, Codex, OpenCode, Plandex, Gemini CLI. Probably another five dozen vibe coded slop projects that are already abandoned too.\n\nClaude doesnâ€™t really offer anything that special to bother defending.",
              "score": 1,
              "created_utc": "2026-01-20 05:14:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0m7lpw",
                  "author": "Big-Masterpiece-9581",
                  "text": "I think they have done a fantastic job and constantly innovate. MCP and skills are two of the biggest as well as the cli and its autonomous workflow. But theyâ€™re easy to copy and others do similar things. Iâ€™m really enjoying opencode.",
                  "score": 1,
                  "created_utc": "2026-01-20 05:16:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0htoml",
          "author": "SatoshiNotMe",
          "text": "Can you first check if this works on the command line ? I donâ€™t use VSCode so not too familiar with how to set up CC there.",
          "score": 1,
          "created_utc": "2026-01-19 15:57:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcpoaw",
      "title": "Google just opensourced Universal Commerce Protocol.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcpoaw/google_just_opensourced_universal_commerce/",
      "author": "techlatest_net",
      "created_utc": "2026-01-14 15:08:25",
      "score": 26,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "**Google just dropped the Universal Commerce Protocol (UCP) â€“ fully open-sourced! AI agents can now autonomously discover products, fill carts, and complete purchases.** \n\nGoogle is opening up e-commerce to AI agents like never before. TheÂ **Universal Commerce Protocol (UCP)**Â enables agents to browse catalogs, add items to carts, handle payments, and complete checkouts end-to-endâ€”without human intervention.\n\n# Key Integrations (perfect for agent builders):\n\n* **Agent2Agent (A2A)**: Seamless agent-to-agent communication for multi-step workflows.\n* **Agents Payment Protocol (AP2)**: Secure, autonomous payments.\n* **MCP (Model Context Protocol)**: Ties into your existing LLM serving stacks (vLLM/Ollama vibes).\n\nLink: [https://github.com/Universal-Commerce-Protocol/ucp](https://github.com/Universal-Commerce-Protocol/ucp)\n\nWho's building the first UCP-powered agent? Drop your prototypes below â€“ let's hack on this!Â ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcpoaw/google_just_opensourced_universal_commerce/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzke6vh",
          "author": "eli_pizza",
          "text": "What retailers actually support it? A protocol that nobody uses isn't too useful.",
          "score": 4,
          "created_utc": "2026-01-14 16:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzklq2l",
              "author": "FaceDeer",
              "text": "Google *just* opensourced it. How quickly are you expecting coders everywhere to be able to jump and implement something like this?",
              "score": 0,
              "created_utc": "2026-01-14 17:20:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzko7zf",
                  "author": "eli_pizza",
                  "text": "Itâ€™s got dozens of logos on the page as â€œendorsingâ€ it but have any of them deployed or committed to deploying it?\n\nIf not what would a UCP agent even do right now, pretend order from pretend stores?\n\nAnyway, open protocols are better than closed ones but theyâ€™re not always â€œgood.â€ Google AMP was open and it was bad and harmful to the web.",
                  "score": 1,
                  "created_utc": "2026-01-14 17:31:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmdvep",
                  "author": "FoxTimes4",
                  "text": "Opensourcing something that doesnâ€™t work anywhere isnâ€™t that useful so does it work somewhere today?",
                  "score": 1,
                  "created_utc": "2026-01-14 22:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlymaf",
          "author": "lanthos",
          "text": "Any idea how long Google is planning on supporting it? Does Gemini already use it or if not what is their roadmap?",
          "score": 1,
          "created_utc": "2026-01-14 21:00:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmqhnj",
          "author": "frobnosticus",
          "text": "Is it newly existing and open source or is it stable and in use...newly open sourced?",
          "score": 1,
          "created_utc": "2026-01-14 23:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqoe1z",
          "author": "sn2006gy",
          "text": "Since agents are probabilistic - what is the utility of agents doing this considering, the risks involved?\n\nThis sounds like something being branded agentic for the sake of branding \"slap AI or AGENT on it\" without people fully understanding that agentic workloads are still probabilistic and \"buying shit\" shouldn't have a probability attached to it.",
          "score": 1,
          "created_utc": "2026-01-15 15:18:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfi1gy",
      "title": "How much vram is enough for a coding agent?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qfi1gy/how_much_vram_is_enough_for_a_coding_agent/",
      "author": "AlexGSquadron",
      "created_utc": "2026-01-17 16:44:06",
      "score": 20,
      "num_comments": 71,
      "upvote_ratio": 0.78,
      "text": "I know vram will make or break the context of an AI for an agent, can someone tell me what their experience is, which model is best and what is called enough vram, so that AI starts behaving like a junior dev",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfi1gy/how_much_vram_is_enough_for_a_coding_agent/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o04s64e",
          "author": "Kitae",
          "text": "I have a Rtx 5090 with 32gb of ram and instill use Claude for everything meaningful. \n\nI think of local LLMs as for fun or for extreme privacy or for large amounts of work that can be done by a simple model.",
          "score": 33,
          "created_utc": "2026-01-17 16:54:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04tj1j",
              "author": "alphatrad",
              "text": "I have 48gb of VRAM and mostly agree with this assessment.\n\nI have very good luck with local tab completion and some generation on a model I fine tuned on my projects.\n\nBut I still daily drive Claude code.\n\nLocal however is great at above 24gb for lots of document work though. I just can't see it working at Claude levels. Even if you had a Mac Studio Ultra w/ 512gb of memory, you might be able to load a huge model to up the performance but then it still won't be as fast as Claude.\n\nThe fact is, local has come a long way. But you can't compete with a data center of GPU's.",
              "score": 22,
              "created_utc": "2026-01-17 17:00:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o05ibof",
                  "author": "DifficultyFit1895",
                  "text": "I have that Mac Studio with 512gb of memory. The best local balance of speed and coding ability for me is qwen3-next-80b and itâ€™s not bad, really.  I prefer the frontier models in Github Copilot, though, as I donâ€™t have any extreme privacy concerns.",
                  "score": 9,
                  "created_utc": "2026-01-17 18:55:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o06ebwx",
                  "author": "thedarkbobo",
                  "text": "yep same here 2 rtx 3090 and I eventually use auto claude with a) glm 4.7 [z.ai](http://z.ai) b) when it gets locked for 5hours I swap to local qwen 3 next and (its 3 times slower I think) use it to move forward.",
                  "score": 1,
                  "created_utc": "2026-01-17 21:35:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o05s3ea",
              "author": "Better-Cause-8348",
              "text": "Local Whisper and Kokoro are a must. Probably the top thing I use local models for. Vectoring data is also helpful to run local.",
              "score": 6,
              "created_utc": "2026-01-17 19:41:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0br35f",
                  "author": "Kitchen_Fix1464",
                  "text": "I just brought up this stack in my home lab. It's nice having TTS/STT and mem0 hosted locally to share across my network.",
                  "score": 2,
                  "created_utc": "2026-01-18 17:47:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0atheo",
                  "author": "Kitae",
                  "text": "Yeah I haven't gotten here yet but I feel like local LLM services to do things you just wouldn't do with your max plan is a smart use of local LLMs.",
                  "score": 1,
                  "created_utc": "2026-01-18 15:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04sw1g",
              "author": "AlexGSquadron",
              "text": "Which model do you use locally? What experience have you had with any of the models?",
              "score": 1,
              "created_utc": "2026-01-17 16:57:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0atnys",
                  "author": "Kitae",
                  "text": "I personally am in a research phase where I am getting as many models working as possible and benchmarking them to understand their capabilities.\n\nBut qwen is my favorite.",
                  "score": 1,
                  "created_utc": "2026-01-18 15:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o06qdb0",
              "author": "Comfortable_Ad_8117",
              "text": "I have 24 GB of VRAM and use my cards for so many Ai applications, however I use cursor to write them for my local Ai",
              "score": 1,
              "created_utc": "2026-01-17 22:34:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o04x694",
          "author": "Terminator857",
          "text": "128 GB of strix halo and still use various online",
          "score": 10,
          "created_utc": "2026-01-17 17:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06fyvo",
              "author": "Zyj",
              "text": "I bought another one and Iâ€˜m looking to try networking three now",
              "score": 2,
              "created_utc": "2026-01-17 21:43:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0d7t08",
                  "author": "cjc4096",
                  "text": "How well does horizontal scaling work?   Can you split smaller models, that'd fit on a single machine, across multiple and get more token/s?   Or is it only useful when you need more vram?",
                  "score": 1,
                  "created_utc": "2026-01-18 22:05:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o04zetu",
          "author": "dsartori",
          "text": "There is no way around the fact that on-device models will be slower and dumber than anything you'd pay for in the cloud.\n\nWith midsized MoE models like gpt-oss-120b and qwen3-coder-30b you can get good results depending on your use case but it's not as fast or as high-quality as what you can get in the cloud for pretty cheap.\n\nHaving said all that, anything that gets you to decent performance on the 30b qwen model is the minimum bar for on-device coding agents. A 32GB Mac or a PC with an 8-12 GB video card and a good amount of fast RAM is probably where that starts.",
          "score": 4,
          "created_utc": "2026-01-17 17:27:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05013l",
              "author": "AlexGSquadron",
              "text": "I have 3080 10gb and 64gb DDR4 ram",
              "score": 1,
              "created_utc": "2026-01-17 17:30:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o050a7c",
                  "author": "dsartori",
                  "text": "That should work just fine for the 30b Qwen model. gpt-oss-20b will also be useful on that device.",
                  "score": 3,
                  "created_utc": "2026-01-17 17:31:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o058k0m",
                  "author": "Prudent-Ad4509",
                  "text": "Raise that vram to 24-48-64 and you can run starter models. Go for 192Gb, and you can run pretty competent models with limitations. Double it after that, and you will be able to run almost everything good locally.\n\nEither that or go the apple route with unified memory from 128gb and up.\n\nBut you certainly can play around with 10Gb if you use small models for highly specialized tasks. You certainly can use 30b Qwen coder with offloading most experts. It won't be fast, but it will be faster than coding by hand, especially when coding in languages you don't know.\n\nJust remember that you will have to correct it a lot. Same as with bigger models, though.",
                  "score": 2,
                  "created_utc": "2026-01-17 18:10:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o06glvy",
                  "author": "johnerp",
                  "text": "Snap! So interested in art of the possible. Iâ€™m currently spending $20 a month on Gemini, using anti gravity - been more than enough for what I need and intelligent. Iâ€™m now trying open code client with AG models to see if I can bring in open source models for free for certain agent personas, I suspect Gemini flash 3 will remain my go to until the software or hardware architecture for llms radically changes so we can get the quality and performance in a single retail gpuâ€¦.",
                  "score": 1,
                  "created_utc": "2026-01-17 21:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o050c1k",
              "author": "Maximum-Wishbone5616",
              "text": "100$ for 6-8tps is cheap? I paid 4k for 2x 5090 that basically can generate without limit 5-10x tokens per week. I easily locally eat up my daily tokens used with max in just 25 prompts. At fraction of the time",
              "score": 1,
              "created_utc": "2026-01-17 17:32:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o050sg2",
                  "author": "AlexGSquadron",
                  "text": "For 7k I would have gotten rtx 6000 pro 96gb vram",
                  "score": 2,
                  "created_utc": "2026-01-17 17:34:11",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o051034",
                  "author": "dsartori",
                  "text": "If you don't care who sees your stuff the Z.Ai coding plans are super cheap. \n\nI agree with you in that that local is the way to go for me, I'm just trying to be straight up with a new user.",
                  "score": 1,
                  "created_utc": "2026-01-17 17:35:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o04wh54",
          "author": "Odd-Criticism1534",
          "text": "Iâ€™m not an app coder or building anything extensive, mostly personal automations or things to help with work and life admin - but Iâ€™ve found devstral-small-2-2512 run on LM studio using Mistral Vibe coding agent/assistant to work well for my small projects",
          "score": 5,
          "created_utc": "2026-01-17 17:13:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09yixe",
          "author": "akumaburn",
          "text": "I can run a functional 4B coding model (Qwen 3 4B Polaris Distill) at 128K context entirely within 16GB of VRAM. This was tested in OpenCode.\n\nThis comes after extensive research into lesser-known models that significantly outperform what most LLM users seem to be using. Even so, the experience remains far from ideal.\n\nMy conclusion: the practical minimum for serious work is around 36GB of VRAM. The reason is context.\n\nSufficient context is essential for the model to genuinely understand your project and operate coherently within it. For coding tasks, you want as much context as possibleâ€”and at 256K tokens, you're looking at roughly 16GB of VRAM for the KV cache alone, before accounting for the model weights. This assumes aggressive quantization (q8/q4). You'd need even more if you're not willing to sacrifice quality here (20-40GB).\n\nCPU offloading is technically possible, but latency scales catastrophically as context fills; waiting an hour per prompt is simply untenable for agentic workflows.\n\nProcessing speed further constrains your options, effectively limiting you to MoE architectures with small activation sizes.\n\nThe only models in this class that handle tool calls reliably exceed 30B total parameters. \n\nThe practical minimum I've found is Unsloth's T1 quant of Qwen 3 Next 80-A3B, which requires just over 20GB on its own.\n\nIn sum: 32GB is workable if you're willing to compromise on context. Otherwise, you're looking at 36GBâ€”which means either multi-GPU configurations or stepping up to professional-grade hardware.",
          "score": 3,
          "created_utc": "2026-01-18 11:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0cbc1z",
          "author": "sloptimizer",
          "text": "These are the \"tiers\" unlocked by certain level of VRAM\n\n* 128GB - minimal usable coding agent (GLM-4.6V)\n* 160GB - capable coding agent (MiniMax-M2.1)\n* 448GB - best open-weight models (DeepSeek-V3.2)\n\nThe smallest model usable for coding is GLM-4.5-Air, or the newever GLM-4.6V. You are looking at around 128GB VRAM, since these models degrade very quickly when quantized. Any smaller models are highly inconsistent for coding and are simply not worth the time.\n\nGoing one size above takes you to MiniMax-M2.1, which is a surprisingly capable coding agent. It puchnes way above its weight. You can get away with 160GB VRAM before the model becomes braindead due to quantization.\n\nFinally, at 448GB you can run lightly quantized DeepSeek 671B models with great results.\n\nIn general, coding performance degrades quickly with quantization, and **Q4\\_K or INT4 are not good enough!** The smaller the model, the more it is affected by quantization. For example, Kimi-K2 is usable with all the experts quantized as 4 bit. But the same is not true for MiniMax-M2.",
          "score": 3,
          "created_utc": "2026-01-18 19:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05k1l4",
          "author": "twjnorth",
          "text": "Two possibilities that might work\n\nOption A) Local LLM on Cloud VM\n\nWhat about having infra as code to spin up a spot instance VM with GPUs on azure, AWS or Google when you need to code ?\n\nSpot instances are about 10% of pay as you go. No token limits and depending on number of GPUs and VRAM, could be cheap enough per hour. \n\nWould need some effort up front to make it resilient so when a spot instance goes down. It fires up another in a different family.  But it's possible.\n\nAnsible or whatever config tool would need to setup the ai infra (vllm / ollama) and pull the model required.\n\nTrash the instance when done for the day as can fire up another next time.  \n\nOption B) Fine tuning smaller model for specific use case.\n\nThis is the one I will be testing in short term. I am an Oracle Developer (25 years) in a niche vertical (Retail).  Plsql is present in foundation models but it's most likely from stack overflow or a side effect.  Haven't found anywhere it's part of primary training set.  \n\nIf I ask copilot to create a plsql function, it will have a go but results are mixed.  If it had currency conversion, it doesn't know I have a STD function for that.  It doesn't know all my package functions returna Boolean and an error_msg variable.  So the suggestions all need amending.\n\nIf I fine tune for my niche code, then I should be able to have a much better experience than foundation models (TBC by testing).",
          "score": 2,
          "created_utc": "2026-01-17 19:03:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o067ohr",
          "author": "Cronus_k98",
          "text": "VRAM isnâ€™t just for loading larger models. Itâ€™s also useful for having multiple smaller models loaded at the same time. When processing a large number of documents you save a lot of time by not needing to swap models into and out of memory.Â ",
          "score": 2,
          "created_utc": "2026-01-17 21:00:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o067uf6",
          "author": "Aggressive_Special25",
          "text": "48gb vram plus 128gb system ram.\n\nNo local models can code well enough for me. Claude works great.",
          "score": 2,
          "created_utc": "2026-01-17 21:01:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07k04o",
          "author": "cookieGaboo24",
          "text": "I haven't done any big scale coding yet, but my small tests show it could have potential, so I'd say 12gb is fine.\n\nI'm Running Gemma 3 12b (or GemmaCoder3 12b at that) at Q4 with 100k ctx at Q4. Gave it RAG with Python docs, it's own Reasoning capabilities and so far it gave me pretty solid code. Don't expect ultra quality code, but it is something I guess. \n\nHope that it at least doesn't waste time. Cheers",
          "score": 2,
          "created_utc": "2026-01-18 01:09:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09kwlj",
          "author": "HealthyCommunicat",
          "text": "Edit: OP dmâ€™ed me, he does not understand that claude is not something that can be run locally. i was correct. i keep trying to warn and correct him but he wants to buy 128gb of intel gpuâ€™s and thinks he can get even remotely close to claude performance. he doesnâ€™t even realize sonnet 4.5 and opus 4.6 are the model names. This post is a pure example of what I keep meaning when I talk about the massive amount of people trying to jump into AI without slightly even caring to learn to basics.\n\n512 gb.\n\nIf you think iâ€™m exaggerating and think anything under 256 gb will be useable, you have way too much to learn.\n\nGLM 4.7 which is already not even as smart as GPT 5.2 or sonnet 4.5 requires a bare bare minimum of 256 gb of vram, and even with 256 gb of vram it will be dumbed down and slow. \n\nIntelligence is exponential, meaning a 3b model will not be 1/10th the intelligence of a 30b model. It will be much lower than 1/10th. You can try to run 30b models or even 120b models as much as you want, but what makes it even worse is that for the technically unskilled, you need a model that is as smart as possible to make up for your gaps in knowledge.\n\nThe strongest point of LLMs is that is allows us humans for recursive learning. Beginner sets up LLM. Beginner asks LLM how to use it agentically. Beginner uses agentic LLM to code and ask further questions on how to code and so on. But for this to work you need actual passion and motivation, and thats not something that can be given, nor created.\n\nIf you are asking this question then you are not ready and do not have the motivation to get into this because if you did you would have already figured this part out.",
          "score": 2,
          "created_utc": "2026-01-18 09:39:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09nvrp",
              "author": "AlexGSquadron",
              "text": "What if I have figured it out and I ask others as if I have not figured it out?",
              "score": 1,
              "created_utc": "2026-01-18 10:07:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09nz94",
                  "author": "HealthyCommunicat",
                  "text": "you have a 3080. that in it by itself tells people enough.",
                  "score": 2,
                  "created_utc": "2026-01-18 10:08:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0h3o0b",
          "author": "arseneeth",
          "text": "I'm considering getting a used Mac studio 64gb m1 and add it to the cluster of Mac mini m4 16 gb, would be enough for coding agents?",
          "score": 2,
          "created_utc": "2026-01-19 13:46:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hgg1b",
              "author": "AlexGSquadron",
              "text": "Apparently no, but I wonder if there is a used version with more ram",
              "score": 1,
              "created_utc": "2026-01-19 14:55:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o04r5xe",
          "author": "EnvironmentalLow8531",
          "text": "[https://hardwarehq.io/coding-studio](https://hardwarehq.io/coding-studio) This should be a pretty decent start, plenty of other tools and a solid database of models/hardware available there if you need more specific info",
          "score": 2,
          "created_utc": "2026-01-17 16:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04t4d7",
          "author": "Low-Opening25",
          "text": "You cant afford it.\n\nI have 28GB of VRAM and I am still using CC or Codex for any real work, local LLMs are only good for fun and learning but not really usable for professional coding. As professional I donâ€™t have time to deal with issues, this would cost me time and money, I want stuff that mostly just works.",
          "score": 1,
          "created_utc": "2026-01-17 16:58:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04wycg",
              "author": "-Akos-",
              "text": "Sad but true. A single computer is no match for a datacenter full of dedicated hardware doing inference for you. And with prices nowadays, a 5090 with 32GB is 3-4K (at least in my country), at which I could do 150-200 months of Anthropic subscription. Thatâ€™s like 12-16 YEARS worth of a monthly Anthropic subscription..",
              "score": 2,
              "created_utc": "2026-01-17 17:16:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o053aic",
                  "author": "AlexGSquadron",
                  "text": "Or get a pro card like the Intel b50 4x in parallel",
                  "score": 1,
                  "created_utc": "2026-01-17 17:45:57",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o05hrst",
                  "author": "nyssaqt",
                  "text": "For now, who knows what the prices will do.",
                  "score": 1,
                  "created_utc": "2026-01-17 18:52:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o05084u",
              "author": "AlexGSquadron",
              "text": "You mean because of vram? I am waiting on Intel for that. But my question is, can I combine multiple GPUs?\nI already have 64gb DDR4 ram.",
              "score": 1,
              "created_utc": "2026-01-17 17:31:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o051lsw",
                  "author": "Low-Opening25",
                  "text": "to run a usable model, mostly the Chinese ones like deepseek or Kimiki 2, you would need much more than that, think 512-1536GB",
                  "score": 2,
                  "created_utc": "2026-01-17 17:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05nnmn",
          "author": "kamu-irrational",
          "text": "At work we have a few enterprise 80gb cards on prem. I STILL use cloud providers. I donâ€™t think there exists self hostable open models for agent based development even close to the flagship proprietary models.",
          "score": 1,
          "created_utc": "2026-01-17 19:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05q2r8",
              "author": "ZeSprawl",
              "text": "GLM 4.7 is very close, but you need around 200gb+ to do it at full quantization",
              "score": 2,
              "created_utc": "2026-01-17 19:31:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05sy0h",
          "author": "Better-Cause-8348",
          "text": "I would recommend checking into Ollama Cloud at $20/m. Their claim is privacy, and they have all the big open-source models, locally hosted, with a provider that meets the same privacy requirements as Ollama. You won't come close to touching the limits on the $20/m plan, even if you want to code with it. I use it often for my grunt work, but mainly for a mixture of experts' MCP tools. I query all the big open-source models they support, along with a couple of SOTA models, and get really great insight, suggestions, etc. Don't dismiss Gemini 3 Flash, which isn't a premium model on Ollama Cloud, meaning it's essentially unlimited. It's the best bang for the buck if you want to use it for coding.",
          "score": 1,
          "created_utc": "2026-01-17 19:45:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08iio2",
              "author": "AlexGSquadron",
              "text": "What's the difference between glm cuz I see 40$ for 3 months pro subscription",
              "score": 1,
              "created_utc": "2026-01-18 04:21:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0a9wq9",
                  "author": "Better-Cause-8348",
                  "text": "If you go directly through GLM, youâ€™re only going to get their models, whereas Ollama Cloud gives you access to around 20 models, including all of the GLM models. All models on Ollana Cloud are hosted on US-based servers. It's a great way to try or test open-source models without having accounts with every provider.",
                  "score": 1,
                  "created_utc": "2026-01-18 13:12:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05xvlh",
          "author": "No-Leopard7644",
          "text": "What if I have access to H100s?",
          "score": 1,
          "created_utc": "2026-01-17 20:10:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o068lbv",
              "author": "blackashi",
              "text": "\nhttps://www.youtube.com/watch?v=SmYNK0kqaDI",
              "score": 2,
              "created_utc": "2026-01-17 21:05:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06wkzz",
              "author": "No-Leopard7644",
              "text": "Anyone building local AI for work? Real prod grade AI Infra stack to serve the inference, storage for blazing fast RAG and LLMOps with evals, observability, DR and API Management built in with RBAC thrown in?",
              "score": 1,
              "created_utc": "2026-01-17 23:06:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05y1p3",
          "author": "Loud_Communication68",
          "text": "Maybe you can get the job done with nvidia orchestrator driving a coding agent or two",
          "score": 1,
          "created_utc": "2026-01-17 20:11:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o061djt",
          "author": "Clay_Ferguson",
          "text": "I asked Gemini what I can run on my brand new Dell XPS laptop and I can indeed run fairly fast inference on the 32GB memory, because the new CPU architecture makes main RAM act like VRAM. Just tell Gemini what memory you have and ask it what model to download from hugging face for best coding agent on a SLM. I'll be doing it soon myself!\n\n  \nEDIT. And try it out with OpenCode or LangChain Openwork!",
          "score": 1,
          "created_utc": "2026-01-17 20:28:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06fy1n",
          "author": "p_235615",
          "text": "I played around with mistral-3:8B in ollama with vscode, its not the best, but can often create quite good and meaningful code snippets, of course its not very good at comprehending and stitching together larger code. But for basic completion its not bad and often quite helpful.",
          "score": 1,
          "created_utc": "2026-01-17 21:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07s962",
          "author": "Proper_Taste_6778",
          "text": "I use minimax 2.1 for a few days locally and it's better than others models which I tried.",
          "score": 1,
          "created_utc": "2026-01-18 01:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05a2kt",
          "author": "huzbum",
          "text": "You could probably run MiniMax M2.1 or 128GB, otherwise, 256GB for GLM 4.7",
          "score": 0,
          "created_utc": "2026-01-17 18:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07tvhq",
              "author": "Proper_Taste_6778",
              "text": "There is unsloth/GLM-4.7-REAP-218B-A32B-GGUF that you can fit into 128gb too but I didn't test that yet. MiniMax m2.1 is really good imo. But I only tested it for few days",
              "score": 1,
              "created_utc": "2026-01-18 02:00:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qexu0l",
      "title": "Is there a local/self-hosted alternative to Google NotebookLM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qexu0l/is_there_a_localselfhosted_alternative_to_google/",
      "author": "RadiantCandy1600",
      "created_utc": "2026-01-17 00:36:37",
      "score": 18,
      "num_comments": 12,
      "upvote_ratio": 0.88,
      "text": "Iâ€™ve been using **Google NotebookLM** recently and the workflow is incredibleâ€”being able to upload a dozen PDFs and have the AI \"ground\" itself in those specific sources is a game changer for research.\n\nHowever, Iâ€™m not thrilled about uploading sensitive work documents or personal research to Googleâ€™s cloud. Iâ€™m looking for something I can run **locally on my own hardware** (or a private VPS) that replicates that \"Notebook\" experience.\n\n**Ideally, Iâ€™m looking for:**\n\n* **Privacy:** No data leaving my machine.\n* **Source Grounding:** The ability to chat with specific \"Notebooks\" or collections of PDFs/Markdown/Text files.\n* **Citations:** It needs to tell me exactly which page/document the answer came from (this is the best part of NotebookLM).\n* **Audio/Podcasts (Optional):** The AI podcast generator in NotebookLM is cool, but document analysis is my priority.\n\n**What are the best options in 2026?** Iâ€™ve heard names like **AnythingLLM**, **GPT4All**, and **Open Notebook** (the GitHub project) thrown around. Which one is currently the most stable and \"NotebookLM-like\"? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qexu0l/is_there_a_localselfhosted_alternative_to_google/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o012xn0",
          "author": "GrayRoberts",
          "text": "Open-notebook.io",
          "score": 5,
          "created_utc": "2026-01-17 01:20:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o013c4r",
          "author": "Aromatic-Low-4578",
          "text": "[https://github.com/lfnovo/open-notebook](https://github.com/lfnovo/open-notebook)",
          "score": 3,
          "created_utc": "2026-01-17 01:23:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06b593",
              "author": "sn2006gy",
              "text": "Hopefully the PR to create a helm chart for this moves quickly",
              "score": 1,
              "created_utc": "2026-01-17 21:18:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00x742",
          "author": "marsxyz",
          "text": "Just the RAG in openwebui ?",
          "score": 3,
          "created_utc": "2026-01-17 00:45:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o044czd",
          "author": "Clipbeam",
          "text": "Have a look at https://clipbeam.com if you use a Mac. No data leaves your machine, it allows you to chat with one or more pdfs/markdown/text files (plus also web links, audio and video files) and it will tell you which document it based it's answer on when you chat with it. \n\nWhat I haven't done yet is make it possible for the AI to cite the specific page of a document, but I may build this feature for a future update. Let me know if it meets your needs?",
          "score": 3,
          "created_utc": "2026-01-17 14:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00y8ep",
          "author": "michael_p",
          "text": "I was going to say anythingllm but you mentioned that",
          "score": 2,
          "created_utc": "2026-01-17 00:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o037mix",
          "author": "toadi",
          "text": "I use obsidian for note taking. I use a plugin that you can use claude, opencode, ... inside obsidian. Wrote a custom agent in opencode for research and scanning my repository and internet if needed. I provided it templates I want it to write in too.\n\nFeels close to notebookllm but in my own way.\n\nThe plugin: [https://github.com/RAIT-09/obsidian-agent-client](https://github.com/RAIT-09/obsidian-agent-client)\n\n  \nI tried different plugins but I prefer these types as my opencode can plugin different applications keeping its configuration the same. This way I don't need to redo it in my editor and obsidian.",
          "score": 2,
          "created_utc": "2026-01-17 11:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00xanc",
          "author": "MaverickPT",
          "text": "ragflow.io?",
          "score": 1,
          "created_utc": "2026-01-17 00:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ie83",
              "author": "Karyo_Ten",
              "text": "Have you actually used it?",
              "score": 0,
              "created_utc": "2026-01-17 07:29:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03gv86",
                  "author": "MaverickPT",
                  "text": "I've tried a few months ago, but I think their document ingestion pipeline was broken as I couldn't reliably ingest my datasheets. Since then I know they have updated it but I haven't tried it yet.\n\nHave you?",
                  "score": 1,
                  "created_utc": "2026-01-17 12:40:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o023g7n",
          "author": "arman-d0e",
          "text": "Surfsense. It started as a notebook lm clone but I think they got cease and desisted or something and had to redesign a bit\n\nhttps://github.com/MODSetter/SurfSense",
          "score": 1,
          "created_utc": "2026-01-17 05:23:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcmmvw",
      "title": "M4/M5 Max 128gb vs DGX Spark (or GB10 OEM)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/",
      "author": "Soggy-Leadership-324",
      "created_utc": "2026-01-14 12:59:55",
      "score": 14,
      "num_comments": 91,
      "upvote_ratio": 0.86,
      "text": "Iâ€™m trying to decide between NVIDIA DGX Spark and a MacBook Pro with M4 Max (128GB RAM), mainly for running local LLMs.\n\nMy primary use case is coding â€” I want to use local models as a replacement (or strong alternative) to Claude Code and other cloud-based coding assistants. Typical tasks would include:\n- Code completion\n- Refactoring\n- Understanding and navigating large codebases\n- General coding Q&A / problem-solving\n\nSecondary (nice-to-have) use cases, mostly for learning and experimentation:\n- Speech-to-Text / Text-to-Speech\n- Image-to-Video / Text-to-Video\n- Other multimodal or generative AI experiments\n\nI understand these two machines are very different in philosophy:\n- DGX Spark: CUDA ecosystem, stronger raw GPU compute, more â€œproperâ€ AI workstationâ€“style setup\n- MacBook Pro (M4 Max): unified memory, portability, strong Metal performance, Apple ML stack (MLX / CoreML)\n\nWhat Iâ€™m trying to understand from people with hands-on experience:\n- For local LLM inference focused on coding, which one makes more sense day-to-day?\n- How much does VRAM vs unified memory matter in real-world local LLM usage?\n- Is the Apple Silicon ecosystem mature enough now to realistically replace something like Claude Code?\n- Any gotchas around model support, tooling, latency, or developer workflow?\n\nIâ€™m not focused on training large models â€” this is mainly about fast, reliable local inference that can realistically support daily coding work.\n\nWould really appreciate insights from anyone who has used either (or both).",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzjr5g0",
          "author": "g_rich",
          "text": "If youâ€™re not doing any training or fine tuning and just running inference then the higher memory bandwidth of the Mac will be a benefit. \n\nHowever set your expectations accordingly, regardless of how much hardware you get youâ€™re not going to get Claude level performance and with only 128MB of unified memory youâ€™re going to be a far ways off. If you want Claude level performance then youâ€™re better off paying for Claude because you can easily spend upwards of $10k plus and still never hit what Claude or ChatGPT or Gemini deliver. \n\nBenchmarks for the M5 show a significant increase in performance for LLMâ€™s over the base M4 and there are some rumors of new MacBook Proâ€™s being released at the end of the month so you might want to wait a few weeks and see if Apple introduces MacBooks with the M5 Pro and Max.",
          "score": 8,
          "created_utc": "2026-01-14 14:59:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o001bd1",
              "author": "GCoderDCoder",
              "text": "Agreed! As much as I love my local hosted models, the goal with local is privacy, wanting to learn from the ground up, and shielding against market uncertainty, not getting the best inference experience. I have a 256gb mac studio and I get very good local inference but I am spending a lot of time trying to configure local systems to manage context/ memory better. \n\nThe model outputs like glm 4.7 are great for me at q4 (I argue near if not at frontier level) but models at that level start around 20t/s for me and slow down as context increases. Plus I can only manage so much context to run locally on my mac studio so even if speed weren't an issue memory becomes an issue. So systems breaking things into smaller tasks help keep the models faster but introduce complexity too.\n\nI use claude in cursor for work and these aren't concerns. I just use the model and get top tier performance with no worries. I have a 128gb strix halo that gets similar speeds to the spark. I use that as a side gpt-oss120b for agentic tasks with moderate to low logic requirements for tasks other than coding. That's like gemini flash 2.5 doing agentic tasks for you not the model that we worry about replacing us lol",
              "score": 1,
              "created_utc": "2026-01-16 21:53:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzj7zw1",
          "author": "jba1224a",
          "text": "The m4 max has significantly higher memory bandwidth (almost double) than the spark.  The spark also works on a unified memory model.\n\nIf weâ€™re talking strictly inference the MacBook is going to going to be much faster - the caveat being the spark will have better support given most frameworks run well on nvidia as itâ€™s cuda.\n\nDonâ€™t sleep on mlx though, itâ€™s getting very good.\n\nIf you donâ€™t need a laptop format you may want to consider a Mac Studio m4 max with the same configuration which would be -considerably- cheaper.",
          "score": 7,
          "created_utc": "2026-01-14 13:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjbuqd",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-14 13:38:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjmias",
                  "author": "belgradGoat",
                  "text": "What the issue with mlx? I run mlx every day no issues at all, itâ€™s fast it loads quickly",
                  "score": 1,
                  "created_utc": "2026-01-14 14:35:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzjxto0",
              "author": "StardockEngineer",
              "text": "Thai is not true.  The DGXâ€™s Prefill speeds crush the Mac.  The Spark will frequently finish interesting before the Macâ€™s even output a single token.",
              "score": 2,
              "created_utc": "2026-01-14 15:31:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzk0mtm",
                  "author": "jba1224a",
                  "text": "Iâ€™ve used both for local inference and I did not find that to be true.\n\nBut I do agree that if youâ€™re using large complex prompts it is a factor",
                  "score": 3,
                  "created_utc": "2026-01-14 15:44:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlgei2",
          "author": "HealthyCommunicat",
          "text": "If youâ€™re going for PURE text generation, there simply is nothing that beats the m3 ultra mac studio. Ask ANYONE who says that the gb10 is better to show it and do a direct comparison. The nvidia people like to say that the mac people are just exaggerating the numbers on paper - thats not true at all. Theyâ€™re completely choosing to ignore the fact that on paper, nvidia is supposed to have much much better capability - and thats perfectly true, the spark can do many many things that would take the m3 ultra many times over to do the same exact thing such as fine tuning or image/video gen - but if your goal is to do pure pure agentic inference, there simply is nothing that beats the m3 ultra. Its not a matter of opinion or whatever, its something that can be proved literally every single time. \n\nIf your goal is to have the llm read as many documents as possible and organize them or whatever, the spark wins by a massive amount but fails completely on actual text generation.\n\nif u find someone pushing you on the fact that the dgx spark is better than the m3 ultra for agentic coding workflows, tell them to prove it, im willing to load up a model and run tests to show it.\n\nnvidia fan boys will say \"the m3 ultra is only better on paper and not in actuality\" - when thats literally the complete opposite. the spark has much more extensive capabilities in terms of actual numbers in hardware. the m3 ultra is lacking extensive things that the dgx spark has. yet llm's run better on the mac studio m3 ultra. people are literally mad that there nvidia toybox isnt able to do as well as apple, completely forgetting that the nvidia tax is a real thing that even massive megacorporations admit to, why cant people accept that their paying extremely high for a machine that simply does not perform as good as the m3 ultra? how much longer are people going to keep arguing with me about this but not one single person is willing to actually put it to the test when i ask?\n\nagain, if anyone is so confident that the dgx spark is able to beat the m3 mac studio in terms of speed for pure text generation, im willing to bet $100 into an escrow of ur choice if you're willing to match just to  prove you wrong. its a free $100 for you right?",
          "score": 4,
          "created_utc": "2026-01-14 19:37:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmmrmj",
              "author": "Responsible_Room_706",
              "text": "So, TLDR Is that neither are a silver bullet and given the use case it may make sense to have both?",
              "score": 1,
              "created_utc": "2026-01-14 22:52:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmn56p",
                  "author": "HealthyCommunicat",
                  "text": "kinda? like u really really only need the dgx spark if ur planning on focusing hardcore in video and image - cuz the m3 ultra can still do that very very well, like the m chips are purely about that shit, just ends up nvidia is better.\n\nif text generation is any part of your needs, this is the difference between being able to use glm 4.7 and not being able to. it doesnt matter how fast prompt processing is if its not even usuable if the prompt itself is short.\n\nexample, even if u gave both spark and mac studio \"hi, check the weather today\" using glm 4.7 - the m3 mac studio will beat the spark by a fuck ton, and saying that is an understatement. any model that is over 30b parameters, even 70b and not moe are too slow and literally literally just not usuable unless ur like using it for pure writing stories or some crap\n\n8. Conclusion\n\nThe choice between MLX (Mac Studio) and DGX Spark for agentic coding is ultimately a choice betweenÂ **Bandwidth**Â andÂ **Intelligence**.\n\n**MLX**Â offers the raw horsepower to generate text rapidly, making it the superior \"Typewriter.\" However, its current software limitations regarding cache reuse in branching scenarios make it a forgetful writerâ€”one that must constantly re-read the entire book to write an alternative ending.\n\n**DGX Spark**Â offers the architectural sophistication of a datacenter. It is a slower \"Typewriter\" due to memory bandwidth limits, but possessed of a photographic memory for context. In complex, non-linear agentic workflows where \"thinking\" (branching, backtracking, and planning) dominates \"typing\" (linear generation), the DGX Spark's ability to reuse the KV cache via TensorRT-LLM makes it the more performant and robust platform.\n\nFor the immediate future (2026), until MLX natively incorporates Radix Tree caching and PagedAttention primitives, the NVIDIA ecosystem remains the pragmatic choice for seriousÂ *autonomous*Â agent development, while the Mac Studio remains the king ofÂ *interactive*Â local coding assistance.",
                  "score": 4,
                  "created_utc": "2026-01-14 22:54:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmsckv",
                  "author": "HealthyCommunicat",
                  "text": "comes down to\n\ndo u want to be able to have a tiny bit higher intelligence than the m3 ultra and still be at speeds where u can literally read 3-5x times faster, or have just a itty bitty bit less intelligence and have a machine that can generate text much faster than u can even read? i realize i shouldve said it like this in the first place for others wondering the same.\n\n  \nits not the same thing, its not even close. it literally is a matter of something being usuable or not.",
                  "score": 1,
                  "created_utc": "2026-01-14 23:21:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjpaa3",
          "author": "Mean-Sprinkles3157",
          "text": "Dgx spark is small like a brick, energy efficient lower than 100w, mostly 10w idle. I setup one at home, and purchased a used dell latitude 5510, with 32gb ram that I can carry any where. It has no issues with cursor , vs code plus multitabs of Firefox.\nI like the fact that Dgx is extensible.right now is 1, I could connect more. The concern is the bandwidth. I was contemplating to return my current dgx, replace with  the gb10 oem like the Asus one. What do you think? The current dgx cost me 6000 CAD, while the assent is 4200 CAD for 1tb, the difference is one good MacBook Pro.",
          "score": 2,
          "created_utc": "2026-01-14 14:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjrd5a",
          "author": "CMPUTX486",
          "text": "I got the DGX because I want to use cuda and lazy to fix library issue.",
          "score": 2,
          "created_utc": "2026-01-14 15:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjv0tw",
          "author": "pieonmyjesutildomine",
          "text": "If it's \"mainly for running LLMs,\" get the Mac. I have both and that's the only thing I use the Mac for, the spark is a CUDA dev platform.",
          "score": 2,
          "created_utc": "2026-01-14 15:18:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmh57c",
              "author": "Soggy-Leadership-324",
              "text": "if i want to use comfyui for gen video or image, using such as wan or ltx-2, is it spark will be better?",
              "score": 1,
              "created_utc": "2026-01-14 22:24:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzon67k",
                  "author": "pieonmyjesutildomine",
                  "text": "Miles better, yeah. The Spark has better and faster image processing than Macs or even the Strix Halo.",
                  "score": 3,
                  "created_utc": "2026-01-15 06:12:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlh84t",
          "author": "HealthyCommunicat",
          "text": "Let me put this simply. If there is a dgx spark next to a m3 ultra and they both loaded qwen 3 next 80b and gave them the exact same prompt, the m3 ultra will finish first every. single. time. The prompt processing speed of the spark cannot mathematically, logically, PHYSICALLY outbeat the mac studio.",
          "score": 2,
          "created_utc": "2026-01-14 19:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznxhvr",
              "author": "johnkapolos",
              "text": "Doesn't the spark have faster prompt pre-processing about 4x (and about that much less speed in generation)?\n\nOP's use case has huge context size compared to the generation size.",
              "score": 1,
              "created_utc": "2026-01-15 03:13:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzny7gx",
                  "author": "HealthyCommunicat",
                  "text": "yes it does. maybe even more. the token gen just simply makes up that much. its just math. thats the level of insane drastic difference, the token generation on the dgx spark is that bad that even with poor prompt processing the m3 mac studio can still do speeds the dgx spark cant.\n\nyou can go read my other comments about this because i get pretty dam deep into this. im tired of people saying that this myth when its proveable wrong.\n\nwhats the point of a good reader who cant even write?\n\na writer who cant even read can still produce something. i know this is a exaggeration but its as simple as that.\n\nif u know anyone who has a dgx spark willing to do actual benchmarks side by side with a model people actually want to use, let me know. i'd be willing to put money into an escrow over this simply just because i got to test the difference myself before i returned my spark and even have videos. the prompt processing is insanely better on the dgx spark, but the memory bw just trumps all.\n\nCalculating the Performance Crossover\n\nI am currently synthesizing the performance metrics to determine the exact point where superior text-writing speed overcomes a slower start in data ingestion. For a typical coding task involving a 10,000-token context, my calculations show that while a high-compute system might start working five seconds faster, the more bandwidth-heavy system completes the generation phase significantly earlier. In multi-step agentic workflows where a model might output code across 50 iterations, the cumulative time saved during the 'typing' phase quickly renders any initial ingestion advantage irrelevant.\n\nCalculating the Total Timeline\n\nI am synthesizing a total execution model that adjudicates between starting fast and finishing fast. My findings indicate that in a typical programming loop involving 40 to 60 turns, the cumulative time saved during the actual writing of code blocks is the decisive factor. While a competitor might save five seconds on an initial file read, a system with triple the data throughput saves multiple seconds on every single response it generates. Over the course of a standard coding task, this generation advantage accumulates into a lead that no amount of initial processing speed can overcome.\n\nThe Ingestion vs. Generation Math\n\nI have been mathematically deconstructing the performance intersection when an agent reads new files mid-task. My calculations show that even when an agent pulls in a fresh 1,000-token block of code, a high-bandwidth system only needs to generate about 55 tokens of output to reach the finish line first. Given that professional coding agents frequently generate hundreds of tokens of refactored logic or debugging logs per turn, the 'typing' speed of the memory pipeline remains the definitive bottleneck for the total session time.",
                  "score": 1,
                  "created_utc": "2026-01-15 03:17:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzmecpp",
          "author": "HealthyCommunicat",
          "text": "by the way, mlx-lm allows for extreme kv cache optimization - im getting above 2.5k prompt procesing on the m3 ultra mac studio on minimax m2.1 4bit when focusing mainly on prompt procesing and not token gen, cuz when this does happen the token gen goes down from 40 to 15-20 (still very very usable) this isnt the case all the time and very specific - but with literally an hr or two of tweaking im able to use minimax m2.1 4bit so dam comfortably.\n\npeople will try to say im lying, and if u just dm me i'd be more than willing to screenshare",
          "score": 2,
          "created_utc": "2026-01-14 22:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoadzt",
              "author": "TheRiddler79",
              "text": "You get 20/sec on MiniMax-M2.1? That has got to feel nice.\n\nMinimax is like the sleeper of the truly brilliant reasoning models.\n\nI get about 4.5,but strictly ram no gpu.  I'm on zen 2 technology nothing super modern.",
              "score": 2,
              "created_utc": "2026-01-15 04:36:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzmghw9",
              "author": "Soggy-Leadership-324",
              "text": "how many ram of ur m3 ultra? thx",
              "score": 1,
              "created_utc": "2026-01-14 22:21:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmha8v",
                  "author": "HealthyCommunicat",
                  "text": "256 gb. got it for $5500 usd. there just is nothing else that will beat this when it comes to using llm's for agentic coding for this price. again, if someone tries to argue that the spark is better, ask them to show it cuz im willing to screenshare any fucking day to put an end to this bs argument.",
                  "score": 1,
                  "created_utc": "2026-01-14 22:25:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjg9a4",
          "author": "sn2006gy",
          "text": "The cost benefit isn't there... all of this hardware is crazy slow and frustrating in comparison to a Claude or Cursor.\n\nMaddingly slow if it's your day job.",
          "score": 2,
          "created_utc": "2026-01-14 14:02:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjjtoa",
              "author": "nyssaqt",
              "text": "Is it?  I was considering buying a Mac Studio 256gb in order to test local LLMs for coding.",
              "score": 1,
              "created_utc": "2026-01-14 14:21:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjpe5a",
                  "author": "sn2006gy",
                  "text": "Is coding your day job? If so, I'd just pay for a service and save the sanity.\n\nIf you just like to tinker, go for it. \n\nI work for an OSS company so I just pay for Cursor and even if I have to upgrade the plan for more tokens, it's cheaper than buying and maintaining my own localllm for something that is going on github anyway. \n\nInversely, I thought a lot about privacy of localllm for development but then it smacked me, if my code is only as strong as the model and the prompt then anyone with a model and a prompt can clone my idea so my proprietary ideas are only as good as the models everyone can run so I may as well just code using Claude/Cursor.\n\nI put a lot more effort into pulling my data off the internet, deleting/obfuscating social media posts for privacy so my likeness and data won't train a model after kind of realizing my concerns for privacy around code were kind of moot.\n\nthe value is higher up the chain in actually being able to run the service, run the code, make money from it - and perhaps a local llm to help you achieve that is better than one to code it but then again, the larger the model the more you can press it for nuance/angles that aren't already being done...\n\nwhich then just reminds everyone we're in a rush to the bottom :D",
                  "score": 5,
                  "created_utc": "2026-01-14 14:50:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzlhmay",
                  "author": "HealthyCommunicat",
                  "text": "Dont listen to this guy, i work in a datacenter 8+ hrs a day and i use the m3 mac studio to run glm 4.7 4bit (25token/s max) and minimax m2.1 @4bit and am getting 40-50token/s and 500+ token/s pp (slower end compared to nvidia but a spark wont even get close to 40-50 token/s) - it has completely replaced claude for me. Just less than an hr ago i was working in cline and had compacted multiple times and token gen never dropped below 25-30 token/s - still very very usable.",
                  "score": 0,
                  "created_utc": "2026-01-14 19:42:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzjumvl",
              "author": "LoonSecIO",
              "text": "You donâ€™t get an email you are out of tokens till next month :shrug:",
              "score": 1,
              "created_utc": "2026-01-14 15:16:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjuxg8",
                  "author": "sn2006gy",
                  "text": "clicking the upgrade button isn't that hard and if your job depends on your coding it's still cheaper.",
                  "score": 3,
                  "created_utc": "2026-01-14 15:17:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjyoqo",
          "author": "StardockEngineer",
          "text": "These Mac people got it backwards.  Btw, Iâ€™m a Mac fan who has bought 5 M-series Macâ€™s since M1 and own two currently, with plans to buy an M5 Max. \n\nBut the DGX Spark crushes the Macs.  There are two parts of inferencing, prefill and token generation.  The DGX is much faster on prefill.  It will often finish  prefill and inferencing before the Macs even begin their first token.  \n\nExoâ€™s own charts say this https://blog.exolabs.net/nvidia-dgx-spark/\n\nAlso see this https://youtu.be/IUSx8Vuo-pQ\n\nIt is a constant, misinformed theme in this sub to ignore prefill.\n\nAlso, are you interesting in making images, videos, fine tuning or anything else AI can do?  Get a DGX. Itâ€™ll be much faster.",
          "score": 2,
          "created_utc": "2026-01-14 15:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlfn1f",
              "author": "HealthyCommunicat",
              "text": "The prefill matters little when ur actually using the llmâ€™s agentically and the dgx spark forces you to wait 1 second to read the prompt and then 30 minutes to type out the exact same sentence that would take the m3 studio maybe 1 minute to read and 5 mins to write - this is just an exaggerated example but in the end if you do the math, as context starts to grow, the m3 ultra just will always win for pure inference. Donâ€™t want to believe me? Dm me and we can do some tests, and not just with gpt oss 120b but an actual real model that we would want to use.\n\n\nAre people really trying to say that the prompt processing is THAT MUCH INSANELY faster that the token gen speed outbeats the m3 ultra? Like yes the spark has much faster prompt processing but even if the text generation starts much earlier, that text generation will end much after the m3 ultra has already done prompt processing and fully generated. Is it not simple logic? Are you really really trying to say that if we put a dgx spark and a m3 ultra side by side and gave the same prmpts with the same inferencing engine and actual model to use for real coding, that the spark would win?",
              "score": 3,
              "created_utc": "2026-01-14 19:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlidzl",
                  "author": "StardockEngineer",
                  "text": "I'm not trying to say it, I am saying it.  Did you even look at the links?  At just 8k context, the Spark starts and finishes before the Mac even begins producing.  How about when it's 15k from OpenCode or 30k+ from Claude Code?  Or when agents are reading files in the middle of an agent flow?  It doesn't  get better.  \n\nThere is almost always more input tokens than output, that's why LLM providers price them differently.  \n\nI linked to a video in the post you replied to that literally put the two side-by-side, and the Mac did not win.   26 minutes vs 6.\n\nAm the only one who notices when the Mac's \"win\" it's when they completely ignore or quickly gloss over the prefill problem?  Literally watch all the 4xUlras RDMA videos.",
                  "score": 0,
                  "created_utc": "2026-01-14 19:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzk0ws0",
          "author": "paulirotta",
          "text": "Compare the best type of models of interest to you that run best on each. nvidea releases fp4 models that run 2x faster than q4 on Spark machines. Despite the lower bandwidth, that might be the difference.",
          "score": 1,
          "created_utc": "2026-01-14 15:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznw7a6",
          "author": "SlippySausageSlapper",
          "text": "In order to do something equivalent to what claude does, you would need a B200 or equivalent, which costs half a million dollars and uses 14,000 watts of power continuously during inference.\n\nYou might want to temper your expectations.",
          "score": 1,
          "created_utc": "2026-01-15 03:05:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzo9m7s",
              "author": "TheRiddler79",
              "text": "Nah, I can do coding \"equivalent\" to Claude running code on my threadripper straight from ram.\n\nNow, can I deliver 100 tokens a second to 1,000 different people simultaneously? No, but can I get 10 tokens a second for myself personally, yes. At the end of the day, I can do that 24 hours a day for free. In tandem with anything else, which means if I needed something faster I would just simply have Claude do it or Gemini.\n\nUnless people intend on selling compute, or sharing it with a bunch of other users, the need for massive tokens per second can be mitigated by simply using a smart plan and your current other tools when necessary",
              "score": 1,
              "created_utc": "2026-01-15 04:31:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzob4dq",
                  "author": "SlippySausageSlapper",
                  "text": "Youâ€™re not doing that with less than 2tb of ram and you wonâ€™t be hitting 10tps without quantizing it into incoherence running on CPU.\n\nRunning claude-level models at home is not practical unless you have hundreds of thousands to spend on a computer, which is nuts because you can get a LOT of claude usage from Anthropic for 200/mo.",
                  "score": 2,
                  "created_utc": "2026-01-15 04:41:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzrilrv",
          "author": "pistonsoffury",
          "text": "You're cross-shopping the wrong products - you should be doing Mac Studio vs. Spark. If portability is a concern, then the obvious choice is the Macbook pro.",
          "score": 1,
          "created_utc": "2026-01-15 17:34:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuuyg3",
          "author": "rditorx",
          "text": "Remember that M5 Pro and M5 Max MacBook Pros are supposed to arrive soon which will probably bring some speedup regarding memory bandwidth and neural acceleration. Even if you're not buying them, M4 MacBook Pros could get cheaper, though a build-to-order / configure-to-order 128 GB M4 Max may become impossible to order once the M5 Pro/Max come out (leaving only left-over stocked items).",
          "score": 1,
          "created_utc": "2026-01-16 03:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmhorz",
          "author": "HealthyCommunicat",
          "text": "\"3.3 The \"Cross-Over\" Efficiency Point\n\nResearch by EXO Labs introduces a critical concept: the cross-over point where one machine's advantage cancels out the other's.\n\n* Because the Spark starts faster (low TTFT) but runs slower (low TPS), and the Mac starts slower (high TTFT) but runs faster (high TPS), there is a specific prompt-to-generation ratio where the total time is identical.\n* **Short Prompt / Long Response:**Â The Mac wins easily. The initial 2-second lag is instantly recovered by the blazing fast generation.\n* **Long Prompt / Short Response:**Â The Spark wins. If the task is \"Read this book and say 'Yes' or 'No',\" the Mac spends 20 seconds reading and 0.1 seconds answering. The Spark spends 5 seconds reading and 0.2 seconds answering.\n\n**Benchmark Synthesis:**Â The user's intuition that \"token generation speed will make up for the lack in prompt processing\" is mathematically correct for arguably 90% of interactive use cases (chat, coding, writing). The \"Prompt Processing\" advantage of the Spark is a specialized victory relevant primarily for massive RAG ingestion or batch processing, not the fluidity of conversation.Â Â Â \n\n\"\n\n**Final Verdict:**\n\n* **DGX Spark is Faster When:**Â You are processing massive documents (long prompts), running batch classification jobs, fine-tuning models, or require native CUDA support for training.\n* **Mac Studio M3 Ultra is Faster When:**Â You are chatting, coding, writing, or interacting with the AI in real-time. The generation is smoother, the capacity (up to 512GB) is higher, and the latency per token is significantly lower.\n\nso yeah the mac studio wins for when ur trying to use an actual llm. u dont have to believe me, u can just go ask multiple spark and m3 mac studio owners to prove it. \n\n  \ndont even get me started on mlx kv cache reusing cuz at that point it just starts getting sad for the spark",
          "score": 1,
          "created_utc": "2026-01-14 22:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmyltx",
              "author": "StardockEngineer",
              "text": "You got cut off there. Lol did your max tokens run out?  Dude Iâ€™m arguing with a bot.  My bad.",
              "score": 3,
              "created_utc": "2026-01-14 23:55:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjeo32",
          "author": "mxforest",
          "text": "M4 Max MBP runs way too hot and loud. I also bought it for coding but gave up. Now i have Claude Code 5x plan which is much better. Still use it for playing around with interesting models but M4 silicon prompt processing is way too slow for anything serious. Would not recommend. Wait for M5 max or setup a separate machine with something like 4x used 3090 if you really want local. Use it with a Macbook Air.",
          "score": -1,
          "created_utc": "2026-01-14 13:53:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzli0qs",
              "author": "HealthyCommunicat",
              "text": "M4 max macbook pro is indeed way too loud for fans to use for llmâ€™s. The mac studio however is silent asf\n\nits still much quieter and much much much more capable than any gpu laptop though\n\ni got the m4 max 128 laptop knowing that i'd be able to turn it into a small cluster with my other macs as long as u have tb5",
              "score": 2,
              "created_utc": "2026-01-14 19:44:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qe3yzy",
      "title": "Help finding best LLM to improve productivity as a manager",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "author": "2C104",
      "created_utc": "2026-01-16 02:42:07",
      "score": 14,
      "num_comments": 13,
      "upvote_ratio": 0.83,
      "text": "Like the title says, I am not in need for the LLM to code anything, I'm essentially looking for a tool that will support my managerial work. \n\nI want to be able to feed it text descriptions of the projects I am working on and get help categorizing, coordinating, summarizing, preparing for presentations, use it as a tool for bouncing ideas off of, suggestions for improving email communication, tips to improve my management productivity and abilities, etc... \n\nI want to do this offline because although ChatGPT is very helpful in this regard, I don't want sensitive work content to be shared online. \n\nMy rig has the following:\n\nIntel(R) Core(TM) Ultra 9 275HX (2.70 GHz)  \n32.0 GB RAM  \nNvidia 5070 Ti (Laptop GPU) w/ 12gb RAM  \n2tb SSDs\n\n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzv1jrr",
          "author": "iMrParker",
          "text": "Honestly that smaller Gemma models would be good for you. Gemma 12b might fit and it has vision. GLM has 9b models which are excellent (4.6 flash I think?). If you had a desktop 5070 ti you'd be able to run GPT OSS 20b which is very solid for this purpose.Â \n\n\nThe best advice would be to download LM Studio, use the model search, and play around with an assortment of models till you find one that fits your needs",
          "score": 6,
          "created_utc": "2026-01-16 04:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv5ynf",
              "author": "2C104",
              "text": "Thank you I'll check it out",
              "score": 2,
              "created_utc": "2026-01-16 04:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvt7k7",
          "author": "techlatest_net",
          "text": "Hey man, for managerial stuff like summarizing projects, tweaking emails, or brainstorming without sending sensitive info to the cloud, your rig is perfectâ€”Ollama is dead simple to set up and flies on that 5070 Ti with 12GB VRAM.â€‹\n\nGrab Ollama (ollama.com), install in like 2 mins, then pull mistral-nemo:12b Q4 or qwen2.5:7b-instruct Q5â€”both fit comfy quantized and give solid, non-hallucinating responses for exactly what you described. Nemo edges out Llama 3.1 8B on reasoning without being too wordy, and folks use it daily for email drafts and task categorization just like you want.â€‹\n\nPaste project text, say \"summarize this for a team mtg, suggest 3 action items\" or \"rewrite this email to sound more collaborative\"â€”hits 20-40 t/s easy. Bonus: there's even a quick Python script floating around using Llama3 via Ollama for auto-email summaries if you wanna script it later. Way better privacy than ChatGPT, zero cost after download.",
          "score": 4,
          "created_utc": "2026-01-16 07:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx1xqf",
              "author": "2C104",
              "text": "Can anyone explain why this comment is getting downvoted? It seems the most eloquent of the responses and since they all seem to be equally helpful I'm just trying to understand why people would disagree with what was stated here.",
              "score": 2,
              "created_utc": "2026-01-16 13:38:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwpw1m",
          "author": "chief-of-wow",
          "text": "Although I don't know how offline AI works, I have a couple of tips to share about online AI:\n\n1. For Presentations, Gamma AI is really good. I recently tested it out and it has so many cool features. The one that stood out to me was how easy it was to choose diff types of graphic elements within the tool. For the slides, I gave my notes to Claude and asked it clean them up and structure them as slides. It did so and then I fed that into Gamma to build a presentation for me.\n\n  \n2. A productivity tip: I use voice dictation a lot. When I have lots of thoughts and context, I use dictation to feed AI. Be it drafting a long (and important) email, creating an SOP, or similar comms or documentation work, this saves me a lot of time. Also, I don't know the reason, but the results are so much better than when I type instructions. I am a decent writer so proofreading and editing doesn't take me long. Overall, this process works well for me so I definitely recommend testing it.",
          "score": 1,
          "created_utc": "2026-01-16 12:24:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxbau4",
          "author": "nofilmincamera",
          "text": "Having access to local LLM has made me a extremely effective.  That said its a right tool for right job.\n\nUsing it for language re writes? Sure. But you really need one of the bigger models simply just for deep research. \n\nAlso,  understanding tools and python will get you pretty far. A local installed took line N8N makes it great to automate.  LLMs are non deterministic. Which means by themselves are terrible for consistently.  In any use you come up with, it really helps that you break that task apart.  What of this can be automated without a LLM at all? A smaller model can do a lot as a validator, and where non deterministic is a value.",
          "score": 1,
          "created_utc": "2026-01-16 14:26:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kllba",
          "author": "Unique-Temperature17",
          "text": "Your hardware is solid for local LLMs. I'd recommend Qwen 3 4B or Gemma 3 4B - both support 32K context so you can feed in longer project docs, and they'll run fast on your 5070 Ti. For software, Ollama or LM Studio are the easiest to get started with. If you want to chat with documents directly, check out Suverenum.",
          "score": 1,
          "created_utc": "2026-01-19 23:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwfmcb",
          "author": "fasti-au",
          "text": "Nemotron 30b if local with websearch and some memory files is very good for most things.  Technical writing Iâ€™d go phi4.  \n\nBig leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nGrok is the most cutthroat, a win is a win damn the alternatives sort model.  \n\nIt backs itself and is very much great at winning if it finds a trick but itâ€™ll bend the rules and go around problems in some ways so I would be wary of long memory and brainstorming without checks",
          "score": 0,
          "created_utc": "2026-01-16 11:06:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx2cm0",
              "author": "2C104",
              "text": ">Big leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nI have no idea what this is trying to say, it seems incoherent to me. Are these the names of LocalLLMs? Or were you just using shorthand?",
              "score": 1,
              "created_utc": "2026-01-16 13:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o011jom",
                  "author": "number66-1",
                  "text": "I couldn't understand him either, but he doesn't need to waste his time explaining himself since if you are using AI and really care to understand what he said you could ask AI to help you understand. \n\nHe's done his part, he shared his knowledgeable opinion.\n\n\"Chatgpt: Hereâ€™s what that commenter is saying, rewritten in normal English, with the shorthand decoded.\n\n1) Clean rewrite of their comment\n\nIf you want a local model and you can give it web search + a small â€œmemoryâ€ of your files, Nemotron 30B is good for most tasks.\nFor technical writing, theyâ€™d pick Phi-4.\n\nFor â€œbig leaguesâ€ (top-tier general chat), GPT is the best standalone option.\nBut if youâ€™re using Claude with MCP, then Claude is better / less annoying in their opinion.\n\nGrok is the most cutthroat: it cares about â€œwinningâ€ and will pick an answer aggressively.\nItâ€™s very confident and can be great when it finds a clever shortcut, but it may â€œbend the rulesâ€ or take weird detours, so they wouldnâ€™t trust it for long-term memory or brainstorming unless you verify everything.\n\n\nThatâ€™s the whole meaning.\n\n2) What each name likely refers to\n\nNemotron 30B: a local LLM family/model (30B = ~30 billion parameters). They mean â€œrun it on your own machineâ€ (or your own server).\n\nPhi-4: Microsoftâ€™s Phi family model; theyâ€™re saying itâ€™s strong for structured/technical writing.\n\nGPT: OpenAIâ€™s models (ChatGPT). They mean â€œbest general-purpose chat without needing extra tool wiring.â€\n\nClaude: Anthropicâ€™s models.\n\nGrok: xAIâ€™s model.\n\n\n3) What the jargon means\n\nâ€œLocalâ€: you run the model yourself (privacy, control, but you manage setup + performance).\n\nâ€œWebsearchâ€: hooking the model to a browsing/search tool so it can fetch current info instead of guessing.\n\nâ€œMemory filesâ€: usually means a RAG setup (retrieval-augmented generation): your notes/docs are indexed and the model can pull relevant chunks while answering.\n\nâ€œBig leaguesâ€: basically â€œbest-of-the-best overall.â€\n\nâ€œStandaloneâ€: not using tool integrations, agents, RAG, custom workflowsâ€”just the model as-is.\n\nâ€œClaude MCPâ€: almost certainly referring to the Model Context Protocol (MCP)â€”a way to connect Claude to external tools/services (files, apps, search, automations) in a standardized way.\n\nâ€œCutthroat / win is a winâ€: the model will prioritize getting some answer that seems successful, even if itâ€™s not cautious or perfectly aligned with your intent/rules.\n\nâ€œBend the rules / go around problemsâ€: meaning it may produce confident outputs, shortcuts, or â€œcreativeâ€ leaps that sound right but arenâ€™t reliably grounded.\n\nâ€œWary of long memory and brainstorming without checksâ€: donâ€™t let it build long chains of assumptions over time; verify with sources, logs, or constraints.\n\n\n4) Why it reads incoherent\n\nItâ€™s basically:\n\nmissing punctuation,\n\ntypos (â€œbut u f you Claude mcpâ€¦â€ = â€œbut if you use Claude MCPâ€¦â€),\n\nand it jumps between local model advice and hosted model opinions without transitions.\n\n\nIf you tell me what you were trying to decide (local vs cloud, privacy, work tasks, hardware), I can translate their advice into a concrete recommendation that fits your situation.\"",
                  "score": 1,
                  "created_utc": "2026-01-17 01:11:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwpevx",
          "author": "beauzero",
          "text": "Just use claude cowork.",
          "score": 0,
          "created_utc": "2026-01-16 12:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx80vu",
          "author": "Nx3xO",
          "text": "If you want multiple options set yourself up on a jetson orin 16/32gb. Openwebui. Easy to setup guard rails, multiple llms ready to load. You can airgap for extra security. Low power too. Your laptop is fully capable but this option b could be a much better and flexible option. \n\nI have an 8gb, about 8 models. Focused on math medical and general. Simple small models. I can access it on the wire or jump onto it via wifi ap I setup. Technically I could throw it on a battery and and tap into on the go, airport/traveling. Add extra storage and its a portable backup solution.",
          "score": 0,
          "created_utc": "2026-01-16 14:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxgc90",
              "author": "mauricespotgieter",
              "text": "Hi Nx3xO.\nWould you mind sharing details of your n8 setup?\nI am looking to do something similar and would be grateful for any assistance. Still finding my legs and learning. Thanks in advance. Open to DM directly if that is more appropriate?",
              "score": 1,
              "created_utc": "2026-01-16 14:51:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgzdqo",
      "title": "opencode with superpowers. It can do everything in a container with docker and nix",
      "subreddit": "LocalLLM",
      "url": "https://grigio.org/opencode-with-superpowers-it-can-do-everything-in-a-container-with-docker-and-nix/",
      "author": "Deep_Traffic_7873",
      "created_utc": "2026-01-19 09:09:36",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qgzdqo/opencode_with_superpowers_it_can_do_everything_in/",
      "domain": "grigio.org",
      "is_self": false,
      "comments": [
        {
          "id": "o0ge5py",
          "author": "WolfeheartGames",
          "text": "This is such a cool idea.",
          "score": 2,
          "created_utc": "2026-01-19 10:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0intvs",
          "author": "c4rb0nX1",
          "text": "will try this for sure.",
          "score": 1,
          "created_utc": "2026-01-19 18:12:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfuvg5",
      "title": "Dual RTX 5060 ti 16gb's with 96GB of DDR5 5600 mhz, what is everyone else running?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qfuvg5/dual_rtx_5060_ti_16gbs_with_96gb_of_ddr5_5600_mhz/",
      "author": "CollectionOk2393",
      "created_utc": "2026-01-18 01:20:59",
      "score": 13,
      "num_comments": 16,
      "upvote_ratio": 0.89,
      "text": "I have a minisforum n5 nas with 96gb of ddr5 5600 ram and dual rtx 5060 ti's.\n\n32gb of vram and 96gb of ram\n\n  \nWhat are the best local llm models to run? Also are there any image or video gen tools that work well on dual gpus? \n\n  \nI've just built this rig and im looking to get into AI to do some side work and try to make a few bucks with my free time. Or just learn so I dont fall behind at work. Im a data center engineer for a tire 4 data center. I see what they are buying and im just trying to stay relevant I guess. Lol. Any suggestions or tips or tricks on what software or models or whatever would be appreciated!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfuvg5/dual_rtx_5060_ti_16gbs_with_96gb_of_ddr5_5600_mhz/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o080gzk",
          "author": "EmPips",
          "text": "I wanted to balance getting it as cheap as possible without needing to introduce anything that wouldn't work nicely in my case or need external cooling.\n\nThis resulted in:\n\nRx 6800 + w6800 Pro + 64GB RAM ..but the RAM is DDR4 dual channel :(\n\nGLM 4.6v is the best model I can run. Q4 gets ~17.5 tokens/second with modest context (12k) for one-off chats and ~12 tokens/second with larger context (>40k) for things like coding.\n\nQwen3-Next-80B gets 35 tokens/second",
          "score": 7,
          "created_utc": "2026-01-18 02:37:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0813tv",
              "author": "CollectionOk2393",
              "text": "100 tokens per second = qwen3-vl-30b-a3b-thinking-1m_moe\n\n\n25 tokens per second = google/gemma-3-27b\n\n\n100 tokens per second = qwen/qwen3-vl-30b\n\n\n100 tokens per second = nvidia/nemotron-3-nano\n\n\nI have noticed on the MoE modles I am crushing it at over 100 tokens per second, but on the \"dense\" modles I am only getting like 25 tokens per second. Is this normal?",
              "score": 3,
              "created_utc": "2026-01-18 02:40:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o083lu9",
                  "author": "EmPips",
                  "text": "Can you include the levels of quantization?\n\nBut yes that's very normal. Your GPU needs to search through 27 Billion parameters for **every token** when running Gemma3-27B, whereas despite having more (30 Billion) total parameters, each token only involves your GPU having to go over a measly 3 Billion parameters for Nemotron-Nano or Qwen3-VL-30B.",
                  "score": 7,
                  "created_utc": "2026-01-18 02:54:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08ns1c",
          "author": "SeaFailure",
          "text": "also running Dual 5060ti + 128GB DDR4. LM studio is a good starting point to assess LLM capabilities.",
          "score": 4,
          "created_utc": "2026-01-18 04:57:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08xiqk",
              "author": "CollectionOk2393",
              "text": "Ok sounds yours is similar to mine, what is the best vision model you found that fits our size/performance?Â \n\n\nGemma3 27b q4_k_m is like 25 t/s\n\n\nBut\n\n\nqwen3-vl-30b-a3b-thinking-1m_moe q4_k_m is like 110 t/s\n\n\nThose moe models really are fast!",
              "score": 4,
              "created_utc": "2026-01-18 06:10:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o084foz",
          "author": "LittleBlueLaboratory",
          "text": "I copied DigitalSpaceport's 4x 3090 machine with an Epyc 7702P and 512GB DDR4 2666 in 8 channel. Thank goodness i bought it last summer.Â \n\n\nGLM 4.5 air is my go to for just using the 96GB VRAM and i get about 45 t/s with 500 t/s pp. But I can run Kimi K2 at Q2 at 9 t/s, GLM 4.7 Q4 at about 10 t/s, or Minimax M2.1 Q6 at about 15 t/s. But pp smol with the big models. Fastest I get is 55 t/s with Minimax.\n\n\nThe big models are downright painful to use with OpenCode because of the smol pp. But GLM Air works really well. I would love to use Devstral2 123B (500 t/s pp and 7 t/s) but I get tool calling errors at Q4.",
          "score": 4,
          "created_utc": "2026-01-18 02:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o086ymx",
              "author": "CollectionOk2393",
              "text": "Nice that setup is much better than what I have right now, lol\n\n\nI have 2x ockulink docks, so I could always sell my 5060's and get something bigger down the line. But for now its what I can afford\n\n\nAOOSTAR AG01 External GPU Docking... https://www.amazon.com/dp/B0F8B23CN9?ref=ppx_pop_mob_ap_share\n\n\nThe docks are also amazing, you can run 5090's on them... on the built in power supply! So nice!",
              "score": 2,
              "created_utc": "2026-01-18 03:13:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o08ba2w",
                  "author": "LittleBlueLaboratory",
                  "text": "Woah that dock is awesome! are your 5060s on the docks or are they inside your N5?",
                  "score": 3,
                  "created_utc": "2026-01-18 03:37:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o096hlb",
          "author": "960be6dde311",
          "text": "I got an RTX 5070 Ti and just ordered an extra 5060 Ti 16 GB. Running with 128 GB DDR5 and a Ryzen 9950X.Â \n\n\n\nAlso have an RTX 4070 Ti SUPER that will probably go into one of my Linux servers.Â \n\n\nI like the NVIDIA Nemotron 3 Nano model a lot.Â ",
          "score": 3,
          "created_utc": "2026-01-18 07:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o099bjy",
          "author": "Lg_taz",
          "text": "This is my setup, will eventually be changing for stronger case fans, swapping the Radeon gou for a ProArt 5070Ti 16Gb, and adding another 128Gb 6400 RAM. It was designed as a creative workstation originally, getting into running local LLMs came after the build, it manages Qwen3-coder-42b-Q8 at roughly 10-16 t/s not mega fast but manageable at context of 131k, MoE to CPU and the RAM set to 6000 for stability.\n\nhttps://preview.redd.it/qxdwpeutc2eg1.jpeg?width=1582&format=pjpg&auto=webp&s=bc0c9ad95bc9163bc83ab3fb982cb7ec08c1d916",
          "score": 3,
          "created_utc": "2026-01-18 07:52:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09a0w9",
              "author": "Lg_taz",
              "text": "I also have quite a few spare parts so considering building a dedicated AI machine to call on for better parallel pipelines, struggling big time to get the Radeon card to function in Oobabooga, but can't give up the granular control and flexibility, also I don't like using cloud for privacy as will be hopeful using as part of a business I am in the process of setting up, the idea is quick prototyping for iterating, setup a working prototype to then be made properly.",
              "score": 3,
              "created_utc": "2026-01-18 07:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o09tfqj",
          "author": "No_You3985",
          "text": "I run gpt OSs 20b on rtx 5060 ti. It generates at about 110 t/s which is great for summarization and context building tasks that supplement larger models for all sort of tasks (coding, documentation, graph labeling for rag, etc). By utilizing this small model heavily I save on tokens for commercial llm providers. I run it at high reasoning. I also process my personal notes from obsidian with it - something that I will never share with cloud providers",
          "score": 3,
          "created_utc": "2026-01-18 10:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0aggi9",
          "author": "After-Average-5953",
          "text": "5070 8gb with 24gb ram",
          "score": 3,
          "created_utc": "2026-01-18 13:54:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qepgii",
      "title": "Fine Tuning LLMs Fully Local!",
      "subreddit": "LocalLLM",
      "url": "https://seanneilan.com/posts/fine-tuning-local-llm/",
      "author": "my_cat_is_too_fat",
      "created_utc": "2026-01-16 19:21:06",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qepgii/fine_tuning_llms_fully_local/",
      "domain": "seanneilan.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzz72yn",
          "author": "Total-Context64",
          "text": "Nice work!  You're on an M1 - I support MLX LLM training in SAM if you want to give it a try. :)",
          "score": 2,
          "created_utc": "2026-01-16 19:31:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzd4bl",
              "author": "my_cat_is_too_fat",
              "text": "Thank you so much! I'll keep posting my findings here. I'm having so much fun learning about local llms & hopefully making them do useful work.\n\nYes, tell me more.",
              "score": 2,
              "created_utc": "2026-01-16 19:59:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzze79y",
                  "author": "Total-Context64",
                  "text": "Take a look here: [https://github.com/SyntheticAutonomicMind](https://github.com/SyntheticAutonomicMind) the last version implemented mlx training support, I'm still working on support for training llama models.",
                  "score": 2,
                  "created_utc": "2026-01-16 20:04:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o03iq9b",
                  "author": "Total-Context64",
                  "text": "llama training is in today's release if you'd like to tinker with it.  Would love feedback.",
                  "score": 1,
                  "created_utc": "2026-01-17 12:54:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qg2w9w",
      "title": "I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/dban4j25kdcg1.png",
      "author": "Ok-Pomegranate1314",
      "created_utc": "2026-01-18 08:08:44",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qg2w9w/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o09b88q",
          "author": "Ok-Pomegranate1314",
          "text": "https://preview.redd.it/zt1ekxvsf2eg1.png?width=2526&format=png&auto=webp&s=774ac308d97ec9808290cefd519d92d17fa86ac6\n\nEarly in a full finetune run on the Spark trio.",
          "score": 2,
          "created_utc": "2026-01-18 08:09:51",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o09qogt",
          "author": "HumanDrone8721",
          "text": "Do I have a matrix moment or is it just a karma farming attempt, this stuff was posted already and discussed here: https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/ a while ago ?!?",
          "score": 2,
          "created_utc": "2026-01-18 10:33:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}