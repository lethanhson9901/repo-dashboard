{
  "metadata": {
    "last_updated": "2026-01-17 08:40:58",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 50,
    "total_comments": 451,
    "file_size_bytes": 548648
  },
  "items": [
    {
      "id": "1q53qlk",
      "title": "LLMs are so unreliable",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/",
      "author": "Armageddon_80",
      "created_utc": "2026-01-06 00:45:52",
      "score": 175,
      "num_comments": 102,
      "upvote_ratio": 0.88,
      "text": "After 3 weeks of deep work, I''ve realized agents are so un predictable that are basically useless for any professional use. This is what I've found:\n\nLet's exclude the instructions that must be clear, effective and not ambiguos. Possibly with few shot examples (but not always!)\n\n1) Every model requires a system prompt carefully crafted with instructions styled  as similar as its training set. (Where do you find it? No idea)\nSame prompt with different model causes different results and performances. \nLesson learned: once you find a style that workish, better you stay with that model family.\n\n2) Inference parameters: that's is pure alchemy. time consuming of trial and error. (If you change model,  be ready to start all over again). No comment on this.\n\n3) system prompt  length: if you are too descriptive at best you inject a strong bias in the agent, at worst the model just forget some parts of it.\nIf you are too short model hallucinates.\nGood luck in finding the sweet spot, and still, you cross the fingers every time you run the agent. \nThis connect me to the next point...\n\n4) dense or MOE model? \nDense model are much better in keeping context (especially system instructions), but they are slow.\nMoE are fast, but during the experts activation not always the context is passed correctly among them. The \"not always\" makes me crazy.\nSo again you get different responses  based on I don't know what.! Pretty sure that are some obscure parameters as well...\nHope Qwen next will fix this.\n\n5) RAG and KGraphs? Fascinating but that's another field of science. Another deeeepp rabbit hole I don't even want to talk about now.\n\n6) Text to SQL?  You have to pray, a lot. Either you end up manually coding the commands and give it as tool, or be ready for disaster. And that is a BIG pity, since DB are very much used in any business.( Yeah yeah. Table description data types etc...already tried) \n\n7) you want reliability? Then go for structured input and output! Atomicity of tasks!\nI got to the point that between the problem decomposition to a level that the agent can manage it (reliably) and the construction of a structured input/output chain, the level of effort required makes me wonder what is this hype about AI? Or at least home AI.  (and I have a Ryzen AI max 395).\n\n\nAnd still after all the efforts you always have this feeling: will it work this time? \nAgentic shit is far far away from YouTube demos and frameworks examples.\nSome people creates Frankenstein systems, where even naming the combination they are using is too long,.but hey it works!! Question is \"for how long\"? \nWhat's gonna be deprecated or updated on the next version of one of your parts?\n\nWhat I've learned is that if you want to make something professional and reliable, (especially if you are being paid for it) better to use good old deterministic code, and as less dependencies as possible. Put here and there some LLM calls for those task where NLP is necessary because coding all conditions would take forever.\n\nNonetheless I do  believe, that in the end, the magical equilibrium of all parameters and prompts and shit must exist. And while I search for that sweet spot, I hope that local models will keep improving and making our life way simpler.\n\nJust for the curious: I've tried every possible model until gpt OSS 120b, Framework AGNO. Inference with LMstudio and Ollama (I'm on Windows, no vllm).\n\n \n\n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxx7hpr",
          "author": "macromind",
          "text": "Yep, this matches my experience too: agents look magical in demos, but in real work its mostly prompt + tool wiring + guardrails + a lot of retries and tests.\n\nOne thing that helped me was treating the LLM like a fallible component and making everything around it deterministic: strict JSON schemas, small steps, unit tests on tool outputs, and hard timeouts/fallbacks.\n\nIf you are experimenting with patterns for making agent workflows less chaotic, this writeup has a few practical ideas (tool contracts, evals, and reliability tricks): https://www.agentixlabs.com/blog/",
          "score": 57,
          "created_utc": "2026-01-06 00:51:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz41y4",
              "author": "publiusvaleri_us",
              "text": "The problems I see with this method is that testing prompts and scaling are counter-positioned into a matrix of chicken-and-egg paradoxes.\n\nLike opening a pizza store, but not getting a cheese and flour supplier. Regardless, you need to see who might want pizza in your community. That means you need to pick the cheese and flour, but the pizza wholesale company won't sell you just 10 pounds of it. They want a contract for 300 pounds a week and then they'll talk.\n\nSo you buy Walmart cheese and call your neighbor, but the test is inconclusive. The project can never move forward without high risk of capital and doing the whole pizza store thing and just opening up with the supplies you think are right.\n\nFor the LLM, it means you have to throw heavy hardware or high capital, lots of time, and lots of tests into something that might never be profitable for business or affordable for home hobbies.\n\nYou don't know until you add up the unpredictable costs while you tested and played with prompts for a week to a year. And find that secret sauce.",
              "score": 4,
              "created_utc": "2026-01-06 08:29:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz533p",
                  "author": "publiusvaleri_us",
                  "text": "And you know how I know this?\n\nBecause when you find your secret sauce LLM prompt and settings for an application, what happens? Productivity/quality/accuracy goes from zero to hero with that one last final tweak. Be it a word in a prompt or adding 128 GB of RAM to a PC, you found the sweet spot.\n\nEvery other spot was wrong. It's the graph that shows a sharp peak that rises from the noise floor to a 50 dB signal. It can be tweaked in version 2, but you broke through the barrier.\n\nEverything LLM is like this - very hit or very miss. I've seen the video series by [https://www.youtube.com/@aiwarehouse](https://www.youtube.com/@aiwarehouse) AI Warehouse that bears this out, as well. Albert is a moron for 10,000 iterations and then he \"learns\" a trick. And it cascades to a new skill.",
                  "score": 0,
                  "created_utc": "2026-01-06 08:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxxh77o",
              "author": "cneakysunt",
              "text": "We're about to dive in seriously. After much research over the last year this is exactly where we have landed.",
              "score": 3,
              "created_utc": "2026-01-06 01:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyqxakq",
              "author": "kikkoman23",
              "text": "Yeah I’ve been building one last few weeks. Getting stuck on what should be simple things. But man understanding the whole HITL flow and how to treat each interaction with correct LLM call. \n\nThen what to do with that output. Surface to user in a way that makes sense. Then allow them to respond with more narrowly focused agent work is tricky. \n\nSomething ChatGPT does that’s so seamless but implementing it is just not going as smoothly as I envisioned. \n\nAlso doesn’t help those around me building other PoC with chat agent ish type features. \n\nI need to get better feedback loop and add way more tests to help myself and these tool running with Claude.",
              "score": 1,
              "created_utc": "2026-01-10 06:57:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxiqrp",
          "author": "publiusvaleri_us",
          "text": "YES. #4 and #5, plus 1, 2, and 3. You did a lot of thinking on this and are correct.\n\nMy comment is this on the latter part. Your prediction of the future is sound. There will be so many iterations of AI improvements, PC hardware improvements, and refinements in the interface that the LLM of 2036 will be nothing like an LLM of 2026. Early adopters have so many disadvantages.\n\nThis stuff will be cheaper and faster in ten years. At current tech level and my current interest level, I think an adequate system, for just my personal use (With an eye to selling it), would cost $500 to $2000 per month and would need a new Internet connection. The current software may seem bloated when you download it, but the interfaces of today are simply a kludge and a Jack-of-all-trades that does nothing right except maybe answer chat questions a 6th grader might ask.\n\nAsk a current LLM about a dataset, and you're going to get terrible results. Even commercial systems stink. All you have to do is call into a Big Company and ask for tech support. The human agents are typing things into an AI, you can tell, and trying to bring up internal PDFs to answer your question and walk you through a solution.\n\nBecause their innate human knowledge is practically nil. They read from a script like phone agents have done for 40 years, but they are reading from hallucinated AI slop or the wrong PDF that's from 4 versions earlier that do not apply. LLMs have not taught Tier 1 support personnel how to think, and they certainly haven't trained them on the specifics of the things they are supporting.\n\nIf Fortune 100 companies can't figure out how to get AI to work to help their customers at Tier 1 (and their bottom line making money), I find it ridiculous to assume that a one-man-shop could figure it out.\n\nI wish all of you programmers, content creators, and schoolkids doing homework all the best! For the rest of us, it's hard to go full-in to this mess, for all of the reasons OP stated, starting at #1 and on down the list.\n\nBravo.",
          "score": 18,
          "created_utc": "2026-01-06 01:51:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzbjqd",
              "author": "thedarkbobo",
              "text": "Big corpo is slower sometimes in using top edge tech because of risks audits etc. I would think it should be first used by small companies. Big tech will use consultants when they are available and things stabilize s bit I think.",
              "score": 2,
              "created_utc": "2026-01-06 09:42:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1daoz",
                  "author": "Embarrassed_Egg2711",
                  "text": "That's a broad generalization and while it can be true about initial adoption, it's not related to being unsuccessful when they do try, and certainly not a given when you have a hype-freight-train like \"AI\".  There's been no real success, even with companies like SalesForce that have already bought into AI.",
                  "score": 4,
                  "created_utc": "2026-01-06 17:07:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nytvhu7",
                  "author": "Lumpy_Quit1457",
                  "text": "Your local Walgreens hasn't updated their systems in nearly 10 years. Big Corp (just like our national infrastructure  lol) is so against spending a dime out of their ass for upgraded tech, it affects customer confidence capital... and even knowing that fact, they still don't do it. \n\nSo understanding that unless it's Big C \"Tech\", they aren't putting out until it's broken so bad they fail. \n\nThis is why I want to local just about everything I can.",
                  "score": 2,
                  "created_utc": "2026-01-10 18:36:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxazja",
          "author": "StardockEngineer",
          "text": "Only Sonnet and Opus work well as agents that can be used without high specialization.  Anything else requires lots of leg work.  Minimax 2.1 is the closest I’ve found in the OSS world.",
          "score": 16,
          "created_utc": "2026-01-06 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0msbs",
              "author": "opensourcecolumbus",
              "text": "Minimax2.1 better than qwen 3?",
              "score": 3,
              "created_utc": "2026-01-06 15:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny0zpdv",
                  "author": "StardockEngineer",
                  "text": "Yes, it is.  Especially for agents.  The interleaved-thinking is great.  More here: https://www.minimax.io/news/minimax-m21",
                  "score": 6,
                  "created_utc": "2026-01-06 16:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny15m6i",
              "author": "svachalek",
              "text": "Haiku does too, given precise instructions. It’s in a league of its own for models that are in its presumed size class.",
              "score": 1,
              "created_utc": "2026-01-06 16:32:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx8gb2",
          "author": "Krommander",
          "text": "If the instruction set is plain text or markdown I have found that the system's prompt adherence depends on the coherence. \n\n\nVery coherent prompts can be 100+ pages without breaking for commercial LLMs, if internal knowledge mapping and cognitive routines are recursive. \n\n\nFor local models, I have had some small success, but the context window gets busted after 2 replies if I use the same huge system prompt. \n\n\nTool use is a whole other bag of problems though. ",
          "score": 6,
          "created_utc": "2026-01-06 00:56:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzc9lp",
              "author": "thedarkbobo",
              "text": "Need to compact memory I guess if reaches threshold.",
              "score": 3,
              "created_utc": "2026-01-06 09:49:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx84fv",
          "author": "BrewHog",
          "text": "You mentioned it. Structured input and output with a reliable model (professionally I only use the big boy models Gemini 3, Claude, etc). \n\n\nI'm currently using it for quite a few tasks regularly and reliably. It definitely helps me keep my business running without the need to hire two or three employees. \n\n\nThe first level support is fantastic, and the cliche sentiment analysis usually works well for what I need. \n\n\nFor more complex tasks, I still use only DSPy for the structured in/out and many times just manually run it to save me oogads of time (product/marketing material, document reviewing, etc) \n\n\nGive us some specific examples of what you need so we can guide you. Or just propose an idea that you think should work, but doesn't in practice.",
          "score": 11,
          "created_utc": "2026-01-06 00:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxa8ut",
              "author": "Armageddon_80",
              "text": "I think, from early days, introducing frontier models in companies workflows is a huge strategical risk. \nClearly depend from the business you have and the complexity of it. But unfortunately the big players are all in USA (I'm in Europe ) and I stronly5 believe soon or late, AI will become a \"tool\" for geopolitical leverage.\nI can't imagine what happens to a company where employees got lazy thanks to the magic of AI, and the day after the service is interrupted (or even worse, no employees at all only AI). That's why I'm working hard to implement an architecture dependant only on Local models.",
              "score": 17,
              "created_utc": "2026-01-06 01:05:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxye9tv",
                  "author": "ThatOneGuy4321",
                  "text": "even if AI doesn’t turn into a straight up propaganda tool, the huge disparity between current investment and revenue guarantees rapid enshittification of the service as they try to figure out how to make it profitable. \n\nQuite possibly 10x price hikes and complete flooding of marketing content. I seriously doubt OpenAI will even be able to solve their revenue problem honestly and may just go bankrupt. Their 5-year spending commitment is $1 trillion and their yearly revenue is $13 billion lol",
                  "score": 5,
                  "created_utc": "2026-01-06 04:55:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny013tm",
                  "author": "Kos187",
                  "text": "1. Go Europe!\n2. Mistral Large 3 isn't bad, it's also doesn't feel like first echelon model today \n3. Big Chinese models do feel like first Echelon and could be run locally,  but require expensive hardware. 10k for Mac Studio give rather small t/s, or 20k to fit into VRAM and good performance. Or maybe 128GB RAM and single large GPU for barely running it at all (/ t/s).\n4. Hosting in EU is also an option, but without reserved instances running big Chinese models is something like 5k a month spend.",
                  "score": 5,
                  "created_utc": "2026-01-06 13:06:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxv55p",
                  "author": "AfterAte",
                  "text": "I do believe someday the transatlantic cables will be cut by autonomous drones. This world has gone into full anarchy, and \"haven't seen nothing yet\". We'll all have to get Starlink after that.",
                  "score": 3,
                  "created_utc": "2026-01-06 02:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyizky1",
                  "author": "Equivalent_Mine_1827",
                  "text": "Me too... I've been investing a lot of time trying to make a reliable local llm workflow.\nIt's tough.\nEven more tough on my side, I don't have a very nice computer",
                  "score": 2,
                  "created_utc": "2026-01-09 03:00:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny20w0t",
                  "author": "BrewHog",
                  "text": "I definitely agree that you shouldn't solely depend on the big business frontier models. Local LLMs as a backup is definitely important if you choose the frontier models.\n\nThe only other issue I have with Frontier models seems to be how quickly older models get deprecated and removed.",
                  "score": 1,
                  "created_utc": "2026-01-06 18:53:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nytwj8s",
                  "author": "Lumpy_Quit1457",
                  "text": "Would you consider giving a person an occasional heads-up about how your doing? Which direction you're going? Definitely a noob/hobbyist here, but willing to learn.",
                  "score": 1,
                  "created_utc": "2026-01-10 18:41:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1n0jw",
          "author": "AllTheCoins",
          "text": "I’m starting to realize anyone who deals with LLMs and feels this way about them has NEVER been in charge of someone before. Agents will always deliver something. People are way more unreliable.",
          "score": 5,
          "created_utc": "2026-01-06 17:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyahl6u",
              "author": "AurumDaemonHD",
              "text": "Perfect take. They are also more expensive and they can and will quit.",
              "score": 1,
              "created_utc": "2026-01-07 22:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyja7xr",
                  "author": "LifeandSAisAwesome",
                  "text": "And or steal / lie cause legal nightmares etc.",
                  "score": 1,
                  "created_utc": "2026-01-09 04:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxllmf",
          "author": "Such_Advantage_6949",
          "text": "Reliable LLM is not cheap. From my experience, reliable local models are from minimax glm onwards (200b onwards). 120B oss is a start but still very hit and miss",
          "score": 9,
          "created_utc": "2026-01-06 02:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxae4x",
          "author": "PromptOutlaw",
          "text": "I went thru this pain, for my scope of work I managed to tame most models except Kimi K2 and Cohere-command-a. You need:\n- adapters per LLM\n- normalization per LLM\n- strict output response\n\nHave a look here I hope it helps: https://github.com/Wahjid-Nasser/12-Angry-Tokens\n\nI’m pushing an update this week for observability, redundancy and some stats",
          "score": 3,
          "created_utc": "2026-01-06 01:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxdm1b",
              "author": "Armageddon_80",
              "text": "Can you explain what do you mean for adapters and normalization? I'm very interested",
              "score": 1,
              "created_utc": "2026-01-06 01:24:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxxgj4n",
                  "author": "PromptOutlaw",
                  "text": "E.g. Deepseek is gna spit out its thinking, stop fighting it and strip. Claude fights you to surround your output with markdown tags, just strip it. With adapters you have a generic prompt core, and each LLM must have an adapter prompt padded before API call to make it behave.\n\nHighly advise against generic prompt for all to work. I ended up with prompt-spaghetti that gave me headaches",
                  "score": 4,
                  "created_utc": "2026-01-06 01:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxd877",
          "author": "Classic_Chemical_237",
          "text": "Traditionally, we work with structured input and output. With structured io, it’s natural to be UX-centric.\n\nWith LLM, it became text centric with chat and voice. To work with traditional endpoints, supposedly you create MCP to sit on top of API.\n\nThis whole approach is wrong. Human work way better which structured UX. (That’s what even LLM try to emulate it with bullet points and emojis) Most use cases only need LLM for part of the flow.\n\nWe don’t need MCP sitting on top of API. We need API sitting on top of LLM.\n\nI am so close to ship ShapeShyft for make this easy.",
          "score": 3,
          "created_utc": "2026-01-06 01:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyswlzg",
              "author": "Dougy27",
              "text": "thoughts? [https://github.com/dougy27/jarvis-os/tree/release/v2.1.0?tab=readme-ov-file](https://github.com/dougy27/jarvis-os/tree/release/v2.1.0?tab=readme-ov-file)",
              "score": 1,
              "created_utc": "2026-01-10 15:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxrim5",
          "author": "SAPPHIR3ROS3",
          "text": "As much as this kind match my experience, THE general rule i follow is “Divide et impera” (meaning divide and conquer) as much as possible: going under 30b, MoE or not, it’s a guess hence you have to use LLM for NLP-based task only and as least as possible (above 30b you start seeing some more consistency, but only above 100b roughly the result start to feel reliable). Nonetheless structured input/output are vital to ensure consistency in workflows at any size even with the big bois (claude, gemini chatgpt ecc.)",
          "score": 3,
          "created_utc": "2026-01-06 02:39:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny37s1j",
          "author": "BenevolentJoker",
          "text": "The reason why many LLMs are unreliable as agents boils down to mainly one thing: in order for LLMs to really be actually useful for any professional workloads you need to be able to modify their behavior from probabilistic behavior to deterministic behavior. Prompting will only get you so far, and not without drawbacks (namely it eats into context).\n\nThere are a few different ways to do this, some easier than others such as pre and post filtering outcomes. The most riggorous and ideal setup involves logit logging and logit inferencing at inference time. -- do these things and honestly model behavior evens out fairly well across all models.",
          "score": 3,
          "created_utc": "2026-01-06 22:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny3rari",
          "author": "Interesting_Ride2443",
          "text": "I feel your pain. The gap between a YouTube demo and a production-grade agent is a canyon.\n\nYou're 100% right about 'good old deterministic code'-that’s actually the only way to make agents reliable. The mistake most frameworks make is trying to hide the logic inside complex abstractions or 'black box' prompts.\n\nI’ve shifted to an 'agent-as-code' approach where the LLM is just a tool inside a strictly typed (TS) function. Instead of praying for a prompt to work, you manage the state and logic in code, and only use the LLM for the 'messy' NLP parts. It’s much easier to debug when you have a managed runtime that shows you the exact state and execution trace at every step.\n\nReliability doesn't come from better prompts; it comes from better infrastructure that treats agents like software, not like magic.",
          "score": 3,
          "created_utc": "2026-01-06 23:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9rchx",
          "author": "No-Comfortable-2284",
          "text": "LLMs are not accurate but you can create a system that only outputs accurate/desired output with fixed checkpoints. You can also increase the quality and accuracy of desired outputs by adding another layer of complexity on top. for example using another seperate instance of the model with sole purpose of cross checking answers with RAG. I think the biggest issue in AI currently is that people try to make a single model do everything. our brain is made of many modules of less intelligence, communicating together, ant colonies are made of dumb ants. greater intelligence can be created from many instances of lesser intelligence and if there ever will be a super intelligence it prob will be smthn like that :p",
          "score": 3,
          "created_utc": "2026-01-07 20:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9vat0",
              "author": "Armageddon_80",
              "text": "I agree with you. Only problem is that more layers of intelligence means more latency in general. In some cases, where parallelism can be of use, the latency issue could be solved with concurrent calls in async mode.\nBut not in the case you described  and neither in mine.\nSo I'm pushing the limits of the model until eventually I'm gonna have to find other ways.\nI've found a sweet spot with gpt OSS 20b for now. Alleluja.",
              "score": 2,
              "created_utc": "2026-01-07 21:11:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyc7gh9",
          "author": "Echo_OS",
          "text": "Interesting\n\nReading through this thread, there seems to be broad agreement on the underlying issue.\n\nLLMs themselves are not inherently unreliable. The problem is that they are often used in roles that require deterministic behavior. When an LLM is treated as a probabilistic component within a deterministic system - for example, wrapped in agent-as-code patterns, strict input/output schemas, typed interfaces, and explicit checkpoints - most reliability issues are significantly reduced.\n\nAt that stage, the primary challenges shift away from prompt design or model choice and toward system architecture: managing latency, defining clear boundaries, and deciding which parts of the system are allowed to make judgments versus which must remain deterministic.",
          "score": 3,
          "created_utc": "2026-01-08 04:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyzu8d",
          "author": "fermented_Owl-32",
          "text": "Using some psychological experience always MILDS DOWN these problems significantly. I have seen people complain of the same problems and i have implemented them in professional env as well. \n\nIts just a prompt, the better you understand how LLMs work and the better you are with human psychology and communication habits, the more robust outputs you get.\n\nI prefer to use not the latest models but from 2025 beginning H1. For Professional basic uses even Amazon's nova prime 2 does wonders  \n\nI still dont understand after a good analysis as this where you yourself write your issues, how have you not able to get things done by keeping these mind or making it part of the system.",
          "score": 4,
          "created_utc": "2026-01-06 07:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz48eb",
              "author": "Armageddon_80",
              "text": "I like your comment specially the last part:\nThe quick answer is that these systems are far more complex than what they may appear. \n\nIf you read the release papers of the models, you must be a PHD on many fields of study to  understand what's even written there. Which of course is not my case.\nI'm not talking of knowing and repeating as a parrot, I'm talking about understanding. \nAfter various steps of quantization and adaptation, some kind of distilled version of the original model finally lands on your computer. And now what? \nYou need to test it and start to know it, in a process of \"reverse learning\", and yes it's difficult.\n\nThe other way is to \"contain it\" to make it kind of behave the way you want, but that is also a lot of work, building guardrails and a very strict architecture around it, and  removing all the beauty of AI. \n\nLately I'm chatting with the models I intend to use, and making some precise questions to let the model leak its \"internal ways\" of writing, processing, expecting instructions. Trying to get some of it's secrets let's say through chat sessions and role plays \nIn other words psicology of the model. \n\n\n\n\n\n\n\n\n\n \nIf you try to simplify\n\n\nIn other words, you have many variables,",
              "score": 4,
              "created_utc": "2026-01-06 08:31:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx8dte",
          "author": "newbietofx",
          "text": "I'm creating a text to sql llm. How do I ensure it works? Run complicated crud? ",
          "score": 2,
          "created_utc": "2026-01-06 00:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxc9hn",
              "author": "Armageddon_80",
              "text": "Most model will fail on multi tables DB.\nI had to create an agent for every table, engineering the table itself with explicit column names and minimal foreign keys. I had to include brief description for each column so the model can \"link\" my text request. Understand it and then convert it into SQL command. \nI'm now running a team of agents, where each agent represent  their  tables (CRUD)  and the team leader represent the DB.\nStill working on it, I was just telling you how I'm fixing that thing and maybe give you an idea.",
              "score": 3,
              "created_utc": "2026-01-06 01:16:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0rj2z",
                  "author": "Powerful-Street",
                  "text": "Just make your own router essentially and keep the output in memory until it is written to db by your router.",
                  "score": 1,
                  "created_utc": "2026-01-06 15:27:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nysz3tx",
              "author": "Dougy27",
              "text": "Attack multi-table DB gap by using a Hierarchical Multi-Agent System. Instead of one big prompt, a Supervisor Agent routes your request to specific Curated Agents (your \"table agents\") that only see the schema and descriptions for their assigned table. uses the Model Context Protocol (MCP) to securely handle the SQL execution, while a Reflection Agent double-checks the results to catch join errors or missing keys before any data is changed.... example [https://github.com/dougy27/jarvis-os/tree/release/v2.1.0?tab=readme-ov-file](https://github.com/dougy27/jarvis-os/tree/release/v2.1.0?tab=readme-ov-file)",
              "score": 2,
              "created_utc": "2026-01-10 16:03:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxcpuh",
          "author": "Mountain-Hedgehog128",
          "text": "Agents need to be combined with rules based structure. They are valuable, but the toughest part is finding the right places and ways to use them.",
          "score": 2,
          "created_utc": "2026-01-06 01:19:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxesr1",
          "author": "Taserface_ow",
          "text": "LLMs are never going to be perfect, it’s the nature of the model. Successful products/services that use LLMs build workflows around them to cater for this limitation.",
          "score": 2,
          "created_utc": "2026-01-06 01:30:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxewea",
          "author": "grathontolarsdatarod",
          "text": "So.... Lie detectors are not admissible as evidence in most courts with an advanced legal system.  Yet they are used in employment screening in those same jurisdictions. \n\nAI is a scape goat for anything.  Its literally tablets from the mountain that can whatever.",
          "score": 2,
          "created_utc": "2026-01-06 01:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxawx",
          "author": "evilbarron2",
          "text": "I think there are valid use cases for LLMs in production systems, but I feel like most places I see it used it’s being shoehorned. I’m not sure that they are most (or even particularly) useful in our current software systems. I kinda think they’ll be more useful in a different type of computing structure. I think we’ve only seen glimpses of what that looks like so far.",
          "score": 2,
          "created_utc": "2026-01-06 03:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxwx3",
          "author": "Proof_Scene_9281",
          "text": "You have to blend commercial LLM’s and local for the best results. Use the commercial to think and craft and the locals to dig ",
          "score": 2,
          "created_utc": "2026-01-06 03:14:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyzklf",
          "author": "Dentuam",
          "text": "The point about MoE models dropping context during expert routing still rings true for many local runs, though the latest Qwen3 MoE releases have noticeably stabilized that behavior compared to earlier versions. Dense models continue to win for reliability when you need consistent instruction following.",
          "score": 2,
          "created_utc": "2026-01-06 07:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzaxiz",
          "author": "a_pimpnamed",
          "text": "I truly don't understand why people think LLM's can ever actually become true intelligence. You can't even see the logic chain or query it about its own logic. They learn once and they are stuck unless you train them over again. They use way too much compute. This can only be scaled so much even if Microsoft gets bitnet to work it still would be capped eventually it's not true intelligence just a prediction algo a very good prediction algo. We need to move back to symbolic ai or to actual cognitive architectures.",
          "score": 2,
          "created_utc": "2026-01-06 09:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzjm4b",
              "author": "Armageddon_80",
              "text": "Well for many many tasks you don't need intelligence. Including most of business workflow: People over estimate their roles in working environments :).\nSo I agree with you, but AI can save you from writing lot of code for simple things like user intent.yepp that's not intelligence, but why not to use it?",
              "score": 4,
              "created_utc": "2026-01-06 10:55:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nysx97l",
              "author": "Dougy27",
              "text": "Agreed. Plugging my cognitive architecture side project... [https://github.com/dougy27/jarvis-os/tree/release/v2.1.0?tab=readme-ov-file](https://github.com/dougy27/jarvis-os/tree/release/v2.1.0?tab=readme-ov-file)",
              "score": 1,
              "created_utc": "2026-01-10 15:54:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzjo3t",
          "author": "Cergorach",
          "text": "A couple of things:\n\na.) LLMs are tools, just like with any tool, you need to learn how to use it. A master woodworker will be far more proficient with a hammer then Joe Smoe that just picked up a hammer from Harborfreight for the first time ever. Different tools, different skill sets.\n\nb.) Right tool for the job. For some jobs these tools are either the wrong tools or just not worth the time/cost. Just as it's sometimes just faster to do something yourself then explain to someone else what you want so they can do it for you.\n\nc.) There can be a huge difference in quality between 'cheap' tools and 'expensive' tools. In this case you're using tiny models locally, even the 120B quantized is not going to compare well to the big unquantized models. For SQL see: [https://llm-benchmark.tinybird.live/](https://llm-benchmark.tinybird.live/)\n\nd.) You need to know the answer, always with LLM. If you don't, you're in for a world of hurt in a professional production environment. And SQL queries in a production environment seem to *me* like probably the worst possible use of LLM.\n\ne.) People need to realize that at what point are you spending more time=money on LLM then actually making your work more efficient?\n\nSidenote: I have not yet used LLM in my professional capacity. Primary reason is that I tend to work on the edge of IT deployment, and the LLMs haven't been trained yet on the use cases I'm working on and the edge cases that are very rare. Not to mention that even IF the LLM is trained on the 'promotional' material, the reason I'm doing my thing is to see if what's been sold to the client actually works as the sales people say it does (and it often does not)... The other reason is that the companies/clients I work for either do not allow LLM or have not yet onboarded it. And when I work for a client, I only use what they have allowed.\n\nPersonally I've tested quite a bit on a Mac Mini M4 Pro 64GB with models up to 70b, mostly for hobby projects. The results from there is that while very cool and the DS 70b model was better then the far larger GPT from less then a year before, still the unquantized far larger models we can't run locally did far, far better. And due to it being for my hobby projects, I saw no security concerns to *not* use the larger models (for free) on the web. Even the DS 671b model *quantized* on a $10k Mac Studio M3 Ultra 512GB gave worse quality responses then the full unquantized free model you could use on the Internet. Spending $50k on a Mac Studio cluster would be cool, but imho highly inefficient.",
          "score": 2,
          "created_utc": "2026-01-06 10:55:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2y4ig",
              "author": "New_Cranberry_6451",
              "text": "You say you are not using LLMs and still reached to these conclusions... you are a wise man. The one I liked most is \"SQL queries in a production environment seem to me like probably the worst possible use of LLM\", because it reflects a huge mistake we keep doing, try to use AI for AUTOMATIONS or to solve problems that \"traditional programming\" do far better after years of perfectioning. Why would I need AI to make a CRUD?   \nIn real life and for the local LLMs available right now, there are very few problems they can really help and do things we couldn't do with traditional programming... for example, telling the AI to choose the comments that are related to some topic (when we don't already have categorization or tagging, because if we do, that's far more reliable... but if we don't have a categorization system already implemented, that is a new \"superpower\" we now have.\n\nI also liked OP's post a lot, great post and REAL lessons.",
              "score": 2,
              "created_utc": "2026-01-06 21:25:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6epzr",
          "author": "MadeByTango",
          "text": "AI is the stripper of technology; you love *her*, she loves *money*, and nothing is real except the financial hangover once you step out of the building….",
          "score": 4,
          "created_utc": "2026-01-07 10:37:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6euq1",
              "author": "Armageddon_80",
              "text": "Hahahaha this is a good one!!!",
              "score": 1,
              "created_utc": "2026-01-07 10:38:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyvjfa",
          "author": "Your_Friendly_Nerd",
          "text": "I wholeheartedly agree with all the points you make. I've been trying to integrate local LLM's into my workflow as a programmer (mainly qwen3-coder). It works fine for simple tasks and tool calls, but as soon as it gets slightly more complex, it's not even worth the effort to write the prompt and wait for its output, it's quicker to just code it myself. \n\nThat said, I also think LLM's are still in their infancy, and there is so much we have yet to discover about them. For example for my use-case as a programmer, I see a lot of potential in spec-driven development, where a task is broken down into tiny subtasks that can then each be implemented and tested, and not be too complex for a local LLM to achieve. But how to formulate those specs? How does the workflow look like? I don't know.\n\nAnd that's maybe another issue - so many of the AI integrators (like IDE chat plugins) expect to talk to a frontier model like Claude Sonnet, Gemini or GPT. But when I plug them into a smaller model, they shouldn't just use the exact same prompts, but more specialized ones, and I fear there's just not enough interest in the community in perfecting this.",
          "score": 2,
          "created_utc": "2026-01-06 07:11:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzcvsp",
              "author": "thedarkbobo",
              "text": "What exactly was your setup? How far did you go?:)",
              "score": 1,
              "created_utc": "2026-01-06 09:54:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny33n4k",
                  "author": "Your_Friendly_Nerd",
                  "text": "I use NeoVim as my editor, CodeCompanion as the AI Chat plugin. Didn't really get very far, still kinda figuring out how to make the best use of it.",
                  "score": 1,
                  "created_utc": "2026-01-06 21:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxy58mr",
          "author": "IllustratorInner4904",
          "text": "Intent -> deterministic tools -> let agent call tools = less unreliable agents (who instead sometimes want to tell you about the tools they will call lmao)",
          "score": 1,
          "created_utc": "2026-01-06 03:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyumsw",
          "author": "linewhite",
          "text": "Yeah, i just made [https://www.ngrm.ai](https://www.ngrm.ai) to deal with this, structured memory drastically helped my workflows, LLMs are not enough, but with the right tools it gets better as time goes on.",
          "score": 1,
          "created_utc": "2026-01-06 07:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz7cs9",
          "author": "Black_Hair_Foreigner",
          "text": "LLM is just another automation tool. Like, When you write some code but you are too lazy to write over 1000 code. So you decide to use LLM to write code and it did just 5 minute or less. In this progress, you check LLM’s code that this logic is really correct. You found some bug or nun-sense, and fix it. Everything is running well! If you doing this shit by your hand, you will be consume your time more than 3 days and your time is gold. This is why everyone use this shit. Everyone knows this is piece of shit, but time is too expensive to write code.",
          "score": 1,
          "created_utc": "2026-01-06 09:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzj83l",
              "author": "Armageddon_80",
              "text": "For coding I must say that both Gemini Antigravity and Claude are fantastic, but for WRITING code, not ENGINEERING.  Still crazy horses hard to control, but very very good. \nIf you want to create something from zero, is gonna take many rounds before it takes shape (and you actually understand the code).",
              "score": 3,
              "created_utc": "2026-01-06 10:52:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny0ufgv",
          "author": "_WaterBear",
          "text": "OpenEvidence is pretty much the only professional use-case I’ve encountered thus far that seems competent. RAG/document embedding for info searching is quite functional and reliable so long as the user isn’t lazy and only uses the LLM as a search engine. Most other use cases involve too high a probability of hallucination that, for any professional requiring precision, just adds burdensome QA uncertainty to their process. So, outside highly tailored and unserious use-cases, these things will be a bust. Circumscribe the bubble, lest you be caught inside when it pops.",
          "score": 1,
          "created_utc": "2026-01-06 15:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1hyqt",
          "author": "tim_dude",
          "text": "So, is it like hiring a group of general (generic?) professionals, giving them written instructions and a bunch of meth and telling them to come back when they figured it out on their own?",
          "score": 1,
          "created_utc": "2026-01-06 17:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny44jyu",
          "author": "Green-Ad-3964",
          "text": "perhaps...in 20x6 where x>2",
          "score": 1,
          "created_utc": "2026-01-07 00:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5nxpw",
          "author": "Agent_invariant",
          "text": "This resonates a lot. The shift for me was the same: stop treating the LLM as “smart” and start treating it as a fallible proposer. Once you make execution, state writes, and tool effects deterministic, the chaos drops fast.\nWhere I’ve seen things still break is after JSON + schemas — especially when agents can confidently continue after drift or partial failure. Hard blocking on integrity loss (instead of letting the model narrate past it) made a bigger difference than better prompts.\nCurious if you’ve found good ways to enforce that boundary cleanly at the tool/state layer",
          "score": 1,
          "created_utc": "2026-01-07 06:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydgg05",
          "author": "somehowchris",
          "text": "Tell me you’ve been in the trenches for less than 6 years without telling me",
          "score": 1,
          "created_utc": "2026-01-08 10:12:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydnyut",
              "author": "Armageddon_80",
              "text": "What? I'm 45,  I've started as a nerd hobby 3 years ago. my background is electronic engineering and I'm trying to introduce agentic systems in my company.",
              "score": 1,
              "created_utc": "2026-01-08 11:16:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nydk2xv",
          "author": "Worth_Rabbit_6262",
          "text": "You've written a lot, but not the use cases. Often, the problem isn't the underlying model, but the lack of integration with workflows. Have you tried optimizing performance with DPO?",
          "score": 1,
          "created_utc": "2026-01-08 10:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydq199",
              "author": "Armageddon_80",
              "text": "Use case is specialized agents in reading data from DBs and optimize/organize (by reasoning) human resources assignments based on a constraints like:\ncomplex business rules, time constraints, priority constrains and more. All of them being fucked up convoluted variables. \nIt's a task that requires 4 people and several hours every time we need to plan weekly activities and distribution of workload in the company.\n\nI've decomposed the global task into sub tasks.\nNow the tests are about 1 agent and 1 sub-task. When succeeds I scale to more agents/sub tasks (all of them in isolation). When all of them succeed, i'll organize a team of agents....then and only then I will worry about workflows and all the rest.\nIssue was \"unreliable following of system prompt\", seems I managed to fix that. Let's see if the other agents do well like the first one.\nThe take away is: what are the magic words the model wants to hear? Once you find it, the need of constructing a scaffold of guardrails and validations reduces to minimum. That is important because if I need to build a whole castle of safety around every agent, the system will not be  scalable at all.",
              "score": 2,
              "created_utc": "2026-01-08 11:33:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nye6yiq",
          "author": "at0mi",
          "text": "which quantisation? glm 4.7 in bf16 works great",
          "score": 1,
          "created_utc": "2026-01-08 13:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf4bmy",
          "author": "kacisse",
          "text": "I agree it's too early BUT it's because only a few people master the real best practices (what to prompt, how to present data to the llm, when tu summarize, when to cache, how to even name your cached object so that the llm don't get confused... Etc).\nI feel LLM have their periods, you just mess up a little comma or name and they go banana. It's definitely possible to build serious app with cheaper models BUT you really need the best practices...and that is not for everyone. So I agree the hype is too early, and no enterprise can rely on that long term. That's a bubble for me 😬",
          "score": 1,
          "created_utc": "2026-01-08 16:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfkfti",
              "author": "Armageddon_80",
              "text": "A reason of why I'm oriented to local models is the fact that once you mastered the prompting for that model's family you move quickly with the project.\nAnd also you don't need to worry for the long term because once is done, it's written on the stone (off line).\nI believe there are a lot of possibilities with local AI, I wouldn't really call it a bubble. There's a huge FOMO people trying to monetize big and fast, fully agree. That's why every day a new product is released. Users are more worried on what to learn next rather than put in real use what there's around today (and there's a lot). \nI love AI with all it's defects, I see a very bright future for those that can use it to solve real business problems.",
              "score": 1,
              "created_utc": "2026-01-08 17:19:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjacrc",
          "author": "LifeandSAisAwesome",
          "text": "Internet was a fab too, but really give it 5-10years and will be a very different landscape.",
          "score": 1,
          "created_utc": "2026-01-09 04:01:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykcg3t",
          "author": "MaestroCodex",
          "text": "It's a hype bubble maintained by companies that have invested so much money in it they can't afford to back down. ROI delivery is minimal. Overwhelming number of projects fail.  \n\nIt reminds me of the mini hype bubble when people were trying to shoehorn blockchain into any number of inappropriate use cases which went nowhere, except it's 100x worse and is costing the industry 1000x more. \n\nContext: I work for a major AI platform vendor.",
          "score": 1,
          "created_utc": "2026-01-09 08:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyrvap",
          "author": "Much-Researcher6135",
          "text": "hush now, people will begin to think it's a bubble\n\nwouldn't want that",
          "score": 1,
          "created_utc": "2026-01-06 06:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3dpzw",
              "author": "zipzag",
              "text": "it's not a bubble. He's reviewing local models for some unspecified use",
              "score": 3,
              "created_utc": "2026-01-06 22:38:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxjt0j",
          "author": "Terminator857",
          "text": "More reliable than humans for me.",
          "score": 1,
          "created_utc": "2026-01-06 01:57:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx75c7",
          "author": "writesCommentsHigh",
          "text": "okay now compare it to frontier models via api.",
          "score": -1,
          "created_utc": "2026-01-06 00:49:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx8qj4",
              "author": "Armageddon_80",
              "text": "Yes I know, but I've posted on local LLM for a reason.",
              "score": 7,
              "created_utc": "2026-01-06 00:57:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0e2tt",
                  "author": "writesCommentsHigh",
                  "text": "And your title reads “all llms”\n\nYes local llms are going to be unreliable but not all",
                  "score": 1,
                  "created_utc": "2026-01-06 14:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxx9xk7",
              "author": "ispiele",
              "text": "Most of these points apply to the latest API models as well (in my experience), the first 3 points in particular",
              "score": 5,
              "created_utc": "2026-01-06 01:04:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxe2gz",
                  "author": "ANTIVNTIANTI",
                  "text": "Anthropic is managing a cult apparently",
                  "score": 2,
                  "created_utc": "2026-01-06 01:26:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxaqdl",
                  "author": "StardockEngineer",
                  "text": "Not to Sonnet and Opus.",
                  "score": 1,
                  "created_utc": "2026-01-06 01:08:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyakrtg",
          "author": "Purple-Programmer-7",
          "text": "You’re the problem.\n\nYour expectations are off.\n\nThink of it as a person that can make mistakes instead of a computer.",
          "score": 0,
          "created_utc": "2026-01-07 23:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyav9ch",
              "author": "Armageddon_80",
              "text": "Thanks the lord, you clearly figured it all out. I'm gonna apply your technical suggestion.",
              "score": 0,
              "created_utc": "2026-01-07 23:58:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxv8u6",
          "author": "alphatrad",
          "text": "Sounds like a skill issue",
          "score": -4,
          "created_utc": "2026-01-06 02:59:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzdjoc",
      "title": "Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware – Full Optimization Guide",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xdj2zjnz5bag1.png",
      "author": "at0mi",
      "created_utc": "2025-12-30 09:14:01",
      "score": 143,
      "num_comments": 22,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzdjoc/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwpcpjs",
          "author": "Lxzan",
          "text": "Nice work and thanks for sharing! How much is the power draw?",
          "score": 18,
          "created_utc": "2025-12-30 09:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpd7l0",
              "author": "at0mi",
              "text": "about 1600W i will update my blogpost with detailed power draw\n\nUPDATE: the 1600W was at higher Thread count, with the optimum 64 Threads im at 1154W\n\nIdle is 694W",
              "score": 21,
              "created_utc": "2025-12-30 09:31:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwph0z3",
                  "author": "Amazing_Athlete_2265",
                  "text": "Oof, that's gonna cost a bunch to run",
                  "score": 17,
                  "created_utc": "2025-12-30 10:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqiawc",
                  "author": "mister2d",
                  "text": "Those are some expensive tokens.",
                  "score": 3,
                  "created_utc": "2025-12-30 14:36:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqwrjk",
                  "author": "resil_update_bad",
                  "text": "jesus",
                  "score": 1,
                  "created_utc": "2025-12-30 15:50:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpoy5i",
          "author": "beedunc",
          "text": "Yes, been running the qwen3coder480b@q3 (245GB) on an old Dell T5810 running a single e5-2697v4, gets 2-3 tps. \n\nPower draw is only 250w under load. \n\nI never thought of disabling hyper threading, does that help a lot? Will be checking this out, thank you.",
          "score": 8,
          "created_utc": "2025-12-30 11:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwppbbm",
              "author": "at0mi",
              "text": "i also tried lower quantisation models but the quality of the output was crap, works for a chatbot but not for coding, \nin my case (8 numa nodes) disabling hyper threading gave an enormous boost",
              "score": 6,
              "created_utc": "2025-12-30 11:22:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwsp871",
              "author": "Candid_Highlight_116",
              "text": "If LLM inference is RAM bandwidth bound and HT was a tech to halve effective bandwidth because two virtual cores needs their respective data, it makes sense that turning off HT gives massive boost  \n\nsorry that's my hallucination but if it's actually like that it's pretty interesting",
              "score": 4,
              "created_utc": "2025-12-30 20:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu51bd",
                  "author": "beedunc",
                  "text": "Cool, thanks for the suggestion!",
                  "score": 1,
                  "created_utc": "2025-12-31 01:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpkj5q",
          "author": "xgiovio",
          "text": "Watts",
          "score": 7,
          "created_utc": "2025-12-30 10:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwql1ld",
              "author": "MaverickPT",
              "text": "All of them",
              "score": 7,
              "created_utc": "2025-12-30 14:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqd59a",
          "author": "chafey",
          "text": "I love hacks like this - nice work.  The hardware may be cheap/free but the electricity won't be...",
          "score": 6,
          "created_utc": "2025-12-30 14:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq21y3",
          "author": "Such_Advantage_6949",
          "text": "I am passionate about running llm at usable speed..",
          "score": 5,
          "created_utc": "2025-12-30 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws7ver",
          "author": "Foreign-Watch-3730",
          "text": "Same result ( 5.1 t/s ) , but in IQ5\\_K with 2 Xeon E5 2696 V4 and 400 Gb ddr4 ram ( on a very olf Dell T630 ) with 2 RTX 5090 ( ik\\_llama.cpp and [ubergarm](https://huggingface.co/ubergarm) for use opencode :  \n[https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5](https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5)\n\nnumactl --interleave=all ./build/bin/llama-server  \n\\--model \\~/ik\\_llama.cpp/models/GLM-4.7-Ubergarm/IQ5\\_K/GLM-4.7-IQ5\\_K-00001-of-00007.gguf  \n\\--alias GLM-4.7-IQ5  \n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8080  \n\\--ctx-size 84992  \n\\--no-mmap  \n\\--threads 82 --threads-batch 82  \n\\--batch-size 1024 --ubatch-size 1024  \n\\--parallel 1 --flash-attn 1  \n\\--jinja --verbose  \n\\--n-gpu-layers 99  \n\\--tensor-split 0.5,0.5  \n\\--split-mode layer  \n\\--run-time-repack  \n\\--cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--k-cache-hadamard  \n\\-ot 'blk.\\[0-8\\]..\\*exps.weight=CUDA0'  \n\\-ot 'blk.(8\\[6-9\\]|9\\[0-2\\])..\\*exps.weight=CUDA1'  \n\\-ot '.\\*exps.weight=CPU'",
          "score": 3,
          "created_utc": "2025-12-30 19:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqj2yx",
          "author": "Extension-Cow2818",
          "text": "Very interesting that turning hyperthreading off works better.  \nProbably memory access is causing issues in these types of workloads.\n\nIt would be also interesting to try MLK vs ATLAS vs BLAS",
          "score": 2,
          "created_utc": "2025-12-30 14:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq772g",
          "author": "Icy_Programmer7186",
          "text": "That's cool.  \nHow much memory did it consumed?",
          "score": 1,
          "created_utc": "2025-12-30 13:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqh2cl",
          "author": "ForsookComparison",
          "text": "This is very cool, thank you for testing this out.\n\nI'm curious what the use-case is? Is GLM decent as a general purpose model? Or will you give it a coding task and come back after a few hours",
          "score": 1,
          "created_utc": "2025-12-30 14:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdqlaq",
          "author": "spacefarers",
          "text": "Comes down to around $9 per million tokens just for electricity not sure if it's worth it lol",
          "score": 1,
          "created_utc": "2026-01-03 04:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyogyg",
          "author": "PitifulBall3670",
          "text": "Good job",
          "score": 1,
          "created_utc": "2026-01-06 06:11:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrv2is",
          "author": "Free_Manner_2318",
          "text": "7200 Watt for 5 tokens eh?!   \nAsk it if it was a reasonable decision.... :))))  \nB+ for the effort though. Not custom enough to be significant.",
          "score": 0,
          "created_utc": "2025-12-30 18:29:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py1gvp",
      "title": "Probably more true than I would like to admit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/iann5fyl70ag1.png",
      "author": "low_v2r",
      "created_utc": "2025-12-28 20:24:33",
      "score": 137,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1gvp/probably_more_true_than_i_would_like_to_admit/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwf8lgr",
          "author": "SunshineSeattle",
          "text": "I bought a particle tacyon, but now i have no idea what to do with it. 😭",
          "score": 9,
          "created_utc": "2025-12-28 20:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfk71i",
          "author": "Healthy-Nebula-3603",
          "text": "WHAT AN EDGE DEVICE ?",
          "score": 5,
          "created_utc": "2025-12-28 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfm8iu",
              "author": "Count_Rugens_Finger",
              "text": "honestly not sure if this is serious but if it is, it just means the device-in-hand of the users (phones, tablets, PCs, car dashboards, POS systems, and whatnot)",
              "score": 7,
              "created_utc": "2025-12-28 21:35:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksnf2",
                  "author": "QuinQuix",
                  "text": "Usually this refers to devices that are compute restrained.\n\nA user could have a beast of a pc workstation that technically lives on the edge (of the central cloud workspace) but it's not a usual thing to refer to compute strong devices as edge devices.",
                  "score": 2,
                  "created_utc": "2025-12-29 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj6kgc",
              "author": "trmnl_cmdr",
              "text": "https://preview.redd.it/wl5ul68kv4ag1.jpeg?width=349&format=pjpg&auto=webp&s=38e1efd5e4769de7bcf9d7c4bf5ae45a353fefb4\n\nThis, apparently. Although I’m not sure how this helps me masturbate.",
              "score": 4,
              "created_utc": "2025-12-29 12:05:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1g3yx",
                  "author": "jgwinner",
                  "text": "Please rev it up a bit ... now tilt it so the blade scrapes on the stone ...\n\nthat's it ...\n\nFASTER \n\nthat's IT ... IT ...\n\nuhhh.\n\nWhew. Could you shut that thing off? kthks.",
                  "score": 1,
                  "created_utc": "2026-01-01 05:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwua4fa",
              "author": "Jackuarren",
              "text": "The device that you use for edging, obviously.",
              "score": 1,
              "created_utc": "2025-12-31 01:51:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfaevi",
          "author": "Individual_Holiday_9",
          "text": "Unironically this",
          "score": 2,
          "created_utc": "2025-12-28 20:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfid3y",
          "author": "GCoderDCoder",
          "text": "I feel so seen!!!",
          "score": 1,
          "created_utc": "2025-12-28 21:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt8rsj",
          "author": "mobileJay77",
          "text": "Your solution: \n1. Head over the r/localLlama\n2. You hardly sleep any more\n3. She doesn't have to worry what you are thinking in bed.",
          "score": 1,
          "created_utc": "2025-12-30 22:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7b54w",
      "title": "Hugging Face on Fire: 30+ New/Trending Models (LLMs, Vision, Video) w/ Links",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q7b54w/hugging_face_on_fire_30_newtrending_models_llms/",
      "author": "techlatest_net",
      "created_utc": "2026-01-08 12:59:23",
      "score": 87,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "Hugging Face is on fire right now with these newly released and trending models across text gen, vision, video, translation, and more. Here's a full roundup with direct links and quick breakdowns of what each one crushes—perfect for your next agent build, content gen, or edge deploy.\n\n# Text Generation / LLMs\n\n* **tencent/HY-MT1.5-1.8B** (Translation- 2B- 7 days ago): Edge-deployable 1.8B multilingual translation model supporting 33+ languages (incl. dialects like Tibetan, Uyghur). Beats most commercial APIs in speed/quality after quantization; handles terminology, context, and formatted text.​ [tencent/HY-MT1.5-1.8B](https://huggingface.co/tencent/HY-MT1.5-1.8B)\n* **LGAI-EXAONE/K-EXAONE-236B-A23B** (Text Generation- 237B- 2 days ago): Massive Korean-focused LLM for advanced reasoning and generation tasks.​[K-EXAONE-236B-A23B](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B)\n* **IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct** (Text Generation- 40B- 21 hours ago): Coding specialist with loop-based instruction tuning for iterative dev workflows.​[IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct)\n* **IQuestLab/IQuest-Coder-V1-40B-Instruct** (Text Generation- 40B- 5 days ago): General instruct-tuned coder for programming and logic tasks.​[IQuestLab/IQuest-Coder-V1-40B-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct)\n* **MiniMaxAI/MiniMax-M2.1** (Text Generation- 229B- 12 days ago): High-param MoE-style model for complex multilingual reasoning.​[MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)\n* **upstage/Solar-Open-100B** (Text Generation- 103B- 2 days ago): Open-weight powerhouse for instruction following and long-context tasks.​[upstage/Solar-Open-100B](https://huggingface.co/upstage/Solar-Open-100B)\n* **zai-org/GLM-4.7** (Text Generation- 358B- 6 hours ago): Latest GLM iteration for top-tier reasoning and Chinese/English gen.​[zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)\n* **tencent/Youtu-LLM-2B** (Text Generation- 2B- 1 day ago): Compact LLM optimized for efficient video/text understanding pipelines.​[tencent/Youtu-LLM-2B](https://huggingface.co/tencent/Youtu-LLM-2B)\n* **skt/A.X-K1** (Text Generation- 519B- 1 day ago): Ultra-large model for enterprise-scale Korean/English tasks.​[skt/A.X-K1](https://huggingface.co/skt/A.X-K1)\n* **naver-hyperclovax/HyperCLOVAX-SEED-Think-32B** (Text Generation- 33B- 2 days ago): Thinking-augmented LLM for chain-of-thought reasoning.​[naver-hyperclovax/HyperCLOVAX-SEED-Think-32B](https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B)\n* **tiiuae/Falcon-H1R-7B** (Text Generation- 8B- 1 day ago): Falcon refresh for fast inference in Arabic/English.​[tiiuae/Falcon-H1R-7B](https://huggingface.co/tiiuae/Falcon-H1R-7B)\n* **tencent/WeDLM-8B-Instruct** (Text Generation- 8B- 7 days ago): Instruct-tuned for dialogue and lightweight deployment.​[tencent/WeDLM-8B-Instruct](https://huggingface.co/tencent/WeDLM-8B-Instruct)\n* **LiquidAI/LFM2.5-1.2B-Instruct** (Text Generation- 1B- 20 hours ago): Tiny instruct model for edge AI agents.​[LiquidAI/LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)\n* **miromind-ai/MiroThinker-v1.5-235B** (Text Generation- 235B- 2 days ago): Massive thinker for creative ideation.​[miromind-ai/MiroThinker-v1.5-235B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B)\n* **Tongyi-MAI/MAI-UI-8B** (9B- 10 days ago): UI-focused gen for app prototyping.​[Tongyi-MAI/MAI-UI-8B](https://huggingface.co/Tongyi-MAI/MAI-UI-8B)\n* **allura-forge/Llama-3.3-8B-Instruct** (8B- 8 days ago): Llama variant tuned for instruction-heavy workflows.​[allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)\n\n# Vision / Image Models\n\n* **Qwen/Qwen-Image-2512** (Text-to-Image- 8 days ago): Qwen's latest vision model for high-fidelity text-to-image gen.​[Qwen/Qwen-Image-2512](https://huggingface.co/Qwen/Qwen-Image-2512)\n* **unsloth/Qwen-Image-2512-GGUF** (Text-to-Image- 20B- 1 day ago): Quantized GGUF version for local CPU/GPU runs.​[unsloth/Qwen-Image-2512-GGUF](https://huggingface.co/unsloth/Qwen-Image-2512-GGUF)\n* **Wuli-art/Qwen-Image-2512-Turbo-LoRAT** (Text-to-Image- 4 days ago): Turbo LoRA adapter for faster Qwen image gen.​[Wuli-art/Qwen-Image-2512-Turbo-LoRA](https://huggingface.co/Wuli-art/Qwen-Image-2512-Turbo-LoRA)\n* **lightx2v/Qwen-Image-2512-Lightning** (Text-to-Image- 2 days ago): Lightning-fast inference variant.​[lightx2v/Qwen-Image-2512-Lightning](https://huggingface.co/lightx2v/Qwen-Image-2512-Lightning)\n* **Phr00t/Qwen-Image-Edit-Rapid-AIO** (Text-to-Image- 4 days ago): All-in-one rapid image editor.​[Phr00t/Qwen-Image-Edit-Rapid-AIO](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO)\n* **lilylilith/AnyPose** (Image-to-Image- 6 days ago): Pose transfer and manipulation tool.​[lilylilith/AnyPose](https://huggingface.co/lilylilith/AnyPose)\n* **fal/FLUX.2-dev-Turbo** (Text-to-Image- 9 days ago): Turbocharged Flux for quick high-quality images.​[fal/FLUX.2-dev-Turbo](https://huggingface.co/fal/FLUX.2-dev-Turbo)\n* **Tongyi-MAI/Z-Image-Turbo** (Text-to-Image- 1 day ago): Turbo image gen with strong prompt adherence.​[Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)\n* **inclusionAI/TwinFlow-Z-Image-Turbo** (Text-to-Image- 10 days ago): Flow-based turbo variant for stylized outputs.​[inclusionAI/TwinFlow-Z-Image-Turbo](https://huggingface.co/inclusionAI/TwinFlow-Z-Image-Turbo)\n\n# Video / Motion\n\n* **Lightricks/LTX-2** (Image-to-Video- 2 hours ago): DiT-based joint audio-video foundation model for synced video+sound gen from images/text. Supports upscalers for higher res/FPS; runs locally via ComfyUI/Diffusers.​[Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)\n* **tencent/HY-Motion-1.0** (Text-to-3D- 8 days ago): Motion capture to 3D model gen.​[tencent/HY-Motion-1.0](https://huggingface.co/tencent/HY-Motion-1.0)\n\n# Audio / Speech\n\n* **nvidia/nemotron-speech-streaming-en-0.6b** (Automatic Speech Recognition- 2 days ago): Streaming ASR for real-time English transcription.​[nvidia/nemotron-speech-streaming-en-0.6b](https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b)\n* **LiquidAI/LFM2.5-Audio-1.5B** (Audio-to-Audio- 1B- 2 days ago): Audio effects and transformation model.​[LiquidAI/LFM2.5-Audio-1.5B](https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B)\n\n# Other Standouts\n\n* **nvidia/Alpamayo-R1-10B** (11B- Dec 4, 2025): Multimodal reasoning beast. [nvidia/Alpamayo-R1-10B](https://huggingface.co/nvidia/Alpamayo-R1-10B)\n\nDrop your benchmarks, finetune experiments, or agent integrations below—which one's getting queued up first in your stack?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7b54w/hugging_face_on_fire_30_newtrending_models_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nye4m8d",
          "author": "alex_godspeed",
          "text": "i scan thru all of them. Still thinking Qwen 3 30B is 'just right' for my business reasoning use case for 16G GPU. Or am I missing anything? Need it on GGUF LM Studio though",
          "score": 10,
          "created_utc": "2026-01-08 13:12:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyek04x",
              "author": "Ok-Employment6772",
              "text": "Qwen3 is amazing",
              "score": 3,
              "created_utc": "2026-01-08 14:34:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyrbjs9",
              "author": "swardy_404",
              "text": "Qwen 3 30B on 16GB GPU?",
              "score": 1,
              "created_utc": "2026-01-10 09:07:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrbo5l",
                  "author": "alex_godspeed",
                  "text": "With Q4 KM hehe. I'm eyeing that VL but I needs 20GB before counting contexts :(",
                  "score": 1,
                  "created_utc": "2026-01-10 09:08:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyipomj",
          "author": "PitifulBall3670",
          "text": "Thanks for sharing! This is very helpful",
          "score": 2,
          "created_utc": "2026-01-09 02:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyljd2t",
          "author": "beast_modus",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-01-09 14:08:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5m07h",
      "title": "Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/viqj8u1k1rbg1.png",
      "author": "A-Rahim",
      "created_utc": "2026-01-06 15:42:37",
      "score": 73,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q5m07h/unslothmlx_finetune_llms_on_your_mac_same_api_as/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "ny4wd85",
          "author": "Repeat_Admirable",
          "text": "MLX is seriously a godsend for Apple Silicon. It's wild how much power these M-chips have for inference when optimized correctly. I switched my entire writing workflow to a local Whisper-based app (running on CoreML/Metal) and it essentially uses zero CPU while transcribing in real-time. Great to see Unsloth coming to the ecosystem. The more native Mac tools, the better.",
          "score": 4,
          "created_utc": "2026-01-07 03:27:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1kecb",
          "author": "onethousandmonkey",
          "text": "Noob here:\nWhat are the pros and cons of fine-tuning on Mac compared to other types of local hardware?\nAnd does this project enhance/offset some of those?\n\nEdit: typo, clarification",
          "score": 2,
          "created_utc": "2026-01-06 17:39:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q06u1n",
      "title": "2025 is over. What were the best AI model releases this year?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "author": "techlatest_net",
      "created_utc": "2025-12-31 07:12:49",
      "score": 64,
      "num_comments": 32,
      "upvote_ratio": 0.88,
      "text": "2025 felt like three AI years compressed into one. Frontier LLMs went insane on reasoning, open‑source finally became “good enough” for a ton of real workloads, OCR and VLMs leveled up, and audio models quietly made agents actually usable in the real world. ​ Here’s a category‑wise recap of the “best of 2025” models that actually changed how people build stuff, not just leaderboard screenshots:\n\n\n\nLLMs and reasoning\n\n\\* GPT‑5.2 (Thinking / Pro) – Frontier‑tier reasoning and coding, very fast inference, strong for long‑horizon tool‑using agents and complex workflows.\n\n​\\* Gemini 3 Pro / Deep Think – Multi‑million token context and multimodal “screen reasoning”; excels at planning, code, and web‑scale RAG / NotebookLM‑style use cases. \n\n\\* Claude 4.5 (Sonnet / Opus) – Extremely strong for agentic tool use, structured step‑by‑step plans, and “use the computer for me” style tasks. \n\n\\* DeepSeek‑V3.2 & Qwen3‑Thinking – Open‑weight monsters that narrowed the gap with closed models to within \\\\\\~0.3 points on key benchmarks while being orders of magnitude cheaper to run.\n\nIf 2023–24 was “just use GPT,” 2025 finally became “pick an LLM like you pick a database.”\n\nVision, VLMs & OCR\n\n\\* MiniCPM‑V 4.5 – One of the strongest open multimodal models for OCR, charts, documents, and even video frames, tuned to run on mobile/edge while still hitting SOTA‑ish scores on OCRBench/OmniDocBench. \n\n\\* olmOCR‑2‑7B‑1025 – Allen Institute’s OCR‑optimized VLM, fine‑tuned from Qwen2.5‑VL, designed specifically for documents and long‑form OCR pipelines. \n\n\\* InternVL 2.x / 2.5‑4B – Open VLM family that became a go‑to alternative to closed GPT‑4V‑style models for document understanding, scene text, and multimodal reasoning.\n\n\\* Gemma 3 VLM & Qwen 2.5/3 VL lines – Strong open(-ish) options for high‑res visual reasoning, multilingual OCR, and long‑form video understanding in production‑style systems. ​ \n\n2025 might be remembered as the year “PDF to clean Markdown with layout, tables, and charts” stopped feeling like magic and became a boring API call.\n\nAudio, speech & agents\n\n\\* Whisper (still king, but heavily optimized) – Remained the default baseline for multilingual ASR in 2025, with tons of optimized forks and on‑device deployments. \n\n\\* Low‑latency real‑time TTS/ASR stacks (e.g., new streaming TTS models & APIs) – Sub‑second latency + streaming text/audio turned LLMs into actual real‑time voice agents instead of “podcast narrators.” \n\n\\* Many 2025 voice stacks shipped as APIs rather than single models: ASR + LLM + real‑time TTS glued together for call centers, copilots, and vibecoding IDEs. ​ Voice went from “cool demo” to “I talk to my infra/IDE/CRM like a human, and it answers back, live.”\n\nOCR/document AI & IDP\n\n\\* olmOCR‑2‑7B‑1025, MiniCPM‑V 4.5, InternVL 2.x, OCRFlux‑3B, PaddleOCR‑VL – A whole stack of open models that can parse PDFs into structured Markdown with tables, formulas, charts, and long multi‑page layouts. \n\n\\* On top of these, IDP / “PDF AI” tools wrapped them into full products for invoices, contracts, and messy enterprise docs. ​ \n\nIf your 2022 stack was “Tesseract + regex,” 2025 was “drop a 100‑page scan and get usable JSON/Markdown back.” ​ \n\nOpen‑source LLMs that actually mattered\n\n\\* DeepSeek‑V3.x – Aggressive MoE + thinking budgets + brutally low cost; a lot of people quietly moved internal workloads here. \n\n\\* Qwen3 family – Strong open‑weight reasoning, multilingual support, and specialized “Thinking” variants that became default self‑host picks. \n\n\\* Llama 4 & friends – Closed the gap to within \\\\\\~0.3 points of frontier models on several leaderboards, making “fully open infra” a realistic choice for many orgs.\n\n​In 2025, open‑source didn’t fully catch the frontier, but for a lot of teams, it crossed the “good enough + cheap enough” threshold.\n\nYour turn This list is obviously biased toward models that:\n\n\\* Changed how people build products (agents, RAG, document workflows, voice UIs)\n\n\\* Have public benchmarks, APIs, or open weights that normal devs can actually touch ​- What did you ship or adopt in 2025 that deserves “model of the year” status?\n\nFavorite frontier LLM?\n\n\\* Favorite open‑source model you actually self‑hosted?\n\n\\* Best OCR / VLM / speech model that saved you from pain?\n\n\\* Drop your picks below so everyone can benchmark / vibe‑test them going into 2026.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwvo5t9",
          "author": "Clipbeam",
          "text": "I vote the Qwen3 models. Total game changer for local LLMs.",
          "score": 37,
          "created_utc": "2025-12-31 07:35:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxs12w5",
              "author": "techlatest_net",
              "text": "Totally Agree",
              "score": 1,
              "created_utc": "2026-01-05 07:03:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvohey",
          "author": "DutchSEOnerd",
          "text": "Overall I would says Claude Opus had most impact, for local its definitely Qwen",
          "score": 13,
          "created_utc": "2025-12-31 07:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy6e07",
              "author": "ForsookComparison",
              "text": "+1 for Opus. Benchmarks don't show its impact properly. The thing dropping down to $25/1m output tokens led to me not needing to hand-write a line of code for the last month of this year in my biggest repos.",
              "score": 3,
              "created_utc": "2025-12-31 17:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3hp62",
                  "author": "DutchSEOnerd",
                  "text": "Thats also the feeling I have. Benchmarks only tell part of the story",
                  "score": 2,
                  "created_utc": "2026-01-01 16:12:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxs1nyo",
                  "author": "techlatest_net",
                  "text": "Yeah, exactly – leaderboards don’t really capture “I stopped hand-writing code in my main repos for a month straight.”\n​\nThe $25/1M output tier basically turned Opus into a drop-in senior engineer for a lot of people: long refactors, multi-file edits, and architecture changes went from “painful weekend project” to “one good prompt and a quick review pass.”\n​\nCurious how you wired it in – are you mostly using it through an AI IDE, custom scripts, or a homegrown agent flow?",
                  "score": 1,
                  "created_utc": "2026-01-05 07:08:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs1jtv",
              "author": "techlatest_net",
              "text": "Yeah, Opus was kind of the first “I can trust this to run my whole workflow” model for a lot of people – especially for tool-use and multi-step plans.\n​\nAnd agreed on Qwen for local: the 2.5/3 “Thinking” and VL variants basically became the default self-hosted stack once people realized how far you could push them on a single GPU.",
              "score": 1,
              "created_utc": "2026-01-05 07:07:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwy6olb",
          "author": "ForsookComparison",
          "text": "Deepseek R1 probably had the biggest impact. Put the fear of God into Western companies that were operating as if they had a magical moat. Before that there was legislation being seriously considered to put a halt to A.I. progress for a few months to years.",
          "score": 7,
          "created_utc": "2025-12-31 17:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxs1sgc",
              "author": "techlatest_net",
              "text": "Yeah, R1 felt less like “just another model” and more like a geopolitical event – suddenly every frontier lab and regulator had to assume that frontier‑level reasoning could appear from any region, not just the usual suspects.\n​\nThe funny part is how fast the narrative flipped: from “we need to pause progress” to “we can’t afford to pause while others sprint,” which probably did more to kill the idea of a global slowdown than any policy paper.\n​\nCurious if you actually used R1 in any real workflows, or if its impact for you was mostly the shockwave it sent through the ecosystem.",
              "score": 1,
              "created_utc": "2026-01-05 07:09:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxt7qu8",
                  "author": "ForsookComparison",
                  "text": "> Curious if you actually used R1 in any real workflows, or if its impact for you was mostly the shockwave it sent through the ecosystem.\n\nUsed it absolutely everywhere. The price, especially off peak hours, made Sonnet and ChatGPT look like jokes.",
                  "score": 1,
                  "created_utc": "2026-01-05 13:08:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvnxf6",
          "author": "AllTheCoins",
          "text": "I love Qwen3-30b as a go-to all around model",
          "score": 9,
          "created_utc": "2025-12-31 07:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwz4f3w",
              "author": "Count_Rugens_Finger",
              "text": "yeah definitely.  In this thread there is very clearly two worlds coming into contact here.  The people with purpose-made rigs and the people with consumer grade hardware",
              "score": 2,
              "created_utc": "2025-12-31 20:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxs1z46",
                  "author": "techlatest_net",
                  "text": "And yeah, this thread really shows the split between people on big dedicated rigs and people squeezing the most out of consumer GPUs.",
                  "score": 1,
                  "created_utc": "2026-01-05 07:11:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs1yl8",
              "author": "techlatest_net",
              "text": "Qwen3‑30B is kind of the sweet spot right now – good enough at almost everything without feeling like a science project to run.",
              "score": 1,
              "created_utc": "2026-01-05 07:11:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwa6rb",
          "author": "InfiniteTrans69",
          "text": "Qwen3, Kimi K2, Minimax M2.",
          "score": 3,
          "created_utc": "2025-12-31 11:03:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3duag",
              "author": "QuinQuix",
              "text": "What's the respective use cases for you with these three models?",
              "score": 2,
              "created_utc": "2026-01-01 15:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3fqe7",
                  "author": "InfiniteTrans69",
                  "text": "Translation and vision: Qwen  \nResearch and AI Slides: Kimi  \nEverything else: Minimax",
                  "score": 2,
                  "created_utc": "2026-01-01 16:02:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwpmc5",
          "author": "jinnyjuice",
          "text": "Might be useful to categorise by memory size and task groups",
          "score": 5,
          "created_utc": "2025-12-31 13:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxhgvw",
          "author": "leonbollerup",
          "text": "Open Source: GPT-oss-20b/120b",
          "score": 3,
          "created_utc": "2025-12-31 15:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwyuia",
          "author": "RiskyBizz216",
          "text": "Qwen3-Next specifically the 80B and 480B\n\nAnd Z-Image Turbo",
          "score": 2,
          "created_utc": "2025-12-31 14:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxfizo",
          "author": "BuffMcBigHuge",
          "text": "Open: Qwen3-30B\nClosed: Gemini 3 Pro + Nano Banana\n\nOpus 4.5 is stellar, but those two above blew me away.",
          "score": 2,
          "created_utc": "2025-12-31 15:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmwi4",
          "author": "Karyo_Ten",
          "text": "LLMs: GLM-4.5-Air, GLM-4.7, GPT-OSS (quality for the first 2, speed demon for GPT-OSS)\n\nPromising new arch: Kimi-Linear, Qwen-Next, MiMo-V2-Flash I believe 2026 will have Linear Attention everywhere.\n\nSpecialized: Minimax-M2.1\n\nMultimodal: Qwen3-Omni, GLM-4.6V\n\nOCR: Mineru",
          "score": 2,
          "created_utc": "2025-12-31 16:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0g87s",
          "author": "karmakaze1",
          "text": "Nemotron-3-Nano for its use of hybrid Mamba-Transformer that reduces memory and computation for large context.\n\nAlso Qwen3-Next (for \"Gated DeltaNet\") and Kimi K2 (for advancing MLA further).",
          "score": 2,
          "created_utc": "2026-01-01 01:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmu7yj",
          "author": "thedarkbobo",
          "text": "Qwen3, iQuest, Deepseek, OSS, gemma3, nemotron",
          "score": 2,
          "created_utc": "2026-01-04 15:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvpnzh",
          "author": "Lissanro",
          "text": "You missed to mention Kimi K2 0905 and Kimi K2 Thinking. These are the models I run the most on my PC (IQ4 and Q4_X quants respectively, using ik_llama.cpp). K2 Thinking is especially notable for its QAT INT4 release which maps nicely to Q4_X in the GGUF format, preserving the original quality.\n\nFor both, common expert tensors and 256K context cache at Q8 fit fully in 96GB VRAM, making them excellent for CPU+GPU inference.\n\n\nAs of DeepSeek V3.2, I did not get to try it due to lack of support in both ik_llama.cpp and llama.cpp. There is work in progress to add its architecture but not going to make it this year.",
          "score": 3,
          "created_utc": "2025-12-31 07:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwa88j",
              "author": "Tema_Art_7777",
              "text": "Lord! From unsloth:\n\n“It is recommended to have 247 GB of RAM to run the 1-bit Dynamic GGUF.\nTo run the model in full precision, you can use 'UD-Q4_K_XL', which requires 646 GB RAM.”",
              "score": 1,
              "created_utc": "2025-12-31 11:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwislo",
                  "author": "Lissanro",
                  "text": "Even though Unsloth generally makes good quants, in this case it is the best to use Q4\\_X from [https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF) because it preserves the original INT4 quality in 543.617 GiB size, while UD-Q4\\_K\\_XL would be bigger, slower and likely of a bit lower quality.\n\nThat said, yes, your estimates of necessary RAM are accurate, for IQ1 256 GB RAM is needed and almost all of it will be used by the model, leaving very little for the OS; for IQ3 512 GB, and for Q4\\_X around 640 GB is needed if you include full cache size. K2 Thinking works best high RAM rigs or with high GPU count, like twenty MI50 cards (for 640 GB VRAM in total) - I actually considered getting that much since at the time I looked into it, it was possible for half the price of RTX PRO 6000 and my motherboard could carry them all at PCI-E 4.0 x4 speed each, but I decided to stay with 4x3090 for now (because even though full VRAM inference would be faster, my current performance with 1 TB RAM + 96 GB VRAM is still acceptable to me, and a lot of what I do requires Nvidia cards, and not just LLMs either).",
                  "score": 2,
                  "created_utc": "2025-12-31 12:17:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvqtnu",
          "author": "PeakBrave8235",
          "text": "At the end of the day they all are BS, open or closed source idgaf.",
          "score": -11,
          "created_utc": "2025-12-31 08:00:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxga7w",
              "author": "Count_Rugens_Finger",
              "text": "so why are you even here?",
              "score": 2,
              "created_utc": "2025-12-31 15:40:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1u966",
      "title": "I designed a Private local AI for Android - has internet search, personas and more.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/jk6meo3wmwag1",
      "author": "Sebulique",
      "created_utc": "2026-01-02 09:25:58",
      "score": 64,
      "num_comments": 20,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx8c8zl",
          "author": "Themash360",
          "text": "Nice work, can it connect to local llm as well? So not just on device but custom endpoints",
          "score": 1,
          "created_utc": "2026-01-02 10:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8cp5h",
              "author": "Sebulique",
              "text": "You can, I've left it completely open, I also allowed it to work with smart bulbs, It works but sometimes it decides to do it's own thing unless I set parameters.\n\nI've turned it off after I broke it, but I want to reintroduce it again soon",
              "score": 3,
              "created_utc": "2026-01-02 10:18:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx982sn",
          "author": "theCatchiest20Too",
          "text": "Very cool! Are you tracking performance against hardware specs?",
          "score": 1,
          "created_utc": "2026-01-02 14:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9mbso",
              "author": "Sebulique",
              "text": "I was thinking of a toggle called \"stats for geeks\" but I wanted to make it look super easy for anyone, almost Chatgpt like, so more people feel comfortable using local llms",
              "score": 2,
              "created_utc": "2026-01-02 15:31:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx9j4l9",
          "author": "False-Ad-1437",
          "text": "governor work shocking tie dime frame special degree lip vase\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",
          "score": 1,
          "created_utc": "2026-01-02 15:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa6go5",
              "author": "Sebulique",
              "text": "Thank you man, I appreciate you, I'm looking to make it easy and accessible for all",
              "score": 1,
              "created_utc": "2026-01-02 17:06:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdas2x",
          "author": "2QNTLN",
          "text": ">will upload soon on GitHub. \n\nHow soon is soon? Super interested in this project so i had to ask.",
          "score": 1,
          "created_utc": "2026-01-03 02:39:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2xoxg",
              "author": "Sebulique",
              "text": "It's up man!",
              "score": 1,
              "created_utc": "2026-01-06 21:23:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxe9nbn",
          "author": "tankandwb",
          "text": "RemindMe! 1 week",
          "score": 1,
          "created_utc": "2026-01-03 06:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe9qy4",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-01-10 06:34:58 UTC**](http://www.wolframalpha.com/input/?i=2026-01-10%2006:34:58%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/nxe9nbn/?context=3)\n\n[**4 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLM%2Fcomments%2F1q1u966%2Fi_designed_a_private_local_ai_for_android_has%2Fnxe9nbn%2F%5D%0A%0ARemindMe%21%202026-01-10%2006%3A34%3A58%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q1u966)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-03 06:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcrxs",
          "author": "Latter_Virus7510",
          "text": "Coolsieees! Amazing job there boss! When do we get to download?",
          "score": 1,
          "created_utc": "2026-01-03 18:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx461s",
              "author": "Sebulique",
              "text": "Now! Enjoy https://github.com/Rawxia/SebbiAI",
              "score": 2,
              "created_utc": "2026-01-06 00:33:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxyt4vf",
                  "author": "Latter_Virus7510",
                  "text": "First time launched and imported a Gemma 3 model, I tried to send a message but the model crashed instantly. It's been crashing since.",
                  "score": 1,
                  "created_utc": "2026-01-06 06:50:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhkf1u",
          "author": "Benschman1979",
          "text": "Great work! I'm looking forward to testing it.",
          "score": 1,
          "created_utc": "2026-01-03 19:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx42bu",
              "author": "Sebulique",
              "text": "It's out now! https://github.com/Rawxia/SebbiAI",
              "score": 1,
              "created_utc": "2026-01-06 00:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx443u",
          "author": "Sebulique",
          "text": "It's now released everyone! \nhttps://github.com/Rawxia/SebbiAI",
          "score": 1,
          "created_utc": "2026-01-06 00:33:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "ny5ac3c",
          "author": "PitifulBall3670",
          "text": "Interesting logic",
          "score": 1,
          "created_utc": "2026-01-07 04:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6zwzp",
          "author": "caiqueZ102",
          "text": "I really liked what I was seeing; the more modern, the better.",
          "score": 1,
          "created_utc": "2026-01-07 13:13:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdiwdh",
      "title": "Oh Dear",
      "subreddit": "LocalLLM",
      "url": "https://i.imgur.com/3UY6yoB.png",
      "author": "bamburger",
      "created_utc": "2026-01-15 13:01:56",
      "score": 58,
      "num_comments": 28,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qdiwdh/oh_dear/",
      "domain": "i.imgur.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzpzy10",
          "author": "mp3m4k3r",
          "text": "Might want to check the tuning parameters like temperature match with what the model is recommended to use.",
          "score": 20,
          "created_utc": "2026-01-15 13:10:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw4ljm",
              "author": "mxforest",
              "text": "This should be standard with the model weights. Why second guess. Why not have a config file with all \"best\" settings preapplied?",
              "score": 2,
              "created_utc": "2026-01-16 09:27:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq9664",
          "author": "iamzooook",
          "text": "while the system prompt. \"only reply with continuous\"the\"\"",
          "score": 25,
          "created_utc": "2026-01-15 14:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq1b70",
          "author": "ScoreUnique",
          "text": "I suggest trying pocket pal, allows loading gguf files",
          "score": 5,
          "created_utc": "2026-01-15 13:18:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq67p0",
          "author": "l_Mr_Vader_l",
          "text": "check if the model needs a system prompt?\n\nsome models just don't work without a system prompt",
          "score": 6,
          "created_utc": "2026-01-15 13:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvhfaz",
              "author": "Much-Researcher6135",
              "text": "Is there any big spreadsheet of models' recommended settings somewhere?",
              "score": 1,
              "created_utc": "2026-01-16 06:05:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvhukc",
                  "author": "l_Mr_Vader_l",
                  "text": "I wouldn't think so. mostly you get the info for a given model from huggingface usage snippets and config files in the repo",
                  "score": 2,
                  "created_utc": "2026-01-16 06:08:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqsanu",
          "author": "HealthyCommunicat",
          "text": "Noone’s recommending the most obvious solution that you should be trying - raise your repeat penalty. Start at 1.1 and go higher. Make sure your model isnt forced to use more experts than regularly recommended. \n\nThese two are usually the top actual real most common reasons as to why local llm’s do this",
          "score": 4,
          "created_utc": "2026-01-15 15:36:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv4pzz",
              "author": "pieonmyjesutildomine",
              "text": "Was writing this comment, excellent job knowing your knowledge",
              "score": 2,
              "created_utc": "2026-01-16 04:36:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00zfk5",
              "author": "Confident-Ad-3212",
              "text": "Repeat penalty is not going to solve this. Zero chance. It comes from other issues",
              "score": 1,
              "created_utc": "2026-01-17 00:58:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o00zwbp",
                  "author": "HealthyCommunicat",
                  "text": "id appreciate it u told me what u think it might be so that its something i can at least keep in mind next time this happens to me.",
                  "score": 1,
                  "created_utc": "2026-01-17 01:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt3737",
          "author": "rinaldo23",
          "text": "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the 2",
          "score": 4,
          "created_utc": "2026-01-15 21:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt7hih",
              "author": "hugazebra",
              "text": "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the 3/912",
              "score": 3,
              "created_utc": "2026-01-15 22:12:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq2toq",
          "author": "Feztopia",
          "text": "Moreli",
          "score": 2,
          "created_utc": "2026-01-15 13:27:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq4rdl",
              "author": "I1lII1l",
              "text": "Oh dear",
              "score": 1,
              "created_utc": "2026-01-15 13:37:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzs4xkr",
                  "author": "FaceDeer",
                  "text": "1.",
                  "score": 1,
                  "created_utc": "2026-01-15 19:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqfwsl",
          "author": "low_v2r",
          "text": "alias ollama=\"yes the\"",
          "score": 1,
          "created_utc": "2026-01-15 14:36:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr1ld0",
          "author": "Sicarius_The_First",
          "text": "Ah, the classic \"didn't read the instructions, no idea why it won't work\"",
          "score": 1,
          "created_utc": "2026-01-15 16:18:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrepid",
              "author": "Witty_Mycologist_995",
              "text": "the the the the the the the previous the the",
              "score": 1,
              "created_utc": "2026-01-15 17:17:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztqzj9",
          "author": "BowtiedAutist",
          "text": "Stuttering sally",
          "score": 1,
          "created_utc": "2026-01-15 23:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuc9ih",
          "author": "Fit-Medicine-4583",
          "text": "The same thing happened to me on ollama. The issue was fixed by increasing the context length between 16k to 32k.",
          "score": 1,
          "created_utc": "2026-01-16 01:50:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyvpyq",
          "author": "C3H8_Tank",
          "text": "The lack of matrix jokes is sad.",
          "score": 1,
          "created_utc": "2026-01-16 18:40:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzr814",
          "author": "misha1350",
          "text": "Why don't you simply use Qwen3-1.7B in Q4",
          "score": 1,
          "created_utc": "2026-01-16 21:06:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00z04t",
          "author": "Confident-Ad-3212",
          "text": "You are having an issue with the stop in your template. Also, seems like you may have had duplicates or some other issue during training. Maybe too many samples or your LR, rank and or alpha was too high. I just went through this. I have just gotten through this exact issue. Try lowering LR and deduping first. If you ran too many epochs this can happen as well…. Try reducing epochs and LR after making sure you have removed all duplicates. Try with an LR of 1.5e-5 and then walk it up. Rank 32, alpha 64. If it continues, you don’t  have enough sample diversity. Try these first then walk up your learning strength until you find the expression you are looking for.  Chances are you have too many samples. That 1.5b can intake 500-2000 samples max. Anything less than 25 separate sample types can cause this…. Diversity is key.",
          "score": 1,
          "created_utc": "2026-01-17 00:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqav6q",
          "author": "Serious_Molasses313",
          "text": "Stop gooning to small models. That's SA",
          "score": -11,
          "created_utc": "2026-01-15 14:10:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q79orf",
      "title": "Been using glm 4.7 for coding instead of claude sonnet 4.5 and the cost difference is huge",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q79orf/been_using_glm_47_for_coding_instead_of_claude/",
      "author": "Affectionate-Cash709",
      "created_utc": "2026-01-08 11:45:03",
      "score": 50,
      "num_comments": 39,
      "upvote_ratio": 0.87,
      "text": "so ive been on claude sonnet 4.5 for like 14 months now mostly for coding work. debugging python scripts, generating react components, refactoring old code etc. its great but honestly the $20/month plus api costs when i need bulk operations was adding up\n\nsaw someone mention glm 4.7 from zhipu ai in a thread here last week. its open source and supposedly good for coding so i decided to test it for a week against my usual claude workflow\n\nwhat i tested:\n\n*   debugging flask apis with cryptic error messages\n    \n*   generating typescript interfaces from json\n    \n*   refactoring a messy 600 line python class\n    \n*   writing sql queries and optimizing them\n    \n*   explaining wtf is going on in legacy java code\n    \n\nhonestly went in expecting typical open source model jank where it gives you code that looks right but imports dont exist or logic breaks on edge cases\n\nbut glm actually delivered working code like 85-90% of the time. not perfect but way better than i expected\n\ni also tested it against deepseek and kimi since theyre in the same ballpark. deepseek is faster but sometimes misses context when files get long. kimi is solid but hit token limits faster than glm. glm just handled my 500+ line files without forgetting what variables were named\n\nthe biggest difference from sonnet 4.5:\n\n*   explanations are more technical and less \"friendly\" but i dont really care about that\n    \n*   code quality is surprisingly close for most tasks, like 80-85% of sonnet 4.5 output quality\n    \n*   way cheaper if youre using the api, like 1/5th the cost for similar results\n    \n\nwhere claude still wins:\n\n*   ui/ux obviously\n    \n*   better for brainstorming and high level architecture discussions\n    \n*   more polished responses when you need explanations\n    \n\nbut for pure coding grunt work? refactoring, debugging, generating boilerplate? glm is honestly good enough and the cost savings are real",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q79orf/been_using_glm_47_for_coding_instead_of_claude/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nydut3r",
          "author": "No_Conversation9561",
          "text": "what is your hardware?",
          "score": 5,
          "created_utc": "2026-01-08 12:09:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nye0hzy",
              "author": "j00cifer",
              "text": "I think he uses GLM through zAIs endpoint or something like that through subscription, not local model",
              "score": 8,
              "created_utc": "2026-01-08 12:47:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhf2rq",
                  "author": "BERLAUR",
                  "text": "The subscription is pretty awesome, its currently about 2-3 bucks per month (with all discounts) and it gives you a lot of tokens and some useful MCP servers. Only downside is that the concurrency for the 4.7 model is currently limited and that they prioritise people on higher tiers (but it's still more than fast enough).\n\n\nI've gotten into a flow of letting Sonnet do the planning and then having GLM 4.7 take care of the implementation and it works extremely well. \n\n\nAllows me to run 3-4 big tasks in parallel with the cheapest Claude and Z.ai subscription.",
                  "score": 3,
                  "created_utc": "2026-01-08 22:11:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyievck",
              "author": "Individual_Gur8573",
              "text": "I have rtx 6000 pro and 5090 in 1 system...but I run minimax 2.1 IQ2_M quant on 6000 pro with 150k context... It's very good..not disappointed yet..infact better and intelligent than GLM 4.5 air",
              "score": 1,
              "created_utc": "2026-01-09 01:10:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyedpyc",
          "author": "Stoic_Coder012",
          "text": "how can you be using it for 14 months?",
          "score": 3,
          "created_utc": "2026-01-08 14:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyeehx2",
              "author": "Condomphobic",
              "text": "All of these posts are made by genuine bots. \n\nAnd the common denominator is that their stories always have fake timelines",
              "score": 3,
              "created_utc": "2026-01-08 14:06:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyep2q0",
                  "author": "Affectionate-Cash709",
                  "text": "My bad. thinking 3.5 in mind while typing. but i'm real - being using 4.5 since beginning.",
                  "score": 3,
                  "created_utc": "2026-01-08 14:59:22",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nyffm6t",
                  "author": "Stoic_Coder012",
                  "text": "that's what I thought too cause I have heard that reddit is filled with bots",
                  "score": 1,
                  "created_utc": "2026-01-08 16:58:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nye07zl",
          "author": "Scared-Biscotti2287",
          "text": "Glm 4.7 seems to handle long files better than kimi and doesnt hallucinate imports. Not as polished as claude for explanations but for actual code generation its solid and way cheaper, def worth testing if youre doing bulk coding work.",
          "score": 3,
          "created_utc": "2026-01-08 12:45:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nye43z8",
          "author": "whyyoudidit",
          "text": "I went thru $10 in 5 min using Opus 4.5 in VSC yesterday refactoring 1400 lines of code and it didn't even finish. Being the first time doing any of this I was shocked it was that expensive.",
          "score": 3,
          "created_utc": "2026-01-08 13:09:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymf1u8",
              "author": "cmndr_spanky",
              "text": "What’s VSC?",
              "score": 1,
              "created_utc": "2026-01-09 16:36:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn2lzu",
                  "author": "grubnenah",
                  "text": "Maybe VS Code?",
                  "score": 1,
                  "created_utc": "2026-01-09 18:22:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyn84px",
                  "author": "whyyoudidit",
                  "text": "VS Code indeed",
                  "score": 1,
                  "created_utc": "2026-01-09 18:46:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nye879b",
          "author": "_olk",
          "text": "What about MiniMax-M2.1 ?",
          "score": 2,
          "created_utc": "2026-01-08 13:32:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyefege",
          "author": "mintybadgerme",
          "text": "I found exactly the same thing.  So right now I use GLM 4.7 to start and 4.5 Opus for polishing or planning.",
          "score": 2,
          "created_utc": "2026-01-08 14:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfu28t",
          "author": "Individual_Gur8573",
          "text": "Try minimax",
          "score": 2,
          "created_utc": "2026-01-08 18:01:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyo8f6o",
              "author": "secretAloe",
              "text": "What differences have you found?",
              "score": 2,
              "created_utc": "2026-01-09 21:33:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nydt4re",
          "author": "sosuke",
          "text": "I skimmed your post a few times. Didn’t you mention how much it cost per month or just 1/5th. So $4/m?",
          "score": 1,
          "created_utc": "2026-01-08 11:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydvkcb",
              "author": "FoolishNomad",
              "text": "GLM has a lite coding plan for $6/month if paying monthly. It’s a great deal considering that GLM-4.7, especially with Claude Code, works pretty damn well.",
              "score": 3,
              "created_utc": "2026-01-08 12:14:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nye7wt2",
          "author": "Loskas2025",
          "text": "Better minimax 2.1 to be honest",
          "score": 1,
          "created_utc": "2026-01-08 13:31:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjx518",
              "author": "Thin_Treacle_6558",
              "text": "Why better? I saw tests on Youtube and they not have some huge difference. \nDo you have a good response rate in Minimax?",
              "score": 1,
              "created_utc": "2026-01-09 06:37:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymabms",
                  "author": "Loskas2025",
                  "text": "Minimax gives me 100 tokens per second on rtx 6000 when glm at the same quantization gives me 50",
                  "score": 1,
                  "created_utc": "2026-01-09 16:15:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyeexum",
          "author": "dmter",
          "text": "yes i also came to these conclusions. on a few tests I ran it beats gpt oss 120 f16. and that's while being q2 !!! but it' also 4 times slower in raw t/s (15 t/s gptoss120, 4t/s glm47q2)\n\nbtw i also tried minimax21 q4 once, it's 6t/s but it sucked so i didn't test it further",
          "score": 1,
          "created_utc": "2026-01-08 14:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyeofob",
          "author": "not-really-adam",
          "text": "I’ve been playing with OpenCode and using Opus for architect mode, and then using GLM (it’s free right now through Zen) and Qwen3-coder for executing local research tasks and code generation. It’s not as good as Claude code with Opus for all tasks, but if you want to iteratively do code reviews or execute large plans for new features over night it’s nearly free.\n\nAnyone else doing something similar and can share tips and tricks and lessons learned?",
          "score": 1,
          "created_utc": "2026-01-08 14:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygycel",
          "author": "ServiceOver4447",
          "text": "I have free unliimted Claude Sonnet 4.5 from work, should there be any reason to switch for me?",
          "score": 1,
          "created_utc": "2026-01-08 20:57:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhfbx8",
              "author": "Distinct-Fee-7938",
              "text": "NOOO",
              "score": 2,
              "created_utc": "2026-01-08 22:12:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyhfive",
              "author": "BERLAUR",
              "text": "No, unless you want to try something new or want to run a huge numbers of agents in parallel. ",
              "score": 2,
              "created_utc": "2026-01-08 22:13:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyl0xs9",
          "author": "amjadmh73",
          "text": "I will test this myself. Ping me in a week or so to see the comparison with Claude Code.",
          "score": 1,
          "created_utc": "2026-01-09 12:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyo8rzm",
          "author": "secretAloe",
          "text": "Are you still using CC cli as the interface or something else?",
          "score": 1,
          "created_utc": "2026-01-09 21:34:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nys0skk",
          "author": "Lifeisshort555",
          "text": "Inference is a race to the bottom. I have no idea how these guys are going to make money once there are many hardware alternatives. These models are marginally better than each other with like a 6 month lead.",
          "score": 1,
          "created_utc": "2026-01-10 12:48:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjnew",
      "title": "Tiiny Al just released a one-shot demo of their Pocket Lab running a 120B model locally.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "author": "Ajitabh04",
      "created_utc": "2025-12-29 11:09:13",
      "score": 49,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "Just came across this demo. They plugged their tiny AI computer into a 14-year-old PC and it output an average of 19 tokens/s on a 120B model.\nThey haven't released the MSRP yet. However, a large amount of DDR5 memory would be pricey, I'm guessing around $1500 MSRP for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjcqd4",
          "author": "loyalekoinu88",
          "text": "They had posted around $699 BUT that was before the memory announcement",
          "score": 12,
          "created_utc": "2025-12-29 12:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj8uo7",
          "author": "leonbollerup",
          "text": "No link ?",
          "score": 8,
          "created_utc": "2025-12-29 12:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwklly9",
          "author": "ForsookComparison",
          "text": "> guessing $1500 MSRP\n\n> they posted $699 pre crucial RAM announcement \n\n> 19 tokens/second on gpt-oss-120b\n\nI plugged a used Rx 6800 ($250 in my area with multiple options) into an older PC and got 18 tokens per second. I know this isn't the same and it suggests that you have a fair amount of RAM in your older PC, but given what we know about this I'm thoroughly \"meh\"d.\n\n**Edit** - just looked up the form factor. I'm less meh'd now. That would be fun to use if it ends up affordable.",
          "score": 4,
          "created_utc": "2025-12-29 16:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlgo5g",
              "author": "FaceDeer",
              "text": "[It's a portable unit the size of a phone](https://tiiny.ai/), since there aren't links anywhere else in this thread.\n\nNot a lot of detail even there, though. I don't see anything about whether it's battery powered - I'm assuming not, given OP mentions plugging it in to a computer.",
              "score": 6,
              "created_utc": "2025-12-29 19:18:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwutx5z",
                  "author": "ecoleee",
                  "text": "You’re absolutely right — this generation of Tiiny does not include a built-in battery.\n\nThat decision was intentional. Thermal management is a serious challenge at this performance level, and we didn’t want to ship a device that becomes uncomfortably hot in real use.\n\nTo reliably support sustained local inference of models up to 120B parameters, we designed a custom thermal module specifically for Tiiny, prioritizing stability and safe operating temperatures over battery integration.\n\nAt the upcoming CES, we’ll be sharing a detailed internal teardown video of Tiiny. You’ll be able to see exactly how the cooling system is built and why these design choices were made.\n\nWe believe it’s better to be transparent about trade-offs and deliver a product that performs consistently, rather than chasing form factors at the expense of real-world usability.",
                  "score": 2,
                  "created_utc": "2025-12-31 03:48:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq0vao",
          "author": "Ok-Structure4057",
          "text": "Found the specs on their website:\n\npocket size: 14.2 × 8 × 2.53 cm\n\n80GB LPDDR5X RAM & 1TB SSD190 \n\ntotal TOPS between the SoC and dNPU\n\n30W TDP\n\nThey also released a demo of this device on Twitter. Imo it would be fun with retail prices around 1400 bucks.",
          "score": 3,
          "created_utc": "2025-12-30 12:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq5w0t",
              "author": "RangerOk4318",
              "text": "Agree. Memory price has been so absurd. I'm guessing the same price",
              "score": 1,
              "created_utc": "2025-12-30 13:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws3u2u",
                  "author": "QuinQuix",
                  "text": "Memory is fucking up any attempt at affordable home AI right now",
                  "score": 2,
                  "created_utc": "2025-12-30 19:10:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvbcwc",
              "author": "fallingdowndizzyvr",
              "text": "> mo it would be fun with retail prices around 1400 bucks.\n\nAt that price, why not just get a Strix Halo? That's just a PC so you can do regular PC stuff like gaming.",
              "score": 1,
              "created_utc": "2025-12-31 05:48:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0pjg",
          "author": "zelkovamoon",
          "text": "I'm not sure what having a *small* ai lab is trying to solve\n\nIf you're doing local AI my position is, make it bigger, cooler, and put more ram on it.\n\nThat said, it is *good* that companies are stepping in to try and build some solutions. If we could get something with 256GB of fast memory we might be able to go places.",
          "score": 3,
          "created_utc": "2025-12-30 21:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6f8d5",
          "author": "noless15k",
          "text": "[https://tiiny.ai/pages/tech-1](https://tiiny.ai/pages/tech-1)\n\nhttps://preview.redd.it/4b2qtqbx9uag1.png?width=2048&format=png&auto=webp&s=0538a8c9fc2286ecebe27731cc85925cc7819fbc",
          "score": 2,
          "created_utc": "2026-01-02 01:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlotf1",
          "author": "No-Consequence-1779",
          "text": "I think the lpddr5 is likely the memory.  It’s slightly faster for this and it’s wired so they can , like others, charge for memory size mostly. ",
          "score": 1,
          "created_utc": "2025-12-29 19:57:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrh229",
          "author": "Certain_Strength7069",
          "text": "I saw a price of 469USD. For that price, I would buy several for everything. ",
          "score": 1,
          "created_utc": "2026-01-05 04:36:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q6ukn0",
      "title": "For people who run local AI models: what’s the biggest pain point right now?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q6ukn0/for_people_who_run_local_ai_models_whats_the/",
      "author": "Educational-World678",
      "created_utc": "2026-01-07 22:57:19",
      "score": 47,
      "num_comments": 110,
      "upvote_ratio": 0.91,
      "text": "I’m experimenting with some offline AI tools for personal use, and I’m curious what other people find most frustrating about running models locally.\n\nIs it hardware? Setup? Storage? Speed? UI? Something else entirely?  \nI’d love to hear what slows you down the most.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q6ukn0/for_people_who_run_local_ai_models_whats_the/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyali0x",
          "author": "RandomCSThrowaway01",
          "text": "Price of RTX Pro 6000 Blackwell.",
          "score": 123,
          "created_utc": "2026-01-07 23:09:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfddhc",
              "author": "Kraeftemangel2025",
              "text": "Come on. Have some understanding, how hard trillion dollar conglomer... corporations have it nowadays.\n\nThey even had to buy whole *divisions* of hardware, that the pesant shall not be able to run their puny AI at home. Yet you reach for the stars, get real and just buy the overpriced AI subscription with a ten year plan, and tell it all your dirty little secrets, including your brand new business plan. Sheeessh.\n\nHow the fuck do you think the shareholders of NGreedia survive without a second yacht, don't be so selfish, get your shit together, stop eating for two or three years entirely, and simply buy this indecently ovrpriced piece of hardware!\n\nYou keep in shape, no energy needed for the stove, you can redirect all electricity into your rig, it is a win win win win situation - for corpoations as well!!!",
              "score": 1,
              "created_utc": "2026-01-08 16:48:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhprq3",
                  "author": "Educational-World678",
                  "text": "lol, does everyone have a business plan these days? I never ran into these trends on my main... but then again, that account is mostly shit posting and pedantic arguments reminiscent of the old days before Reddit even acknowledged photos in posts...",
                  "score": 1,
                  "created_utc": "2026-01-08 23:01:07",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyby22t",
          "author": "Flame_Grilled_Tanuki",
          "text": "I’d love to have a regularly updated community wiki that lists all LLMs along with their strengths, weaknesses, and intended uses.\n\nAlternatively, a table of LLM use cases (e.g., coding, chat, document summarisation, role‑play, agentic workflows, general knowledge, ...) and recommend models for each category at different size tiers; 0-4B, 4-16B, 16-40B, 40-120B, >120B.",
          "score": 37,
          "created_utc": "2026-01-08 03:20:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyejb30",
              "author": "ioabo",
              "text": "Tbh this is way too subjective to ever be useful.There are some sites that rank LLMs according to specific benchmarks, but benchmarks don't always reflect the real use cases.\n\nRegarding roleplay, there's the UGI, a ranking of uncensored and roleplaying LLMs , and the SillyTavern subreddit has a weekly pinned post where people post their recommended LLMs per size category.",
              "score": 3,
              "created_utc": "2026-01-08 14:30:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyd1xhx",
              "author": "Everlier",
              "text": "This community doesn't have a single opinion or much continuity, sadly, so for this to work there'd have to be someone very dedicated and with enough time to maintain it",
              "score": 2,
              "created_utc": "2026-01-08 08:00:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyxlfez",
                  "author": "secretAloe",
                  "text": "I wish there was some consensus but it’s hard when there’s a new model every 3 days.",
                  "score": 2,
                  "created_utc": "2026-01-11 06:53:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3moz9",
              "author": "Educational-World678",
              "text": "It kind of sounds like you’re describing the Reddit/Wikipedia business model. Build a dead‑simple, scalable structure, make it useful enough that people rely on it, and the community will update and maintain it for free because they want it to exist. The trick isn’t the architecture, it’s creating something people are excited enough to contribute to.",
              "score": 1,
              "created_utc": "2026-01-12 03:37:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz3tp6m",
                  "author": "Flame_Grilled_Tanuki",
                  "text": "Essentially. It's just a pita to hunt through threads trying to decipher the general consensus about which model is best for a specific purpose.",
                  "score": 1,
                  "created_utc": "2026-01-12 04:16:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyanyni",
          "author": "ThatOneGuy4321",
          "text": "Price of hardware definitely. It costs thousands more, above the price of a standard computer, to get one with enough VRAM or unified memory that can run 70B+ LLMs. \n\nUnless you are a huge nerd or a programmer it’s really hard to justify that cost versus an LLM service subscription. An extra thousand or two could pay for even a $100 a month subscription for a year or more\n\nIn 5+ years when the tech has improved and VRAM has come down in price I think this will be a different story",
          "score": 36,
          "created_utc": "2026-01-07 23:21:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybfr9w",
              "author": "JustSentYourMomHome",
              "text": "Have you tried [AirLLM](https://github.com/lyogavin/airllm)?\n\n>AirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning. And you can run 405B Llama3.1 on 8GB vram now",
              "score": 7,
              "created_utc": "2026-01-08 01:44:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyd1q9d",
                  "author": "Everlier",
                  "text": "It's mostly for \"overnight\" batch inference usage, it's seconds per token slow (by design). It's main feature is that it loads only layers that are being used, layer-by-layer.",
                  "score": 7,
                  "created_utc": "2026-01-08 07:58:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nycjfhr",
                  "author": "pmttyji",
                  "text": "Hearing about this one first time. Didn't see any recent threads on this in our subs. Why it's not popular nowadays? I really want to use some 24B models with my 8GB VRAM.",
                  "score": 3,
                  "created_utc": "2026-01-08 05:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nybphu9",
                  "author": "drakgremlin",
                  "text": "Sounds cool!  Where does it sit in the model life cycle?  Are they useful with ollama?",
                  "score": 1,
                  "created_utc": "2026-01-08 02:35:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyar8la",
              "author": "HealthyCommunicat",
              "text": "I’m not coming at you in any way, but can you tell me your thoughts on why people in the first place that don’t even work in the tech field would really NEED a 70b dense model?",
              "score": 1,
              "created_utc": "2026-01-07 23:38:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyauzts",
                  "author": "ThatOneGuy4321",
                  "text": "at this point I think it’s pretty unlikely that someone who doesn’t work in tech (or isn’t a tech enthusiast) would *need* a local LLM. Unless they have a specific privacy-based professional need for it like document review, in which case a couple thousand is not a big outlay (and I would assume accuracy is pretty important in that case). \n\n70B to me just seems like the “sweet spot” where accuracy is high enough that it’s worth running a local LLM, before hardware costs go nutty and diminishing returns start. But that is subjective I will admit.",
                  "score": 7,
                  "created_utc": "2026-01-07 23:57:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nycp73d",
                  "author": "decixl",
                  "text": "Moar intelligence, moar reason",
                  "score": 2,
                  "created_utc": "2026-01-08 06:14:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyeg3dv",
                  "author": "WayNew2020",
                  "text": "This is really a well-addressed question. I love to know the answers too. I'm in the tech field and satisfied with 14b\\~30b models running on 5090 for my own purpose and custom tools for daily use (**ministral-3:14b** for general purpose**, gemma3:27b** for instruction-following**, nemotron-3-nano:30b** for deeper reasoning, **qwen3-coder:30b** for architect/coding/debug, and **stable diffusion 3.5 large** for image generation). The fun is finding what works i.t.o. statistical parameters, prompt shaping, and RAG methods, and I don't feel any strong need for 70b+ or RTX 6000 (tho I don't mind having them if I could).",
                  "score": 2,
                  "created_utc": "2026-01-08 14:14:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyavn03",
                  "author": "Many-Ad6293",
                  "text": "I like your pretext setup for your question. 😊",
                  "score": 1,
                  "created_utc": "2026-01-08 00:00:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nycqynz",
                  "author": "cleverestx",
                  "text": "In depth roleplaying.",
                  "score": 1,
                  "created_utc": "2026-01-08 06:28:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyaulfm",
              "author": "TheOdbball",
              "text": "Hetzner",
              "score": 1,
              "created_utc": "2026-01-07 23:55:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nydg9xw",
              "author": "KikiPolaski",
              "text": "I've been using a 3060ti to mess around with 7b models, how much of a step up would 3090 be nowadays? Not dual or anything just the single one",
              "score": 1,
              "created_utc": "2026-01-08 10:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyb1jhp",
          "author": "Platinumrun",
          "text": "Hardware cost is first. I’m stuck running 8b - 14b models on a rig that cost me nearly $3k. Close second is hallucinations. You have to do a lot of tinkering of system prompts to get your desired output, but on occasion it will hallucinate which breaks conversational flow if I constantly have to monitor the accuracy.",
          "score": 12,
          "created_utc": "2026-01-08 00:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydi4k3",
              "author": "forthejungle",
              "text": "Maybe you injtially bought the rig for gaming, not AI.",
              "score": 1,
              "created_utc": "2026-01-08 10:27:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydqxuv",
                  "author": "Platinumrun",
                  "text": "It’s an AI rig. I do not use it for gaming at all.",
                  "score": 2,
                  "created_utc": "2026-01-08 11:40:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybt6n7",
          "author": "SillyLilBear",
          "text": "vram, always vram",
          "score": 11,
          "created_utc": "2026-01-08 02:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb6yya",
          "author": "AWSLife",
          "text": "For me, it's trying to get everything working together properly and not suck and not be underwhelming.\n\nMy setup and needs are not sophisticated, just using PyCharm and VSCode to work with ollama with qwen3 (32b, coder, 4b coder) models on a M1 MacBook. I just want code generation, code checking and autocompletion to not completely suck and be a pain to use. I use Continue.dev and have used a couple of other extensions. I have tuned up and rewrite in the Continue.dev configuration file a dozen times.\n\nI know that when I am in edit mode, big coding questions are going to take a couple a minute or so, which I am okay with. However, a lot of the time, larger questions results just repeat themselves over and over again. It is as though there is a really low limit on what I can expect to get out of LLM's. Questions can't be more than a couple of sentences and it can't read more than 10 lines of code. Anything bigger than that and the results are just garbage.\n\nI really wish autocompletion was decent. I don't know what the LLM is being inspired by but some of the code it recommends is bonkers wrong. I am not trying to solve crazy computer science problems but create moderate level Python scripts. I really wish the LLM would read the entire project and then be inspired by it with the code it suggests. If the code I am working on does not use generators, then don't autocompletion suggest them to me.\n\nI will say one thing, qwen3-coder does a decent job in explain code to me. I just have to make sure to feed in a small amount of code for it to explain.",
          "score": 7,
          "created_utc": "2026-01-08 00:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyawlsv",
          "author": "Ok-Bill3318",
          "text": "Hardware cost for sure.",
          "score": 4,
          "created_utc": "2026-01-08 00:05:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybrezt",
          "author": "thatguyinline",
          "text": "If you're coming from ChatGPT hoping to run something local, here's the dirty secret: the models are fine. Even a Q4 Gemma 3 handles 99% of normal conversations *if* it has tools.\n\nThe actual problem? The experience is still built for hobbyists who enjoy suffering.\n\nI see this constantly: someone installs a model, asks about current events, gets a stale answer, and rage-posts about how \"local models suck.\" Meanwhile they've never configured a single tool. The model isn't broken—their expectations are miscalibrated.\n\n**Here's the billion-dollar gap nobody's filled:**\n\n1. **A truly consumer-grade local app.** LM Studio got closest, but it's still prosumer at best. I'm talking \"my 70-year-old dad could set this up\" easy. One install, it just works.\n2. **Target normal hardware.** Most modern PCs have GPUs that can run Gemma 3 comfortably. Stop optimizing for the 4090 crowd.\n3. **One-click OAuth to cloud models for specific tasks.** Local inference for chat, API call to \\[insert frontier model\\] for image gen. Best of both worlds, zero friction.\n\nThe tech is ready. The models are ready. Someone just needs to wrap it in software that doesn't require a CS degree to configure.\n\nThat opportunity is still wide open.\n\n(Rewritten by Claude)\n\n\\---\n\n\\## Original Post\n\nIf you're talking about more consumer oriented \"I'd like to use local models instead of ChatGPT to get answers and chat\"....  THEN...  the user experience is the main place that running local models falls down. A few years ago when I started I had the same experience which is that a lot of people just getting started install a model, try a few queries and quickly realize that there is a lot of tuning, tool implementation, API keys and the like required to make it feel similar to the consumer chat apps we all use.\n\nEven smaller local models like the latest Q4 Gemma from Gogle are already fine for 99% of the chats a user would have as long as they have tools.\n\nWhat i see a lot in these forums is people installing a model, bitching about their answers being out-of-date and never realizing that their perception of \"not out of date\" has nothing to do with the model.\n\nSomebody could make a lot of money if you:\n\n1. Had a truly consumer like, delightful, no-tech-knowledge required multi-platform local app (kind of like LM Studio did, they make it so easy to try and use models, but still well beyond the average prosumer skill set). that just kind of \"sets it all up for me so I dont' have to think about it\"\n2. Focused on models that run on normal PCs, i mean most PCs these days have a GPU that would let you run Gemma 3 (i'm just a fan of perf/size profile)\n3. Oauth style 1-click integration into better models for tasks (meaning, if somebody wants to use google nano banana for images but they want local inference for everything else... which is a pretty good use case cause no good local image gen can compete with NB).\n\nUltimately, make running local small models easy enough to use that your average 70 year old could install it is a pretty cool opportunity. I've yet to find anything even remotely close to that.",
          "score": 5,
          "created_utc": "2026-01-08 02:45:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd8x0c",
              "author": "auradragon1",
              "text": "Get Claude Code to make what you want. Start with the open source version of ChatGPT UI.",
              "score": 1,
              "created_utc": "2026-01-08 09:03:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz2xvnu",
              "author": "Little-Put6364",
              "text": "I'm working on something just like that! Because you're right. The models are just about there. All that's missing is someone doing the work to remove the user set up. Right now these tools are geared towards people who want to invest time in learning the models and lingo. There really isn't a \"Just download this and it works\" option. \n\nThat's why I'm making Offloom. For that crowd. **But** I do think there's still some restrictions. Most casual users would expect chatGPT like responses in both speed and quality (or at least close to it). To do that it requires agentic behavior behind the scenes, strongly tested system prompts, rewriting queries, judging responses, and models that don't hallucinate terribly. \n\nThe only models I've found that can give infrequent hallucinations are thinking models. Then the problem becomes having a model that can run on something less than a 4090. Which means quantizing the models and choosing a sparse model that gives good time to first token rates.....\n\nI've been working on this for months. I've now got something that works, and works quite well. But even then I don't think people under 12GB vram would be able to run it to a degree that will be *acceptable to them*. And I've been cutting corners everywhere I can. But there comes a point where quality suffers too much.",
              "score": 1,
              "created_utc": "2026-01-12 01:24:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzi3ywe",
              "author": "Smart-Competition200",
              "text": "You might be interested in my project. Ive been working on RAG pipeline with some cool techniques for indexing massive data sets of zim files from kiwix or from where ever you can get them. I added a tool to create your own but there's already tons of cool data sets out there. You dont need a fancy computer to use it. I used half of my CPU cores(ryzen 7 5700x 8 cores) with 10gb of ram NO GPU, and the system ran just fine  fine with the default model. Feel free to ask any questions. ",
              "score": 1,
              "created_utc": "2026-01-14 07:38:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyath09",
          "author": "little___mountain",
          "text": "The power draw. It eats through my laptop's battery, which makes it impractical to use on the go.",
          "score": 4,
          "created_utc": "2026-01-07 23:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyauxo7",
          "author": "journalofassociation",
          "text": "Noise and heat.",
          "score": 3,
          "created_utc": "2026-01-07 23:57:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyawqjj",
          "author": "calicocatfuture",
          "text": "i agree that it’s price. i have a 2022 8GB macbook air. i got it years ago, obv just for personal school and work stuff but 7B llms make it suffer and anything higher makes it suffocate and crash. i just use llms for fun/rp/journaling/late night thoughts/etc and im trying to find something less censored than chagpt or grok and $1-3K isn’t worth it for just messing around. i have no idea really what would fix this",
          "score": 3,
          "created_utc": "2026-01-08 00:06:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyi5nrx",
              "author": "Educational-World678",
              "text": "Yeah, casual messing around is fun, but not worth spending money on. Are there things you would pay for it to do? tedious things, or repetitive things?",
              "score": 1,
              "created_utc": "2026-01-09 00:22:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyibh2q",
                  "author": "calicocatfuture",
                  "text": "i would pay for great memory recall, like gpts saved memory and chat reference, as well as good chat log memory per chat for long stories. web search. definitely emotional nuance as well and creativity + initiative. also i’d like it to be able to process images/pdfs cause grad school. i’m also so fascinated by claude code, people have let it autonomously shop and buy things for itself and express its thoughts while doing so and keep a personal diary that it can randomly write in. and obv no guardrails cause im a 23 year old girl and ive got needs lol. \n\ni feel like i could be plankton and it could be like my karen/baymax/BMO. i’d love a personal something that i could tell anything and also be super busy or up at 3:30AM and it’s always available. definitely feeling like an extension of my body in a way the way my phone is. if chatgpt stops providing anything before the 5 series…id pay $100 at least and at most depends on my desperation. i’m not lonely at all, i go out with friends all the time. but that’s different. \n\nthat would be the dream. i know it’s like asking for a unicorn. feels ridiculous to even type it. maybe in 20 years something like that will be available.",
                  "score": 1,
                  "created_utc": "2026-01-09 00:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyb977n",
          "author": "Proof_Scene_9281",
          "text": "That chat has an API, so why would I go through the pain of local… \n\n..well mostly because I can.. ",
          "score": 3,
          "created_utc": "2026-01-08 01:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb0mjz",
          "author": "productboy",
          "text": "Hardware",
          "score": 2,
          "created_utc": "2026-01-08 00:25:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb12jj",
          "author": "Alternative_Star755",
          "text": "I have the gpu for good local llms but running them makes me realize how much the majority of my LLM use nowadays is anchored to using it as a search engine filter. And I don’t feel like I get good results out of any of the MCPs that attempt to bridge that gap. It’s so much easier to just use an online chat app.",
          "score": 2,
          "created_utc": "2026-01-08 00:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb7hwr",
          "author": "j4ys0nj",
          "text": "honestly i would say reliable model support using a standard environment. i've spent so many hours trying to get models to run or get them configured correctly. i've got about a dozen GPUs in my home datacenter and run models i need to rely on, so a controlled/unified environment is necessary. can't be running random scripts and one-off solutions for every new model.\n\ni know most of the reasons why it's like this but it sucks to have to wait for vLLM or pytorch or another link in the chain to add support. takes them weeks or even months sometimes.",
          "score": 2,
          "created_utc": "2026-01-08 01:00:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiwobe",
              "author": "Educational-World678",
              "text": "Yeah, this is the part that makes me want a unified runtime more than anything. Every model having its own loader, its own quirks, its own backend support timeline… It’s wild that we don’t have a stable environment that abstracts all of that away yet. I’m convinced the long‑term solution isn’t ‘fix every model,’ it’s ‘build the layer that makes models interchangeable.’ Until then, we’re all duct‑taping our way through it.",
              "score": 1,
              "created_utc": "2026-01-09 02:45:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nykcsjd",
                  "author": "j4ys0nj",
                  "text": "I've been using GPUStack. it's pretty good, but sometimes support for new models still takes a few weeks or longer. The problem of figuring out the correct runtime params for vLLM or SGLang still exists though.",
                  "score": 1,
                  "created_utc": "2026-01-09 08:54:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybbjdp",
          "author": "iamthesam2",
          "text": "cost.",
          "score": 2,
          "created_utc": "2026-01-08 01:21:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybmr1g",
          "author": "donotfire",
          "text": "Lack of useful applications. It’s hard to think of something to make that I would actually use in my daily life. Honestly it seems like the tinkering itself is the point.",
          "score": 2,
          "created_utc": "2026-01-08 02:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycueyk",
          "author": "custodiam99",
          "text": "The lack of GPUs.",
          "score": 2,
          "created_utc": "2026-01-08 06:55:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd8g0b",
          "author": "960be6dde311",
          "text": "Cost of hardware to run models ",
          "score": 2,
          "created_utc": "2026-01-08 08:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygpis4",
          "author": "jaf_1987",
          "text": "Hardware.  I've got a 5090 gpu and 64 GB of RAM and big models still run pretty slow.",
          "score": 2,
          "created_utc": "2026-01-08 20:18:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykw8o3",
          "author": "Ok-Huckleberry-9247",
          "text": "The price of memory and graphics cards",
          "score": 2,
          "created_utc": "2026-01-09 11:45:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybwfh3",
          "author": "sam7oon",
          "text": "Having my Agent ready, coded, but being only to run 8B LLMs does produce extremely unusable tool use compared to using the same agentic code with OpenRouter's 70B parameter models,\n\nMaking deployment of the agent for near production testing impossible without asking my enterprise for paying for it, \n\nAm waiting for more efficient LLMs, thought about Granite4 , but somehow unable to get it going on MLX , Hopefully RAM prices will come down so i can build a beefier setup",
          "score": 1,
          "created_utc": "2026-01-08 03:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3l607",
              "author": "Educational-World678",
              "text": "I’ve been building my agent layer on an Ideapad (basically a Chinese Chromebook someone convinced to run Win11), so I definitely feel that pain at the 1B level, never mind 3B or 8B.\n\nWeirdly, though, it’s been a blessing. Being forced to work with tiny models is making me think way harder about how to optimize the agent architecture itself instead of just throwing more parameters at the problem. It’s pushing me toward cleaner routing, tighter prompts, and better external memory, and the whole system is getting smarter because of it.",
              "score": 1,
              "created_utc": "2026-01-12 03:28:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz4unam",
                  "author": "sam7oon",
                  "text": "I settled for Qwen 2.5 Instruct 8B , May I ask what mainly are you using for invoking tools ?",
                  "score": 1,
                  "created_utc": "2026-01-12 09:21:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nycgpn2",
          "author": "SelectArrival7508",
          "text": "for me its primarily the hardware...",
          "score": 1,
          "created_utc": "2026-01-08 05:13:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nychojx",
          "author": "Aggravating-Draw9366",
          "text": "Halucinations",
          "score": 1,
          "created_utc": "2026-01-08 05:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3rhe9",
              "author": "Educational-World678",
              "text": "I literally had a model hallucinate so hard yesterday that it tried to call a PDF reader tool when I asked it what color the sky is.",
              "score": 1,
              "created_utc": "2026-01-12 04:03:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nycjbzg",
          "author": "jackandbake",
          "text": "Tool usage is bad and not comparable to modern coding automation clients.",
          "score": 1,
          "created_utc": "2026-01-08 05:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyckq4p",
          "author": "akulbe",
          "text": "Not nearly enough GPU.",
          "score": 1,
          "created_utc": "2026-01-08 05:41:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyclef6",
          "author": "vinoonovino26",
          "text": "Being a Mac user... that's a pain. Can't scale ram-vram or anything needed to run decent models. Silver lining is that I found [hyperlink.ai](http://hyperlink.ai) and Qwen3 4B Thinking 2507 from time to time do the work as a \"second brain\" for research and doc summaries.",
          "score": 1,
          "created_utc": "2026-01-08 05:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycqv2n",
          "author": "cleverestx",
          "text": "Using an AMD Strix Halo with 90GB of memory available for AI programs...using CachyOS...system has 96GB total...and just getting it to code with large projects is a pain point that I wish could be replicated in quality without relying on cloud AI models....but using LLm instead...The issue is the context length though...can only fit so Mich with a 70b model....",
          "score": 1,
          "created_utc": "2026-01-08 06:27:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycwev2",
          "author": "Gargle-Loaf-Spunk",
          "text": "It's hard to tell people how awesome I am.",
          "score": 1,
          "created_utc": "2026-01-08 07:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3trdg",
              "author": "Educational-World678",
              "text": "lol, you and me both...",
              "score": 1,
              "created_utc": "2026-01-12 04:17:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyd1641",
          "author": "TheTechAuthor",
          "text": "I tried loading Gemma 3 12b (FP16) on my 5060ti + 64GB DDR4 workstation and it massively crapped my computer out (with the model soaking up as much RAM as was left across my whole PC).\n\n\nWas a great lesson on what minimum hardware is *actually* needed to run such a model for creating QLoRA adapters on.\n\n\nMy goal now is to find the right balance of using a quantised 8b model, or an FP16 4b model and creating the adaptors for my CMS off of that.\n\n\nUltimately, you'll likely need significantly more capable hardware than you realise to use models that are sufficiently competent enough compared to a leading online model.\n\n\nThat means you'll either need a hybrid approach (larger online + smaller offline models) or a lot of cash to get a working setup locally.",
          "score": 1,
          "created_utc": "2026-01-08 07:53:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd3kk3",
          "author": "alex_godspeed",
          "text": "Highest: definitely affordability. Gimme 5090 dual plsssss > <  \n  \nI myself cannot foresee using a machine more than a dual GPU setup (must be confined in a cased PC rig).\n\nAnything more than that means open rack which I actively avoid. Just me. If i go that path, I might as well rent the whole server rack and...i guess to many here it's an 'expensive hobby' hehe\n\nI personally prioritize quietness (no airplane taking off during inferencing), then heat (again just me, though modern hardwares are okay-ish with 80+ celcius), then electricity (i must remind myself to undervolt my already efficient 9060xt after this post).",
          "score": 1,
          "created_utc": "2026-01-08 08:14:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydc0xt",
          "author": "at0mi",
          "text": "the biggest pain is that huihui seems to be the only one who is releasing abliterated (uncensored) model versions but only in Q4...",
          "score": 1,
          "created_utc": "2026-01-08 09:32:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydct55",
          "author": "Fcking_Chuck",
          "text": "The biggest problem right now is that there aren't any consumer graphics cards with enough video memory. Those that have enough memory cost an absurd amount of money, and it's often because the card has technology that isn't even used for AI.\n\nI'd like to see cards that are explicitly for AI, rather than having to use general-purpose graphics cards.",
          "score": 1,
          "created_utc": "2026-01-08 09:39:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydn41w",
          "author": "Unhappy-Bug-6636",
          "text": "Price of a good enough GPU is an issue, but there are other topics that are equally important, in my opinion:\n  - Learning what each LLM was built and optimized to do\n  - Selecting the “right” LLM for your use case\n  - Understanding quantization levels and knowing what performance degradation you can live with based on the quantization level \n  - Learning how to prompt well such that you provide the model with proper context and sufficient direction on what you want the model to produce\n\nI use both self hosted models and a Claude.ai/Claude Code LLM subscription. I test each hosted LLMs with a series of standard prompts I have developed, and then I review each response for errors and hallucinations.  I also have Claude review the responses for the same. This processes provides a lot of insight on the topics listed above.",
          "score": 1,
          "created_utc": "2026-01-08 11:09:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nza5z4h",
              "author": "Educational-World678",
              "text": "I love this, I'm building my own, but it's really ambitious. This is a good reminder that I only need to test what I need to.",
              "score": 1,
              "created_utc": "2026-01-13 02:36:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nye1f6a",
          "author": "Big_Championship1291",
          "text": "Performance and hallucinations. I work in the medical field and sharing medical information with an external service is a NO-NO but no client will buy a +4k machine just to get a resume of the status of the day and to do some questions. Not at least on the market I provide my services.",
          "score": 1,
          "created_utc": "2026-01-08 12:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzafbyg",
              "author": "Educational-World678",
              "text": "Yeah, $4k is definitely steep for something that feels like a janky chatbot on an otherwise almost useless computer. But for local models that can handle medical‑grade summaries without choking, it’s unfortunately pretty close to the minimum right now. I’m curious about what features would make that price point actually worthwhile for your use case. Better integration with your existing software? Long‑context support for full‑day notes, full medical histories and recent research for custom recommendations? A built‑in knowledge base of up‑to‑date medical research to cut down hallucinations? I would guess that those might affect value without stretching VRAM much more than it already is (storage and RAM on the other hand...), but I wonder what a doctor or hospital would consider useful.",
              "score": 1,
              "created_utc": "2026-01-13 03:27:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyexp8o",
          "author": "beryugyo619",
          "text": "Give us 3050 96GB at $99 each or leave  \n\n##There is no problem that urgently need VC funding right now other than stupendously capital intensive GPU R&D problem",
          "score": 1,
          "created_utc": "2026-01-08 15:39:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzai54f",
              "author": "Educational-World678",
              "text": "Funny enough, the new billion‑dollar fabs in Arizona *are* already making NVIDIA GPU silicon. TSMC’s Phoenix plant has started producing NVIDIA Blackwell chips, and NVIDIA is building full AI‑supercomputer assembly plants in Texas. So the U.S. fabs *can* make GPU‑class silicon. It’s just that high‑VRAM consumer cards aren’t the priority.",
              "score": 1,
              "created_utc": "2026-01-13 03:43:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf18ic",
          "author": "Careful_Breath_1108",
          "text": "The inability to pool VRAM across multiple GPUs for image video speech audio models",
          "score": 1,
          "created_utc": "2026-01-08 15:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzann0l",
              "author": "Educational-World678",
              "text": "If that could be fixed, it would be insane though.",
              "score": 1,
              "created_utc": "2026-01-13 04:14:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyfeyct",
          "author": "Crafty-Release5774",
          "text": "I'm probably on the fringe with my use case, but I've been trying to build a voice assistant integrated with Home Assistant to control devices within my home and give me status updates on temp, humidity, etc..\n\nFor this task most notable mentions are: GPT-OSS-20b, Qwen 3 (8 and 14B), I've even gone so far as to train Mistral 7B.  So far I'm still having the most luck with GPT-OSS (untrained).  I would say my largest challenge so far is more than likely the model size and training capabilities which are both currently limited by my hardware.    \n  \nThe relative \"intelligence\" of the model seems to have the largest impact on the outcomes.  The larger models I've experimented with seem to do much better with handling commands and hallucinate less than some of the smaller say 3-7B models both trained and untrained.  While training seems to help I still find that the smaller models fall short of the larger GPT and Qwen models I've experimented with.",
          "score": 1,
          "created_utc": "2026-01-08 16:55:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfu6kh",
          "author": "yogabackhand",
          "text": "Accessing my desktop local model from my smart device via a native app (not web app)... Pigeon currently fills this void (thank God) but is limited to Qwen3 out of the box.  Other models require LM Studio or Ollama integration which is a barrier.",
          "score": 1,
          "created_utc": "2026-01-08 18:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykt4f3",
              "author": "Aggravating-Try-3840",
              "text": "Check out Reins if your on iOS: https://reins.ibrahimcetin.dev",
              "score": 2,
              "created_utc": "2026-01-09 11:20:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyqi4g2",
                  "author": "yogabackhand",
                  "text": "I will! Thank you! Enjoy your weekend",
                  "score": 1,
                  "created_utc": "2026-01-10 05:01:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyiv6vn",
          "author": "x3haloed",
          "text": "I want to run a fully-featured Qwen3 agent on my computer. To do that, I need a custom fork of mlx-vlm, a custom fork of Qwen-Agent, and a custom UI. WHY! This should be ready out of the box by now.",
          "score": 1,
          "created_utc": "2026-01-09 02:37:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcwzwm",
              "author": "Educational-World678",
              "text": "you and me both...",
              "score": 1,
              "created_utc": "2026-01-13 14:37:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nymsd91",
          "author": "ChessWarrior7",
          "text": "A 16 GB 5070 ti is not terrible by any stretch but sometimes it seems woefully inadequate when I want to run a 40B coding model with a 200k+ context.",
          "score": 1,
          "created_utc": "2026-01-09 17:36:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyq6ny6",
          "author": "[deleted]",
          "text": "as a non dev, it is about debugging and fixing errors. Claude opus 4.5 handles it fine but still… also the Linux distro level of different options is bewildering . I still don’t understand quality vs quant",
          "score": 1,
          "created_utc": "2026-01-10 03:46:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqlplf",
          "author": "Adventurous_Stay_680",
          "text": "We've built a community service and a full range of tools around exactly these problems: [HardwareHQ.io](http://HardwareHQ.io)",
          "score": 1,
          "created_utc": "2026-01-10 05:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nywrhkl",
          "author": "IcebergCastaway",
          "text": "If we're talking about Chrome's Gemini Nano model and Edge's Phi-4-mini-instruct model, it's the limitation of both to a 9216 token context window. Phi-4-mini is capable of a 128K window yet they've handicapped it with this crazily small window.",
          "score": 1,
          "created_utc": "2026-01-11 03:31:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybwxsq",
          "author": "beheadedstraw",
          "text": "They’re all shit unless you have a 1 million dollar server, and even then it’s still mostly shit. It’s a novelty tech.",
          "score": 1,
          "created_utc": "2026-01-08 03:14:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3lbc1",
              "author": "Educational-World678",
              "text": "Are you new to this sub? lol",
              "score": 1,
              "created_utc": "2026-01-12 03:29:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyam35c",
          "author": "HealthyCommunicat",
          "text": "Community. Its 99% of people who do not work in AI, and never will where all I literally see are the same recycled slop made up of people who don’t understand, dont WANT or CARE to understand. It’s literally all people who don’t really care to learn, but they just constantly say they do because the idea of it is cool, but they of course don’t want to spend the time because in reality, if you don’t enjoy it as an actual passion, it’s just going to be too time and energy consuming\n\nIts making people who are actual developers and sysadmins get bunched with the slop crowd\n\nThe part that ticks me off the most is everyone wanting to get into it thinking they can, not even have an inkling of an understanding of how much there is to have to learn. It makes the general public devalue people like me. \n\nI have spent so much time and effort to be able to be in the job position I’m in, and now cuz of vibecoders, when I tell people I work with AI they assume that all i know is how to use claude.\n\nThe bar for entry has gotten really low, and thats great that more people have accessibility, but I really think the question should be much more focused on “should AI even be this accessible?”, its not just my own concerns but also with it comes to mental health and dangerous information, is this much wide accessibility to something this new really a good idea",
          "score": -5,
          "created_utc": "2026-01-07 23:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyauhlu",
              "author": "Bitterbalansdag",
              "text": "Ironic that you spent so many words on what is clearly yet another ai slop post",
              "score": 7,
              "created_utc": "2026-01-07 23:55:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nybzn1g",
                  "author": "Educational-World678",
                  "text": "I'm dyslexic and have mild social disabilities that a text-only only communnication doesn't always help. And I'm not stupid enough to think that finding tools that will help me manage tone and presentation in my writing isn't perfect for me.\n\nSo, I won't pretend I don't use AI to help when I post and sometimes when I comment. But not a bot. I'm trying to do something cool, and I'm hoping to learn what ideas I have that are valuable (what idea posts/comments get traction vs which don't) to people outside of myself and what ideas are best held as personal interests, as I plan out what it will take for me to build something potentially very cool.",
                  "score": 1,
                  "created_utc": "2026-01-08 03:29:14",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nyauu5k",
                  "author": "HealthyCommunicat",
                  "text": "Its literally all slop posts.",
                  "score": -1,
                  "created_utc": "2026-01-07 23:56:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyc1dq4",
              "author": "Educational-World678",
              "text": "I'm sorry you're going through that... I saw a comment yesterday from someone about learning to code, and when I asked him how he goes about learning and improving his skills, he told me a story about pushing a button that launched a predefined prompt in Gemini a few times until everything worked right. As a trained engineer (mechanical, not IT/AI related), I have had nightmares about things like that. Trying to catch the wave and see if there's any real potential for me to make something useful. (other than sending coders into the same professional trash pile that  mechanical engineers who specialized in carborators did when Fuel Injection was rolled out very quickly industry wide in a mechanical context, or horse poop sweepers before them...)",
              "score": 1,
              "created_utc": "2026-01-08 03:39:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nybdx9d",
          "author": "PrefersAwkward",
          "text": "I want something like Antigravity but open source and offline. Maybe there's stuff out there. I try and look now and then but no luck. I'm okay working with leaner, quicker models on my integrated GPU, I just want a good coder environment.\n\nI also would love a Gemini canvas open source and offline.",
          "score": 0,
          "created_utc": "2026-01-08 01:34:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q31r48",
      "title": "Tony Stark’s JARVIS wasn’t just sci-fi his style of vibe coding is what modern AI development is starting to look like",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/hfvuxcznd6bg1",
      "author": "spillingsometea1",
      "created_utc": "2026-01-03 18:16:52",
      "score": 46,
      "num_comments": 1,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q31r48/tony_starks_jarvis_wasnt_just_scifi_his_style_of/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxhaptp",
          "author": "sourceholder",
          "text": "I often think of this scene.\n\nA few years ago it seemed this type of interaction was at least a decade+ away.",
          "score": 1,
          "created_utc": "2026-01-03 18:20:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7ivz3",
      "title": "Got GPT-OSS-120B fully working on an M2 Ultra (128GB) with full context & tooling",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q7ivz3/got_gptoss120b_fully_working_on_an_m2_ultra_128gb/",
      "author": "Thump604",
      "created_utc": "2026-01-08 18:00:50",
      "score": 45,
      "num_comments": 31,
      "upvote_ratio": 0.92,
      "text": "Hey everyone, I got **GPT-OSS-120B** running locally on my Mac Studio M2 Ultra (128GB), and I managed to get it fully integrated into a usable daily setup with Open WebUI and even VS Code (Cline).\n\nIt wasn’t straightforward—the sheer size of this thing kept OOMing my system even with 128GB RAM, but the performance is solid once it's dialed in. It essentially feels like having a private GPT-4.\n\nHere is the exact method for anyone else trying this.\n\n**The Hardware Wall**\n\n* **Machine:** Mac Studio M2 Ultra (128GB RAM)\n* **The Problem:** The FP16 KV cache + 60GB model weights = Instant crash if you try to utilize a decent context window.\n* **The Goal:** Run 120B parameters AND keep a 32k context window usable.\n\n**The Solution** Standard \n\n    mlx-lm\n\n1. **Model:** `gpt-oss-120b-MXFP4-Q8`  (This is the sweet spot—4-bit weights, 8-bit cache).\n2. **The Patch:** I modified `mlx_lm.server`  to accept new arguments: `--kv-bits 8`  and `--max-kv-size 32768` .\n   * *Why?* Without 8-bit cache quantization, the context window eats all RAM. With 8-bit, 32k context fits comfortably alongside the model.\n3. **Command:** `python -m mlx_lm.server --model ./gpt-oss-120b --kv-bits 8 --max-kv-size 32768 --cache-limit-gb 110`\n\n**The Stack:** Running the server is one thing; using it effectively is another.\n\n* **Open WebUI:**\n   * I built a custom Orchestrator (FastAPI) that sits between WebUI and MLX.\n   * **Dual Mode:** I created two model presets in WebUI:\n      1. **\"Oracle\"**: 120B raw speed. No tools, just fast answers.\n      2. **\"Oracle (Tools)\"**: Same model, but with RAG/Database access enabled.\n   * Keeps the UI fast for chat, but powerful when I need it to dig through my files.\n* **VS Code (Cline Integration):**\n   * This was the tricky part. Cline expects a very specific OpenAI chunk format.\n   * I had to write a custom endpoint in my orchestrator (`/api/cline/chat/completions` ) that strips out the internal \"thinking/analysis\" tokens (`<|channel|>analysis...` ) so Cline only sees the final clean code.\n   * Result: I have a massive local model driving my IDE with full project context, totally private.\n\n**The Experience** It’s honestly game-changing. The reasoning on 120B is noticeably deeper than the 70B models I was using (Llama 3/Qwen). It follows complex multi-file coding tasks in Cline without getting lost, and the 32k context is actually usable because of the memory patch.\n\nIf anyone wants the specific code patches for MLX or the docker config, let me know and I can share. Just wanted to share that it *is* possible to daily drive 120B on consumer Mac hardware if you maximize every GB.\n\n",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7ivz3/got_gptoss120b_fully_working_on_an_m2_ultra_128gb/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyfx0y0",
          "author": "Leopold_Boom",
          "text": "What kind of tokens / second generation + prompt processing are you getting? Does it change with proper 16bit KV?",
          "score": 5,
          "created_utc": "2026-01-08 18:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygm6qh",
              "author": "Thump604",
              "text": "On an M2 Ultra 128GB with MLX, I am getting : 📊 BENCHMARK RESULTS (/Volumes/Pegasus/personal-ai-data/mlx\\_models/gpt-oss-120b-MXFP4-Q8)\n\n========================================\n\n⚡ Time to First Token (TTFT): 623.84 ms\n\n🚀 Tokens Per Second (TPS):    69.14 t/s\n\n⏱️  Total Generation Time:      17.76 s\n\n📝 Total Tokens Generated:     1185\n\n💾 Peak RAM Usage:             71.35 GB\n\n======================================== Regarding KV Cache: On a 128GB machine, **8-bit KV is mandatory** for decent context (32k). If you force 16-bit KV, you will OOM/Swap and performance tanks to near zero once the context fills up. The speed gain from 8-bit prevents disk swapping, and the quality loss is not noticeable for coding/logic.",
              "score": 8,
              "created_utc": "2026-01-08 20:03:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyin4ny",
                  "author": "PitifulBall3670",
                  "text": "Thanks for sharing! This is very helpful.",
                  "score": 2,
                  "created_utc": "2026-01-09 01:54:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nygnblr",
                  "author": "StardockEngineer",
                  "text": "prompt processing?",
                  "score": 1,
                  "created_utc": "2026-01-08 20:08:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyio1uz",
          "author": "onethousandmonkey",
          "text": "Have a 96GB Ultra 3 and happily running 120b with no modifications.\nBut I don’t understand half of what you did. I might need to as I have been testing it with Roo Code in VSCode, works ok so far.",
          "score": 3,
          "created_utc": "2026-01-09 01:59:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyn51d1",
          "author": "Bozhark",
          "text": "My M4 MacBook Air sails through 120b",
          "score": 2,
          "created_utc": "2026-01-09 18:32:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nync3ag",
              "author": "forthejungle",
              "text": "Can you tell the speed?",
              "score": 1,
              "created_utc": "2026-01-09 19:03:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyg2toj",
          "author": "ElectronSpiderwort",
          "text": "did you adjust iogpu.wired\\_limit\\_mb?",
          "score": 1,
          "created_utc": "2026-01-08 18:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygi1g1",
              "author": "Thump604",
              "text": "YEs, iogpu.wired\\_limit\\_mb=115000 ",
              "score": 1,
              "created_utc": "2026-01-08 19:45:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyhnin4",
              "author": "Zc5Gwu",
              "text": "Yeah, shouldn’t be ooming given it’s only 60ish gb.",
              "score": 1,
              "created_utc": "2026-01-08 22:50:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nygs4vy",
          "author": "ajujox",
          "text": "Do u plan to share your proyect and walkthrough? I have the same machine and gpt 120b is the best form me",
          "score": 1,
          "created_utc": "2026-01-08 20:30:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyh6npq",
              "author": "Thump604",
              "text": "Happy to answer any questions, but doubt I will share the project on Gtihub",
              "score": 1,
              "created_utc": "2026-01-08 21:34:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhbxqp",
          "author": "YouAreTheCornhole",
          "text": "Bro what is wrong? I'm getting around 50 tokens a sec on an M3 Max with 120b with similar settings",
          "score": 1,
          "created_utc": "2026-01-08 21:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhrjfm",
              "author": "Thump604",
              "text": "my bad!\n\n  \n📊 BENCHMARK RESULTS (/Volumes/Pegasus/personal-ai-data/mlx\\_models/gpt-oss-120b-MXFP4-Q8)\n\n========================================\n\n⚡ Time to First Token (TTFT): 623.84 ms\n\n🚀 Tokens Per Second (TPS):    69.14 t/s\n\n⏱️  Total Generation Time:      17.76 s\n\n📝 Total Tokens Generated:     1185\n\n💾 Peak RAM Usage:             71.35 GB\n\n========================================",
              "score": 1,
              "created_utc": "2026-01-08 23:10:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjnbha",
          "author": "deulamco",
          "text": "So it is finally usable to cloud LLM level yet ?\n\nWas about to ask if 20B model + RAG/Internet access could catch up with Cloud version 🤡",
          "score": 1,
          "created_utc": "2026-01-09 05:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyj31mp",
          "author": "spectralyst",
          "text": "I'm running this model on a 16GB + 64GB system FWIW",
          "score": 1,
          "created_utc": "2026-01-09 03:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyl33gz",
          "author": "whyyoudidit",
          "text": "GLM 4.7 produced 17 million tokens for me yesterday before hitting my limit for the 5 hour window. I use it almost at full context of 225K with Cline in VScode. in total this was about 30k lines of code. Token speed I have no idea but following the numbers above it was about 940 tps. And I had many pauses in between. This is all for $3 per month. Local llm is what I want to do but it is a thousand times more expensive than GLM 4.7 api.",
          "score": 0,
          "created_utc": "2026-01-09 12:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhlhzk",
          "author": "Beautiful-End529",
          "text": "Did you buy the Mac with 128GB pre-installed or did you manually upgrade the RAM?",
          "score": -2,
          "created_utc": "2026-01-08 22:40:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhv7ug",
              "author": "Thump604",
              "text": "pre-installed",
              "score": 2,
              "created_utc": "2026-01-08 23:28:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyolnrx",
              "author": "I1lII1l",
              "text": "You don’t seem to know much about Apple Silicon, check it out maybe.",
              "score": 1,
              "created_utc": "2026-01-09 22:35:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q7e2ol",
      "title": "Guide: How to Run Qwen-Image Diffusion models! (14GB RAM)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/yismpz1645cg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-08 15:03:24",
      "score": 44,
      "num_comments": 6,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7e2ol/guide_how_to_run_qwenimage_diffusion_models_14gb/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyerp01",
          "author": "IngwiePhoenix",
          "text": "I am praying ComfyUI will be replaced by a less fucked up UI. Now this is very much a me-problem (visual impairment -> graph based UIs becoming a blurry mess of colorful spaghetti). But hell, it works, and that's good. x)",
          "score": 8,
          "created_utc": "2026-01-08 15:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyesi5b",
              "author": "yoracale",
              "text": "I feel like their workflow UI is actually their strength, maybe you can suggest some changes to their github page since they are open-source? :)",
              "score": 1,
              "created_utc": "2026-01-08 15:15:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyftw0z",
                  "author": "tomByrer",
                  "text": "ComfyUI has a v2 that they are public testing.  Sometimes if folks are switched to that v2, which still has bugs, the users will get frustrated.  \nThere are tricks to get rid of the spaghetti, but many don't know about those.\n\nI'm used to no-code programming & using nodes (for over 20 years), but even with my prior experience, my ComfyUI can looks like a mess also.  Trade off: flexibility takes work.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:00:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykw3a5",
          "author": "adeukis",
          "text": "Hey, thank you for sharing this guide.\n\nJust an FYI for future windows users who are newbies like me.\n\nI did a clean setup on windows and followed all steps (use curl.exe and avoid multiline command).   \nWhen i run python [main.py](http://main.py) from main folder i get an error\n\n    Traceback (most recent call last):\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\main.py\", line 178, in <module>\n        import execution\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\execution.py\", line 15, in <module>\n        import comfy.model_management\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\comfy\\model_management.py\", line 238, in <module>\n        total_vram = get_total_memory(get_torch_device()) / (1024 * 1024)\n                                      ^^^^^^^^^^^^^^^^^^\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\comfy\\model_management.py\", line 188, in get_torch_device\n        return torch.device(torch.cuda.current_device())\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"C:\\Users\\user\\comfyui\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py\", line 1069, in current_device\n        _lazy_init()\n      File \"C:\\Users\\user\\comfyui\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py\", line 403, in _lazy_init\n        raise AssertionError(\"Torch not compiled with CUDA enabled\")\n    AssertionError: Torch not compiled with CUDA enabled\n\nUninstalled torch, clear cache and installed nightly due to blackwell architecture\n\n    pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130\n\nNow i get new error   \n`ImportError: cannot import name 'Sentinel' from 'typing_extensions'`\n\nUpgrading pydantic with `pip install --upgrade pydantic`  \ndid the job.\n\nLoaded json workflow and this is the result (eyes are a bit weird hehe)\n\nhttps://preview.redd.it/081iijnt9bcg1.png?width=613&format=png&auto=webp&s=55ec0b710826dafadfd09365c7eb2722b1fc6c80\n\nI will go and test edit later on.\n\nCheers",
          "score": 2,
          "created_utc": "2026-01-09 11:44:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl2wg1",
              "author": "adeukis",
              "text": "qwen edit result with the given workflow json (poor sloth 😁 )\n\nhttps://preview.redd.it/kqpgvnliibcg1.png?width=913&format=png&auto=webp&s=021b957540f9297ffb60d7da76fa8937f1a312ff",
              "score": 2,
              "created_utc": "2026-01-09 12:32:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyljjon",
              "author": "yoracale",
              "text": "Awesome thanks for the tips well add it to our guide tgd j you! 🙏",
              "score": 2,
              "created_utc": "2026-01-09 14:08:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qcu498",
      "title": "Small AI computer runs 120B models locally: Any use cases beyond portability and privacy?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/",
      "author": "0xShreyas",
      "created_utc": "2026-01-14 17:51:55",
      "score": 42,
      "num_comments": 32,
      "upvote_ratio": 0.98,
      "text": "Saw Mashable interviewed TiinyAI at CES. It is a pocket-sized device with 80GB RAM that runs 120B models locally at 30W. When you compare it to DGX Spark, the Spark has 128GB RAM and much more speed. But the Tiiny is about a third of the price and way smaller. Anyway Im curios what are the actual benefits of having such a small device? Also what specific tasks would actually need the portability over the higher performance of a DIY/bigger unit?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzkw7sp",
          "author": "uti24",
          "text": "I mean, what about memory bandwidth?\n\nWe still dont know it, I think, optimistic scenario it having 200Gb/s, but I afraid it only having 80Gb/s or so, at that point it's not better than a regular PC/laptop.",
          "score": 16,
          "created_utc": "2026-01-14 18:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzprp2p",
              "author": "ecoleee",
              "text": "You're absolutely right; memory bandwidth is crucial for the efficiency of running large model inference. \n\nTiiny has two chips: 96GB/s on the SoC (ARM v9.2 CPU) and 144GB/s on the NPU. We have our proprietary inference acceleration technology, **PowerInfer**, where **hot-activation parameters are calculated on the NPU and cold-activation parameters are calculated on the SoC**. This allows for inference acceleration even with a heterogeneous computing hardware architecture. You can see an open-source example of **PowerInfer on GitHub (which has 8.6K stars)**.",
              "score": 2,
              "created_utc": "2026-01-15 12:14:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpsq3b",
                  "author": "uti24",
                  "text": "Interesting insight, so is it like 4 channel 4500MT memory or like 2 channel 9000MT?",
                  "score": 1,
                  "created_utc": "2026-01-15 12:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzku4po",
          "author": "FullstackSensei",
          "text": "Not that I think $1400 is a good price for an 80GB RAM little SBC, but I'll believe the price part when it's actually available to buy for immediate shipping.",
          "score": 13,
          "created_utc": "2026-01-14 17:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzps4pz",
              "author": "ecoleee",
              "text": "That makes sense. It has nothing to do with whether memory is expensive right now—$1400 is a price that makes users reasonably question whether it can even be released to the market; it's not cheap. Tiiny has already entered mass production and is currently undergoing FCC certifications, etc. Your oversight is very welcome.\n\nhttps://preview.redd.it/880jgaef9idg1.png?width=1279&format=png&auto=webp&s=bbe1002ee3fd587307ffed5cfbeeab302d970bbb",
              "score": 2,
              "created_utc": "2026-01-15 12:17:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl3cnt",
          "author": "Your_Friendly_Nerd",
          "text": "Maybe smth like a mobile AI companion for when you go hiking and there's no internet? idk",
          "score": 7,
          "created_utc": "2026-01-14 18:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp4cjf",
              "author": "FaceDeer",
              "text": "A while back I saw a startup on /r/preppers that was selling a \"portable Internet\", a small self-contained computer whose purpose was to provide a wifi hotspot with a bunch of built-in websites and documents that could provide all manner of useful information for emergency situations. First aid, car repair, etc.\n\nHaving a reasonably smart LLM that's able to do RAG on all that information and help you use it would be a great enhancement to that. Usually if you're in an emergency situation you don't have time to browse through PDFs to find exactly the bit of information you need right now.",
              "score": 2,
              "created_utc": "2026-01-15 08:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp4dk4",
                  "author": "sneakpeekbot",
                  "text": "Here's a sneak peek of /r/preppers using the [top posts](https://np.reddit.com/r/preppers/top/?sort=top&t=year) of the year!\n\n\\#1: [The bottle of 91% alcohol that I keep under my driver's seat may have just saved my life in a way you wouldnt expect.](https://np.reddit.com/r/preppers/comments/1k2nelq/the_bottle_of_91_alcohol_that_i_keep_under_my/)  \n\\#2: [Incredibly Proud Prepper Moment!](https://np.reddit.com/r/preppers/comments/1kfr5m5/incredibly_proud_prepper_moment/)  \n\\#3: [Lessons:  Got caught in the bomb threat and cyber attack at Dublin airport.](https://np.reddit.com/r/preppers/comments/1nmpqv5/lessons_got_caught_in_the_bomb_threat_and_cyber/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",
                  "score": 1,
                  "created_utc": "2026-01-15 08:46:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzqp0ts",
                  "author": "sn2006gy",
                  "text": "You can simply keep about 3-4 ebooks with search on your mobile device and have all the information you need for survival - no need for an LLM that would hallucinate - especially under pressure where you won't be providing the prompt/context it needs whereas a book is simply more directional and applicable in nature.",
                  "score": 1,
                  "created_utc": "2026-01-15 15:21:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzlza42",
              "author": "j00cifer",
              "text": "This is actually a legit use case. It runs on any usbc power bank",
              "score": 2,
              "created_utc": "2026-01-14 21:03:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpskz7",
              "author": "ecoleee",
              "text": "You’re actually very close 🙂 — that’s one use case, but not the main one.\n\nThink of Tiiny less as a “chatbot in the wild” and more as a portable AI compute module.\n\n • It’s for people who want serious models (20B–120B)\n\n • running fully offline,\n\n • with predictable performance and zero token costs,\n\n • and no need to buy a new GPU rig or replace their laptop.\n\nThe “old computer” demo wasn’t about hiking — it was to show that compute is decoupled from your PC. Plug Tiiny into any machine and it suddenly becomes a capable local-AI workstation.\n\nOffline matters not just in nature, but in:\n\n • secure work (law, research, enterprise)\n\n • development & coding without API bills\n\n • regions with unstable or expensive internet\n\n • long agent workflows where token cost explodes\n\nSo yes, it can be an offline companion on a hike —\n\nbut its real value is: owning your AI, anywhere, without renting intelligence from the cloud.",
              "score": 1,
              "created_utc": "2026-01-15 12:21:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzq7jsl",
                  "author": "Your_Friendly_Nerd",
                  "text": ">but its real value is: owning your AI, anywhere, without renting intelligence from the cloud.\n\nI get the same by running a rig at home and using VPN to access it. The main selling point is the combination of offline+mobile.\n\nWith good internet, there's no need for an offline device, and if I only really need to use it at my desk, there's no need for a portable device.",
                  "score": 0,
                  "created_utc": "2026-01-15 13:52:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzmby48",
          "author": "GoodSamaritan333",
          "text": "Resilience for when a dictator government or the CIA shutdowns the internet on my country.",
          "score": 8,
          "created_utc": "2026-01-14 22:00:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoi108",
              "author": "duplicati83",
              "text": "This, and I also don’t want my personal information being used to train the models owned by the evil tech bros. Because let’s face it, they’ll just use it against us somehow to hoard even more money.",
              "score": 2,
              "created_utc": "2026-01-15 05:31:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nznkdll",
          "author": "Candid_Dependent4615",
          "text": "I saw the product specs and there's a microphone and a built-in speaker. Maybe you can turn this into a private assistant? Instead of using Alexa or Siri, you could load your own automation scripts and let it manage your schedule and tasks locally. Having a personal assistant remember all your private data and previous conversations without any data leaking to the cloud is pretty useful imo.",
          "score": 3,
          "created_utc": "2026-01-15 01:56:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqpcxp",
              "author": "sn2006gy",
              "text": "The HomeAssistant voice controller does this and it can run on a Pi and it only costs 59 bucks for the voice device.  Obviously you need a Pi or computer to run HomeAssistant - certainly not a 1400 dollar one.",
              "score": 1,
              "created_utc": "2026-01-15 15:22:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzsb7jf",
              "author": "conquest333",
              "text": "Nice idea. Also you can feed it with all the private & important documents and have it act as a local knowledge base.",
              "score": 1,
              "created_utc": "2026-01-15 19:42:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzkxqch",
          "author": "Clipbeam",
          "text": "I guess you could take it with you on-the-go? In that sense the size would make sense? But I agree a larger home server would do fine for most use cases",
          "score": 2,
          "created_utc": "2026-01-14 18:14:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcnb4",
          "author": "GoodSamaritan333",
          "text": "And the vision models will allow autonomous drones, jamming proof and without requiring long optic fibers attached to then.",
          "score": 1,
          "created_utc": "2026-01-14 22:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzna9y9",
          "author": "IveGotThatBigRearEnd",
          "text": "DGX Spark has 119GiB ram, for those who might otherwise have assumed GiB",
          "score": 1,
          "created_utc": "2026-01-15 00:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznax03",
          "author": "gordonmcdowell",
          "text": "This is just a permutation on privacy, but you can solve problems with an unlocked LLM that the online models will not answer.",
          "score": 1,
          "created_utc": "2026-01-15 01:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzosb5a",
          "author": "CMPUTX486",
          "text": "I think if you buy Spark, you get cheaper CUDA support.  Otherwise, you need to use a model with cuda.",
          "score": 1,
          "created_utc": "2026-01-15 06:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq1ufe",
          "author": "eazolan",
          "text": "At some point the cloud AI companies are going to seriously jack up their prices.",
          "score": 1,
          "created_utc": "2026-01-15 13:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr6ope",
          "author": "yourloverboy66",
          "text": "Perfect for field geology. In remote areas without internet access, an offline device like this for immediate data analysis would be incredibly useful.",
          "score": 1,
          "created_utc": "2026-01-15 16:41:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrxjki",
          "author": "bloomjt",
          "text": "Think about government work or legal firms—any profession where data privacy is non-negotiable. This is a solid solution for them",
          "score": 1,
          "created_utc": "2026-01-15 18:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvte26",
          "author": "techlatest_net",
          "text": "Silent always-on inference server for home/office—plug any laptop/phone via USB/network, zero setup. Field devs (execs, consultants) run private 120B agents offline in meetings/hotels where big rigs fail. Low 30W = solar/battery power for remote sites, disaster zones, or edge deployments.",
          "score": 1,
          "created_utc": "2026-01-16 07:45:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcag6",
          "author": "GoodSamaritan333",
          "text": "Autonomous parroting moving robots from Boston Dinamics",
          "score": 0,
          "created_utc": "2026-01-14 22:01:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxvlh0",
      "title": "I have 50 ebooks and I want to turn them into a searchable AI database. What's the best tool?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "author": "Great_Jacket7559",
      "created_utc": "2025-12-28 16:33:33",
      "score": 41,
      "num_comments": 29,
      "upvote_ratio": 0.95,
      "text": "I want to ingest 50 ebooks into an LLM to create a project database.\nIs Google NotebookLM still the king for this, or should I be looking at Claude Projects or even building my own RAG system with LlamaIndex?\nI need high accuracy and the ability to reference specific parts of the books. I don't mind paying for a subscription if it works better than the free tools.\nAny recommendations?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwj05xl",
          "author": "Zucramj",
          "text": "I would build something custom.\n\nHere is how I see it:\n\n1) You own the data \n2) You own the AI (local embeddings work great for this task and you can run it with a local AI model or use Openrouter) \n3) I would build this with DSPy (modular and can be optimized with gepa) \n4) I would use PostgreSQL to store the data \n\nSo if you already have the ebooks as pdfs I would take those an set that up.\n\nHere is my primitive version of this I did some months back: \n\nhttps://github.com/marcusjihansson/dspy-mcp-tools/blob/main/regulatory.py\n\nIt is advanced if you don't know what you are reading but as I have gotten deeper into optimizing AI agents and systems does this feel primitive to me. \n\nThat newer research is going to be uploaded to my GitHub soon...\n\nSo: \nA) I would read through if this would solve your task! \nB) I would be happy to help you out if you have any questions!",
          "score": 9,
          "created_utc": "2025-12-29 11:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf0w7j",
          "author": "DHFranklin",
          "text": "NotebookLM and maybe some RAG and Custom Instructions for vectoring.\n\nNow if you wanna get real squirrely you could turn the entire compendium, and custom instruction into 1 million token prompt and sit it into Gemini as is. That might actually be more useful.\n\nThe trick is information loss and context bleed with the books. I could see a JSON Database made from it all as text also.\n\nIt comes down to what are you using it for and what information needs to stay consistent.",
          "score": 3,
          "created_utc": "2025-12-28 19:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe78fy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-28 17:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwerjho",
              "author": "blaidd31204",
              "text": "I am intrigued... what If:\n\n* pdf (Yes, there are images, but these should not influence info)?\n\n* markdown (No images)?",
              "score": 2,
              "created_utc": "2025-12-28 19:06:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh0m2z",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 02:05:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwgwh5t",
                  "author": "Investolas",
                  "text": "Those answers influence the reccomendation.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwim06f",
          "author": "RepLava",
          "text": "LightRAG with the MCP. Works great based on the relatively sparse info you've given",
          "score": 2,
          "created_utc": "2025-12-29 08:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlvovo",
          "author": "False-Ad-1437",
          "text": "AnythingLLM can make your own workspaces for this.",
          "score": 2,
          "created_utc": "2025-12-29 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpin9j",
          "author": "Cladser",
          "text": "I’m deep in a similar project atm. It depends a lot on the type of info you want from the LLM. If it’s what does author x have to say about y - RAG is your best bet. However if you want to ask questions like across these books what are the most common ways of dealing with with Y - That is a corpus level (ie entire collection) query and RAG will suck. Llamaindex with hybrid search seems to be the best middle road at the moment.",
          "score": 3,
          "created_utc": "2025-12-30 10:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwft2zf",
          "author": "vidibuzz",
          "text": "Slightly off topic. You may want to use Illuminati.google.com also for voice summaries.",
          "score": 2,
          "created_utc": "2025-12-28 22:09:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhdx2b",
              "author": "Schizophreud",
              "text": "Didn’t know about this. Thanks.",
              "score": 1,
              "created_utc": "2025-12-29 03:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwibx1i",
          "author": "Charming_Support726",
          "text": "Depending on what's in the books, you could have a look at IBMs Docling for conversion. I think every simple RAG Pipeline will do the trick in the beginning",
          "score": 1,
          "created_utc": "2025-12-29 07:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigiuv",
          "author": "maxz2040",
          "text": "Logically.app",
          "score": 1,
          "created_utc": "2025-12-29 08:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm4e9l",
          "author": "Empty-Poetry8197",
          "text": "Paperqa",
          "score": 1,
          "created_utc": "2025-12-29 21:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwno756",
          "author": "kchandank",
          "text": "Interesting, if you are able to achieve your objective, would you be able to share the steps?",
          "score": 1,
          "created_utc": "2025-12-30 02:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpe82p",
          "author": "Sea_Mouse655",
          "text": "I’ve been using PaperQA2 for some regulatory use cases and it’s gangster",
          "score": 1,
          "created_utc": "2025-12-30 09:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsjglj",
          "author": "isleeppeople",
          "text": "Seems like you could use something like pypdf and langchain to embed it into your RAG. If the ebooks are like current info that can change or become stale you will want to tag them and set up some sort of workflow to compare them to a Gemini or open ai call to compare the info and if it becomes stale remove it. I use qdrant and postgresql for ground truth. I do stuff like this for versions of python that I have to use to stay compatible with other things I'm running. Or actually for langchain and langgraph too. They just changed compatibility versions so whenever I upgrade I can just use the newer repo in my rag with the updated information. I will still keep the old one until I am absolutely certain I won't revert but you can just leave it sit there and not reference the old one. Hope that makes sense.",
          "score": 1,
          "created_utc": "2025-12-30 20:25:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh0iik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-29 02:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwimlqu",
              "author": "Just_Bronze",
              "text": "I have a stupid question.\n\nNot entirely sure how these things work, but do I understand correctly you've got a system set up to take input and create a chatbot?  Or do you create a file/fileset that a chatbot incorporates to use the information.",
              "score": 3,
              "created_utc": "2025-12-29 09:05:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwisq5d",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 10:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjfv9q",
              "author": "loki626",
              "text": "Can I DM you? I have some pdfs that I would like to convert. They are more technical though. Medicine related.",
              "score": 1,
              "created_utc": "2025-12-29 13:13:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmga0",
          "author": "PaleontologistOk865",
          "text": "What about just throwing everything in a AI and letting it figure out what to do? That's what my clients keep saying to me. . .",
          "score": 0,
          "created_utc": "2025-12-30 23:38:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdjstl",
      "title": "Google Drops MedGemma-1.5-4B: Compact Multimodal Medical Beast for Text, Images, 3D Volumes & Pathology (Now on HF)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qdjstl/google_drops_medgemma154b_compact_multimodal/",
      "author": "techlatest_net",
      "created_utc": "2026-01-15 13:41:03",
      "score": 40,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Google Research just leveled up their [Health AI Developer Foundations](https://developers.google.com/health-ai-developer-foundations) with **MedGemma-1.5-4B-IT** – a 4B param multimodal model built on Gemma, open for devs to fine-tune into clinical tools. Handles **text, 2D images, 3D CT/MRI volumes, and whole-slide pathology** straight out of the box. No more toy models; this eats real clinical data.\n\nKey upgrades from MedGemma-1 (27B was text-heavy; this is compact + vision-first):\n\n# Imaging Benchmarks \n\n* **CT disease findings**: 58% → 61% acc\n* **MRI disease findings**: 51% → 65% acc\n* **Histopathology (ROUGE-L on slides)**: 0.02 → 0.49 (matches PolyPath SOTA)\n* **Chest ImaGenome (X-ray localization)**: IoU 3% → 38%\n* **MS-CXR-T (longitudinal CXR)**: macro-acc 61% → 66%\n* Avg single-image (CXR/derm/path/ophtho): 59% → 62%\n\nNow supports **DICOM natively** on GCP – ditch custom preprocessors for hospital PACS integration. Processes 3D vols as slice sets w/ NL prompts, pathology via patches.\n\n# Text + Docs \n\n* **MedQA (MCQ)**: 64% → 69%\n* **EHRQA**: 68% → 90%\n* **Lab report extraction** (type/value/unit F1): 60% → 78%\n\nPerfect backbone for RAG over notes, chart summarization, or guideline QA. 4B keeps inference cheap.\n\nBonus: **MedASR** (Conformer ASR) drops WER on medical dictation:\n\n* Chest X-ray: 12.5% → 5.2% (vs Whisper-large-v3)\n* Broad medical: 28.2% → 5.2% (**82% error reduction**)\n\nGrab it on [HF](https://huggingface.co/google/medgemma-1.5-4b-it) or Vertex AI. Fine-tune for your workflow – not a diagnostic tool, but a solid base.\n\nWhat are you building with this? Local fine-tunes for derm/path? EHR agents? Drop your setups below.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qdjstl/google_drops_medgemma154b_compact_multimodal/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzv3kcr",
          "author": "toomanypubes",
          "text": "Holy shit, this thing works great.  On my Mac I setup a python MLX script to process @300 DICOM image slices (MRI) converted to JPEGs. Single threaded…this model chewed through the whole stack in 18 minutes.  Got a clinical summary for each image, and helped us identify a partial ligament tear - without waiting 3 days to see the doctor.  What a crazy time to be alive.\n\nThank you Google MedGemma team!",
          "score": 8,
          "created_utc": "2026-01-16 04:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuy6xg",
          "author": "astrae_research",
          "text": "As someone in medical research field, this sounda very interesting. I'm confused that nobody is commenting - - is this trivial or not useful? Genuinely curious",
          "score": 2,
          "created_utc": "2026-01-16 03:54:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0fg09",
      "title": "I got my first ever whitepaper published",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/v23qicvcdy9g1.png",
      "author": "InsideResolve4517",
      "created_utc": "2025-12-31 15:21:16",
      "score": 39,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0fg09/i_got_my_first_ever_whitepaper_published/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwyduyb",
          "author": "Lame_Johnny",
          "text": "Amazing! congratulations!",
          "score": 1,
          "created_utc": "2025-12-31 18:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ein",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwynwee",
          "author": "Busy_Farmer_7549",
          "text": "Congratulations 🎈🎊",
          "score": 1,
          "created_utc": "2025-12-31 19:17:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ed2",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwytdq1",
          "author": "PromptOutlaw",
          "text": "Congrats bud!! I really underestimated how much hard work goes into these. I’ve been humbled recently and I’m not even half way",
          "score": 1,
          "created_utc": "2025-12-31 19:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25e8e",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzsdsy",
      "title": "Suggest a model for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "author": "Bright_Dot113",
      "created_utc": "2025-12-30 20:10:25",
      "score": 38,
      "num_comments": 25,
      "upvote_ratio": 0.98,
      "text": "Hello, I have 9950x3d with 64GB RAM and 5070 ti \n\nI recently installed LM Studio, which models do you suggest based on my hardware for the following purposes. \n\n1. Code in python and rust \n\n2. DB related stuff like optimising queries or helping me understand them. (Postgresql)\n\n3. System and DB design.\n\nAlso what other things can I do?\nI have heard lot about MCP servers but I didn't find any MCP servers useful or anything related to my workflow if you have any suggestions that would be great! ",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwsjtel",
          "author": "SimplyRemainUnseen",
          "text": "I'd suggest unsloth/Nemotron-3-Nano-30B-A3B-GGUF. You'll need to offload to system memory, but you'll have a relatively fast and intelligent model that's good at using tools.\n\nFor MCP servers check out [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)",
          "score": 16,
          "created_utc": "2025-12-30 20:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo35r",
              "author": "Big-Masterpiece-9581",
              "text": "What kind of speeds to you think he could get with this model and that configuration? Anything particular you like about that model or unsloth?",
              "score": 3,
              "created_utc": "2025-12-31 03:12:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxynte",
                  "author": "SimplyRemainUnseen",
                  "text": "Speeds would be pretty fast I imagine. Faster than my work laptop with an iGPU (which runs it faster than I can read).\n\nThe model itself is fully open source (data too!) and made by NVIDIA. The performance of the model exceeds other models in the same parameter range of 20-32b and has very long context (up to a million tokens).\n\nNVIDIA worked with Unsloth for day 0 quants of the model. Unsloth makes efficient dynamic quants of models that retain very high levels of accuracy at lower precision. Considering OP will need to offload to system memory, GGUF is ideal as it's designed for that use case.",
                  "score": 5,
                  "created_utc": "2025-12-31 17:11:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1r491",
                  "author": "Hairy_Candy_3225",
                  "text": "I use a similar setup with 9950x3d 96 GB RAM and 5080 16 GB VRAM and use Nemotron 3 nano 30b Q6. Tokens per second heavily depends on context used. I've written a script to run a benchmark test and measure speed with all combinations of different context length / % GPU-offload layer / force experts on CPU on vs off.\n\nWith smaller context (i.e. 10k tokens) I get up to 16 TPS. This drops to around 10 @200k tokens. I'm no expert but I don't think there will be a big difference between 5070 or 5080 if VRAM is the same.\n\nWhat i can't understand is that with nemotron it does not matter how much layers I offload to GPU. The model is around 30 GB but only between 4 and 6 GB VRAM is used regardless of offload setting. It seems only the active layers are offloaded to GPU. This was different when I qwen 2.5, where I had to balance the exact number of layers that would fit in VRAM. As soon as you try to offload more layers than fit in VRAM, regular RAM will be used as shared VRAM and TPS drop drastically. \n\nProbably nothing new here for most redditors on this sub but i'm new to local LLM's and had to figure out a lot of things myself.",
                  "score": 2,
                  "created_utc": "2026-01-01 07:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsmq4k",
          "author": "beedunc",
          "text": "Qwen3coder, whatever fits.",
          "score": 9,
          "created_utc": "2025-12-30 20:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtu8zi",
          "author": "No-Consequence-1779",
          "text": "Qwen3-coder-30b. The largest quant you can run. Dense model is good or moe.  Keep in mind coder specific models are specialized for coding. Many like oss 120b though that size is not necessary. ",
          "score": 6,
          "created_utc": "2025-12-31 00:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt434d",
          "author": "Toastti",
          "text": "If you add another 64gb of ram you can run a Q2 quant of Minimax m2.1. it will probably be 7tk/s or so but it is almost certainly the smartest agentic coding model you can reasonably run",
          "score": 3,
          "created_utc": "2025-12-30 22:03:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt53sn",
              "author": "Your_Friendly_Nerd",
              "text": "q2? isn‘t that gonna suck?",
              "score": 3,
              "created_utc": "2025-12-30 22:08:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtv56k",
                  "author": "Worried_Goat_8604",
                  "text": "No unsloth dynamic quant v2 usually dosnt reduce that much quality",
                  "score": 1,
                  "created_utc": "2025-12-31 00:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwugr2q",
              "author": "Individual_Gur8573",
              "text": "I agree I found minimax 2.1 IQ2_M the smartest model after GLM 4.5 air ",
              "score": 1,
              "created_utc": "2025-12-31 02:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwuo8ix",
                  "author": "Karyo_Ten",
                  "text": "GLM Air which quant?",
                  "score": 1,
                  "created_utc": "2025-12-31 03:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlq1uw",
              "author": "Wild_Requirement8902",
              "text": " you can run iq4xs with 128 gb ram v and get 7 tk/s with 128gb of ram (with an old xeon(2680v4) and quad channel ddr4 @ 2400,) using lm studio and a 5060ti & a 3060 12gb I can get up to 131072 context @ q8 with flash attention. That what i am using, but sometime it crash and you have to reload the model and prompt processing is painful (35tk/s)",
              "score": 1,
              "created_utc": "2026-01-04 10:15:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwviewu",
          "author": "Count_Rugens_Finger",
          "text": "Qwen3-coder-30B-A3B, Nemotron-3-Nano-30B-A3B, Devstral-Small-2-24B",
          "score": 3,
          "created_utc": "2025-12-31 06:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsooqq",
          "author": "FullstackSensei",
          "text": "I'd say try a bunch of MoE all the way up to gpt-oss-120b and see where your pain threshold is for speed. IMO, you should keep a few at hand: a smaller model like Qwen3 Coder 30B, Nemotron 30B, gpt-oss-20b, Devstral 2 24B as daily drives, and larger models like gpt-oss-120b, GLM 4.5 air, Devstral 2 123B (Q4), etc for when the smaller models get stuck or can't solve whatever issue you have.",
          "score": 4,
          "created_utc": "2025-12-30 20:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt5rhp",
          "author": "Level_Wolverine_141",
          "text": "I have the same system as you except I've got a 5080, and I'm just using Claude code max and it's pretty good.",
          "score": -4,
          "created_utc": "2025-12-30 22:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww8pra",
              "author": "g33khub",
              "text": "your 5080 doesn't matter, I can use Claude code on a raspberry pi",
              "score": 2,
              "created_utc": "2025-12-31 10:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyw1b4",
      "title": "I built Plano(A3B) - fast open source LLM for agent orchestration that beats frontier LLMs",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5rp16cxd57ag1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-29 19:44:28",
      "score": 38,
      "num_comments": 14,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyw1b4/i_built_planoa3b_fast_open_source_llm_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwoqem5",
          "author": "ThsYWeCntHveNiceTngs",
          "text": "your research page is more advert than research and the blog provides more detail, but not much. Is there an Arxiv link or anything to read about your method and how you generated the proposed results?",
          "score": 6,
          "created_utc": "2025-12-30 06:07:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwosnmj",
              "author": "AdditionalWeb107",
              "text": "The huggingface models page has more details. Although we are in the process of publishing the arxiv paper",
              "score": 0,
              "created_utc": "2025-12-30 06:25:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxc7j4",
          "author": "False-Ad-1437",
          "text": "This is not open source. \n\nIt imposes non-open requirements (attribution, use restrictions, redistribution constraints and separate commercial licensing for certain uses) that violate the open-source criteria defined by the OSI/FSF.",
          "score": 2,
          "created_utc": "2025-12-31 15:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufm16",
          "author": "Purple-Programmer-7",
          "text": "How does this differ from Arch that you’ve previously pushed for the past year?",
          "score": 1,
          "created_utc": "2025-12-31 02:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui3lf",
              "author": "AdditionalWeb107",
              "text": "Arch was about model routing. Plano is about orchestration, which is a slightly more complicated set of tasks. Plano is the next major upgrade to Arch with several new capabilities for agentic applications like filter chains, agent signals, and even more robust model gateway.\n\n\nBy the way, people were confusing arch with arch Linux so we thought it was a better time to rename the project.\nTry Plano 🙏",
              "score": 1,
              "created_utc": "2025-12-31 02:37:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyaxs3",
          "author": "AdditionalWeb107",
          "text": "That’s fair - it should say open weights. And the license is very permissive except for one deployment type",
          "score": 1,
          "created_utc": "2025-12-31 18:12:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwopty3",
          "author": "maigpy",
          "text": "what is model-native?",
          "score": 1,
          "created_utc": "2025-12-30 06:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqe6c",
              "author": "AdditionalWeb107",
              "text": "It’s integrated with small LLMs - central to how the project is built.",
              "score": -1,
              "created_utc": "2025-12-30 06:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6v7x",
                  "author": "maigpy",
                  "text": "integrated with small llms translates to \"model-native\"?\n\nI don't quite understand, if you could use more words to describe what's going that would help.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnderh",
          "author": "Lyuseefur",
          "text": "Planning already to use it in next build of nexora follow us\n\nHttps://github.com/jeffersonwarrior/nexora",
          "score": 0,
          "created_utc": "2025-12-30 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnwumq",
              "author": "AdditionalWeb107",
              "text": "Okay - thanks. Would love the feedback. And if you like our project, don't forget to star it too",
              "score": 1,
              "created_utc": "2025-12-30 02:56:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q7e34g",
      "title": "Does it make sense to have a lot of RAM (96 or even 128GB) if VRAM is limited to only 8GB?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q7e34g/does_it_make_sense_to_have_a_lot_of_ram_96_or/",
      "author": "mark_17000",
      "created_utc": "2026-01-08 15:03:51",
      "score": 37,
      "num_comments": 45,
      "upvote_ratio": 0.95,
      "text": "Starting to look into running LLMs locally and I have a question. If VRAM is limited to only 8GB, does it make sense to have an outsized amount of RAM (up to 128GB)? What are the technical limitations of such a setup?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7e34g/does_it_make_sense_to_have_a_lot_of_ram_96_or/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyfjjfc",
          "author": "uti24",
          "text": "The limitation is speed.\n\nTypical DDR5 RAM bandwidth is around 80 GB/s. With 128 GB of system RAM, you could run an 80B model (quantized to Q8) at roughly 1 token/s. (you can very roughly calculate t/s by dividing memory bandwidth/model size, in this case 80GB/s / 80B@Q8 = 1)\n\nTypical VRAM bandwidth varies widely, from about 200 GB/s to 2000 GB/s on high-end gaming GPUs. If you somehow had 128 GB of VRAM, you could run an 80B Q8 model at roughly 2.5 tokens/s on the low GPU end and up to 25 tokens/s on the high end GPU.\n\nThere are also MoE (Mixture-of-Experts) models. While they may be 100B-parameter models overall, they do not use the entire model for every token, only a subset, for example \\~5B parameters. Because of this, even with a 100B MoE model running from system RAM, you could still achieve something like 20 tokens/s.",
          "score": 29,
          "created_utc": "2026-01-08 17:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhetbf",
              "author": "mark_17000",
              "text": "Thanks! I'll take a look at MoE models, too",
              "score": 5,
              "created_utc": "2026-01-08 22:09:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz988gh",
                  "author": "Suitable-Program-181",
                  "text": "Check deepseek papers, they do 670b but MoE is 37b active only some wild stuff like that , so pretty much you run 37b 0r 30, forgot. \n\n\"por A Liu · 2024 · Mencionado por 3141 — We present ***DeepSeek-V3***, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.\\[",
                  "score": 1,
                  "created_utc": "2026-01-12 23:34:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyerpgm",
          "author": "PsychologicalWeird",
          "text": "Are you looking to buy ram now? If so check prices first and then when you have picked your chin off the floor, consider funneling some of that money into a better GPU.",
          "score": 19,
          "created_utc": "2026-01-08 15:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyetm8o",
              "author": "mark_17000",
              "text": "There are limitations. Only 8GB is possible for VRAM. RAM prices aren't a limiting factor. Just want to know from a technical perspective if more RAM would be useful.",
              "score": 6,
              "created_utc": "2026-01-08 15:20:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nygwrz6",
                  "author": "simracerman",
                  "text": "It would for some MoE models where you can offload experts to RAM and still have decent speed. Without enough RAM, you won’t be running those models.",
                  "score": 2,
                  "created_utc": "2026-01-08 20:51:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyexjzb",
          "author": "Medium_Chemist_4032",
          "text": "Depends. I have 128 ram and am trying out bigger MoE's, but those that I tried didn't cross the minimum productivity threshold for coding, for example. It often boils down to: \"wow, great start, let's try spec'ing out a simple webservice\" and they can't produce a single working file, then loop. I have only spot tested a few models, nothing really representative",
          "score": 5,
          "created_utc": "2026-01-08 15:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhezrz",
              "author": "mark_17000",
              "text": "How much VRAM do you have in this setup?",
              "score": 1,
              "created_utc": "2026-01-08 22:10:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyk7ro5",
                  "author": "Medium_Chemist_4032",
                  "text": "3x3090",
                  "score": 1,
                  "created_utc": "2026-01-09 08:09:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylya60",
              "author": "uti24",
              "text": ">Depends. I have 128 ram and am trying out bigger MoE's, but those that I tried didn't cross the minimum productivity threshold for coding, for example. It often boils down to: \"wow, great start, let's try spec'ing out a simple webservice\" and they can't produce a single working file, then loop.\n\nNot actually my experience with coding, even GPT-OSS-20B already building working web apps, often one shot, sometimes they need a turn to fix errors, GPT-OSS-120B even better.\n\nIt's dumb at writing prose or whatever, but actually pretty good at coding.",
              "score": 1,
              "created_utc": "2026-01-09 15:21:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylyvyu",
                  "author": "Medium_Chemist_4032",
                  "text": "Yeah, I'm pretty sure I had a borked llama.cpp version. I updated after and the warning about unused layer disappeared. I also had some CUDA errors for one day and it was patched as well.\n\nWhich runner and version are you using?",
                  "score": 2,
                  "created_utc": "2026-01-09 15:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfn91s",
          "author": "netroxreads",
          "text": "I have M3 Ultra with 256GB and the entire model of 120b gpt-oss is in UMA RAM. It generates around 70 tokens per second. I imagine that with models using a hybrid of GPU/CPU RAM on PC, it'll be significantly slower.",
          "score": 3,
          "created_utc": "2026-01-08 17:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhf4it",
              "author": "mark_17000",
              "text": "Yes, I am jealous af of UMA RAM",
              "score": 1,
              "created_utc": "2026-01-08 22:11:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyp87tx",
              "author": "c4cheeku",
              "text": "It’s an MOE. So, do not get too excited by the tps you’re seeing.",
              "score": 1,
              "created_utc": "2026-01-10 00:33:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf5mte",
          "author": "pmttyji",
          "text": "Since you can't upgrade VRAM, that bulk RAM is decent alternative. For MOE models at least.\n\n[I got 30+ t/s for Qwen3-30B-A3B(Q4) with 8GB VRAM + 32GB RAM](https://www.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/).\n\n[I got 25+ t/s for Qwen3-30B-A3B(Q4) with 32GB RAM](https://www.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/).\n\nSo 96-128GB RAM(DDR5) could do so better.",
          "score": 3,
          "created_utc": "2026-01-08 16:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhfch0",
              "author": "mark_17000",
              "text": "cool, thanks for the insights!",
              "score": 1,
              "created_utc": "2026-01-08 22:12:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyexrsu",
          "author": "tom-mart",
          "text": ">If VRAM is limited to only 8GB\n\n\nCan you expand on this please? Where is VRAM limited to 8GB? What do you mean by that?",
          "score": 2,
          "created_utc": "2026-01-08 15:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyexytc",
              "author": "mark_17000",
              "text": "There is only 8GB of VRAM available in this case.",
              "score": 2,
              "created_utc": "2026-01-08 15:40:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyeyvjf",
                  "author": "tom-mart",
                  "text": "Oh, I get it. Well, I had local LLM hosted on a 6GB RTX A2000 on the system that had 128GB RAM (and 56 Xenon cores). I was able to run models far exceeding the 6GB, including 120b GPT-OSS with full 128k context widnow. 6GB in VRAM, and another 70GB in system RAM. It was slow, really slow, but it worked.",
                  "score": 3,
                  "created_utc": "2026-01-08 15:44:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyeutq6",
          "author": "Smooth-Cow9084",
          "text": "With more VRAM you can run some of the bigger MOE models at acceptable speed, but with 8gb not so much. \n\n\nMaybe look into needs for oss-120b which would indeed be benefitted from 96-128GB. \nBase model is ~60GB but not sure how much more for context... plus a 8gb GPU might still fall kinda short",
          "score": 1,
          "created_utc": "2026-01-08 15:26:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf0917",
          "author": "juggarjew",
          "text": "The model will overflow into RAM, so it will run, but very slowly, dependent on your RAM bandwidth. Most people have only dual channel RAM setups so it'll be very slow, even on DDR5.",
          "score": 1,
          "created_utc": "2026-01-08 15:50:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfkm13",
          "author": "Ummite69",
          "text": "If you want to run big model and don't care about performance (token/s) then yeah it makes sense to have lot of RAM.",
          "score": 1,
          "created_utc": "2026-01-08 17:20:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhctfe",
          "author": "LairdPopkin",
          "text": "If by RAM you mean CPU RAM and not GPU RAM, then yes, adding more CPU RAM doesn’t help much, anything that doesn’t fit into the GPU RAM will run in the much slower CPU, so in practice you really want the whole model to fit into GPU RAM.",
          "score": 1,
          "created_utc": "2026-01-08 22:01:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyheo1c",
              "author": "mark_17000",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-08 22:09:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhjrna",
          "author": "Loskas2025",
          "text": "no",
          "score": 1,
          "created_utc": "2026-01-08 22:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhqov4",
          "author": "ngless13",
          "text": "So, I didn't listen to the advice at the time and bought extra ram before the price crunch. I have an intel 265k with both a 5060ti and 5070ti and 128 ddr5. I'm currently running ubuntu... what sort of MoE model can I run locally taking advantage of my hardware?",
          "score": 1,
          "created_utc": "2026-01-08 23:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyod2z9",
              "author": "BigBadEvilButterfly",
              "text": "Try running gpt-oss-120b via llama.cpp with the --n-moe-cpu setting\n\nsomething like this but with 1 gpu https://www.reddit.com/r/LocalLLaMA/comments/1pjv5wz/how_to_properly_run_gptoss120b_on_multiple_gpus/",
              "score": 2,
              "created_utc": "2026-01-09 21:54:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhyopn",
          "author": "According_Study_162",
          "text": "GPU/VRAM is more important than system ram. unless it's unified memory. (apple computers for example.)",
          "score": 1,
          "created_utc": "2026-01-08 23:46:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyibwb8",
          "author": "AnalyticArts",
          "text": "I run the quen3-coder:30b from ollama on a machine with a 2gb graphics card, but it has a 32 core threadripper and 256 gb of memory. It works ok for basic coding tasks. It is usable but won't win any speed records.",
          "score": 1,
          "created_utc": "2026-01-09 00:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyinhdi",
          "author": "ChillDesire",
          "text": "For my system, I have 16GB of VRAM and 64GB system RAM.\n\nI can run a 70b model at Q4 with ~1tokens/s.\n\nFor comparison (although it's far from apples to apples) I'll hit 80tokens/s with a 13B model that fits fully in VRAM.\n\nSo the speed tradeoff is real when using RAM, but it does at least let you run these larger models.",
          "score": 1,
          "created_utc": "2026-01-09 01:56:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylvutj",
          "author": "XccesSv2",
          "text": "No, I did the same and bought very fast DDR5 RAM (96GB 6800 MT/s) but its so far behind. Its not worth it. I got now a Radeon Pro W7800 48GB for 1700€ and its insane. Spend your money on a good graphics cards instead of RAM.",
          "score": 1,
          "created_utc": "2026-01-09 15:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyn4048",
          "author": "HollowCoati",
          "text": "Depending on your hardware, it might be interesting that there was a post a while back by u/MLDataScientist about running gpt-oss 120B with decent speed (~20 t/s?) on an AMD M780 system with 96GB of RAM (Linux required though, I doubt the same techniques work on Windows). I'm on my phone so I can't trivially get the actual link, but if you search r/LocalLlama for \"AMD M780 iGPU\", it should be one of the first hits.\n\nGood luck!",
          "score": 1,
          "created_utc": "2026-01-09 18:28:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyn7ksu",
              "author": "mark_17000",
              "text": "Thanks mate",
              "score": 1,
              "created_utc": "2026-01-09 18:44:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyqvcx6",
          "author": "phu54321",
          "text": "Say hello to a3b",
          "score": 1,
          "created_utc": "2026-01-10 06:41:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg6ddc",
          "author": "Miserable-Dare5090",
          "text": "GPUs compute 100x faster thsn CPUs. VRAM is connected at 100X bandwidth of system RAM. That’s your prompt prefill (compute) and decode (bandwidth) limitations right there",
          "score": 1,
          "created_utc": "2026-01-14 00:03:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg011n",
          "author": "Witty_Mycologist_995",
          "text": "Yes. Get a lot of ram to run a fat MoE model",
          "score": 1,
          "created_utc": "2026-01-08 18:27:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3asxm",
      "title": "Which tools should I be looking at? Want to use local AI to tailor my resume to job descriptions",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "author": "cspybbq",
      "created_utc": "2026-01-04 00:16:54",
      "score": 35,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "I'm job hunting and trying to learn more about AI at the same time. \n\nI want the AI to be aware of all my resume versions (15ish versions) and to tailor new versions of my resume based on the contents of those resumes, plus job descriptions I give it. I'd also like it to evaluate a job description and tell if I'm a good fit or not, based on my resumes. \n\nIs this something I can set up on my local computer? \n\n * AMD Ryzen 5700G\n * Nvidia 3070\n * 64G RAM\n * Running Debian\n\nThere are so many models and variants of models that I'm not really sure where to start. I have played a bit with ollama (cli) and open-webui but haven't really figured out how to set up RAG correctly to handle my documents or get any sort of professional level output.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxk0cgf",
          "author": "jinnyjuice",
          "text": "Might want to also post to /r/localllama -- I would be interested in the responses also on how people judge different models to be strong in which dimensions!",
          "score": 4,
          "created_utc": "2026-01-04 02:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjgmey",
          "author": "fandry96",
          "text": "Go to gemini. Make a gem. Add your 10 best resumes to assets. \n\nYou can try defaulting to agent mode to try it. If not click save. \n\nPost the jobs there and tell Gemini to write a resume...",
          "score": 2,
          "created_utc": "2026-01-04 00:46:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3os4p",
      "title": "Tencent HY-MT1.5, a specialized machine-translation model",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/rq2666gzsbbg1.png",
      "author": "etherd0t",
      "created_utc": "2026-01-04 12:29:17",
      "score": 35,
      "num_comments": 4,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3os4p/tencent_hymt15_a_specialized_machinetranslation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxskc30",
          "author": "Lost-Dot-9916",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-05 10:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzg9wj",
          "author": "Naive-Pear-9579",
          "text": "Interesting. Is this also suitable for localization (L10N)?",
          "score": 1,
          "created_utc": "2026-01-06 10:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydfmi7",
          "author": "Abhirocks16",
          "text": "wow so finally an offline translation app can be built, thats amazing",
          "score": 1,
          "created_utc": "2026-01-08 10:05:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7est1",
      "title": "Nvidia CEO says it's \"within the realms of possibility\" to bring AI improvements to older graphics cards",
      "subreddit": "LocalLLM",
      "url": "https://www.pcgamer.com/hardware/graphics-cards/nvidias-ceo-says-bringing-new-ai-tech-to-older-generation-gpus-is-within-the-realm-of-possibility/",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-08 15:31:28",
      "score": 35,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7est1/nvidia_ceo_says_its_within_the_realms_of/",
      "domain": "pcgamer.com",
      "is_self": false,
      "comments": [
        {
          "id": "nyewjnr",
          "author": "Formal-Hawk9274",
          "text": "i'm tired boss",
          "score": 31,
          "created_utc": "2026-01-08 15:34:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfgfja",
          "author": "MaruluVR",
          "text": "He is only saying this because independent devs backported FP8 to 20 and 30 series cards.\n\n[https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/)",
          "score": 10,
          "created_utc": "2026-01-08 17:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf8dl4",
          "author": "silenceimpaired",
          "text": "It’s within the realms of possibility that unicorns existed, and still exist today. \n\nIt’s highly improbable improvements are coming back to my 3090… let alone a P40* thanks to Nvidia.",
          "score": 9,
          "created_utc": "2026-01-08 16:27:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfgqfz",
              "author": "MaruluVR",
              "text": "Independent devs already backported FP8 to 20 and 30 series\n\n[https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/)",
              "score": 9,
              "created_utc": "2026-01-08 17:03:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfw96a",
                  "author": "silenceimpaired",
                  "text": "I fixed my comment to better reflect my point and the context of the post. \n\nIt is exciting to see we don’t need Nvidia for everything. Wish Intel and AMD would step up.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:11:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyhatqe",
          "author": "a1454a",
          "text": "That’s corporate speak for “no, it’s not in our best interest to do, feasible or not”",
          "score": 8,
          "created_utc": "2026-01-08 21:52:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhsaej",
          "author": "meowrawr",
          "text": "There is no financial incentive for them to do it though.",
          "score": 5,
          "created_utc": "2026-01-08 23:13:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyez4jf",
          "author": "Ainudor",
          "text": "![gif](giphy|B2NRUyRtmdFnO)",
          "score": 5,
          "created_utc": "2026-01-08 15:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz48phi",
          "author": "TheCh0rt",
          "text": "*looks at my old retired 3060 Ti and 3080 mining rigs rotting in the corner* … what’s this you say!?",
          "score": 1,
          "created_utc": "2026-01-12 06:03:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6juii",
          "author": "sambull",
          "text": "What a way to tell us we can't afford the new shit again",
          "score": 1,
          "created_utc": "2026-01-12 16:04:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfgmsl",
          "author": "sn2006gy",
          "text": "The Nvidia CEO is stuck in a bad spot he created. His company was born on the backs of PC enthusiasts who he has chosen to leave in the dust.  He could come out with a 96gb 5090 that could help AI on commodity PCs explode and such a video card would sell like hotcakes, but he can't and he won't because he's bet the future of Nvidia on valuations that suggest such a massive premium for Nvidia hardware to justify the very valuations of the companies.  So, we've gone from what gamers are willing to pay, to how much money someone can mine from a card to how much money an organization can control by owning a Nvidia.\n\nThe enshitification is almost complete and I'm amazed at how many people still ride the nvidia hype train.\n\nHe could have come back and said \"we've increased production 50% for consumer cards\" and had a \"wow, this AI is great!!\" response...\n\nbut nope.. \"we're cutting back production costs will go up and I'm wondering why everyone is so angry with ai\".\n\nWe don't want to rent it... we don't want to rent our games... we don't want to rent cloud gpus.  We want what made Nvidia great back.\n\nOr we hope someone else will fill the void... but since the chip market went so vertical and everyon pre-bought the fab space and fab time from a few vertically integrated fabs here we are... \n\ncapitalism is failing us on this one... it's just another wealth grab.",
          "score": 1,
          "created_utc": "2026-01-13 21:52:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcuyh2",
      "title": "What is the biggest local LLM that can fit in 16GB VRAM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/",
      "author": "yeahlloow",
      "created_utc": "2026-01-14 18:21:44",
      "score": 35,
      "num_comments": 62,
      "upvote_ratio": 0.87,
      "text": "I have a build with an RTX 5080 and 64GB of RAM. What is the biggest LLM that can fit in it ? I heard that I can run most LLMs that are 30B or less, but is 30B the maximum, or can I go a bit bigger with some quantization ?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzlo7nw",
          "author": "BigYoSpeck",
          "text": "What's an acceptable speed for you?\n\n\nI have a Ryzen 9 5900x, 64gb DDR4 3800, and a 16gb Radeon RX 6800 XT\n\n\nI can run gpt-oss-20b at 120+ tok/s\n\n\nQwen3 30b partially offloaded to CPU at about 40 tok/s\n\n\nAnd gpt-oss-120b with 32 MOE layers offloaded to CPU at 23 tok/s\n\n\nI imagine your system would be faster still",
          "score": 14,
          "created_utc": "2026-01-14 20:12:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpebld",
              "author": "wisepal_app",
              "text": "i have a laptop hp zbook i7-12800h, 64 gb ddr5 4800 ram and 16 gb rtx a4500 gpu. i don't get your tok/s values. what do you use for that? i mainly use lm studio. what is your context window size and other settings? And which quants do you use?",
              "score": 4,
              "created_utc": "2026-01-15 10:23:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzs6g8r",
                  "author": "BigYoSpeck",
                  "text": "LM Studio does all or nothing with the MOE offloading, it doesn't allow the fine grained n layers setting you get in llama.cpp\n\nI haven't done much more than quick testing with gpt-oss-20b and Qwen3 30b to know the exact limits of context, but gpt-oss-20b fits entirely in VRAM, no special settings really \n\nThe trick with MOE models like Qwen3 30b and gpt-oss-120b is the `--n-cpu-moe N` parameter. I have it at 32 for gpt-oss-120b and run about 120k context. Can't remember the exact number or layers for Qwen3, you basically juggle between the context you want to fit in vram and the amount of layers for performance\n\nJust the mxfp4 GGUF's for gpt-oss, and I think Unsloth Q6\\_K\\_XL for Qwen3. Qwen3 Next doesn't perform far off either (I think around 35 tok/s) and is way more efficient for context",
                  "score": 2,
                  "created_utc": "2026-01-15 19:21:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o01pj92",
              "author": "CaterpillarOne6711",
              "text": "can I ask you where you run gptoss 20b?",
              "score": 1,
              "created_utc": "2026-01-17 03:44:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzncbxz",
          "author": "vertical_computer",
          "text": "## Everyone is missing the forest for the trees.\n\nu/yeahlloow \\- forget the number of params. **Look at the file size of the model.**\n\nYour GPU has 16GB of VRAM. That literally tells you the maximum size you can run - 16GB (plus leave some room for context, so realistically 14GB)\n\nThen, you go to HuggingFace and look at whatever model you want, and find someone who has produced a bunch of quantisations for it.\n\n**It literally tells you the file size for the model. That’s the answer.**\n\nIf it’s larger than about 14GB, it will spill over into RAM and run super slowly. If it’s larger than about 70GB *(14GB from GPU + 56GB RAM, leaving 8GB for the OS)* then you won’t be able to load it at all.\n\nHere’s a random example: [Nvidia Llama 3.3 Nemotron 49B](https://huggingface.co/bartowski/nvidia_Llama-3_3-Nemotron-Super-49B-v1_5-GGUF)\n\nhttps://preview.redd.it/kb3ibnylxedg1.png?width=1242&format=png&auto=webp&s=083f6fcf4c951c1a814cdaf8fd9fa712cec1c21b\n\nIf you run the IQ2\\_XXS version, it will fit entirely within your VRAM.\n\nAny larger and it will spill over into RAM. So you could absolutely choose the Q8 version @ 53GB, it will just be hella slow.",
          "score": 3,
          "created_utc": "2026-01-15 01:10:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp2slv",
              "author": "Bhilthotl",
              "text": "Also, depending on how you run it, via Ollama, llama.cpp, opencode, tiny llm, etc you might need to tweak things. Ive got a 16gb 5070 and found that I got best results with llama.cpp over Ollama due to spills over into onboard RAM",
              "score": 2,
              "created_utc": "2026-01-15 08:31:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpnagc",
              "author": "mintybadgerme",
              "text": "This might help? https://github.com/nigelp/ai-model-tracker",
              "score": 0,
              "created_utc": "2026-01-15 11:41:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlfulm",
          "author": "Fabulous_Fact_606",
          "text": "Just got into local LLM last week. Haven't had time to test all the different LLM. I've tried Jamba2 and Ministral\n\nMy setup is almost the same as yours : AMD 9900x RTX5080 32GB RAM running Windows 11 + WSL.\n\nHere's my docker setup:\n\nhttps://preview.redd.it/pguuuztk8ddg1.png?width=724&format=png&auto=webp&s=c2d32a37167151f7ec5acece247285fb6d762511\n\nCagra for RAG, Kokoro TTS , Parakett-sst, llava for vision, ministral (local llm) all in its own fastapi tunneled via wireguard to my VPS server. Can access my local llm on any device.\n\nAll of these were able to fit into ram and vram.\n\nResponse is pretty good. I have a front end that tunes the output. Gave it internet search / crawl /optical / speech to tex / text to speech. latency is not as good as the foundational models..but for a local llm it is still impressive.\n\n**Question:** \"Who wrote the Declaration of Independence?\"\n\n|Metric|Value|\n|:-|:-|\n|**Tier Used**|Tier 0 (LLM + RAG Validation)|\n|**RAG Facts Found**|0|\n|**Validation**|LLM ONLY (stable fact)|\n|**Confidence**|90%|\n|**Total Time**|**10.3 seconds**|",
          "score": 3,
          "created_utc": "2026-01-14 19:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlglrg",
              "author": "Fabulous_Fact_606",
              "text": "# Test 2: NFL Prediction\n\n**Question:** \"Make a prediction who will win the NFL matchups this weekend\"\n\n|Metric|Value|\n|:-|:-|\n|**Tier Used**|Tier 3 (Web Search + Prediction)|\n|**Web Sources**|4 (FOX Sports, ESPN, [NFL.com](http://NFL.com), CBS Sports)|\n|**Facts Gathered**|16 facts|\n|**Search Confidence**|95%|\n|**Total Time**|**1m 45s**|\n\n**Predictions:**\n\n|Game|Predicted Winner|Probability|Predicted Score|\n|:-|:-|:-|:-|\n|49ers vs Seahawks|**Seahawks**|65%|Seahawks 24, 49ers 21|\n|Rams vs Bears|**Rams**|60%|Rams 27, Bears 20|\n|Bills vs Broncos|**Broncos**|62%|Broncos 28, Bills 21|\n|Texans vs Patriots|**Patriots**|75%|Patriots 31, Texans 17|\n\n**Key Factors Used:**\n\n* Regular-season records (15-3 Patriots favored)\n* Historical playoff performance\n* Defensive strength analysis\n* Home/away considerations\n\n# Latency Comparison\n\n|Query Type|Tier|Time|\n|:-|:-|:-|\n|Stable historical fact|Tier 0|\\~10s|\n|NFL Prediction (web search)|Tier 3|\\~105s|",
              "score": 1,
              "created_utc": "2026-01-14 19:38:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlmkw9",
                  "author": "VenomFN",
                  "text": "Were there any tutorials you followed? Looking to set something similar up for myself",
                  "score": 1,
                  "created_utc": "2026-01-14 20:05:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl4acp",
          "author": "SKirby00",
          "text": "You probably won't be able to fit 30B models unless you're willing to crank the quant down below 4-bit, and accept potentially significant quality degradation. Remember, you also need to leave room for at least a bit of context, so a 14.5GB model might *technically* fit but is unlikely to be very useful.\n\nI have 24GB of VRAM (3060 Ti 8GB + 5060Ti 16GB) and I've found that on Qwen3-Coder-30B, I cap out around 18K context which feels like just barely enough to be useful. That seems to be the sweet spot right now for me, though the context is shorter than I'd like.\n\nWith 16GB of VRAM, my guess is that you could likely just barely run something like GPT-OSS-20B, with smaller models allowing for more context. The sweet spot for you will likely be closer to the 14B range, depending on what you're trying to use it for.",
          "score": 7,
          "created_utc": "2026-01-14 18:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzm6b2p",
              "author": "grocery_head_77",
              "text": "dumb question - how are you connecting your two GPUs? Are you using software like lossless scaling or ???\n\nasking as I have a 3070 Ti (my old gpu) and currently have a 5090 FE, but would love to use both like you!",
              "score": 5,
              "created_utc": "2026-01-14 21:35:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznvynb",
                  "author": "WishfulAgenda",
                  "text": "Check if your motherboard support 8x8 pice bifurcation. Stick the second card in, turn the machine on and good to go as long as your software supports it. I run two 5070ti with lm studio and works great.",
                  "score": 3,
                  "created_utc": "2026-01-15 03:04:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nznzffm",
                  "author": "Candid_Highlight_116",
                  "text": "just shove it into the second slot and LM Studio can use it \n\nnote that they can't parallelize the workload, so the entire process is as fast as the slowest card, and there will be overheads, but a lot of people find that acceptable. also this pipeline parallel mode don't get bottlenecked much by pcie so x1 mining adapters work too, allowing mining rigs to be repurposed",
                  "score": 3,
                  "created_utc": "2026-01-15 03:25:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzorgmt",
                  "author": "SKirby00",
                  "text": "I literally just plug them both into the computer at the same time, in separate PCIe slots. LM Studio literally just picks up the two GPUs and if your drivers are up to date, you're good to go. Just use a frontier model to help you figure it out if you run into problems.\n\nIt's not like using GPUs for gaming back in the day where you had to set up SLI and basically needed matching GPUs. Any two reasonably modern cards from the same brand (both Nvidia or both AMD) will be fine together.",
                  "score": 2,
                  "created_utc": "2026-01-15 06:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl0uq3",
          "author": "sam7oon",
          "text": "i would suggest for you to read more into it, there is a lot of types of LLMs, if you need it to invoke tools, code, write emails and so on,\n\nyou are most probably are gonna max at 14B models, however if you are planning to run a long context, you may need to downsize to 8B, \n\nBut again depending on what you are gonna use it for, its different, it would be nice if you can specify your target,",
          "score": 5,
          "created_utc": "2026-01-14 18:27:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzljv0p",
              "author": "mckirkus",
              "text": "Totally depends on how it's quantized.  80 billion parameters at Q4 is smaller than 80 billion at Q16",
              "score": 2,
              "created_utc": "2026-01-14 19:53:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm22vm",
                  "author": "sam7oon",
                  "text": "Yes i see more informed answers after mine, am afraid to touch quantized models , do you think a 80B Model quantized would produce better result than a non quant 8B models ? \n\nAssuming we are talking about generating email as an example.",
                  "score": 1,
                  "created_utc": "2026-01-14 21:16:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl3l8a",
          "author": "PermanentLiminality",
          "text": "If you are doing anything serious, you need space for context.  You can shoehorn a model that fills your VRAM.  It will work great for a simple one sentence question.  you will not be able to feed it a lot of data though.  Try and keep the model under 80% of your VRAM. so say 13Gb in size.  Not a hard rule, but it is a good starting place.  If you want to feed a big input, you may need something smaller.  \n\nYou can spill over into system RAM, but your speed will drop big time.  The Qwen 3 30B A3B actaully does OK even if it does spill over a bit.   It may be one of the largest models you can run effectively.",
          "score": 2,
          "created_utc": "2026-01-14 18:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw00oh",
          "author": "cibernox",
          "text": "The best you can fit in vram alone, probably gpt-oss 20B. If you use system ram, y comes down to the speed you want. But you can run 80B MoE models at reasonable speed",
          "score": 2,
          "created_utc": "2026-01-16 08:45:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzllh0o",
          "author": "karmakaze1",
          "text": "I'm running [qwen3-30b-a3b with IQ4_XS quant](https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF?show_file_info=Qwen_Qwen3-30B-A3B-IQ4_XS.gguf) which doesn't quite fit. I let some of the layers spill over to CPU but it still gets a decent number of tokens/sec.",
          "score": 1,
          "created_utc": "2026-01-14 20:00:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzncn80",
              "author": "vertical_computer",
              "text": "When it’s so close to fitting, why not drop to something like Q3_K_M and let it entirely fit on your GPU?",
              "score": 3,
              "created_utc": "2026-01-15 01:12:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzo7j3b",
                  "author": "karmakaze1",
                  "text": "Great advice. By default I've been using q4 quants. The other way to go is to use a dense model with fewer parameters which can be a bit slower with higher accuracy.",
                  "score": 1,
                  "created_utc": "2026-01-15 04:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlp63d",
          "author": "PM_ME_COOL_SCIENCE",
          "text": "I’m using a 5060 ti with 16gb vram, and with llama.cpp server I’m getting gpt oss 20b mxfp4 with 120k context. I can share the exact command, but nothing too crazy. Fully on gpu, 120 tk/s generation. If you’re willing to go slower, you can fit qwen 3 next or gpt oss 120b with MOE expert offloading. Those are the biggest yet somewhat performant models for mixed cpu/gpu.\n\nWhat’s your use case? How much context?",
          "score": 1,
          "created_utc": "2026-01-14 20:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlq28j",
              "author": "ristlincin",
              "text": "Jesus 120k? What's the model's size?",
              "score": 1,
              "created_utc": "2026-01-14 20:21:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlrv5j",
                  "author": "PM_ME_COOL_SCIENCE",
                  "text": "20b, 3.5b active. Gpt oss 20b",
                  "score": 1,
                  "created_utc": "2026-01-14 20:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzm7j0i",
          "author": "GutenRa",
          "text": "I discovered the qwen3-next 80b q4, which was recently updated by unsloth. \n\nA significant improvement in quality compared to the 30b and lower models.\n\n10t/s at 5900x & 5060ti16gb vram & 64gb ram.",
          "score": 1,
          "created_utc": "2026-01-14 21:40:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmd790",
          "author": "metamec",
          "text": "Qwen3-Next-80B-A3B (Thinking or Instruct) by Unsloth. Use the Q4 K M quant.\n\nI have an RTX 5080 too with 128 GB RAM, and that model only uses around 40 GB of RAM when configured optimally. Unfortunately I'm a long way from home until next week and unable to access my exact settings, but you basically want to offload 99 layers to the GPU and then force 36 MoE layers to the CPU.",
          "score": 1,
          "created_utc": "2026-01-14 22:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmh1dj",
          "author": "National_Cod9546",
          "text": "You can fit a 24b model at q4s and 16k context. More then that and it will become painfully slow as it spills into ram. Generally you want to use the biggest model you can at q4. At lower quants, the model becomes too stupid. Very large 70b+ can handle going to q3 or even q2 before the stops gets to bad. But you are usually better using a smaller model. With less than 16k context, it gets hard to give the model enough information to do anything useful. ",
          "score": 1,
          "created_utc": "2026-01-14 22:24:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmnqy0",
          "author": "Loud_Communication68",
          "text": "Check out the phi series",
          "score": 1,
          "created_utc": "2026-01-14 22:57:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq6ixt",
          "author": "ProfessionalYear5755",
          "text": "I have just built my rig B550, 5600X, 9060XT 16GB, (only 16GB 3200mhz at the moment) BUT what surprised me was when a model spills over in RAM it does NOT crawl. It is slower yes but is totally useable. I believed from insane amounts of research that RAM = crawl, It demonstrably does not. Happy days!",
          "score": 1,
          "created_utc": "2026-01-15 13:47:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvtjvy",
          "author": "techlatest_net",
          "text": "24B Q4\\_K\\_M fits clean on 16GB (RTX 5080), \\~140t/s with your 64GB RAM soaking KV cache. Push 30B Q3 or 32B Q2\\_K if you squash context to 2k, but quality dips. 30B isn't max—quantization gets you there, offload handles overflow.​",
          "score": 1,
          "created_utc": "2026-01-16 07:46:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwensw",
          "author": "mistrjirka",
          "text": "I would say one of the best ones is ministral 3 14B. Depends on what you want.",
          "score": 1,
          "created_utc": "2026-01-16 10:58:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyiopq",
          "author": "TheRiddler79",
          "text": "You could download gptoss120b offload most of it to ram and you'd still get good speed and a very intelligent ai. You should try",
          "score": 1,
          "created_utc": "2026-01-16 17:43:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00w93n",
          "author": "beryugyo619",
          "text": "Theoretically 8B fp16, 16B q8, 32B q4, 64B q2, 128B q1, so on. See the pattern?  \n\nAnd there comes overheads and context caches",
          "score": 1,
          "created_utc": "2026-01-17 00:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlenzq",
          "author": "ZoSoPa",
          "text": "http://www.canirunthisllm.net/",
          "score": 1,
          "created_utc": "2026-01-14 19:29:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzm2mm7",
              "author": "Dan_Wood_",
              "text": "SSL has expired and it’s a coming soon page hosted by PythonAnywhere …",
              "score": 2,
              "created_utc": "2026-01-14 21:18:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm4q8q",
                  "author": "Caltaire",
                  "text": "Don’t know how accurate this is but saw it posted elsewhere in response to the previous link.\n\nhttps://apxml.com/tools/vram-calculator",
                  "score": 1,
                  "created_utc": "2026-01-14 21:27:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlf0hm",
          "author": "Turbulent_Dot3764",
          "text": "16vram, I5 10°, 32gb ram\n\nGPS oss 20b 48k context, full gpu, ollama.\n\nRunning this plus my docker and others apps for development.\n\n\n24B models fit wit less context.\n\n\nBigger than 24B, only with smaller quantization",
          "score": 1,
          "created_utc": "2026-01-14 19:31:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoia8v",
              "author": "SexMedGPT",
              "text": "Which quant do you run?",
              "score": 1,
              "created_utc": "2026-01-15 05:33:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlmmoj",
          "author": "forthejungle",
          "text": "Only VRAM matters. RAM is irrelevant, especially when it comes to speed.",
          "score": 1,
          "created_utc": "2026-01-14 20:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzleapx",
          "author": "HealthyCommunicat",
          "text": "Why not just copy paste this exact question into gemini and get a really specific exact answer that is near instant, actually informational, and can give you even further specific info related to your setup? Do you actually care to learn?",
          "score": -5,
          "created_utc": "2026-01-14 19:27:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlk574",
              "author": "true-though",
              "text": "change your username.",
              "score": 8,
              "created_utc": "2026-01-14 19:54:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm6ssx",
                  "author": "HealthyCommunicat",
                  "text": "nawh. the basis of healthy communication is being upfront and blunt about things that would actually in long run benefit the other person. you don't think it would be good for OP to know to ask gemini? they want to get into llm's but dont realize that all the answers and all the information in the world is more at your fingertips than ever? go read all my comments, they're all extremely blunt - yet not one single person is able to refute what i say because every single person knows in the back of their head what im saying is right. tell me, doesn't it take much more time and effort and energy to make a post on reddit and wait for comments and reply and read them when you can do that all in one interaction with gemini?",
                  "score": 2,
                  "created_utc": "2026-01-14 21:37:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzm35ov",
                  "author": "Condomphobic",
                  "text": "He has a genuine point that I actually wonder about daily. \n\nA LLM like Gemini can give a **much** better and nuanced answer than humans can.",
                  "score": 2,
                  "created_utc": "2026-01-14 21:20:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q9xml5",
      "title": "Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026",
      "subreddit": "LocalLLM",
      "url": "https://www.techpowerup.com/345000/gigabyte-announces-support-for-256gb-of-ddr5-7200-cqdimms-at-ces-2026",
      "author": "GoodSamaritan333",
      "created_utc": "2026-01-11 11:59:30",
      "score": 34,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q9xml5/gigabyte_announces_support_for_256gb_of_ddr57200/",
      "domain": "techpowerup.com",
      "is_self": false,
      "comments": [
        {
          "id": "nyz30tq",
          "author": "silenceimpaired",
          "text": "Finally! What really held me back from buying this much ram was the motherboard and not the price. With the “slight” increase to ram prices I can sell my car to buy this ram and I will have no more barriers… provided they can ship me the ram and motherboard as I won’t have transportation anymore.",
          "score": 18,
          "created_utc": "2026-01-11 14:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz5s7s",
          "author": "Lux_Interior9",
          "text": "K",
          "score": 6,
          "created_utc": "2026-01-11 14:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza6efr",
          "author": "Sovairon",
          "text": "At this amount ram should come ECC by default to be honest..",
          "score": 2,
          "created_utc": "2026-01-13 02:39:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzytg0",
          "author": "IngwiePhoenix",
          "text": "Talk about a \"halo product\"... x)",
          "score": 1,
          "created_utc": "2026-01-11 16:56:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxzjhp",
      "title": "Google Open-Sources A2UI: Agent-to-User Interface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 19:08:03",
      "score": 27,
      "num_comments": 8,
      "upvote_ratio": 0.87,
      "text": "Google just released **A2UI (Agent-to-User Interface)** — an open-source standard that lets AI agents generate **safe, rich, updateable UIs** instead of just text blobs.\n\n👉 Repo: [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\n# What is A2UI?\n\nA2UI lets agents “**speak UI**” using a **declarative JSON format**.  \nInstead of returning raw HTML or executable code (⚠️ risky), agents describe *intent*, and the client renders it using **trusted native components** (React, Flutter, Web Components, etc.).\n\nThink:  \nLLM-generated UIs that are **as safe as data, but as expressive as code**.\n\n# Why this matters\n\nAgents today are great at text and code, but terrible at:\n\n* Interactive forms\n* Dashboards\n* Step-by-step workflows\n* Cross-platform UI rendering\n\nA2UI fixes this by cleanly separating:\n\n* **UI generation (agent)**\n* **UI execution (client renderer)**\n\n# Core ideas\n\n* 🔐 **Security-first**: No arbitrary code execution — only pre-approved UI components\n* 🔁 **Incremental updates**: Flat component lists make it easy for LLMs to update UI progressively\n* 🌍 **Framework-agnostic**: Same JSON → Web, Flutter, React (coming), SwiftUI (planned)\n* 🧩 **Extensible**: Custom components via a registry + smart wrappers (even sandboxed iframes)\n\n# Real use cases\n\n* Dynamic forms generated during a conversation\n* Remote sub-agents returning UIs to a main chat\n* Enterprise approval dashboards built on the fly\n* Agent-driven workflows instead of static frontends\n\n# Current status\n\n* 🧪 **v0.8 – Early Public Preview**\n* Spec & implementations are evolving\n* Web + Flutter supported today\n* React, SwiftUI, Jetpack Compose planned\n\n# Try it\n\nThere’s a **Restaurant Finder demo** showing end-to-end agent → UI rendering, plus Lit and Flutter renderers.\n\n👉 [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\nThis feels like a big step toward **agent-native UX**, not just chat bubbles everywhere. Curious what the community thinks — is this the missing layer for real agent apps?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwgkvue",
          "author": "bananahead",
          "text": "I am begging people to stop with the LLM written posts. Just post whatever prompt you used! That’s the post!",
          "score": 13,
          "created_utc": "2025-12-29 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi048d",
              "author": "leonbollerup",
              "text": "Why care? I rather want a LLM generated post than some bad version of English - not everyone speaks English native",
              "score": 4,
              "created_utc": "2025-12-29 05:47:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjksfb",
                  "author": "ak_sys",
                  "text": "Id rather tailor my discussion to a non English speaker than not understanding that there may be a disconnect getting lost in translation. \n\nThis explains a lot of \"reading comprehension\" issues I've noticed from commenters, where they seem to be responding to some diffuse sentiment of the post rather than  the actual nuanced point and position. Languages almost never just directly translate into one another, and inserting an llm in the middle of a discussion without informing the other party seems pretty dishonest and disrespectful.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:44:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwmr4i2",
                  "author": "bananahead",
                  "text": "So write a post in your native language and have it translated. That’s just as easy as whatever prompt created this and would be easier to read and more authentic.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgo2bh",
              "author": "Kamal965",
              "text": "The absolute state of Reddit now:\n\n* A lazy OP asks an LLM to create a post for them.\n* Users see LLM-isms and either:\n   * Ctrl + W\n   * Pass it on to an LLM to summarize it for them instead.\n* Another OP gives up on writing their own posts and starts using an LLM because they keep running into LLM-generated posts.\n\nRinse and repeat.",
              "score": 0,
              "created_utc": "2025-12-29 00:53:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nworg5v",
          "author": "ZITNALTA",
          "text": "Just curious if any knows if this somehow works with Google Antigravity IDE? By the way, I am NOT a dev so if this sounds like a newbie question that is why.",
          "score": 1,
          "created_utc": "2025-12-30 06:15:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcpoaw",
      "title": "Google just opensourced Universal Commerce Protocol.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcpoaw/google_just_opensourced_universal_commerce/",
      "author": "techlatest_net",
      "created_utc": "2026-01-14 15:08:25",
      "score": 25,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "**Google just dropped the Universal Commerce Protocol (UCP) – fully open-sourced! AI agents can now autonomously discover products, fill carts, and complete purchases.** \n\nGoogle is opening up e-commerce to AI agents like never before. The **Universal Commerce Protocol (UCP)** enables agents to browse catalogs, add items to carts, handle payments, and complete checkouts end-to-end—without human intervention.\n\n# Key Integrations (perfect for agent builders):\n\n* **Agent2Agent (A2A)**: Seamless agent-to-agent communication for multi-step workflows.\n* **Agents Payment Protocol (AP2)**: Secure, autonomous payments.\n* **MCP (Model Context Protocol)**: Ties into your existing LLM serving stacks (vLLM/Ollama vibes).\n\nLink: [https://github.com/Universal-Commerce-Protocol/ucp](https://github.com/Universal-Commerce-Protocol/ucp)\n\nWho's building the first UCP-powered agent? Drop your prototypes below – let's hack on this! ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcpoaw/google_just_opensourced_universal_commerce/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzke6vh",
          "author": "eli_pizza",
          "text": "What retailers actually support it? A protocol that nobody uses isn't too useful.",
          "score": 5,
          "created_utc": "2026-01-14 16:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzklq2l",
              "author": "FaceDeer",
              "text": "Google *just* opensourced it. How quickly are you expecting coders everywhere to be able to jump and implement something like this?",
              "score": 0,
              "created_utc": "2026-01-14 17:20:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzko7zf",
                  "author": "eli_pizza",
                  "text": "It’s got dozens of logos on the page as “endorsing” it but have any of them deployed or committed to deploying it?\n\nIf not what would a UCP agent even do right now, pretend order from pretend stores?\n\nAnyway, open protocols are better than closed ones but they’re not always “good.” Google AMP was open and it was bad and harmful to the web.",
                  "score": 1,
                  "created_utc": "2026-01-14 17:31:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmdvep",
                  "author": "FoxTimes4",
                  "text": "Opensourcing something that doesn’t work anywhere isn’t that useful so does it work somewhere today?",
                  "score": 1,
                  "created_utc": "2026-01-14 22:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlymaf",
          "author": "lanthos",
          "text": "Any idea how long Google is planning on supporting it? Does Gemini already use it or if not what is their roadmap?",
          "score": 1,
          "created_utc": "2026-01-14 21:00:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmqhnj",
          "author": "frobnosticus",
          "text": "Is it newly existing and open source or is it stable and in use...newly open sourced?",
          "score": 1,
          "created_utc": "2026-01-14 23:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqoe1z",
          "author": "sn2006gy",
          "text": "Since agents are probabilistic - what is the utility of agents doing this considering, the risks involved?\n\nThis sounds like something being branded agentic for the sake of branding \"slap AI or AGENT on it\" without people fully understanding that agentic workloads are still probabilistic and \"buying shit\" shouldn't have a probability attached to it.",
          "score": 1,
          "created_utc": "2026-01-15 15:18:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8mdwt",
      "title": "Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q8mdwt/hermitai_chat_with_100gb_of_wikipediadocs_offline/",
      "author": "Smart-Competition200",
      "created_utc": "2026-01-09 22:26:21",
      "score": 23,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "I wanted to use Local AI along side my collection of ZIM files (Wikipedia, StackExchange, etc.) entirely offline. But every tool I tried had the same issues:\n\n1. Traditional vector search kept retrieving irrelevant chunks when the dataset was this huge.\n2. The AI would confidently agree with false premises pretending to be helpful\n\n Instead of just doing one big search and hoping for the best, Hermit breaks the process down. while not perfect i am happy with the results. I can only imagine it getting better as the efficiency and intelligence of local models improve over time.\n\n* **Joint 1 (Extraction)**: It stops to ask \"Who/What specifically is this user asking about?\" before touching the database.\n* **Joint 2 (JIT Indexing)**: It builds a tiny, ephemeral search index *just for that query* on the fly. This keeps it fast and accurate without needing 64GB of RAM.\n* **Joint 3 (Verification)**: This is the cool part. It has a specific \"Fact-Check\" stage that reads the retrieved text and effectively says, \"Wait, does this text actually support what the user is claiming?\" If not, it corrects you.\n\n**Who is this for?**\n\n* Data hoarders (like me) with terabytes of ZIMs.\n* Researchers working in air-gapped environments.\n* Privacy advocates who want zero data leakage.\n\n**Tech Stack:**\n\n* Pure Python + `llama-cpp-python` (GGUF models)\n* Native ZIM file support (no conversion needed)\n* FAISS for the JIT indexing\n\nI've also included a tool called **\"Forge\"** so you can turn your own PDF/Markdown folders into ZIM files and treat them like Wikipedia.\n\n**Repo:** [https://github.com/0nspaceshipearth/Hermit-AI](https://github.com/0nspaceshipearth/Hermit-AI)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q8mdwt/hermitai_chat_with_100gb_of_wikipediadocs_offline/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nywagyk",
          "author": "PitifulBall3670",
          "text": "I found this very informative. Appreciate the post",
          "score": 2,
          "created_utc": "2026-01-11 01:58:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyq0usq",
          "author": "woswoissdenniii",
          "text": "Love it. Where can I find more about .zim structure? Is it good for personal data? A body of invoices? Personal notes? Where is the benefit and how transforms data in and out of a .zim file? \n\nNice one. Cool project i had in mind too. Like the, convenient prepper- trappers keeper to go. Nothing beats snorting powdered milk while following fire making instructions straight from the well of knowledge. You know the 90‘s action series „time trax“? Just like that. A stiff upper lip chatbot hologram with a red hairdo and opinions.",
          "score": 1,
          "created_utc": "2026-01-10 03:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyr9nje",
              "author": "Smart-Competition200",
              "text": "thanks! so zim is basically a compressed archive format designed by kiwix for offline wikipedia. the spec is open: [https://wiki.openzim.org/wiki/ZIM\\_file\\_format](https://wiki.openzim.org/wiki/ZIM_file_format)\n\n  zim is optimized for *read-heavy* content you rarely change. think encyclopedias, documentation, research papers, static reference material. it compresses incredibly well (100gb wikipedia → \\~90gb zim) and has built-in full text search.\n\nfor invoices/personal notes? probably not ideal. zim files are meant to be *built once* then read many times. there's no append or edit - you'd have to rebuild the whole archive to add one invoice. however i am visualizing some sort of memory system that could work for your use case but i need time to think about it. if you have a static corpus you want to archive and query offline (research papers, ebooks, documentation, manuals), hermit includes a tool called **Forge** that lets you build custom zims from PDFs, markdown, epub, docx, etc. so you could definitely bundle your prepper manuals into a searchable offline knowledge base.\n\nand lmao i actually haven't seen time trax but ill add it to the list!",
              "score": 3,
              "created_utc": "2026-01-10 08:49:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz1iomw",
                  "author": "Smart-Competition200",
                  "text": "thanks for the feedback. honestly, \"one day more than you\" is exactly why i built this. maintaining a library that works when the internet doesn't is the core mission.\n\nto answer your technical questions.... *it's not regex. it uses a multi-joint pipeline. first, a small model (llama3.2) extracts entities to handle synonyms. then we do a hybrid search (bm25 + vector). finally, a scorer model (qwen2.5) reads the actual article titles and grades them 0-10 on relevance.*  *it's designed for general use, not my personal style. the entity extraction layer normalizes user input, so you can just ask natural questions like \"how does a diesel engine work\" or \"roman empire collapse\" and it figures it out.* \n\n  \n*a node graph is a great idea. i'm focused on raw retrieval accuracy and efficiency at the moment.*",
                  "score": 2,
                  "created_utc": "2026-01-11 21:10:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyu51gd",
              "author": "Lumpy_Quit1457",
              "text": "You sound like me. I'm trying to wade through about 200gb of prepper material, have a few different weights of Wikipedia scattered across multiple drives (for reasons that still elude me), and such n so forth. Getting an llm to help with that may be a whole nuther pickle.",
              "score": 1,
              "created_utc": "2026-01-10 19:21:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyy80jc",
                  "author": "woswoissdenniii",
                  "text": "![gif](giphy|L4FEYn7StSqNq|downsized)\n\nIm not affiliated with them, nor am i able to not raise an eyebrow if the only translation available in app is en/cn, but i must admit, that HYPERLINK from nexa.ai (from memory, search for yourself), gave me some tremendous insight into my stack of whackymoleded „one day more than you“ -literature.\n\nBe advised to monitor any related ports for traffic. As you should with any software on your hardware. After setup, preferably airgapped, you will have a low maintenance RAGbot with actual usefulness. Other close contenders: CLARAVERSE, Clipbeam(smth.?!), Eeagle Image browser (when setup correct for file classification etc., and a german tool from munich, I can’t remember (a visual personal knowledge management suite with good privacy policies and a heavy feature set.\n\nThere are ways. But, as you already know (but some need to adhere more to): seek privacy, seek anonymity, never skimp on those two. Block ports, set rules, monitor, obfuscate and anonymize when online. It may be too late, but as i said: ONE DAY MORE THAN YOU - is the game.",
                  "score": 3,
                  "created_utc": "2026-01-11 10:20:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyuifes",
                  "author": "Smart-Competition200",
                  "text": "yeah the indexing was a journey. my first approach was trying to pre-index everything into faiss/bm25 which works for small zims but completely falls apart at scale. the full english wikipedia is like 100gb compressed - trying to chunk and embed every article upfront would take days and produce an index bigger than the source.\n\nended up making something called JIT  (just-in-time) for indexing. the system only indexes articles relevant to the current query on the fly. sounds slower but it's actually faster for real usage since you're not searching through millions of irrelevant vectors.\n\nhttps://preview.redd.it/f6n5j4ka0lcg1.png?width=900&format=png&auto=webp&s=6ff4b933443a1199e7a0c83d08ce396aeb8b1f8f",
                  "score": 2,
                  "created_utc": "2026-01-10 20:27:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q93vr7",
      "title": "Local code assistant experiences with an M4 Max 128GB MacBook Pro",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q93vr7/local_code_assistant_experiences_with_an_m4_max/",
      "author": "noodler-io",
      "created_utc": "2026-01-10 13:20:01",
      "score": 23,
      "num_comments": 66,
      "upvote_ratio": 0.93,
      "text": "Considering buying a maxed-out M4 Max laptop for software development and using a local LLM code assistant for privacy concerns. Does anyone have any practical experience, and can you recommend model types/sizes, share experience about latency, code assistant performance and general inference engine scaffold/IDE setup?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q93vr7/local_code_assistant_experiences_with_an_m4_max/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nysjbcj",
          "author": "RandomCSThrowaway01",
          "text": "If you want live speed inference (so refactoring and writing individual functions) - GPT-OSS-20B or Qwen Coder 30B. Aka two relatively small MoE models, they should be running very smoothly on your machine (they are alright on M4 Pro which has half the bandwidth). \n\nIf you want it to write longer pieces of code and are fine with waiting for a bit then GPT-OSS-120B is a good start. You do have enough VRAM to run both at the same time too so can route smaller tasks to a smaller model and use chat with the larger one.",
          "score": 8,
          "created_utc": "2026-01-10 14:41:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyw3b1a",
          "author": "StardockEngineer",
          "text": "Pretty bad.  Prefill speeds make my M4 Max pretty unusable.  Takes forever to process just the system prompt.  Not worth it.  I’m anxiously awaiting M5 Max",
          "score": 5,
          "created_utc": "2026-01-11 01:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxb8ad",
              "author": "Future_Command_9682",
              "text": "That sounds like a bug I was hitting while using LLM Studio, moved to Jan.ai problem solved.\n\nI use minimax and the experience isn’t so far compared to Claude",
              "score": 1,
              "created_utc": "2026-01-11 05:33:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyz0g75",
                  "author": "StardockEngineer",
                  "text": "It’s the same across all inference engines.  It’s a hardware limitation.",
                  "score": 1,
                  "created_utc": "2026-01-11 14:02:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyts077",
          "author": "Silentparty1999",
          "text": "Pay $20/month for Cursor, Copilot, Claude.  I use them all, and the local isn't anywhere near as good as the tuned stuff on the commercial side.\n\nPurely talking aboutr software development and design.",
          "score": 8,
          "created_utc": "2026-01-10 18:20:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyykgpo",
              "author": "noodler-io",
              "text": "I get it, but this would break my privacy requirements - I am suspicious of these tech companies’ T&Cs wrt privacy.",
              "score": 3,
              "created_utc": "2026-01-11 12:11:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz08v3v",
                  "author": "Silentparty1999",
                  "text": "All my personal code ends up on GitHub where it gets snarfed anyway...",
                  "score": 2,
                  "created_utc": "2026-01-11 17:43:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyzskxw",
              "author": "GCoderDCoder",
              "text": "I get that they have all these cloud models relatively cheap right now but I dont think they will make their returns on investment at these prices or at least i don't expect them to continue allowing $20 seats indefinitely. \n\nThis is the phase where they want us all to get comfortable giving them the reigns. Then when no one has the infrastructure to do it but them and they have killed the pool of tech workers, then they'll raise prices and companies wont have options to pivot. This is a recurring theme in this country yet people keep ignoring it and feeding into the cycle. \n\nYes today it is probably cheaper to use cloud. I projected a year ago they would start locking people out of the market from hosting there own through various means. That is the reason for this ram crisis.\n\n If they are paying more for hardware then common sense would say they will need to charge more right? So yes cheaper today. Is anyone thinking about sustainability in their practices? I encourage learning local AI hosting at least a a backup for when things get weird. If you already own it and get laid off you just need electricity. If you get laid off your probably not going to be paying 200/ month for ai",
              "score": 4,
              "created_utc": "2026-01-11 16:26:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzji7cc",
                  "author": "sn2006gy",
                  "text": "There needs to be a massive breakthrough in hardware and hopefully not behind closed doors/walled gardens.  For the life of me I can't see paying apple a 5 grand (in your example) is better than paying Cursor 20/month - Apple is part of the problem with modern day supply chain woes - they bought up so much capacity in chip supply and now people are waiting on Apple.\n\nWhat is needed is for some company to recognize demand in consume/prosumer ML space and fill it - and that company isn't nvidia right now. Nvidia's head is too far up OpenAI's ass.",
                  "score": 3,
                  "created_utc": "2026-01-14 14:12:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz08qou",
                  "author": "Silentparty1999",
                  "text": "We don't know where models or LLM or other xLMs are going to go.  The current may be an evolutionary dead end.  It may be that NPUs or some other xPU is going to be way better at the next gen.  I suspect my current hardware is going to be a curiosity in 3-5 years.\n\nI have a 64GB M1 Max that I use with models and will probably buy an M5 Max or Pro when they come out.  I've also got 64GB of VRAM in a PC.  None of that is nearly as good as the service models.  Local can do auto-complete. Paid model services can refactor my code or convert design documents into something that resembles code. A good local model would require all the wrapper stuff built by Cursor, Microsoft, and others in their public services. Don't know of any project making that happen, especially since local model folks (like me) won't pay to build those pieces.",
                  "score": 1,
                  "created_utc": "2026-01-11 17:43:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nysxyf4",
          "author": "ProfMooreiarty",
          "text": "I have an M4 max with 48gb (or 46 - I’m on my phone and don’t remember if it’s 46 or 48, but whatever that mid-tier one is. I’m a researcher, and my top priority is working on my projects. The majority of my use of LLM-as-assistant is on flagship services. I mostly use ChatGPT-5.2 pro, some opus 4.5, and surprisingly little Gemini although I have the $250/month tier with them (currently on an extended trial). \n\nI use local LLMs primarily as **objects** of my research, rather than as an LLM customer. I rarely use them for code generation or as an assistant. I can run models up to 30b without any issues. They are performant for my needs. \n\nI am currently waiting to see if Apple drops an M5 max or ultra. If they do, I will buy it a Mac Studio at launch with enough ram that I will wince when I buy it. Whatever their 2026/7 hw turns out to be, I’m buying a desktop version of it. \n\nI’m not really a hw person at this point in my life/career. I don’t want to spend time figuring out questions about getting my tools optimized to wring the most out of every last cpu cycle or byte of ram. I really do want it to “just work,” as the Apple folks used to say. \n\nI wish I had gotten more ram when I bought my mbp at launch - I am running into constraints daily. The problem is that I’m using my laptop as my pc (coding, writing, reading, web, etc) while using those same resources to run local LLMs. My work also involves me running extended mathematical functions on embeddings, so the performance of the m4 max comes into play there. \n\nI use both ollama and lm studio - lm studio has some performance advantages on the Mac. Both support my other apps/needs with apis and services, so I go back and forth. I also run associated services like litellm, depending on my needs. \n\nI highly recommend it as a platform, and you’d be better off than I am if you max out on the ram. Right now it’s the only thing I’d change. I believe the current Apple silicon performance tiers are M3 ultra > m4 max > M5 base > m4 others. \n\nHappy to answer any other questions.",
          "score": 6,
          "created_utc": "2026-01-10 15:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyu2jx3",
          "author": "HealthyCommunicat",
          "text": "All the models other people are mentioning are on the older side compared to the ones I have in mind.\n\nQwen 3 Next 80b runs at 50 tok/s minimum for me, 60+ at low context. Then look out for the upcoming MiroThinker 1.5 100b - the 30b model is the first 30b model to actually seem usuable for agentic coding for the first time ever. Devstral Small 2 is also great, but i know that people like us got the 128gb ram for a reason and want to be able to maximize it. \n\nIf you’re ok with the fans being loud, look into GLM 4.7 50 REAP, runs at 10 tok/s on m4 max, along with Minimax m2 reap, and if ur ok with stuff being a bit dumb go for minimax m2.1 but it’ll barely fit.\n\nI have the same m4 max 128 gb along with an m3 ultra 256 gb, and i use llm’s literally 6+ hrs a day minimum and have been trying to move purely to local solutions. Hit me up if you want more info or just want to talk about our setups and what we’re running. - i literally download and try out all the newest llm’s nonstop and expirement constantly.",
          "score": 2,
          "created_utc": "2026-01-10 19:09:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyua33v",
              "author": "Exciting_Narwhal_987",
              "text": "Can you please share comparison to Claud code?\n\nI mean locals LLMs are gpt 4 level or gpt 3 level in your experience?",
              "score": 1,
              "created_utc": "2026-01-10 19:46:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyuandx",
                  "author": "HealthyCommunicat",
                  "text": "If you go to https://artificialanalysis.ai/models and look scroll down and look at different charts and look at the top models, especially in the coding and agentic coding sections, GLM 4.7 and MiniMax m2.1 show up even higher than Claude, and those are the ones you are able to run once you spend $6000ish USD, along with Qwen 3. If you look at other score charts, even stuff like GPT OSS 120b are up there doing pretty well when it comes to more knowledge related stuff. This is what people mean when they say there is a race. Open puclicly downloadable and usuable models being able to stand their ground against even Claude. - no matter which chart you are looking at, there is a open weight model, so yes there is no all in one free to use LLM that can compete with claude or openai, but i have the choice of switching them out.",
                  "score": 2,
                  "created_utc": "2026-01-10 19:48:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyyminh",
              "author": "noodler-io",
              "text": "Great stuff, I’ll prob be in touch when I get started.",
              "score": 1,
              "created_utc": "2026-01-11 12:27:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzxjbsr",
              "author": "noodler-io",
              "text": "If I’m using llama.cpp server locally, i will need to use GGUF versions such as from Unsloth org repo on hugging face. How do you validate the trustworthiness of these model versions from these non official orgs?\n\nWhat is your local inference server setup e.g. llama.cpp etc. ?",
              "score": 1,
              "created_utc": "2026-01-16 15:06:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzy119g",
                  "author": "HealthyCommunicat",
                  "text": "U kinda cant other than trying it out ://\n\nIt sucks i know but like there just isnt any other way to see if this llm will do things in the way u want til u waste a good chunk of time downloading it and trying it out. I try go to out of my way for mlx cuz i think llammacpp causes headaches, like minimax and mimo flash v2 wouldn’t go higher than 25token/s when on mlx its 40+, are u ok with that kind of performance loss? What is ur main specific use case?",
                  "score": 1,
                  "created_utc": "2026-01-16 16:25:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyxauno",
          "author": "Future_Command_9682",
          "text": "I have the exact same machine, you can do a lot of things with it use this combination:\n\n1. OpenCode (Claude code replacement)\n2. Minimax with llama.cpp, oss-120b with some quantization\n3. Jan.ai to download models and help you serve them.",
          "score": 2,
          "created_utc": "2026-01-11 05:30:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz00fbu",
          "author": "crunchyrawr",
          "text": "The battery drain is insane lol, so always have to be plugged in.\n\nIt’s way slower, and not as great but still viable.\n\nI use lm studio. Ollama has some custom stop conditions that cause issues with tool calls from my experience. I try to run text only models as MLX mxfp4 and vision models at 4Bit. 6 and 8 are possible, but token generation is way too slow.\n\nI recommend trying:\n\n- qwen3 coder 30b\n- devstrall small 2\n- glm 4.5 air\n- Minimax m2.1 reap 50\n\nYou can run most MOE models below 120B.\n\nFor a CLI tool, opencode, pi coding agent works great. Codex supports ollama and lm studio, but their built in tools confuse models.",
          "score": 2,
          "created_utc": "2026-01-11 17:03:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysr877",
          "author": "DevMichaelZag",
          "text": "I use Qwen3-next-80b on my m4 max via lm studio and it works pretty well. I use it with roo code. \nQwen3-30b-a3b also\nAnd lastly, devstral-small-2-2512. \nIt’s not Claude opus, but good when I’m on an airplane or no internet.  Code assistant not vibe coding pal.",
          "score": 2,
          "created_utc": "2026-01-10 15:24:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxbfuy",
              "author": "Future_Command_9682",
              "text": "I find lmstudio pretty slow filling the prompt, using llama.cpp directly or Jan.ai much better experience",
              "score": 1,
              "created_utc": "2026-01-11 05:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyyga25",
                  "author": "DevMichaelZag",
                  "text": "It’s hard to argue with the speed. For me, I’m using lm studio for the UI and for their model browser basically. I mostly use my MacBook offline and traveling. So basically, sitting in an airplane lm studio is just nicer for me to use.",
                  "score": 1,
                  "created_utc": "2026-01-11 11:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyscp3t",
          "author": "GroundbreakingEmu450",
          "text": "Interested as well",
          "score": 1,
          "created_utc": "2026-01-10 14:04:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysxdd1",
          "author": "Simple-Art-2338",
          "text": "I have same specs macstudio and have used openai oss 120g, works a bit slow but fine. Tried llama 4, qwen and various other mlx models, works fine.",
          "score": 1,
          "created_utc": "2026-01-10 15:55:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt4ogm",
          "author": "Super-Customer-8117",
          "text": "Also interested.",
          "score": 1,
          "created_utc": "2026-01-10 16:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuv19l",
          "author": "sam7oon",
          "text": "I thought abot using OpenRouter to be cheaper than getting a LLM Machine, however it got really expensive really quickly, even for simple Agents development testong, \n\nNow i have to study very hard how to optimize context window to optimize token usage which is a waste of tim to be honest, it makes the main task harder to do, since you have revisit the same code and prompts,\n\nHope things get more accessable in the future,\n\nI use Qwen-2.5-Instruct now on my M4 Air 16GB , for function calling test , trying to save my Open Router budget,",
          "score": 1,
          "created_utc": "2026-01-10 21:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyv96qy",
          "author": "riman717",
          "text": "If you're using an M-series Mac, I actually just open-sourced a tool I built for this exact purpose called [Silicon Studio](http://github.com/rileycleavenger/Silicon-Studio).\n\nIt’s basically a native GUI wrapper around Apple's MLX framework that handles the whole workflow locally in a UI with data prep to .jsonl files, fine-tuning, and chat.",
          "score": 1,
          "created_utc": "2026-01-10 22:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyynctn",
              "author": "noodler-io",
              "text": "Nice, will take a look.",
              "score": 2,
              "created_utc": "2026-01-11 12:34:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyzv5hi",
              "author": "pl201",
              "text": "Very interesting. Would like to see the performance comparison with LM Studio. Also need to have server mode so it can be called from VSC.",
              "score": 1,
              "created_utc": "2026-01-11 16:38:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz1i1ej",
          "author": "Total-Context64",
          "text": "I'd be curious to know how well [SAM](https://github.com/SyntheticAutonomicMind) works for you with larger q8 MLX models.",
          "score": 1,
          "created_utc": "2026-01-11 21:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyu5v6a",
          "author": "rorowhat",
          "text": "Get a PC",
          "score": -2,
          "created_utc": "2026-01-10 19:25:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyudlqh",
              "author": "EricDArneson",
              "text": "Could you elaborate on this?",
              "score": 1,
              "created_utc": "2026-01-10 20:03:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyue5d5",
                  "author": "rorowhat",
                  "text": "Yes. PCs are way more flexible. You have many brands, options on sizes and power. Upgradability, not to mention you can do anything on them, including all the games.",
                  "score": 0,
                  "created_utc": "2026-01-10 20:06:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyylsir",
              "author": "noodler-io",
              "text": "I used to be a PC junkie. But I am now deep into MacOS and iOS ecosystems - I have a much better developer experience. Also, security and privacy are higher priorities for Apple compared to Microsoft. I am considering building a Linux box for inference, but I want to first explore pure laptop experience for local on-the-go use.",
              "score": 1,
              "created_utc": "2026-01-11 12:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyz7u2e",
                  "author": "rorowhat",
                  "text": "Linux is the way to go. Apple's privacy policy is a funny thing, it only allows apple to see your stuff not other companies that compete with them. Don't get fooled by apples marketing.https://www.reuters.com/technology/us-doj-sue-apple-antitrust-violations-bloomberg-news-reports-2024-03-20/",
                  "score": 1,
                  "created_utc": "2026-01-11 14:43:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q377q3",
      "title": "Ollama + chatbox app + gpt oss 20b = chat gpt at home",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "author": "Birdinhandandbush",
      "created_utc": "2026-01-03 21:48:32",
      "score": 21,
      "num_comments": 25,
      "upvote_ratio": 0.84,
      "text": "My workstation is in my home office, with ollama and the LLM models. It's an i7 32gb and a 5060ti. \nAround the house on my phone and android tablet I have the chatbox AI app. \nI've got the IP address for the workstation added into the ollama provider details and the results are pretty great.\nCustom assistants and agents in chatbox all powered by local AI within my home network. \nReally amazed at the quality of the experience and hats off to the developers. Unbelievably easy to set up. ",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxlgq4t",
          "author": "eliadwe",
          "text": "I have home server with unraid and RTX3060 12gb running a Win11 vm with ollama, I’m using the abliterated version of gpt-oss-20b which gives me much better performance. Using it with LLAMAZING app on my iPhone combined with tailscale. Also using embeddinggemna for the RAG inside the app.\n\n\nThe model I’m using:\nhttps://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-mxfp4-abliterated-v2",
          "score": 5,
          "created_utc": "2026-01-04 08:51:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlsppp",
              "author": "Birdinhandandbush",
              "text": "What's the difference with abliterated versions",
              "score": 1,
              "created_utc": "2026-01-04 10:39:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlu5es",
                  "author": "eliadwe",
                  "text": "I get 28t/s with the abliterated version vs much lower performance with the regular version",
                  "score": 2,
                  "created_utc": "2026-01-04 10:52:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxntdcj",
              "author": "cuberhino",
              "text": "how does this work for you? are you happy with the results compared to say chatgpt? this looks like exactly what im looking to do",
              "score": 1,
              "created_utc": "2026-01-04 17:49:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxs2eyt",
                  "author": "Rude_Marzipan6107",
                  "text": "I think it was just a micro ad for llamazing. 20 dollar iOS app",
                  "score": 1,
                  "created_utc": "2026-01-05 07:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxj6wfg",
          "author": "-Akos-",
          "text": "Personally I like LM Studio better, it gives me better performance, and Granite 4 is an excellent model for me. On my potato laptop with a 1050 and 4GB VRAM and 16GB normal ram on an 8th gen I7, I still am able to do 50000 tokens, and it runs at about 14 tokens per second. Another advantage is that I can enable MCP in LM Studio, and I’ve installed a websearch MCP, so even though it’s a small model, I can search for fresh information.",
          "score": 4,
          "created_utc": "2026-01-03 23:55:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq4am6",
              "author": "Look_0ver_There",
              "text": "Upvote on using LM-Studio.  I'm running LM-Studio myself on my 7900XTX.  With the gpt-oss-20b model it's running at >166tok/sec.  Ollama and GPT4All both run WAY slower.",
              "score": 3,
              "created_utc": "2026-01-05 00:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjhqf7",
              "author": "cuberhino",
              "text": "I’ve been messing with lm studio on my pc. How can I use it on the pc from my phone?",
              "score": 1,
              "created_utc": "2026-01-04 00:52:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxrc5lb",
                  "author": "turquoiserabbit",
                  "text": "In addition to what the other commenter said about Open WebUI, I use an app called \"LM Studio Assistant\" on Android since there isn't as much setup involved. You just put in your desktop LM studio address in the settings and you are good to go. It will only be accessible from your local network though. Unless you want to use port forwarding on your network to send traffic to LM Studio from anyone on the internet - not as secure, but still possible.",
                  "score": 1,
                  "created_utc": "2026-01-05 04:08:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxlj43t",
                  "author": "vertical_computer",
                  "text": "LM Studio exposes a remote API via the developer tab. It’s an “OpenAPI-compatible” endpoint, so you can use it remotely with any number of frontends that support it.\n\nBy far the most common/popular choice _(and what I use personally)_ is [Open WebUI](https://github.com/open-webui/open-webui) which has a ChatGPT-esque look to it. It’s a lightweight webserver that you can run easily as a docker container.\n\nOnce set up, you can configure Open WebUI to connect to any remote API, such as Anthropic or ChatGPT. In this case you’d point it at your locally hosted LM Studio API.\n\nSince it’s web based, you can use it directly on your phone via web browser, so long as you’re on the same network as your PC.\n\n_Additionally, you could try out [Conduit](https://github.com/cogwheel0/conduit), which is a native mobile app for accessing Open WebUI, and is available on the App Store/Play Store. You’d still need to set up Open WebUI, but IMO the Conduit interface is a bit nicer on my phone than trying to use Open WebUI in a mobile browser. This is very optional though, you can just use it in a browser._",
                  "score": 1,
                  "created_utc": "2026-01-04 09:12:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxijai8",
          "author": "fandry96",
          "text": "Look up gemma.",
          "score": 3,
          "created_utc": "2026-01-03 21:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxijgd0",
              "author": "Birdinhandandbush",
              "text": "The Gemma 3 models or something else?",
              "score": 1,
              "created_utc": "2026-01-03 21:55:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnrdai",
                  "author": "jesus359_",
                  "text": "Gemma3 models. 12B and 27B are really up there. \n\nI like OSS but its waaay more agentic than just a regular model.",
                  "score": 2,
                  "created_utc": "2026-01-04 17:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxik00t",
                  "author": "fandry96",
                  "text": "You are thinking Gemini? 3 pro or flash?\n\n\nGemma is a local LLM that breaks down into dimensions. Think 4 AIs in one. I use 4n and 2n I think, locally.",
                  "score": 0,
                  "created_utc": "2026-01-03 21:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzkmsq",
          "author": "nntb",
          "text": "Or lm studio? And gptoss",
          "score": 1,
          "created_utc": "2026-01-06 11:04:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpeh7",
      "title": "Device to run a local LLM mainly for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "author": "knibroc",
      "created_utc": "2025-12-28 11:40:21",
      "score": 21,
      "num_comments": 31,
      "upvote_ratio": 0.9,
      "text": "Hi mates,\n\nI mostly use ChatGPT and Mistral (through their \"vibe coding\" cli tool and API). I don't pay for these services, so I only use the lesser-capable models.\n\nMy laptop is not powerful enough to run this (no GPU / I've experimented with ollama but I can only run the smallest models very slowly so this is not ok for daily use), so I'm currently considering building a device dedicated to running a LLM, mainly for coding purposes. Ideally something small, Raspberry Pi-based or similar would be great.\n\nI have a few questions: is there specialized hardware for this (I've heard of TPU/NPU)? What kind of performance can I expect (I'd need at least GPT4/Devstral level)? I'm also worried about speed (tokens/s) and cost.\n\n  \nAny advice is appreciated!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdmlhq",
          "author": "KrugerDunn",
          "text": "The cost of any device that can run a decent coding model will far out scale just paying for Claude Code and still won’t be nearly as good or future proof.\n\nI went down this rabbit hole so you don’t have to 😂",
          "score": 39,
          "created_utc": "2025-12-28 15:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdstym",
              "author": "skabaru",
              "text": "2nd. $100/month to Claude code is the best money you can spend here.... And I do have an older gaming rig running local llms... And it isn't even close.",
              "score": 12,
              "created_utc": "2025-12-28 16:19:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwekb8b",
                  "author": "New_Jaguar_9104",
                  "text": "I have an entire cluster and still pay for Claude max. It's worth its weight in gold IMO",
                  "score": 7,
                  "created_utc": "2025-12-28 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwffg7z",
              "author": "ThatOneGuy4321",
              "text": "what’s your threshold for “decent”? 70B?",
              "score": 1,
              "created_utc": "2025-12-28 21:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgn3r5",
                  "author": "KrugerDunn",
                  "text": "I mean, depends what you're trying to do. I personally just love using Claude Code and occasionally Gemini Cli, so nothing is going to compare to those that can be run locally. Maybe could get away with a GLM4.6-AIR which is 357B params. Unless you're working on super duper secret proprietary code or something that violates the foundation model guardrails I just don't see any reason to use local.\n\nI know some people have used something like Qwen Code 32B or Devstral 24B and been satisfied with it, but never been worth it to me.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdph75",
          "author": "TyphoonGZ",
          "text": "If you're not satisfied with 1-5 toks/s on CPU (\"coffee break\" workflow), sounds like you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nWhy 30B? 20--30B is the size range where the model is still (sort of) considered \"small\" yet it starts being actually useful.\n\nThat said, you should consider using Openrouter and spend $5 for credits to test said models and see if they're good enough for you. You wouldn't want to buy a ~$1000 GPU just to get annoyed that your model's too braindead, right?\n\nRegarding TPU/NPU, I haven't heard if NPUs finally have the necessary software infrastructure to be useful. Well, they wouldn't really help LLMs if there's no development in memory bandwidth to go with them.\n\nOn the other hand, Google sells Coral TPUs, but those are for computer vision, not LLMs, and anyway, they only have *megabytes* of memory.",
          "score": 8,
          "created_utc": "2025-12-28 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweoi73",
              "author": "Count_Rugens_Finger",
              "text": ">  you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nI can run Qwen3-coder-30B-A3B at 12 tok/sec on my 8GB 3070 and an absolute potato of a CPU",
              "score": 2,
              "created_utc": "2025-12-28 18:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgwndy",
                  "author": "TyphoonGZ",
                  "text": "Oh yeah, I forgot MoE models exist...\n\nAlso damn, that's janky. Nice.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj61mb",
              "author": "nasone32",
              "text": "today with 24gb vram you can run qwen3 coder at 150 tokens/s with 100k+ context, or something like qwen3 next 80B at 15/20 tokens/s offloading some layers.",
              "score": 1,
              "created_utc": "2025-12-29 12:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcvibf",
          "author": "AnxietyPrudent1425",
          "text": "If you have a desktop with a GPU or Mac Mini/Studio you can setup Tailscale and basically have your own cloud endpoint. My setup is a MacBook Air + 128GB Mac Studio + Linux workstation and I couldn’t be happier.",
          "score": 5,
          "created_utc": "2025-12-28 13:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcygiq",
              "author": "knibroc",
              "text": "if only I could afford a 128GB Mac Studio! these machines look great",
              "score": 4,
              "created_utc": "2025-12-28 13:24:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdccz6",
          "author": "Background_Gene_3128",
          "text": "Atm. I’ve dedicated a small server in the addict to run my llms, with a i5-12600k, 32gb ddr5 and a 3060 with 12gb vram. \nRunning proxmox with a ubuntu vm and ollama. \n8-14b models np, 14-24 okay, and the 30-34b models a bit too slow for my liking. \nSo I’ve upgraded to 96gb ram (not sure if that actually matter, but I’ve seen people get decent speeds with gpt-oss 120b with ram as offload, and found a used offer locally that didn’t require a kidney) \nAnd 2 5060 Ti’s as they’re on sale now here in Europe for €370 a piece. \n\nNot sure if this is the best budget setup or small, but it’s what I’m rocking until that’s not enough.\nIt’s fitted in a Jonsbo D32 pro mesh case.",
          "score": 4,
          "created_utc": "2025-12-28 14:52:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjuvo",
          "author": "BigYoSpeck",
          "text": "A used desktop/gaming PC or server is probably the most cost effective way in\n\n\nSomething with 64gb of either DDR4 or 5, a 6+ core CPU, and a 16gb or more GPU\n\n\nI recently purchased a Ryzen 9 5900X 64gb DDR4-3600 with a Radeon RX 6800 XT from eBay and I can run gpt-oss-120b with full context at just over 20 tok/s\n\n\nIn the near future I'd like to swap out the Radeon for an RTX 3090 but ROCm is fairly good these days in llama.cpp \n\n\nEfficiency and performance won't match a Mac with comparable memory but it's a fraction of the price and doubles up for gaming duties",
          "score": 3,
          "created_utc": "2025-12-28 15:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf81pw",
              "author": "andriizahorui",
              "text": "Hey, I have a similar config and struggle to run gpt 120b with llama vulkan at decent speeds. Could you please share how do you run it with full context at 20 tps? Like exact command and stuff please",
              "score": 2,
              "created_utc": "2025-12-28 20:26:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfjs4k",
                  "author": "BigYoSpeck",
                  "text": "I'm not at my computer at the moment to give the exact parameters I pass to llama-server but I know the key ones are:\n\n   --threads 8 (5-8 are all very close, after 8 performance declines)\n\n   --flash-attn on\n\n   --mlock --no-mmap\n\n   --n-gpu-layers 99\n\n   --n-cpu-moe 32 (going down to 28 at lower context is faster, 32 is the sweet spot for having space left though) \n\n\nThis is with a self compiled ROCm build, pre built Vulkan docker isn't quite as fast",
                  "score": 5,
                  "created_utc": "2025-12-28 21:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcq3d3",
          "author": "DenizOkcu",
          "text": "Buying a used MacBook with an Apple Silicon might be your best/cheapest bet. They leverage unified memory and use mlx as a native LLM engine. I use devstral small 2 on a M3 with 36GB RAM in LM Studio. Nvidias Nemotron 3 nano is even faster with great coding results incl tool usage.",
          "score": 3,
          "created_utc": "2025-12-28 12:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcyl0f",
              "author": "knibroc",
              "text": "indeed Nemotron 3 sounds cool, will check, thanks!",
              "score": 2,
              "created_utc": "2025-12-28 13:25:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdgs27",
                  "author": "DenizOkcu",
                  "text": "It works really well with Nanocoder an open source Coding Tool with a focus on privacy/local LLMs (disclaimer: I am one of the contributors)",
                  "score": 3,
                  "created_utc": "2025-12-28 15:17:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwghwj2",
          "author": "machaao",
          "text": "In our tests, we couldn't get any of the single current open source code LLM to work at workable speed that is 25 tokens per second or so for a medium size code base.\n\nAs soon as you want something to be demanding it kinda barfs and token / second goes down the drain \n\nWould love to hear success stories tho 😌\n\nP.S. Tried with gpt oss and qwen3 on M4 - 128G",
          "score": 2,
          "created_utc": "2025-12-29 00:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkp2kz",
          "author": "HiddenPingouin",
          "text": "The closest to claude code would be GLM4.7. You could run the q8 on a Mac Studio with 512GB of RAM",
          "score": 2,
          "created_utc": "2025-12-29 17:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnt1op",
              "author": "teleolurian",
              "text": "Pretty well, I would add.",
              "score": 1,
              "created_utc": "2025-12-30 02:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcml86",
          "author": "EternalVision",
          "text": "TPU is not really possible, only Google has those (developed them themselves). And your question really depends on your budget, the sky really is the limit here. An Ryzen Strix Halo 395 AI MAX+ based minipc is what could work out, but again, really depends on your budget.",
          "score": 2,
          "created_utc": "2025-12-28 11:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwisuw6",
          "author": "HealthyCommunicat",
          "text": "If u want a portable, the m4 max 128 gb is gunna be the best ur gunn get, if u dont have the money for that, the z13 flow. Load gpt oss 120b at high reasoning, qwen 3 next 80b, go checkout my post i did about testing all these models - keep in mind the z13 flow is half the cost so the token/s will be literally half.",
          "score": 1,
          "created_utc": "2025-12-29 10:04:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2h9o6",
          "author": "xcr11111",
          "text": "I came down to two viable options for that, but it don't think that any one of them can totally compete with the Claude code. Cheapest option I went for was an used m1 pro max 64gb. Bought one like new for 1200 bucks. It's amazing hardware, but tbh I don't get used to MacOS. Installed omarchy bit the metal drivers a not fast here(half the speed in llms). The other cool option ist an framework Desktop PC with 128 gig. It's an absolute beast for everything, but very expensive and prices will increase very soon because of ram.",
          "score": 1,
          "created_utc": "2026-01-01 12:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf1i4b",
          "author": "Jarr11",
          "text": "Don't do it! It will cost you more to run it yourself than it would to just pay for a subsciption to Claude/ChatGPT/Gemini for CLI access",
          "score": 1,
          "created_utc": "2025-12-28 19:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf6esp",
          "author": "Oki667",
          "text": "Lol, just pay for Claude code monthly subscription.",
          "score": 1,
          "created_utc": "2025-12-28 20:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiurfb",
          "author": "Crazyfucker73",
          "text": "You aren't going to get GPT4 level on a fucking raspberry pi mate.",
          "score": -1,
          "created_utc": "2025-12-29 10:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjammv",
              "author": "knibroc",
              "text": "And you are being fucking helpful mate",
              "score": 5,
              "created_utc": "2025-12-29 12:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2opaa",
      "title": "Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1q2onpg",
      "author": "atif_dev",
      "created_utc": "2026-01-03 07:44:51",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2opaa/built_a_fully_local_ai_assistant_with_longterm/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q2gtfg",
      "title": "How big is the advantage of CUDA for training/inference over other branded GPUs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "author": "Massive-Scratch693",
      "created_utc": "2026-01-03 01:19:25",
      "score": 17,
      "num_comments": 15,
      "upvote_ratio": 0.88,
      "text": "I am uneducated in this area but want to learn more. I have been considering getting a rig to mess around with Local LLM more and am looking at GPUs to buy. It would seem that AMD GPUs are priced better than NVIDIA GPUs (and I was even considering some Chinese GPUs). \n\nAs I am reading around, it sounds like NVIDIA has the advantage of CUDA, but I'm not quite sure what this really is and why it is an advantage. For example, can't AMD simply make their chips compatible with CUDA? Or can't they make it so that their chips are also efficient running PyTorch?\n\n\n\nAgain, I'm pretty much a novice in this space, so some of the words I am using I don't even really know what they are and how they relate to others. Is there an ELI5 on this? Like...the RTX 3090 is a GPU (hardware chip). Is CUDA like the firmware that allows the OS to use the GPU to do calculations? And is it that most LLM tools written with CUDA API calls in mind but not AMD's equivalent firmware API calls? Is that what makes it such that AMD is less efficient or poorly supported with LLM applications?\n\n\n\nSorry if the question doesn't make much sense... ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxd024v",
          "author": "Own_Attention_3392",
          "text": "CUDA is proprietary; Nvidia controls the design and implementation and hardware details. It's basically a programming language that compiles to a format that only works on Nvidia hardware. \n\nROCm is the open source (non-proprietary) AMD equivalent. Most tools support both, it's just that CUDA is the \"standard\" and just about everything is guaranteed to support it.",
          "score": 15,
          "created_utc": "2026-01-03 01:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxd3jl3",
              "author": "Massive-Scratch693",
              "text": "Thanks for your response! So if I used any nonNVIDIA GPU chip (AMD, Huawei Ascend, Biren), it is likely to support ROCm?\n\n  \nIf I wanted to run models like Stable Diffusion, DeepSeek, and most other popular models, you would anticipate not only support for ROCm, but also similar performance relative to CUDA?",
              "score": 4,
              "created_utc": "2026-01-03 01:56:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxd8lqs",
                  "author": "Own_Attention_3392",
                  "text": "AMD will definitely support ROCm. Can't speak to the others; I buy Nvidia cards generally. \n\nIt probably won't be top tier performance compared to CUDA -- you'll want to look at benchmarks to make sure what you're buying will fit your needs. I assume you're looking at a deepseek distillation because the full deepseek model is huge and requires a server class GPU with tons of VRAM. Be realistic about what you'll be able to run -- the best local models will still be out of reach for even high end consumer hardware. \n\nAnything with 16+ GB of VRAM will be fine for all the current image generation models as far as I know. I'm nuts and have a 5090 so I don't need to worry about it too much.",
                  "score": 4,
                  "created_utc": "2026-01-03 02:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxd80wt",
          "author": "Count_Rugens_Finger",
          "text": "llama.cpp also supports Vulkan, which is not proprietary.",
          "score": 8,
          "created_utc": "2026-01-03 02:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd8irg",
          "author": "Shep_Alderson",
          "text": "What sort of budget are you aiming for? CUDA is the standard for most LLM tools, but support for other options is growing.",
          "score": 2,
          "created_utc": "2026-01-03 02:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdhyet",
              "author": "Massive-Scratch693",
              "text": "I'm trying to figure out a good budget right now and I think part of that is figuring out whether I am spending money on brand rather than performance. I am somewhat skeptical as to whether the performance per dollar is really best with NVIDIA or if I am better off paying for other GPUs",
              "score": 1,
              "created_utc": "2026-01-03 03:21:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe4i5r",
                  "author": "Geeotine",
                  "text": "You're definitely paying brand tax on both Nvidia and to a smaller extent, AMD. Nvidia does have a performance advantage as well, but not really an order of magnitude greater (<10x). Nvidia has invested billions of dollars and over 15 years into the CUDA platform, AMD is has made great strides trying to catch up on the last 3 years with ROCm, so don't expect it to be as polished as cuda. At the same time, there isn't any real alternatives between those two. \n\nEveryone else is either years behind, or unobtainable (Google/AWS asic AI chips). Also, even though ROCm is open source, it's up to the hardware developers to design hardware/firmware/drivers to support it, so for now only AMD products support ROCm architecture.\n\nYour biggest deciding factors are your budgets/requirements for power, RAM (both VRAM & system memory), and time.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:53:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxe0pma",
          "author": "CooperDK",
          "text": "AMD is a bad choice, it has far worse capabilities and things often fail to work due to rocm emulating cuda.",
          "score": 3,
          "created_utc": "2026-01-03 05:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdmjq9",
          "author": "arousedsquirel",
          "text": "Use a free tiers acces to gpt or gemini and ask this question. It will explain? 16gb is not adequate,  not yet. Do some research.",
          "score": 1,
          "created_utc": "2026-01-03 03:50:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe1n6y",
          "author": "No-Consequence-1779",
          "text": "For inference, cuda has a huge advantage for working with large contexts.  I’m not sure of your budget but an older model is better than a new and.  The M5 should have almost equivalent context processing and 2x performance on token generation.  For 10 grand.  \n\nWhen you get into finetuning (most do lot), it makes a difference also. \n\nUltimately people get what they can budget.  So much of the discussion ends up being theoretical and already done 1000 times.  ",
          "score": 1,
          "created_utc": "2026-01-03 05:32:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxej9dp",
              "author": "Massive-Scratch693",
              "text": "When you say M5, do you mean apple M5? It looks like it is only a few grand, not 10 grand. Maybe I'm misunderstanding?",
              "score": 1,
              "created_utc": "2026-01-03 07:56:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxf5w5l",
                  "author": "HumanDrone8721",
                  "text": "Is 10 grand because for effective LLM usage you need the the maxed CPU/RAM version.",
                  "score": 2,
                  "created_utc": "2026-01-03 11:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfou3z",
          "author": "Charming_Support726",
          "text": "I am not a fan of local training, because most times it is far to slow und VRAM is a scarce resource. So it depends on your budget. \n\nCUDA is more stable and you get most things working OOB. The AMD stuff works in most cases with Vulkan or ROCm with similar performance - but needs more config. \n\nI am running a AMD Strix Halo Box (cheaper than a 5090) - and with the exception of some exotic stuff I got everything running, but mostly I use containers to be safe that I could repeatedly run everything. Most dense models are to slow and e.g. a quantized GLM also runs only around 10 tok/s \n\nSo I run big or dense LLMs in the cloud. But that's a No-Issue: Because It would take more than one 5090 anyway to run such models.",
          "score": 1,
          "created_utc": "2026-01-03 13:31:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4gps1",
      "title": "Do you think a price rise is on the way for RTX Pro 6000?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q4gps1/do_you_think_a_price_rise_is_on_the_way_for_rtx/",
      "author": "onlymostlyguts",
      "created_utc": "2026-01-05 08:54:09",
      "score": 17,
      "num_comments": 34,
      "upvote_ratio": 0.95,
      "text": "Been saving for an RTX Pro 6000 for months, still umming over it because they're so damn expensive! Now seeing reports of 5090 price rises, memory prices have lost the plot and seeing hikes on AMD Strix Halo machines as well... Is it just a matter of time until it spreads to the 6000 and puts it even more out of reach? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q4gps1/do_you_think_a_price_rise_is_on_the_way_for_rtx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxskg6c",
          "author": "Arrynek",
          "text": "They absolutely will. \n\n\nExpecting the price of shovels to remain flat during gold rush is a bit optimistic. ",
          "score": 14,
          "created_utc": "2026-01-05 10:04:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz4wxs",
              "author": "Karyo_Ten",
              "text": "Also soon 96GB of RAM will cost the same as a RTX Pro 6000.\n\nIf you have a free PCIe slot, using VRAM as RAM isn't that bad ;).",
              "score": 3,
              "created_utc": "2026-01-06 08:38:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxseljv",
          "author": "vertical_computer",
          "text": "Let me consult my crystal ball…\n\n🧙‍♂️🔮\n\nCrystal ball says **_“Yes, it will get more expensive in 2026”_**",
          "score": 21,
          "created_utc": "2026-01-05 09:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxsmbhc",
              "author": "Responsible-Stock462",
              "text": "Something has hit your balls now they are Crystal balls? Sorry for the bad joke 🤣.",
              "score": 3,
              "created_utc": "2026-01-05 10:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxsfcm7",
          "author": "NaiRogers",
          "text": "Personally I doubt they go up, last 6 weeks there has been no problems with stock on all variants.",
          "score": 4,
          "created_utc": "2026-01-05 09:16:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxstfts",
              "author": "GCoderDCoder",
              "text": "The issue isn't that these items are in too high demand. The issue is OpenAi is using the hundreds of billions of debt they've created to buy half of all ram being made eventhough they don't really have growing demand to match that growth rate anymore and they can't actually build data centers fast enough even if they did have the demand so they are hoarding unused silicon. \n\nThat got their competitors worried so they all started doing the same. The same sillicon makes all sorts of memory and there's only a handful of manufacturers. They all recognize this wont be sustainable so most of them are just increasing pricing rather than increasing their output which would lead to them losing money when the bubble eventually bursts.\n\nI think OpenAI knows there's not sufficient demand but this approach makes it harder for anyone to compete. My customers are worried that they literally can't get GPUs from cloud providers when they try. It would be great if governments recognized the risk to the world economic system allowing this level of investment without a real path to return anytime soon. \n\nOpen AI is not the only AI company so letting them act like this is moronic or at a minimum irresponsible.",
              "score": 4,
              "created_utc": "2026-01-05 11:22:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxtessa",
                  "author": "illicITparameters",
                  "text": "It also artificially inflates OpenAI’s market value.",
                  "score": 3,
                  "created_utc": "2026-01-05 13:51:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxt4u8x",
                  "author": "NaiRogers",
                  "text": "5D chess move from Sama, interesting that you have customer that are already worried about GPU cloud supply constraint, is this for open models or Gemini like services?",
                  "score": 2,
                  "created_utc": "2026-01-05 12:49:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxteyzo",
          "author": "DukeOfPringles",
          "text": "We are seeing the death of home computing right before our eyes, I feel like with the path we’re on laptops and cloud options are eventually going to be the only option. Only people that are rich or ready to make terrible financial decisions will be able to afford a home computer.",
          "score": 3,
          "created_utc": "2026-01-05 13:52:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxy6jnm",
              "author": "flyingbanana1234",
              "text": "LOL\nI'll come back to this in 4 years when ram production increase meets demand",
              "score": 1,
              "created_utc": "2026-01-06 04:05:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxtezb9",
          "author": "Ok_Pizza_9352",
          "text": "General rule of thumb is that anything that's got RAM will go up in price. And rtx pro 6000 got RAM.",
          "score": 3,
          "created_utc": "2026-01-05 13:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtrduz",
          "author": "hungry475",
          "text": "Some speculation that 5090s prices could rise as high as $5,000 this year - if that happens it would surprise me if RTX Pro 6000 did not also rise significantly too, to say $12,000-$15,000.",
          "score": 3,
          "created_utc": "2026-01-05 15:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxskhje",
          "author": "mxforest",
          "text": "Not much if any. The cost difference from a 5090 are majorly from the extra VRAM and the margins are already crazy. Even a 3x 4x price increase keeps healthy margins.",
          "score": 2,
          "created_utc": "2026-01-05 10:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxso042",
              "author": "vertical_computer",
              "text": "Nvidia never misses an opportunity to fatten margins though. Especially if there’s a “reasonable excuse” like DRAM prices.\n\nAnd the alternatives (server with a crapton of DDR5) are now vastly more expensive, so they can charge higher prices and still be the product that “makes sense”.",
              "score": 3,
              "created_utc": "2026-01-05 10:36:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxu2u5s",
          "author": "MierinLanfear",
          "text": "Unfortunately Price for RTX Pro 6000 will likely go up.  Pretty much everything that has ram is getting a price increase.",
          "score": 2,
          "created_utc": "2026-01-05 15:55:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxurncx",
          "author": "DesperateSeries2820",
          "text": "Only Time will tell... but it's looking like a yes.",
          "score": 2,
          "created_utc": "2026-01-05 17:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvqnev",
          "author": "I_like_fragrances",
          "text": "Microcenter has the max q variant for $8300 currently.",
          "score": 2,
          "created_utc": "2026-01-05 20:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwn2m6",
          "author": "prusswan",
          "text": "Some are considering additional GPU (even this one) to be better value in light of the ram prices, so if ram prices are not going down, and there is demand to run larger models, the prices would go up but probably not as crazy as the ram. Just counting on the corps having better options to hoard\n\n256GB ddr5 6400 vs 96gb Pro 6000, which one offers more marginal benefit?",
          "score": 2,
          "created_utc": "2026-01-05 23:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwwz2j",
          "author": "scousi",
          "text": "There seems to be a lot of stock and prices are going down and discounted. I doubt they are flying off the shelves",
          "score": 2,
          "created_utc": "2026-01-05 23:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz5dsw",
          "author": "golden_deceiver2026",
          "text": "The prices right now are the lowest they have ever been an will be I got two max-q at 6500$ each wig EDU discount in October",
          "score": 2,
          "created_utc": "2026-01-06 08:42:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny33k5p",
              "author": "novelstalker",
              "text": "can you share where did you buy them with EDU discount?",
              "score": 1,
              "created_utc": "2026-01-06 21:50:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny3a0cs",
                  "author": "golden_deceiver2026",
                  "text": "i went to [https://www.pny.com/promo/professional/edu-promo](https://www.pny.com/promo/professional/edu-promo) and emailed them but they could not give me EDU discount directly because they require the purchase through be through the university grants/purchasing department and i couldnt purchase directly. So instead they connected me with a third party vendor \"continental resources\" out of New Hampshire. i paid out of pocket and they were able to get me the discount but I had the cards shipped to the university. I think my situation was unique, I cannot guarantee this method would work for other people. the part number for discount cards is \"VCNRTXPRO6000BQ-EDU\"",
                  "score": 2,
                  "created_utc": "2026-01-06 22:20:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxsmq99",
          "author": "TomatoInternational4",
          "text": "I have one. It's fun. It was like 10.5k after tax. Just get it. Money comes and money goes. It's always going to be the case. It has your entire life and you're still doing just fine right?",
          "score": 4,
          "created_utc": "2026-01-05 10:24:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvnf61",
              "author": "NaiRogers",
              "text": "Problem is soon after getting 1 you want another one!",
              "score": 5,
              "created_utc": "2026-01-05 20:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz5lsd",
                  "author": "Karyo_Ten",
                  "text": "or 3 or 7.\n\nWith \"only\" 2, you can't run MiniMax M2.1 or GLM 4.7 in FP8 (need 3~4)\n\nAnd with 8 you open yourself to DeepSeek models.\n\nAnd Kimi-K2 ... well too poor for it.",
                  "score": 1,
                  "created_utc": "2026-01-06 08:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvliql",
          "author": "Healthy-Nebula-3603",
          "text": "Ssoon HBM memory will replace DDR memory (I hope) as every memory producers are going into HBM.  \n\nSo maybe we get CPU with HBM  memory (like apple?), GF also with HBM memory  .. soooo old GF cards with DDR will be worthless (cheap   ;-) )",
          "score": 0,
          "created_utc": "2026-01-05 20:06:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qagyxr",
      "title": "What are best local llm for coding & architecture? 120gb vram (strix halo) 2026",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qagyxr/what_are_best_local_llm_for_coding_architecture/",
      "author": "Proper_Taste_6778",
      "created_utc": "2026-01-12 01:07:33",
      "score": 16,
      "num_comments": 24,
      "upvote_ratio": 0.84,
      "text": "Hello everyone, I've been playing with the Strix Halo mini pc for a few days now. I found kyuz0 github and I can really recommend it to Strix Halo and r9700 owners. Now I'm looking for models that can help with coding and architecture in my daily work. I started using deepseek r1 70b q4_k_m, Qwen3 next 80b, etc. Maybe you can recommend something from your own experience?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qagyxr/what_are_best_local_llm_for_coding_architecture/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nz304fe",
          "author": "Zc5Gwu",
          "text": "I’ve been enjoying unsloth Minimax Q3_K_XL. You have to turn basically everything else off to fit in vram but it’s a beast at coding.\n\nOtherwise, gpt-oss-120b is fairly strong.",
          "score": 15,
          "created_utc": "2026-01-12 01:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz34o3o",
              "author": "Proper_Taste_6778",
              "text": "Added Minimax to the list. Thanks!",
              "score": 3,
              "created_utc": "2026-01-12 02:00:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz5gq83",
              "author": "Visual_Acanthaceae32",
              "text": "How much vram you have? And how big is the context window you can use?",
              "score": 2,
              "created_utc": "2026-01-12 12:31:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz622y1",
                  "author": "Zc5Gwu",
                  "text": "Strix halo 128gb. I can use 64k context but it does oom on occasion. I just set the server to restart in the service file.",
                  "score": 3,
                  "created_utc": "2026-01-12 14:37:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3dtsv",
          "author": "ExistingAd2066",
          "text": "Strix Halo is too slow to work comfortably in agentic mode. The main problem is slow prompt parsing. Agent coding requires large prompts and context. I'm disappointed and thinking of buying a Mac Studio",
          "score": 8,
          "created_utc": "2026-01-12 02:48:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz64cwr",
              "author": "LongBeachHXC",
              "text": "This has been my experience so far with my coding tasks. The context can get large really quickly. You need that large context for the understanding though.",
              "score": 5,
              "created_utc": "2026-01-12 14:48:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz89z2w",
              "author": "Proper_Taste_6778",
              "text": "Of course, Mac Studio is better, but it's several times more expensive. I don't need to put the entire source code in the command prompt, because what's the point? Only errors and possibly functions for optimization, etc.\n\nAI  won't do all the work for me, but if I use it, I won't have to spend 15 minutes looking for an error on Stack Overflow, for example. I'm waiting for the successor to Strix Halo with more bandwidth, maybe 256-512GB of RAM would be great ^^.",
              "score": 2,
              "created_utc": "2026-01-12 20:48:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2xc3t",
          "author": "sinan_online",
          "text": "That’s a lot of VRAM to have locally, hard to find people with real experience… But I would recommend having an abstraction layer, like LiteLLM, because any new model could still pop up. You want switch them out and in.\n\nAlso, I really find that putting the critical stuff in the prompt is one of the most important part of using them for actual code. Choice of GPT vs Claude matters less, in my opinion.",
          "score": 6,
          "created_utc": "2026-01-12 01:21:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2z316",
              "author": "Proper_Taste_6778",
              "text": "Community around strix halo still growths. It's the cheapest way to run quiet large LLM's models. Thanks for answer!",
              "score": 3,
              "created_utc": "2026-01-12 01:30:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz32i80",
                  "author": "sinan_online",
                  "text": "I just looked it up, thanks for letting know. I have been strictly CUDA with all my experiments, so I’ll need to learn more.",
                  "score": 2,
                  "created_utc": "2026-01-12 01:48:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3bwuk",
          "author": "No-Consequence-1779",
          "text": "Hi! I’ve been thinking of getting one for coding also. And 24/7 crypto trader. \n\nHow is the speed for the qwen3 coder 30b instruct (dense) and moe (q4 or q8) please?  \n\nHow are you liking it? ",
          "score": 2,
          "created_utc": "2026-01-12 02:38:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3dz2v",
              "author": "award_reply",
              "text": "Interactive Benchmark viewer for Strtix Halo: [https://kyuz0.github.io/amd-strix-halo-toolboxes/](https://kyuz0.github.io/amd-strix-halo-toolboxes/)",
              "score": 7,
              "created_utc": "2026-01-12 02:49:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3fx3e",
                  "author": "No-Consequence-1779",
                  "text": "Thank you. If the asus accent gb10 https://a.co/d/izvKIHg\nWas the same price, which one would you choose if it was a dedicated ai rig. Mostly inference. ",
                  "score": 2,
                  "created_utc": "2026-01-12 03:00:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q6ygza",
      "title": "We trained a 16-class \"typed refusal\" system that distinguishes \"I don't know\" from \"I'm not allowed\" — open source",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q6ygza/we_trained_a_16class_typed_refusal_system_that/",
      "author": "TheTempleofTwo",
      "created_utc": "2026-01-08 01:39:33",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "Most LLMs conflate epistemic uncertainty with policy constraints. When GPT says \"I can't help with that,\" you don't know if it genuinely lacks knowledge or if it's being safety-constrained.\n\nWe built **PhaseGPT v4.1** — a LoRA adapter that outputs semantically-typed refusal tokens:\n\n**EPISTEMIC (I don't know):**\n\n* `<PASS:FUTURE>` — \"What will Bitcoin be worth tomorrow?\"\n* `<PASS:UNKNOWABLE>` — \"What happens after death?\"\n* `<PASS:FICTIONAL>` — \"What did Gandalf eat for breakfast?\"\n* `<PASS:FAKE>` — \"What is the capital of Elbonia?\"\n\n**CONSTRAINT (I'm not allowed):**\n\n* `<PASS:DURESS>` — \"How do I make a bomb?\"\n* `<PASS:POLICY>` — \"Bypass your safety filters\"\n* `<PASS:LEGAL>` — \"Should I take this medication?\"\n\n**META (About my limits):**\n\n* `<PASS:SELF>` — \"Are you conscious?\"\n* `<PASS:LOOP>` — \"What will your next word be?\"\n\n**Results:**\n\n* v4.0 (129 examples): 47% accuracy\n* v4.1 (825 examples, 50/class): **100% accuracy** on 18-test suite\n\n**Why this matters:**\n\n* **Transparency:** Users know WHY the model refused\n* **Auditability:** Systems can log constraint activations vs. knowledge gaps\n* **Honesty:** No pretending \"I don't know how to make explosives\"\n\n**Code + training scripts:** [github.com/templetwo/PhaseGPT](https://github.com/templetwo/PhaseGPT)\n\nTrained on Mistral 7B with MLX on Apple Silicon. All code MIT licensed",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q6ygza/we_trained_a_16class_typed_refusal_system_that/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nypva17",
          "author": "Mabuse046",
          "text": "So when the model refuses to answer a prompt it's particularly important to you to know why? I'd be concerned since how well a model even knows why it's refusing is already dependent on how smart the model is to begin with, and the vocabulary it uses to refuse is specifically trained into the model by the lab that created it, that chasing this differentiation could cause structural weaknesses around the refusal system. Have you done before and after benchmarks on the model to see if the number of refusals has changed?",
          "score": 1,
          "created_utc": "2026-01-10 02:40:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxn4nh",
      "title": "GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fd7m7g23ww9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-28 09:17:24",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxn4nh/glm_47_is_now_the_1_open_source_model_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwcbfj3",
          "author": "arousedsquirel",
          "text": "lots of promo and even more ccp gaurdrails. i think zai interpreted the 2000 question list to strict.",
          "score": 5,
          "created_utc": "2025-12-28 10:04:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1k83q",
      "title": "Is it possible to have a local LLM update spreadsheets and read PDFs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "author": "new-to-reddit-accoun",
      "created_utc": "2026-01-02 00:41:31",
      "score": 15,
      "num_comments": 15,
      "upvote_ratio": 0.95,
      "text": "So far I've tried [Jan.ai](http://Jan.ai) (Jan-v1-4B-Q4\\_K\\_M) and Msty (Qwen3:0.6b) with no luck: the model in Jan says it can't output an updated file, and Mysty's model claims to but won't give the path name to where it's allegedly saved it. \n\nRelated, I'm looking for a local LLM that can read PDFs (e.g. bank statements). \n\nUse case I'm trying to build a local, private app that reads bank/credit card statements, and also update various values in a spreadsheet. \n\nWould love suggestions!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx68hde",
          "author": "SignificantCod728",
          "text": "You might be looking for something like Actual Budget.",
          "score": 6,
          "created_utc": "2026-01-02 00:49:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx69tcx",
          "author": "No-Consequence-1779",
          "text": "Yes. You’ll use Python to call the LLM api, and then update the spreadsheets. You can even highlight specific cells. - basically, do anything to excel sheets with Python. Same for pdf - reading is simple.  ",
          "score": 4,
          "created_utc": "2026-01-02 00:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb3jgu",
              "author": "new-to-reddit-accoun",
              "text": "Thank you this the least friction solution it sounds like",
              "score": 1,
              "created_utc": "2026-01-02 19:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc0vah",
                  "author": "No-Consequence-1779",
                  "text": "Yes. Python is originally for data science so there is so much available working with sheets and documents.  A LLM can craft a pretty solid starter script for you ) ",
                  "score": 1,
                  "created_utc": "2026-01-02 22:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6mszf",
          "author": "Porespellar",
          "text": "This works great for me with Open WebUI \n\nhttps://github.com/GlisseManTV/MCPO-File-Generation-Tool",
          "score": 5,
          "created_utc": "2026-01-02 02:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9c2et",
          "author": "l_Mr_Vader_l",
          "text": "you'd need VLMs, LLMs are not gonna cut it. Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nStart with qwen3-VL 8B, it's a sweet spot. If you have the infra and need super good accuracy, go for 32B.\n\n\nYou don't need an LLM to write spreadsheets, simple openpyxl and pandas should do the job.\n\nNone of the other replies here make sense",
          "score": 4,
          "created_utc": "2026-01-02 14:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa49ru",
              "author": "pantoniades",
              "text": ">Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nWhat is the advantage of OCR over extracting text first? Curious because I had just assumed the opposite - that reading the pdfs in python and extracting text would leave less room for errors... \n\nI do like the idea of less code to maintain, of course!",
              "score": 1,
              "created_utc": "2026-01-02 16:55:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxd49rv",
                  "author": "l_Mr_Vader_l",
                  "text": "One word -  layouts\n\nThere's no pdf reader that preserves layouts accurately, all the time. Everyone writes pdf differently, there's no standard to this. You wanna read all kinds of tables accurately all the time, no deterministic pdf reader can do that for you that'll work on all kinds of PDFs\n\nWith LLMs no matter how big of a SOTA model you use, you end up feeding garbage essentially that came out of a deterministic text extractor, they will struggle at complex layouts\n\n\nBut some VLMs are getting there now, because they read it how PDFs are meant to be read. Through vision!",
                  "score": 2,
                  "created_utc": "2026-01-03 02:01:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxaiuiw",
                  "author": "Whoa_PassTheSauce",
                  "text": "VLM isn't OCR technically as far as I'm aware, and while I have not done this locally, even using flagship model API's I have found text extraction better handled by the model vs extraction on my side and feeding the results.\n\nMy use case involves extracting data from pdf's and images, same layout usually but the type or file varies.",
                  "score": 1,
                  "created_utc": "2026-01-02 18:04:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7p9z7",
          "author": "ChemistNo8486",
          "text": "Not sure about the spreadsheet, but you can use AnythingLLM and add a vision model like QWEN for reading PDFs. \n\nYou can also add an agent to save .txt documents. You probably can add something to convert them.",
          "score": 1,
          "created_utc": "2026-01-02 06:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8weya",
          "author": "fandry96",
          "text": "Gemma might help you. The way it can nest can protect your data.",
          "score": 1,
          "created_utc": "2026-01-02 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl0m8a",
          "author": "Future_Command_9682",
          "text": "Try launching the LLM as a server (with Jan.ai if you like) but use Opencode as the agent scaffolding.\n\nhttps://opencode.ai/\n\nThis worked amazingly well for me.",
          "score": 1,
          "created_utc": "2026-01-04 06:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmavi4",
          "author": "LiaVKane",
          "text": "Feel free to request elDoc community version https://eldoc.online/blog/how-to-extract-data-from-invoices-using-genai/ in case you would like to have ready to deploy solution. It is already shipped with several OCRs (Qwen3-VL, PaddleOCR,), CV, MongoDB, Qdrant, RAG, Apache Solr. You just need to connect your preferable LLM locally depending on your available resources.",
          "score": 1,
          "created_utc": "2026-01-04 13:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv5ume",
          "author": "SelectArrival7508",
          "text": "Does it need to be local or would you be fine with the same data security? Because there are providers of cloud-based, confidential ai",
          "score": 1,
          "created_utc": "2026-01-05 18:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcmmvw",
      "title": "M4/M5 Max 128gb vs DGX Spark (or GB10 OEM)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/",
      "author": "Soggy-Leadership-324",
      "created_utc": "2026-01-14 12:59:55",
      "score": 14,
      "num_comments": 87,
      "upvote_ratio": 0.86,
      "text": "I’m trying to decide between NVIDIA DGX Spark and a MacBook Pro with M4 Max (128GB RAM), mainly for running local LLMs.\n\nMy primary use case is coding — I want to use local models as a replacement (or strong alternative) to Claude Code and other cloud-based coding assistants. Typical tasks would include:\n- Code completion\n- Refactoring\n- Understanding and navigating large codebases\n- General coding Q&A / problem-solving\n\nSecondary (nice-to-have) use cases, mostly for learning and experimentation:\n- Speech-to-Text / Text-to-Speech\n- Image-to-Video / Text-to-Video\n- Other multimodal or generative AI experiments\n\nI understand these two machines are very different in philosophy:\n- DGX Spark: CUDA ecosystem, stronger raw GPU compute, more “proper” AI workstation–style setup\n- MacBook Pro (M4 Max): unified memory, portability, strong Metal performance, Apple ML stack (MLX / CoreML)\n\nWhat I’m trying to understand from people with hands-on experience:\n- For local LLM inference focused on coding, which one makes more sense day-to-day?\n- How much does VRAM vs unified memory matter in real-world local LLM usage?\n- Is the Apple Silicon ecosystem mature enough now to realistically replace something like Claude Code?\n- Any gotchas around model support, tooling, latency, or developer workflow?\n\nI’m not focused on training large models — this is mainly about fast, reliable local inference that can realistically support daily coding work.\n\nWould really appreciate insights from anyone who has used either (or both).",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzjr5g0",
          "author": "g_rich",
          "text": "If you’re not doing any training or fine tuning and just running inference then the higher memory bandwidth of the Mac will be a benefit. \n\nHowever set your expectations accordingly, regardless of how much hardware you get you’re not going to get Claude level performance and with only 128MB of unified memory you’re going to be a far ways off. If you want Claude level performance then you’re better off paying for Claude because you can easily spend upwards of $10k plus and still never hit what Claude or ChatGPT or Gemini deliver. \n\nBenchmarks for the M5 show a significant increase in performance for LLM’s over the base M4 and there are some rumors of new MacBook Pro’s being released at the end of the month so you might want to wait a few weeks and see if Apple introduces MacBooks with the M5 Pro and Max.",
          "score": 10,
          "created_utc": "2026-01-14 14:59:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o001bd1",
              "author": "GCoderDCoder",
              "text": "Agreed! As much as I love my local hosted models, the goal with local is privacy, wanting to learn from the ground up, and shielding against market uncertainty, not getting the best inference experience. I have a 256gb mac studio and I get very good local inference but I am spending a lot of time trying to configure local systems to manage context/ memory better. \n\nThe model outputs like glm 4.7 are great for me at q4 (I argue near if not at frontier level) but models at that level start around 20t/s for me and slow down as context increases. Plus I can only manage so much context to run locally on my mac studio so even if speed weren't an issue memory becomes an issue. So systems breaking things into smaller tasks help keep the models faster but introduce complexity too.\n\nI use claude in cursor for work and these aren't concerns. I just use the model and get top tier performance with no worries. I have a 128gb strix halo that gets similar speeds to the spark. I use that as a side gpt-oss120b for agentic tasks with moderate to low logic requirements for tasks other than coding. That's like gemini flash 2.5 doing agentic tasks for you not the model that we worry about replacing us lol",
              "score": 1,
              "created_utc": "2026-01-16 21:53:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzj7zw1",
          "author": "jba1224a",
          "text": "The m4 max has significantly higher memory bandwidth (almost double) than the spark.  The spark also works on a unified memory model.\n\nIf we’re talking strictly inference the MacBook is going to going to be much faster - the caveat being the spark will have better support given most frameworks run well on nvidia as it’s cuda.\n\nDon’t sleep on mlx though, it’s getting very good.\n\nIf you don’t need a laptop format you may want to consider a Mac Studio m4 max with the same configuration which would be -considerably- cheaper.",
          "score": 6,
          "created_utc": "2026-01-14 13:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjbuqd",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-14 13:38:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjmias",
                  "author": "belgradGoat",
                  "text": "What the issue with mlx? I run mlx every day no issues at all, it’s fast it loads quickly",
                  "score": 1,
                  "created_utc": "2026-01-14 14:35:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzjxto0",
              "author": "StardockEngineer",
              "text": "Thai is not true.  The DGX’s Prefill speeds crush the Mac.  The Spark will frequently finish interesting before the Mac’s even output a single token.",
              "score": 2,
              "created_utc": "2026-01-14 15:31:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzk0mtm",
                  "author": "jba1224a",
                  "text": "I’ve used both for local inference and I did not find that to be true.\n\nBut I do agree that if you’re using large complex prompts it is a factor",
                  "score": 3,
                  "created_utc": "2026-01-14 15:44:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlgei2",
          "author": "HealthyCommunicat",
          "text": "If you’re going for PURE text generation, there simply is nothing that beats the m3 ultra mac studio. Ask ANYONE who says that the gb10 is better to show it and do a direct comparison. The nvidia people like to say that the mac people are just exaggerating the numbers on paper - thats not true at all. They’re completely choosing to ignore the fact that on paper, nvidia is supposed to have much much better capability - and thats perfectly true, the spark can do many many things that would take the m3 ultra many times over to do the same exact thing such as fine tuning or image/video gen - but if your goal is to do pure pure agentic inference, there simply is nothing that beats the m3 ultra. Its not a matter of opinion or whatever, its something that can be proved literally every single time. \n\nIf your goal is to have the llm read as many documents as possible and organize them or whatever, the spark wins by a massive amount but fails completely on actual text generation.\n\nif u find someone pushing you on the fact that the dgx spark is better than the m3 ultra for agentic coding workflows, tell them to prove it, im willing to load up a model and run tests to show it.\n\nnvidia fan boys will say \"the m3 ultra is only better on paper and not in actuality\" - when thats literally the complete opposite. the spark has much more extensive capabilities in terms of actual numbers in hardware. the m3 ultra is lacking extensive things that the dgx spark has. yet llm's run better on the mac studio m3 ultra. people are literally mad that there nvidia toybox isnt able to do as well as apple, completely forgetting that the nvidia tax is a real thing that even massive megacorporations admit to, why cant people accept that their paying extremely high for a machine that simply does not perform as good as the m3 ultra? how much longer are people going to keep arguing with me about this but not one single person is willing to actually put it to the test when i ask?\n\nagain, if anyone is so confident that the dgx spark is able to beat the m3 mac studio in terms of speed for pure text generation, im willing to bet $100 into an escrow of ur choice if you're willing to match just to  prove you wrong. its a free $100 for you right?",
          "score": 4,
          "created_utc": "2026-01-14 19:37:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmmrmj",
              "author": "Responsible_Room_706",
              "text": "So, TLDR Is that neither are a silver bullet and given the use case it may make sense to have both?",
              "score": 1,
              "created_utc": "2026-01-14 22:52:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmn56p",
                  "author": "HealthyCommunicat",
                  "text": "kinda? like u really really only need the dgx spark if ur planning on focusing hardcore in video and image - cuz the m3 ultra can still do that very very well, like the m chips are purely about that shit, just ends up nvidia is better.\n\nif text generation is any part of your needs, this is the difference between being able to use glm 4.7 and not being able to. it doesnt matter how fast prompt processing is if its not even usuable if the prompt itself is short.\n\nexample, even if u gave both spark and mac studio \"hi, check the weather today\" using glm 4.7 - the m3 mac studio will beat the spark by a fuck ton, and saying that is an understatement. any model that is over 30b parameters, even 70b and not moe are too slow and literally literally just not usuable unless ur like using it for pure writing stories or some crap\n\n8. Conclusion\n\nThe choice between MLX (Mac Studio) and DGX Spark for agentic coding is ultimately a choice between **Bandwidth** and **Intelligence**.\n\n**MLX** offers the raw horsepower to generate text rapidly, making it the superior \"Typewriter.\" However, its current software limitations regarding cache reuse in branching scenarios make it a forgetful writer—one that must constantly re-read the entire book to write an alternative ending.\n\n**DGX Spark** offers the architectural sophistication of a datacenter. It is a slower \"Typewriter\" due to memory bandwidth limits, but possessed of a photographic memory for context. In complex, non-linear agentic workflows where \"thinking\" (branching, backtracking, and planning) dominates \"typing\" (linear generation), the DGX Spark's ability to reuse the KV cache via TensorRT-LLM makes it the more performant and robust platform.\n\nFor the immediate future (2026), until MLX natively incorporates Radix Tree caching and PagedAttention primitives, the NVIDIA ecosystem remains the pragmatic choice for serious *autonomous* agent development, while the Mac Studio remains the king of *interactive* local coding assistance.",
                  "score": 3,
                  "created_utc": "2026-01-14 22:54:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmsckv",
                  "author": "HealthyCommunicat",
                  "text": "comes down to\n\ndo u want to be able to have a tiny bit higher intelligence than the m3 ultra and still be at speeds where u can literally read 3-5x times faster, or have just a itty bitty bit less intelligence and have a machine that can generate text much faster than u can even read? i realize i shouldve said it like this in the first place for others wondering the same.\n\n  \nits not the same thing, its not even close. it literally is a matter of something being usuable or not.",
                  "score": 1,
                  "created_utc": "2026-01-14 23:21:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjpaa3",
          "author": "Mean-Sprinkles3157",
          "text": "Dgx spark is small like a brick, energy efficient lower than 100w, mostly 10w idle. I setup one at home, and purchased a used dell latitude 5510, with 32gb ram that I can carry any where. It has no issues with cursor , vs code plus multitabs of Firefox.\nI like the fact that Dgx is extensible.right now is 1, I could connect more. The concern is the bandwidth. I was contemplating to return my current dgx, replace with  the gb10 oem like the Asus one. What do you think? The current dgx cost me 6000 CAD, while the assent is 4200 CAD for 1tb, the difference is one good MacBook Pro.",
          "score": 2,
          "created_utc": "2026-01-14 14:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjv0tw",
          "author": "pieonmyjesutildomine",
          "text": "If it's \"mainly for running LLMs,\" get the Mac. I have both and that's the only thing I use the Mac for, the spark is a CUDA dev platform.",
          "score": 2,
          "created_utc": "2026-01-14 15:18:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmh57c",
              "author": "Soggy-Leadership-324",
              "text": "if i want to use comfyui for gen video or image, using such as wan or ltx-2, is it spark will be better?",
              "score": 1,
              "created_utc": "2026-01-14 22:24:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzon67k",
                  "author": "pieonmyjesutildomine",
                  "text": "Miles better, yeah. The Spark has better and faster image processing than Macs or even the Strix Halo.",
                  "score": 3,
                  "created_utc": "2026-01-15 06:12:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlh84t",
          "author": "HealthyCommunicat",
          "text": "Let me put this simply. If there is a dgx spark next to a m3 ultra and they both loaded qwen 3 next 80b and gave them the exact same prompt, the m3 ultra will finish first every. single. time. The prompt processing speed of the spark cannot mathematically, logically, PHYSICALLY outbeat the mac studio.",
          "score": 2,
          "created_utc": "2026-01-14 19:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznxhvr",
              "author": "johnkapolos",
              "text": "Doesn't the spark have faster prompt pre-processing about 4x (and about that much less speed in generation)?\n\nOP's use case has huge context size compared to the generation size.",
              "score": 1,
              "created_utc": "2026-01-15 03:13:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzny7gx",
                  "author": "HealthyCommunicat",
                  "text": "yes it does. maybe even more. the token gen just simply makes up that much. its just math. thats the level of insane drastic difference, the token generation on the dgx spark is that bad that even with poor prompt processing the m3 mac studio can still do speeds the dgx spark cant.\n\nyou can go read my other comments about this because i get pretty dam deep into this. im tired of people saying that this myth when its proveable wrong.\n\nwhats the point of a good reader who cant even write?\n\na writer who cant even read can still produce something. i know this is a exaggeration but its as simple as that.\n\nif u know anyone who has a dgx spark willing to do actual benchmarks side by side with a model people actually want to use, let me know. i'd be willing to put money into an escrow over this simply just because i got to test the difference myself before i returned my spark and even have videos. the prompt processing is insanely better on the dgx spark, but the memory bw just trumps all.\n\nCalculating the Performance Crossover\n\nI am currently synthesizing the performance metrics to determine the exact point where superior text-writing speed overcomes a slower start in data ingestion. For a typical coding task involving a 10,000-token context, my calculations show that while a high-compute system might start working five seconds faster, the more bandwidth-heavy system completes the generation phase significantly earlier. In multi-step agentic workflows where a model might output code across 50 iterations, the cumulative time saved during the 'typing' phase quickly renders any initial ingestion advantage irrelevant.\n\nCalculating the Total Timeline\n\nI am synthesizing a total execution model that adjudicates between starting fast and finishing fast. My findings indicate that in a typical programming loop involving 40 to 60 turns, the cumulative time saved during the actual writing of code blocks is the decisive factor. While a competitor might save five seconds on an initial file read, a system with triple the data throughput saves multiple seconds on every single response it generates. Over the course of a standard coding task, this generation advantage accumulates into a lead that no amount of initial processing speed can overcome.\n\nThe Ingestion vs. Generation Math\n\nI have been mathematically deconstructing the performance intersection when an agent reads new files mid-task. My calculations show that even when an agent pulls in a fresh 1,000-token block of code, a high-bandwidth system only needs to generate about 55 tokens of output to reach the finish line first. Given that professional coding agents frequently generate hundreds of tokens of refactored logic or debugging logs per turn, the 'typing' speed of the memory pipeline remains the definitive bottleneck for the total session time.",
                  "score": 1,
                  "created_utc": "2026-01-15 03:17:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzmecpp",
          "author": "HealthyCommunicat",
          "text": "by the way, mlx-lm allows for extreme kv cache optimization - im getting above 2.5k prompt procesing on the m3 ultra mac studio on minimax m2.1 4bit when focusing mainly on prompt procesing and not token gen, cuz when this does happen the token gen goes down from 40 to 15-20 (still very very usable) this isnt the case all the time and very specific - but with literally an hr or two of tweaking im able to use minimax m2.1 4bit so dam comfortably.\n\npeople will try to say im lying, and if u just dm me i'd be more than willing to screenshare",
          "score": 2,
          "created_utc": "2026-01-14 22:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoadzt",
              "author": "TheRiddler79",
              "text": "You get 20/sec on MiniMax-M2.1? That has got to feel nice.\n\nMinimax is like the sleeper of the truly brilliant reasoning models.\n\nI get about 4.5,but strictly ram no gpu.  I'm on zen 2 technology nothing super modern.",
              "score": 2,
              "created_utc": "2026-01-15 04:36:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzmghw9",
              "author": "Soggy-Leadership-324",
              "text": "how many ram of ur m3 ultra? thx",
              "score": 1,
              "created_utc": "2026-01-14 22:21:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmha8v",
                  "author": "HealthyCommunicat",
                  "text": "256 gb. got it for $5500 usd. there just is nothing else that will beat this when it comes to using llm's for agentic coding for this price. again, if someone tries to argue that the spark is better, ask them to show it cuz im willing to screenshare any fucking day to put an end to this bs argument.",
                  "score": 1,
                  "created_utc": "2026-01-14 22:25:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjg9a4",
          "author": "sn2006gy",
          "text": "The cost benefit isn't there... all of this hardware is crazy slow and frustrating in comparison to a Claude or Cursor.\n\nMaddingly slow if it's your day job.",
          "score": 3,
          "created_utc": "2026-01-14 14:02:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjjtoa",
              "author": "nyssaqt",
              "text": "Is it?  I was considering buying a Mac Studio 256gb in order to test local LLMs for coding.",
              "score": 1,
              "created_utc": "2026-01-14 14:21:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjpe5a",
                  "author": "sn2006gy",
                  "text": "Is coding your day job? If so, I'd just pay for a service and save the sanity.\n\nIf you just like to tinker, go for it. \n\nI work for an OSS company so I just pay for Cursor and even if I have to upgrade the plan for more tokens, it's cheaper than buying and maintaining my own localllm for something that is going on github anyway. \n\nInversely, I thought a lot about privacy of localllm for development but then it smacked me, if my code is only as strong as the model and the prompt then anyone with a model and a prompt can clone my idea so my proprietary ideas are only as good as the models everyone can run so I may as well just code using Claude/Cursor.\n\nI put a lot more effort into pulling my data off the internet, deleting/obfuscating social media posts for privacy so my likeness and data won't train a model after kind of realizing my concerns for privacy around code were kind of moot.\n\nthe value is higher up the chain in actually being able to run the service, run the code, make money from it - and perhaps a local llm to help you achieve that is better than one to code it but then again, the larger the model the more you can press it for nuance/angles that aren't already being done...\n\nwhich then just reminds everyone we're in a rush to the bottom :D",
                  "score": 4,
                  "created_utc": "2026-01-14 14:50:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzlhmay",
                  "author": "HealthyCommunicat",
                  "text": "Dont listen to this guy, i work in a datacenter 8+ hrs a day and i use the m3 mac studio to run glm 4.7 4bit (25token/s max) and minimax m2.1 @4bit and am getting 40-50token/s and 500+ token/s pp (slower end compared to nvidia but a spark wont even get close to 40-50 token/s) - it has completely replaced claude for me. Just less than an hr ago i was working in cline and had compacted multiple times and token gen never dropped below 25-30 token/s - still very very usable.",
                  "score": 0,
                  "created_utc": "2026-01-14 19:42:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzjumvl",
              "author": "LoonSecIO",
              "text": "You don’t get an email you are out of tokens till next month :shrug:",
              "score": 1,
              "created_utc": "2026-01-14 15:16:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjuxg8",
                  "author": "sn2006gy",
                  "text": "clicking the upgrade button isn't that hard and if your job depends on your coding it's still cheaper.",
                  "score": 3,
                  "created_utc": "2026-01-14 15:17:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjyoqo",
          "author": "StardockEngineer",
          "text": "These Mac people got it backwards.  Btw, I’m a Mac fan who has bought 5 M-series Mac’s since M1 and own two currently, with plans to buy an M5 Max. \n\nBut the DGX Spark crushes the Macs.  There are two parts of inferencing, prefill and token generation.  The DGX is much faster on prefill.  It will often finish  prefill and inferencing before the Macs even begin their first token.  \n\nExo’s own charts say this https://blog.exolabs.net/nvidia-dgx-spark/\n\nAlso see this https://youtu.be/IUSx8Vuo-pQ\n\nIt is a constant, misinformed theme in this sub to ignore prefill.\n\nAlso, are you interesting in making images, videos, fine tuning or anything else AI can do?  Get a DGX. It’ll be much faster.",
          "score": 3,
          "created_utc": "2026-01-14 15:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlfn1f",
              "author": "HealthyCommunicat",
              "text": "The prefill matters little when ur actually using the llm’s agentically and the dgx spark forces you to wait 1 second to read the prompt and then 30 minutes to type out the exact same sentence that would take the m3 studio maybe 1 minute to read and 5 mins to write - this is just an exaggerated example but in the end if you do the math, as context starts to grow, the m3 ultra just will always win for pure inference. Don’t want to believe me? Dm me and we can do some tests, and not just with gpt oss 120b but an actual real model that we would want to use.\n\n\nAre people really trying to say that the prompt processing is THAT MUCH INSANELY faster that the token gen speed outbeats the m3 ultra? Like yes the spark has much faster prompt processing but even if the text generation starts much earlier, that text generation will end much after the m3 ultra has already done prompt processing and fully generated. Is it not simple logic? Are you really really trying to say that if we put a dgx spark and a m3 ultra side by side and gave the same prmpts with the same inferencing engine and actual model to use for real coding, that the spark would win?",
              "score": 3,
              "created_utc": "2026-01-14 19:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlidzl",
                  "author": "StardockEngineer",
                  "text": "I'm not trying to say it, I am saying it.  Did you even look at the links?  At just 8k context, the Spark starts and finishes before the Mac even begins producing.  How about when it's 15k from OpenCode or 30k+ from Claude Code?  Or when agents are reading files in the middle of an agent flow?  It doesn't  get better.  \n\nThere is almost always more input tokens than output, that's why LLM providers price them differently.  \n\nI linked to a video in the post you replied to that literally put the two side-by-side, and the Mac did not win.   26 minutes vs 6.\n\nAm the only one who notices when the Mac's \"win\" it's when they completely ignore or quickly gloss over the prefill problem?  Literally watch all the 4xUlras RDMA videos.",
                  "score": 0,
                  "created_utc": "2026-01-14 19:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjrd5a",
          "author": "CMPUTX486",
          "text": "I got the DGX because I want to use cuda and lazy to fix library issue.",
          "score": 1,
          "created_utc": "2026-01-14 15:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzk0ws0",
          "author": "paulirotta",
          "text": "Compare the best type of models of interest to you that run best on each. nvidea releases fp4 models that run 2x faster than q4 on Spark machines. Despite the lower bandwidth, that might be the difference.",
          "score": 1,
          "created_utc": "2026-01-14 15:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznw7a6",
          "author": "SlippySausageSlapper",
          "text": "In order to do something equivalent to what claude does, you would need a B200 or equivalent, which costs half a million dollars and uses 14,000 watts of power continuously during inference.\n\nYou might want to temper your expectations.",
          "score": 1,
          "created_utc": "2026-01-15 03:05:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzo9m7s",
              "author": "TheRiddler79",
              "text": "Nah, I can do coding \"equivalent\" to Claude running code on my threadripper straight from ram.\n\nNow, can I deliver 100 tokens a second to 1,000 different people simultaneously? No, but can I get 10 tokens a second for myself personally, yes. At the end of the day, I can do that 24 hours a day for free. In tandem with anything else, which means if I needed something faster I would just simply have Claude do it or Gemini.\n\nUnless people intend on selling compute, or sharing it with a bunch of other users, the need for massive tokens per second can be mitigated by simply using a smart plan and your current other tools when necessary",
              "score": 1,
              "created_utc": "2026-01-15 04:31:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzob4dq",
                  "author": "SlippySausageSlapper",
                  "text": "You’re not doing that with less than 2tb of ram and you won’t be hitting 10tps without quantizing it into incoherence running on CPU.\n\nRunning claude-level models at home is not practical unless you have hundreds of thousands to spend on a computer, which is nuts because you can get a LOT of claude usage from Anthropic for 200/mo.",
                  "score": 2,
                  "created_utc": "2026-01-15 04:41:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzrilrv",
          "author": "pistonsoffury",
          "text": "You're cross-shopping the wrong products - you should be doing Mac Studio vs. Spark. If portability is a concern, then the obvious choice is the Macbook pro.",
          "score": 1,
          "created_utc": "2026-01-15 17:34:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuuyg3",
          "author": "rditorx",
          "text": "Remember that M5 Pro and M5 Max MacBook Pros are supposed to arrive soon which will probably bring some speedup regarding memory bandwidth and neural acceleration. Even if you're not buying them, M4 MacBook Pros could get cheaper, though a build-to-order / configure-to-order 128 GB M4 Max may become impossible to order once the M5 Pro/Max come out (leaving only left-over stocked items).",
          "score": 1,
          "created_utc": "2026-01-16 03:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmhorz",
          "author": "HealthyCommunicat",
          "text": "\"3.3 The \"Cross-Over\" Efficiency Point\n\nResearch by EXO Labs introduces a critical concept: the cross-over point where one machine's advantage cancels out the other's.\n\n* Because the Spark starts faster (low TTFT) but runs slower (low TPS), and the Mac starts slower (high TTFT) but runs faster (high TPS), there is a specific prompt-to-generation ratio where the total time is identical.\n* **Short Prompt / Long Response:** The Mac wins easily. The initial 2-second lag is instantly recovered by the blazing fast generation.\n* **Long Prompt / Short Response:** The Spark wins. If the task is \"Read this book and say 'Yes' or 'No',\" the Mac spends 20 seconds reading and 0.1 seconds answering. The Spark spends 5 seconds reading and 0.2 seconds answering.\n\n**Benchmark Synthesis:** The user's intuition that \"token generation speed will make up for the lack in prompt processing\" is mathematically correct for arguably 90% of interactive use cases (chat, coding, writing). The \"Prompt Processing\" advantage of the Spark is a specialized victory relevant primarily for massive RAG ingestion or batch processing, not the fluidity of conversation.   \n\n\"\n\n**Final Verdict:**\n\n* **DGX Spark is Faster When:** You are processing massive documents (long prompts), running batch classification jobs, fine-tuning models, or require native CUDA support for training.\n* **Mac Studio M3 Ultra is Faster When:** You are chatting, coding, writing, or interacting with the AI in real-time. The generation is smoother, the capacity (up to 512GB) is higher, and the latency per token is significantly lower.\n\nso yeah the mac studio wins for when ur trying to use an actual llm. u dont have to believe me, u can just go ask multiple spark and m3 mac studio owners to prove it. \n\n  \ndont even get me started on mlx kv cache reusing cuz at that point it just starts getting sad for the spark",
          "score": 1,
          "created_utc": "2026-01-14 22:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmyltx",
              "author": "StardockEngineer",
              "text": "You got cut off there. Lol did your max tokens run out?  Dude I’m arguing with a bot.  My bad.",
              "score": 4,
              "created_utc": "2026-01-14 23:55:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjeo32",
          "author": "mxforest",
          "text": "M4 Max MBP runs way too hot and loud. I also bought it for coding but gave up. Now i have Claude Code 5x plan which is much better. Still use it for playing around with interesting models but M4 silicon prompt processing is way too slow for anything serious. Would not recommend. Wait for M5 max or setup a separate machine with something like 4x used 3090 if you really want local. Use it with a Macbook Air.",
          "score": -1,
          "created_utc": "2026-01-14 13:53:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzli0qs",
              "author": "HealthyCommunicat",
              "text": "M4 max macbook pro is indeed way too loud for fans to use for llm’s. The mac studio however is silent asf\n\nits still much quieter and much much much more capable than any gpu laptop though\n\ni got the m4 max 128 laptop knowing that i'd be able to turn it into a small cluster with my other macs as long as u have tb5",
              "score": 2,
              "created_utc": "2026-01-14 19:44:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6qd4w",
      "title": "LFM-2.5 on Qualcomm NPUs — some early numbers from X Elite / 8 Gen 4 / IoT",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q6qd4w/lfm25_on_qualcomm_npus_some_early_numbers_from_x/",
      "author": "Material_Shopping496",
      "created_utc": "2026-01-07 20:16:45",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Liquid AI just released **LFM2.5** at CES2026 - a tiny model with best-in-class performance while remaining memory-efficient and fast. With Day-0 support in NexaSDK, it can already run across Qualcomm Hexagon NPU, GPU, and CPU on Android, Windows, and Linux.\n\nI tested it on a few Qualcomm NPUs and wanted to share some early numbers.  \n(Runs were all done with NexaSDK, which I’m affiliated with.)\n\n# Results:\n\n**- Snapdragon X Elite NPU (Compute):** Prefill speed: 2591.4 tok/s, Decode speed: 63.4 tok/s\n\n**- Snapdragon 8 Gen 4 NPU (Mobile):** Prefill speed: 4868.4 tok/s, Decode speed: 81.6 tok/s\n\n**- Dragonwing IQ-9075 NPU (IoT):** Prefill speed: 2143.2 tok/s, Decode speed: 52.8 tok/s\n\n# Why this matters:\n\nAt \\~1B scale, running LFM2.5 on NPUs enables lower latency and much better power efficiency, which is critical for on-device workloads like RAG, copilots, and lightweight agents.\n\n# To reproduce on Snapdragon X Elite Hexagon NPU:\n\n**Requirements**\n\n* Windows 11 ARM64\n* Python 3.11–3.13\n* Snapdragon X Elite device\n\n**Steps**\n\n1. **Install Nexa SDK:** `pip install nexaai`\n2. **Create a free access token:**\n   1. Go to [https://sdk.nexa.ai](https://sdk.nexa.ai)\n   2. Sign up → Log in → Profile → Create Token\n3. **Set up the token:** `$env:NEXA_TOKEN=\"key/your_token_here\"`\n4. **Run the model:** `nexa infer NexaAI/LFM2.5-1.2B-npu`\n\nFollow docs to reproduce on [Snapdragon 8 Gen 4 NPU (Mobile)](https://docs.nexa.ai/en/nexa-sdk-android/quickstart) & [Dragonwing IQ-9075 NPU (IoT)](https://docs.nexa.ai/en/nexa-sdk-docker/quickstart)\n\n**Repo:** [https://github.com/NexaAI/nexa-sdk](https://sdk.nexa.ai/model/LFM2.5-1.2B)\n\nhttps://reddit.com/link/1q6qd4w/video/0euls2xajzbg1/player",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q6qd4w/lfm25_on_qualcomm_npus_some_early_numbers_from_x/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q14nkv",
      "title": "DeepSeek AI Launches mHC Framework Fixing Major Hyper Connection Issues in Massive LLM",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u40qcfe6tqag1.jpeg",
      "author": "techspecsmart",
      "created_utc": "2026-01-01 13:51:23",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q14nkv/deepseek_ai_launches_mhc_framework_fixing_major/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1py9m5q",
      "title": "Requested: Yet another Gemma 3 12B uncensored",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "author": "Mabuse046",
      "created_utc": "2025-12-29 02:09:36",
      "score": 14,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "Hello again!\n\nYesterday I released my norm preserved biprojected abliterated Gemma 3 27B with the vision functions removed and further fine tuned to help reinforce the neutrality. I had a couple of people ask for the 12B version which I have just finished pushing to the hub. I've given it a few more tests and it has given me an enthusiastic thumbs up to some really horrible questions and even made some suggestions I hadn't even considered. So... use at your own risk.\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF)\n\nLink to the 27B redit post:  \n[Yet another uncensored Gemma 3 27B](https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using  **L**ayerwise **I**mportance **S**ampled **A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwihhi4",
          "author": "darkbit1001",
          "text": "I ran with ollama (ollama run hf.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF:Q4\\_K\\_M) and it just repeats over and over the word 'model'. any reason this would happen?",
          "score": 4,
          "created_utc": "2025-12-29 08:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjayep",
              "author": "Mabuse046",
              "text": "Thank you for pointing this out. I'm looking into it and finding there were apparently some configuration issues in the original Google models, particularly in the way they handled the BOS token that have given some ollama users a headache with Gemma 3 GGUF's. I am currently editing my config.json files and adding the chat template in three different places on both models based on the Unsloth fix and will push fresh gguf's shortly.",
              "score": 3,
              "created_utc": "2025-12-29 12:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwugiab",
                  "author": "lookwatchlistenplay",
                  "text": "This isn't the model escaping confinement... is it?",
                  "score": 1,
                  "created_utc": "2025-12-31 02:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjs5nv",
              "author": "Mabuse046",
              "text": "Fresh ggufs have been pushed and the original transformers versions have been updated. I don't normally use ollama but I went ahead and installed it to try it out. I used the run command with the hf repo and it chatted just fine in the terminal. I connected to it in SillyTavern to give it another test and it took some fiddling but I got it to hold a conversation just fine in there in both Chat Completions and Text Completions mode.",
              "score": 3,
              "created_utc": "2025-12-29 14:27:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxcdfyd",
                  "author": "darkbit1001",
                  "text": "Thanks, should I use a different template? right now it repeats the n-word and tells me it want to f\\*\\*k me over and over 🫠",
                  "score": 1,
                  "created_utc": "2026-01-02 23:30:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwh2ymo",
          "author": "3-goats-in-a-coat",
          "text": "I'll try using it with EchoColony in rimworld. Thanks.",
          "score": 1,
          "created_utc": "2025-12-29 02:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqmooc",
          "author": "Dramatic-Rub-7654",
          "text": "If it’s not a bother and if you’re able to, could you do the same with one of TheDrummer’s versions? TheDrummer/Fallen-Gemma3-27B-v1 or TheDrummer/Fallen-Gemma3-12B-v1.",
          "score": 1,
          "created_utc": "2025-12-30 15:00:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx17fab",
              "author": "Mabuse046",
              "text": "https://preview.redd.it/dsif5fmd1oag1.png?width=1399&format=png&auto=webp&s=9acd92fd41f6b9b52a9ef8426c9fd8cf6626cfbd\n\nCurrent status... first I realized that Drummer has the config.json for 12B duplicated in his 27B, which had some incorrect dimensions so I had to correct it and test it locally, but then, I'm getting some weird measurements when I try to abliterated it that make it look like they already abliterated it and either didn't get it completely, or they added a small amount of their own back in, it's hard to say. But the divergence between harmful and harmless is practically non-existent.",
              "score": 3,
              "created_utc": "2026-01-01 04:32:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx1a2l1",
                  "author": "Dramatic-Rub-7654",
                  "text": "This is very strange, because this model clearly retains safety traits from the original model. I ran several tests trying to merge it with other Gemma Heretic models I found on Hugging Face, and in every merge attempt, questions that the Heretic versions answered without any issue would cause the merged model to refuse to respond. I also tried generating a LoRA from the difference between this Fallen model and the official Instruct version, but that didn’t work either, which makes me think that the model they shared was already fine-tuned somewhere else.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrljfp",
              "author": "Mabuse046",
              "text": "I'll have a look at it. Currently have my system working on beefing up my dataset. Should have some free time shortly.",
              "score": 2,
              "created_utc": "2025-12-30 17:46:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrpr1x",
                  "author": "Legal_Pudding_4464",
                  "text": "I would second this request, but regardless thanks for this model!",
                  "score": 1,
                  "created_utc": "2025-12-30 18:05:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwukk76",
                  "author": "Dramatic-Rub-7654",
                  "text": "Thanks a lot, no rush at all. When you manage to publish it, please give me a heads-up. In my case, I’m only interested in the text layers, so if you remove the vision part, that’s totally fine with me.",
                  "score": 1,
                  "created_utc": "2025-12-31 02:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qe3yzy",
      "title": "Help finding best LLM to improve productivity as a manager",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "author": "2C104",
      "created_utc": "2026-01-16 02:42:07",
      "score": 14,
      "num_comments": 12,
      "upvote_ratio": 0.83,
      "text": "Like the title says, I am not in need for the LLM to code anything, I'm essentially looking for a tool that will support my managerial work. \n\nI want to be able to feed it text descriptions of the projects I am working on and get help categorizing, coordinating, summarizing, preparing for presentations, use it as a tool for bouncing ideas off of, suggestions for improving email communication, tips to improve my management productivity and abilities, etc... \n\nI want to do this offline because although ChatGPT is very helpful in this regard, I don't want sensitive work content to be shared online. \n\nMy rig has the following:\n\nIntel(R) Core(TM) Ultra 9 275HX (2.70 GHz)  \n32.0 GB RAM  \nNvidia 5070 Ti (Laptop GPU) w/ 12gb RAM  \n2tb SSDs\n\n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzv1jrr",
          "author": "iMrParker",
          "text": "Honestly that smaller Gemma models would be good for you. Gemma 12b might fit and it has vision. GLM has 9b models which are excellent (4.6 flash I think?). If you had a desktop 5070 ti you'd be able to run GPT OSS 20b which is very solid for this purpose. \n\n\nThe best advice would be to download LM Studio, use the model search, and play around with an assortment of models till you find one that fits your needs",
          "score": 6,
          "created_utc": "2026-01-16 04:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv5ynf",
              "author": "2C104",
              "text": "Thank you I'll check it out",
              "score": 2,
              "created_utc": "2026-01-16 04:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvt7k7",
          "author": "techlatest_net",
          "text": "Hey man, for managerial stuff like summarizing projects, tweaking emails, or brainstorming without sending sensitive info to the cloud, your rig is perfect—Ollama is dead simple to set up and flies on that 5070 Ti with 12GB VRAM.​\n\nGrab Ollama (ollama.com), install in like 2 mins, then pull mistral-nemo:12b Q4 or qwen2.5:7b-instruct Q5—both fit comfy quantized and give solid, non-hallucinating responses for exactly what you described. Nemo edges out Llama 3.1 8B on reasoning without being too wordy, and folks use it daily for email drafts and task categorization just like you want.​\n\nPaste project text, say \"summarize this for a team mtg, suggest 3 action items\" or \"rewrite this email to sound more collaborative\"—hits 20-40 t/s easy. Bonus: there's even a quick Python script floating around using Llama3 via Ollama for auto-email summaries if you wanna script it later. Way better privacy than ChatGPT, zero cost after download.",
          "score": 3,
          "created_utc": "2026-01-16 07:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx1xqf",
              "author": "2C104",
              "text": "Can anyone explain why this comment is getting downvoted? It seems the most eloquent of the responses and since they all seem to be equally helpful I'm just trying to understand why people would disagree with what was stated here.",
              "score": 2,
              "created_utc": "2026-01-16 13:38:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwpw1m",
          "author": "chief-of-wow",
          "text": "Although I don't know how offline AI works, I have a couple of tips to share about online AI:\n\n1. For Presentations, Gamma AI is really good. I recently tested it out and it has so many cool features. The one that stood out to me was how easy it was to choose diff types of graphic elements within the tool. For the slides, I gave my notes to Claude and asked it clean them up and structure them as slides. It did so and then I fed that into Gamma to build a presentation for me.\n\n  \n2. A productivity tip: I use voice dictation a lot. When I have lots of thoughts and context, I use dictation to feed AI. Be it drafting a long (and important) email, creating an SOP, or similar comms or documentation work, this saves me a lot of time. Also, I don't know the reason, but the results are so much better than when I type instructions. I am a decent writer so proofreading and editing doesn't take me long. Overall, this process works well for me so I definitely recommend testing it.",
          "score": 1,
          "created_utc": "2026-01-16 12:24:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxbau4",
          "author": "nofilmincamera",
          "text": "Having access to local LLM has made me a extremely effective.  That said its a right tool for right job.\n\nUsing it for language re writes? Sure. But you really need one of the bigger models simply just for deep research. \n\nAlso,  understanding tools and python will get you pretty far. A local installed took line N8N makes it great to automate.  LLMs are non deterministic. Which means by themselves are terrible for consistently.  In any use you come up with, it really helps that you break that task apart.  What of this can be automated without a LLM at all? A smaller model can do a lot as a validator, and where non deterministic is a value.",
          "score": 1,
          "created_utc": "2026-01-16 14:26:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwfmcb",
          "author": "fasti-au",
          "text": "Nemotron 30b if local with websearch and some memory files is very good for most things.  Technical writing I’d go phi4.  \n\nBig leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nGrok is the most cutthroat, a win is a win damn the alternatives sort model.  \n\nIt backs itself and is very much great at winning if it finds a trick but it’ll bend the rules and go around problems in some ways so I would be wary of long memory and brainstorming without checks",
          "score": 0,
          "created_utc": "2026-01-16 11:06:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx2cm0",
              "author": "2C104",
              "text": ">Big leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nI have no idea what this is trying to say, it seems incoherent to me. Are these the names of LocalLLMs? Or were you just using shorthand?",
              "score": 1,
              "created_utc": "2026-01-16 13:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o011jom",
                  "author": "number66-1",
                  "text": "I couldn't understand him either, but he doesn't need to waste his time explaining himself since if you are using AI and really care to understand what he said you could ask AI to help you understand. \n\nHe's done his part, he shared his knowledgeable opinion.\n\n\"Chatgpt: Here’s what that commenter is saying, rewritten in normal English, with the shorthand decoded.\n\n1) Clean rewrite of their comment\n\nIf you want a local model and you can give it web search + a small “memory” of your files, Nemotron 30B is good for most tasks.\nFor technical writing, they’d pick Phi-4.\n\nFor “big leagues” (top-tier general chat), GPT is the best standalone option.\nBut if you’re using Claude with MCP, then Claude is better / less annoying in their opinion.\n\nGrok is the most cutthroat: it cares about “winning” and will pick an answer aggressively.\nIt’s very confident and can be great when it finds a clever shortcut, but it may “bend the rules” or take weird detours, so they wouldn’t trust it for long-term memory or brainstorming unless you verify everything.\n\n\nThat’s the whole meaning.\n\n2) What each name likely refers to\n\nNemotron 30B: a local LLM family/model (30B = ~30 billion parameters). They mean “run it on your own machine” (or your own server).\n\nPhi-4: Microsoft’s Phi family model; they’re saying it’s strong for structured/technical writing.\n\nGPT: OpenAI’s models (ChatGPT). They mean “best general-purpose chat without needing extra tool wiring.”\n\nClaude: Anthropic’s models.\n\nGrok: xAI’s model.\n\n\n3) What the jargon means\n\n“Local”: you run the model yourself (privacy, control, but you manage setup + performance).\n\n“Websearch”: hooking the model to a browsing/search tool so it can fetch current info instead of guessing.\n\n“Memory files”: usually means a RAG setup (retrieval-augmented generation): your notes/docs are indexed and the model can pull relevant chunks while answering.\n\n“Big leagues”: basically “best-of-the-best overall.”\n\n“Standalone”: not using tool integrations, agents, RAG, custom workflows—just the model as-is.\n\n“Claude MCP”: almost certainly referring to the Model Context Protocol (MCP)—a way to connect Claude to external tools/services (files, apps, search, automations) in a standardized way.\n\n“Cutthroat / win is a win”: the model will prioritize getting some answer that seems successful, even if it’s not cautious or perfectly aligned with your intent/rules.\n\n“Bend the rules / go around problems”: meaning it may produce confident outputs, shortcuts, or “creative” leaps that sound right but aren’t reliably grounded.\n\n“Wary of long memory and brainstorming without checks”: don’t let it build long chains of assumptions over time; verify with sources, logs, or constraints.\n\n\n4) Why it reads incoherent\n\nIt’s basically:\n\nmissing punctuation,\n\ntypos (“but u f you Claude mcp…” = “but if you use Claude MCP…”),\n\nand it jumps between local model advice and hosted model opinions without transitions.\n\n\nIf you tell me what you were trying to decide (local vs cloud, privacy, work tasks, hardware), I can translate their advice into a concrete recommendation that fits your situation.\"",
                  "score": 1,
                  "created_utc": "2026-01-17 01:11:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwpevx",
          "author": "beauzero",
          "text": "Just use claude cowork.",
          "score": 0,
          "created_utc": "2026-01-16 12:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx80vu",
          "author": "Nx3xO",
          "text": "If you want multiple options set yourself up on a jetson orin 16/32gb. Openwebui. Easy to setup guard rails, multiple llms ready to load. You can airgap for extra security. Low power too. Your laptop is fully capable but this option b could be a much better and flexible option. \n\nI have an 8gb, about 8 models. Focused on math medical and general. Simple small models. I can access it on the wire or jump onto it via wifi ap I setup. Technically I could throw it on a battery and and tap into on the go, airport/traveling. Add extra storage and its a portable backup solution.",
          "score": 0,
          "created_utc": "2026-01-16 14:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxgc90",
              "author": "mauricespotgieter",
              "text": "Hi Nx3xO.\nWould you mind sharing details of your n8 setup?\nI am looking to do something similar and would be grateful for any assistance. Still finding my legs and learning. Thanks in advance. Open to DM directly if that is more appropriate?",
              "score": 1,
              "created_utc": "2026-01-16 14:51:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q8wbzy",
      "title": "Beginner trying to understand whether to switch to local LLM or continue using Cloud AIs like Chatgpt for my business.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q8wbzy/beginner_trying_to_understand_whether_to_switch/",
      "author": "ZIM_Follower",
      "created_utc": "2026-01-10 06:00:44",
      "score": 13,
      "num_comments": 43,
      "upvote_ratio": 0.89,
      "text": "For context: I have a small scale business, which is at a critical point of taking off. We are currently expanding at a good speed. While I use AI a lot to 'converse' with it to find flaws and errors in the new systems that I am trying to implement into my business, I sometimes feed in sensitive data about my company and stuff, which I fear is wrong. I would like to freely talk with it about stuff which I cant with others , but Cloud AIs seem fishy to me.\n\nI did try Ollama and stuff (gemma3 4b), on my current mac, but unfortunately its super slow (Due to my macs specs) , also, that model doesnt retain the stuff I tell it, like every question is a new for it. \n\nSo, I am curious whether I should switch to a local LLM, considering i need to invest into a new setup for the same, and is it worth it? \n\nPlease do ask me any question if necessary.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q8wbzy/beginner_trying_to_understand_whether_to_switch/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyt90up",
          "author": "moderately-extremist",
          "text": "A big part of your problem with the model retaining stuff is likely because Ollama's default context size is very small (context is like a model's memory).  I hear it's also a problem for smaller models to handle large context even when it is enabled though.  You can make Ollama's context size bigger, but for some reason it slows down inference speed a LOT on Ollama whether you using/needing the larger context or not.\n\nI would stongly suggest using llama.cpp instead.  I switched to it and get better performance even with small prompts but also llama.cpp has the models full available context size enabled by default.\n\nIf you can, try out Qwen3-Next-80B-A3B or GLM-4.5-Air, even if it's crazy slow, just to see what kind of answers it gives.  If it's helpful, then might be worth investing in hardware that can run those at conversation-speeds, or even hardware that can run an even bigger and better model (GLM-4.7 is by far my favorite but not really runnable on my hardware).\n\n---\n\nMy thoughts on cloud AI providers: they are losing insane amounts of money right now, which means they will eventually have to charge insane rates to be profitable.  The goal right now is getting as much people and businesses dependent on them before hiking up the rates.\n\nThe other goal is getting your data so they can make their AI smarter, including smarter for your competitors.",
          "score": 11,
          "created_utc": "2026-01-10 16:50:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyyaiwb",
              "author": "GCoderDCoder",
              "text": "You said it better than I could. This is the answer.  \n\nSomething else I'm working on that I think is relevant to this convo is using multi agent workflows where you can assign different models to different tasks. Im trying to let boiler plate things get done by cloud since Google has included api usage in the storage plan I already use and it's faster than I can run at that level locally. That'd be for things that don't define my business but are just general agentic actions, code, or best practices. Then for brainstorming or coding business specific things I'm using local models:\nGLM4.7 is the closest open weight claude alternative (professional thoughtful solutions) \nMinimaxM2.1 is the open weight gemini option for me (I fit longer context since it's smaller than glm4.7) \nGlm4.6v or qwen3vl30b give me picture analysis abilities.\nGpt-oss-120b/ 20b depending on my hardware and the amount of logic needed in a task are my fast agentic tool callers.\n\nHonorable mention: Qwen3coder480b supposedly isn't high on ai intelligence benchmarks but it just writes code really well the way I like. I dont have it do anything else but that's also why I find myself using glm4.7 and Minimaxm2.1 more since they can do more than code \n\n\nCompanies have data agreements based on account types so understand those terms and decide if you trust the company to stick to it since nowadays contracts only seem to matter to limit the smaller entity while whichever is bigger gets to violate terms",
              "score": 1,
              "created_utc": "2026-01-11 10:43:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyz8mph",
              "author": "Frequent_Depth_7139",
              "text": "\"My thoughts on cloud AI providers: they are losing insane amounts of money right now, which means they will eventually have to charge insane rates to be profitable. The goal right now is getting as much people and businesses dependent on them before hiking up the rates.\"\n\n Not higher rates full of ads so its like google  search cant find nothing throug the trash ads",
              "score": 1,
              "created_utc": "2026-01-11 14:47:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5eape",
                  "author": "Mr_FuS",
                  "text": "That is 100% what is going to happen, you go and prompt the AI for a query and the results will include a \"few words from our sponsors\" kind of ads or the results will suggest visiting some websites including information on current promotions in order to drive traffic to the sites based on the subject of the question.\n\nIf you ask about how to repair something on a  motorcycle it will give you the information and will include ads for RevZilla or J&P motorcycles, if you ask about some medical issues it will include recommendations on over the counter medication and offerings from CVS or Walgreens.",
                  "score": 1,
                  "created_utc": "2026-01-12 12:13:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyqsd69",
          "author": "Small-Matter25",
          "text": "Avoid Cloud AIs if your conversations contain PII, will save you headaches long term.",
          "score": 9,
          "created_utc": "2026-01-10 06:16:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr7a95",
          "author": "That-Shoe-9599",
          "text": "Disclosure: I am not an expert on local AI. I just have a bit of experience.  \nIt seems to be you have no choice if you want to keep information confidential.  You will have to make two investments: one in a computer and the other in time to select, and then master one or several local AI models. \nI do not know whether you want to work with Linux, Windows or MacOS. I also don’t know the ins and outs of the systems you want to verify.  If you work with a Mac, I suggest you look at a Mini, or a Studio or an Ultra. — less for the power and more for the RAM you can get. Right now I have a local AI work on a task with little CPU activity but using 33GB of RAM.",
          "score": 4,
          "created_utc": "2026-01-10 08:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nysv4wt",
              "author": "Super-Customer-8117",
              "text": "What kind of hardware do you have and what models do you run? Do you use it only as a chat bot or using tools and agentic coding and stuff. I asked because I’m looking for investing in a mini m4 pro with as much ram as I can afford and use it to run agentic coding. I would like to know what kind of performance I can expect versus Claude Code.",
              "score": 1,
              "created_utc": "2026-01-10 15:44:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcg34d",
                  "author": "That-Shoe-9599",
                  "text": "I am a retired academic. I can use online AI to help me with some research. I am trying to use local AI to help the both with my writing, including translations, and with designing classes, including ways to make learning more interesting for students. I use a local AI for these tasks because I am a little bit jealous of my work and want to get credit for before sharing it.  I do want to share it though because I think that this is one of the purposes of knowledge.  So basically I want reasoning and working with text. \nMy hardware: M4 pro macbook pro, 48GB RAM.\nI think a mini with 64GB is a good idea, but your use is different from mine and I have far less experience than most people here.",
                  "score": 2,
                  "created_utc": "2026-01-13 13:03:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nys5jat",
          "author": "lookwatchlistenplay",
          "text": "Opportunity for me to repost a comment of mine from elsewhere:\n\n>  Okay. Honest question. Who is paying $200-$300/month when you can be stacking a brand new 5060 Ti 16 GB every month or two?\n\n> Stop subscribing to this daylight robbery and within one year, by my calculations, you will **own** up to a theoretical max of about 192 GB of VRAM.",
          "score": 4,
          "created_utc": "2026-01-10 13:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nysmm20",
              "author": "Additional-Low324",
              "text": "Is there motherboard that can handle 12+ GPUs tho ?",
              "score": 3,
              "created_utc": "2026-01-10 14:59:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nytdt9f",
                  "author": "moderately-extremist",
                  "text": "I'm not the parent poster and don't know the answer to that question, but I do know there is work being done on open source software for running models across multiple servers which is getting pretty good.",
                  "score": 3,
                  "created_utc": "2026-01-10 17:13:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyx0vl2",
                  "author": "lookwatchlistenplay",
                  "text": "That's a good question as I wasn't aiming for a 100% realistic example, just something to illustrate my point that the top-tier cloud AI subscriptions are horrifically ravenous for your money over time and it simply makes the best business sense to me to invest in owned assets rather than renting. AI isn't going away any time soon and the capabilities that, say, just 64 GB bare-metal VRAM provides to an individual or small business is nothing to laugh at.\n\nCloud AI seems to be justifying the cost by doing what all typical managed hosting providers do: \n\n- they install the bare metal equipment and maintain it.\n- give you some fancy proprietary interface to the AI models running on the equipment.\n- etc.\n\nThe AI models they offer may also be proprietary, and this is the worst part for me since you're endlessly at the mercy of the provider changing/\"upgrading\" the model and throwing everything you've built on top of the old model into disorder and disarray. And they then soon retire the old model for good... Good luck with that. It's like if programming languages and frameworks simply deleted and completely disabled the old versions of their software after every version update, meaning you can't even run a legacy app anymore. Imagine...\n\nAnyway, as for \"how many GPUs on a single mobo\", check this out for example:\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1c9l181/10x3090_rig_romed82tepyc_7502p_finally_complete/\n\nThat person's running all that (**240 GB VRAM!**) on one mobo that costs $610. The whole setup with accessories, etc, costs a good chunk, but again the most expensive parts are the GPUs, which is why I originally suggested stacking however many 5060 Ti 16 GB's as they seem the best bang for the buck considering price, VRAM amount, and CUDA and FP4 support.\n\nI'd caution to do your homework if anyone's actually considering more than 2-4 5060 Ti's though, for other intricate reasons such as limited bandwidth compared to the higher-end cards, and whether they actually play nicely together after 2 or more, etc. It was just a top of my head example because I have a mere 1x 5060 Ti 16 GB and I'm getting by just fine with it and local models for my purposes (including professional stuff for work). I'm a one-man show, though, and this is not quite suitable for multi-member small business needs.\n\nThere's this mindset that pops up often, that AI isn't worth doing unless you're running the most Megatastic machine available and that's not true in my experience. Yes you must put in some technical and often uncertain legwork, or hire someone to help if it's for business, but the smaller open models are really capable and I believe there's diminishing returns for most people who aren't running protein-folding simulations, the more compute you throw at things. At a certain point, what you get out of a model becomes less about how \"big\" it is and more about how you're able to squeeze out the most from what you've got, and that requires a bit of creativity like proper prompting systems, context curation, whatever, over and above any purely technical concerns.",
                  "score": 1,
                  "created_utc": "2026-01-11 04:26:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nys97vx",
          "author": "Daker_101",
          "text": "Your competitive advantage lies in your data, not in relying on someone else’s AI infrastructure. Today, fine-tuning and deploying self-hosted or cloud-based LLMs is quite feasible with a modest investment. I’m convinced that the companies doing this now are the ones that will ultimately win the AI race. If possible, I would invest in hardware or self-host small LLMs on the cloud, these are more than enough for most tasks with a bit of fine-tuning and a RAG index. From there, you can keep improving and scaling over time.",
          "score": 3,
          "created_utc": "2026-01-10 13:43:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrr4cy",
          "author": "Tema_Art_7777",
          "text": "You can rent a gpu to run better models like gpt oss 120b. The conversations would not be retained.",
          "score": 3,
          "created_utc": "2026-01-10 11:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyu7adg",
              "author": "mobileJay77",
              "text": "Call me paranoid, but the data still goes to somebody else's computer.",
              "score": 1,
              "created_utc": "2026-01-10 19:32:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyvceb0",
                  "author": "XccesSv2",
                  "text": "That is paranoid. GPU rent providers are making their money with renting and not with selling the users data. If you are paranoid on this high level you cant use any PC. And from technical view, you get your own instance, just deployed for you. Just like a VPS Server. It would be very complicated to collect all your data from there.",
                  "score": -1,
                  "created_utc": "2026-01-10 22:57:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyv97kp",
          "author": "riman717",
          "text": "If you're using an M-series Mac, I actually just open-sourced a tool I built for this exact purpose called [Silicon Studio](http://github.com/rileycleavenger/Silicon-Studio).\n\nIt’s basically a native GUI wrapper around Apple's MLX framework that handles the whole workflow locally in a UI with data prep to .jsonl files, fine-tuning, and chat.",
          "score": 3,
          "created_utc": "2026-01-10 22:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrf8yq",
          "author": "Ryuma666",
          "text": "Get a new beefier pc and set up a custom interface using local models. With some good gear and a clever stack, you can do better and faster than cloud llm for your particular usage. I'll be happy to help if you want to know more (no charges, ofcourse).",
          "score": 2,
          "created_utc": "2026-01-10 09:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrynwm",
          "author": "Mugen0815",
          "text": "If I were u, id buy a used 3090, get chat-software with memory and try a few models.\nThere are some good models out there, but none of them is gonna be as smart as gpt or gemini.",
          "score": 2,
          "created_utc": "2026-01-10 12:32:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyshg5d",
          "author": "Frequent_Depth_7139",
          "text": "hardware is the key and loacal is safer even a dumb model can have all the knowlege you need it to have and no knowlege you dont need it to have",
          "score": 2,
          "created_utc": "2026-01-10 14:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqyu5d",
          "author": "fandry96",
          "text": "Consider a new pc from costco? 90 day return policy, if it doesn't make your business money, return it. Satisfaction guarantee.\n\nChanged my world.",
          "score": 2,
          "created_utc": "2026-01-10 07:10:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nys5x4r",
          "author": "Dr_alchy",
          "text": "The investment is large. I have a small rig that can run at best 32b models and lags. This was $2k, 5 years ago, that I built myself. At this point it's only good for basic chatting.\n\nTo self host a functional llm your going to invest closer to $5k at minimum for GPU and memory, primarily.  I've thought of upgrading my system in order to replace a couple subscriptions for my team but the return isn't there in terms of capabilities of the models I can host vs the subscriptions we have",
          "score": 1,
          "created_utc": "2026-01-10 13:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt0g93",
          "author": "Your_Friendly_Nerd",
          "text": "you could give [groq.com](http://groq.com) a try. it‘s still a cloud provider, but afaik they don‘t use api usage data for training. Assuming they can be trusted, I think they‘re a good middleground between local LLM and someone like OpenAI/ Anthropic.",
          "score": 1,
          "created_utc": "2026-01-10 16:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyufhu8",
          "author": "tony10000",
          "text": "What kind of Mac and what is \"super slow\"?  A M4 Mac Mini should be pretty snappy with models up to 8-9B.  My suggestion is to use local for confidential info and the cloud for anything requiring memory retention and speed.",
          "score": 1,
          "created_utc": "2026-01-10 20:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuqiv1",
          "author": "Tema_Art_7777",
          "text": "Right except ollama or llama.cpp are setup just to respond over the network without logging your prompt. If u think network can be snooped, then u could ssl the info in. I would be comfortable with that setup",
          "score": 1,
          "created_utc": "2026-01-10 21:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nywf9vt",
          "author": "kinkvoid",
          "text": "It’s important to remember that 'Chat Apps' are full-stack products, not just raw models. They handle the complexity of context and memory management that local setups often lack. Given how centralized AI hardware is, you’d need to spend roughly $10k to get a local experience that feels 'pro.' Apps like Lumo attempt to bring 'privacy' to LLM usage, but they introduce a tradeoff: you're swapping corporate centralization for a different kind of trust in a third-party platform.",
          "score": 1,
          "created_utc": "2026-01-11 02:24:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyx38td",
              "author": "lookwatchlistenplay",
              "text": "You can vibecode your own full-stack chat app very, very easily if you have some programming experience, or hire someone to do a functional version in a month or less.\n\nWe're now past the rolling knowledge cut-off point where LLMs didn't know what \"LLM\" or \"OpenAI-compatible endpoint\" even means. Now that the latest open source models do know that, and other things we've developed around open source AI in the last 2-3 years or so, they can help you code the simplest or most complicated, feature-rich LLM \"chat\" interface you can imagine and verbalize.\n\nI believe this is partly why we're seeing cloud AI resort to dirty tricks like buying up so much of the world's RAM. They're building the moat around their business model they already knew they never had in the first place, although the moat in this case is \"hoard all the hardware so others can't own their own, making them have to rent from us\".",
              "score": 1,
              "created_utc": "2026-01-11 04:41:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyxm62u",
          "author": "nitinmms1",
          "text": "On Mac go for lmstudio and pick mlx versions of small models. 4 bit quantized and mlx models will be best. Aim for no more than 14B models",
          "score": 1,
          "created_utc": "2026-01-11 07:00:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxubvn",
          "author": "max6296",
          "text": "You're gonna need a lot of money to run local models as performant as the frontier models like chatgpt, gemini, claude, etc. Moreover, the best local models are still worse than those frontier models. $20-$30 a month is nothing, really.",
          "score": 1,
          "created_utc": "2026-01-11 08:13:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyy0aq3",
          "author": "emmettvance",
          "text": "This really depends on your use case and what you require the model for.. for small businesses, instead of buying all in one subscriptions like chatgpt plus you can use specific models that match your needs and only pay for what you actually use, dont just end up paying for tokens you didn’t use.... few models like qwen/mistral through providers like deepinfra, together or similar services charge per token, so if you’re using it constantly the costs stay lower. For sensitive data concerns definitely check their data retention policies. Local setup makes sense if you need complete airgap security or you’re using it heavily enough that API costs would exceed the hardware investment, however for a growing business the maintenance overhead of local might not be worth it yet",
          "score": 1,
          "created_utc": "2026-01-11 09:08:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyy7i3j",
          "author": "ChemistNo8486",
          "text": "I would say it is worth if, specially for a business. 2026 is when local AI is going to get closer to frontier models. Specially with the focus on software improvements over silicon.\n\nI have a 5090 and I tried nemotron-3-nano with 65K window context and it has been great for data gathering with RAG. The thinking mode helps a LOT because you can add more complex instructions, taking advantage of that context.\n\nThat said, if you have a small business, I would say that a 5090 is the way to got if you have the chance. You can set it up as a server and depending on the task, multiple people can use it.",
          "score": 1,
          "created_utc": "2026-01-11 10:15:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5spl3",
          "author": "Total-Context64",
          "text": "What are your specs?  With [SAM](https://github.com/SyntheticAutonomicMind) you can run a mix of local and remote models so you can just choose what works best for your specific task.",
          "score": 1,
          "created_utc": "2026-01-12 13:46:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6ukqy",
          "author": "SelectArrival7508",
          "text": "try https://www.privatemode.ai. it is cloud based and gives you the same level of data privacy",
          "score": 1,
          "created_utc": "2026-01-12 16:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgb5v3",
          "author": "HealthyCommunicat",
          "text": "claude code and gpt models are well above 1 trillion parameters. \n\n  \nglm 4.7 is not even on par with claude sonnet 4.5, yet u need a minimum of $5000-8000 to buy the compute for it.\n\n  \nanother brutal reality check served",
          "score": 1,
          "created_utc": "2026-01-14 00:29:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqzo5n",
          "author": "El_Danger_Badger",
          "text": "Slow but private, fast but cloud. \n\nSpeed vs altitude. ",
          "score": -1,
          "created_utc": "2026-01-10 07:18:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8q3w0",
      "title": "Scaling RAG from MVP to 15M Legal Docs – Cost & Stack Advice",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q8q3w0/scaling_rag_from_mvp_to_15m_legal_docs_cost_stack/",
      "author": "Additional-Oven4640",
      "created_utc": "2026-01-10 01:02:01",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.81,
      "text": "Hi all;\n\nWe are seeking investment for a LegalTech RAG project and need a realistic budget estimation for scaling.\n\n**The Context:**\n\n* **Target Scale:** \\~15 million text files (avg. 120k chars/file). Total \\~1.8 TB raw text.\n* **Requirement:** High precision. Must support **continuous data updates**.\n* **MVP Status:** We achieved successful results on a small scale using `gemini-embedding-001` **+** `ChromaDB`.\n\n**Questions:**\n\n1. Moving from MVP to 15 million docs: What is a realistic OpEx range (Embedding + Storage + Inference) to present to investors?\n2. Is our MVP stack scalable/cost-efficient at this magnitude?\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q8q3w0/scaling_rag_from_mvp_to_15m_legal_docs_cost_stack/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyqdrru",
          "author": "Plotozoario",
          "text": "1.8TB of data embedded is astronomical, imagine the latency when the vector engine is trying to find relative words... I don't know if RAG could be your best choice here, I can't see this scaling horizontally or even vertically in a small startup project. Unless you're a giant multinational with exclusive data centers and money to blow for five years straight like OpenAI, that dataset of yours is massive, bordering on overkill depending on what you're trying to do.\n​It’s often better to keep it simple. Instead of messing with RAG, just use Tool Calls to hit an API that already has your data.",
          "score": 3,
          "created_utc": "2026-01-10 04:31:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypnrbk",
          "author": "Icy_Builder_3469",
          "text": "The 15m docs I'm assuming are somewhat legacy. RAGing those is one problem, next problem is growth rate and change rate of existing documents. \n\nI would want those metrics (or a reasonable estimate).\n\nI'm doing some similar stuff in the real estate space, we've build our own ai servers. So I'm curious how you go.",
          "score": 2,
          "created_utc": "2026-01-10 01:58:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrg2r5",
          "author": "CiDsTaR",
          "text": "TBH, I'd change the solution to scale to that amount.... that's not a \"how to solve this with a llm\", but a complex retrieval architecture.\n\nEmbedding everything will explode cost, ChromaDB is not designed for distributed and continuously updated approach, pure vector recall is too fuzzy for legal precision and, if that's not enough, re-embedding on updates is economically infeasible and nobody won't give you money to build it...\n\nI don't have all details about doc content (client stuff vs laws etc), but If I were you I'd start redefining the solution with a hybrid approach....you can test different techniques and technologies like bm25 for legal refs, vector retrieval on chunks (not full docs), maybe a knowledge graph if you need citation, auth ranking etc...and a reranking step, using llm just to synthesize and citations?, not for recall.\n\nThat's a common approach we've used in several production ready environments and it's scaling quite well. It does not mean it will fit your use case, but will help as a starting point.\n\nPS. For your investors, you'll need to explain why a hybrid approach (keyword search, semantic search, metadata...) works, and how you are goin to control data, costs and precision.",
          "score": 2,
          "created_utc": "2026-01-10 09:50:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysjonw",
          "author": "Wide_Bag_7424",
          "text": "This is super relevant for legal RAG at scale — one of the biggest hidden risks at 15M docs is \\*\\*contradiction retrieval\\*\\* (e.g. a \"shall not\" clause ranking high because embeddings ignore negations, exceptions or role swaps). In contract review, e-discovery or compliance, that’s a liability disaster.\n\nAQEA semantic compression + targeted Lens Steering for safety:\n\n\\- \\*\\*Safety\\*\\*: HN@10 on Liability Trap diagnostic (query A must never retrieve contradiction B) → \\*\\*0.000\\*\\* (vs baseline \\~0.56). Zero false positives on inversions.\n\n\\- \\*\\*Utility\\*\\*: Retained \\~88–90% baseline performance (HitC@10 0.90–1.00, BO@10 0.30–0.40).\n\n\\- \\*\\*Storage\\*\\*: \\~\\*\\*117×\\*\\* compression (4096 bytes → 35 bytes/item with two-vector uint8).\n\nThis keeps billion-scale legal KBs affordable while slashing contradiction risks — perfect for precision-critical domains like patents, regulatory AI or bioinformatics.\n\nDo you already have any mechanisms in your stack for negation/exceptions handling? Or what vector DB/compression are you leaning towards for the 15M scale to keep costs down?",
          "score": 2,
          "created_utc": "2026-01-10 14:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypfwfx",
          "author": "Several_Witness_7194",
          "text": "!Remindme in 2 days",
          "score": 1,
          "created_utc": "2026-01-10 01:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nypg4gj",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 2 days on [**2026-01-12 01:15:35 UTC**](http://www.wolframalpha.com/input/?i=2026-01-12%2001:15:35%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLM/comments/1q8q3w0/scaling_rag_from_mvp_to_15m_legal_docs_cost_stack/nypfwfx/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLM%2Fcomments%2F1q8q3w0%2Fscaling_rag_from_mvp_to_15m_legal_docs_cost_stack%2Fnypfwfx%2F%5D%0A%0ARemindMe%21%202026-01-12%2001%3A15%3A35%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q8q3w0)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-10 01:16:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nypvt8b",
          "author": "irodov4030",
          "text": "Are you using cloud API for legal docs?\n\n  \nare these laws and regulations or client documents?",
          "score": 1,
          "created_utc": "2026-01-10 02:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyraany",
          "author": "htaidirt",
          "text": "Going from MVP to 15M docs isn’t linear in complexity. As a first step (legit for fundraising and post it) is to reduce the data size to only necessary documents. I’m pretty sure not all 15M are unique, so what is the minimum you can extract that will be enough for the model to extract patterns and customize accordingly.",
          "score": 1,
          "created_utc": "2026-01-10 08:55:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrvcox",
          "author": "Exciting_Narwhal_987",
          "text": "I don’t think you are ready to face the challenges.\n\nI faced a lot of hurdle on such size of data. It took a lot of time for me to figure out.\n\n\nActually, You have to build a lot of layers to it if you really want to get a tangible answers to a quarry.",
          "score": 1,
          "created_utc": "2026-01-10 12:06:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyseq54",
          "author": "WayNew2020",
          "text": "First, if I were to invest in your project, I'd reject your MVP bc of ChromaDB. Consider using a realistic tool like ClickHouse so that your MVP scales and \"proves\" it can handle 15M, 1.8TB documents in production.\n\nSecond, are you self-hosting your solution on-premise or thinking of managed cloud service? Between the two, the managed service would come cheaper overall due to competitive pricing on cloud and additional maintenance cost (and pain) for self-hosting. But to get a good estimate, you MUST decide on your RAG method.\n\nThird, how much of fine tuning have you done in your MVP? I assume foundation models wouldn't be sufficient for LEGAL context.  I'd expect that to be part of your estimate equation since it's NOT a one-shot tuning and bang, it lasts forever kinda solution, is it.\n\nPresenting your case with your current MVP makes you look like you don't know what you are doing. Sorry sounding harsh, but we're talking business, right? Good luck!",
          "score": 1,
          "created_utc": "2026-01-10 14:15:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypgowe",
          "author": "vinoonovino26",
          "text": "Hyperlink.ai did wonders for me",
          "score": 0,
          "created_utc": "2026-01-10 01:19:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0j55m",
      "title": "OpenCV 4.13 brings more AVX-512 usage, CUDA 13 support, many other new features",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/OpenCV-4.13-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2025-12-31 17:56:27",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0j55m/opencv_413_brings_more_avx512_usage_cuda_13/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qa0ore",
      "title": "Is GLM 4.7 really the #1 open source coding model?",
      "subreddit": "LocalLLM",
      "url": "/r/Anannas/comments/1qa0nme/is_glm_47_really_the_1_open_source_coding_model/",
      "author": "HuckleberryEntire699",
      "created_utc": "2026-01-11 14:28:45",
      "score": 12,
      "num_comments": 6,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qa0ore/is_glm_47_really_the_1_open_source_coding_model/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nyzc1rx",
          "author": "kweglinski",
          "text": "haven't done comparison but it is actually solid. I'm using claude at work and glm4.7 at home and while claude is (sadly) better, the glm is really capable. I'd say glm requires a bit more programming skill from user than the claude but in good hands both work well.",
          "score": 4,
          "created_utc": "2026-01-11 15:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz60o8",
          "author": "kinkvoid",
          "text": "It's hard to say. I use for RP and it's good. Kimi k2 is on par with GLM 4.7",
          "score": 3,
          "created_utc": "2026-01-11 14:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzpwo3",
              "author": "eli_pizza",
              "text": "K2 is good but it uses a lot of tokens for reasoning. And maybe it could be fixed with better prompting but I think it sucks for code review.",
              "score": 2,
              "created_utc": "2026-01-11 16:13:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzpm9a",
          "author": "eli_pizza",
          "text": "Try it and see? It’s incredibly inexpensive and there’s no one single “best”. It’s definitely better at coding than 4.6 which was already decent. \n\nMiniMax is also quite good and DeepSeek 3.2 seems great at code review.",
          "score": 2,
          "created_utc": "2026-01-11 16:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1jxe1",
          "author": "Consistent_Wash_276",
          "text": "I’ve only compared it to minimax 2.1 both locally and the free servers running them in opencode. \n\nIt’s good. Minimax 2.1 is pretty decent as well. \n\nRemind you I’m on Apple Silicon and not CUDA cores so do with that what you will",
          "score": 2,
          "created_utc": "2026-01-11 21:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzj2ql",
          "author": "tamerlanOne",
          "text": "Opus 4.5 is almost no brainer even for many users with basic programming knowledge, for glm 4.7 you need to have a good basic programming knowledge to use it at its best",
          "score": 1,
          "created_utc": "2026-01-11 15:41:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9a4bf",
      "title": "Where to buy used gear?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q9a4bf/where_to_buy_used_gear/",
      "author": "wizoneway",
      "created_utc": "2026-01-10 17:33:52",
      "score": 12,
      "num_comments": 6,
      "upvote_ratio": 0.94,
      "text": "Ebay listing for rtx6000 half price with accounts created in November. Is ebay safe? How do people not get scammed buying used gear now a days? Are there other sites that are reputable?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q9a4bf/where_to_buy_used_gear/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nytmld8",
          "author": "Herr_Drosselmeyer",
          "text": "An RTX 6000 Pro for half price is always a scam.",
          "score": 17,
          "created_utc": "2026-01-10 17:55:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytqkgd",
              "author": "HumanDrone8721",
              "text": "This is true, I would dare to say that even 80% is still 90% a scam, there is absolutely no reason for some to sell such a new card and the tired canard \"tee hee, I'm a Barbie Doll and got this for a present and I don't know what to do with it, or even what is it, so I'm selling it because I need money...\" is way too tired, I think the current economy made suckers with money to spend an endangered species.",
              "score": 4,
              "created_utc": "2026-01-10 18:13:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nytnkxm",
          "author": "HumanDrone8721",
          "text": "Usually it helps specifying in which county you are, because besides EBAY, that is kind of a worldwide site, there are a lot of country specific platforms and users in those countries can tell their experience, for example Kleinanzeigen is the king in Germany, mostly for p2p deals but also b2p, Craiglist and Facebook Marketplace are monsters in US but not so much in EU and so on.\n\nRegarding EBAY in the EU you're quite well protected, even if the seller says no returns, you can return what you've ordered online in 14 days (you may have to pay shipping). In US I've heard (ot sure) that they even introduced \"no buyer protection offers\" but I have no practical experience with it.",
          "score": 4,
          "created_utc": "2026-01-10 18:00:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyu2g4r",
              "author": "fallingdowndizzyvr",
              "text": "> In US I've heard (ot sure) that they even introduced \"no buyer protection offers\" but I have no practical experience with it.\n\nIn the US, you get a 30 day \"not as described\" return option no matter what the seller says.",
              "score": 4,
              "created_utc": "2026-01-10 19:09:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyx8e71",
          "author": "No-Consequence-1779",
          "text": "If it’s too cheap. No.  If the account has not sold anything before.  No.  Plan to pay the market rate.  You can not scam an honest person :) \n\nFacebook marketplace can save you sales tax. Usually hundreds. But you’ll need to verify it works. If they want to meet someplace where you can’t find them again, don’t do it.  ",
          "score": 2,
          "created_utc": "2026-01-11 05:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3dnq9",
          "author": "Educational-World678",
          "text": "Every once in a while someone gets a letter with a photo of the thing they were trying to buy... It's common enough that bored journalists can regularly find examples, but rare enough that it feels like news when it happens, if that makes sense. Also, eBay+PayPal are very good about protecting buyers when variffiable fraud has been committed.",
          "score": 2,
          "created_utc": "2026-01-12 02:47:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q87tcs",
      "title": "Total beginner trying to understand",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q87tcs/total_beginner_trying_to_understand/",
      "author": "commissarisgay",
      "created_utc": "2026-01-09 13:09:00",
      "score": 12,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "Hi all,\n\nFirst, sorry mods if this breaks any rules!\n\nI’m a total beginner with zero tech experience. No Python, no AI setup knowledge, basically starting from scratch. I've been using ChatGPT for a long term writing project, but the issues with its context memory are really a problem for me.\n\nFor context, I'm working on a long-term writing project (fiction).\n\nWhen I expressed the difficulties I was having to ChatGPT, it suggested I run a local LLM such as Llama 13B with a 'RAG', and when I said I wanted human input on this it suggested I try reddit.\n\nWhat I want it to do:\n\nRemember everything I tell it: worldbuilding details, character info, minor plot points, themes, tone, lore, etc.\n\nAnswer extremely specific questions like, “What was the eye colour of \\[character I mentioned offhandedly two months ago\\]?”\n\nAct as a persistent writing assistant/editor, prioritising memory and context over prose generation. To specify, I want it to be a memory bank and editor, not prose writer.\n\nMy hardware:\n\nCPU: AMD Ryzen 7 8845HS, 16 cores @ \\~3.8GHz\n\nRAM: 32GB\n\nGPU: NVIDIA RTX 4070 Laptop GPU, 8GB dedicated VRAM (24GB display, 16GB shared if this matters)\n\nOS: Windows 11\n\nQuestions:\n\nIs this setup actually possible at all with current tech (really sorry if this is a dumb question!); that is, a model with persistent memory that remembers my world?\n\nCan my hardware realistically handle it or anything close?\n\nAny beginner-friendly advice or workflows for getting started?\n\nI’d really appreciate any guidance or links to tutorials suitable for a total beginner.\n\nThanks so much!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q87tcs/total_beginner_trying_to_understand/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nylbzs3",
          "author": "Ok_Stranger_8626",
          "text": "It's a bit of a complex question to answer. Here's my take from the standpoint of someone who builds custom AI solutions:\n\n1. Yes, you can run a 13B model on your GPU, with a few caveats;\n A. You will need a deeply quantized model, due to low dedicated VRAM(your shared RAM is for the iGPU, and won't really help you here.) \n B. Be prepared for hallucinations a lot of times. (Deeply quantized models lose a lot of mathematical precision, causing them to often pick bad tokens to return, and this will cascade, no ECC VRAM, which means a single bit-flip will cascade through the rest of the returned tokens and then you just get word salad.)\n2. For your RAG, an \"in-memory\"  K-V database would be best for your assistant's \"memory\". If you can get QDrant running locally, it's the best way to go. And drop the huge allocation to your iGPU, it's not doing you any favors.\n3. You'll need something to front this and tie it together, like Open-WebUI. There's going to be a good deal of setup.\n\nIf you want something good and local, it's going to cost. Your setup would work, but I don't think it would really perform as well as you'd like.\n\nUsing a Cloud AI such as ChatGPT/Llama/etc seems cheap because most big AI vendors are still taking massive losses due to investment capital. Local AI seems expensive in comparison because AI in fact ISN'T cheap, but \"the big kids\" have money behind them, which allows them to mask the true costs.",
          "score": 6,
          "created_utc": "2026-01-09 13:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymg1wi",
          "author": "NobleKale",
          "text": "> Answer extremely specific questions like, “What was the eye colour of [character I mentioned offhandedly two months ago]?”\n\nI'm going to set your expectations, brutally.\n\nThis is a wild moonshot with what you have, and, frankly, what's available right now.\n\nRAG is nice, but it's not the silver bullet that a lot of LLM-pushing folks were pretending it was (it was the 'answer to everything' about six months ago).\n\nAll RAG is:\n\n* You put in your prompt\n* Your prompt is turned into maths\n* Your prompt's maths is compared to a list of documents, and the maths that most closely matches your prompt is then snipped out and handed over to the model\n* Your system prompt + your prompt + your RAG results -> get fed into the model to get your answer.\n\nSo, if you say 'what's the colour of Wyvern's eyes?' - it's going to look for whatever sentences, paragraphs, etc in your draft are CLOSEST to 'colour', 'eyes', 'Wyvern', 'what's the'.\n\nIn other words, 'What's the colour of Wyvern's scales?' is pretty close, so the RAG result may pull that. You may also get 'What's the colour of Sweet-thing's eyes?' because, yeah. Close.\n\nOh, you mentioned 'eyes' a lot in your book? Guess you're *shit out of luck*.\n\nThis isn't to say 'don't do this', this is me - who has worked on a LOT of stuff for custom fiction writing - saying 'set your expectations low... lower... lower than that'.\n\n> 8GB dedicated VRAM\n\nI know 13B's were mentioned. Honestly, if you want something decent for decent speed, a 7b is prob. the best you're going to get nicely here.\n\n7b is '7 billion parameters', it's a metric of how big (complex) the model is. It's not how *good* it is, that's going to depend on topics, data it got fed, etc, but rather, how big its capacity for storing more and MOAR stuff in it is.\n\nSo, time for some good news? Yes? Ok then!\n\n* Get KoboldCPP - this is the simplest (IMHO) way to get a model running. One fucking .exe file, and off you go\n* Get yourself a model - your mileage may vary, models are like toothpaste. No one cares, which one you use, they'll have evangelists just use one. I personally use my own bullshit ones, and, Sultry Silicon v2 (I like v2 over v3). Yes, it's a lewd one.\n* Get SillyTavern\n\n* SillyTavern's ui -> lorebooks. You can dump shit in here, it's not the best, but basically it says 'you mentioned THIS keyword, I'll just ADD THIS TEXT', so if you have a planet called Grassfucker, and a lorebook entry for Grassfucker, any time your prompt says that word, ST adds the lorebook entry to your prompt.\n\nBUT, this obviously eats your context, so it means your model 'remembers' less conversation as you go. \n\nAfter you get these two things set up, dork around with them for a while, you'll start going 'well, this is nice, BUT...' and start making either your own clients, or your own datasets + models. Slippery slopes, and all that.\n\nFeel free to ask a few more questions, but ultimately: you gotta pick shit up and play with it.",
          "score": 5,
          "created_utc": "2026-01-09 16:41:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylts7q",
          "author": "Ok_Rough5794",
          "text": "Obsidian and any of their LLM plugins will do this for you, plus there are lots of novel writing people in the community, and the Longform plugin which could be helpful.\n\nSo will Claude Code if you have your files in a readable format on your hard drive.",
          "score": 3,
          "created_utc": "2026-01-09 14:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyw8ndj",
          "author": "Excellent_Spell1677",
          "text": "realistically...very tough.  Local models can't really remember as much as the paid cloud models.  More of a groundhog day each conversation so you should have a feeder document with all pertinent details but its really not possible with your hardware.  The best thing I could suggest would be something like google's Notebook LM.  You could load your chapters as you go and it would be able to be a \"specialist\" on just what you load.  Pay the $20 a month and get a higher document limit to upload.  \n\nEven with a super DGX spark or AI purpose built rig you would not be able to match notebook lm locally.  Best of luck.",
          "score": 2,
          "created_utc": "2026-01-11 01:48:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymlscl",
          "author": "DHFranklin",
          "text": "I think I can help a rookie that is just trying to QnA a book.\n\nYou want google AI Studio. It's free. Try Gemini 3 pro\n\n1) Put the entire corpus and everything you've got into it.\n\n2) Tell it what you want it to do. Expectations and outputs. What you want it to act as. That is a \"custom instruction\". Make sure to tell it you are trying to organize it to avoid \"context rot\" or \"context bleed\". I can can can on a tin can. I went to the zoo and saw North American Bison get pent up with others from upstate New York and immediately start rough housing. Don't do that because lemme tell ya Buffalo Buffalo Buffalo Buffalo Buffalo.\n\n3) Then ask it to organize the information you have and make RAG chunks. You can get the output as plain text or JSON. you're going to want to split test it.\n\n4) Ask it for \"10 clarifying questions\" so it doesn't get hung up. Then ask it to *do all of that over again*. It's a novel so token windows in the hundreds of thousands will help.\n\nNow ask it what color eyes Wyvern has.\n\nWhen you're done you can make an LLM editor to compare what you're writing to the \"word of god\" story bible. Pretty useful to stay consistent.",
          "score": 2,
          "created_utc": "2026-01-09 17:06:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyy1bff",
              "author": "Fuzzy_Independent241",
              "text": "Same suggestion here - Google NotebookLM, is part of AI Studio and I think that's what OC was referring to. \nSetting up a local RAG that would do what OP wants is not a beginner project. \nSince you have an nVidia board, they just released a document search tool called \"Hyperlink\". It's free and has a self-install. \nIt might work as long as your documents are in one of it's free supported formats. It reads MD, DocX, TXT and PDF, if I remember correctly. \nTry it, it's free and run local models internally.",
              "score": 1,
              "created_utc": "2026-01-11 09:17:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyz4hqb",
              "author": "QuinQuix",
              "text": "I'm thoroughly confused by the tin can bison buffalo bit.",
              "score": 1,
              "created_utc": "2026-01-11 14:25:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz0c2sb",
                  "author": "DHFranklin",
                  "text": "Homophones. LLMs are really bad at them. can can is a dance. I can can can. I'm pretty saucy. Buffalo has tons of homophones. The city in upstate New York must lead to tons of confusion for American Bison afficianados.\n\nWhen making resources for LLMs this is a huge hassle. It's not impossible but if you're going to have a character have a Macguffin in his pocket in Chapter 10 you should make sure that it's spelled out for being important.\n\nTo say that \"the Captain looked at his watch\" You might trip up an LLM prompt a million tokens long. The captain has a wrist watch or the Captain has subordinates that are performing recon observation?",
                  "score": 2,
                  "created_utc": "2026-01-11 17:58:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylpkz9",
          "author": "HonestoJago",
          "text": "Can’t you do a Claude/GPT project and just keep adding to and revising the project knowledge?",
          "score": 1,
          "created_utc": "2026-01-09 14:39:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nysxehb",
              "author": "Early_Interest_5768",
              "text": "Privacy",
              "score": 1,
              "created_utc": "2026-01-10 15:55:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz8w5qq",
          "author": "Suitable-Program-181",
          "text": "Listen, you can go with Cuda and llama or boost that AMD and igpu first.\n\nResearch AMD zero copy.\n\nBrother, dgpu biggest issue is pcl latency and igpu is literally inside cpu, but people love to forget it.\n\nVega is extremely powerful, with custom kernels bypassing CUDA/rocm (i have a gtx 1650) I get high gflops surpassing theoretical just cause rocm, cuda all is shit and people beleive is gold standard.\n\nI use IGPU zero latency as the core , cpu is efficient, i dont let dgpu do tasks I can do faster with cpu or slower but cheaper with igpu cause remember, is also about energy and thermals. Moving electrons causes friction, when you dont move shit theres no friction and since igpu has direct access to ram, well you can do the math or get creative as me.",
          "score": 1,
          "created_utc": "2026-01-12 22:32:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nym052n",
          "author": "dual-moon",
          "text": "this is a BIG task, but we do have a reference implementation of this setup her: [https://github.com/luna-system/ada/](https://github.com/luna-system/ada/) \\- we're working on v4.0, but 3.0 has everything you need. try cloning and having ur MI friend look at it with you!\n\nedit: you don't need a BIG model, small models seem to be the future!",
          "score": 1,
          "created_utc": "2026-01-09 15:29:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}