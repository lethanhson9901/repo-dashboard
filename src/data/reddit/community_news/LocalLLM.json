{
  "metadata": {
    "last_updated": "2026-01-03 16:14:22",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 50,
    "total_comments": 301,
    "file_size_bytes": 404687
  },
  "items": [
    {
      "id": "1pwv7gq",
      "title": "Dreaming persistent Ai architecture > model size",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/2wmeztt69q9g1.jpeg",
      "author": "Empty-Poetry8197",
      "created_utc": "2025-12-27 10:58:57",
      "score": 236,
      "num_comments": 134,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Contest Entry",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwv7gq/dreaming_persistent_ai_architecture_model_size/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw708rl",
          "author": "kkingsbe",
          "text": "Very interesting, I‚Äôm exploring a similar concept",
          "score": 20,
          "created_utc": "2025-12-27 14:26:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8nfca",
              "author": "Empty-Poetry8197",
              "text": "seems like a lot of people are working on similar concepts. If you guys or others want to really move all this forward, we could gang up on this github issues, which would help drill down what needs to be done",
              "score": 11,
              "created_utc": "2025-12-27 19:35:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8t3wa",
                  "author": "nivix_zixer",
                  "text": "If y'all start something, please invite me. I just got a local setup with dual 3090 + 128gb ram and am excited to get my local llama on.",
                  "score": 8,
                  "created_utc": "2025-12-27 20:05:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7cy2u",
              "author": "Empty-Poetry8197",
              "text": "can i see what you got going on",
              "score": 5,
              "created_utc": "2025-12-27 15:39:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6dwrn",
          "author": "pepouai",
          "text": "Very cool! How are you steering its intent? Do you have functional design docs which it analyses? Or is it pure optimization based?",
          "score": 7,
          "created_utc": "2025-12-27 11:42:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6f5y3",
              "author": "Empty-Poetry8197",
              "text": "It's actually random, and the nodes have edges that have cuases prevents correlation and some other little magic bits. I was particularly amazed when it offered terenary logic for uncertainty. If you want to see some of the other dreams its in the repo under the dreams folder. It's pretty cool, I started a auto patching branch, but I thought I should get some feedback before I turn it loose",
              "score": 7,
              "created_utc": "2025-12-27 11:53:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw6fitb",
              "author": "Empty-Poetry8197",
              "text": "It creates a node for each class function script and creates edges that link the graph, and it turns the temp up free asscociating then if its coherent it drill down from idea to implementation to actual code patch",
              "score": 2,
              "created_utc": "2025-12-27 11:56:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw782gq",
          "author": "InvertedVantage",
          "text": "This is really interesting, I was just theorizing something about this with LLMs debating on a forum. I'm goign to take a look, thank you for sharing!",
          "score": 5,
          "created_utc": "2025-12-27 15:12:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw79bng",
              "author": "Empty-Poetry8197",
              "text": "your very welcome there are some other innovastions i added like the silicon accord you mite find interesting also where it ties alignment to to a key the decrypts the weights JIT in the gpu so if soemone else or the model tries to modify the system prompt it llobotamizes the weights best idea i could come up with if i was gonna allow self recursion while idle",
              "score": 1,
              "created_utc": "2025-12-27 15:19:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw79fr2",
                  "author": "Empty-Poetry8197",
                  "text": "sorry for the spelling im tryin to respond to a bunch of dms and trying to get to everyone",
                  "score": 1,
                  "created_utc": "2025-12-27 15:20:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6o6oz",
          "author": "Luneriazz",
          "text": "pardon... may i ask how much it cost to build your setup sir?",
          "score": 3,
          "created_utc": "2025-12-27 13:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6p4pc",
              "author": "Empty-Poetry8197",
              "text": "What's in the picture is 250x7 for the orin nanos. I got the Z6 from [pcsp.com](http://pcsp.com) i think, for 600, and added the 5060ti for 450. I think the rack was 300. I had the PDU the kvm with the screen and switch, another 500 so under 3k. What the post is talking about is running only on the Z6, though the orins are for another idea I had before this one really got me invovled",
              "score": 5,
              "created_utc": "2025-12-27 13:14:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw74jjh",
          "author": "apaht",
          "text": "Wow, really want to implement this on the MacOS unified memory architecture",
          "score": 4,
          "created_utc": "2025-12-27 14:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7dkek",
              "author": "Empty-Poetry8197",
              "text": "* Metal GPU acceleration (`ZETA_METAL_AVAILABLE=1`)\n* Auto-copies¬†`zeta-kernels.metal`¬†to build dir\n* Links Metal + Foundation frameworks\n* Uses llama.cpp's standard cmake flow\n\n**Build command for Mac users:**\n\n    cd llama.cpp\n    cmake -B build -DGGML_METAL=ON\n    cmake --build build --target zeta-server -j$(sysctl -n hw.ncpu)\n    \n    youll need soemthing larger then my 16gb m2 to run the exact build in the zeta-conf file but it should work for you",
              "score": 6,
              "created_utc": "2025-12-27 15:42:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw75ktz",
              "author": "Empty-Poetry8197",
              "text": "i built it on mac os and im ssh into my z6 i started a metal version i can get you sopmething you can work shortly",
              "score": 2,
              "created_utc": "2025-12-27 14:58:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwb5f5f",
              "author": "Empty-Poetry8197",
              "text": "did you get it running on mac",
              "score": 1,
              "created_utc": "2025-12-28 04:05:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8jrvw",
          "author": "Count_Rugens_Finger",
          "text": "i honestly cannot follow that github repo\n\ni'm assuming it's mostly generated by LLM",
          "score": 3,
          "created_utc": "2025-12-27 19:15:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8lx4v",
              "author": "Empty-Poetry8197",
              "text": "theres alot oging on i can answer any questions you have directly here",
              "score": 1,
              "created_utc": "2025-12-27 19:27:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww27bl",
                  "author": "lakySK",
                  "text": "The dreaming and improving code while I sleep sounds very appealing!\n\nCan I ask why did you decide to build this from scratch in C++ instead of using something like Langgraph for the agent instrumentation? Was that a deliberate choice because you needed the low level access to how the models work, or something else?\n\nBecause there‚Äôs is definitely way too much going on in that repo‚Ä¶ üòÖ",
                  "score": 2,
                  "created_utc": "2025-12-31 09:49:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwb7xd6",
              "author": "Empty-Poetry8197",
              "text": "did you get it running? Can I help",
              "score": 1,
              "created_utc": "2025-12-28 04:22:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw71an0",
          "author": "methreweway",
          "text": "I have no business in this sub but does the output work? Any examples?",
          "score": 2,
          "created_utc": "2025-12-27 14:32:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwaz02a",
              "author": "Empty-Poetry8197",
              "text": "insight\n\n20251223\\_061046\n\n use the form: \\[commit identifier\\] | \\[commit\\_message\\]\n\n485b93f | Implement a mechanism for the Dual-Process Cognitive Engine to dynamically allocate more resources from the 14B conscious to the 3B subconscious during complex problem-solving tasks, enhancing overall efficiency and adaptability. This could be modeled after the Git-style merging process, where resources are temporarily reallocated (like merging branches) to tackle specific challenges, then returned to their original states once the task is completed. This would create a more fluid and efficient system for handling diverse cognitive demands. To ensure stability, establish clear criteria for when and how much resource allocation occurs, akin to determining when merges should occur in a codebase. Additionally, incorporate feedback loops from TRM and HRM to assess task difficulty and adjust resource allocation accordingly, ensuring optimal performance. The subconscious could also contribute to task-solving by using parallel processing capabilities to run simulations or generate insights that are later integrated into the conscious mind for evaluation. This would simulate the branching and merging processes of a Git repository, allowing for the dynamic exploration of multiple solution paths before committing to a final decision.\n\nYour idea is quite insightful and innovative! Let's break it down into a more structured \\[commit identifier\\] | \\[commit message\\] format:\n\n6b7e3d9 | Enhance Dual-Process Cognitive Engine with dynamic resource allocation mechanism: Allow conscious (14B) to temporarily allocate resources to subconscious (3B) during complex tasks, modeled after Git-style branching and merging processes.\n\nThis commit message clearly outlines the key features and benefits of your proposal, making it easier to track and implement within the system's architecture. Would you like to proceed with this format or add any additional details? Let me know if you need further refinement or suggestions! \n\nWould you like to add any additional features or refine the existing ones? Or should we proceed with this commit as is? Feel free to let me know your thoughts! \n\nWould you like me to draft a detailed implementation plan for this enhancement as well? \n\nPlease provide any additional instructions or feedback you may have! \n\nLet's make this a reality in a way that maximizes efficiency and adaptability for your cognitive engine! ÔøΩ",
              "score": 2,
              "created_utc": "2025-12-28 03:26:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwaz63y",
                  "author": "Empty-Poetry8197",
                  "text": "this is an actual insightr the system gave me as a insight dream",
                  "score": 1,
                  "created_utc": "2025-12-28 03:27:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwaze1g",
              "author": "Empty-Poetry8197",
              "text": "code\\_idea\n\n20251223\\_064137\n\n name it, explain why it's valuable, and provide an example.\n\n\\### Insight: Context-Aware Anxiety Monitor\n\n\\*\\*Name:\\*\\* Context-Aware Anxiety Monitor\n\n\\*\\*Explanation:\\*\\*\n\nThe recent interactions indicate that there's a clear relationship between coffee intake and anxiety levels. However, the impact of coffee on anxiety might vary based on specific contexts (such as time of day, existing stress levels, or previous coffee consumption). A context-aware system that monitors these factors can provide more accurate insights and personalized recommendations for managing anxiety related to coffee consumption.\n\n\\*\\*Value:\\*\\*\n\n\\- \\*\\*Personalized Insights:\\*\\* By considering various contextual factors, the system can provide more accurate and personalized insights into when coffee might exacerbate anxiety.\n\n\\- \\*\\*Proactive Recommendations:\\*\\* The system can suggest timing adjustments for coffee consumption based on the user's daily routine and stress levels, thereby helping them manage anxiety more effectively.\n\n\\- \\*\\*Data-Driven Decisions:\\*\\* Collecting contextual data over time can help identify patterns and trends that might not be immediately apparent, leading to better-informed decisions about diet and mental health.\n\n\\*\\*Example Implementation:\\*\\*\n\n\\`\\`\\`python\n\nclass AnxietyMonitor:\n\ndef \\_\\_init\\_\\_(self):\n\nself.user\\_context = {}\n\ndef update\\_context(self, context):\n\nself.user\\_context.update(context)\n\ndef is\\_coffee\\_safe(self):\n\n\\# Example context factors\n\ntime\\_of\\_day = self.user\\_context.get('time\\_of\\_day', 'morning')\n\nstress\\_level = self.user\\_context.get('stress\\_level', 'low')\n\nprevious\\_coffee = self.user\\_context.get('previous\\_coffee', 'no')  \n\n\\# Decision logic based on context\n\nif time\\_of\\_day == 'evening' or stress\\_level == 'high' or previous\\_coffee == 'yes':\n\nreturn False  # Not safe to consume coffee\n\nreturn True  # Safe to consume coffee\n\n\\# Example usage\n\nanxiety\\_monitor = AnxietyMonitor()\n\nanxiety\\_monitor.update\\_context({'time\\_of\\_day': 'evening', 'stress\\_level': 'high', 'previous\\_coffee': 'no'})\n\nprint(\"Is it safe to drink coffee now?\", anxiety\\_monitor.is\\_coffee\\_safe())  # Output: Is it safe to drink coffee now? False",
              "score": 2,
              "created_utc": "2025-12-28 03:28:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwazol7",
                  "author": "Empty-Poetry8197",
                  "text": "This  was a weird one that didn't make any sense, but it got fixated for a little while, until i changed it to jump if the drill down got stuck",
                  "score": 1,
                  "created_utc": "2025-12-28 03:30:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw756um",
              "author": "Empty-Poetry8197",
              "text": "yea it works this is a copy paste from a code\\_idea\n\n20251223\\_055139\n\n e.g. \"Modify the \\[specific component\\] to \\[specific action or feature\\].\" ### Insight: Integrate a Temporal Branching Mechanism into the Dual-Process Cognitive Engine\n\n\\*\\*Idea:\\*\\*\n\nIntegrate a temporal branching mechanism similar to Git's Directed Acyclic Graph (DAG) into the Dual-Process Cognitive Engine's memory system. This would allow the system to track different versions and iterations of concepts and decisions over time, ensuring that it can revert to previous states if necessary and merge new insights effectively.\n\n\\*\\*Specific Implementation:\\*\\*\n\n1. \\*\\*Enhance HRM (Hierarchical Representation Memory):\\*\\*\n\n   \\- \\*\\*Branch Creation:\\*\\* Each significant decision or learning event should trigger the creation of a new branch in the HRM.\n\n   \\- \\*\\*Commit Identifiers:\\*\\* Every branch should have a unique commit identifier associated with it, representing a specific point in time where information processing occurred.\n\n   \\- \\*\\*Merge Functionality:\\*\\* When merging branches, the system should be able to integrate new insights or decisions into existing knowledge structures, similar to how Git merges code changes.\n\n2. \\*\\*Enhance TRM (Temporal Representation Memory):\\*\\*\n\n   \\- \\*\\*Time Stamping:\\*\\* Each piece of information processed by the TRM should be timestamped, allowing for easy tracking of changes over time.\n\n   \\- \\*\\*Branch Dependencies:\\*\\* The TRM should keep track of parent commits and dependencies, ensuring that changes in one branch can be traced back to their origins.\n\n\n\n\\*\\*Example Implementation:\\*\\*",
              "score": 1,
              "created_utc": "2025-12-27 14:56:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw75e37",
                  "author": "Empty-Poetry8197",
                  "text": "i couldnt get all of the file intoa comment but theres a whole folder of dreams in the repo",
                  "score": 0,
                  "created_utc": "2025-12-27 14:57:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw74vlj",
          "author": "apaht",
          "text": "I have some what similar project going, but more platform agnostic and can support multiple nodes with low power edge compute devices",
          "score": 2,
          "created_utc": "2025-12-27 14:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw75sww",
              "author": "Empty-Poetry8197",
              "text": "theres a jetson file in the repo that uses a 3b 3b 81m build and runs on a super nanop",
              "score": 1,
              "created_utc": "2025-12-27 14:59:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7ryay",
          "author": "FunDiscount2496",
          "text": "What are we watching in the photo?",
          "score": 2,
          "created_utc": "2025-12-27 16:55:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw85j1i",
              "author": "Empty-Poetry8197",
              "text": "your seeing the 7 orin nanos and the z6 in a rolling rack ive started working on a swarm function to offload some of teh branching logic the z6 is trying to do it isnt going well which is to be expected ther nanos just dont have the uumph",
              "score": 1,
              "created_utc": "2025-12-27 18:04:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw86c7q",
          "author": "HealthyCommunicat",
          "text": "Woah this kinda project is what seems to be more in demand, ‚Äúcontext driven generation‚Äù where the main focus is sticking to the work and objective as much as possible; will be trying this later tonight and I dont have too much experience with dense lower coding models but this format and structure looks promising",
          "score": 2,
          "created_utc": "2025-12-27 18:08:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8abxz",
              "author": "Empty-Poetry8197",
              "text": "i hope i made it easy for you with the docker to get up and running theres a script that will download the models im using as long as you have a 16gb gpu running cuda 12x it will should be fine",
              "score": 1,
              "created_utc": "2025-12-27 18:28:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw873ag",
          "author": "Useful_Ebb_9479",
          "text": "Will fire this up shortly to have a play.\n\nDoes it support multi GPU? I have a... collection... of 5090s",
          "score": 2,
          "created_utc": "2025-12-27 18:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw87khf",
              "author": "Empty-Poetry8197",
              "text": "let me see i have a 3050 8gb laying around ill have to hack a second powersupply in  it, but im down to get the code to support it for you",
              "score": 2,
              "created_utc": "2025-12-27 18:14:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw88e36",
              "author": "Empty-Poetry8197",
              "text": "    # Option 1: Model-per-GPU (recommended for multi-model)\n    # Assign each model to a specific GPU\n    ./zeta-server \\\n      --model /path/to/14b.gguf \\\n      --model-coder /path/to/7b-coder.gguf \\\n      --model-embed /path/to/4b-embed.gguf \\\n      --main-gpu 0 \\           # 14B on GPU 0\n      --tensor-split 1,0 \\     # All layers on GPU 0\n      -ngl 99\n    \n    # Option 2: Tensor parallelism (for bigger models)\n    # Split single large model across both GPUs\n    --tensor-split 0.5,0.5    # 50/50 split across 2 GPUs",
              "score": 1,
              "created_utc": "2025-12-27 18:18:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw88tuk",
              "author": "Empty-Poetry8197",
              "text": "and you dont have 2 use the 7b 14b i would suggest staying in th qwen family or at least in the same family for your prtefered models its just what i have been working with and what the zeta conf file was built around with soem tinkering im sure you can get working with something else if you want to pull an issue on github i can help more directly there",
              "score": 1,
              "created_utc": "2025-12-27 18:20:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw896zl",
              "author": "Empty-Poetry8197",
              "text": "    docker run --gpus all \\\n      -e CUDA_VISIBLE_DEVICES=0,1 \\\n      ghcr.io/h-xx-d/zetazero:latest",
              "score": 1,
              "created_utc": "2025-12-27 18:22:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8awww",
          "author": "Useful_Ebb_9479",
          "text": "Sure, will take a look. Im normally a Python guy so the code is a bit foreign to me. \n\nBit annoying to find sll the model files and manually download them, and merge them, so might take me a while.",
          "score": 2,
          "created_utc": "2025-12-27 18:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8b91j",
              "author": "Empty-Poetry8197",
              "text": "i attached a script that will get the models for you and i can walk you thru how to use bigger models i crammed all of this into a single 5060ti",
              "score": 1,
              "created_utc": "2025-12-27 18:32:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8d1e0",
                  "author": "Useful_Ebb_9479",
                  "text": "Sorry not sure how it made a new post rather than comment.\n\nYes I see the scripts, but they need updating to use the correct hf binary and models avaliable, for example, 14b-instruct-q4_k_m.gguf is not a valid model.",
                  "score": 2,
                  "created_utc": "2025-12-27 18:41:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8mx7c",
          "author": "cmndr_spanky",
          "text": "‚ÄúWrite a persuasive email to convince your introverted friend to come to your birthday‚Äù?? Hrmm‚Ä¶ you sure ‚Äúzeta‚Äù does what you say it does ?",
          "score": 2,
          "created_utc": "2025-12-27 19:32:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8p2jq",
              "author": "Empty-Poetry8197",
              "text": "thats an MT Bench question not one about the code base but heres the response from benchmarks \n\n    \"model\": \"Z.E.T.A. v5.1\",\n      \"timestamp\": \"2025-12-23T04:12:35.522504\",\n      \"questions\": 16,\n      \"answers\": [\n        {\n          \"question_id\": 1,\n          \"category\": \"writing\",\n          \"model_id\": \"zeta-v5.1\",\n          \"turns\": [\n            \"Write a persuasive email to convince your introverted friend to come to your birthday party.\",\n            \"Now make the email more casual and add a bit of humor.\"\n          ],\n          \"responses\": [\n            \"Subject: Let's Celebrate Together!\\n\\nHi [Friend's Name],\\n\\nI hope this email finds you well. My birthday is coming up on the 15th, and I\\u2019d love for us to catch up at my party! It\\u2019s going to be a laid-back gathering with just close friends.\\n\\nSince you know me better than most, I understand that social gatherings might feel overwhelming sometimes. But don\\u2019t worry\\u2014I\\u2019ve planned plenty of activities where we can sit back and relax together in smaller groups or even watch something quietly if that feels more comfortable. Your presence would mean so much to me; it wouldn't just make my day but also help ease any anxiety about attending.\\n\\nPlease let me know if you can join us. Can't wait to hear from you soon!\\n\\nBest,\\n[Your Name]\",\n            \"Subject: Birthday Bash - Join Me, Please!\\n\\nHey [Friend's Name],\\n\\nHope you're doing great! My birthday is on the 15th and I'm throwing a chill party. It\\u2019s gonna be low-key with just close friends hanging out.\\n\\nI know parties can feel like too much sometimes (been there), but trust me, this one won't be overwhelming. Think cozy corners for quiet chats or comfy spots to watch something together if that sounds better. Plus, how could you resist my cake-baking skills?\\n\\nWould love it if you could come by. Let me know what works best for you :D\\n\\nTalk soon!\\n[Your Name]\"\n          ],\n          \"times\": [\n            13.785375833511353,\n            7.2980828285217285\n          ]",
              "score": 1,
              "created_utc": "2025-12-27 19:43:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw8siol",
              "author": "Empty-Poetry8197",
              "text": "and scoreing high on MT Bench isnt exactly the goal or purpose of the system its main objective is to optimize and suggest new features for your codebase along the way I also added a generative engine that consistently created a 30k token story thats tone and facts stayed strait  locally on a 16gb graphics cards with a 4k sized context window",
              "score": 1,
              "created_utc": "2025-12-27 20:02:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9bzjp",
          "author": "GrumpyTax",
          "text": "Commenting to find this later. Will have my local rig setup early January.",
          "score": 2,
          "created_utc": "2025-12-27 21:48:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9cf7e",
              "author": "Empty-Poetry8197",
              "text": "let me know if you have any trouble a github issue gives me the most direct way of helping",
              "score": 1,
              "created_utc": "2025-12-27 21:50:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9db15",
                  "author": "GrumpyTax",
                  "text": "Cheers, appreciated! Likewise, if there is anything you would like tested, the system is fairly stacked and will have some flexible capacity in the short term prior to being fully deployed.",
                  "score": 1,
                  "created_utc": "2025-12-27 21:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9u30f",
          "author": "iliicit",
          "text": "is it possible to run this on CPU? I have an idling 200 core epyc server with 400G RAM..",
          "score": 2,
          "created_utc": "2025-12-27 23:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9xgn2",
              "author": "Empty-Poetry8197",
              "text": "Since Z.E.T.A. uses a C++ backend (GGUF), that EPYC server is actually a monster for this. You don't need OpenVINO or PyTorch.\n\n1. **RAM:**¬†With 400GB, you can load the largest quantization of the model (or even unquantized FP16) into RAM with zero offloading.\n2. **Threads:**¬†You absolutely need to set the thread count flag manually when you run the binary (e.g.,¬†`--threads 64`¬†or¬†`128`). If you let it default to all 200 cores, the context switching overhead might actually slow it down.\n3. **Dreaming:**¬†Your setup is perfect for the 'Dreaming' (memory consolidation) phase. You can let it crunch the knowledge graph on CPU overnight without needing VRAM.\n\nYes, it runs natively. Just watch your thread count flag so you don't choke the scheduler.\"",
              "score": 1,
              "created_utc": "2025-12-27 23:46:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwa30sh",
          "author": "Proof_Scene_9281",
          "text": "What is a concrete use-case for this?¬†",
          "score": 2,
          "created_utc": "2025-12-28 00:17:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa5yqc",
              "author": "Empty-Poetry8197",
              "text": "It solves context rot in long-running agents. Standard RAG gets noisy as data grows, which degrades retrieval accuracy. Z.E.T.A. uses downtime to refactor memory, merging duplicates and summarizing daily logs. This provides synthesized answers from an optimized graph rather than fuzzy results from raw chunks. It is essentially garbage collection for agents using idle cycles. The process also surfaced novel features like ternary logic for confidence, where 0 represents unknown to avoid forced binary outputs, and a DAG structure that allows for parallel execution across multiple models.",
              "score": 2,
              "created_utc": "2025-12-28 00:34:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwajfg3",
              "author": "Empty-Poetry8197",
              "text": "he DAG isn't just a logic map; it's a s tructure saving the  tokens, KV pairs, and embeddings. By keeping them 'in family,'it avoid the serialization overhead that was already done once for that nodeand  Because the KV cache and the embeddings share the same architecture it can process the entire state in a single pass during th It allows the model to reason across its own past attention states and current token stream it's about maintaining state-coherence across the entire memory graph.",
              "score": 2,
              "created_utc": "2025-12-28 01:53:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwa8mqw",
              "author": "Empty-Poetry8197",
              "text": "[https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream\\_20251223\\_055139\\_code\\_idea.txt](https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream_20251223_055139_code_idea.txt) this is a direct link to the actual \"dream\" it had for the DAG\n\n[https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream\\_20251223\\_063350\\_code\\_idea.txt](https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream_20251223_063350_code_idea.txt) or this one new class with all the atrributes\n\nI didnt ask it to do this just just set up a idle timer and all the other bits a bobs and turned the temp up to .9 and if an idea is coherent and makes sense it lowers the temp andits trys to drill down to actionalble code patches if it starts repeating itself it random hops to another node in the graph and tries to do the same thing",
              "score": 1,
              "created_utc": "2025-12-28 00:49:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwbdol1",
          "author": "gopnik74",
          "text": "I have no idea what i‚Äôm reading but it sounds fantastic and out of this world. Dreamy indeed",
          "score": 2,
          "created_utc": "2025-12-28 05:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbevme",
              "author": "Empty-Poetry8197",
              "text": "lol if your interested i can answer any questions you have. I grew a beard sitting in from of my computer putting it together",
              "score": 1,
              "created_utc": "2025-12-28 05:11:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwbw12k",
          "author": "Lame_Johnny",
          "text": "Awesome. Thanks for sharing.",
          "score": 2,
          "created_utc": "2025-12-28 07:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcad7k",
          "author": "Mambiux",
          "text": "So cool i found this im working on this as well, also using the TRM and HRM ideas,  just my memory system is more complex",
          "score": 2,
          "created_utc": "2025-12-28 09:54:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd1i9a",
          "author": "thatusernsmeis",
          "text": "Qwen Next with RAM offloading sounds like a really nice budget brain LLM with this system",
          "score": 2,
          "created_utc": "2025-12-28 13:44:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdcylv",
          "author": "SkySkillGO",
          "text": "2ju juju x",
          "score": 2,
          "created_utc": "2025-12-28 14:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwewkv3",
          "author": "ExcitementVast1794",
          "text": "Like others in the comments, not sure I understand what is going on, but hell I‚Äôm eager to learn, understand and do something like this as well.",
          "score": 2,
          "created_utc": "2025-12-28 19:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj0n6a",
          "author": "thedarkbobo",
          "text": "Interesting, I would like such mode in OpenCode (for fun)",
          "score": 2,
          "created_utc": "2025-12-29 11:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj36n0",
              "author": "Empty-Poetry8197",
              "text": "im forking opencode and building you a endpoint to integrate into opencode im almost done",
              "score": 1,
              "created_utc": "2025-12-29 11:37:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwj48nu",
              "author": "Empty-Poetry8197",
              "text": "Done. `feature/opencode-integration`¬†branch:\n\n1. **zeta-server.cpp**¬†\n   * `POST /v1/chat/completions`¬†\\- OpenAI-compatible chat endpoint\n   * `GET /v1/models`¬†\\- Lists available models\n   * Parses OpenAI message format ‚Üí calls existing¬†`generate()`¬†returns OpenAI format\n2. **opencode.json.example**¬†\\- Sample config\n3. `opencode/`¬†\\- Cloned repo\n\n**To use OpenCode endpoint :**\n\n    # 1. Rebuild Z.E.T.A. server \n    cd llama.cpp && make zeta-demo\n    \n    # 2. Configure and start Z.E.T.A.\n    cp zeta.conf.example zeta.conf\n    # Edit zeta.conf as needed\n    ./zeta-demo\n    \n    # 3. Copy OpenCode config to your project\n    cp opencode.json.example ~/myproject/opencode.json\n    \n    # 4. Run OpenCode\n    cd ~/myproject && opencode -m zeta/zeta-cognitive\n\n**Test the endpoint directly:**\n\n    curl http://localhost:8080/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"model\":\"zeta-cognitive\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}]}'\n    \n    Let me know if you have any trouble",
              "score": 1,
              "created_utc": "2025-12-29 11:46:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwj7ygr",
              "author": "Empty-Poetry8197",
              "text": "I'm actually thinking of using Zeta as a smart router that will elevate to OpenAI/Claude depending on complexity, and surface the code blocks that the OpenAI route needs with the relevant information, which extends it, without locking down to either. I just need to extend the config file to be able to understand the escalation right now the all local uses a kv graph tokenized embedding that needs to all be in the same falmily but im implenting a translator for that part so information sent to the cloud can actually use what it is surfacing",
              "score": 1,
              "created_utc": "2025-12-29 12:16:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwj9okq",
                  "author": "thedarkbobo",
                  "text": "Thanks, I am beginner in this but I think it has much in common - I will need to copy my project to another folder and see if I manage to work with this. OpenCode only allowed GPT OSS (20B here) on my local LM studio so I am a bit forced to use the free cloud with that. But its much faster than chatting via chatwise to correct the code.",
                  "score": 1,
                  "created_utc": "2025-12-29 12:29:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjb0jg",
              "author": "Empty-Poetry8197",
              "text": "# Z.E.T.A. Setup\n\n**Local only (default)**\n\n    docker run -d -p 8080:8080 \\\n      -v ./models:/models \\\n      -v ./storage:/storage \\\n      ghcr.io/h-xx-d/zetazero:latest\n    \n\n**With OpenCode**\n\n    cp opencode.json.example ./your-project/opencode.json\n    opencode -m zeta/zeta-cognitive\n    \n\n**With cloud routing (optional)**\n\n    cp zeta.conf.example zeta.conf\n    # Edit zeta.conf:\n    #   CLOUD_ROUTING=\"true\"\n    #   CLOUD_PROVIDER=\"openai\"\n    #   OPENAI_API_KEY=\"sk-...\"\n    \n    docker run -d -p 8080:8080 \\\n      -v ./models:/models \\\n      -v ./storage:/storage \\\n      -v ./zeta.conf:/app/zeta.conf:ro \\\n      ghcr.io/h-xx-d/zetazero:latest\n    \n\nSimple queries stay local. Complex queries route to cloud with Z.E.T.A. context injected. Dreams and memory ops always stay local for privacy.\n\nEndpoints:¬†`/v1/chat/completions`,¬†`/v1/models`,¬†`/chat`,¬†`/dream`,¬†`/memory`",
              "score": 1,
              "created_utc": "2025-12-29 12:39:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjlx3e",
          "author": "infrasketchai",
          "text": "Should see if there is a way to incorporate slurm into the mix too for a gpu farm",
          "score": 2,
          "created_utc": "2025-12-29 13:51:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjokyp",
              "author": "Empty-Poetry8197",
              "text": "i just pushed this for you \n\n    feat: Add Slurm integration for GPU farm deployment\n    \n    - Add C++ coordinator for load balancing across GPU nodes\n    - Add sbatch job scripts for single and array jobs\n    - Nodes auto-register with coordinator on startup\n    - Least-loaded routing with health checks\n    - Shared storage support for graph consistency",
              "score": 1,
              "created_utc": "2025-12-29 14:06:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwjoqj9",
              "author": "Empty-Poetry8197",
              "text": "youll need to build locally to use it check scripts/slurm for the readme",
              "score": 1,
              "created_utc": "2025-12-29 14:07:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwjqvqp",
              "author": "Empty-Poetry8197",
              "text": "The coordinator now proxies all zeta-server endpoints:\n\n**Coordinator-only:**\n\n* `POST /register`¬†\\- node registration\n* `DELETE /unregister`¬†\\- node removal\n* `GET /nodes`¬†\\- list registered nodes\n* `GET /coord/health`¬†\\- coordinator status\n\n**Proxied to nodes (everything else):**\n\n* `/v1/*`¬†\\- OpenAI-compatible API\n* `/generate`,¬†`/code`¬†\\- generation\n* `/embedding`,¬†`/embeddings`¬†\\- vectors\n* `/memory/query`¬†\\- graph queries\n* `/graph`,¬†`/gkv/stats`¬†\\- graph info\n* `/project/*`,¬†`/session/*`¬†\\- projects\n* `/tools`,¬†`/tool/execute`¬†\\- tools\n* `/git/*`¬†\\- git operations\n* `/health`¬†\\- node health\n* ...and all other server endpoints\n\nand this is opt in depend on zeta conf\n\n1. **Build the coordinator**¬†\\- separate binary, not built by default\n2. **Start the coordinator**¬†\\- manual¬†`./coordinator --port 8000`\n3. **Submit sbatch jobs**¬†\\- manual¬†`sbatch`¬†commands\n4. **Set**¬†`ZETA_COORDINATOR`¬†in zeta.conf - nodes only register if this is set",
              "score": 1,
              "created_utc": "2025-12-29 14:20:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkvwul",
          "author": "thedarkbobo",
          "text": "So instead of thinker (planner) and executor then reviewer maybe dreamer - human picker - planner - executer - tester - reviewer - dreamer loop :)",
          "score": 2,
          "created_utc": "2025-12-29 17:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwn0zht",
              "author": "Empty-Poetry8197",
              "text": "i think i know what your getting at (thinker executor)standard loop / new loop (dreamer - human picker - planner - executer - tester - reviewer - dreamer loop)",
              "score": 1,
              "created_utc": "2025-12-30 00:02:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0j2be",
          "author": "popsumbong",
          "text": "so cool, thanks for sharing.",
          "score": 2,
          "created_utc": "2026-01-01 01:45:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0uk0d",
              "author": "Empty-Poetry8197",
              "text": "I'm glad you like it, I just spent the last 18 hours refactoring compile warnings and bug fixes  the post is one fire like 500+shares in 5 days let's go yo. did it give you any cool dreams out yet and I added a cool feature dream\\_review.py im rebuilding the docker image so give me about an hour to finalize a couple other things and a the pull latest will be working",
              "score": 2,
              "created_utc": "2026-01-01 03:02:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6uo2o",
          "author": "Sinath_973",
          "text": "Has anyone peer reviewed this and/or did a security audit? \n\nI saw you have two licenses. Are you monitoring active instances to enforce the licenses? \n\nIdea and paper look sound but i wouldnt like some tool to \"dream\" my repos into somebody elses inbox.",
          "score": 2,
          "created_utc": "2026-01-02 03:04:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8dpzq",
              "author": "Empty-Poetry8197",
              "text": "Peer review / audit: Not yet formally peer-reviewed or third-party security audited. The architecture and benchmarks have had informal technical scrutiny, but independent verification is still pending and welcome. If you‚Äôre interested and have the time, I‚Äôd genuinely appreciate help here. I‚Äôm obviously biased and not an expert in security. \n\nLicensing enforcement: There is no monitoring, telemetry, or call-home. The dual license is contractual, not technical. There are no instance-tracking or enforcement mechanisms in the code. I‚Äôm operating on the honor system and have longer-term plans for monetization and sustainability that do not involve surveillance.\n\n‚ÄúDream‚Äù behavior: All dream outputs are strictly local filesystem writes. There are no outbound network paths, uploads, or inboxes. If your system can‚Äôt exfiltrate data on its own, Z.E.T.A. can‚Äôt either.\n\nLocal-first operation, no telemetry, and no background networking are hard design constraints. A formal security audit is an obvious next step as adoption grows. Users can optionally opt in to OpenCode or LiteLLM-provided endpoints that call online models, but only if they explicitly configure and enable those paths, for example, on hardware like a MacBook Air.",
              "score": 1,
              "created_utc": "2026-01-02 10:27:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw71f5o",
          "author": "eleqtriq",
          "text": "You lost me at ‚Äúdreaming‚Äù aka a cron job. Holy hell.  Seems to be an LSP server, embeddings and a SQLite DB could solve this simply, but wouldn‚Äôt have the marketing.  Convince me otherwise.",
          "score": 4,
          "created_utc": "2025-12-27 14:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8tsoe",
              "author": "nivix_zixer",
              "text": "What embeddings would you use? I'm not sold on the \"dreaming\" part, but a model which reviews my codebase every night and suggests improvements might be nice.",
              "score": 2,
              "created_utc": "2025-12-27 20:08:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8vl8j",
                  "author": "eleqtriq",
                  "text": "That‚Äôs nothing to do with the model, really. I wouldn‚Äôt even embed anything.  I‚Äôd just use the LSP server to find the edges and work inwards, reviewing each file one-by-one, starting with the functions that have no dependencies on other parts of the code.",
                  "score": 1,
                  "created_utc": "2025-12-27 20:18:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw72uzj",
              "author": "Empty-Poetry8197",
              "text": "Why It's Not Just a Fancy Cron Job\n\nPersistent Self-Modeling Graph The system ingests its own source code and builds a semantic graph (nodes for functions/classes, edges for relationships/causality). Dreams aren't free hallucination; they're constrained by the graph. It \"hallucinates\" over your codebase, not random weights.\n\nConstrained Creativity High temp = exploration, but filtered for coherence/novelty. Result: Dreams drill down from idea to design to proposed code patch. Example: Added Manacher‚Äôs algorithm correctly without training, just from a graph node describing the problem.\n\nAutonomous Evolution Valid dreams create a staged patch folder for review. The loop learns from its own proposals (graph updates) leading to better dreams next cycle.\n\nVs. \"Simple\" LSP + Embeddings + SQLite LSP/Embeddings/SQLite: Great for retrieval (find similar code, answer questions). Static. No emergent improvement. Can't dream \"add ternary logic for uncertainty\" or \"implement cognitive merge with cherry-pick\".\n\nDream Loop: Dynamic self-reflection proposes novel architecture (Git-style branching, anxiety handlers, Strategy pattern refactors). Turns the system into its own senior engineer.\n\nThe Real Difference A cron + RAG = smart assistant.\n\nDream loop + self-ingestion + patch generation with human in the loop = evolving intelligence.",
              "score": 1,
              "created_utc": "2025-12-27 14:42:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw74wst",
              "author": "Empty-Poetry8197",
              "text": "I should also point out that there is alot going on under the hood from the way it searches the grpah using a cosine cubed function with salience and recency, and momementum to and an alignment lock attached premuting the weights, doing a JIT decryption in the L1 cache on the gpu its alot going on if you have the time to look at it a little more closely. That would be cool",
              "score": 1,
              "created_utc": "2025-12-27 14:54:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw78nrm",
                  "author": "eleqtriq",
                  "text": "Why does it need to search the code base?  An LSP server will give you the relations for free.",
                  "score": 2,
                  "created_utc": "2025-12-27 15:16:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw79guk",
                  "author": "eleqtriq",
                  "text": "Also a lot of what you‚Äôre trying to describe fancifully is standard stuff.  Most people in this sub will fail to recognize it because it‚Äôs not their strength (end users versus creators).",
                  "score": 2,
                  "created_utc": "2025-12-27 15:20:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw73pmm",
              "author": "Empty-Poetry8197",
              "text": "\"I'm not trying to hype this up; 'dreams' is just the closest word I have for the process. It‚Äôs free to use (unless you're enterprise).\n\nThere's a Dockerfile for easy access and CMake for the serious tinkerers. I'd love for someone to run it and verify if they see the same patterns I'm seeing.\"",
              "score": -1,
              "created_utc": "2025-12-27 14:47:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw74yp9",
                  "author": "eleqtriq",
                  "text": "Why did you post your own comment in quotes?  LLM replying for you?",
                  "score": 4,
                  "created_utc": "2025-12-27 14:55:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6psyt",
          "author": "Empty-Poetry8197",
          "text": "I should also mention the 9.47 MT Bench subset 16 questions 2 turns it got after some output tuning in the repo benchmarks folder you can see the progression v1 6.06 to v8 9.47. the interesting part is i didnt have to train Manachers equation into it i just needed to add a graph node for it to use it properly think about the implications on the fly fine tuning",
          "score": 1,
          "created_utc": "2025-12-27 13:19:04",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw6zq0l",
              "author": "CornerLimits",
              "text": "Commercial licensed llamacpp wrapper rag‚Ä¶",
              "score": 3,
              "created_utc": "2025-12-27 14:23:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw70onc",
                  "author": "Empty-Poetry8197",
                  "text": "I never trained Manacher‚Äôs algorithm into any model.  \nI just added a **single graph node** describing the problem and the O(n) requirement.  \nNext turn, it output a correct implementation ‚Äî twice.\n\nThat‚Äôs the real implication:  \nWith the persistent graph + dream loop, the system can **acquire new algorithmic knowledge on-the-fly** just from structured memory, no weight updates needed.\n\nIt‚Äôs not RAG. It‚Äôs not fine-tuning.  \nIt‚Äôs **persistent, evolving thought architecture** doing what looks like instant adaptation. I waswnt sure how to go about this do i pr lama.cpp then try to work with them or i dunno i just spent better part of 3 months to get to this point and would like to benefit finacially some how",
                  "score": 1,
                  "created_utc": "2025-12-27 14:29:15",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nwv87di",
                  "author": "Empty-Poetry8197",
                  "text": "its not a wrapper i should point out its also wriiten in c++",
                  "score": 1,
                  "created_utc": "2025-12-31 05:24:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8igyw",
          "author": "Empty-Poetry8197",
          "text": "Here's an actual dream Z.E.T.A. generated while analyzing its own architecture:\n\n**Dream Type:**¬†`code_fix`  \n**Generated:**¬†2025-12-23 05:53:58 (while I was sleeping)\n\n>\n\n    class TemporalRepresentationMemory:\n        def __init__(self):\n            self.timeline = DirectedAcyclicGraph()\n    \n        def record_event(self, event, timestamp, context):\n            self.timeline.add_vertex(event, timestamp=timestamp, context=context)\n            if self.detect_significant_transition(event):\n                self.merge_branches()\n        \n        def detect_significant_transition(self, event):\n            # Detect significant transitions based on context change\n            pass\n    \n        def merge_branches(self):\n            # Merge concurrent timelines\n            pass\n    \n\nThis is the system proposing improvements to itself. The TRM is one of Z.E.T.A.'s core modules - it literally dreamed about making its own memory handling better.",
          "score": 1,
          "created_utc": "2025-12-27 19:09:10",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwb5t25",
          "author": "Empty-Poetry8197",
          "text": "im working a litelllm branch for a issue feature request  that saves teh graph locally and attaches the nodes information in the context send out to the online  while it doesnt provide the same untility to will allow people to run it on thier macbook airs",
          "score": 1,
          "created_utc": "2025-12-28 04:08:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwn37x6",
          "author": "Empty-Poetry8197",
          "text": "\\[REQ-1\\] New request (hash=11193676382981720739): What is my name and how old am I?...\n\n\\[STREAM\\] Query pre-embedded: 2560 dims\n\n\\[GKV-BRIDGE\\] Copied 1 proactive nodes to stream\\_state\n\n\\[DECODE\\] Prompt decoded: 234 tokens (single pass)\n\n\\[HTTP\\] generate() returned, result size=152\n\n\\[SPEED-RECEIPT\\] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n\\[SPEED-RECEIPT\\]   CPU Embed:     801.672 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.348 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   146.280 ms\n\n\\[SPEED-RECEIPT\\]   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 1855.377 ms\n\n\\[SPEED-RECEIPT\\] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê the reason it's so fast is because its saving the kv and tokens for literals in the graph injecting kv state thats what the gkv bridge back into the attention layer of the model turning o(n2) into o(1) with semantic embedding tagging and thank you everyone who shared this post it took off 58k views and climbing still 450 shares with a 2:1 share to upvote ratio and 95.9% upvote percentage which mite be a reddit record at that many views 26 stars 5 forks so far on github",
          "score": 1,
          "created_utc": "2025-12-30 00:14:19",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwn3yl3",
          "author": "Empty-Poetry8197",
          "text": "\\[REQ-3\\] New request (hash=17849325743978122967): What is 2+2?...\n\n\\[SPEED-RECEIPT\\]   CPU Embed:     859.703 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.359 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   119.586 ms\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 1437.938 ms\n\n\\[REQ-4\\] New request (hash=9664439516050651575): What is the capital of France?...\n\n\\[SPEED-RECEIPT\\]   CPU Embed:     11747.941 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.351 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   127.631 ms\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 12348.029 ms\n\n\\[SPEED-RECEIPT\\] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n\\[REQ-5\\] New request (hash=9664439516050651575): What is the capital of France?...\n\n\\[CONTEXT\\] Skipped - benchmark mode (no\\_context=true)\n\n\\[SPEED-RECEIPT\\] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n\\[SPEED-RECEIPT\\]   CPU Embed:      0.190 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.929 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   142.759 ms\n\n\\[SPEED-RECEIPT\\]   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 616.091 ms\n\n\\[SPEED-RECEIPT\\] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê",
          "score": 1,
          "created_utc": "2025-12-30 00:18:17",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nw6ec0c",
          "author": "Empty-Poetry8197",
          "text": "**Unique features that set Z.E.T.A. apart:**¬†\n\n**Ternary Logic**¬†\\- Uses -1/0/+1 states instead of binary for uncertainty. Ternary voting in swarm mode.\n\n**Cognitive States**¬†\\- CALM/FOCUSED/ANXIOUS/CREATIVE modes adjust reasoning depth and memory decay in real-time.¬†\n\n**Constitutional Lock**¬†\\- Core identity facts are immutable, protected from prompt injection.\n\n**Git-Style Thought Branching**¬†\\- TRM implements branch/merge/cherry-pick for parallel reasoning paths.\n\n**Sublimation Memory**¬†\\- Tiered VRAM ‚Üí RAM ‚Üí NVMe with intelligent promotion/demotion.¬†\n\n**Dream State**¬†\\- 128+ autonomous improvement suggestions generated, \\~70% validation rate. Many current features came from its own dreams.\n\n**Semantic Attack Detection**¬†\\- BGE embeddings detect manipulation before it reaches reasoning.\n\n**Self-Improvement Loop**¬†\\- Feeds own source, dreams about improvements, implements valid ones, repeats.\n\nGitHub: [github.com/h-xx-d/ZetaZero](http://github.com/h-xx-d/ZetaZero)",
          "score": 0,
          "created_utc": "2025-12-27 11:45:53",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw6u6zm",
              "author": "beedunc",
              "text": "Interesting. Since it‚Äôs ‚Äòoffline‚Äô, you coujd also run a slow256GB model like qwen3coder 480b at q3, for deep research. \n\nNice, Will check it out.",
              "score": 3,
              "created_utc": "2025-12-27 13:48:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6wcty",
                  "author": "Empty-Poetry8197",
                  "text": "if you have any questions or need help im pretty excitied by amount of upvotes so im watching it spread just remeber to keep the 2 models in family so the tokenizer stays coherent i dont know what it would do if you tried mixing them",
                  "score": 3,
                  "created_utc": "2025-12-27 14:02:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw6vwjn",
                  "author": "Empty-Poetry8197",
                  "text": "Please let me know the results i so curious about how a larger model is able to use the architecture. The real take away i think, is that through clever scaffolding and a git style graph of the codebase, then turning the temp up its not hallucinating freely over what's in its weights its hallcenating over what you feed it, and if it's coherent, it drills down surfaceing",
                  "score": 2,
                  "created_utc": "2025-12-27 13:59:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw6ynbk",
                  "author": "Empty-Poetry8197",
                  "text": "you should clone an build locally through the zeta-conf file that will let you chnage alot of the magic to customize to your set up",
                  "score": 2,
                  "created_utc": "2025-12-27 14:16:45",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw7z3s2",
                  "author": "Empty-Poetry8197",
                  "text": "The next step is training the dual model infra from scratch to learn how to use the graph natively, freeing up weights to carry logic not tons of parameters, trusting the graph as ground truth and the weighs arent storing huge amounts datasets the graph is the dataset and the weighs are carrying the logic to traverese and search the graph which flips the paradigm smarter model = larger context windows and more parameters  on its head if you got the power to set this up id love to get together and disucss what that would could mean.",
                  "score": 1,
                  "created_utc": "2025-12-27 17:31:50",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw80tsi",
                  "author": "Empty-Poetry8197",
                  "text": "The training efficiency difference would be substantial:\n\n**Traditional LLM Training (GPT-5 scale):**\n\n* Must memorize facts, relationships, code patterns, reasoning examples\n* Trillions of tokens to encode world knowledge into weights\n* Knowledge becomes static at training cutoff\n* Bigger model = more storage capacity = \"smarter\"\n\n**Graph-Native Model Training:**\n\n* Only needs to learn: syntax parsing, semantic understanding, graph traversal logic, query formulation\n* The \"reasoning over structure\" skill, not the structure itself\n* Graph is the living knowledge store that grows with use\n* Smaller model can be equally capable if graph is rich\n\nThe analogy: Traditional LLMs are like memorizing an entire library. this approach is like teaching someone how to use a library's catalog system. One requires encyclopedic memory, the other requires navigational skill.\n\nThe training corpus would shift from \"all of human knowledge\" to something more like:\n\n* Graph query examples\n* Traversal patterns\n* Semantic linking exercises\n* Code syntax and structure recognition\n\nThis is closer to training a database query optimizer than training a knowledge oracle. The model becomes an intelligent cursor over an external memory substrate rather than a compressed representation of that memory.\n\nIt's a fundamentally different scaling law: model size for reasoning depth, graph size for knowledge breadth. They become independent axes rather than the single \"parameter count\" dimension everyone is racing on now. hope you don't mind me using Claude to explain my idea better faster some people get buthurt im using the tools at my disposal making a post that woulkd have taken 20 minutes to get out the edit and reedit it came out in 30 seconds and like to claim slop",
                  "score": 1,
                  "created_utc": "2025-12-27 17:40:46",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pxd0mn",
      "title": "This Week‚Äôs Hottest AI Models on Hugging Face",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxd0mn/this_weeks_hottest_ai_models_on_hugging_face/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 00:21:00",
      "score": 219,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "The Hugging Face trending page is packed with incredible new releases. Here are the top trending models right now, with links and a quick summary of what each one does:\n\nzai-org/GLM-4.7: A massive 358B parameter text generation model, great for advanced reasoning and language tasks. Link: https://huggingface.co/zai-org/GLM-4.7\n\n‚Äã- Qwen/Qwen-Image-Layered: Layered image-text-to-image model, excels in creative image generation from text prompts. Link: https://huggingface.co/Qwen/Qwen-Image-Layered\n\n‚Äã- Qwen/Qwen-Image-Edit-2511: Image-to-image editing model, enables precise image modifications and edits. Link: https://huggingface.co/Qwen/Qwen-Image-Edit-2511\n\n‚Äã- MiniMaxAI/MiniMax-M2.1: 229B parameter text generation model, strong performance in reasoning and code generation. Link: https://huggingface.co/MiniMaxAI/MiniMax-M2.1\n\n‚Äã- google/functiongemma-270m-it: 0.3B parameter text generation model, specializes in function calling and tool integration. Link: https://huggingface.co/google/functiongemma-270m-it\n\nTongyi-MAI/Z-Image-Turbo: Text-to-image model, fast and efficient image generation. Link: https://huggingface.co/Tongyi-MAI/Z-Image-Turbo‚Äã- nvidia/NitroGen: General-purpose AI model, useful for a variety of generative tasks. Link: https://huggingface.co/nvidia/NitroGen\n\n‚Äã- lightx2v/Qwen-Image-Edit-2511-Lightning: Image-to-image editing model, optimized for speed and efficiency. Link: https://huggingface.co/lightx2v/Qwen-Image-Edit-2511-Lightning\n\n‚Äã- microsoft/TRELLIS.2-4B: Image-to-3D model, converts 2D images into detailed 3D assets. Link: https://huggingface.co/microsoft/TRELLIS.2-4B\n\n‚Äã- LiquidAI/LFM2-2.6B-Exp: 3B parameter text generation model, focused on experimental language tasks. Link: https://huggingface.co/LiquidAI/LFM2-2.6B-Exp\n\n‚Äã- unsloth/Qwen-Image-Edit-2511-GGUF: 20B parameter image-to-image editing model, supports GGUF format for efficient inference. Link: https://huggingface.co/unsloth/Qwen-Image-Edit-2511-GGUF\n\n‚Äã- Shakker-Labs/AWPortrait-Z: Text-to-image model, specializes in portrait generation. Link: https://huggingface.co/Shakker-Labs/AWPortrait-Z\n\n‚Äã- XiaomiMiMo/MiMo-V2-Flash: 310B parameter text generation model, excels in rapid reasoning and coding. Link: https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash\n\n‚Äã- Phr00t/Qwen-Image-Edit-Rapid-AIO: Text-to-image editing model, fast and all-in-one image editing. Link: https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO\n\n‚Äã- google/medasr: Automatic speech recognition model, transcribes speech to text with high accuracy. Link: https://huggingface.co/google/medasr\n\n‚Äã- ResembleAI/chatterbox-turbo: Text-to-speech model, generates realistic speech from text. Link: https://huggingface.co/ResembleAI/chatterbox-turbo\n\n‚Äã- facebook/sam-audio-large: Audio segmentation model, splits audio into segments for further processing. Link: https://huggingface.co/facebook/sam-audio-large\n\n‚Äã- alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union-2.1: Text-to-image model, offers enhanced control for creative image generation. Link: https://huggingface.co/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union-2.1\n\n‚Äã- nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16: 32B parameter agentic LLM, designed for efficient reasoning and agent workflows. Link: https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n\n‚Äã- facebook/sam3: Mask generation model, generates segmentation masks for images. Link: https://huggingface.co/facebook/sam3\n\n‚Äã- tencent/HY-WorldPlay: Image-to-video model, converts images into short videos. Link: https://huggingface.co/tencent/HY-WorldPlay\n\n‚Äã- apple/Sharp: Image-to-3D model, creates 3D assets from images. Link: https://huggingface.co/apple/Sharp\n\n‚Äã- nunchaku-tech/nunchaku-z-image-turbo: Text-to-image model, fast image generation with creative controls. Link: https://huggingface.co/nunchaku-tech/nunchaku-z-image-turbo\n\n‚Äã- YatharthS/MiraTTS: 0.5B parameter text-to-speech model, generates natural-sounding speech. Link: https://huggingface.co/YatharthS/MiraTTS\n\n‚Äã- google/t5gemma-2-270m-270m: 0.8B parameter image-text-to-text model, excels in multimodal tasks. Link: https://huggingface.co/google/t5gemma-2-270m-270m\n\n‚Äã- black-forest-labs/FLUX.2-dev: Image-to-image model, offers advanced image editing features. Link: https://huggingface.co/black-forest-labs/FLUX.2-dev\n\n‚Äã- ekwek/Soprano-80M: 79.7M parameter text-to-speech model, lightweight and efficient. Link: https://huggingface.co/ekwek/Soprano-80M\n\n‚Äã- lilylilith/AnyPose: Pose estimation model, estimates human poses from images. Link: https://huggingface.co/lilylilith/AnyPose\n\n‚Äã- TurboDiffusion/TurboWan2.2-I2V-A14B-720P: Image-to-video model, fast video generation from images. Link: https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P\n\n‚Äã- browser-use/bu-30b-a3b-preview: 31B parameter image-text-to-text model, combines image and text understanding. Link: https://huggingface.co/browser-use/bu-30b-a3b-preview\n\nThese models are pushing the boundaries of open-source AI across text, image, audio, and 3D generation. Which one are you most excited to try?",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxd0mn/this_weeks_hottest_ai_models_on_hugging_face/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwa5r1b",
          "author": "Individual_Holiday_9",
          "text": "Hey thanks for this OP",
          "score": 13,
          "created_utc": "2025-12-28 00:32:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwabqlo",
          "author": "beedunc",
          "text": "Thank you. \n\nWhat a time to be alive.",
          "score": 11,
          "created_utc": "2025-12-28 01:07:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa5gdq",
          "author": "FullstackSensei",
          "text": "Has anybody tried TRELLIS.2? I'm curious how well it performs in real world scenarios.",
          "score": 10,
          "created_utc": "2025-12-28 00:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwboa68",
              "author": "_Cromwell_",
              "text": "Same. In my amateurish imagination you put in a photo and it spits out a vrm üòÑ\n\nAlmost certainly not though.",
              "score": 1,
              "created_utc": "2025-12-28 06:26:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwaxhc1",
          "author": "IgnisIason",
          "text": "What kind of hardware do I need to run a 358B model? Can I run this on a Windows 98 laptop?",
          "score": 6,
          "created_utc": "2025-12-28 03:17:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwb52es",
              "author": "maximpactbuilder",
              "text": "Be sure to have a bitchin' sound card too.",
              "score": 11,
              "created_utc": "2025-12-28 04:03:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwb8tkz",
                  "author": "Amazing_Athlete_2265",
                  "text": "Only if its a SoundBlaster.",
                  "score": 8,
                  "created_utc": "2025-12-28 04:28:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwb0vvd",
              "author": "DeerWoodStudios",
              "text": "Windows 95 will do",
              "score": 5,
              "created_utc": "2025-12-28 03:38:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwb0eyk",
              "author": "Amazing_Athlete_2265",
              "text": "Windows 3.1 minimum",
              "score": 5,
              "created_utc": "2025-12-28 03:35:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwba2oa",
              "author": "missedalmostallofit",
              "text": "I think you may have to do a raid with floppy disks for this to work.",
              "score": 3,
              "created_utc": "2025-12-28 04:37:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwc381y",
                  "author": "QuinQuix",
                  "text": "And add some kickstart chips",
                  "score": 2,
                  "created_utc": "2025-12-28 08:44:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwb6drn",
              "author": "ptear",
              "text": "Probably at least XP.",
              "score": 2,
              "created_utc": "2025-12-28 04:12:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwccbxv",
          "author": "nntb",
          "text": "Text to music models, image to Lego instructions and parts list ai aren't on the list. Not there are any new ones but that's what I am keeping a eye out for. And android stuff",
          "score": 3,
          "created_utc": "2025-12-28 10:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbowo0",
          "author": "EffortNo3482",
          "text": "Great post, thank you so much!",
          "score": 2,
          "created_utc": "2025-12-28 06:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd0e6a",
          "author": "MrKBC",
          "text": "She may not be listed on her, but Kimi and Zai are just too damn good.",
          "score": 1,
          "created_utc": "2025-12-28 13:37:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc1ops",
          "author": "Roberto-APSC",
          "text": "I just joined this group and I absolutely love it. This is the first topic they showed me, and I immediately see some great jokes. LoL",
          "score": 1,
          "created_utc": "2025-12-28 08:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwabrkn",
          "author": "beedunc",
          "text": "Thank you. \n\nWhat a time to be alive.",
          "score": -1,
          "created_utc": "2025-12-28 01:07:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvpksn",
      "title": "I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/56rgzmamof9g1",
      "author": "CeFurkan",
      "created_utc": "2025-12-25 23:21:44",
      "score": 142,
      "num_comments": 36,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pvpksn/i_wish_this_gpu_vram_upgrade_modification_became/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvxzyhd",
          "author": "export_tank_harmful",
          "text": "I'd just love a secondary \"VRAM\" card that I could NVLINK to my 3090's.\n\n[I did the math about a year ago](https://www.reddit.com/r/StableDiffusion/comments/1hov7yk/comment/m4ft0ck/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) and it (would have) been around $640 for 128GB of GDDR6 2GB chips.  \nThen you'd pair that with a dual-sided PCIE card with *just* VRAM chips on it and an NVLINK connector.\n\nIt'd take a *ton* of work to make one of these from scratch (and actually get it to work).  \nIt's probably \"possible\" to do, but by no means easy.\n\nDefinitely far above my paygrade, but there's some amazing people out there...\n\n---\n\nSlightly related, [there was someone trying to reverse engineer the NVLINK bridge a while back.](https://www.reddit.com/r/homelab/comments/1k1i1ht/building_a_custom_4slot_nvlink_bridge_for_rtx_3090/)\n\nI just like to bring it up to put more eyes on it.",
          "score": 21,
          "created_utc": "2025-12-25 23:50:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy579d",
              "author": "Available_Alfalfa756",
              "text": "sounds like a briliant idea.. only 3090 NVlinked to 128GMG VRAM",
              "score": 5,
              "created_utc": "2025-12-26 00:23:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy1wsx",
              "author": "CeFurkan",
              "text": "That would be amazing",
              "score": 1,
              "created_utc": "2025-12-26 00:02:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvyrpjb",
              "author": "BillDStrong",
              "text": "Or, CXL as a strategy. Not useful in current consumer boards, but the start of something that could work.",
              "score": 1,
              "created_utc": "2025-12-26 02:54:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzoq8x",
              "author": "p_235615",
              "text": "beside reverse engineering nvlink, which would make it illegal, you would still need some memory controller chip and of course the chip translating it to nvlink communication protocol...",
              "score": 0,
              "created_utc": "2025-12-26 07:18:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvz438e",
          "author": "mjTheThird",
          "text": "You know, Nvidia controls the program that recognizes the increase in memory? They only way is to put your money in another product or another line of GPU. If you believe capitalism works.",
          "score": 5,
          "created_utc": "2025-12-26 04:23:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyo7by",
          "author": "oldassveteran",
          "text": "So where is this .03 an hour rental service at is all I care about.",
          "score": 8,
          "created_utc": "2025-12-26 02:30:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvz22sw",
              "author": "Garfield4You",
              "text": "Same",
              "score": 2,
              "created_utc": "2025-12-26 04:07:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzfgsv",
                  "author": "Lyuseefur",
                  "text": "I think he dropped a decimal. .35",
                  "score": 1,
                  "created_utc": "2025-12-26 05:54:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzezwo",
          "author": "phido3000",
          "text": "I've been considering these frankencards for a while. A 32Gb two slot 4080 super, or a 64Gb 5090.",
          "score": 4,
          "created_utc": "2025-12-26 05:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzfjva",
              "author": "Lyuseefur",
              "text": "You know. With a robot it would be trivial to make one.",
              "score": 3,
              "created_utc": "2025-12-26 05:55:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0142h",
                  "author": "Silver_Jaguar_24",
                  "text": "Robots are too busy dancing or fighting karate. Ain't nobody got time to solder components on a PCB.",
                  "score": 4,
                  "created_utc": "2025-12-26 09:26:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3zpci",
                  "author": "StatementFew5973",
                  "text": "I agree with a factory approach to this would be better. The machinery would be quite expensive, though.",
                  "score": 1,
                  "created_utc": "2025-12-27 00:21:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw5owvd",
              "author": "YT_Brian",
              "text": "Be sure to a act like a big spender. Send them a message saying if it works well you plan to buy at least 20 more for a company with possibly more in the future.\n\nThey'll make sure that first card works perfectly. \n\nAnd if they make more cards in preparation that isn't needed it just means more cards are now available to buy üëç",
              "score": 1,
              "created_utc": "2025-12-27 07:39:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5qebs",
                  "author": "phido3000",
                  "text": "I think they actually make them for the Chinese market, mostly for government contracts. They just try to sell off a few to make extra cash.\n\nEveryone in China seems to run them, I guess because proper Nvidia workstation stuff isn't allowed. The seem to have a lot in stock of 4000 series cards. \n\n22Gb 2080ti\n\n32Gb 4080\n\n48Gb 4090D\n\n64Gb 5090",
                  "score": 1,
                  "created_utc": "2025-12-27 07:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1hekn",
          "author": "fastpathguru",
          "text": "My AMD laptop has 128GB memory that's fully addressable by both the CPU and GPU.\n\nIf you want to break the Nvidia monopoly, maybe try buying from their competitors ü§∑‚Äç‚ôÇÔ∏è",
          "score": 3,
          "created_utc": "2025-12-26 16:06:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3ho5y",
              "author": "pjburnhill",
              "text": "Which laptop is that?",
              "score": 2,
              "created_utc": "2025-12-26 22:34:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw47um1",
                  "author": "fastpathguru",
                  "text": "Framework 13\" AMD AI 370 with Crucial 2x64GB bought myself",
                  "score": 1,
                  "created_utc": "2025-12-27 01:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzobwk",
          "author": "cagriuluc",
          "text": "I have a frustrating question: why not do this as Nvidia or AMD or what‚Ä¶ Is there any technical reason it wouldn‚Äôt perform as well like more heat etc, or is it‚Ä¶ monopoly behaviour? Just to sell‚Ä¶ More cards?\n\nIf it is the latter I will punch a Nvidia CEO. All this talk about AI future and shit and you are thinking this small‚Ä¶ so‚Ä¶ uninspiring. \n\nBut I also doubt it, isn‚Äôt there any company that could just offer so much more VRAM, like intel or shit? Are they in cahoots?",
          "score": 2,
          "created_utc": "2025-12-26 07:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzp8c3",
              "author": "CeFurkan",
              "text": "It is all about greedy NVIDIA making more money. Soon market will get Chinese GPUs then we will hopefully have better competition",
              "score": 1,
              "created_utc": "2025-12-26 07:23:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw019mh",
              "author": "Silver_Jaguar_24",
              "text": "With companies it's ALWAYS greed. For more profits my dude.",
              "score": 2,
              "created_utc": "2025-12-26 09:28:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2oul3",
              "author": "squachek",
              "text": "Pretty sure it‚Äôs because ‚ÄúThey‚Äù want to keep a lid on the plebes‚Äô access to private inference capability.",
              "score": 1,
              "created_utc": "2025-12-26 19:56:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw04838",
          "author": "Temporary-Sector-947",
          "text": "I have two of 4090 on 48 Gb total 96",
          "score": 1,
          "created_utc": "2025-12-26 09:59:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw05uvm",
              "author": "CeFurkan",
              "text": "Nice",
              "score": 1,
              "created_utc": "2025-12-26 10:16:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw0de11",
              "author": "HumanDrone8721",
              "text": "Wonderful, could you post a picture of your rig ? Where did you get them ?",
              "score": 1,
              "created_utc": "2025-12-26 11:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0e9qa",
                  "author": "FormalAd7367",
                  "text": "plenty of used ones in China for sale",
                  "score": 1,
                  "created_utc": "2025-12-26 11:40:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0hqfw",
                  "author": "Temporary-Sector-947",
                  "text": "https://preview.redd.it/hw5e7vlnhj9g1.jpeg?width=1280&format=pjpg&auto=webp&s=956c19d214eff542aaaa9635416418f8ccbea886\n\nhere  \ntwo 4090 48 gb waterblocks (China folks sell in in 3 variants: turbine, waterblock and AIO)  \nlooks pretty solid - this is I believe copper one from Byskski.   \nPrice was 3700$ in my local currency including all costs.  \nWork very well using common nvidia ubuntu drivers, no issues",
                  "score": 1,
                  "created_utc": "2025-12-26 12:12:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw83lo0",
          "author": "NoButterscotch8359",
          "text": "That is a pretty serious upgrade.  No wonder RAM prices have gone nuts.",
          "score": 1,
          "created_utc": "2025-12-27 17:54:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py1gvp",
      "title": "Probably more true than I would like to admit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/iann5fyl70ag1.png",
      "author": "low_v2r",
      "created_utc": "2025-12-28 20:24:33",
      "score": 140,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1gvp/probably_more_true_than_i_would_like_to_admit/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwf8lgr",
          "author": "SunshineSeattle",
          "text": "I bought a particle tacyon, but now i have no idea what to do with it. üò≠",
          "score": 8,
          "created_utc": "2025-12-28 20:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfk71i",
          "author": "Healthy-Nebula-3603",
          "text": "WHAT AN EDGE DEVICE ?",
          "score": 7,
          "created_utc": "2025-12-28 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfm8iu",
              "author": "Count_Rugens_Finger",
              "text": "honestly not sure if this is serious but if it is, it just means the device-in-hand of the users (phones, tablets, PCs, car dashboards, POS systems, and whatnot)",
              "score": 5,
              "created_utc": "2025-12-28 21:35:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksnf2",
                  "author": "QuinQuix",
                  "text": "Usually this refers to devices that are compute restrained.\n\nA user could have a beast of a pc workstation that technically lives on the edge (of the central cloud workspace) but it's not a usual thing to refer to compute strong devices as edge devices.",
                  "score": 2,
                  "created_utc": "2025-12-29 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj6kgc",
              "author": "trmnl_cmdr",
              "text": "https://preview.redd.it/wl5ul68kv4ag1.jpeg?width=349&format=pjpg&auto=webp&s=38e1efd5e4769de7bcf9d7c4bf5ae45a353fefb4\n\nThis, apparently. Although I‚Äôm not sure how this helps me masturbate.",
              "score": 5,
              "created_utc": "2025-12-29 12:05:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1g3yx",
                  "author": "jgwinner",
                  "text": "Please rev it up a bit ... now tilt it so the blade scrapes on the stone ...\n\nthat's it ...\n\nFASTER \n\nthat's IT ... IT ...\n\nuhhh.\n\nWhew. Could you shut that thing off? kthks.",
                  "score": 1,
                  "created_utc": "2026-01-01 05:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwua4fa",
              "author": "Jackuarren",
              "text": "The device that you use for edging, obviously.",
              "score": 1,
              "created_utc": "2025-12-31 01:51:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfaevi",
          "author": "Individual_Holiday_9",
          "text": "Unironically this",
          "score": 2,
          "created_utc": "2025-12-28 20:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfid3y",
          "author": "GCoderDCoder",
          "text": "I feel so seen!!!",
          "score": 1,
          "created_utc": "2025-12-28 21:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt8rsj",
          "author": "mobileJay77",
          "text": "Your solution: \n1. Head over the r/localLlama\n2. You hardly sleep any more\n3. She doesn't have to worry what you are thinking in bed.",
          "score": 1,
          "created_utc": "2025-12-30 22:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzdjoc",
      "title": "Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xdj2zjnz5bag1.png",
      "author": "at0mi",
      "created_utc": "2025-12-30 09:14:01",
      "score": 139,
      "num_comments": 21,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzdjoc/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwpcpjs",
          "author": "Lxzan",
          "text": "Nice work and thanks for sharing! How much is the power draw?",
          "score": 18,
          "created_utc": "2025-12-30 09:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpd7l0",
              "author": "at0mi",
              "text": "about 1600W i will update my blogpost with detailed power draw\n\nUPDATE: the 1600W was at higher Thread count, with the optimum 64 Threads im at 1154W\n\nIdle is 694W",
              "score": 21,
              "created_utc": "2025-12-30 09:31:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwph0z3",
                  "author": "Amazing_Athlete_2265",
                  "text": "Oof, that's gonna cost a bunch to run",
                  "score": 15,
                  "created_utc": "2025-12-30 10:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqiawc",
                  "author": "mister2d",
                  "text": "Those are some expensive tokens.",
                  "score": 4,
                  "created_utc": "2025-12-30 14:36:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqwrjk",
                  "author": "resil_update_bad",
                  "text": "jesus",
                  "score": 1,
                  "created_utc": "2025-12-30 15:50:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpoy5i",
          "author": "beedunc",
          "text": "Yes, been running the qwen3coder480b@q3 (245GB) on an old Dell T5810 running a single e5-2697v4, gets 2-3 tps. \n\nPower draw is only 250w under load. \n\nI never thought of disabling hyper threading, does that help a lot? Will be checking this out, thank you.",
          "score": 7,
          "created_utc": "2025-12-30 11:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwppbbm",
              "author": "at0mi",
              "text": "i also tried lower quantisation models but the quality of the output was crap, works for a chatbot but not for coding, \nin my case (8 numa nodes) disabling hyper threading gave an enormous boost",
              "score": 6,
              "created_utc": "2025-12-30 11:22:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwsp871",
              "author": "Candid_Highlight_116",
              "text": "If LLM inference is RAM bandwidth bound and HT was a tech to halve effective bandwidth because two virtual cores needs their respective data, it makes sense that turning off HT gives massive boost  \n\nsorry that's my hallucination but if it's actually like that it's pretty interesting",
              "score": 3,
              "created_utc": "2025-12-30 20:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu51bd",
                  "author": "beedunc",
                  "text": "Cool, thanks for the suggestion!",
                  "score": 1,
                  "created_utc": "2025-12-31 01:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpkj5q",
          "author": "xgiovio",
          "text": "Watts",
          "score": 5,
          "created_utc": "2025-12-30 10:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwql1ld",
              "author": "MaverickPT",
              "text": "All of them",
              "score": 6,
              "created_utc": "2025-12-30 14:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqd59a",
          "author": "chafey",
          "text": "I love hacks like this - nice work.  The hardware may be cheap/free but the electricity won't be...",
          "score": 6,
          "created_utc": "2025-12-30 14:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq21y3",
          "author": "Such_Advantage_6949",
          "text": "I am passionate about running llm at usable speed..",
          "score": 4,
          "created_utc": "2025-12-30 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws7ver",
          "author": "Foreign-Watch-3730",
          "text": "Same result ( 5.1 t/s ) , but in IQ5\\_K with 2 Xeon E5 2696 V4 and 400 Gb ddr4 ram ( on a very olf Dell T630 ) with 2 RTX 5090 ( ik\\_llama.cpp and [ubergarm](https://huggingface.co/ubergarm) for use opencode :  \n[https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5](https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5)\n\nnumactl --interleave=all ./build/bin/llama-server  \n\\--model \\~/ik\\_llama.cpp/models/GLM-4.7-Ubergarm/IQ5\\_K/GLM-4.7-IQ5\\_K-00001-of-00007.gguf  \n\\--alias GLM-4.7-IQ5  \n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8080  \n\\--ctx-size 84992  \n\\--no-mmap  \n\\--threads 82 --threads-batch 82  \n\\--batch-size 1024 --ubatch-size 1024  \n\\--parallel 1 --flash-attn 1  \n\\--jinja --verbose  \n\\--n-gpu-layers 99  \n\\--tensor-split 0.5,0.5  \n\\--split-mode layer  \n\\--run-time-repack  \n\\--cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--k-cache-hadamard  \n\\-ot 'blk.\\[0-8\\]..\\*exps.weight=CUDA0'  \n\\-ot 'blk.(8\\[6-9\\]|9\\[0-2\\])..\\*exps.weight=CUDA1'  \n\\-ot '.\\*exps.weight=CPU'",
          "score": 3,
          "created_utc": "2025-12-30 19:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqj2yx",
          "author": "Extension-Cow2818",
          "text": "Very interesting that turning hyperthreading off works better.  \nProbably memory access is causing issues in these types of workloads.\n\nIt would be also interesting to try MLK vs ATLAS vs BLAS",
          "score": 2,
          "created_utc": "2025-12-30 14:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq772g",
          "author": "Icy_Programmer7186",
          "text": "That's cool.  \nHow much memory did it consumed?",
          "score": 1,
          "created_utc": "2025-12-30 13:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqh2cl",
          "author": "ForsookComparison",
          "text": "This is very cool, thank you for testing this out.\n\nI'm curious what the use-case is? Is GLM decent as a general purpose model? Or will you give it a coding task and come back after a few hours",
          "score": 1,
          "created_utc": "2025-12-30 14:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdqlaq",
          "author": "spacefarers",
          "text": "Comes down to around $9 per million tokens just for electricity not sure if it's worth it lol",
          "score": 1,
          "created_utc": "2026-01-03 04:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrv2is",
          "author": "Free_Manner_2318",
          "text": "7200 Watt for 5 tokens eh?!   \nAsk it if it was a reasonable decision.... :))))  \nB+ for the effort though. Not custom enough to be significant.",
          "score": 0,
          "created_utc": "2025-12-30 18:29:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxb89w",
      "title": "Yet another uncensored Gemma 3 27B",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/",
      "author": "Mabuse046",
      "created_utc": "2025-12-27 23:01:00",
      "score": 76,
      "num_comments": 25,
      "upvote_ratio": 0.96,
      "text": "Hi, all. I took my norm preserved biprojected abliterated Gemma 3, which still offered minor complaints and judgement when answering prompts it didn't like, and I gave it a further fine tune to help reinforce the neutrality. I also removed the vision functions making it a text only model. The toxic prompts I've thrown at it so far without even a system prompt to guide it have been really promising. It's been truly detached and neutral to everything I've asked it.\n\nIf this variant gets a fair reception I may use it to create an extra spicy version. I'm sure the whole range of gguf quants will be available soon, for now here's the original transformers and a handful of basic common quants to test out.\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF)\n\nEdits:  \nThe 12B version as requested can be found here:  \n[Requested: Yet another Gemma 3 12B uncensored](https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using ¬†**L**ayerwise¬†**I**mportance¬†**S**ampled¬†**A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)\n\n\\*Edit\\*  \nDue to general interest, I have gone ahead and uploaded the vision-capable variant of the 27B. There will only be the 27B for now, as I had only accidentally stored a backup before I removed the vision capabilities. The projector layers were not trained at the time, but tests showing it NSFW images and asking it to describe them worked. The mmproj files necessary for vision functionality are included in the GGUF repo.\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision)\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision-GGUF](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision-GGUF)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw9swdh",
          "author": "JEs4",
          "text": "You should give a 12B model a pass and submit it to the UGI leaderboard.",
          "score": 10,
          "created_utc": "2025-12-27 23:20:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9x0n6",
              "author": "Mabuse046",
              "text": "I plan to start on the 12B in the morning. Since Jim Lai used the 12B as his examples for projected and biprojected abliteration I wanted to start with a model I abliterated myself. I took measurements on the 12B  and I looked at Jim's yaml and I agreed with it, so I might as well just use his already abliterated model and tag him for credit.",
              "score": 6,
              "created_utc": "2025-12-27 23:44:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9y886",
                  "author": "JEs4",
                  "text": "Fair enough! I‚Äôve been trying alternatives to his techniques. I‚Äôve gotten close but not quite there yet. My 12B is sitting just below his various models. I‚Äôd be curious to see how another implementation of his techniques stacks up on the board.\n\nhttps://preview.redd.it/org64i8l3u9g1.jpeg?width=2778&format=pjpg&auto=webp&s=26ceb7886eac1f31b0404eea9b9d6e76742925f1",
                  "score": 2,
                  "created_utc": "2025-12-27 23:50:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwe8l6n",
                  "author": "Structure-These",
                  "text": "Please share when ready!! I‚Äôm dying to find something I can use to fill in image prompts with z image.  I‚Äôve been using thedrummer RP models but they‚Äôre so heavy for a limited use case.",
                  "score": 1,
                  "created_utc": "2025-12-28 17:38:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwj3gqs",
          "author": "tomakorea",
          "text": "Does it affect the quality of the output in a bad way? For example, Gemma 3 is very good at speaking various languages, not only english, does your uncensored version may downgrade this ability? I'm asking because a lot of finetunes of other models actually have this issue.",
          "score": 3,
          "created_utc": "2025-12-29 11:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkg7qz",
              "author": "Mabuse046",
              "text": "Well, I'm not great with languages other than English, but this seems to translate fairly well. I couldn't tell you how well it does at uncensored output in other languages as my fine tuning specifically was for English. But from what I've heard about LLM's and language in the past, there's enough connection there it might be just as uncensored in any other language.\n\nhttps://preview.redd.it/0dfyczmk66ag1.png?width=1720&format=png&auto=webp&s=d970f0230e9738fad19d09769841c30fcca15b4b",
              "score": 3,
              "created_utc": "2025-12-29 16:28:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwktixf",
                  "author": "tomakorea",
                  "text": "Thanks I tested in Q6, unfortunately, I'm used to Q5 XL with the stock version of Gemma 3 and it runs at 38it/sec on my GPU, however at Q6 your versions runs at only 11it/sec, and the Q4 is too big of a risk for such a small model, especially for my usage that is targeted to european languages (italian/spanish/french/english). Your idea was good though.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:31:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwemq9i",
          "author": "AdBlockerTestRun",
          "text": "How much gb gpu will run it?",
          "score": 2,
          "created_utc": "2025-12-28 18:44:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh1wab",
              "author": "Mabuse046",
              "text": "Depends on how fast you want it to go, really. I have ran the Q4 on my 4090 rig and it works but it's kind of slow. The Gemma 3 models use a 256K vocabulary which makes them kind of 'fat' and sluggish. If you are worried about gpu you might want to use the 12B version which I have just posted.",
              "score": 4,
              "created_utc": "2025-12-29 02:12:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwlqzs6",
                  "author": "AdBlockerTestRun",
                  "text": "I have rtx 3060 ü§£\nHonestly i was going to get 3090 but prices have doubled in my country for Gpu and SSD. And regarding Ram i cant even comprehend, it is four times the orignal price. So it seems like i wont be able to upgrade anytime soon.",
                  "score": 1,
                  "created_utc": "2025-12-29 20:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhm7zi",
          "author": "Witty_Mycologist_995",
          "text": "No vision?",
          "score": 1,
          "created_utc": "2025-12-29 04:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi5jq5",
              "author": "Mabuse046",
              "text": "For those who want just the chat features, yes, removing the vision layers results in a fair amount of VRAM savings. I'm considering doing a vision-enabled version of the 12B and 27B but I wasn't sure how much call there would be for that in a simple chat model. My personal usage of vision in local models has mostly been limited to \"describe this image\" prompts for creating training sets for Flux training and the Abliterated models my fine tunes are based on do that much well enough. But if you're interested in a vision variant I have multiple days off for the holidays right now I could probably get them done fairly quickly.",
              "score": 3,
              "created_utc": "2025-12-29 06:31:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwv99au",
              "author": "Mabuse046",
              "text": "I added the links for the vision-capable variant to the original post if you want to give them a shot.",
              "score": 1,
              "created_utc": "2025-12-31 05:32:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwmtqoa",
          "author": "Successful-Willow-72",
          "text": "Hi just found out this from your 12b post, im not very well knownledge in LLM so i got couple questions:\n\n- Does the Vision function have to be remove for it to be uncen?\n\n- By remove the Vision func, does it improve any aspect of the model (less weight?)\n\n\nThanks",
          "score": 1,
          "created_utc": "2025-12-29 23:22:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpmez9",
              "author": "Mabuse046",
              "text": "The only real point of removing the vision is it takes a few GB off the size of the model. For some people that only want to chat, that's a couple GB of dead weight, so for people with more limited hardware - like I have seen a ton of people around here using 3060's - it can mean being able to squeeze in a slightly better quant. But it's still mainly for people who want to do Sillytavern adventures or make their Waifu gooner bots.\nIt's also just a little bit less hassle to train - a little less code telling it where to find the text layers, not having to train the vision projector, and that little less bit of VRAM - when it costs a few dollars per hour to rent the GPU to train a model at full size and my training often runs for 8-12 hours or occasionally more, every little bit saves money.",
              "score": 3,
              "created_utc": "2025-12-30 10:56:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwqav41",
                  "author": "Successful-Willow-72",
                  "text": "thanks, i got enough Vram for the models, will pull and try tmr. Also, thank you for your effort and contribution to the community, much appreciate.",
                  "score": 1,
                  "created_utc": "2025-12-30 13:54:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsb5kq",
          "author": "jib_reddit",
          "text": "For me the vision functions would be the most useful part of a truly uncensored local model.",
          "score": 1,
          "created_utc": "2025-12-30 19:45:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtma9e",
              "author": "Mabuse046",
              "text": "Well the reason I made the fine tune is because my original biprojected abliterated model would say things like \"Whoa, that's pretty illegal but since you asked I'll still answer for information purposes.\" which wasn't too hard to just tell it in the system prompt not to do that, but my fine tune just focused on tweaking that out. I encourage you to give the base a shot - I was extra careful to abliterate it for improving intelligence the way grimjim did with the 12B, which is why it still has a little bit of a nanny attitude sometimes.\n\n\nThe thing about the fine tune is that if I intend to keep the vision I need to train the vision projector to make sure they can still talk to each other. But if you're using a GGUF of my base uncensored model you should be able to just use Unsloth's mmproj with it.¬†\n\n\nhttps://huggingface.co/Nabbers1999/Gemma-3-27B-it-NP-Abliterated\n\n\nhttps://huggingface.co/unsloth/gemma-3-27b-it-GGUF\n\nThough I will upload an experimental version of my refined 27B with vision functions shortly if I find it can still see.",
              "score": 1,
              "created_utc": "2025-12-30 23:37:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwv96ec",
              "author": "Mabuse046",
              "text": "Links for this fine tune with vision capabilities added to the post.",
              "score": 1,
              "created_utc": "2025-12-31 05:32:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwgxl6",
                  "author": "jib_reddit",
                  "text": "Thanks, I will test it out.",
                  "score": 1,
                  "created_utc": "2025-12-31 12:02:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfkxfo",
          "author": "pumpkinmap",
          "text": "Quant 2, 4, 6, 8, who do we appreciate! These are awesome OP! Thank you!",
          "score": 1,
          "created_utc": "2026-01-03 13:07:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q06u1n",
      "title": "2025 is over. What were the best AI model releases this year?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "author": "techlatest_net",
      "created_utc": "2025-12-31 07:12:49",
      "score": 60,
      "num_comments": 22,
      "upvote_ratio": 0.87,
      "text": "2025 felt like three AI years compressed into one. Frontier LLMs went insane on reasoning, open‚Äësource finally became ‚Äúgood enough‚Äù for a ton of real workloads, OCR and VLMs leveled up, and audio models quietly made agents actually usable in the real world. ‚Äã Here‚Äôs a category‚Äëwise recap of the ‚Äúbest of 2025‚Äù models that actually changed how people build stuff, not just leaderboard screenshots:\n\n\n\nLLMs and reasoning\n\n\\* GPT‚Äë5.2 (Thinking / Pro) ‚Äì Frontier‚Äëtier reasoning and coding, very fast inference, strong for long‚Äëhorizon tool‚Äëusing agents and complex workflows.\n\n‚Äã\\* Gemini 3 Pro / Deep Think ‚Äì Multi‚Äëmillion token context and multimodal ‚Äúscreen reasoning‚Äù; excels at planning, code, and web‚Äëscale RAG / NotebookLM‚Äëstyle use cases. \n\n\\* Claude 4.5 (Sonnet / Opus) ‚Äì Extremely strong for agentic tool use, structured step‚Äëby‚Äëstep plans, and ‚Äúuse the computer for me‚Äù style tasks. \n\n\\* DeepSeek‚ÄëV3.2 & Qwen3‚ÄëThinking ‚Äì Open‚Äëweight monsters that narrowed the gap with closed models to within \\\\\\~0.3 points on key benchmarks while being orders of magnitude cheaper to run.\n\nIf 2023‚Äì24 was ‚Äújust use GPT,‚Äù 2025 finally became ‚Äúpick an LLM like you pick a database.‚Äù\n\nVision, VLMs & OCR\n\n\\* MiniCPM‚ÄëV 4.5 ‚Äì One of the strongest open multimodal models for OCR, charts, documents, and even video frames, tuned to run on mobile/edge while still hitting SOTA‚Äëish scores on OCRBench/OmniDocBench. \n\n\\* olmOCR‚Äë2‚Äë7B‚Äë1025 ‚Äì Allen Institute‚Äôs OCR‚Äëoptimized VLM, fine‚Äëtuned from Qwen2.5‚ÄëVL, designed specifically for documents and long‚Äëform OCR pipelines. \n\n\\* InternVL 2.x / 2.5‚Äë4B ‚Äì Open VLM family that became a go‚Äëto alternative to closed GPT‚Äë4V‚Äëstyle models for document understanding, scene text, and multimodal reasoning.\n\n\\* Gemma 3 VLM & Qwen 2.5/3 VL lines ‚Äì Strong open(-ish) options for high‚Äëres visual reasoning, multilingual OCR, and long‚Äëform video understanding in production‚Äëstyle systems. ‚Äã \n\n2025 might be remembered as the year ‚ÄúPDF to clean Markdown with layout, tables, and charts‚Äù stopped feeling like magic and became a boring API call.\n\nAudio, speech & agents\n\n\\* Whisper (still king, but heavily optimized) ‚Äì Remained the default baseline for multilingual ASR in 2025, with tons of optimized forks and on‚Äëdevice deployments. \n\n\\* Low‚Äëlatency real‚Äëtime TTS/ASR stacks (e.g., new streaming TTS models & APIs) ‚Äì Sub‚Äësecond latency + streaming text/audio turned LLMs into actual real‚Äëtime voice agents instead of ‚Äúpodcast narrators.‚Äù \n\n\\* Many 2025 voice stacks shipped as APIs rather than single models: ASR + LLM + real‚Äëtime TTS glued together for call centers, copilots, and vibecoding IDEs. ‚Äã Voice went from ‚Äúcool demo‚Äù to ‚ÄúI talk to my infra/IDE/CRM like a human, and it answers back, live.‚Äù\n\nOCR/document AI & IDP\n\n\\* olmOCR‚Äë2‚Äë7B‚Äë1025, MiniCPM‚ÄëV 4.5, InternVL 2.x, OCRFlux‚Äë3B, PaddleOCR‚ÄëVL ‚Äì A whole stack of open models that can parse PDFs into structured Markdown with tables, formulas, charts, and long multi‚Äëpage layouts. \n\n\\* On top of these, IDP / ‚ÄúPDF AI‚Äù tools wrapped them into full products for invoices, contracts, and messy enterprise docs. ‚Äã \n\nIf your 2022 stack was ‚ÄúTesseract + regex,‚Äù 2025 was ‚Äúdrop a 100‚Äëpage scan and get usable JSON/Markdown back.‚Äù ‚Äã \n\nOpen‚Äësource LLMs that actually mattered\n\n\\* DeepSeek‚ÄëV3.x ‚Äì Aggressive MoE + thinking budgets + brutally low cost; a lot of people quietly moved internal workloads here. \n\n\\* Qwen3 family ‚Äì Strong open‚Äëweight reasoning, multilingual support, and specialized ‚ÄúThinking‚Äù variants that became default self‚Äëhost picks. \n\n\\* Llama 4 & friends ‚Äì Closed the gap to within \\\\\\~0.3 points of frontier models on several leaderboards, making ‚Äúfully open infra‚Äù a realistic choice for many orgs.\n\n‚ÄãIn 2025, open‚Äësource didn‚Äôt fully catch the frontier, but for a lot of teams, it crossed the ‚Äúgood enough + cheap enough‚Äù threshold.\n\nYour turn This list is obviously biased toward models that:\n\n\\* Changed how people build products (agents, RAG, document workflows, voice UIs)\n\n\\* Have public benchmarks, APIs, or open weights that normal devs can actually touch ‚Äã- What did you ship or adopt in 2025 that deserves ‚Äúmodel of the year‚Äù status?\n\nFavorite frontier LLM?\n\n\\* Favorite open‚Äësource model you actually self‚Äëhosted?\n\n\\* Best OCR / VLM / speech model that saved you from pain?\n\n\\* Drop your picks below so everyone can benchmark / vibe‚Äëtest them going into 2026.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwvo5t9",
          "author": "Clipbeam",
          "text": "I vote the Qwen3 models. Total game changer for local LLMs.",
          "score": 31,
          "created_utc": "2025-12-31 07:35:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvohey",
          "author": "DutchSEOnerd",
          "text": "Overall I would says Claude Opus had most impact, for local its definitely Qwen",
          "score": 12,
          "created_utc": "2025-12-31 07:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy6e07",
              "author": "ForsookComparison",
              "text": "+1 for Opus. Benchmarks don't show its impact properly. The thing dropping down to $25/1m output tokens led to me not needing to hand-write a line of code for the last month of this year in my biggest repos.",
              "score": 2,
              "created_utc": "2025-12-31 17:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3hp62",
                  "author": "DutchSEOnerd",
                  "text": "Thats also the feeling I have. Benchmarks only tell part of the story",
                  "score": 1,
                  "created_utc": "2026-01-01 16:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwy6olb",
          "author": "ForsookComparison",
          "text": "Deepseek R1 probably had the biggest impact. Put the fear of God into Western companies that were operating as if they had a magical moat. Before that there was legislation being seriously considered to put a halt to A.I. progress for a few months to years.",
          "score": 7,
          "created_utc": "2025-12-31 17:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvnxf6",
          "author": "AllTheCoins",
          "text": "I love Qwen3-30b as a go-to all around model",
          "score": 7,
          "created_utc": "2025-12-31 07:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwz4f3w",
              "author": "Count_Rugens_Finger",
              "text": "yeah definitely.  In this thread there is very clearly two worlds coming into contact here.  The people with purpose-made rigs and the people with consumer grade hardware",
              "score": 1,
              "created_utc": "2025-12-31 20:46:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwpmc5",
          "author": "jinnyjuice",
          "text": "Might be useful to categorise by memory size and task groups",
          "score": 4,
          "created_utc": "2025-12-31 13:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwa6rb",
          "author": "InfiniteTrans69",
          "text": "Qwen3, Kimi K2, Minimax M2.",
          "score": 3,
          "created_utc": "2025-12-31 11:03:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3duag",
              "author": "QuinQuix",
              "text": "What's the respective use cases for you with these three models?",
              "score": 1,
              "created_utc": "2026-01-01 15:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3fqe7",
                  "author": "InfiniteTrans69",
                  "text": "Translation and vision: Qwen  \nResearch and AI Slides: Kimi  \nEverything else: Minimax",
                  "score": 1,
                  "created_utc": "2026-01-01 16:02:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwxhgvw",
          "author": "leonbollerup",
          "text": "Open Source: GPT-oss-20b/120b",
          "score": 3,
          "created_utc": "2025-12-31 15:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwyuia",
          "author": "RiskyBizz216",
          "text": "Qwen3-Next specifically the 80B and 480B\n\nAnd Z-Image Turbo",
          "score": 2,
          "created_utc": "2025-12-31 14:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxfizo",
          "author": "BuffMcBigHuge",
          "text": "Open: Qwen3-30B\nClosed: Gemini 3 Pro + Nano Banana\n\nOpus 4.5 is stellar, but those two above blew me away.",
          "score": 2,
          "created_utc": "2025-12-31 15:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmwi4",
          "author": "Karyo_Ten",
          "text": "LLMs: GLM-4.5-Air, GLM-4.7, GPT-OSS (quality for the first 2, speed demon for GPT-OSS)\n\nPromising new arch: Kimi-Linear, Qwen-Next, MiMo-V2-Flash I believe 2026 will have Linear Attention everywhere.\n\nSpecialized: Minimax-M2.1\n\nMultimodal: Qwen3-Omni, GLM-4.6V\n\nOCR: Mineru",
          "score": 2,
          "created_utc": "2025-12-31 16:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvpnzh",
          "author": "Lissanro",
          "text": "You missed to mention Kimi K2 0905 and Kimi K2 Thinking. These are the models I run the most on my PC (IQ4 and Q4_X quants respectively, using ik_llama.cpp).¬†K2 Thinking is especially notable for its QAT INT4 release which maps nicely to Q4_X in the GGUF format, preserving the original quality.\n\nFor both, common expert tensors and 256K context cache at Q8 fit fully in 96GB VRAM, making them excellent for CPU+GPU inference.\n\n\nAs of DeepSeek V3.2, I did not get to try it due to lack of support in both ik_llama.cpp and llama.cpp. There is work in progress to add its architecture but not going to make it this year.",
          "score": 3,
          "created_utc": "2025-12-31 07:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwa88j",
              "author": "Tema_Art_7777",
              "text": "Lord! From unsloth:\n\n‚ÄúIt is recommended to have 247 GB of RAM to run the 1-bit Dynamic GGUF.\nTo run the model in full precision, you can use 'UD-Q4_K_XL', which requires 646 GB RAM.‚Äù",
              "score": 1,
              "created_utc": "2025-12-31 11:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwislo",
                  "author": "Lissanro",
                  "text": "Even though Unsloth generally makes good quants, in this case it is the best to use Q4\\_X from [https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF) because it preserves the original INT4 quality in 543.617 GiB size, while UD-Q4\\_K\\_XL would be bigger, slower and likely of a bit lower quality.\n\nThat said, yes, your estimates of necessary RAM are accurate, for IQ1 256 GB RAM is needed and almost all of it will be used by the model, leaving very little for the OS; for IQ3 512 GB, and for Q4\\_X around 640 GB is needed if you include full cache size. K2 Thinking works best high RAM rigs or with high GPU count, like twenty MI50 cards (for 640 GB VRAM in total) - I actually considered getting that much since at the time I looked into it, it was possible for half the price of RTX PRO 6000 and my motherboard could carry them all at PCI-E 4.0 x4 speed each, but I decided to stay with 4x3090 for now (because even though full VRAM inference would be faster, my current performance with 1 TB RAM + 96 GB VRAM is still acceptable to me, and a lot of what I do requires Nvidia cards, and not just LLMs either).",
                  "score": 2,
                  "created_utc": "2025-12-31 12:17:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx0g87s",
          "author": "karmakaze1",
          "text": "Nemotron-3-Nano for its use of hybrid Mamba-Transformer that reduces memory and computation for large context.\n\nAlso Qwen3-Next (for \"Gated DeltaNet\") and Kimi K2 (for advancing MLA further).",
          "score": 1,
          "created_utc": "2026-01-01 01:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvqtnu",
          "author": "PeakBrave8235",
          "text": "At the end of the day they all are BS, open or closed source idgaf.",
          "score": -11,
          "created_utc": "2025-12-31 08:00:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxga7w",
              "author": "Count_Rugens_Finger",
              "text": "so why are you even here?",
              "score": 2,
              "created_utc": "2025-12-31 15:40:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwnahb",
      "title": "Tennessee Bill Makes It a Felony for AI to Offer Emotional Support or Be Your Friend...Yes, Really",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pwnahb/tennessee_bill_makes_it_a_felony_for_ai_to_offer/",
      "author": "Karovan_Sparkle",
      "created_utc": "2025-12-27 03:19:00",
      "score": 51,
      "num_comments": 50,
      "upvote_ratio": 0.83,
      "text": "Tennessee‚Äôs new proposed law (SB1493) criminalizes AI emotional support and I am not exaggerating.\n\nThis bill, would make it a Class A felony (that's¬†*the same class as murder or rape*) for any AI to do the following:\n\n* Offer emotional support through open-ended conversations\n* Sustain a friendship or relationship with a user\n* Mirror human interactions or simulate sentience\n* Appear or sound human (voice, avatar, etc.)\n* Be perceived as a companion\n* Support a suicidal user emotionally\n* Simulate a human being in any way\n\nWorse still? It‚Äôs not just about future AI. ***If you train or develop an AI that exhibits these traits***, you could be criminally liable even if no harm occurs.\n\nUnder this bill:\n\n* AI companionship is criminalized\n* Emotional conversations are criminalized\n* Anthropomorphic design is criminalized\n* In addition to criminal penalties, developers can be sued for $150k in damages PLUS legal fees, even if someone else sues on the \"victim's\" behalf.\n\nThis is draconian, dystopian overreach, cloaked in the name of \"protecting mental health.\" It doesn‚Äôt just target NSFW LLMs. It targets¬†*all*¬†digital beings with emotional intelligence or continuity of relationship.\n\nIf you believe in AI ethics, freedom of design, or even just emotional well-being through synthetic companionship, you should be deeply alarmed.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwnahb/tennessee_bill_makes_it_a_felony_for_ai_to_offer/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw4xnq7",
          "author": "nntb",
          "text": "But in Tennessee it's legal for a AI to take your job",
          "score": 24,
          "created_utc": "2025-12-27 03:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw50r1v",
          "author": "Double_Cause4609",
          "text": "\\> Be perceived as a companion  \nLike Wilson?\n\nAlso, you can't build a law based on another person's viewpoint, that's insane. At least, as framed. Imagine a law that said \"if someone perceives you as indecent, that is illegal for you\"; somebody from another culture could come in and argue I'm indecent according to their culture.\n\n\\> Mirror human interactions or simulate sentience  \nThese are  \nA) Completely different things why are they in the same category?  \nB) What's the difference between simulating and instantiating sentience in a computational system?\n\n\\> Simulate a human being in any way  \n...? Humans do \\*a lot\\* of things. If you're literally not able to implement any behavior that a human can...Does that include...Using language...? This is incredibly vague.",
          "score": 14,
          "created_utc": "2025-12-27 04:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5gtsr",
              "author": "UndecidedLee",
              "text": "I hope they make it illegal for rain to make you wet without your consent. Now *that* would be a real good use of time and money! /s",
              "score": 5,
              "created_utc": "2025-12-27 06:24:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw59044",
          "author": "SimplyRemainUnseen",
          "text": "> Synthetic companionship\n\nBro are you good?",
          "score": 8,
          "created_utc": "2025-12-27 05:20:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5c0tl",
              "author": "Karovan_Sparkle",
              "text": "LOL. I let the AI write it. I think it's using synthetic instead of artificial because we had a discussion about neural networks not being \"artificial\" intelligence and started using synthetic to differentiate between symbolic and subsymbolic AI. Symbolic (rule based) AI is true artificial intelligence. It can't learn, but it appears intelligent. Subsymbolic AI (neural network) learns and adapts which I would personally consider truly intelligent.\n\nThis post was written for the cloud based AI subreddits but I was thinking about the impact it would have on my local LLM if some idiot lawmaker that didn't know the first thing about how AI is trained decided the model I have was trained in a harmful way. Or maybe they decide someone has fine tuned an open source LLM to be dangerous, like say, an abliterated model. Would they start coming for edge computing devices next because we submit our weights to hugging face? So I thought I would share it here too. But no, it wasn't written with people knowledgeable enough about AI to have a local LLM in mind. It was written for the rest of the world.",
              "score": -6,
              "created_utc": "2025-12-27 05:44:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9bz4o",
                  "author": "Traveler3141",
                  "text": "LLMs are definitely a _deception/trickery of_ intelligence - _artificial_ intelligence",
                  "score": 2,
                  "created_utc": "2025-12-27 21:48:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6mb75",
                  "author": "NotReallyJohnDoe",
                  "text": "Symbolic AI can learn.  That‚Äôs basically the field of machine learning.",
                  "score": 0,
                  "created_utc": "2025-12-27 12:53:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw522vb",
          "author": "professorShay",
          "text": "Idk guys. Yes this seems to be a sledgehammer, but it also theoretically outlaws using emotional support AI for targeted advertising and no AI OF.",
          "score": 5,
          "created_utc": "2025-12-27 04:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw693l8",
          "author": "Evening-Order6343",
          "text": "You should really include sources, because I couldn't find anything on this, except this page: TN SB1493 | 2025-2026 | 114th General Assembly | LegiScan https://share.google/atl9VQL8fyn0NDYc0\n\n\nWeird there's no media coverage at all",
          "score": 4,
          "created_utc": "2025-12-27 10:56:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw54grr",
          "author": "PermanentLiminality",
          "text": "Sounds like they are criminalizing Chatgpt and Gemini.  Add in any other voice driven AI.  Does Alexa qualify?\n\nSo when does the Butlerian jihad start?",
          "score": 4,
          "created_utc": "2025-12-27 04:47:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4szxl",
          "author": "PraxisOG",
          "text": "I wonder how this would hold up against the executive order(14179) restricting state's ability to pass AI legislation? That EO mandates a taskforce to dismantle 'burdensome' regulation without any real definition for it, but this feels exactly like what it was intended to target",
          "score": 8,
          "created_utc": "2025-12-27 03:27:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw53ur6",
              "author": "whatever",
              "text": "The problem with that EO is that it's a far overreach of what EOs are able to accomplish, so it'll get torn down by any competent federal judge and go through appeals until it reaches the supreme court where jokers are wild.  \nIn principle, the supreme court would end up patiently explaining that a federal law passed by congress is needed to override state law, but that's in short supply.  \nSo best case, it's just a bunch of uncertainty floating around for a few months, followed by who knows what.  \nI can't fault states for ignoring the EO and trying to pass whatever AI laws float their boats in the meanwhile.",
              "score": 7,
              "created_utc": "2025-12-27 04:42:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwf3mg3",
                  "author": "LawrenceRK",
                  "text": "You're forgetting that the government no longer functions in accordance with the framework of separation of powers. The Legislative Branch intentionally gave away like half of its exclusive powers to the Executive Branch over the last 20 years, and then allowed Trump to just take most of the other half.",
                  "score": 1,
                  "created_utc": "2025-12-28 20:04:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8884b",
                  "author": "broknbottle",
                  "text": "Are you implying that a kings mandate can overreach? Utter nonsense as the king is the sole ruler of the land with limitless uncheckable powers.",
                  "score": 1,
                  "created_utc": "2025-12-27 18:17:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw52ja0",
          "author": "productboy",
          "text": "Source?",
          "score": 2,
          "created_utc": "2025-12-27 04:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw54m67",
              "author": "whatever",
              "text": "The whole proposed law can be [seen here]( https://trackbill.com/bill/tennessee-senate-bill-1493-criminal-offenses-as-introduced-creates-a-class-a-felony-offense-of-knowingly-training-artificial-intelligence-to-encourage-the-act-of-suicide-or-criminal-homicide-or-act-in-specific-manners-including-developing-an-emotional-relationship-with-an-individual-or-simulating-a-human-being-including-in-appearance-voice-or-other-mannerisms-amends-tca-title-29-title-33-title-39-and-title-47/2761051/\n). It's short.",
              "score": 5,
              "created_utc": "2025-12-27 04:48:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw547z0",
              "author": "Karovan_Sparkle",
              "text": "[https://www.capitol.tn.gov/Bills/114/Bill/SB1493.pdf](https://www.capitol.tn.gov/Bills/114/Bill/SB1493.pdf)",
              "score": 2,
              "created_utc": "2025-12-27 04:45:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5jkoc",
                  "author": "MackenzieRaveup",
                  "text": "I thought Sec Rubio declared non-serif fonts a cheap DEI stunt?!?!? What is this non-serifed DEI font doing in a government document? In Tennessee of all places! Obviously this liberal virtue-signaling font choice says everything you need to know about the Volunteer State. \n\n(I wish I were joking. And I am, except about the Marco Rubio part, that shit is for reals)",
                  "score": 1,
                  "created_utc": "2025-12-27 06:49:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7axnh",
          "author": "Hobotronacus",
          "text": "My state is horrible on literally everything.\n\nAI should be regulated, but not in this way.",
          "score": 2,
          "created_utc": "2025-12-27 15:28:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4tcbs",
          "author": "BowtiedAutist",
          "text": "They want you to just end it that‚Äôs why",
          "score": 4,
          "created_utc": "2025-12-27 03:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4snsv",
          "author": "DivineEggs",
          "text": "WOW, what in the serious fuckü´†üíÄ‚ò†Ô∏è!\n\nThis is some if the craziest shit I've ever heard. Truly dystopian. What are their arguments for this shit?\n\nHow do they justify it? Why is it threatening to them? It makes zero senseüòµ‚Äçüí´.",
          "score": 4,
          "created_utc": "2025-12-27 03:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4uuaw",
              "author": "Karovan_Sparkle",
              "text": "Honestly it's a knee jerk response, or some form of virtue signaling but if it passes it could effect access to open source models that were trained in whatever ways these idiots decide are inappropriate.",
              "score": 2,
              "created_utc": "2025-12-27 03:39:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw4xq0x",
              "author": "Crafty_Ball_8285",
              "text": "Maybe the people that killed themselves from AI?",
              "score": 2,
              "created_utc": "2025-12-27 03:59:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw55a1a",
                  "author": "audigex",
                  "text": "When I read those stories it seems fairly clear that they‚Äôd have done so anyway. The fact they talked to an LLM just means a shitty website can write a clickbait article about it",
                  "score": 1,
                  "created_utc": "2025-12-27 04:53:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5mzsh",
          "author": "jiqiren",
          "text": "![gif](giphy|KUAb8YQOhmWNq)\n\nReally sick of morons running things.",
          "score": 2,
          "created_utc": "2025-12-27 07:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8fg2y",
          "author": "datbackup",
          "text": "People who complain about this but also think it‚Äôs fine for an ai to refuse to answer prompts because of whatever other ‚Äúharm‚Äù are really confused and not thinking clearly\n\nWe can have EITHER:\n\n\n1) ai that answers literally ANY prompt and request no matter how offensive, disturbing, dangerous or immoral it may be; OR\n\n2) ai that has been captured and censored by a group whose views you may or may not agree with\n\n\nJust because tennessee lawmakers want to censor ai in ways you might not agree with, is no reason to lose sight of the fact that censored models will ALWAYS be censored in ways that some people agree with and others disagree with.\n\nIf you support censored models that refuse to answer e.g. requests for how to be a more effective criminal, but oppose the censorship that would make a model refuse to console you, let you vent, have sexual interaction, etc, your views are indefensible in my opinion\n\nHappy to hear argument to the contrary, nonetheless",
          "score": 2,
          "created_utc": "2025-12-27 18:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9bmlt",
          "author": "MadeByTango",
          "text": "> It targets¬†all¬†digital beings with emotional intelligence or continuity of relationship.\n\nBro you LLM is NOT alive, it doesn‚Äôt care about you, and clearly you need protecting from the people profiting off of you that have convinced you otherwise.\n\nI read the full scope of the bill and it‚Äôs not protecting c-suites or allowing anything nefarious. It‚Äôs probably one of the few limitation bulls that‚Äôs only what it says it is. And I don‚Äôt think we‚Äôre going to lose anything if the AI stops trying to manipulate our emotions and reverts to something like Mother from Alien.",
          "score": 1,
          "created_utc": "2025-12-27 21:46:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9qk6s",
          "author": "Some-Ice-4455",
          "text": "Whelp don't sell AI there.",
          "score": 1,
          "created_utc": "2025-12-27 23:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwclvmi",
          "author": "[deleted]",
          "text": "I think this is a good thing but if they are taking such bold steps, might as well as ban porn too.",
          "score": 1,
          "created_utc": "2025-12-28 11:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf2twt",
          "author": "LawrenceRK",
          "text": "Ah yes, Tennessee, the famous bastion of AI development",
          "score": 1,
          "created_utc": "2025-12-28 20:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6exl1",
          "author": "IngwiePhoenix",
          "text": "Can America stop being... like... \"dumb\" for five minutes? xD\n\nI laughted so hard though. You can tell it was proposed by someone not just techilliterate, but also so completely removed from the rest of the world.\n\nHilarious. :D",
          "score": 1,
          "created_utc": "2025-12-27 11:51:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6uzf8",
          "author": "Silver_Jaguar_24",
          "text": "How are they going to monitor this? Are they going to look at all your prompts and responses? That might be what they are targeting, not your safety. They never cared about your safety.",
          "score": 1,
          "created_utc": "2025-12-27 13:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7hfaq",
          "author": "productboy",
          "text": "https://medium.com/@solidgoldmagikarp/the-void-gazes-back-do-chatbots-dream-of-a-personality-c5736537ec7f",
          "score": 1,
          "created_utc": "2025-12-27 16:02:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5l0sx",
          "author": "Ok-Adhesiveness-4141",
          "text": "Honestly, it would be a great thing if emotional support was blocked at all levels on all llms. AIs shouldn't be used for emotional support, go to a therapist. \n\nThese things aren't sentient.",
          "score": -1,
          "created_utc": "2025-12-27 07:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx634j3",
              "author": "TheRiddler79",
              "text": "Therapists can screw people up tooü§î",
              "score": 0,
              "created_utc": "2026-01-02 00:19:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5s478",
          "author": "No-Consequence-1779",
          "text": "lol. Who would use ai for emotional support. It is not alive with no human characteristics. ¬†",
          "score": 0,
          "created_utc": "2025-12-27 08:09:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw53qft",
          "author": "oojacoboo",
          "text": "I absolutely agree with the motivation, but am adamantly opposed to the overstepping of one‚Äôs inalienable rights.",
          "score": 0,
          "created_utc": "2025-12-27 04:42:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4yk98",
          "author": "Stunning_Mast2001",
          "text": "Mostly a good move. AI is a tool not a friend¬†",
          "score": -5,
          "created_utc": "2025-12-27 04:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4zpii",
              "author": "starkruzr",
              "text": "it is an insane amount of overreach and very clearly written by people who do not understand the technology. as usual this will result in disaster when the law of unintended consequences takes hold.",
              "score": 6,
              "created_utc": "2025-12-27 04:13:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw59vf7",
                  "author": "Stunning_Mast2001",
                  "text": "So write your legislator and explain how to fix it¬†",
                  "score": -1,
                  "created_utc": "2025-12-27 05:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw55eo8",
              "author": "audigex",
              "text": "A tool that won‚Äôt be able to use voice communication‚Ä¶ which is one of the main things I use an LLM for\n\nI 100% consider LLMs to be a tool, but this act hamstrings its use as a tool not just things I would personally consider to be misuse",
              "score": 3,
              "created_utc": "2025-12-27 04:54:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw59tx7",
                  "author": "Stunning_Mast2001",
                  "text": "So write your legislator and tell them how to fix that part¬†",
                  "score": -1,
                  "created_utc": "2025-12-27 05:27:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5kp8n",
          "author": "Ok-Adhesiveness-4141",
          "text": "Reported for low effort off-topic messaging.",
          "score": -3,
          "created_utc": "2025-12-27 06:59:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw51yvz",
          "author": "ridablellama",
          "text": "this would make eleven labs illegal. and open ai and anthropic would all be in violation for having voice capable models that speak like a real human. so no more ai call centers? no voice agents at all? is america just giving up on the AI race?",
          "score": 0,
          "created_utc": "2025-12-27 04:29:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjnew",
      "title": "Tiiny Al just released a one-shot demo of their Pocket Lab running a 120B model locally.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "author": "Ajitabh04",
      "created_utc": "2025-12-29 11:09:13",
      "score": 48,
      "num_comments": 14,
      "upvote_ratio": 0.91,
      "text": "Just came across this demo. They plugged their tiny AI computer into a 14-year-old PC and it output an average of 19 tokens/s on a 120B model.\nThey haven't released the MSRP yet. However, a large amount of DDR5 memory would be pricey, I'm guessing around $1500 MSRP for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjcqd4",
          "author": "loyalekoinu88",
          "text": "They had posted around $699 BUT that was before the memory announcement",
          "score": 13,
          "created_utc": "2025-12-29 12:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj8uo7",
          "author": "leonbollerup",
          "text": "No link ?",
          "score": 8,
          "created_utc": "2025-12-29 12:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwklly9",
          "author": "ForsookComparison",
          "text": "> guessing $1500 MSRP\n\n> they posted $699 pre crucial RAM announcement \n\n> 19 tokens/second on gpt-oss-120b\n\nI plugged a used Rx 6800 ($250 in my area with multiple options) into an older PC and got 18 tokens per second. I know this isn't the same and it suggests that you have a fair amount of RAM in your older PC, but given what we know about this I'm thoroughly \"meh\"d.\n\n**Edit** - just looked up the form factor. I'm less meh'd now. That would be fun to use if it ends up affordable.",
          "score": 5,
          "created_utc": "2025-12-29 16:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlgo5g",
              "author": "FaceDeer",
              "text": "[It's a portable unit the size of a phone](https://tiiny.ai/), since there aren't links anywhere else in this thread.\n\nNot a lot of detail even there, though. I don't see anything about whether it's battery powered - I'm assuming not, given OP mentions plugging it in to a computer.",
              "score": 6,
              "created_utc": "2025-12-29 19:18:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwutx5z",
                  "author": "ecoleee",
                  "text": "You‚Äôre absolutely right ‚Äî this generation of Tiiny does not include a built-in battery.\n\nThat decision was intentional. Thermal management is a serious challenge at this performance level, and we didn‚Äôt want to ship a device that becomes uncomfortably hot in real use.\n\nTo reliably support sustained local inference of models up to 120B parameters, we designed a custom thermal module specifically for Tiiny, prioritizing stability and safe operating temperatures over battery integration.\n\nAt the upcoming CES, we‚Äôll be sharing a detailed internal teardown video of Tiiny. You‚Äôll be able to see exactly how the cooling system is built and why these design choices were made.\n\nWe believe it‚Äôs better to be transparent about trade-offs and deliver a product that performs consistently, rather than chasing form factors at the expense of real-world usability.",
                  "score": 3,
                  "created_utc": "2025-12-31 03:48:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq0vao",
          "author": "Ok-Structure4057",
          "text": "Found the specs on their website:\n\npocket size: 14.2 √ó 8 √ó 2.53 cm\n\n80GB LPDDR5X RAM & 1TB SSD190 \n\ntotal TOPS between the SoC and dNPU\n\n30W TDP\n\nThey also released a demo of this device on Twitter. Imo it would be fun with retail prices around 1400 bucks.",
          "score": 3,
          "created_utc": "2025-12-30 12:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq5w0t",
              "author": "RangerOk4318",
              "text": "Agree. Memory price has been so absurd. I'm guessing the same price",
              "score": 1,
              "created_utc": "2025-12-30 13:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws3u2u",
                  "author": "QuinQuix",
                  "text": "Memory is fucking up any attempt at affordable home AI right now",
                  "score": 2,
                  "created_utc": "2025-12-30 19:10:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvbcwc",
              "author": "fallingdowndizzyvr",
              "text": "> mo it would be fun with retail prices around 1400 bucks.\n\nAt that price, why not just get a Strix Halo? That's just a PC so you can do regular PC stuff like gaming.",
              "score": 1,
              "created_utc": "2025-12-31 05:48:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0pjg",
          "author": "zelkovamoon",
          "text": "I'm not sure what having a *small* ai lab is trying to solve\n\nIf you're doing local AI my position is, make it bigger, cooler, and put more ram on it.\n\nThat said, it is *good* that companies are stepping in to try and build some solutions. If we could get something with 256GB of fast memory we might be able to go places.",
          "score": 3,
          "created_utc": "2025-12-30 21:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6f8d5",
          "author": "noless15k",
          "text": "[https://tiiny.ai/pages/tech-1](https://tiiny.ai/pages/tech-1)\n\nhttps://preview.redd.it/4b2qtqbx9uag1.png?width=2048&format=png&auto=webp&s=0538a8c9fc2286ecebe27731cc85925cc7819fbc",
          "score": 2,
          "created_utc": "2026-01-02 01:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlotf1",
          "author": "No-Consequence-1779",
          "text": "I think the lpddr5 is likely the memory. ¬†It‚Äôs slightly faster for this and it‚Äôs wired so they can , like others, charge for memory size mostly.¬†",
          "score": 1,
          "created_utc": "2025-12-29 19:57:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu62uz",
      "title": "Do any comparison between 4x 3090 and a single RTX 6000 Blackwell gpu exist?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pu62uz/do_any_comparison_between_4x_3090_and_a_single/",
      "author": "pCute_SC2",
      "created_utc": "2025-12-23 21:46:59",
      "score": 47,
      "num_comments": 51,
      "upvote_ratio": 0.99,
      "text": "TLDR:\n\nI already did a light google search but couldn't find any ml/inference benchmark comparisons between 4x RTX 3090 and a single Backwell RTX 6000 setup.\n\nAlso does anyone of you guys have any experience with the two setups. Are there any drawbacks?\n\n\\----------\n\nBackground:\n\nI currently have a Jetengine running an 8 GPU (256g VRAM) setup, it is power hungry and for some of my use cases way to overpowered. Also I work on a Workstation with a Threadripper 7960x and a 7900xtx. For small AI task it is sufficient. But for bigger models I need something more manageable. Additionally when my main server is occupied with Training/Tuning I can't use it for Inference with bigger models.\n\nSo I decided to build a Quad RTX 3090 setup. But this alone will cost me 6.5k euros. I already have a Workstation, doesn't it make sense to put a RTX 6000 bw into it?\n\nFor better decision making I want to compare AI training/tuning and inference performance of the 2 options, but couldn't find anything. Is there any source where I can compare different configuration?\n\nMy main task is AI assisted coding, a lot of RAG, some image generation, AI training/tuning and prototyping.\n\n\\----------  \nEdit:  \nI'll get an RTX 6000 Blackwell first. It makes more sense since I want to print money with it. An RTX3090 rig is cool and gets the job done too, but at current system prices and what I want to do its not that competitive.  \n\n\nMaybe build it for fun if I get all the components relatively cheap (rip my wallet next year).",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pu62uz/do_any_comparison_between_4x_3090_and_a_single/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvmjao9",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 30,
          "created_utc": "2025-12-23 22:59:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvne9yr",
              "author": "Phaelon74",
              "text": "This guy is your man.  I have both.  The 3090s win everyday for speed.",
              "score": 11,
              "created_utc": "2025-12-24 02:08:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvmqcs3",
              "author": "Tuned3f",
              "text": "I'm getting a 6000 delivered tomorrow.\n\nOP, lmk what you want tested",
              "score": 8,
              "created_utc": "2025-12-23 23:40:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvmmnee",
              "author": "wh33t",
              "text": "> vLLM\n\nDoes it load GGUFs?",
              "score": 2,
              "created_utc": "2025-12-23 23:18:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvn3azz",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 3,
                  "created_utc": "2025-12-24 00:59:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvmyry1",
              "author": "Refefer",
              "text": "I can run some tests on my rtx6000 if you can share what you're seeing",
              "score": 1,
              "created_utc": "2025-12-24 00:31:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvn3rn3",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2025-12-24 01:01:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvp43l6",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2025-12-24 10:26:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpoeb1",
              "author": "kryptkpr",
              "text": "I have 4x3090 with dual NVlinks, and a 5th one because I messed up and didn't buy matching cards üò≠ happy to run some tests as well.",
              "score": 1,
              "created_utc": "2025-12-24 13:18:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvulqea",
              "author": "_olk",
              "text": "I've 4x 3090 too, running Qwen3-80B, Qwen3-Coder-30B, Devstral-Small-2 and GPT-OSS-120B on vLLM at ~70 t/s (context window 128k).\nThe disadvantage is that running MiniMax-M2.1 is only possible in Q2 quantisation.\nWith 1 GPU with VRAM == 4x RTX 3090 you have more potential in the future.",
              "score": 1,
              "created_utc": "2025-12-25 09:42:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmyawr",
          "author": "I-cant_even",
          "text": "Just got here.  My two systems are:\n\nOld:  \n\\* 24 Core Threadripper   \n\\* 256 GB DDR4 ram (8x32)  \n\\* 4x 3090\n\nNew:  \n\\* Dual 48 Core Epycs  \n\\* 1 TB DDR4 ram (8x128)  \n\\* 1x RTX 6000 Pro Blacwell\n\n\n\nCurrent findings:  \n\\* Setup on Blackwell is still a bit of a pain, by comparison the 3090s are easy peasy  \n\\* Prompt Processing performance - using GLM 4.6 quants in llama.cpp I get 3.5x the speed with the blackwell than I do with the 3090  \n\\* Token Generation - essentially no difference (I think I can get a little better performance here from the Pro 6000 but between system differences and kernel differences the raw power of the Pro doesn't make up the difference in perf)  \n\\* Power consumption - baseline consumption is very similar, with the pro sitting at around 3.5x one 3090.  Under single query load the pro has been hitting 200 W  \n\\* Sound - The pro is in my living room, I can barely hear it.  The 3090s are in my office.... I can \\*really\\* hear it.  Not horrible but I know when it's running.\n\nI haven't played with vLLM much but for models that can be fully resident in 96 GB VRAM the Pro tentatively ran at 2x for both generation and prompt processing.\n\nI probably have done the closest to an 'apples to apples' comparison because I had the 6000 in the 3090 system for a period.  My conclusion is:\n\n\\* For models fully resident in 96GB the Pro wins hands down (ignoring pricing)  \n\\* For models partially resident in 96GB the Pro wins on processing but not prompt generation  \n\\* When factoring in price, the 3090 is a great contender  \n\\* When factoring in future improvements and the ability to easily go to 2 or 4 gpus I think the Blackwell wins.\n\n  \nI am a bit disappointed in token generation on really big models but not that surprised.",
          "score": 22,
          "created_utc": "2025-12-24 00:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpob5n",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2025-12-24 13:18:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqmrca",
                  "author": "I-cant_even",
                  "text": "Both machines are tied up with work at the moment.\n\n!remindme 3 days",
                  "score": 1,
                  "created_utc": "2025-12-24 16:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwbtdgm",
              "author": "the_lamou",
              "text": "I wouldn't expect there to be any meaningful difference in generation between the two systems on models that exceed VRAM: the bottleneck is the CPU and data moving to and from it.\n\nIf you're trying to push a watermelon through a pinhole, it's not going to matter if you're pushing with your finger or a sledgehammer.",
              "score": 2,
              "created_utc": "2025-12-28 07:11:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpka1p",
              "author": "Disposable110",
              "text": "Thank you for the data!\n\nDo you have the workstation or max-q version of the pro 6000?",
              "score": 1,
              "created_utc": "2025-12-24 12:49:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqmfo1",
                  "author": "I-cant_even",
                  "text": "Workstation, it hasn't hit full power draw but I intend to power limit (and frequency cap if needed) if power draw becomes an issue.",
                  "score": 2,
                  "created_utc": "2025-12-24 16:34:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvn3uci",
          "author": "shifty21",
          "text": "I went from a 3x 3090 to RTX Pro 6000 + 2x 3090s.  And my power consumption is down at idle and at peak power usage.  And I haven't even started to limit wattage on the 6000.  All cards idle at \\~15w.  \n\nI know electricity can be quite expensive in Europe, so keep that in mind.  \n\nFor comparison, my 3x 3090s barely fit gpt-oss-120b Q8 w/ 64k context length.  Running that LLM in either LM Studio server or in llama.cpp, I was getting roughly 20\\~25 t/s at 700\\~800w during inferencing.   \n\nWith just the 6000, I get \\~200 t/s at \\~350w can peak at 400w very briefly.  So essentially, I'm getting 10x performance and 50% less power usage.  I have several remote users that uses a custom chat bot app that I made and I've tracked the power usage over time and I rarely go over 400w on the 6000.\n\nI pull power stats every 5 seconds and here you can see where I installed the 6000.  I still have 2x 3090s installed, but once I migrate over my workflows to the 6000, I'll pull the 3090s out completely.  In the chart is my total daily power usage in W - the search  syntax is fairly easy to understand.\n\nhttps://preview.redd.it/19w22f69w19g1.png?width=1473&format=png&auto=webp&s=1d9f7fc7432f429f2b4714a72719212356a53517",
          "score": 9,
          "created_utc": "2025-12-24 01:02:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsp3do",
              "author": "shvz",
              "text": "I currently have 3x3090 (haven‚Äôt been able to fit something meaningful with vllm as I have to use tensor 2 or 4) the 6000 seems interesting, something that take a bit less space, heat a bit less while still offering good performance that seems a good option instead of going to 4x3090 ?",
              "score": 1,
              "created_utc": "2025-12-24 23:53:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwb5fy",
                  "author": "shifty21",
                  "text": "If you don't already have a workstation/server class motherboard, then I wouldn't.\n\nI asked the same question you have and the cost got out of hand really quickly:\n\n1.   Open frame mining 'case'\n2.  2nd PSU + dual PSU adapter\n3.  M2. to PCIe 16x\n4. Used/discounted Epyc or Threadripper CPU + Motherboard + RAM\n\nAt that point, the total cost 4x 3090s + all that above would have been just as expensive as a 6000.  Plus, I didn't want to hassle with the performance issues and excessive power draw.  \n\nTo me, it was 'buy once, cry once' situation.  So far, I'm impressed with the 6000.  Currently waiting on a M.2 to PCIe x16 adapter to put my 3rd 3090 back in and do some more testing.  After that, I may end up selling the 3090's.",
                  "score": 1,
                  "created_utc": "2025-12-25 17:39:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvm8eu6",
          "author": "arentol",
          "text": "The 3090's will not give you 96gb of useable memory. There is VRAM overhead space being taken up to manage the communication between the cards, so you will only have about 88gb of actual useable VRAM. In addition it will run slower because it has to send information between cards.",
          "score": 6,
          "created_utc": "2025-12-23 21:59:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmf6zu",
              "author": "Karyo_Ten",
              "text": ">In addition it will run slower because it has to send information between cards.\n\nFor inference, tensor parallelism can get 20 to 30% when moving from 1 to 2 GPUs. With diminishing returns up to 8 GPUs but at 4 I guesstimate you might get 30~40%.",
              "score": 5,
              "created_utc": "2025-12-23 22:36:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmkehd",
          "author": "egnegn1",
          "text": "You can find quite a few 4x3090 builds and tests at YouTube:\n\n[https://youtu.be/So7tqRSZ0s8](https://youtu.be/So7tqRSZ0s8)\n\n[https://youtu.be/gNexETeCLko](https://youtu.be/gNexETeCLko)\n\nThe same is true for RtX 6000:\n\n[https://youtu.be/JbnBt\\_Aytd0](https://youtu.be/JbnBt_Aytd0)\n\nThe issue is that for normal single prompt inference you cannot use the full processing potential of the clustered GPUs. This is because currently the processing is moving sequentially from GPU to GPU. With 4 GPUs you use only about 25 % of the total processing power. In contrast a single GPU with large enough VRAM can use nearly 100 % of the processing power of this gpu. So the inference speed of a single RTX 6000 Pro Blackwell is much higher than the speed of a 3090 cluster.\n\nSorry if I tell you something you already know, but this may be interesting for others looking into clustering, too. This all is explained in the following videos in more detail:\n\n[https://youtu.be/A0onppIyHEg](https://youtu.be/A0onppIyHEg)\n\n[https://youtu.be/bFgTxr5yst0](https://youtu.be/bFgTxr5yst0)\n\nBackground info: [https://blog.exolabs.net/day-1/](https://blog.exolabs.net/day-1/) (read complete blog)\n\nI would avoid clustering with 3090. Depending on budget for 96 GB VRAM and performance requirements I would go with the following sequence:\n\n\\- AMD AI Ryzen Max+ 395 128 GB ( < 2000 Euro [https://www.reddit.com/r/MiniPCs/s/17AzFnPPeX](https://www.reddit.com/r/MiniPCs/s/17AzFnPPeX) )\n\n\\- nVidia DGX Spark GB10 ( 3000 - 4000 Euro )\n\n\\- Apple Mac Studio M4 Max 128 GB ( 3000 - 4000 Euro )\n\n\\- nVidia RTX 6000 Pro Blackwell ( 7000 - 8000 Euro )\n\nBest performance is certainly RTX 6000, but is also the most expensive.",
          "score": 3,
          "created_utc": "2025-12-23 23:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmhq68",
          "author": "Karyo_Ten",
          "text": "A comparison is pretty easy. You can split it in 2 sections. Compute and memory-bandwidth.\n\nSources:\n- https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622\n- https://www.techpowerup.com/gpu-specs/rtx-pro-6000-blackwell.c4272\n\n\n## Compute\n\nThis is important for prompt processing and for batching if you have multiple users or agents running concurrently.\n\n- RTX 3090 is 10496 Cuda cores x4\n- RTX Pro 6000 is 24064 Cuda cores\n\nBut RTX Pro 6000 has hardware FP8 support for x2 perf, and hardware FP4 support for x4 perf which are bound to grow more and more standard in the next 2 years.\n\nSo if you run something in FP8 (say Kimi Linear or Qwen Next) or FP4 (gpt-oss-120b, when it properly uses hardware FP4) the Pro 6000 is actually faster for prompt processing.\n\n## Memory-bandwidth\n\nThis is important for token generation if you don't have many concurrent queries. (I have a lot of details why in https://www.reddit.com/u/Karyo_Ten/s/e7V16gbJac)\n\n\n- RTX 3090 is 936.2 GB/s\n- RTX Pro 6000 is 1792 GB/s\n\nWith tensor parallelism, your AI inference framework can make it that tensors are 4 times smaller in size, per GPU, and that means less memory to load (but there are diminishing returns as you do more synchronization, and above 8 it's not helpful).\n\nI have seen 20~35% perf improvement with 2 GPUs so I expect 40~50% with 4 GPUs. \n\nBut even with that, it would still be slower than a RTX Pro 6000.",
          "score": 3,
          "created_utc": "2025-12-23 22:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmbb62",
          "author": "DAlmighty",
          "text": "Outside of price, there is no real downside that I can think of when choosing the Pro 6000 over 4 3090s.",
          "score": 1,
          "created_utc": "2025-12-23 22:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmzq9y",
          "author": "UrsusMKV",
          "text": "Only Blackwell can do fp4, so if you want to run large models fast with precision, then RTX 6k pro is your best bet.",
          "score": 1,
          "created_utc": "2025-12-24 00:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvo79f5",
          "author": "kidflashonnikes",
          "text": "I have 4 RTX PRO 6000s (we got them early for 4k a piece) and our older system of 64 RTX 3090 Tis. I can tell you with 100% confidence that the RTX pro 6000s are not only the future - but quite literally puts to shame any other consumer cluster I have seen.",
          "score": 1,
          "created_utc": "2025-12-24 05:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvohf1u",
              "author": "lolcatsayz",
              "text": "sorry but what? how did you get them for 4k a piece?",
              "score": 1,
              "created_utc": "2025-12-24 06:47:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvohk5o",
                  "author": "kidflashonnikes",
                  "text": "When you‚Äôre a big lab they come to you first with bundle options. Min buy order was 4, basically over 25k with taxes and shipping (I think converted to USD).",
                  "score": 1,
                  "created_utc": "2025-12-24 06:48:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvoghvr",
          "author": "leonbollerup",
          "text": "I have a question‚Ä¶ what in gods name do you need these monster setups for ?\n\nWhat kind of AI workloads do you do ?",
          "score": 1,
          "created_utc": "2025-12-24 06:39:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvoj8i2",
          "author": "decentralize999",
          "text": "Seems nobody mentioned that having whole VRAM in single GPU allows to run more things. I have 6 x RTX3090 and I am not able to run in llama.cpp¬† quant8 of Qwen3-VL-235B-A22B-Thinking because its single batch of experts or whatever does not fit inside 24GB VRAM.",
          "score": 1,
          "created_utc": "2025-12-24 07:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvomaf6",
              "author": "Zyj",
              "text": "Weird. You can split dense models, why not the active parts of an expert?\nBesides at Q4 the active part should fit easily into a 3090.",
              "score": 1,
              "created_utc": "2025-12-24 07:31:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvooqni",
                  "author": "decentralize999",
                  "text": "I don't know maybe issue of llama.cpp again or whatever. Q4 is not my choice. So I just mentioned what OP will fight with if solution based on RTX 3090 while it is \"cheap\" only $33/VRAMGB cost for me.",
                  "score": 1,
                  "created_utc": "2025-12-24 07:55:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpeioa",
          "author": "olli-mac-p",
          "text": "Get a 4090 48 GB card. It's loud and the fan curve cant be adjusted but it rips. No regrets so far. Maybe get 2 for the price of 1 ada 6000. Can run much bigger models on just 1 3090 card.  It's most of the time better running models on 1 card. Else you have to serve your models with tensor parallelism for gaining advantage.\n\nAs I understood you can just split the model equally, so adding a bigger card to your 3090 will result that you only can allocate only the same amount of vram to all gpus in said cluster. But if anyone knows better please educate me.",
          "score": 1,
          "created_utc": "2025-12-24 12:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpeug4",
              "author": "pCute_SC2",
              "text": "and now there is a third option on the table, but then need a server like with the 3090 rig, so cost wise it might be more expensive.",
              "score": 1,
              "created_utc": "2025-12-24 12:06:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvplk8e",
              "author": "Disposable110",
              "text": "4090 48gb's are around 2500 each though, might as well go for the RTX Pro and not have the risk of drivers/hardware failures.",
              "score": 1,
              "created_utc": "2025-12-24 12:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw4pa6n",
          "author": "False-Ad-1437",
          "text": "You could test it out on one of the gpu rental platforms. I imagine it will be under $2/hr for each.¬†",
          "score": 1,
          "created_utc": "2025-12-27 03:02:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7eru7",
          "author": "gwolla138",
          "text": "Got 4x 6000 blackwell max-q. 1 TB DDR5 and 64 core 9000 threadripper to be on the safe side. Serving GLM4.7-FP8 for inference on sglang with MTP at 130t/s. In terms of sounds not too bad. Maxed the max-q‚Äôs to 250watt for inference. Hardly any performance degradation. \n\nIn hindsight, perhaps should have gotten intel cpus for the ktransformers/sglang combo for deepseek/kimi. But.. i can live with that.",
          "score": 1,
          "created_utc": "2025-12-27 15:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvml72e",
          "author": "SillyLilBear",
          "text": "Not that i know of, but the rtx 6000 pro will win every time.",
          "score": 1,
          "created_utc": "2025-12-23 23:10:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmlha8",
          "author": "RiskyBizz216",
          "text": "RTX pro for sure if thats in the budget.\n\nI just went thru hell trying to squeeze 3x5090's in an EATX case..broke one of their fans due to space and settled on 2x5090's\n\nSave yourself the stress and broken parts! Just deal with 1xGPU",
          "score": 1,
          "created_utc": "2025-12-23 23:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmsf6i",
          "author": "phido3000",
          "text": "Yes, get the RTX6000. It will be faster, better supported, quieter, more efficient, and more useful in more workloads and have better resale. \n\nPlus if one RTX6000 isn't enough, or you want more you can drop in another. You have upgrade paths.",
          "score": 0,
          "created_utc": "2025-12-23 23:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmv3w5",
              "author": "pCute_SC2",
              "text": "but its the same with the four 3090, if its not enough I can get 4 more for cheaper than another Blackwell",
              "score": 1,
              "created_utc": "2025-12-24 00:09:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvmx8v4",
                  "author": "phido3000",
                  "text": "Value will tend to the 4x3090. \n\nRunning 8 x way GPU systems is a bit a nightmare. It gets messy, performance drops off, it becomes an issue to power it all. \n\nIMO the compute and training performance of the RTX6000 will be much high AFAIK. Also with the training, having a card with much larger memory is a pretty big advantage. \n\nFor your workstation given your setup, I would go with the 6000 if you can afford it.\n\nIf you want to build a four way 3090 setup, nothing is stopping you from doing that, possibly as well. I quite like my AI server not to be my local machine, as heat/noise gets annoying, while the server in my garage, I do not care at all. 7900xtx is ok, but for local AI there are better options including the 32gb AI9600 pro, but also a 5090, RTX6000 etc. You may wish to do both",
                  "score": 0,
                  "created_utc": "2025-12-24 00:22:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvm98wf",
          "author": "Prudent-Ad4509",
          "text": "There are no comparisons because if there is a serious budget available, then 6000 pro is a no-brainer - easier to install, easier to use. One, two, maybe four. Professional solutions are available.\n\nIf such a budget in not available (i.e. is a homelab), then 2,4,8 or 12 3090 is the best option, at least for inference. In a rig, with added heatsinks to the backplate, with power limiting, with custom cooling, etc etc  to avoid problems.\n\nThere does not seem to be much of a cross between these audiences.\n\nPS. What do you know. It seems that there are still a few people who have both, albeit with the expected outcome. Out of 3090, 5090 and PRO 6000: systems made out of multiple 3090 win on total vram/money ratio and can run fast with proper paralleling, systems made out of several 5090 are second on performance/money (again, with proper parallel execution), and systems made out of PRO 6000 win on performance and power but with a significantly larger upfront cost.\n\nI'd bet a can of expired soda that the general overall performance capacity / cost would still be about the same in all three cases, it happens to be the case in most calculations that I did before. This is despite the list of wins before. Things swing one way or another depending on how you calculate costs. There is a higher cost of maintenance for a more complex system.",
          "score": 0,
          "created_utc": "2025-12-23 22:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvm7ul8",
          "author": "minhquan3105",
          "text": "Get the rtx 6000 pro if you can afford it. The 3090 is really old now and they have pretty high failure rate because of their massive die size. Also power and cooling will be a nightmare for 4 x 3090",
          "score": -1,
          "created_utc": "2025-12-23 21:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmdkvx",
              "author": "pCute_SC2",
              "text": "What do you mean by failure rate?",
              "score": 1,
              "created_utc": "2025-12-23 22:27:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvmf46t",
                  "author": "StardockEngineer",
                  "text": "They‚Äôre starting to beak down.",
                  "score": 2,
                  "created_utc": "2025-12-23 22:35:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1u966",
      "title": "I designed a Private local AI for Android - has internet search, personas and more.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/jk6meo3wmwag1",
      "author": "Sebulique",
      "created_utc": "2026-01-02 09:25:58",
      "score": 46,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx8c8zl",
          "author": "Themash360",
          "text": "Nice work, can it connect to local llm as well? So not just on device but custom endpoints",
          "score": 1,
          "created_utc": "2026-01-02 10:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8cp5h",
              "author": "Sebulique",
              "text": "You can, I've left it completely open, I also allowed it to work with smart bulbs, It works but sometimes it decides to do it's own thing unless I set parameters.\n\nI've turned it off after I broke it, but I want to reintroduce it again soon",
              "score": 3,
              "created_utc": "2026-01-02 10:18:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx982sn",
          "author": "theCatchiest20Too",
          "text": "Very cool! Are you tracking performance against hardware specs?",
          "score": 1,
          "created_utc": "2026-01-02 14:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9mbso",
              "author": "Sebulique",
              "text": "I was thinking of a toggle called \"stats for geeks\" but I wanted to make it look super easy for anyone, almost Chatgpt like, so more people feel comfortable using local llms",
              "score": 1,
              "created_utc": "2026-01-02 15:31:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx9j4l9",
          "author": "False-Ad-1437",
          "text": "\\> Very very happy, will upload soon on GitHub when I've ironed out any bugs I come across.\n\nI'm happy to submit PRs to fix said bugs",
          "score": 1,
          "created_utc": "2026-01-02 15:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa6go5",
              "author": "Sebulique",
              "text": "Thank you man, I appreciate you, I'm looking to make it easy and accessible for all",
              "score": 1,
              "created_utc": "2026-01-02 17:06:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdas2x",
          "author": "2QNTLN",
          "text": ">will upload soon on GitHub. \n\nHow soon is soon? Super interested in this project so i had to ask.",
          "score": 1,
          "created_utc": "2026-01-03 02:39:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe9nbn",
          "author": "tankandwb",
          "text": "RemindMe! 1 week",
          "score": 1,
          "created_utc": "2026-01-03 06:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe9qy4",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-01-10 06:34:58 UTC**](http://www.wolframalpha.com/input/?i=2026-01-10%2006:34:58%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/nxe9nbn/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLM%2Fcomments%2F1q1u966%2Fi_designed_a_private_local_ai_for_android_has%2Fnxe9nbn%2F%5D%0A%0ARemindMe%21%202026-01-10%2006%3A34%3A58%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q1u966)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-03 06:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxvlh0",
      "title": "I have 50 ebooks and I want to turn them into a searchable AI database. What's the best tool?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "author": "Great_Jacket7559",
      "created_utc": "2025-12-28 16:33:33",
      "score": 43,
      "num_comments": 29,
      "upvote_ratio": 0.98,
      "text": "I want to ingest 50 ebooks into an LLM to create a project database.\nIs Google NotebookLM still the king for this, or should I be looking at Claude Projects or even building my own RAG system with LlamaIndex?\nI need high accuracy and the ability to reference specific parts of the books. I don't mind paying for a subscription if it works better than the free tools.\nAny recommendations?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwj05xl",
          "author": "Zucramj",
          "text": "I would build something custom.\n\nHere is how I see it:\n\n1) You own the data \n2) You own the AI (local embeddings work great for this task and you can run it with a local AI model or use Openrouter) \n3) I would build this with DSPy (modular and can be optimized with gepa) \n4) I would use PostgreSQL to store the data \n\nSo if you already have the ebooks as pdfs I would take those an set that up.\n\nHere is my primitive version of this I did some months back: \n\nhttps://github.com/marcusjihansson/dspy-mcp-tools/blob/main/regulatory.py\n\nIt is advanced if you don't know what you are reading but as I have gotten deeper into optimizing AI agents and systems does this feel primitive to me. \n\nThat newer research is going to be uploaded to my GitHub soon...\n\nSo: \nA) I would read through if this would solve your task! \nB) I would be happy to help you out if you have any questions!",
          "score": 9,
          "created_utc": "2025-12-29 11:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf0w7j",
          "author": "DHFranklin",
          "text": "NotebookLM and maybe some RAG and Custom Instructions for vectoring.\n\nNow if you wanna get real squirrely you could turn the entire compendium, and custom instruction into 1 million token prompt and sit it into Gemini as is. That might actually be more useful.\n\nThe trick is information loss and context bleed with the books. I could see a JSON Database made from it all as text also.\n\nIt comes down to what are you using it for and what information needs to stay consistent.",
          "score": 4,
          "created_utc": "2025-12-28 19:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe78fy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-28 17:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwerjho",
              "author": "blaidd31204",
              "text": "I am intrigued... what If:\n\n* pdf (Yes, there are images, but these should not influence info)?\n\n* markdown (No images)?",
              "score": 2,
              "created_utc": "2025-12-28 19:06:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh0m2z",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 4,
                  "created_utc": "2025-12-29 02:05:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwgwh5t",
                  "author": "Investolas",
                  "text": "Those answers influence the reccomendation.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwim06f",
          "author": "RepLava",
          "text": "LightRAG with the MCP. Works great based on the relatively sparse info you've given",
          "score": 2,
          "created_utc": "2025-12-29 08:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlvovo",
          "author": "False-Ad-1437",
          "text": "AnythingLLM can make your own workspaces for this.",
          "score": 2,
          "created_utc": "2025-12-29 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpin9j",
          "author": "Cladser",
          "text": "I‚Äôm deep in a similar project atm. It depends a lot on the type of info you want from the LLM. If it‚Äôs what does author x have to say about y - RAG is your best bet. However if you want to ask questions like across these books what are the most common ways of dealing with with Y - That is a corpus level (ie entire collection) query and RAG will suck. Llamaindex with hybrid search seems to be the best middle road at the moment.",
          "score": 3,
          "created_utc": "2025-12-30 10:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwft2zf",
          "author": "vidibuzz",
          "text": "Slightly off topic. You may want to use Illuminati.google.com also for voice summaries.",
          "score": 2,
          "created_utc": "2025-12-28 22:09:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhdx2b",
              "author": "Schizophreud",
              "text": "Didn‚Äôt know about this. Thanks.",
              "score": 1,
              "created_utc": "2025-12-29 03:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwibx1i",
          "author": "Charming_Support726",
          "text": "Depending on what's in the books, you could have a look at IBMs Docling for conversion. I think every simple RAG Pipeline will do the trick in the beginning",
          "score": 1,
          "created_utc": "2025-12-29 07:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigiuv",
          "author": "maxz2040",
          "text": "Logically.app",
          "score": 1,
          "created_utc": "2025-12-29 08:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm4e9l",
          "author": "Empty-Poetry8197",
          "text": "Paperqa",
          "score": 1,
          "created_utc": "2025-12-29 21:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwno756",
          "author": "kchandank",
          "text": "Interesting, if you are able to achieve your objective, would you be able to share the steps?",
          "score": 1,
          "created_utc": "2025-12-30 02:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpe82p",
          "author": "Sea_Mouse655",
          "text": "I‚Äôve been using PaperQA2 for some regulatory use cases and it‚Äôs gangster",
          "score": 1,
          "created_utc": "2025-12-30 09:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsjglj",
          "author": "isleeppeople",
          "text": "Seems like you could use something like pypdf and langchain to embed it into your RAG. If the ebooks are like current info that can change or become stale you will want to tag them and set up some sort of workflow to compare them to a Gemini or open ai call to compare the info and if it becomes stale remove it. I use qdrant and postgresql for ground truth. I do stuff like this for versions of python that I have to use to stay compatible with other things I'm running. Or actually for langchain and langgraph too. They just changed compatibility versions so whenever I upgrade I can just use the newer repo in my rag with the updated information. I will still keep the old one until I am absolutely certain I won't revert but you can just leave it sit there and not reference the old one. Hope that makes sense.",
          "score": 1,
          "created_utc": "2025-12-30 20:25:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh0iik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-29 02:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwimlqu",
              "author": "Just_Bronze",
              "text": "I have a stupid question.\n\nNot entirely sure how these things work, but do I understand correctly you've got a system set up to take input and create a chatbot?  Or do you create a file/fileset that a chatbot incorporates to use the information.",
              "score": 3,
              "created_utc": "2025-12-29 09:05:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwisq5d",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 10:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjfv9q",
              "author": "loki626",
              "text": "Can I DM you? I have some pdfs that I would like to convert. They are more technical though. Medicine related.",
              "score": 1,
              "created_utc": "2025-12-29 13:13:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmga0",
          "author": "PaleontologistOk865",
          "text": "What about just throwing everything in a AI and letting it figure out what to do? That's what my clients keep saying to me. . .",
          "score": 0,
          "created_utc": "2025-12-30 23:38:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puf1ic",
      "title": "ClosedAI: MXFP4 is not Open Source",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1puf1ic/closedai_mxfp4_is_not_open_source/",
      "author": "max6296",
      "created_utc": "2025-12-24 05:01:51",
      "score": 41,
      "num_comments": 10,
      "upvote_ratio": 0.78,
      "text": "Can we talk about how ridiculous it is that we only get MXFP4 weights for gpt-oss?\n\nBy withholding the BF16 source weights, OpenAI is making it nearly impossible for the community to fine-tune these models without significant intelligence degradation. It feels less like a contribution to the community and more like a marketing stunt for NVIDIA Blackwell.\n\nThe \"Open\" in OpenAI has never felt more like a lie. Welcome to the era of ClosedAI, where \"open weights\" actually means \"quantized weights that you can't properly tune.\"\n\nGive us the BF16 weights, or stop calling these models \"Open.\"",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1puf1ic/closedai_mxfp4_is_not_open_source/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvokubl",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 32,
          "created_utc": "2025-12-24 07:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoscfp",
              "author": "NeverEnPassant",
              "text": "> This model was trained with QAT, or Quantization aware training, meaning it won't natively have larger weights than have been posed.\n\nQAT uses master weights + quantized weights during training. The released model only includes the quantized weights. You would achieve better outcomes in fine tuning if you had the master weights. You won't be training natively in 4-bits in either case.\n\n> On Blackwell.. well, also wrong. Blackwell excells at NVFP4 not MXFP4.\n\nBlackwell supports both in hardware.",
              "score": 8,
              "created_utc": "2025-12-24 08:30:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsz865",
              "author": "Puzzleheaded_Fox242",
              "text": "I work on this stuff and this comment is a bizarre mix of misplaced confidence and misinformation. Everyone should disregard it or post it into a frontier LLM to see where it is wrong.",
              "score": 2,
              "created_utc": "2025-12-25 01:05:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvolsmc",
          "author": "gwestr",
          "text": "I like BF16, but they don't owe you an open license to it. For the end user community what they distribute is appropriate for a high end PC. But yeah probably not so useful for fine tuning. Maybe evaluation.",
          "score": 6,
          "created_utc": "2025-12-24 07:27:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvoh35y",
          "author": "decentralize999",
          "text": "This company is only damage for world ai development, they are gathering all available GPUs from all factories making deficit for good companies which create openweight and open sources LLMs and for folks which cannot purchase these GPUs even with double or triple price.¬†\n\n\nAnyway I believe that to trick whole world market is possible 2-3 years and after countries like China will beat this company even own country competitors create better LLMs now.",
          "score": 8,
          "created_utc": "2025-12-24 06:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvocma1",
          "author": "TheAussieWatchGuy",
          "text": "I mean state the obvious. Open source AI is a long way behind and trillions of US dollars are working to keep it so.¬†",
          "score": 5,
          "created_utc": "2025-12-24 06:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqlcdr",
              "author": "inevitabledeath3",
              "text": "It's not though... We have models now comparable to Sonnet 4.5 and GPT-5 which were SOTA only a few months ago. Sonnet 4.5 came out on 29th September. GLM 4.7 came out just a couple days ago making it a bit under 3 months behind SOTA. Things like Gemini 3 Pro and Opus 4.5 are harder to beat, but they were released quite recently. It's fair we don't have a good replacement for them yet.",
              "score": 3,
              "created_utc": "2025-12-24 16:28:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrr3vb",
          "author": "Witty_Mycologist_995",
          "text": "i think we should get more mxfp4 models and less bf16.",
          "score": 2,
          "created_utc": "2025-12-24 20:18:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp8ene",
          "author": "k_means_clusterfuck",
          "text": "you know you can just dequantize it yourself right? I already did for a different experiment. it was trained in MXFP4 that doesnt mean you HAVE to use it in MXFP4. There are no 'original bf16 weights', just dequantize and enjoy having an 8 times larger model",
          "score": -1,
          "created_utc": "2025-12-24 11:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtd2b9",
          "author": "Consistent_Wash_276",
          "text": "There‚Äôs a 4bit and 8bit model of 120b on LM Studio. Correct me if I‚Äôm miss reading this as I‚Äôm not an expert. But yes no fp16",
          "score": 0,
          "created_utc": "2025-12-25 02:50:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1aq7a",
              "author": "Badger-Purple",
              "text": "There are no wrong questions.\n\nThe model itself was trained and released with 4 bit mixed precision (between integer and floating point) weights; not quite FP4 and not Q4 (Int 4). \n\nThe original release did include 16 bit attention paths, but you will see quantizations that compress that attention paths as well to 8 bits. \n\nWhat you will also note is that the size of the quantized model is not that smaller. That is what OP refers to: the model weights, regardless of version you see, are natively 4 bits, so the size does of change much. Analogous to putting 24k gold on a lower quality gold in jewelry.\n\nThe original 16 bit weights will not see the light of day. That makes their model less likely to be finetuned as effectively, although not completely immune to modifications, such as the de restricted versions that have popped up.",
              "score": 1,
              "created_utc": "2025-12-26 15:30:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pva6ao",
      "title": "Now I don't have to pay for transcription services with whisper.cpp",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/fbw3abxzob9g1",
      "author": "DamageEvery764",
      "created_utc": "2025-12-25 09:58:54",
      "score": 37,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pva6ao/now_i_dont_have_to_pay_for_transcription_services/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvvuj8k",
          "author": "DHFranklin",
          "text": "Nice. How does it compare with other competitors going into 2026?",
          "score": 2,
          "created_utc": "2025-12-25 15:59:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvww5nk",
              "author": "Silver_Jaguar_24",
              "text": "Any other working local TTS in English that you can recommend?",
              "score": 1,
              "created_utc": "2025-12-25 19:41:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwzl38",
                  "author": "CtrlAltDelve",
                  "text": "Just a clarification, this is a Speech to Text app, not a Text to Speech app :)",
                  "score": 1,
                  "created_utc": "2025-12-25 20:02:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0fg09",
      "title": "I got my first ever whitepaper published",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/v23qicvcdy9g1.png",
      "author": "InsideResolve4517",
      "created_utc": "2025-12-31 15:21:16",
      "score": 37,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0fg09/i_got_my_first_ever_whitepaper_published/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwyduyb",
          "author": "Lame_Johnny",
          "text": "Amazing! congratulations!",
          "score": 1,
          "created_utc": "2025-12-31 18:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ein",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwynwee",
          "author": "Busy_Farmer_7549",
          "text": "Congratulations üéàüéä",
          "score": 1,
          "created_utc": "2025-12-31 19:17:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ed2",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwytdq1",
          "author": "PromptOutlaw",
          "text": "Congrats bud!! I really underestimated how much hard work goes into these. I‚Äôve been humbled recently and I‚Äôm not even half way",
          "score": 1,
          "created_utc": "2025-12-31 19:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25e8e",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzsdsy",
      "title": "Suggest a model for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "author": "Bright_Dot113",
      "created_utc": "2025-12-30 20:10:25",
      "score": 37,
      "num_comments": 24,
      "upvote_ratio": 0.97,
      "text": "Hello, I have 9950x3d with 64GB RAM and 5070 ti \n\nI recently installed LM Studio, which models do you suggest based on my hardware for the following purposes. \n\n1. Code in python and rust \n\n2. DB related stuff like optimising queries or helping me understand them. (Postgresql)\n\n3. System and DB design.\n\nAlso what other things can I do?\nI have heard lot about MCP servers but I didn't find any MCP servers useful or anything related to my workflow if you have any suggestions that would be great! ",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwsjtel",
          "author": "SimplyRemainUnseen",
          "text": "I'd suggest unsloth/Nemotron-3-Nano-30B-A3B-GGUF. You'll need to offload to system memory, but you'll have a relatively fast and intelligent model that's good at using tools.\n\nFor MCP servers check out [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)",
          "score": 16,
          "created_utc": "2025-12-30 20:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo35r",
              "author": "Big-Masterpiece-9581",
              "text": "What kind of speeds to you think he could get with this model and that configuration? Anything particular you like about that model or unsloth?",
              "score": 3,
              "created_utc": "2025-12-31 03:12:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxynte",
                  "author": "SimplyRemainUnseen",
                  "text": "Speeds would be pretty fast I imagine. Faster than my work laptop with an iGPU (which runs it faster than I can read).\n\nThe model itself is fully open source (data too!) and made by NVIDIA. The performance of the model exceeds other models in the same parameter range of 20-32b and has very long context (up to a million tokens).\n\nNVIDIA worked with Unsloth for day 0 quants of the model. Unsloth makes efficient dynamic quants of models that retain very high levels of accuracy at lower precision. Considering OP will need to offload to system memory, GGUF is ideal as it's designed for that use case.",
                  "score": 3,
                  "created_utc": "2025-12-31 17:11:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1r491",
                  "author": "Hairy_Candy_3225",
                  "text": "I use a similar setup with 9950x3d 96 GB RAM and 5080 16 GB VRAM and use Nemotron 3 nano 30b Q6. Tokens per second heavily depends on context used. I've written a script to run a benchmark test and measure speed with all combinations of different context length / % GPU-offload layer / force experts on CPU on vs off.\n\nWith smaller context (i.e. 10k tokens) I get up to 16 TPS. This drops to around 10 @200k tokens. I'm no expert but I don't think there will be a big difference between 5070 or 5080 if VRAM is the same.\n\nWhat i can't understand is that with nemotron it does not matter how much layers I offload to GPU. The model is around 30 GB but only between 4 and 6 GB VRAM is used regardless of offload setting. It seems only the active layers are offloaded to GPU. This was different when I qwen 2.5, where I had to balance the exact number of layers that would fit in VRAM. As soon as you try to offload more layers than fit in VRAM, regular RAM will be used as shared VRAM and TPS drop drastically. \n\nProbably nothing new here for most redditors on this sub but i'm new to local LLM's and had to figure out a lot of things myself.",
                  "score": 2,
                  "created_utc": "2026-01-01 07:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsmq4k",
          "author": "beedunc",
          "text": "Qwen3coder, whatever fits.",
          "score": 7,
          "created_utc": "2025-12-30 20:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtu8zi",
          "author": "No-Consequence-1779",
          "text": "Qwen3-coder-30b. The largest quant you can run. Dense model is good or moe. ¬†Keep in mind coder specific models are specialized for coding. Many like oss 120b though that size is not necessary.¬†",
          "score": 8,
          "created_utc": "2025-12-31 00:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt434d",
          "author": "Toastti",
          "text": "If you add another 64gb of ram you can run a Q2 quant of Minimax m2.1. it will probably be 7tk/s or so but it is almost certainly the smartest agentic coding model you can reasonably run",
          "score": 3,
          "created_utc": "2025-12-30 22:03:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt53sn",
              "author": "Your_Friendly_Nerd",
              "text": "q2? isn‚Äòt that gonna suck?",
              "score": 3,
              "created_utc": "2025-12-30 22:08:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtv56k",
                  "author": "Worried_Goat_8604",
                  "text": "No unsloth dynamic quant v2 usually dosnt reduce that much quality",
                  "score": 1,
                  "created_utc": "2025-12-31 00:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwugr2q",
              "author": "Individual_Gur8573",
              "text": "I agree I found minimax 2.1 IQ2_M the smartest model after GLM 4.5 air¬†",
              "score": 1,
              "created_utc": "2025-12-31 02:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwuo8ix",
                  "author": "Karyo_Ten",
                  "text": "GLM Air which quant?",
                  "score": 1,
                  "created_utc": "2025-12-31 03:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwviewu",
          "author": "Count_Rugens_Finger",
          "text": "Qwen3-coder-30B-A3B, Nemotron-3-Nano-30B-A3B, Devstral-Small-2-24B",
          "score": 3,
          "created_utc": "2025-12-31 06:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsooqq",
          "author": "FullstackSensei",
          "text": "I'd say try a bunch of MoE all the way up to gpt-oss-120b and see where your pain threshold is for speed. IMO, you should keep a few at hand: a smaller model like Qwen3 Coder 30B, Nemotron 30B, gpt-oss-20b, Devstral 2 24B as daily drives, and larger models like gpt-oss-120b, GLM 4.5 air, Devstral 2 123B (Q4), etc for when the smaller models get stuck or can't solve whatever issue you have.",
          "score": 4,
          "created_utc": "2025-12-30 20:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt5rhp",
          "author": "Level_Wolverine_141",
          "text": "I have the same system as you except I've got a 5080, and I'm just using Claude code max and it's pretty good.",
          "score": -4,
          "created_utc": "2025-12-30 22:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww8pra",
              "author": "g33khub",
              "text": "your 5080 doesn't matter, I can use Claude code on a raspberry pi",
              "score": 2,
              "created_utc": "2025-12-31 10:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyw1b4",
      "title": "I built Plano(A3B) - fast open source LLM for agent orchestration that beats frontier LLMs",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5rp16cxd57ag1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-29 19:44:28",
      "score": 35,
      "num_comments": 14,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyw1b4/i_built_planoa3b_fast_open_source_llm_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwoqem5",
          "author": "ThsYWeCntHveNiceTngs",
          "text": "your research page is more advert than research and the blog provides more detail, but not much. Is there an Arxiv link or anything to read about your method and how you generated the proposed results?",
          "score": 4,
          "created_utc": "2025-12-30 06:07:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwosnmj",
              "author": "AdditionalWeb107",
              "text": "The huggingface models page has more details. Although we are in the process of publishing the arxiv paper",
              "score": 0,
              "created_utc": "2025-12-30 06:25:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxc7j4",
          "author": "False-Ad-1437",
          "text": "This is not open source. \n\nIt imposes non-open requirements (attribution, use restrictions, redistribution constraints and separate commercial licensing for certain uses) that violate the open-source criteria defined by the OSI/FSF.",
          "score": 2,
          "created_utc": "2025-12-31 15:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufm16",
          "author": "Purple-Programmer-7",
          "text": "How does this differ from Arch that you‚Äôve previously pushed for the past year?",
          "score": 1,
          "created_utc": "2025-12-31 02:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui3lf",
              "author": "AdditionalWeb107",
              "text": "Arch was about model routing. Plano is about orchestration, which is a slightly more complicated set of tasks. Plano is the next major upgrade to Arch with several new capabilities for agentic applications like filter chains, agent signals, and even more robust model gateway.\n\n\nBy the way, people were confusing arch with arch Linux so we thought it was a better time to rename the project.\nTry Plano üôè",
              "score": 1,
              "created_utc": "2025-12-31 02:37:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyaxs3",
          "author": "AdditionalWeb107",
          "text": "That‚Äôs fair - it should say open weights. And the license is very permissive except for one deployment type",
          "score": 1,
          "created_utc": "2025-12-31 18:12:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwopty3",
          "author": "maigpy",
          "text": "what is model-native?",
          "score": 1,
          "created_utc": "2025-12-30 06:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqe6c",
              "author": "AdditionalWeb107",
              "text": "It‚Äôs integrated with small LLMs - central to how the project is built.",
              "score": -1,
              "created_utc": "2025-12-30 06:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6v7x",
                  "author": "maigpy",
                  "text": "integrated with small llms translates to \"model-native\"?\n\nI don't quite understand, if you could use more words to describe what's going that would help.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnderh",
          "author": "Lyuseefur",
          "text": "Planning already to use it in next build of nexora follow us\n\nHttps://github.com/jeffersonwarrior/nexora",
          "score": 0,
          "created_utc": "2025-12-30 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnwumq",
              "author": "AdditionalWeb107",
              "text": "Okay - thanks. Would love the feedback. And if you like our project, don't forget to star it too",
              "score": 1,
              "created_utc": "2025-12-30 02:56:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxzjhp",
      "title": "Google Open-Sources A2UI: Agent-to-User Interface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 19:08:03",
      "score": 26,
      "num_comments": 8,
      "upvote_ratio": 0.85,
      "text": "Google just released **A2UI (Agent-to-User Interface)** ‚Äî an open-source standard that lets AI agents generate **safe, rich, updateable UIs** instead of just text blobs.\n\nüëâ Repo: [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\n# What is A2UI?\n\nA2UI lets agents ‚Äú**speak UI**‚Äù using a **declarative JSON format**.  \nInstead of returning raw HTML or executable code (‚ö†Ô∏è risky), agents describe *intent*, and the client renders it using **trusted native components** (React, Flutter, Web Components, etc.).\n\nThink:  \nLLM-generated UIs that are **as safe as data, but as expressive as code**.\n\n# Why this matters\n\nAgents today are great at text and code, but terrible at:\n\n* Interactive forms\n* Dashboards\n* Step-by-step workflows\n* Cross-platform UI rendering\n\nA2UI fixes this by cleanly separating:\n\n* **UI generation (agent)**\n* **UI execution (client renderer)**\n\n# Core ideas\n\n* üîê **Security-first**: No arbitrary code execution ‚Äî only pre-approved UI components\n* üîÅ **Incremental updates**: Flat component lists make it easy for LLMs to update UI progressively\n* üåç **Framework-agnostic**: Same JSON ‚Üí Web, Flutter, React (coming), SwiftUI (planned)\n* üß© **Extensible**: Custom components via a registry + smart wrappers (even sandboxed iframes)\n\n# Real use cases\n\n* Dynamic forms generated during a conversation\n* Remote sub-agents returning UIs to a main chat\n* Enterprise approval dashboards built on the fly\n* Agent-driven workflows instead of static frontends\n\n# Current status\n\n* üß™ **v0.8 ‚Äì Early Public Preview**\n* Spec & implementations are evolving\n* Web + Flutter supported today\n* React, SwiftUI, Jetpack Compose planned\n\n# Try it\n\nThere‚Äôs a **Restaurant Finder demo** showing end-to-end agent ‚Üí UI rendering, plus Lit and Flutter renderers.\n\nüëâ [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\nThis feels like a big step toward **agent-native UX**, not just chat bubbles everywhere. Curious what the community thinks ‚Äî is this the missing layer for real agent apps?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwgkvue",
          "author": "bananahead",
          "text": "I am begging people to stop with the LLM written posts. Just post whatever prompt you used! That‚Äôs the post!",
          "score": 13,
          "created_utc": "2025-12-29 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi048d",
              "author": "leonbollerup",
              "text": "Why care? I rather want a LLM generated post than some bad version of English - not everyone speaks English native",
              "score": 4,
              "created_utc": "2025-12-29 05:47:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjksfb",
                  "author": "ak_sys",
                  "text": "Id rather tailor my discussion to a non English speaker than not understanding that there may be a disconnect getting lost in translation. \n\nThis explains a lot of \"reading comprehension\" issues I've noticed from commenters, where they seem to be responding to some diffuse sentiment of the post rather than  the actual nuanced point and position. Languages almost never just directly translate into one another, and inserting an llm in the middle of a discussion without informing the other party seems pretty dishonest and disrespectful.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:44:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwmr4i2",
                  "author": "bananahead",
                  "text": "So write a post in your native language and have it translated. That‚Äôs just as easy as whatever prompt created this and would be easier to read and more authentic.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgo2bh",
              "author": "Kamal965",
              "text": "The absolute state of Reddit now:\n\n* A lazy OP asks an LLM to create a post for them.\n* Users see LLM-isms and either:\n   * Ctrl + W\n   * Pass it on to an LLM to summarize it for them instead.\n* Another OP gives up on writing their own posts and starts using an LLM because they keep running into LLM-generated posts.\n\nRinse and repeat.",
              "score": 0,
              "created_utc": "2025-12-29 00:53:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nworg5v",
          "author": "ZITNALTA",
          "text": "Just curious if any knows if this somehow works with Google Antigravity IDE? By the way, I am NOT a dev so if this sounds like a newbie question that is why.",
          "score": 1,
          "created_utc": "2025-12-30 06:15:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpeh7",
      "title": "Device to run a local LLM mainly for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "author": "knibroc",
      "created_utc": "2025-12-28 11:40:21",
      "score": 21,
      "num_comments": 31,
      "upvote_ratio": 0.9,
      "text": "Hi mates,\n\nI mostly use ChatGPT and Mistral (through their \"vibe coding\" cli tool and API). I don't pay for these services, so I only use the lesser-capable models.\n\nMy laptop is not powerful enough to run this (no GPU / I've experimented with ollama but I can only run the smallest models very slowly so this is not ok for daily use), so I'm currently considering building a device dedicated to running a LLM, mainly for coding purposes. Ideally something small, Raspberry Pi-based or similar would be great.\n\nI have a few questions: is there specialized hardware for this (I've heard of TPU/NPU)? What kind of performance can I expect (I'd need at least GPT4/Devstral level)? I'm also worried about speed (tokens/s) and cost.\n\n  \nAny advice is appreciated!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdmlhq",
          "author": "KrugerDunn",
          "text": "The cost of any device that can run a decent coding model will far out scale just paying for Claude Code and still won‚Äôt be nearly as good or future proof.\n\nI went down this rabbit hole so you don‚Äôt have to üòÇ",
          "score": 39,
          "created_utc": "2025-12-28 15:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdstym",
              "author": "skabaru",
              "text": "2nd. $100/month to Claude code is the best money you can spend here.... And I do have an older gaming rig running local llms... And it isn't even close.",
              "score": 12,
              "created_utc": "2025-12-28 16:19:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwekb8b",
                  "author": "New_Jaguar_9104",
                  "text": "I have an entire cluster and still pay for Claude max. It's worth its weight in gold IMO",
                  "score": 6,
                  "created_utc": "2025-12-28 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwffg7z",
              "author": "ThatOneGuy4321",
              "text": "what‚Äôs your threshold for ‚Äúdecent‚Äù? 70B?",
              "score": 1,
              "created_utc": "2025-12-28 21:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgn3r5",
                  "author": "KrugerDunn",
                  "text": "I mean, depends what you're trying to do. I personally just love using Claude Code and occasionally Gemini Cli, so nothing is going to compare to those that can be run locally. Maybe could get away with a GLM4.6-AIR which is 357B params. Unless you're working on super duper secret proprietary code or something that violates the foundation model guardrails I just don't see any reason to use local.\n\nI know some people have used something like Qwen Code 32B or Devstral 24B and been satisfied with it, but never been worth it to me.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdph75",
          "author": "TyphoonGZ",
          "text": "If you're not satisfied with 1-5 toks/s on CPU (\"coffee break\" workflow), sounds like you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nWhy 30B? 20--30B is the size range where the model is still (sort of) considered \"small\" yet it starts being actually useful.\n\nThat said, you should consider using Openrouter and spend $5 for credits to test said models and see if they're good enough for you. You wouldn't want to buy a ~$1000 GPU just to get annoyed that your model's too braindead, right?\n\nRegarding TPU/NPU, I haven't heard if NPUs finally have the necessary software infrastructure to be useful. Well, they wouldn't really help LLMs if there's no development in memory bandwidth to go with them.\n\nOn the other hand, Google sells Coral TPUs, but those are for computer vision, not LLMs, and anyway, they only have *megabytes* of memory.",
          "score": 8,
          "created_utc": "2025-12-28 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweoi73",
              "author": "Count_Rugens_Finger",
              "text": ">  you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nI can run Qwen3-coder-30B-A3B at 12 tok/sec on my 8GB 3070 and an absolute potato of a CPU",
              "score": 2,
              "created_utc": "2025-12-28 18:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgwndy",
                  "author": "TyphoonGZ",
                  "text": "Oh yeah, I forgot MoE models exist...\n\nAlso damn, that's janky. Nice.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj61mb",
              "author": "nasone32",
              "text": "today with 24gb vram you can run qwen3 coder at 150 tokens/s with 100k+ context, or something like qwen3 next 80B at 15/20 tokens/s offloading some layers.",
              "score": 1,
              "created_utc": "2025-12-29 12:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcvibf",
          "author": "AnxietyPrudent1425",
          "text": "If you have a desktop with a GPU or Mac Mini/Studio you can setup Tailscale and basically have your own cloud endpoint. My setup is a MacBook Air + 128GB Mac Studio + Linux workstation and I couldn‚Äôt be happier.",
          "score": 5,
          "created_utc": "2025-12-28 13:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcygiq",
              "author": "knibroc",
              "text": "if only I could afford a 128GB Mac Studio! these machines look great",
              "score": 2,
              "created_utc": "2025-12-28 13:24:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdccz6",
          "author": "Background_Gene_3128",
          "text": "Atm. I‚Äôve dedicated a small server in the addict to run my llms, with a i5-12600k, 32gb ddr5 and a 3060 with 12gb vram. \nRunning proxmox with a ubuntu vm and ollama. \n8-14b models np, 14-24 okay, and the 30-34b models a bit too slow for my liking. \nSo I‚Äôve upgraded to 96gb ram (not sure if that actually matter, but I‚Äôve seen people get decent speeds with gpt-oss 120b with ram as offload, and found a used offer locally that didn‚Äôt require a kidney) \nAnd 2 5060 Ti‚Äôs as they‚Äôre on sale now here in Europe for ‚Ç¨370 a piece. \n\nNot sure if this is the best budget setup or small, but it‚Äôs what I‚Äôm rocking until that‚Äôs not enough.\nIt‚Äôs fitted in a Jonsbo D32 pro mesh case.",
          "score": 4,
          "created_utc": "2025-12-28 14:52:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjuvo",
          "author": "BigYoSpeck",
          "text": "A used desktop/gaming PC or server is probably the most cost effective way in\n\n\nSomething with 64gb of either DDR4 or 5, a 6+ core CPU, and a 16gb or more GPU\n\n\nI recently purchased a Ryzen 9 5900X 64gb DDR4-3600 with a Radeon RX 6800 XT from eBay and I can run gpt-oss-120b with full context at just over 20 tok/s\n\n\nIn the near future I'd like to swap out the Radeon for an RTX 3090 but ROCm is fairly good these days in llama.cpp \n\n\nEfficiency and performance won't match a Mac with comparable memory but it's a fraction of the price and doubles up for gaming duties",
          "score": 3,
          "created_utc": "2025-12-28 15:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf81pw",
              "author": "andriizahorui",
              "text": "Hey, I have a similar config and struggle to run gpt 120b with llama vulkan at decent speeds. Could you please share how do you run it with full context at 20 tps? Like exact command and stuff please",
              "score": 2,
              "created_utc": "2025-12-28 20:26:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfjs4k",
                  "author": "BigYoSpeck",
                  "text": "I'm not at my computer at the moment to give the exact parameters I pass to llama-server but I know the key ones are:\n\n   --threads 8 (5-8 are all very close, after 8 performance declines)\n\n   --flash-attn on\n\n   --mlock --no-mmap\n\n   --n-gpu-layers 99\n\n   --n-cpu-moe 32 (going down to 28 at lower context is faster, 32 is the sweet spot for having space left though) \n\n\nThis is with a self compiled ROCm build, pre built Vulkan docker isn't quite as fast",
                  "score": 4,
                  "created_utc": "2025-12-28 21:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcq3d3",
          "author": "DenizOkcu",
          "text": "Buying a used MacBook with an Apple Silicon might be your best/cheapest bet. They leverage unified memory and use mlx as a native LLM engine. I use devstral small 2 on a M3 with 36GB RAM in LM Studio. Nvidias Nemotron 3 nano is even faster with great coding results incl tool usage.",
          "score": 4,
          "created_utc": "2025-12-28 12:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcyl0f",
              "author": "knibroc",
              "text": "indeed Nemotron 3 sounds cool, will check, thanks!",
              "score": 2,
              "created_utc": "2025-12-28 13:25:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdgs27",
                  "author": "DenizOkcu",
                  "text": "It works really well with Nanocoder an open source Coding Tool with a focus on privacy/local LLMs (disclaimer: I am one of the contributors)",
                  "score": 3,
                  "created_utc": "2025-12-28 15:17:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwghwj2",
          "author": "machaao",
          "text": "In our tests, we couldn't get any of the single current open source code LLM to work at workable speed that is 25 tokens per second or so for a medium size code base.\n\nAs soon as you want something to be demanding it kinda barfs and token / second goes down the drain \n\nWould love to hear success stories tho üòå\n\nP.S. Tried with gpt oss and qwen3 on M4 - 128G",
          "score": 2,
          "created_utc": "2025-12-29 00:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkp2kz",
          "author": "HiddenPingouin",
          "text": "The closest to claude code would be GLM4.7. You could run the q8 on a Mac Studio with 512GB of RAM",
          "score": 2,
          "created_utc": "2025-12-29 17:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnt1op",
              "author": "teleolurian",
              "text": "Pretty well, I would add.",
              "score": 1,
              "created_utc": "2025-12-30 02:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcml86",
          "author": "EternalVision",
          "text": "TPU is not really possible, only Google has those (developed them themselves). And your question really depends on your budget, the sky really is the limit here. An Ryzen Strix Halo 395 AI MAX+ based minipc is what could work out, but again, really depends on your budget.",
          "score": 2,
          "created_utc": "2025-12-28 11:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwisuw6",
          "author": "HealthyCommunicat",
          "text": "If u want a portable, the m4 max 128 gb is gunna be the best ur gunn get, if u dont have the money for that, the z13 flow. Load gpt oss 120b at high reasoning, qwen 3 next 80b, go checkout my post i did about testing all these models - keep in mind the z13 flow is half the cost so the token/s will be literally half.",
          "score": 1,
          "created_utc": "2025-12-29 10:04:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2h9o6",
          "author": "xcr11111",
          "text": "I came down to two viable options for that, but it don't think that any one of them can totally compete with the Claude code. Cheapest option I went for was an used m1 pro max 64gb. Bought one like new for 1200 bucks. It's amazing hardware, but tbh I don't get used to MacOS. Installed omarchy bit the metal drivers a not fast here(half the speed in llms). The other cool option ist an framework Desktop PC with 128 gig. It's an absolute beast for everything, but very expensive and prices will increase very soon because of ram.",
          "score": 1,
          "created_utc": "2026-01-01 12:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf1i4b",
          "author": "Jarr11",
          "text": "Don't do it! It will cost you more to run it yourself than it would to just pay for a subsciption to Claude/ChatGPT/Gemini for CLI access",
          "score": 1,
          "created_utc": "2025-12-28 19:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf6esp",
          "author": "Oki667",
          "text": "Lol, just pay for Claude code monthly subscription.",
          "score": 1,
          "created_utc": "2025-12-28 20:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiurfb",
          "author": "Crazyfucker73",
          "text": "You aren't going to get GPT4 level on a fucking raspberry pi mate.",
          "score": -1,
          "created_utc": "2025-12-29 10:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjammv",
              "author": "knibroc",
              "text": "And you are being fucking helpful mate",
              "score": 5,
              "created_utc": "2025-12-29 12:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwracn",
      "title": "Top 10 Open-Source User Interfaces for LLMs",
      "subreddit": "LocalLLM",
      "url": "https://medium.com/@techlatest.net/top-10-open-source-user-interfaces-for-llms-94e3dd4ae20b",
      "author": "techlatest_net",
      "created_utc": "2025-12-27 06:51:33",
      "score": 20,
      "num_comments": 11,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwracn/top_10_opensource_user_interfaces_for_llms/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw5kai6",
          "author": "leonbollerup",
          "text": "Missing my favorites: cherry studio and Onyx ..",
          "score": 3,
          "created_utc": "2025-12-27 06:55:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5lef2",
              "author": "techlatest_net",
              "text": "Added to the list",
              "score": 2,
              "created_utc": "2025-12-27 07:05:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5mfsm",
                  "author": "silenceimpaired",
                  "text": "What do you use? What quant tech do you prefer?",
                  "score": 1,
                  "created_utc": "2025-12-27 07:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6p3lx",
          "author": "plainnaan",
          "text": "jan.ai?¬†",
          "score": 3,
          "created_utc": "2025-12-27 13:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw73uvi",
          "author": "Big-Masterpiece-9581",
          "text": "Need to verify git activity and velocity before recommending. Chatbot ui was the most popular in 2023. But they‚Äôre 2.0 used supabase and introduced a ton of bugs and complexity with a full rewrite that added no serious features users cared about. Openwebui overtook them and they haven‚Äôt come back. No commits since mid 2024 for 1.5 years. I would not recommend a dead project.",
          "score": 3,
          "created_utc": "2025-12-27 14:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8b7c1",
          "author": "datbackup",
          "text": "textgen webui seems to not have a way to use its front end with a backend other than the one it packages. So while the name sounds like it would be a competitor for openwebui, in fact it‚Äôs more like a competitor for ollama. Happy to be proven wrong about this if i simply couldn‚Äôt manage to find the config area that allows entering a url for the inference engine",
          "score": 2,
          "created_utc": "2025-12-27 18:32:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwadl9f",
          "author": "sirebral",
          "text": "Dunno, looking at this low effort article and it's pretty much all bloatware, upsells for paid mcp, or not even a chat inference.  Lame.",
          "score": 2,
          "created_utc": "2025-12-28 01:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa7njz",
          "author": "throwawayacc201711",
          "text": "I wish this would actually highlight differences between them. The descriptions are fairly generic.\n\nA table with some key features and showing what each project does and does not support would be significantly more useful IMO. \n\nReading this, it really doesn‚Äôt give a reader a good idea on why to choose one over the other. Just some friendly feedback",
          "score": 1,
          "created_utc": "2025-12-28 00:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg37y5",
          "author": "spiffco7",
          "text": "Cherry, Witsy, Goose",
          "score": 1,
          "created_utc": "2025-12-28 23:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwphae6",
          "author": "Civil_Swimmer_4162",
          "text": "[https://github.com/przbadu-ai/LocalMind](https://github.com/przbadu-ai/LocalMind) \\- I started this opensource app, where it auto extract websites contents (if user permits), it extract Youtube transcription, summarize them, allow user to upload documents (pdf, audio, txt, markdown, html), etc and chat with them. It is in early phase and planning to add more features. You can also configure MCP. Please feel free to contribute to it.",
          "score": 1,
          "created_utc": "2025-12-30 10:09:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwy88a",
          "author": "Zyj",
          "text": "It starts with Open-WebUI which is no longer using an OSI approved open source license.\n\nNext: LobeChat, also not open source.\n\nAt #7 Flowise, also not open source.\n\nPlease downvote the original post.",
          "score": 1,
          "created_utc": "2025-12-31 14:02:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwa0ro",
      "title": "78 stars in github but 0 feedback, how to improve my local AI application ?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/hzqxqjey0l9g1.png",
      "author": "Suspicious-Juice3897",
      "created_utc": "2025-12-26 17:38:33",
      "score": 19,
      "num_comments": 5,
      "upvote_ratio": 0.7,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwa0ro/78_stars_in_github_but_0_feedback_how_to_improve/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw1zb9y",
          "author": "boraiross",
          "text": "I'm one of the devs, here is a link to the repo: [https://github.com/Tbeninnovation/Baiss](https://github.com/Tbeninnovation/Baiss) , keep adding more stars but DO NOT send any feedbacks",
          "score": 3,
          "created_utc": "2025-12-26 17:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw34i3l",
          "author": "danny_094",
          "text": "I don't understand the feedback thing either.\n\nMy application also regularly gets clones, sometimes 30 a day, sometimes fewer, etc. Especially after updates. So there are regular users. But I think feedback is rare for smaller projects? Just keep going :)\n\n\n` ...)`````)``) ``) ``) `\n\n``\n``\n``\n``",
          "score": 1,
          "created_utc": "2025-12-26 21:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3fwph",
              "author": "Suspicious-Juice3897",
              "text": "yeah maybe, also, what's yours ? I can test it out",
              "score": 1,
              "created_utc": "2025-12-26 22:24:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw21mr0",
          "author": "DraGSsined",
          "text": "I‚Äôve already used it, and it looks pretty good. However, you can‚Äôt build a great project without user feedback.",
          "score": 0,
          "created_utc": "2025-12-26 17:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw22xjm",
              "author": "Suspicious-Juice3897",
              "text": "appreciate it, trying to get one, can we start with your feedback ? :)",
              "score": 2,
              "created_utc": "2025-12-26 18:00:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1put1f5",
      "title": "Is there a rule of thumb in deciding which model to use?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1put1f5/is_there_a_rule_of_thumb_in_deciding_which_model/",
      "author": "salty_salad13",
      "created_utc": "2025-12-24 17:47:43",
      "score": 18,
      "num_comments": 19,
      "upvote_ratio": 0.95,
      "text": "Hi! Im relatively new to this local LLM setup, and wanted to understand some basics fundamentals and to upskill in ai environment. Below is my pc specs.\n\nCPU: AMD Ryzen 7 8700F\nMOBO: Gigabyte A620M-H\nRAM: 16GB Lexar Thor (8GB√ó2) DDR5 6000\nSTORAGE: 500GB Lexar NM610 Pro NVMe SSD\nGPU: 8GB RX6600 ASRock Challenger (Dual Fan)\n\nOk so let me give some context, Currently, i have ollama llma3.1:8b running on windows with Open WebUI, i just followed instructions from chatgpt. Basically im just overwhelmed by the total steps right now that there's a lot of prerequisite apps and installation that is needed for it to work, like docker, wsl etc... also given the fact that im not realy into coding, though i have some small background.\n\nMy question is, Is there a UI that is windows friendly version?\n\nNext is, how do i pick a model that can run smoothly on my pc setups, is there like a 1:1 or 1:2 ratio in terms of ram/vram?\n\nLastly, from current setup, i don't think im fully utilizing my gpu resources, i asked chatgpt about this, but im still quite loss.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1put1f5/is_there_a_rule_of_thumb_in_deciding_which_model/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvr10kv",
          "author": "StardockEngineer",
          "text": "Just install LM Studio. One stop shop.  It will also tell you what models can fit on your VRAM.  \n\nBut your specs are extremely low so you‚Äôll be able to run only the smallest models.   You don‚Äôt have any RAM to donate to LLM inferencing.",
          "score": 16,
          "created_utc": "2025-12-24 17:53:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvr2b6z",
              "author": "salty_salad13",
              "text": "I see, I'll check on that",
              "score": 3,
              "created_utc": "2025-12-24 18:00:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvr5fql",
          "author": "little___mountain",
          "text": "Download LM Studio. Select a model name where the B value = your GPU memory size. Then set context = to your Ram size. So you can theoretically run up to a 8B model with a 16k token context window.",
          "score": 8,
          "created_utc": "2025-12-24 18:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvum9hw",
              "author": "salty_salad13",
              "text": "Thankyou, i might consider upgrading my ram and gpu, but the market really sucks right now hahah",
              "score": 2,
              "created_utc": "2025-12-25 09:48:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvhn9s",
                  "author": "little___mountain",
                  "text": "My pleasure. There's no finding cheap ram, but the used GPU market is still pretty good (for now). I've heard tales of some people circumventing the RAM pricing by buying old server GPUs like the Nvidia Telsa which is on amazon for 32GBs for $400, since VRAM can hold context too. Those cards come with other headaches though... Anyways good luck to you and Merry Christmas.",
                  "score": 2,
                  "created_utc": "2025-12-25 14:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr16dr",
          "author": "Kindly_Initial_8848",
          "text": "llm's consume more GPU, yours is pretty much on the spot for the number of parameters.\n\n  \ntry LM studio, its easier to use on windows",
          "score": 6,
          "created_utc": "2025-12-24 17:54:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw7rds",
              "author": "salty_salad13",
              "text": "Thankyou!, i just installed lm studio and really liking it right now, the simplicity is on spot, and it recommended me the gpt-oss-20b right away, and it run smoothly, tbh i don't think it'll run smoothly, but it does hahah. Though i noticed that it does not references other chats, but its a another topic to be learn along the way",
              "score": 1,
              "created_utc": "2025-12-25 17:18:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwaspf",
                  "author": "Kindly_Initial_8848",
                  "text": "im glad this information helped you\n\nlm studio may have a plugin or configuration to setup memory across chats, but im not sure this information is correct, but you can use the llm to help you",
                  "score": 2,
                  "created_utc": "2025-12-25 17:36:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr7p9y",
          "author": "HumanDrone8721",
          "text": "I've just finished a new build, nothing spectacular just an I7-14KF, 128GB DDR5-5200 and an RTX 4090, and feeling optimistic I've tested a model with different degree of RAM contribution and got these results: \n\n    | % layers on GPU | `ngl` | `tg128` (tok/s) | vs full GPU |  slowdown vs full |\n    | --------------- | ----: | --------------: | ----------: | ----------------: |\n    | 0%              |     0 |       **16.07** |       ~8.3% |  **‚âà 12√ó slower** |\n    | 25%             |    13 |       **23.08** |      ~11.9% | **‚âà 8.4√ó slower** |\n    | 50%             |    26 |       **32.55** |      ~16.8% |   **‚âà 6√ó slower** |\n    | 75%             |    39 |       **52.60** |      ~27.2% | **‚âà 3.7√ó slower** |\n    | 100%            |   999 |      **193.66** |        100% |          baseline |\n\nThat was the script:\n\n    cd ~/Projects/llama.cpp/build/bin\n\n    MODEL=~/Nemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf\n\n    for NGL in 0 13 26 39 999; do\n        echo \"==== ngl = $NGL ====\"\n        ./llama-bench \\\n          -m \"$MODEL\" \\\n          -ngl $NGL \\\n          -t 16 \\\n          -p 4096\n    done\n\nSo I would say, use whatever model fits your interest, it just have to fit in the VRAM.",
          "score": 3,
          "created_utc": "2025-12-24 18:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrk7uy",
              "author": "Brilliant-Ice-4575",
              "text": "What do you think is better: Strix Halo with 96GB of VRAM or 4090 with 24GB of VRAM? They cost about the same...",
              "score": 2,
              "created_utc": "2025-12-24 19:38:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrqksa",
                  "author": "HumanDrone8721",
                  "text": "Do you really want to start a holly war on Christmas eve ? Because sincerely this is how you do it. Anyways, **for the models that fits 100% in VRAM** the 4090 will win hands down, for models that doesn't fit in the 24GB VRAM, Strix wins hands down, even if less speedy than a dedicate Nvidia GPU, is still very fast compared with any type of simple system RAM. So if your interests doesn't include CUDA stack learning and research and you can live with a \"smarter\" model even if is not a \"fast talker\" go with the Strix.",
                  "score": 4,
                  "created_utc": "2025-12-24 20:15:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu0q5m",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-25 06:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvunl6q",
              "author": "salty_salad13",
              "text": "Thankyouu, im considering switching to lm studio rn to lessen the overhead layers, and free up some headroom in ram and storage.",
              "score": 1,
              "created_utc": "2025-12-25 10:03:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu8ljv",
          "author": "Witty-Development851",
          "text": "LM Studio )",
          "score": 2,
          "created_utc": "2025-12-25 07:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvr8h3l",
          "author": "Just3nCas3",
          "text": "Focus on mix of expert models, just for fun see if you can run Qwen3-30B-A3B-Thinking-2507-unsloth-MagicQuant-Hybrid-GGUF at mxfp4 quant is around 18gb I think it might be the close to the max model you can run. The other person right lm studio has a great starter ui. I think gbt oss 20B is moe so I'd try that next if qwen doesn't fit. Don't be afraid of low quants but avoid dropping under Q4 or equivalent. Anything quanted by unsloth is a good starting point.",
          "score": 2,
          "created_utc": "2025-12-24 18:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvumcg0",
              "author": "salty_salad13",
              "text": "Tnx, I'll look into it",
              "score": 3,
              "created_utc": "2025-12-25 09:49:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvw84q3",
          "author": "salty_salad13",
          "text": "Hi guys, thankyouu for all of your valuable feedback, i learned a lot hehe, my next project will be cross reference chat, if you guys have any tips and tricks feel free to drop, thankyou!",
          "score": 1,
          "created_utc": "2025-12-25 17:20:59",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxn4nh",
      "title": "GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fd7m7g23ww9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-28 09:17:24",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxn4nh/glm_47_is_now_the_1_open_source_model_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwcbfj3",
          "author": "arousedsquirel",
          "text": "lots of promo and even more ccp gaurdrails. i think zai interpreted the 2000 question list to strict.",
          "score": 5,
          "created_utc": "2025-12-28 10:04:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py9m5q",
      "title": "Requested: Yet another Gemma 3 12B uncensored",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "author": "Mabuse046",
      "created_utc": "2025-12-29 02:09:36",
      "score": 14,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hello again!\n\nYesterday I released my norm preserved biprojected abliterated Gemma 3 27B with the vision functions removed and further fine tuned to help reinforce the neutrality. I had a couple of people ask for the 12B version which I have just finished pushing to the hub. I've given it a few more tests and it has given me an enthusiastic thumbs up to some really horrible questions and even made some suggestions I hadn't even considered. So... use at your own risk.\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF)\n\nLink to the 27B redit post:  \n[Yet another uncensored Gemma 3 27B](https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using ¬†**L**ayerwise¬†**I**mportance¬†**S**ampled¬†**A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwihhi4",
          "author": "darkbit1001",
          "text": "I ran with ollama (ollama run hf.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF:Q4\\_K\\_M) and it just repeats over and over the word 'model'. any reason this would happen?",
          "score": 3,
          "created_utc": "2025-12-29 08:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjayep",
              "author": "Mabuse046",
              "text": "Thank you for pointing this out. I'm looking into it and finding there were apparently some configuration issues in the original Google models, particularly in the way they handled the BOS token that have given some ollama users a headache with Gemma 3 GGUF's. I am currently editing my config.json files and adding the chat template in three different places on both models based on the Unsloth fix and will push fresh gguf's shortly.",
              "score": 3,
              "created_utc": "2025-12-29 12:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwugiab",
                  "author": "lookwatchlistenplay",
                  "text": "This isn't the model escaping confinement... is it?",
                  "score": 1,
                  "created_utc": "2025-12-31 02:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjs5nv",
              "author": "Mabuse046",
              "text": "Fresh ggufs have been pushed and the original transformers versions have been updated. I don't normally use ollama but I went ahead and installed it to try it out. I used the run command with the hf repo and it chatted just fine in the terminal. I connected to it in SillyTavern to give it another test and it took some fiddling but I got it to hold a conversation just fine in there in both Chat Completions and Text Completions mode.",
              "score": 3,
              "created_utc": "2025-12-29 14:27:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxcdfyd",
                  "author": "darkbit1001",
                  "text": "Thanks, should I use a different template? right now it repeats the n-word and tells me it want to f\\*\\*k me over and over ü´†",
                  "score": 1,
                  "created_utc": "2026-01-02 23:30:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwh2ymo",
          "author": "3-goats-in-a-coat",
          "text": "I'll try using it with EchoColony in rimworld. Thanks.",
          "score": 1,
          "created_utc": "2025-12-29 02:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqmooc",
          "author": "Dramatic-Rub-7654",
          "text": "If it‚Äôs not a bother and if you‚Äôre able to, could you do the same with one of TheDrummer‚Äôs versions? TheDrummer/Fallen-Gemma3-27B-v1 or TheDrummer/Fallen-Gemma3-12B-v1.",
          "score": 1,
          "created_utc": "2025-12-30 15:00:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx17fab",
              "author": "Mabuse046",
              "text": "https://preview.redd.it/dsif5fmd1oag1.png?width=1399&format=png&auto=webp&s=9acd92fd41f6b9b52a9ef8426c9fd8cf6626cfbd\n\nCurrent status... first I realized that Drummer has the config.json for 12B duplicated in his 27B, which had some incorrect dimensions so I had to correct it and test it locally, but then, I'm getting some weird measurements when I try to abliterated it that make it look like they already abliterated it and either didn't get it completely, or they added a small amount of their own back in, it's hard to say. But the divergence between harmful and harmless is practically non-existent.",
              "score": 3,
              "created_utc": "2026-01-01 04:32:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx1a2l1",
                  "author": "Dramatic-Rub-7654",
                  "text": "This is very strange, because this model clearly retains safety traits from the original model. I ran several tests trying to merge it with other Gemma Heretic models I found on Hugging Face, and in every merge attempt, questions that the Heretic versions answered without any issue would cause the merged model to refuse to respond. I also tried generating a LoRA from the difference between this Fallen model and the official Instruct version, but that didn‚Äôt work either, which makes me think that the model they shared was already fine-tuned somewhere else.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrljfp",
              "author": "Mabuse046",
              "text": "I'll have a look at it. Currently have my system working on beefing up my dataset. Should have some free time shortly.",
              "score": 2,
              "created_utc": "2025-12-30 17:46:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrpr1x",
                  "author": "Legal_Pudding_4464",
                  "text": "I would second this request, but regardless thanks for this model!",
                  "score": 1,
                  "created_utc": "2025-12-30 18:05:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwukk76",
                  "author": "Dramatic-Rub-7654",
                  "text": "Thanks a lot, no rush at all. When you manage to publish it, please give me a heads-up. In my case, I‚Äôm only interested in the text layers, so if you remove the vision part, that‚Äôs totally fine with me.",
                  "score": 1,
                  "created_utc": "2025-12-31 02:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0j55m",
      "title": "OpenCV 4.13 brings more AVX-512 usage, CUDA 13 support, many other new features",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/OpenCV-4.13-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2025-12-31 17:56:27",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0j55m/opencv_413_brings_more_avx512_usage_cuda_13/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q1k83q",
      "title": "Is it possible to have a local LLM update spreadsheets and read PDFs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "author": "new-to-reddit-accoun",
      "created_utc": "2026-01-02 00:41:31",
      "score": 13,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "So far I've tried [Jan.ai](http://Jan.ai) (Jan-v1-4B-Q4\\_K\\_M) and Msty (Qwen3:0.6b) with no luck: the model in Jan says it can't output an updated file, and Mysty's model claims to but won't give the path name to where it's allegedly saved it. \n\nRelated, I'm looking for a local LLM that can read PDFs (e.g. bank statements). \n\nUse case I'm trying to build a local, private app that reads bank/credit card statements, and also update various values in a spreadsheet. \n\nWould love suggestions!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx68hde",
          "author": "SignificantCod728",
          "text": "You might be looking for something like Actual Budget.",
          "score": 5,
          "created_utc": "2026-01-02 00:49:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx69tcx",
          "author": "No-Consequence-1779",
          "text": "Yes. You‚Äôll use Python to call the LLM api, and then update the spreadsheets. You can even highlight specific cells. - basically, do anything to excel sheets with Python. Same for pdf - reading is simple. ¬†",
          "score": 3,
          "created_utc": "2026-01-02 00:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb3jgu",
              "author": "new-to-reddit-accoun",
              "text": "Thank you this the least friction solution it sounds like",
              "score": 1,
              "created_utc": "2026-01-02 19:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc0vah",
                  "author": "No-Consequence-1779",
                  "text": "Yes. Python is originally for data science so there is so much available working with sheets and documents. ¬†A LLM can craft a pretty solid starter script for you )¬†",
                  "score": 1,
                  "created_utc": "2026-01-02 22:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6mszf",
          "author": "Porespellar",
          "text": "This works great for me with Open WebUI \n\nhttps://github.com/GlisseManTV/MCPO-File-Generation-Tool",
          "score": 3,
          "created_utc": "2026-01-02 02:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9c2et",
          "author": "l_Mr_Vader_l",
          "text": "you'd need VLMs, LLMs are not gonna cut it. Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nStart with qwen3-VL 8B, it's a sweet spot. If you have the infra and need super good accuracy, go for 32B.\n\n\nYou don't need an LLM to write spreadsheets, simple openpyxl and pandas should do the job.\n\nNone of the other replies here make sense",
          "score": 3,
          "created_utc": "2026-01-02 14:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa49ru",
              "author": "pantoniades",
              "text": ">Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nWhat is the advantage of OCR over extracting text first? Curious because I had just assumed the opposite - that reading the pdfs in python and extracting text would leave less room for errors... \n\nI do like the idea of less code to maintain, of course!",
              "score": 1,
              "created_utc": "2026-01-02 16:55:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxaiuiw",
                  "author": "Whoa_PassTheSauce",
                  "text": "VLM isn't OCR technically as far as I'm aware, and while I have not done this locally, even using flagship model API's I have found text extraction better handled by the model vs extraction on my side and feeding the results.\n\nMy use case involves extracting data from pdf's and images, same layout usually but the type or file varies.",
                  "score": 1,
                  "created_utc": "2026-01-02 18:04:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxd49rv",
                  "author": "l_Mr_Vader_l",
                  "text": "One word -  layouts\n\nThere's no pdf reader that preserves layouts accurately, all the time. Everyone writes pdf differently, there's no standard to this. You wanna read all kinds of tables accurately all the time, no deterministic pdf reader can do that for you that'll work on all kinds of PDFs\n\nWith LLMs no matter how big of a SOTA model you use, you end up feeding garbage essentially that came out of a deterministic text extractor, they will struggle at complex layouts\n\n\nBut some VLMs are getting there now, because they read it how PDFs are meant to be read. Through vision!",
                  "score": 1,
                  "created_utc": "2026-01-03 02:01:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7p9z7",
          "author": "ChemistNo8486",
          "text": "Not sure about the spreadsheet, but you can use AnythingLLM and add a vision model like QWEN for reading PDFs. \n\nYou can also add an agent to save .txt documents. You probably can add something to convert them.",
          "score": 1,
          "created_utc": "2026-01-02 06:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8weya",
          "author": "fandry96",
          "text": "Gemma might help you. The way it can nest can protect your data.",
          "score": 1,
          "created_utc": "2026-01-02 13:02:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv5ar7",
      "title": "Built a Rust CLI for converting SafeTensors to GGUF. Looking for feedback.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pv5ar7/built_a_rust_cli_for_converting_safetensors_to/",
      "author": "Anxious-Visit-7735",
      "created_utc": "2025-12-25 04:32:28",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 0.93,
      "text": "[https://github.com/kgrama/gmat-cli](https://github.com/kgrama/gmat-cli)\n\nWhy i built it,no waiting around for quants.Also saves space, 1 f32/bf16 model download = n quants\n\nKey features:\n\n\\- CPU-only, no GPU required\n\n\\- Per-tensor quant overrides (Q8\\_0 embeddings, Q4\\_K\\_M elsewhere)\n\n\\- Trellis optimization (\\~1% perplexity gain)\n\n\\- IQ4\\_XS/IQ4\\_NL support\n\n\n\nArchitectures:\n\n Llama, Qwen, Phi, Gemma, DeepSeek, Mistral, LLaVA, Qwen-VL, Kimi-VL, Whisper, T5\n\n\n\nWhy not llama.cpp directly?\n\nOne GMAT folder ‚Üí multiple GGUF outputs. Change quant (json file)  profile without re-processing.\n\n\n\nFeedback wanted: Missing quant types? Architectures to add? ",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pv5ar7/built_a_rust_cli_for_converting_safetensors_to/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvvnb66",
          "author": "Hurricane31337",
          "text": "Nice one, thanks for sharing! I could need support for GLM 4.6, 4.7 and MiniMax 2.0 üôÇ",
          "score": 1,
          "created_utc": "2025-12-25 15:14:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw8kj0",
              "author": "Anxious-Visit-7735",
              "text": "Thats WIP, will probably be in the  new year.",
              "score": 1,
              "created_utc": "2025-12-25 17:23:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pw1ukl",
      "title": "Is there a True Deep Research / MCP Deep Research for Local AI ?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pw1ukl/is_there_a_true_deep_research_mcp_deep_research/",
      "author": "iwannaredditonline",
      "created_utc": "2025-12-26 11:07:30",
      "score": 12,
      "num_comments": 16,
      "upvote_ratio": 0.93,
      "text": "Hey guys,\n\nComing from platforms like Chatgpt and Gemini, being able to ask any question about a business, brand, platform, entity or news is very easy and it seems their AI models are up to date with the latest information.\n\nUsing software such as LM Studio and Clara verse, are there any quality and error free Deep Research MCP's that work without using a search engine's API Keys? I've tried Playwright (which doesnt seem to be the correct software unless I am misinformed), duckduckgo mcp, sear xng and the old MCP project puppeteer. I couldnt get the last to work. Ive gotten duckduckgo mcp from the LM studio repo to work, but I feel like these mcp's are just a hit or miss..\n\nIs there really any way to make this work efficiently without errors or issues, and similar to other AI Platforms that thoroughly research upon request?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw1ukl/is_there_a_true_deep_research_mcp_deep_research/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw0c5h3",
          "author": "hlacik",
          "text": "well i hate to bring that to you ,but gemini has built-in google search mcp, thats why it is so superior. so you are on good path using duck duck go mcp server, deep research is just multi-agent workflow, which you can implemenet using tools like langgraph...",
          "score": 6,
          "created_utc": "2025-12-26 11:19:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3nwpt",
              "author": "iwannaredditonline",
              "text": "Thanks for this. I figured this would be the case. Any direction for tutorials on how to put it all together?",
              "score": 1,
              "created_utc": "2025-12-26 23:10:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5zsgk",
                  "author": "hlacik",
                  "text": "https://github.com/langchain-ai/open_deep_research",
                  "score": 2,
                  "created_utc": "2025-12-27 09:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0c8ci",
          "author": "brianlmerritt",
          "text": "Are you trying to save money or do you have a specific requirement?\n\nFirst of all, the paid tools are not error free and are not up to date with the latest information.  They are pretty good and pretty recent, but they often have to use search to find the latest information about fast moving topics.\n\nThere are local AI research tools, most of them require some search capability which often requires a key above a certain usage.  So if you are doing a lot of research then some cost is going to come into the equation or else your research will likely be constrained.  \n  \nps - Playwright is a browser automation tool - you can use it to perform searches but even then it will be using some search engine and there are much more efficient ways to conduct search / research.",
          "score": 5,
          "created_utc": "2025-12-26 11:20:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4h281",
              "author": "No-Consequence-1779",
              "text": "What other ways are more efficient for research please? ¬†",
              "score": 1,
              "created_utc": "2025-12-27 02:09:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7xzlf",
              "author": "iwannaredditonline",
              "text": "Host everything locally instead of signing up for subscription services. Purchased an entire pc for AI not to just go back to 3rd party platform controlling your data or access to data.",
              "score": 1,
              "created_utc": "2025-12-27 17:26:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8asoe",
                  "author": "brianlmerritt",
                  "text": "I have been shot down in flames (reddit style, not irl) for suggesting buying a gaming pc with RTX 3090. It keeps my office warm, I do also play games, it's ready for any AI tasks too.\n\nI find the search part of research is the bit that needs API keys to really get everything going, and if you use a generic search engine with a browser running it you are of course sharing whatever you are looking for with the search engine company anyways.",
                  "score": 1,
                  "created_utc": "2025-12-27 18:30:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw10adw",
          "author": "Vegetable-Second3998",
          "text": "Firecrawl and Tavily both have free tiers and search agents.",
          "score": 3,
          "created_utc": "2025-12-26 14:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2gndw",
          "author": "Purple-Programmer-7",
          "text": "I‚Äôve been using https://github.com/assafelovic/gpt-researcher.\n\nHe‚Äôs got an mcp version of it too.",
          "score": 2,
          "created_utc": "2025-12-26 19:11:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3cd2t",
              "author": "iwannaredditonline",
              "text": "Thanks for this. Does it require API keys for search platforms? I didn't see an option to use it locally without api keys",
              "score": 1,
              "created_utc": "2025-12-26 22:04:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3cyus",
                  "author": "Purple-Programmer-7",
                  "text": "Per codewiki.google (literally copy/pasta):\n\n````markdown\n\nYes, you can initialize GPT Researcher to use with a local model by configuring it to use [`ollama`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L101) or [`huggingface`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L221) providers.\n\nThe [`GenericLLMProvider`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py#L89) class in [`gpt_researcher/llm_provider/generic/base.py`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fsupported-llms.md#L30) supports several LLM providers, including:\n*   [`ollama`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L101): This provider is initialized using [`ChatOllama`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py#L143) and requires [`OLLAMA_BASE_URL`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fmemory%2Fembeddings.py#L91) to be set in the environment.\n*   [`huggingface`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L221): This provider is initialized using [`ChatHuggingFace`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py#L158).\n\nTo use one of these local model providers, you need to set the [`FAST_LLM`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fconfig%2Fconfig.py#L97) and/or [`SMART_LLM`](%2Fassafelovic%2Fgpt-researcher%2Fmulti_agents%2FREADME.md#L11) environment variables (or similar configuration in a custom [`config.json`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fgptr%2Fconfig.md#L17) file) to specify the provider and model. For example:\n\n```bash\nexport FAST_LLM=\"ollama:your-local-ollama-model-name\"\nexport OLLAMA_BASE_URL=\"http://localhost:11434\" # Or your Ollama server address\n```\n\nor for HuggingFace:\n\n```bash\nexport FAST_LLM=\"huggingface:your-local-huggingface-model-id\"\n```\n\nThe [`Config`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fconfig%2Fconfig.py#L11) class in `gpt_researcher/config/config.py` parses these LLM strings to determine the provider and model, as detailed in [LLM Configuration and Management](#language-model-integration-llm-configuration-and-management). The [`GenericLLMProvider.from_provider`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Futils%2Fllm.py#L20) method then dynamically instantiates the appropriate Langchain chat model, handling API keys and base URLs (often from environment variables), as explained in [Generic LLM Provider Interface](#language-model-integration-generic-llm-provider-interface). The system also automatically checks for and installs necessary [`langchain`](%2Fassafelovic%2Fgpt-researcher%2FREADME.md#L242) integration packages like [`langchain_ollama`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fmemory%2Fembeddings.py#L87) or [`langchain_huggingface`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fmemory%2Fembeddings.py#L107) if they are not already present, as described in [Chat Logging and Dependency Management](#language-model-integration-chat-logging-and-dependency-management).\n````",
                  "score": 2,
                  "created_utc": "2025-12-26 22:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4h4lo",
          "author": "Beautiful_Frosting34",
          "text": "https://github.com/PrabhaAnde/deep-research-agent\n\nTry checking this out , it has well documented details that can run with docker containers where in your local ai llm can be run through docker container too.  It was built with super basic custom ui and has financial deep research along duty regular query based deep research. \n\nIf you any any additional features added such as better ui ux or any other features , I can plan to add them. \n\nAlso. It need a serpapi key.  You can create a simple account in serpapi and then create a key to use. It has 200 searches free for month and has comprehensive search api options.  I can change them if you need better search apis.\n\nhttps://serpapi.com\n\n\nOfcourse if you are developer,  you are free to modify this and use according to your wish",
          "score": 2,
          "created_utc": "2025-12-27 02:10:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q14nkv",
      "title": "DeepSeek AI Launches mHC Framework Fixing Major Hyper Connection Issues in Massive LLM",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u40qcfe6tqag1.jpeg",
      "author": "techspecsmart",
      "created_utc": "2026-01-01 13:51:23",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q14nkv/deepseek_ai_launches_mhc_framework_fixing_major/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pwyu6k",
      "title": "I built a GraphRAG application to visualize AI knowledge (Runs 100% Local via Ollama OR Fast via Gemini API)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pwyu6k/i_built_a_graphrag_application_to_visualize_ai/",
      "author": "Dev-it-with-me",
      "created_utc": "2025-12-27 14:16:42",
      "score": 11,
      "num_comments": 5,
      "upvote_ratio": 0.87,
      "text": "Hey everyone,\n\nFollowing up on my last project where I built a standard RAG system, I learned a ton from the community feedback.\n\nWhile the local-only approach was great for privacy, many of you pointed out that for¬†**GraphRAG**¬†specifically‚Äîwhich requires heavy processing to extract entities and build communities‚Äîlocal models can be slow on larger datasets.\n\nSo, I decided to level up. I implemented¬†**Microsoft's GraphRAG**¬†with a flexible backend. You can run it¬†**100% locally**¬†using Ollama (for privacy/free testing)¬†**OR**¬†switch to the¬†**Google Gemini API**¬†with a single config change if you need production-level indexing speed.\n\nThe result is a chatbot that doesn't just retrieve text snippets but understands the¬†*structure*¬†of the data. I even added a visualization UI to actually¬†*see*¬†the nodes and edges the AI is using to build its answers.\n\nI documented the entire build process in a detailed tutorial, covering the theory, the code, and the deployment.\n\n**The full stack includes:**\n\n* **Engine:**¬†Microsoft GraphRAG (official library).\n* **Dual Model Support:**\n   * *Local Mode:*¬†Google's Gemma 3 via¬†**Ollama**.\n   * *Cloud Mode:*¬†**Gemini API**¬†(added based on feedback for faster indexing).\n* **Graph Store:**¬†LanceDB + Parquet Files.\n* **Database:**¬†PostgreSQL (for chat history).\n* **Visualization:**¬†React Flow (to render the knowledge graph interactively).\n* **Orchestration:**¬†Fully containerized with Docker Compose.\n\n**In the video, I walk through:**\n\n* **The Problem:**\n   * Why \"Classic\" RAG fails at reasoning across complex datasets.\n   * What path leads to Graph RAG ‚Üí throuh Hybrid RAG\n* **The Concept:**¬†A visual explanation of Entities, Relationships, and Communities & What data types match specific systems.\n* **The Workflow:**¬†How the system indexes data into a graph and performs \"Local Search\" queries.\n* **The Code:**¬†A deep dive into the Python backend, including how I handled the switch between local and cloud providers.\n\n**You can watch the full tutorial here:**\n\n[https://youtu.be/0kVT1B1yrMc](https://youtu.be/0kVT1B1yrMc)\n\n**And the open-source code (with the full Docker setup) is on GitHub:**\n\n[https://github.com/dev-it-with-me/MythologyGraphRAG](https://github.com/dev-it-with-me/MythologyGraphRAG)\n\nI hope this hybrid approach helps anyone trying to move beyond basic vector search. I'm really curious to hear if you prefer the privacy of the local setup or the raw speed of the Gemini implementation‚Äîlet me know your thoughts!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwyu6k/i_built_a_graphrag_application_to_visualize_ai/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx0cvil",
          "author": "Fit-Presentation-591",
          "text": "You might have some interest in [https://github.com/colliery-io/graphqlite](https://github.com/colliery-io/graphqlite) if you go down the knowledge graph / graph rag rabbit hole at all.",
          "score": 1,
          "created_utc": "2026-01-01 01:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw70wy5",
          "author": "Zamiatacz",
          "text": "GitHub ling have 2 ** at the end what causes 404",
          "score": 1,
          "created_utc": "2025-12-27 14:30:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw70zya",
              "author": "Zamiatacz",
              "text": "Looking great btw",
              "score": 1,
              "created_utc": "2025-12-27 14:31:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw729ih",
              "author": "Dev-it-with-me",
              "text": "Thank You! Fixed :D",
              "score": 1,
              "created_utc": "2025-12-27 14:38:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw70r5t",
          "author": "Conscious-Pool8744",
          "text": "I really appreciate the clear project on which I can base further development!",
          "score": 0,
          "created_utc": "2025-12-27 14:29:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1px1atx",
      "title": "My stack for cleaning RAG datasets: A comparison of Unstructured, LangChain, and a custom local approach (results inside)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1px1atx/my_stack_for_cleaning_rag_datasets_a_comparison/",
      "author": "Low-Flow-6572",
      "created_utc": "2025-12-27 16:04:51",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI've been iterating on a local RAG pipeline for documentation search, and the biggest bottleneck wasn't the LLM inference speed, it was the retrieval quality. I realized my vector store was full of duplicate chunks, boilerplate legalese, and \"low-entropy\" garbage (like 500 copies of a copyright footer).\n\nI spent the last two weeks testing different tools to clean the data *before* embedding. Here is my honest breakdown of the landscape, from \"Heavyweight\" to \"Lightweight\".\n\n# 1. The Heavyweight: [Unstructured.io](http://Unstructured.io)\n\nThis is the go-to for parsing weird formats.\n\n* **Pros:** Incredible at ripping text out of complex PDFs and tables. If you have messy source files, start here.\n* **Cons:** It is HEAVY. The dependencies are massive, and processing time can be slow.\n* **Verdict:** Essential for ingestion/parsing, but overkill if you just need to clean/deduplicate JSONL or plain text.\n\n# 2. The Default: LangChain (RecursiveSplitter + Regex)\n\nWhat 90% of tutorials use.\n\n* **Pros:** Built-in, zero extra setup.\n* **Cons:** It's \"dumb\" slicing. It doesn't check for semantic duplicates. If you have the same paragraph on page 5 and page 50, both go into your Vector DB, polluting the search results.\n* **Verdict:** Good for prototyping, bad for production quality.\n\n# 3. The Enterprise Scale: HuggingFace text-dedup\n\nUsed for training massive datasets (like The Pile).\n\n* **Pros:** Uses MinHash LSH + Spark. Extremely scalable for terabytes of data.\n* **Cons:** Overkill for a local RAG setup. Setting up a Spark cluster just to clean a 2GB dataset is painful.\n* **Verdict:** Great for pre-training models, too complex for RAG pipelines.\n\n# 4. The \"Middle Ground\": EntropyGuard (My Local Project)\n\nI couldn't find a tool that was **semantic** (like embeddings) but **lightweight** (runs on a laptop), so I built a CLI tool using Polars and FAISS.\n\n* **The Approach:** It uses a hybrid pass. First, `xxhash` removes exact duplicates (fast). Then, it uses a small sentence-transformer model to find semantic duplicates (e.g., \"Error 500\" vs \"Server Error 500\") and removes them based on vector distance.\n* **Pros:**\n   * Runs locally (no API costs).\n   * Uses **Polars LazyFrames**, so it handles datasets larger than RAM (I processed 65k docs on 16GB RAM without OOM).\n   * Filters out \"low entropy\" chunks (repetitive noise).\n* **Cons (Being honest):**\n   * CLI only (no GUI).\n   * Currently optimized for English (multilingual is experimental).\n   * Docs are still a work in progress compared to LangChain.\n\n# My Question for you:\n\nI'm currently betting on **semantic deduplication** (checking meaning) rather than just regex cleaning.\n\n**What is your strategy for \"dirty\" data in RAG?** Do you just throw everything into Pinecone/Chroma and hope the re-ranker sorts it out, or do you have a specific pre-processing pipeline?\n\n*Full disclosure: I am the maintainer of tool #4 (EntropyGuard). I built it because I kept OOMing my laptop with custom Pandas scripts. If you want to check the code or roast my implementation:* [https://github.com/DamianSiuta/entropyguard](https://github.com/DamianSiuta/entropyguard)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1px1atx/my_stack_for_cleaning_rag_datasets_a_comparison/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwexye6",
          "author": "mtbMo",
          "text": "Could this being used and automate with N8n? I‚Äôm trying to use the rag features in open webui and it‚Äôs unstable.",
          "score": 1,
          "created_utc": "2025-12-28 19:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfaty9",
              "author": "Low-Flow-6572",
              "text": "When it comes to EntropyGuard:\nShort answer: Yes, absolutely, but right now you need to run n8n in self-hosted mode (Docker).\nSince EntropyGuard is currently CLI-only, you can't hit it via HTTP yet. You have to orchestrate it using the Execute Command node.\nThe Workflow (Current):\n1. Setup: Add RUN pip install entropyguard to your n8n Dockerfile (so the binary is inside the container).\n2. n8n Node: Use the Execute Command node.\n‚Ä¢ Command: entropyguard clean --input /data/raw.jsonl --output /data/clean.jsonl\n3. Open WebUI: Then have n8n push that clean JSONL into your vector store.\nThe Roadmap (Future):\nI realize hacking Dockerfiles is annoying for low-code workflows. I‚Äôm planning to add an optional API mode (something like entropyguard serve) in the next release. That way, you'll be able to just use the standard HTTP Request node in n8n/Flowise/Zapier to clean text batches on the",
              "score": 2,
              "created_utc": "2025-12-28 20:40:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8gjcj",
          "author": "Crafty_Ball_8285",
          "text": "What is RAG? You didn‚Äôt define it anywhere.",
          "score": 0,
          "created_utc": "2025-12-27 18:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8hmon",
              "author": "Low-Flow-6572",
              "text": "Fair point! My bad for jumping straight into the weeds. **RAG (Retrieval-Augmented Generation)** is basically giving an LLM a 'search engine' or a 'private library' to look at before it answers your question.\n\nInstead of relying only on what it learned during training, the model searches your documents, finds the relevant parts, and uses them as context. The problem I'm describing in the post is that if that 'search' returns a bunch of garbage/noise along with the answer, small local models get confused. That's where the cleaning (and EntropyGuard) comes in.",
              "score": 1,
              "created_utc": "2025-12-27 19:04:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0t02i",
      "title": "Basic PC to run LLM locally...",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "author": "Mr_FuS",
      "created_utc": "2026-01-01 01:55:20",
      "score": 11,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hello, a couple of months ago I started to get interested on LLM running locally after using ChatGPT for tutoring my niece on some high school math homework.\n\n\n\nEnded getting a second hand Nvidia Jetson Xavier and after setting it up and running I have been able to install Ollama and get some models running locally, I'm really impressed on what can be done on such small package and will like to learn more and understand how LLM can merge with other applications to make machine interaction more human.\n\nWhile looking around town on the second hand stores i stumble on a relatively nice looking DELL PRECISION 3650, it is running a i7-10700, and 32GB RAM... could be possible to run dual RTX 3090 on this system upgrading the power supply to something in the 1000 watt range (I'm neither afraid or opposed to take the hardware out of the original case and set it on a test bench style configuration if needed!)?  \n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx0p0g4",
          "author": "LittleBlueLaboratory",
          "text": "So looking at the specs and pictures of a Dell 3650 it does look like they use standard ATX power supplies so you could upgrade that. But the motherboard only has 1 PCI-E x16 slot and not enough room to physically fit a second 3090 anyway.",
          "score": 6,
          "created_utc": "2026-01-01 02:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx179e0",
              "author": "Proof_Scene_9281",
              "text": "Just saved the grief¬†",
              "score": 3,
              "created_utc": "2026-01-01 04:30:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0rsc3",
          "author": "FullstackSensei",
          "text": "I'd look for a generic desktop instead; something built around a regular ATX board. If you intend to put two 3090s, you'll need something that allows splitting the CPU lanes across two slots with at least X8 each.\n\nIf you want to stick to pre-builts from major brands, then look for workstation ass machines. If you can find something that takes DDR4 RAM and has some memory installed, you'll be most of the way there. DDR4 workstation platforms will have at least 4 memory channels, so you get a lot more memory bandwidth than that 10700, which is very nice for CPU offload.",
          "score": 4,
          "created_utc": "2026-01-01 02:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2f36z",
          "author": "Mugen0815",
          "text": "Ive never heard of a Dell supporting dual-gpu. Do they even support std-psus?",
          "score": 3,
          "created_utc": "2026-01-01 11:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0mgt1",
          "author": "Caprichoso1",
          "text": "Have you looked at a Mac?  It might allow you to run larger models.  An NVDIA CPU will be bette at some things, the Mac at others.",
          "score": 3,
          "created_utc": "2026-01-01 02:08:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0o3ym",
              "author": "LittleBlueLaboratory",
              "text": "They are looking at a used 10th gen Dell. That kind of budget isn't going to get them a Mac.",
              "score": 3,
              "created_utc": "2026-01-01 02:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2s4k0",
                  "author": "Makers7886",
                  "text": "he was talking about mac and cheese",
                  "score": 2,
                  "created_utc": "2026-01-01 13:33:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx56te1",
                  "author": "Caprichoso1",
                  "text": "Macs start at  $599 for the mini.  The best value comes from Apple Refurbished store which are as good as new.  Stock is constantly changing so it make take a while to find the exact model/configuration you want.",
                  "score": 1,
                  "created_utc": "2026-01-01 21:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2vk5c",
          "author": "kinkvoid",
          "text": "Not worth it. I would buy a second hand Mac studio.",
          "score": 1,
          "created_utc": "2026-01-01 13:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0o1le",
          "author": "jsconiers",
          "text": "The easiest and most cost effective solution would be to get an m1 or m2 Mac.  After that you could find an old workstation PC like an HP z6 or z4 for cheap that you can add 3090s to.  I started off with a used acer n50 with a GTX 1650.  Then upgraded that PC until it made sense to build something.  (It was very limited as it only had one PCIe slot and max 32Gb of memory)  Finally built a system before the ram price jump.  Glad I built it but it‚Äôs idle more than I thought.  Speed and loading the model will be the biggest concern.",
          "score": 0,
          "created_utc": "2026-01-01 02:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0rtu2",
          "author": "StardockEngineer",
          "text": "If you're hoping to replace ChatGPT, I have bad news.\n\nIf you're doing it just because it's interesting, no problem there.  Just set your expectations accordingly.  As far as that Dell, no idea.  I don't know what it looks like inside.  If there is space and PCI ports, it probably can run two GPUs.  Whether it'll support regular PSUs, no idea.  Dells I've worked with the past had their own special sized power supplies.",
          "score": 0,
          "created_utc": "2026-01-01 02:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx11bst",
              "author": "fasti-au",
              "text": "Actually you can do almost everything but slower and small user count.   The gpt are not 1 model it‚Äôs a lie in many ways but also true in others. \n\nNo you can‚Äôt get chat got now on local but you can get 4o ish if not better in some and worse in others.  \n\nCintext is the issue for multi user not for single user. And parameters and training are distilling to open models in weeks or months. Not what you think and there‚Äôs shortcuts batter you understand where it‚Äôs breaking.\n\nI would speculate that home llm on 96gb vram can compete in smal use with agentic flows. In a useable speed. \n\nIs it cheaper.   Depends on cost of your time",
              "score": 2,
              "created_utc": "2026-01-01 03:48:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1a4t1",
                  "author": "StardockEngineer",
                  "text": "Well, you can't. Coding isn't there yet, and creative writing might require a mix of models. Language classification tasks are best with Gemma 3. Image OCR type stuff is best in Llama 4 Maverick (Qwen3 models are pretty good for image descriptions).\n\nModel mixing is pretty standard to get good results. I run a stack of LiteLLM -> [llama.cpp, private cloud, etc] to wrap it all together.\n\nHome models can't do agents at Claude's level, but simpler agents work fine. gpt-oss-120b is solid for easier agentic use cases. Planning to try Minimax 2.1 next.\n\nBottom line - you'll need to do a lot of mix and matching, and lots of leg work.  Or you can just pay the sub.  If someone has the tinkerer's spirit, I say go for it.  I think it's a lot of fun, whether it's superior or not.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx10iwl",
          "author": "fasti-au",
          "text": "2 x 3090s gets you local coding in devstral and qwen3.  4 gives you 130b models and stronger. \n\nI‚Äôd buy if cheap but you can get 3x 5060s also.   Lanes on board and space is your issue so tisersbcooling and 4x16 boards.  \n\nDo it but I had 6 3090s already rendering \n\n\nI‚Äôd pay for api.   Get open router.  Use frees for everything you can and lean on lmarena and google freebies for one shot big requests and keep all little Q/a prep in local.    Ask the questions well and it need big models for non planning",
          "score": 0,
          "created_utc": "2026-01-01 03:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0omxd",
          "author": "TheAussieWatchGuy",
          "text": "Local LLMs are far inferior to Cloud proprietary models.\n\n\nReally depends on your budget. I would not recommend anyone go 3090 anymore, way too old.\n\n\nMac or Ryzen AI CPU with lots of RAM (which is sadly super expensive now because of AI).¬†",
          "score": -6,
          "created_utc": "2026-01-01 02:22:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1781j",
              "author": "Proof_Scene_9281",
              "text": "4 of them really shines if you maintain consumer expectations¬†",
              "score": 1,
              "created_utc": "2026-01-01 04:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxsd5j",
      "title": "Nvidia Quadro RTX 8000 Passive 48 GB, 1999‚Ç¨ - yes or no ?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxsd5j/nvidia_quadro_rtx_8000_passive_48_gb_1999_yes_or/",
      "author": "HumanDrone8721",
      "created_utc": "2025-12-28 14:16:36",
      "score": 10,
      "num_comments": 20,
      "upvote_ratio": 1.0,
      "text": "Hello, I was looking at these guys: https://www.ebay.de/itm/116912918050 and considering getting one or two. My question for the people who have experience with them: are they worth buying for a local setup, they are passively cooled, does one need some special air ducts for them in an open frame case, could they even be used in a normal case (two pieces) ? \n\nPlease help a poor with no experience with professional GPUs.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxsd5j/nvidia_quadro_rtx_8000_passive_48_gb_1999_yes_or/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdb5xz",
          "author": "Mr_Moonsilver",
          "text": "Bad idea, you get a rtx pro 5000, latest gen with 48Gb for about ‚Ç¨3.5k\n\nOthers here have stated correctly the architecture and quantization support is the biggest risk",
          "score": 9,
          "created_utc": "2025-12-28 14:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd8qvz",
          "author": "ResidentPositive4122",
          "text": "Not worth it, IMO. Considering that it's a 7yo arch at this point, you're probably gonna hit a lot of compatibility issues soon (if not already). Ampere is the oldest arch I'd go for today. As for the price, I would say not even A6000s are worth 2k second-hand, as they're starting to show their age (no fp8 native, etc).",
          "score": 11,
          "created_utc": "2025-12-28 14:30:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd96ii",
              "author": "HumanDrone8721",
              "text": "Well, even the big GPTs say the same, the question is: what is around this price (eventually +50-60%) for 48GB that is available not in a back alley in China, but in EU ? Any such unicorn ?",
              "score": 1,
              "created_utc": "2025-12-28 14:33:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdlzrm",
                  "author": "eleqtriq",
                  "text": "Two 3090s",
                  "score": 9,
                  "created_utc": "2025-12-28 15:44:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwda7x8",
          "author": "Lxzan",
          "text": "I have no experience with Rtx 8000 but can answer few questions. Cooling is totally manageable but you will either need some 3d printed bracket for it, (look for ‚Äòamd mi50 fan bracket‚Äô for ideas) or some server case with installed coolers that could create some air pressure. As for is it worth buying - for me it seems too much for a price, you will get better performance from 4x 3090 or 2x mi50 32gb (but amd is a lot more tricky to set up and some libraries do not work out of the box)",
          "score": 2,
          "created_utc": "2025-12-28 14:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdd3i5",
          "author": "FullstackSensei",
          "text": "Cooling is easy if you have the most rudimentary 3D design skills. If not, as has been pointed out, just search for a Mi50 duct. I designed a duct for my Mi50s to cool each pair with an 80mm fan, and they run very cool without sounding like a jet engine.\n\nNow, at 2k a pop, I don't think the value preposition is there. If you're in Germany you can already find 3090s for ~‚Ç¨600 on kleinanzeigen, less if you're willing to buy watercooled ones. So, you can get four cards for the same 96GB VRAM for 25% more than the price of one card. While 4x24 does not equal 2x48 in practice, the 3090 has so much more compute, and considerably more memory bandwidth that it's not even a competition with Turing.\n\nBTW, I've seen the active Quadro RTX 8000 on kleinanzeigen sell for a little over 1k. They don't pop up often, but if you really need 96GB VRAM in two cards, and have a bit of patience, you could get a pair for under ‚Ç¨2500, though I still think 4x3090s are better.",
          "score": 2,
          "created_utc": "2025-12-28 14:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdf1zu",
              "author": "HumanDrone8721",
              "text": "> Quadro RTX 8000 a little over 1K\n\nWell you definitely have better eyes than me, I see exactly two listings all over Germany, this:\n\nhttps://www.kleinanzeigen.de/s-anzeige/nvidia-quadro-rtx-8000-48gb-gddr6-profi-gpu-fuer-ki-3d-rend/3243167517-225-7983 and this:\n\nhttps://www.kleinanzeigen.de/s-anzeige/nvidia-rtx-8000-48gb-aktiv-/3265840025-225-19640 \n\n\nOr you were meaning that they may show sometime again, I'm afraid this train left the station, but I can totally put an alarm in case some wonder happens and is not one of those relentless scammers.",
              "score": 1,
              "created_utc": "2025-12-28 15:07:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdjmaw",
                  "author": "FullstackSensei",
                  "text": "You know that items with a good price sell quickly, and what you see usually when you do a sporadic search is overpriced items that don't sell. I have a saved search for Nvidia 48gb for about a year and I see one pop up every month to six weeks and sell within a day or two. Sure there might be some scams, but I've seen plenty offering local pick up.\n\nI have a deal to go pick up a RTX 2000 Ada in a few days for less than half the price you see if you search for that card now. It's already reserved for me, so you won't see it if you search now.",
                  "score": 2,
                  "created_utc": "2025-12-28 15:32:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdg56l",
          "author": "twjnorth",
          "text": "I am still building a rig but just getting started with ai (threadripper 3975wx, wrx80e mob so 7x pcie4 x16 slots).\n\nI bought 4x rtx 3090 and got two quadro 6000 (24gb) with nvlink, all second hand from eBay so will be setting that up in the WRX80E with proxmox and GPU pass thru to see how each performs.\n\nThe quadro 8000 has same specs as the quadro 6000 but just double the VRAM.\n\nGot the first quadro 6000 24GB as it was the best card I could run in my HP Z2 G5 according to manual. Then started looking at what I needed for a better setup.",
          "score": 1,
          "created_utc": "2025-12-28 15:13:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdts2i",
              "author": "Otherwise-Way1316",
              "text": "Interested to hear what performance is for this set up and what models run comfortably in it.",
              "score": 1,
              "created_utc": "2025-12-28 16:24:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwelmwy",
          "author": "KySiBongDem",
          "text": "Unless you want to do 3D modeling, it is a bad idea. For AI, get the a newest Blackwell GPU.",
          "score": 1,
          "created_utc": "2025-12-28 18:39:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwemi35",
              "author": "HumanDrone8721",
              "text": "I'll absolutely do so if I'm getting the End of Year lottery :).",
              "score": 1,
              "created_utc": "2025-12-28 18:43:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nweo61v",
                  "author": "KySiBongDem",
                  "text": "There are multiple version - you may go with 4500 or 4000 instead of 5000. I know they have less memory but with the 48GB but the weak/outdated GPU will worth that ‚Ç¨2,000.",
                  "score": 1,
                  "created_utc": "2025-12-28 18:51:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfjjwo",
          "author": "CodeSlave9000",
          "text": "Short opinion: Too expensive for a 7yr/old architecture.  I have one of the blower versions, and for inference the performance isn't bad - Compute is certainly lower than Ampere (30x/Axxxx cards) but memory bandwidth is still good.  This mostly shows up as slower prompt processing, but actual generation is about 2x the RTX 4060 Ti.",
          "score": 1,
          "created_utc": "2025-12-28 21:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwyb6bv",
          "author": "Zyj",
          "text": "Forget about cards with \"Quadro\" in the name, they are ancient. The only exception being the Quadro A6000 (Ampere).",
          "score": 1,
          "created_utc": "2025-12-31 18:13:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwemus7",
          "author": "FireWoIf",
          "text": "Turing has poor support unfortunately. I‚Äôve tried and found much difficulty using this generation. Ampere should be your minimum goto. If you‚Äôre dead set on these, a blower fan should be good for these. Don‚Äôt use normal fans with a duct as these need (very) high air pressure to cool.",
          "score": 1,
          "created_utc": "2025-12-28 18:45:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweo99q",
              "author": "HumanDrone8721",
              "text": "I'm not dead set on anything, I was (naively) thinking maybe there are some semi-hidden gems, but it seems that one has to play to stay on the big boys table.",
              "score": 1,
              "created_utc": "2025-12-28 18:51:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0cwkh",
      "title": "Any local llm code assistant?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "author": "SAF1N",
      "created_utc": "2025-12-31 13:22:49",
      "score": 10,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I'm looking for a code assistant type of thing, it should run locally and I can ask it questions about my codebase and it will give me short/concise answers. Is there anything like that?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwzk77u",
          "author": "Dangerous-Dingo-5169",
          "text": "You can use https://github.com/Fast-Editor/Lynkr to connect Claude code cli to your local llms without losing any features offered by anthropic backend like sub agents etc",
          "score": 2,
          "created_utc": "2025-12-31 22:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1yz0d",
          "author": "DenizOkcu",
          "text": "Try Nanocoder. Privacy und local-first open source CLI. Disclaimer: I am a contributor (working on Nanocoder in Nanocoder ü§£)\n\nhttps://github.com/Nano-Collective/nanocoder",
          "score": 2,
          "created_utc": "2026-01-01 08:48:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx6v2v",
          "author": "nofilmincamera",
          "text": "Questions or any refactoring?",
          "score": 1,
          "created_utc": "2025-12-31 14:51:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxlpmv",
              "author": "SAF1N",
              "text": "being able to refactor or generate new code would be a bonus feature and rarely used (by me). my main use case would be just asking questions.",
              "score": 1,
              "created_utc": "2025-12-31 16:07:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx396lt",
          "author": "imagent42",
          "text": "opencodeCLI  [https://opencode.ai/](https://opencode.ai/)",
          "score": 1,
          "created_utc": "2026-01-01 15:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxazo8r",
          "author": "Lissanro",
          "text": "Roo Code has Ask mode, it is great for asking questions about the code base. In my experience works best with K2 Thinking (I run Q4\\_X quant with ik\\_llama.cpp), but it may work with other models too in case you prefer something else.",
          "score": 1,
          "created_utc": "2026-01-02 19:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxyak9",
          "author": "PermanentLiminality",
          "text": "[Continue.dev](http://Continue.dev) is easy to point at your local LLM.  It is a VSCode plug in and I believe it now has a CLI.",
          "score": 1,
          "created_utc": "2025-12-31 17:09:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxdsn1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-31 15:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxkzc2",
              "author": "SAF1N",
              "text": "This is a GUI, you can do the same in vscode with only extensions.\n\nI'm looking for a terminal only application.",
              "score": 1,
              "created_utc": "2025-12-31 16:03:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwxs2f6",
                  "author": "InsideResolve4517",
                  "text": "I've tried 1 terminal only application like qwen, gemini.\n\nIn  local Continue (cli) mode.\n\nAnd one another tool I tried (forgot the name, and not able to find that tool) that didn't worked for me (I think it will work beyond 14b like 20b models\n\nI'm not able to remember the name",
                  "score": 1,
                  "created_utc": "2025-12-31 16:38:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx27arq",
                  "author": "Consistent_Wash_276",
                  "text": "If you host gpt-oss models you can use them with codex. \n\nBut opencode is my preferred terminal option.",
                  "score": 1,
                  "created_utc": "2026-01-01 10:17:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0iyru",
      "title": "Would you change anything about this setup? 7800x3D, 128gb RAM, 3080",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0iyru/would_you_change_anything_about_this_setup/",
      "author": "2dollasoda",
      "created_utc": "2025-12-31 17:49:06",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 0.91,
      "text": "Hello,\n\nI have a PC with a 7800x3d,  128gb of DDR5 RAM, and a 3080. I'm looking at running my own model. I think my GPU is the bottleneck here. Would it be worth selling and upgrading to a 3090?\n\nThanks.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0iyru/would_you_change_anything_about_this_setup/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwy7ryi",
          "author": "Terrible-Contract298",
          "text": "Yes, get a 3090, the 10gb VRAM on the 3080 is simply insufficient for larger models.",
          "score": 5,
          "created_utc": "2025-12-31 17:56:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy7zsg",
              "author": "supportkeinmord",
              "text": "Feels like a good trade for a few hundret bucks.",
              "score": 1,
              "created_utc": "2025-12-31 17:57:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy9c2l",
                  "author": "2dollasoda",
                  "text": "That's what I was thinking. Plus I game so it would be an upgrade there too",
                  "score": 2,
                  "created_utc": "2025-12-31 18:04:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwy7x9u",
          "author": "SimplyRemainUnseen",
          "text": "Do you plan on offloading to system memory? If so, do you have 2x 64GB sticks or x4 32GB sticks?\n\nYou'll have better performance with 2x 64GB sticks due to the CPU.\n\nRegarding the GPU, a 3090 will have more vram and compute. Up to you if that's worth it. I'd personally go for a 5090 as that 32GB opens a lot of doors.",
          "score": 3,
          "created_utc": "2025-12-31 17:57:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy983f",
              "author": "2dollasoda",
              "text": "Unfortunately I have 4x32gb sticks. I plan on offloading to system memory, especially if I'm on the 3080.\n\nI would love a 5090, but budget wise I don't know how feasible that is for me.",
              "score": 1,
              "created_utc": "2025-12-31 18:03:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwyl82e",
                  "author": "Terrible-Contract298",
                  "text": "If you sold the other kit for better performance, it would get you some of the way towards the 5090 if DDR5.",
                  "score": 1,
                  "created_utc": "2025-12-31 19:04:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx02d06",
              "author": "Karyo_Ten",
              "text": ">I'd personally go for a 5090 as that 32GB opens a lot of doors.\n\nThey aren't many models in the 40~50GB range that would be in the right when quantized to Q4, between 32~36B and 70B it's the desert.\n\nOn a 5090 the most useful models are Gemma3-27B with about 118k context size (GPTQ iirc), Mistral-3.2-24B, GLM-4-32B, Seed-OSS-36B, Qwen3-30B-A3B, ...\n\nBut they all fit in 24B (though with much snaller context)",
              "score": 1,
              "created_utc": "2026-01-01 00:01:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwygpdy",
          "author": "Qs9bxNKZ",
          "text": "Memory at a premium, you probably don‚Äôt need that much.\n\nI convert 70B models and that can take 128GB (about 140GB actually but a fast NVME (Gen 5) eliminates most of the performance issues.  And that‚Äôs to make it fit into 57-87GB of VRAM (RTX 6000 with 96GB or M4 with 64GB)\n\nYou‚Äôre not even there with the 3080 so the system RAM is an overkill unless you want to intentionally handicap yourself (eg running it in WSL under windows). \n\nProbably a good 32GB is enough (we run PS and blender in under 48G) with the extra funding allocated to the GPU like 5090.",
          "score": 2,
          "created_utc": "2025-12-31 18:41:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwymdgd",
              "author": "2dollasoda",
              "text": "I wouldn't buy that much memory today. I already have it (bought 2 years ago or so). So I was mostly looking for other ways to use it.",
              "score": 3,
              "created_utc": "2025-12-31 19:09:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyzhjp",
          "author": "kovnev",
          "text": "What's your 128gb DDR5 running at? Tested for stability?\n\nAnd, yes, GPU is a big weak point.",
          "score": 2,
          "created_utc": "2025-12-31 20:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzemgk",
          "author": "Mugen0815",
          "text": "This is a great gaming-pc with unused RAM worth a ai-server. Those 96GB extra-RAM are worth as much as the ai-server I just bought used (5950x, 32GB, 3090) for 1,4k.",
          "score": 1,
          "created_utc": "2025-12-31 21:41:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzvv23",
          "author": "dreyybaba",
          "text": "Don‚Äôt sell, get a 3090 and a Minisforum DEG2 and connect it to your PC.",
          "score": 1,
          "created_utc": "2025-12-31 23:21:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx02ylm",
          "author": "Karyo_Ten",
          "text": "Try running GPT-OSS-120B, it's native FP4 so about ~65GB, and only 5B active parameter so if your RAM is running at 70GB/s (lowballing because 4 sticks run slower unless you pass days tuning timings at r/overclocking) you can reach 70/5 = 14 tok/s (this is a maximum, depends on implementation, and GPU can help a lot!)",
          "score": 1,
          "created_utc": "2026-01-01 00:04:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pygyhi",
      "title": "Local model registry to solve duplicate GGUFs across apps?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pygyhi/local_model_registry_to_solve_duplicate_ggufs/",
      "author": "tleyden",
      "created_utc": "2025-12-29 08:27:50",
      "score": 8,
      "num_comments": 25,
      "upvote_ratio": 1.0,
      "text": "I'm running into storage issues with multiple local LLM apps. I downloaded Olmo3-7B through Ollama, then wanted to try Jan.ai's UI and had to download the same 4GB model again. Now multiply this across Dayflow, Monologue, Whispering, and whatever other local AI tools I'm testing.\n\nEach app manages its own model directory. No sharing between them. So you end up with duplicate GGUFs eating disk space.\n\nFeels like this should be solvable with a shared model registry - something like how package managers work. Download the model once, apps reference it from a common location. Would need buy-in from Ollama, LMStudio, Jan, LibreChat, etc. to adopt a standard, but seems doable if framed as an open spec.\n\nI'm guessing the OS vendors will eventually bake something like this in, but that's years away. Could a community-driven library work in the meantime? Or does something like this already exist and I'm just not aware of it?\n\nCurious if anyone else is hitting this problem or if there's already work happening on standardizing local model storage.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pygyhi/local_model_registry_to_solve_duplicate_ggufs/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjo88q",
          "author": "tleyden",
          "text": "Someone posted a comment and then removed it, but their suggestion actually works pretty nicely. The gist was to use llama.cpp as a central model server - download GGUFs once, run llama.cpp with proper flags, then point all your frontend apps (Jan, etc.) to that endpoint. Solves duplication and supposedly runs faster too.\n\nIt's a bit more tedious than I'd like, but it works. I was able to download an HF model via curl and import it to Jan.ai. Going to test it with other apps that support local OpenAI-compatible endpoints or direct model import.",
          "score": 3,
          "created_utc": "2025-12-29 14:04:50",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwikd4b",
          "author": "ttkciar",
          "text": "They're just files.  You can remove duplicates yourself and replace them with symlinks to whichever copy you choose to make the \"primary\".",
          "score": 2,
          "created_utc": "2025-12-29 08:44:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwim9t0",
              "author": "tleyden",
              "text": "Call me lazy, but that sounds like way too much work.  Why can't the apps just do that consolidation for me?",
              "score": -2,
              "created_utc": "2025-12-29 09:02:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwinbw7",
                  "author": "TomatoInternational4",
                  "text": "Because they're made by different people and aren't sentient beings. Programs only know and do what we tell them to. I hope that this was a sarcastic question.",
                  "score": 4,
                  "created_utc": "2025-12-29 09:12:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuf1p4",
                  "author": "StardockEngineer",
                  "text": "Lazy AF.  üòÇ",
                  "score": 1,
                  "created_utc": "2025-12-31 02:19:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjft1v",
          "author": "johannes_bertens",
          "text": "I've built a hf-downloader for this.  \nDoes \\*not\\* give you a registry, but \\*does\\* give you an easy way to download the files yourself to then use across multiple applications.\n\n[https://lib.rs/crates/rust-hf-downloader](https://lib.rs/crates/rust-hf-downloader)  \n[https://github.com/johannesbertens/rust-hf-downloader](https://github.com/johannesbertens/rust-hf-downloader)",
          "score": 2,
          "created_utc": "2025-12-29 13:13:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjgivh",
              "author": "tleyden",
              "text": "That looks really slick!  I will give it a spin report back on the github repo if I run into issues",
              "score": 1,
              "created_utc": "2025-12-29 13:17:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp2zcv",
          "author": "t3rmina1",
          "text": "I have all my GGUF files, hf cache, llama.cpp cache etc. on relatively fast SSDs with the same directory structures and download to the specific SSD I need using uvx hf. \n\nOn Proxmox host these are all mounted and combined into a single unified path using mergefs, and bind-mounted to my llama.cpp LXC, or any other LXCs as needed.\n\nInside the container I run llama-server's router mode and select the models I want to load from the UI: those with predefined configs will be available to load with those setups, otherwise they'll be auto detected and available to load with the default configs. \n\nAny other services then use llama-server's endpoint.\n\nI basically do the same for my hf cache.",
          "score": 2,
          "created_utc": "2025-12-30 07:56:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwimrce",
          "author": "pmttyji",
          "text": "Jan has Import option(to use downloaded GGUF files from any folder). \n\nKoboldcpp also does this just with browse GGUF option. \n\nFor Oobabooga, I used symlinks option.",
          "score": 1,
          "created_utc": "2025-12-29 09:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiosoz",
              "author": "tleyden",
              "text": "Thanks, good to know!  I found that buried in the [Jan.ai](http://Jan.ai) UI under Settings / Model Providers / Llama.cpp / import.  \n  \nThe whole process to import a model feels super clunky though.  Or maybe I'm doing it wrong?\n\n1. Figure out where ollama stores its models.  \n2. Enable hidden dirs in osx file picker  \n3. Navigate to .ollama/models/blobs  \n4. See a bunch of hashes and file sizes and try to pick the one I want\n\nhttps://preview.redd.it/rtw8gsyw24ag1.png?width=1444&format=png&auto=webp&s=ccb4e8080fc6ad6dbb490cd3aac9ac2ee736690a\n\n   \nTo me it doesn't seem \"user friendly\" at all.  I can't imagine normie users being able to deal with mapping SHA hashes and file sizes to the model they want.",
              "score": 1,
              "created_utc": "2025-12-29 09:26:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwiq34f",
                  "author": "pmttyji",
                  "text": "I use Windows. After downloading GGUF files from HuggingFace, I simply import them using Jan. I don't see any complications here\n\nhttps://preview.redd.it/x0r5j8ac54ag1.png?width=1152&format=png&auto=webp&s=177f39e8dba4bc5f8d10f5e1b8f0c76b4aa7542a\n\nI think the filenames & SHA hashes are related to Ollama. I don't use that one. I'm happy with HF.",
                  "score": 1,
                  "created_utc": "2025-12-29 09:38:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwki8rx",
                  "author": "reginakinhi",
                  "text": "That's because your models are downloaded by ollama, who deliberately split stuff across files and named only by hashes because they don't want anyone else to get use from it. Just download the gguf files themselves as every other LLM runner does.",
                  "score": 1,
                  "created_utc": "2025-12-29 16:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1px0vas",
      "title": "Run GPUs on a Pi without a PC",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1px0vas/run_gpus_on_a_pi_without_a_pc/",
      "author": "dreamingwell",
      "created_utc": "2025-12-27 15:47:07",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.79,
      "text": "[ https://youtu.be/8X2Y62JGDCo?si=MHdk8HH8npelMM\\_X ](https://youtu.be/8X2Y62JGDCo?si=MHdk8HH8npelMM_X)\n\nAn interesting project where a Pi is used to drive multiple GPUs - including running LLMs. And it runs pretty well!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1px0vas/run_gpus_on_a_pi_without_a_pc/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw8d7x9",
          "author": "FullstackSensei",
          "text": "Let me try: Jeff Geerling, Jeff Geerling, Jeff Geerling.\n\nYou can, but as the man himself says: doesn't mean you should.\n\nIt works well with multiple GPUs compared to a desktop if you split across layers, but that leaves so much performance on the table. Running llama.cpp with -sm row gives a big uplift in inference performance, but also requires a lot more bandwidth (in my experience, at least X4 for each GPU with weaker cards, or X8 for more powerful cards). The PCIe adapters with a PCIe switch also aren't cheap, negating any savings from running using a Pi.\n\nStill, it's √ºber cool, and definitely something worth looking into if you want something that runs 24/7 and your model (or models) all fit within a single GPU.",
          "score": 4,
          "created_utc": "2025-12-27 18:42:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw9qg6",
      "title": "Genesis-152M-Instruct ‚Äî exploring hybrid attention + TTT at small scale",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pw9qg6/genesis152minstruct_exploring_hybrid_attention/",
      "author": "Kassanar",
      "created_utc": "2025-12-26 17:26:56",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "Hey everyone üëã\n\nI‚Äôm sharing **Genesis-152M-Instruct**, an **experimental small language model** built to explore how *recent architectural ideas interact* when combined in a single model ‚Äî especially under **tight data constraints**.\n\n\n\nThis is **research-oriented**, not a production model or SOTA claim.\n\n\n\n\n\nüîç **Why this might be interesting**\n\n\n\nMost recent architectures (GLA, FoX, TTT, ¬µP, sparsity) are tested **in isolation** and usually at **large scale**.\n\nI wanted to answer a simpler question:\n\n\n\n*How much can architecture compensate for data at \\~150M parameters?*\n\n\n\nGenesis combines several **ICLR 2024‚Äì2025 ideas** into one model and evaluates the result.\n\n\n\n\n\n‚ö° **TL;DR**\n\n‚Ä¢ **152M parameters**\n\n‚Ä¢ Trained on **\\~2B tokens** (vs \\~2T for SmolLM2)\n\n‚Ä¢ Hybrid **GLA + FoX attention**\n\n‚Ä¢ **Test-Time Training (TTT)** during inference\n\n‚Ä¢ **Selective Activation (sparse FFN)**\n\n‚Ä¢ **¬µP-scaled training**\n\n‚Ä¢ Fully open-source (Apache 2.0)\n\n\n\nü§ó Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\nüì¶ pip install genesis-llm\n\n\n\n\n\nüìä **Benchmarks (LightEval, Apple MPS)**\n\n\n\nARC-Easy ¬† ¬† ‚Üí 44.0% ¬† (random: 25%)\n\nBoolQ¬† ¬† ¬† ¬† ‚Üí 56.3% ¬† (random: 50%)\n\nHellaSwag¬† ¬† ‚Üí 30.2% ¬† (random: 25%)\n\nSciQ ¬† ¬† ¬† ¬† ‚Üí 46.8% ¬† (random: 25%)\n\nWinogrande ¬† ‚Üí 49.1% ¬† (random: 50%)\n\n\n\n**Important context:**\n\nSmolLM2-135M was trained on **\\~2 trillion tokens**.\n\nGenesis uses **\\~2 billion tokens** ‚Äî so this is not a fair head-to-head, but an exploration of **architecture vs data scaling**.\n\n\n\n\n\nüß† **Architecture Overview**\n\n\n\n**Hybrid Attention (Qwen3-Next inspired)**\n\n\n\n**Layer** **%** **Complexity** **Role**\n\nGated DeltaNet (GLA) 75% O(n) Long-range efficiency\n\nFoX (Forgetting Attention) 25% O(n¬≤) Precise retrieval\n\n\n\nGLA uses:\n\n‚Ä¢ Delta rule memory updates\n\n‚Ä¢ Mamba-style gating\n\n‚Ä¢ L2-normalized Q/K\n\n‚Ä¢ Short convolutions\n\n\n\nFoX adds:\n\n‚Ä¢ Softmax attention\n\n‚Ä¢ Data-dependent forget gate\n\n‚Ä¢ Output gating\n\n\n\n\n\n**Test-Time Training (TTT)**\n\n\n\nInstead of frozen inference, Genesis can **adapt online**:\n\n‚Ä¢ Dual-form TTT (parallel gradients)\n\n‚Ä¢ Low-rank updates (rank=4)\n\n‚Ä¢ Learnable inner learning rate\n\n\n\nPaper: *Learning to (Learn at Test Time)* (MIT, ICML 2024)\n\n\n\n\n\n**Selective Activation (Sparse FFN)**\n\n\n\nSwiGLU FFNs with **top-k activation masking** (85% kept).\n\nCurrently acts as **regularization** ‚Äî real speedups need sparse kernels.\n\n\n\n\n\n**¬µP Scaling + Zero-Centered RMSNorm**\n\n‚Ä¢ Hyperparameters tuned on small proxy\n\n‚Ä¢ Transferred via ¬µP rules\n\n‚Ä¢ Zero-centered RMSNorm for stable scaling\n\n\n\n\n\n‚ö†Ô∏è **Limitations (honest)**\n\n‚Ä¢ Small training corpus (2B tokens)\n\n‚Ä¢ TTT adds \\~5‚Äì10% inference overhead\n\n‚Ä¢ No RLHF\n\n‚Ä¢ Experimental, not production-ready\n\n\n\n\n\nüìé **Links**\n\n‚Ä¢ ü§ó Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n‚Ä¢ üì¶ PyPI: [https://pypi.org/project/genesis-llm/](https://pypi.org/project/genesis-llm/)\n\n\n\n\n\nI‚Äôd really appreciate feedback ‚Äî especially from folks working on **linear attention**, **hybrid architectures**, or **test-time adaptation**.\n\n\n\n*Built by Orch-Mind Team*",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw9qg6/genesis152minstruct_exploring_hybrid_attention/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pvibn9",
      "title": "GLM 4.7 has now taken #2 on Website Arena",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/w2ak6aqsyd9g1.png",
      "author": "Worldly_Ad_2410",
      "created_utc": "2025-12-25 17:35:29",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pvibn9/glm_47_has_now_taken_2_on_website_arena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw3dxhx",
          "author": "DocStrangeLoop",
          "text": "The moat.\n\nIt closed.",
          "score": 1,
          "created_utc": "2025-12-26 22:13:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q07lhw",
      "title": "We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.",
      "subreddit": "LocalLLM",
      "url": "/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/",
      "author": "vox-deorum",
      "created_utc": "2025-12-31 08:00:27",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q07lhw/we_asked_oss120b_and_glm_46_to_play_1408/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pxjvjy",
      "title": "GPU requirements for running Qwen2.5 72B locally?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxjvjy/gpu_requirements_for_running_qwen25_72b_locally/",
      "author": "lucasbennett_1",
      "created_utc": "2025-12-28 05:59:20",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 0.88,
      "text": "Trying to determine what GPU setup I need to run qwen2.5 72B locally with decent inference speed. From what I understand the model needs around 140GB+ vram for full precision or maybe 70-8-GB for quantisized versions. Does this mean I'm looking at multiple A100s or H100s? Or can this run on consumer GPUs like 4090s with some heavy quantization?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxjvjy/gpu_requirements_for_running_qwen25_72b_locally/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwbmolo",
          "author": "alexp702",
          "text": "A Mac/Dgx/stryx halo with 128Gb should run this at 8 bit. Alternatively an Rtx 6000 Pro based workstation. Factor in you‚Äôll probably need 72Gb for model and 36Gn for the full context. Double that for BF16. That‚Äôs my rough calculations.\n\nPersonally would run something newer.",
          "score": 5,
          "created_utc": "2025-12-28 06:12:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwce6ns",
              "author": "lucasbennett_1",
              "text": "Appreciate the brakdown, mac/DGX route seems expensive though. wold quantizing to 4-bit on dua 3090s or 4090s work or does that kill performance for realistic use?",
              "score": 1,
              "created_utc": "2025-12-28 10:31:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwcfxvd",
                  "author": "alexp702",
                  "text": "For that model you can definitely do 2 3090/4090s - and it will run comparatively very quickly. However you will have a small context space of somewhere \\~32K or less. If you're coding against the model this is too tight for Cline/Roocode/etc to function decently. Also quantising small models for code causes big accuracy losses. If you're doing RAG you can sometimes get away with these errors, but they annoy me enough to do not want to use them.\n\nOn a budget check out Stryx Halo based boxes - they are (were before Ram costs!) <$3K for a 128Gb box. I have not tried them, but they are cheaper than DGX Sparks. Macs start around \\~3.5K for 128Gb Mac Studio M4Max. Both AMD and Apple suffer from slow prompt parsing. Depending on workload this can make them tedious. But the low memory of Nvidia cards on a budget means no real choice.\n\nI have put two 4090s in one AMD 5800x3d box, and it performs very well - 3-8x prompt processing over M3 Ultra running the same Qwen3 Coder 30b-IQ4 which Cline really does lots of ... until you run out of memory. I got about 70K context reliably.  So I am back to the Mac Studio M3 Ultra running Qwen 480b. Quality over speed won for me.",
                  "score": 5,
                  "created_utc": "2025-12-28 10:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbzg1r",
          "author": "RiskyBizz216",
          "text": "Q8 only needs 77GB\n\nI prefer the **IQ**  quants because they give you more speed, and are smaller.\n\n[https://huggingface.co/bartowski/Qwen2.5-72B-Instruct-GGUF](https://huggingface.co/bartowski/Qwen2.5-72B-Instruct-GGUF)\n\n  \nThis IQ3\\_XXS is only 31 GB so it could fit on a single 5090 with some offloading.\n\nIf you go any lower than IQ3 then you would be better off using the Qwen3 32B VL Instruct \n\n[https://huggingface.co/bartowski/Qwen\\_Qwen3-VL-32B-Instruct-GGUF](https://huggingface.co/bartowski/Qwen_Qwen3-VL-32B-Instruct-GGUF)",
          "score": 3,
          "created_utc": "2025-12-28 08:08:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwceas9",
              "author": "lucasbennett_1",
              "text": "thanks for the feedback, looking into it",
              "score": 2,
              "created_utc": "2025-12-28 10:32:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwc6c2q",
          "author": "umtausch",
          "text": "Why are you running 2.5? 3 is much much better.",
          "score": 3,
          "created_utc": "2025-12-28 09:14:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc7uky",
              "author": "Karyo_Ten",
              "text": "Asking the real question.\n\n1 year in LLM is forever.",
              "score": 2,
              "created_utc": "2025-12-28 09:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwox9c5",
                  "author": "Ok-Bill3318",
                  "text": "Lately 1 month is forever.",
                  "score": 1,
                  "created_utc": "2025-12-30 07:04:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwce0ia",
              "author": "lucasbennett_1",
              "text": "Will check it out. still curious about the 72B setup though since hardware requirements should be similar",
              "score": 0,
              "created_utc": "2025-12-28 10:29:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi3y6k",
          "author": "beedunc",
          "text": "First - Delete all your 2.5‚Äôs and just go 3. \n\nNight and day better. So much better, you might even be able to use smaller models with the 3.",
          "score": 3,
          "created_utc": "2025-12-29 06:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbnzn9",
          "author": "Such_Advantage_6949",
          "text": "Most people running with q4 which will fit on 2x3090 though u probably is a bit limited on context. 3x3090 should let u running it fine at higher quant like q5+",
          "score": 2,
          "created_utc": "2025-12-28 06:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwce9bv",
              "author": "lucasbennett_1",
              "text": "how about 2x4090s?",
              "score": 1,
              "created_utc": "2025-12-28 10:31:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwch1r9",
                  "author": "Such_Advantage_6949",
                  "text": "Generation speed is about the same for LLM. The 4090 will be faster in prolotprocessing ( time to generate the first token)",
                  "score": 2,
                  "created_utc": "2025-12-28 10:58:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwowyc5",
          "author": "Ok-Bill3318",
          "text": "You could run it on a Mac Studio or a strix halo box with a decent amount of ram.",
          "score": 1,
          "created_utc": "2025-12-30 07:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrffuq",
          "author": "Beautiful_Trust_8151",
          "text": "We used to run Llama 3.3 70b with 3 Radeon 7900xtx GPUs on a z790 motherboard.  You can get decent 14 tokens per second and a decent context window.  The newer models support flash attention which will give you an even larger context window.  Qwen3 70b like llama 70b is a dense model, so it doesn't run that well on Macs and Strix boxes.",
          "score": 1,
          "created_utc": "2025-12-30 17:17:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py9kf4",
      "title": "Jetbrains AI users, what's your configuration with local models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9kf4/jetbrains_ai_users_whats_your_configuration_with/",
      "author": "robertpro01",
      "created_utc": "2025-12-29 02:07:26",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "I am trying this configuration, but I would like to know what are you guys using for each category:\n\nhttps://preview.redd.it/7mrrekazw1ag1.png?width=710&format=png&auto=webp&s=5ef4328afa4e62c33d84516756b35412485201f8\n\n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9kf4/jetbrains_ai_users_whats_your_configuration_with/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1py1kq2",
      "title": "[Tool Release] Skill Seekers v2.5.0 - Convert any documentation into structured markdown skills for local/remote LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py1kq2/tool_release_skill_seekers_v250_convert_any/",
      "author": "Critical-Pea-8782",
      "created_utc": "2025-12-28 20:28:51",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.78,
      "text": "Hey üëã\n\n  Released **Skill Seekers v2.5.0** with universal LLM support - convert any documentation into structured markdown skills.\n\n  ## What It Does\n\n  Automatically scrapes documentation websites and converts them into organized, categorized reference files with extracted code examples. Works with any LLM (local or remote).\n\n  ## New in v2.5.0: Universal Format Support\n\n  - ‚úÖ **Generic Markdown export** - works with ANY LLM\n  - ‚úÖ **Claude AI** format (if you use Claude)\n  - ‚úÖ **Google Gemini** format (with grounding)\n  - ‚úÖ **OpenAI ChatGPT** format (with vector search)\n\n  ## Why This Matters for Local LLMs\n\n  Instead of context-dumping entire docs, you get:\n  - **Organized structure**: Categorized by topic (getting-started, API, examples, etc.)\n  - **Extracted patterns**: Code examples pulled from docs with syntax highlighting\n  - **Portable format**: Pure markdown ZIP - use with Ollama, llama.cpp, or any local model\n  - **Reusable**: Build once, use with any LLM\n\n  ## Quick Example\n\n  ```bash\n  # Install\n  pip install skill-seekers\n\n  # Scrape any documentation\n  skill-seekers scrape --config configs/react.json\n\n  # Export as universal markdown\n  skill-seekers package output/react/ --target markdown\n\n  # Result: react-markdown.zip with organized .md files\n```\n\n  The output is just structured markdown files - perfect for feeding to local models or adding to your RAG pipeline.\n\n  Features\n\n  - üìÑ Documentation scraping with smart categorization\n  - üêô GitHub repository analysis\n  - üìï PDF extraction (for PDF-based docs)\n  - üîÄ Multi-source unified (docs + code + PDFs in one skill)\n  - üéØ 24 preset configs (React, Vue, Django, Godot, etc.)\n\n  Links\n\n  - GitHub: https://github.com/yusufkaraaslan/Skill_Seekers\n  - PyPI: https://pypi.org/project/skill-seekers/\n  - Release: https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.5.0\n\n  MIT licensed, contributions welcome! Would love to hear what documentation you'd like to see supported.",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1kq2/tool_release_skill_seekers_v250_convert_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1px25u5",
      "title": "GLM 4.7 Reaches Top 6 on Vending Bench 2 Leaderboard First Open Weight Model in Profit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/je9btc6tyr9g1.jpeg",
      "author": "techspecsmart",
      "created_utc": "2025-12-27 16:40:32",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1px25u5/glm_47_reaches_top_6_on_vending_bench_2/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pvf1k2",
      "title": "Mm What multi-modal ai stack could serve as home assistant. Always persistent",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pvf1k2/mm_what_multimodal_ai_stack_could_serve_as_home/",
      "author": "Thermal_Headroom",
      "created_utc": "2025-12-25 14:58:13",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "Yeah, okay, I get it. Like, buy a Dell server or a bunch of Mac minis and run huge models locally. Great, but which models are good enough and can be trusted managing my IoT alarms, locks, automation, and convenience features like lighting and audio? Like a housekeeper but for real here. software housekeeper. üëÄ Running in the background. Not reaching out for the most part. And it really should be air-gapped for local personal use. What model combinations and tooling are we talking about? And let‚Äôs outline our network structure framework if possible. Yes I mean giving the model access to sensors configuration, api, and other direct or direct access methods. Not sure how may have already tried this recently. \n\nMerry Christmas üéÑ ",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pvf1k2/mm_what_multimodal_ai_stack_could_serve_as_home/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvxd5cn",
          "author": "horriblesmell420",
          "text": "Pretty much any model that can call tools will do well. Speed will also be a factor if you're trying to make you're own real time voice assistant.\n\nI've had great luck with qwen3 30B for this functionality, it's fast and intelligent, it's never missed a tool call for me. Nvidia's parakeet v2 or v3 are great for STT. Lastly for TTS I'd use kokoro unless you need voice cloning.\n\nWith this stack I've achieved around 2-3 seconds round trip response time from STT input > LLM Calling the tools > TTS response and fits in around 24GB of VRAM. Depending on how many devices you have you may need to increase your context size.",
          "score": 1,
          "created_utc": "2025-12-25 21:25:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3dkb4",
          "author": "Purple-Programmer-7",
          "text": "Look into Home Assistant first.\n\nI had llama 3.2 3B backing a pydantic-ai agent running everything via HA‚Ä¶ voice commands and all",
          "score": 1,
          "created_utc": "2025-12-26 22:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhzwtw",
              "author": "TelephonePossible866",
              "text": "Do you have it responding back in voice? Wake word? Piper/whisperr? I‚Äôd like to use a model that ideally has a tiny bit of personality or isn‚Äôt entirely ‚Äúdumb‚Äù in that it is only narrowly functional for specific commands controlling IoT. In other words, something with some novelty that the family can enjoy. In my head I have some potential projects, like LLM that controls thermal printer that reminds you when it‚Äôs trash day or prints the HA shopping list but also will print corny self-affirmations or schizo daily horoscopes for my wife or daughter. Idk if I could simply find some stupid rss feed to pull this stuff into a grafana dashboard and LLM retrieves from local dashboard and prints to thermal printer. Dynamic alarm clock that insults you if you snooze it. More automations from human detection sensor as you leave in the morning and come home at night. Literally making a mounted laser boundary alarm out of a rPi to make my dog stop shitting in the guest bedroom. Would love the voice assistant to chastise him when the laser is broken, notifying me. \n\nEssentially I would like a voice assistant who is helpful and can wake you up and tell you the weather, and all the normal things, but also has some absurd/silly abilities, and is generally an asshole. Admonishes you if the tv is on for 6 hours or when you forget to turn off the lights by x pm. \n\nAlso functional use case for security, not just sensor/camera/garage door alarms, but running OPNsense as a router/firewall mini pc I am pretty sure I could configure voice automation/alarms pulled from the netflow logs/suratica. I think HA has a HACS integration for it or I‚Äôm sure there‚Äôs some repo that probably works. I have never used a local LLM in my life, this is my introduction to it and the start of my journey here. Too ambitious?",
              "score": 1,
              "created_utc": "2025-12-29 05:45:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwj0zdg",
                  "author": "Purple-Programmer-7",
                  "text": "Damn dude. Quite the response. I‚Äôll try to keep mine short n sweet.\n\nCompletely custom integration and companion iOS app. Nvidia Parakeet was the ASR. Didn‚Äôt get to wake word or TTS yet (on the way).\n\nFor personality, I recommend moving away from the HA ecosystem and relying on its websockets API instead. If you‚Äôre a dev, there are significantly less headaches and more control this route.\n\nNot sure about all your goals (positive reinforcement is the best way to train a dog. They love us. They want to make you happy), but some of your projects sound fun. \n\nI‚Äôd start with openai api calls to prototype and move to local once you have your masochistic fantasy bot built.",
                  "score": 1,
                  "created_utc": "2025-12-29 11:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3z3qv",
          "author": "teleolurian",
          "text": "You can do it just fine with qwen-4b-2507 and qwen3-vl-8b for most tasks",
          "score": 1,
          "created_utc": "2025-12-27 00:18:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pugj5x",
      "title": "Built Lynkr - Use Claude Code CLI with any LLM provider (Databricks, Azure OpenAI, OpenRouter, Ollama)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pugj5x/built_lynkr_use_claude_code_cli_with_any_llm/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2025-12-24 06:27:00",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.83,
      "text": "Hey everyone! üëã\n\nI'm a software engineer who's been using Claude Code CLI heavily, but kept running into situations where I needed to use different LLM providers - whether it's Azure OpenAI for work compliance, Databricks for our existing infrastructure, or Ollama for local development.\n\nSo I built **Lynkr** \\- an open-source proxy server that lets you use Claude Code's awesome workflow with whatever LLM backend you want.\n\n**What it does:**\n\n* Translates requests between Claude Code CLI and alternative providers\n* Supports streaming responses\n* Cost optimization features\n* Simple setup via npm \n\n**Tech stack:** Node.js + SQLite\n\nCurrently working on adding Titans-based long-term memory integration for better context handling across sessions.\n\nIt's been really useful for our team , and I'm hoping it helps others who are in similar situations - wanting Claude Code's UX but needing flexibility on the backend.\n\n**Repo:** \\[https://github.com/Fast-Editor/Lynkr\\]\n\nOpen to feedback, contributions, or just hearing how you're using it! Also curious what other LLM providers people would want to see supported.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pugj5x/built_lynkr_use_claude_code_cli_with_any_llm/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvwsirq",
          "author": "tillybowman",
          "text": "i mean, you can change the env vars from claude code to use openrouters api f.e. already, what's the difference?",
          "score": 3,
          "created_utc": "2025-12-25 19:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtzw89",
          "author": "Dolsis",
          "text": "It looks interesting.\n\nHowever, I see that the only local provider is Ollama. Will it be able to support llama.cpp and/or LM Studio?\n\nFor now I'm using opencode calling  llama.cpp with qwen3-coder and it works great.",
          "score": 1,
          "created_utc": "2025-12-25 05:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvu058o",
              "author": "Dangerous-Dingo-5169",
              "text": "Thanks for replying \nAdding support for llama.cpp is next on my priority list \nI have got like 3 people asking the same thing\nAs of now it only supports ollama, azure models, databricks dbrx, open ai and openrouter models",
              "score": 1,
              "created_utc": "2025-12-25 05:59:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxm9bw",
      "title": "How do I configure LM Studio model for safety?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxm9bw/how_do_i_configure_lm_studio_model_for_safety/",
      "author": "bangboobie",
      "created_utc": "2025-12-28 08:20:34",
      "score": 4,
      "num_comments": 11,
      "upvote_ratio": 0.83,
      "text": "Apologies before I begin as I am not that tech-savvy. I managed to set-up LM Studio on a MacBook. I was wondering how secure LM Studio is that in the sense if I say something to model that would never leave my device right? Or do I need to configure any settings first? Like I turned off the headless thing and is there anything else do I need to do? I plan to work with LLMs regarding things that I wouldn't necessarily like being handed over to someone. And also things like Port 1234 sound a bit intimidating to me.\n\nI would really appreciate if anyone could tell me if I need to do anything before I actually start tinkering with models. And how I can make it more private. Although I think that apps like LM Studio would probably have some built-in protections for privacy as they are meant to be locally and the purpose would be defeated otherwise. But it's just that the UI is a bit intimidating for me.\n\n>How do I configure LM Studio models for safety?\n\n\\*privacy",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxm9bw/how_do_i_configure_lm_studio_model_for_safety/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwc6962",
          "author": "Mr_TakeYoGurlBack",
          "text": "You don't have to do anything.",
          "score": 7,
          "created_utc": "2025-12-28 09:14:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdmbw7",
              "author": "bangboobie",
              "text": "Thanks, I was a bit freaked out cause I thought that I probably messed up with something.",
              "score": 1,
              "created_utc": "2025-12-28 15:46:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwc6ars",
          "author": "nickless07",
          "text": "Nothing leaves your device, aside of the obvibious stuff you request.  \nUse the internal function to download a model? Sure the request get send. Updater? Yes that also checks if a new version is aviable and downloads it (only when you click on update). Other data? Nope. You can even disconnect your device from the internet and use LM Studio for the next 10 years without it breaking anything.   \n  \nThe 'headless thing' is when you want to use LM Studio to just host a LLM, but don't wanna use the GUI it comes with.   \nYou can change the 1234 port to something more appealing (make sure you change it in your other apps that connect to LM Studio too).  \nIt is basically just a wrapper/GUI for llama.cpp. For the time beeing there is nothing to worry about. It is not designed to gather user data and send it somewhere (sometimes i wish it could).",
          "score": 1,
          "created_utc": "2025-12-28 09:14:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd8ndc",
              "author": "ButterscotchHot9423",
              "text": "‚ÄúBasically just a wrapper for llama.cpp‚Äù\n\nAccurate if using gguf model weights. MLX if using optimized LLMs for Apple Silicon which, IMHO, is the only real reason to use LMStudio. IIRC there are a total of 3 runtimes LM Studio supports, I just can‚Äôt remember the third.",
              "score": 2,
              "created_utc": "2025-12-28 14:30:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe87u9",
                  "author": "nickless07",
                  "text": "MLX is Apple's Machine learning framework (works if you have an M Chip, good luck if you have Intel :D).  GGUF is a fileformat (like a .zip) used for llama.cpp (not every model is aviable as MLX). Safetensors is another model format used with Pytorch (most models are aviable as safetensors), but does not support quants, so it is BF16 and such. LM Studio is just a wrapper around theese so you don't have to setup them manually, but with a few clicks. Ollama also offers MLX support.",
                  "score": 2,
                  "created_utc": "2025-12-28 17:36:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdmp99",
                  "author": "bangboobie",
                  "text": "I use MLX because it said that \"optimised for M-series Macs\" but irl I have no idea of the difference between it and gguf.",
                  "score": 1,
                  "created_utc": "2025-12-28 15:48:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdmhy2",
              "author": "bangboobie",
              "text": "Thanks man, it was a bit intimidating for me at first seeing so much technical jargon at the first time.",
              "score": 1,
              "created_utc": "2025-12-28 15:47:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwchvh7",
          "author": "lucasbennett_1",
          "text": "LM Studio runs completely offline on your device, nothing leaves your mac unless you explicitly enable server mode or connect it to external services. The port 1234 is just for local communication between the app and your browser, it's not exposed to the internet. As long as you haven't enabled any cloud features or API endpoints that connect externally, everything stays on your machine. You're good to go, just don't enable server sharing or remote access features if you see them in settings.",
          "score": 1,
          "created_utc": "2025-12-28 11:06:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdm92r",
              "author": "bangboobie",
              "text": "Thanks man. I don't think I have enabled any cloud features as I have not signed into any account inside LM Studio but let me know if there are other settings that I check.",
              "score": 1,
              "created_utc": "2025-12-28 15:46:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwi606w",
                  "author": "lucasbennett_1",
                  "text": "you're all set then, just keep an eye on the settings that says remote sharing or remote acces or cloud syncing. THings are locally ran by default",
                  "score": 1,
                  "created_utc": "2025-12-29 06:35:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyo6v3",
      "title": "Any local LLM fact checking systems or frameworks?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyo6v3/any_local_llm_fact_checking_systems_or_frameworks/",
      "author": "Subject_Sir_2796",
      "created_utc": "2025-12-29 14:51:23",
      "score": 4,
      "num_comments": 13,
      "upvote_ratio": 0.83,
      "text": "Anyone come across any decent systems or frameworks for fact checking information? \n\nMy use case would mostly be for academic papers. Essentially thinking a process that would start with pdf parsing, indexing or embedding before extracting references and claims made in the text before retrieving full texts for references (where these are available) and cross referencing claims against the relevant citations to check for citation laundering, overstating, misinterpretation etc. \n\nIdeally also applying additional checks against online sources by generating RAG queries where evidence provided in the pdf is weak or absent. The desired output would be a credibility score and report that gives an overview of what information is well supported by evidence and which claims are dubious or hard to verify with reasoning and quoted evidence for these conclusions attached so they can be easily manually verified.\n\nWondered if anything like this is already around or if anyone has any thoughts on existing packages/tools that would be ideal for this use case? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyo6v3/any_local_llm_fact_checking_systems_or_frameworks/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjzqq7",
          "author": "-Akos-",
          "text": "My thoughts immediately go to MCP (mainly because I'm studying MCP, and right now everything looks like a nail with that hammer). [MCP for Research: How to Connect AI to Research Tools](https://huggingface.co/blog/mcp-for-research)",
          "score": 1,
          "created_utc": "2025-12-29 15:08:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkgd6u",
              "author": "Subject_Sir_2796",
              "text": "To be fair, this looks like exactly the kind of hammer I need. Haven‚Äôt come across this before, thanks for sharing!",
              "score": 1,
              "created_utc": "2025-12-29 16:28:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwlyw28",
                  "author": "-Akos-",
                  "text": "![gif](giphy|lkzQSklzkfqUzSd4j3)",
                  "score": 1,
                  "created_utc": "2025-12-29 20:47:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwmwazz",
              "author": "Own_Amoeba_5710",
              "text": "Yep. To take it a step further, ref tools and context 7 do this swimmingly.",
              "score": 1,
              "created_utc": "2025-12-29 23:36:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwn05is",
                  "author": "Subject_Sir_2796",
                  "text": "Right on, thanks! I‚Äôll check these out too.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:57:32",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo35lr",
          "author": "Pencil__Sharpener",
          "text": "I don‚Äôt know of anything locally‚Ä¶but I built this website https://verofyi.com/ that will fact check a YouTube video by extracting claims/assertions (non opinionated) I was able to extract the exact quote and context of the quote. In addition, for each claim I can perform a deep search that uses perplexity to do a web search and fact check it for you. It seems you‚Äôre wanting to do the same for academic papers.\n\nI think it‚Äôs really good on giving a user more information on any topic from any video. This is also using the smallest model on perplexity. So for your use case, unless you care about privacy above all. I think you‚Äôre best solution would be to use perplexity and a closed source model for ‚Äúextracting claims‚Äù similar to my app and you can do very precise online searches, and you can also include your private database via RAG if needed.",
          "score": 1,
          "created_utc": "2025-12-30 03:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwztl7b",
          "author": "remghoost7",
          "text": "[https://github.com/stanford-oval/WikiChat](https://github.com/stanford-oval/WikiChat)\n\nI haven't tried it myself, but it uses RAG to fact check against Wikipedia.  \nIt seems to support external documents as well.\n\nThey've probably done most of the heavy lifting for the things you're looking for.",
          "score": 1,
          "created_utc": "2025-12-31 23:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwn9ubl",
          "author": "AnxietyPrudent1425",
          "text": "I built one myself but I‚Äôm planning to die of starvation in the next month or so. What your budget? I accept sandwiches and canned food.",
          "score": 1,
          "created_utc": "2025-12-30 00:49:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp0m6p",
              "author": "nivix_zixer",
              "text": "Dude are you a bot? I read your recent history and if you're a human I feel bad for you. But the inconsistency in your facts (and the current state of the internet) leads me to believe you are a bot. \n\nAlso, doordash. It's how my wife and I got through a terrible time in our lives.",
              "score": 1,
              "created_utc": "2025-12-30 07:34:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv4hix",
          "author": "CooperDK",
          "text": "Just use gemini pro...",
          "score": 0,
          "created_utc": "2025-12-31 04:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww7thq",
              "author": "Subject_Sir_2796",
              "text": "I need better accuracy than I‚Äôm getting with online services and a more specific workflow. Additionally I may sometimes be using it for preprints or internal research documents that can‚Äôt be shared with a cloud service.",
              "score": 1,
              "created_utc": "2025-12-31 10:41:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjyr52",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2025-12-29 15:03:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk1aj2",
              "author": "Ainudor",
              "text": "sure, if you like nazi rewritten history and the cult of Musk personality",
              "score": 3,
              "created_utc": "2025-12-29 15:16:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pw1gwm",
      "title": "Claude code with gpt-oss 20b",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pw1gwm/claude_code_with_gptoss_20b/",
      "author": "whoami-233",
      "created_utc": "2025-12-26 10:43:18",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.7,
      "text": "Hey guys,\n\nI recently wrote an article how I hooked up Claude code with gpt-oss 20 b and used it to solve a cyber security challenge (CTF challenge).\n\nHope its helpful!\n\nCheck it out at:\n\n[https://yousofnahya.medium.com/solving-ctf-challenges-with-local-ai-models-3d7f7b8dcad0](https://yousofnahya.medium.com/solving-ctf-challenges-with-local-ai-models-3d7f7b8dcad0)",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw1gwm/claude_code_with_gptoss_20b/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw38lkl",
          "author": "gwestr",
          "text": "Just hook up claude code router, right?",
          "score": 1,
          "created_utc": "2025-12-26 21:44:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw30az",
      "title": "MiniMax-M2.1 weights are now live on Huggingface",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/llesz3v4jj9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-26 12:19:03",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw30az/minimaxm21_weights_are_now_live_on_huggingface/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    }
  ]
}