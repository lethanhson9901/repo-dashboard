{
  "metadata": {
    "last_updated": "2026-01-31 08:47:09",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 290,
    "file_size_bytes": 320121
  },
  "items": [
    {
      "id": "1qp880l",
      "title": "Finally We have the best agentic AI at home",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/4qklburem2gg1.jpeg",
      "author": "moks4tda",
      "created_utc": "2026-01-28 10:54:31",
      "score": 330,
      "num_comments": 93,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qp880l/finally_we_have_the_best_agentic_ai_at_home/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o270k87",
          "author": "Recent-Success-1520",
          "text": "If you can host Kimi 2.5 1T+ model at home then it tells me you have a really big home",
          "score": 163,
          "created_utc": "2026-01-28 11:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27dftb",
              "author": "HenkPoley",
              "text": "Apparently it‚Äôs a native 4 bit weights. So ‚Äúonly‚Äù 640 GB needed (according to Apple MLX dev).\n\nIt‚Äôs about $10k for 1 TB RAM. And then a whole server around that.",
              "score": 41,
              "created_utc": "2026-01-28 12:42:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27i5x0",
                  "author": "TechnicalGeologist99",
                  "text": "Sorry...you're going to run that model on RAM? You'll get approximately 0.00000005 tokens per second....also wouldn't the kv cache be like 2.5gb per 1000 tokens?\n\nEdit:\n\nNo one takes hyperbole as seriously as apple bottoms do",
                  "score": 27,
                  "created_utc": "2026-01-28 13:11:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27zrqj",
                  "author": "Conscious-Lobster60",
                  "text": "Page out to tape drives, it‚Äôll be fine!",
                  "score": 3,
                  "created_utc": "2026-01-28 14:44:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dxsr1",
                  "author": "Specialist-2193",
                  "text": "10k for 1TB ? You are from 2 month ago ü´†",
                  "score": 1,
                  "created_utc": "2026-01-29 10:21:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o271ff0",
          "author": "No_Conversation9561",
          "text": "not in my home",
          "score": 81,
          "created_utc": "2026-01-28 11:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27id97",
              "author": "gonxot",
              "text": "https://preview.redd.it/smbj92s3b3gg1.png?width=864&format=png&auto=webp&s=20f7fa29368ce98043f29374d70b032abee97ff3\n\nMaybe it's the same guy lol",
              "score": 46,
              "created_utc": "2026-01-28 13:12:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e98de",
                  "author": "NoIntention4050",
                  "text": "that guy still cant run it at full precision probably",
                  "score": 2,
                  "created_utc": "2026-01-29 11:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o272n51",
          "author": "rookan",
          "text": "yeah, my 16GB VRAM card can easily handle it /s",
          "score": 50,
          "created_utc": "2026-01-28 11:25:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c359c",
              "author": "ihatebadpe0ple",
              "text": "Bro, I have a Ryzen 5 5600G üò≠",
              "score": 1,
              "created_utc": "2026-01-29 02:08:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271uts",
          "author": "keypa_",
          "text": "\"at home\" we probably don't have the same home...and the same budget for hardware and electricity üí∏",
          "score": 26,
          "created_utc": "2026-01-28 11:18:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28pcqt",
              "author": "Proof_Scene_9281",
              "text": "Home with dedicated custom circuit¬†",
              "score": 4,
              "created_utc": "2026-01-28 16:38:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2784wj",
          "author": "RiskyBizz216",
          "text": "ragebait? or a troll post?",
          "score": 20,
          "created_utc": "2026-01-28 12:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27l2ep",
              "author": "shaolinmaru",
              "text": "Yes",
              "score": 9,
              "created_utc": "2026-01-28 13:28:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2728aq",
          "author": "macumazana",
          "text": "\"at home\"? who dafuq calls data center home?",
          "score": 10,
          "created_utc": "2026-01-28 11:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27ejbv",
              "author": "DasBlueEyedDevil",
              "text": "Data",
              "score": 24,
              "created_utc": "2026-01-28 12:49:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27gxem",
                  "author": "macumazana",
                  "text": "true",
                  "score": 5,
                  "created_utc": "2026-01-28 13:04:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29the1",
              "author": "Maleficent-Ad5999",
              "text": "Shouldn‚Äôt these technically be called compute-center as they don‚Äôt hoard or serve any data",
              "score": 2,
              "created_utc": "2026-01-28 19:32:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o272b0m",
          "author": "IntroductionSouth513",
          "text": "like how much memory u need for this",
          "score": 4,
          "created_utc": "2026-01-28 11:22:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272waz",
              "author": "LavishnessCautious37",
              "text": "ideally at least 512gb RAM and then depending on the generation speed you'd like an amount of VRAM approaching the same figure.",
              "score": 4,
              "created_utc": "2026-01-28 11:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2735xo",
                  "author": "IntroductionSouth513",
                  "text": "well fuck me then who has 512gb geez",
                  "score": 1,
                  "created_utc": "2026-01-28 11:29:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2797g6",
          "author": "butterninja",
          "text": "Go home, Jensen. You're drunk...",
          "score": 11,
          "created_utc": "2026-01-28 12:14:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271nwn",
          "author": "Visible_Football_852",
          "text": "\"at home\"",
          "score": 4,
          "created_utc": "2026-01-28 11:17:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2744yt",
          "author": "Birdinhandandbush",
          "text": "Who's home is this",
          "score": 4,
          "created_utc": "2026-01-28 11:37:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o274hgq",
          "author": "Tema_Art_7777",
          "text": "OP lives in a datacenter‚Ä¶",
          "score": 4,
          "created_utc": "2026-01-28 11:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27lmzj",
          "author": "pgrijpink",
          "text": "To be fair, Unsloth did show that larger models can withstand more aggressive quantisation better due to their redundancy. Their dynamic 1.58 bit version of R1 performed very well given its size was reduced by 80%. A similar setup for K2.5 would take roughly 200gb which is expensive but not unheard of. \n\nhttps://unsloth.ai/blog/deepseekr1-dynamic",
          "score": 3,
          "created_utc": "2026-01-28 13:31:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275n7q",
          "author": "Soft_Examination1158",
          "text": "In my opinion, spending money unnecessarily is like\nbuying your\nfirst electric car or your first photovoltaic system.\nSystems that cost and operate for 10,000 euros today will run on 1,000 euros systems tomorrow.\nIn another 2-3 years, everything will change.",
          "score": 8,
          "created_utc": "2026-01-28 11:48:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27ku5d",
              "author": "jay3686",
              "text": "my solar system i bought 5 years ago would cost something like 50% more today in the US.  And that's not even taking into acccount loss of federal subsidy.  but I guess US is an outlier on solar cost.  \nSame with EVs.  US is weird.",
              "score": 8,
              "created_utc": "2026-01-28 13:26:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28g76m",
                  "author": "AndersonBlackstar",
                  "text": "Im American and can confirm this",
                  "score": 3,
                  "created_utc": "2026-01-28 15:59:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29p7e2",
                  "author": "Soft_Examination1158",
                  "text": "I'm Italian, but in general, technology changes over time in favor of costs.\nTelevisions, tablets, PCs, and so on, cannot compare to the performance/costs of 10\nyears ago.",
                  "score": 0,
                  "created_utc": "2026-01-28 19:13:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bhujn",
                  "author": "duplicati83",
                  "text": "I wonder if that orange skidmark and his fucken tariffs might have something to do with that?",
                  "score": 0,
                  "created_utc": "2026-01-29 00:13:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bwi0x",
          "author": "abmateen",
          "text": "Is it good at OpenAI style tool calling? I tried it in Ollama cloud it didn't call any tool I attached to it.",
          "score": 2,
          "created_utc": "2026-01-29 01:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o273zli",
          "author": "tiffanytrashcan",
          "text": "When your home is simply an extension of RunPod. üòÇ",
          "score": 1,
          "created_utc": "2026-01-28 11:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27av42",
          "author": "trmnl_cmdr",
          "text": "We‚Äôve seen this show before, it doesn‚Äôt end the way you think it does.",
          "score": 1,
          "created_utc": "2026-01-28 12:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27fqu6",
          "author": "_VirtualCosmos_",
          "text": "52 fucking percent on Humanity Last Exam? is that really true?",
          "score": 1,
          "created_utc": "2026-01-28 12:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aw8r7",
              "author": "PerformanceRound7913",
              "text": "Benchmaxed",
              "score": 1,
              "created_utc": "2026-01-28 22:23:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2axliy",
                  "author": "_VirtualCosmos_",
                  "text": "Even if so, do you know how hard those tests are? Also only resolving only 50% of them would be pretty lame if trained directly on the set.",
                  "score": 1,
                  "created_utc": "2026-01-28 22:30:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27g59v",
          "author": "Atomzwieback",
          "text": "Must be ragebait",
          "score": 1,
          "created_utc": "2026-01-28 12:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27rzth",
          "author": "Available-Craft-5795",
          "text": "What kinda GPU do you own!?",
          "score": 1,
          "created_utc": "2026-01-28 14:04:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28j5kz",
          "author": "Separ0",
          "text": "How does Claude continue to cook so hard on coding no matter what. How did they train that thing.",
          "score": 1,
          "created_utc": "2026-01-28 16:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28j92i",
          "author": "Separ0",
          "text": "1 TPS",
          "score": 1,
          "created_utc": "2026-01-28 16:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kks4",
          "author": "hocuspocus4201",
          "text": "At home aka a datacenter",
          "score": 1,
          "created_utc": "2026-01-28 16:18:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rj8e",
          "author": "Witty_Mycologist_995",
          "text": "1T, you must have a really big home",
          "score": 1,
          "created_utc": "2026-01-28 16:48:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a21ov",
          "author": "-PM_ME_UR_SECRETS-",
          "text": "Whose home exactly",
          "score": 1,
          "created_utc": "2026-01-28 20:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aw3w8",
          "author": "PerformanceRound7913",
          "text": "Artificial Analysis is the most useless, for-profit and never aligned with real world",
          "score": 1,
          "created_utc": "2026-01-28 22:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e3c43",
          "author": "danny_094",
          "text": "I still consider Deepseek to be the most powerful large model that one could \"run\" at home.",
          "score": 1,
          "created_utc": "2026-01-29 11:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gqvgu",
          "author": "cranberry-strawberry",
          "text": "I don't see a need to use Kimi",
          "score": 1,
          "created_utc": "2026-01-29 19:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gy27u",
          "author": "HealthyCommunicat",
          "text": "‚ÄúAt home‚Äù - i spent $12k for inference and its barely usuable at 15-20token/s.\n\n‚ÄúAt home‚Äù my ass.",
          "score": 1,
          "created_utc": "2026-01-29 19:52:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2i3606",
          "author": "LowRentAi",
          "text": "Local Ai or forget it, the Flagships are garbage and getting worse by the day. Programming Bias...",
          "score": 1,
          "created_utc": "2026-01-29 23:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2js3yd",
          "author": "Salazopyrin6666",
          "text": "Where you use them ? coz for me they are so dumb and not correct",
          "score": 1,
          "created_utc": "2026-01-30 04:57:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2728iz",
          "author": "ptear",
          "text": "Does it need a SMR to power at home?",
          "score": 1,
          "created_utc": "2026-01-28 11:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o276ege",
          "author": "IngwiePhoenix",
          "text": "\"At home\"? Are you sure? XD The VRAM you'd need for that will destroy your wallet...",
          "score": 1,
          "created_utc": "2026-01-28 11:54:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o288v6l",
          "author": "ab2377",
          "text": "any mention of clawdbot should be banned. absolutely pathetic advertisement is going on of it here.",
          "score": 1,
          "created_utc": "2026-01-28 15:27:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28qcn9",
          "author": "P3rpetuallyC0nfused",
          "text": "Would the only viable option for running this on consumer gpu be 2x m3ultra 512gb clustered with exo?",
          "score": 1,
          "created_utc": "2026-01-28 16:43:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr0pom",
      "title": "Clawdbot ‚Üí Moltbot ‚Üí OpenClaw. The Fastest Triple Rebrand in Open Source History",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vueo4hoefggg1.png",
      "author": "blondewalker",
      "created_utc": "2026-01-30 09:20:17",
      "score": 203,
      "num_comments": 76,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qr0pom/clawdbot_moltbot_openclaw_the_fastest_triple/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2l26j0",
          "author": "WestTraditional1281",
          "text": "How about ClawMydia? Because this hype train is going viral....",
          "score": 146,
          "created_utc": "2026-01-30 11:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l7bdg",
              "author": "Digiarts",
              "text": "We got a winner",
              "score": 19,
              "created_utc": "2026-01-30 12:02:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lzo1h",
                  "author": "CarpenterAlarming781",
                  "text": "And a¬†¬†Wiener ?",
                  "score": 9,
                  "created_utc": "2026-01-30 14:44:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2n0rlp",
              "author": "BurntUnluckily",
              "text": "Uhm ackshually, chlamydia is a bacteria",
              "score": 8,
              "created_utc": "2026-01-30 17:32:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2n804d",
                  "author": "WestTraditional1281",
                  "text": "I'm aware. But that doesn't have the same sting to it.\n\nI could have just said that vibe coding is infectious. It's spreading like crabs. My girlfriend even admitted that vibing is contagious, especially if others are doing it with you and especially if you're vibing on each other's dedicated ClawMydia filled boxes.\n\nBut viral seemed more wholesome.",
                  "score": 4,
                  "created_utc": "2026-01-30 18:04:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2kvygg",
          "author": "ZenEngineer",
          "text": "Vibe naming",
          "score": 37,
          "created_utc": "2026-01-30 10:31:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kq3oz",
          "author": "qaf23",
          "text": "How about DeepClaw next?",
          "score": 25,
          "created_utc": "2026-01-30 09:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ks398",
              "author": "Sp3ctre18",
              "text": "Then, MistClaw?",
              "score": 4,
              "created_utc": "2026-01-30 09:56:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kyy5h",
                  "author": "Artistic_Regard_QED",
                  "text": "clawGPT",
                  "score": 6,
                  "created_utc": "2026-01-30 10:56:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ldlm5",
                  "author": "macumazana",
                  "text": "ClawMolt/ MoltenClaw",
                  "score": 5,
                  "created_utc": "2026-01-30 12:45:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2l6rw0",
          "author": "Senior_Delay_5362",
          "text": "It's not a rebrand, it's just the repo undergoing 'rapid biological mutation'. By March, it‚Äôll probably just be an ASCII icon of a lobster",
          "score": 26,
          "created_utc": "2026-01-30 11:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lj4sl",
              "author": "Elder_SysOp",
              "text": "I think they all end up crabs.",
              "score": 7,
              "created_utc": "2026-01-30 13:18:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lodw5",
                  "author": "hornynnerdy69",
                  "text": "Rust strikes again",
                  "score": 2,
                  "created_utc": "2026-01-30 13:47:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nyxr3",
                  "author": "Traveler3141",
                  "text": "It's crab and other parasites all the way down.",
                  "score": 1,
                  "created_utc": "2026-01-30 20:05:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ky705",
          "author": "luchtverfrissert",
          "text": "Next up: Clawrizard",
          "score": 17,
          "created_utc": "2026-01-30 10:50:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n3g9h",
              "author": "_Cromwell_",
              "text": "I mean that's better than any of the three they use so far. Has the claw in it. Funny play on pok√©mon name. Everybody loves pok√©mon. Plus also sounds like \"wizard\" and the thing is supposed to act like a wizard.\n\nYou pulled a better name out of your ass to make fun of them then they have managed in three attempts.\n\nFrom a marketing angle I still wouldn't use it because it's too hard for people to remember how to spell randomly to type in. But it's still better than their three.",
              "score": 3,
              "created_utc": "2026-01-30 17:45:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kvsco",
          "author": "jeremiah256",
          "text": "Yeah, Moltbot wasn‚Äôt cutting it.",
          "score": 12,
          "created_utc": "2026-01-30 10:29:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l394r",
          "author": "duplicati83",
          "text": "They‚Äôre all pretty shit names.",
          "score": 25,
          "created_utc": "2026-01-30 11:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mimof",
              "author": "onethousandmonkey",
              "text": "To be fair, they are gen AI names generated by gen AI.",
              "score": 1,
              "created_utc": "2026-01-30 16:12:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l8f16",
          "author": "ThisGuyCrohns",
          "text": "Moltbot was a fucking terrible name",
          "score": 15,
          "created_utc": "2026-01-30 12:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2miy6g",
              "author": "ThenExtension9196",
              "text": "Yeah it was awful.",
              "score": 1,
              "created_utc": "2026-01-30 16:13:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ltbgu",
          "author": "ab2377",
          "text": " # 1 software to avoid.\nalso their fake advertisement is sickening.",
          "score": 14,
          "created_utc": "2026-01-30 14:12:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mj3tj",
              "author": "ThenExtension9196",
              "text": "Nah. It‚Äôs legit, I‚Äôm just waiting kimi 2.5 support. Plus the developer is an OG programmer. Dude wrote the pdf renderer used in like a billion devices.",
              "score": 1,
              "created_utc": "2026-01-30 16:14:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oh8bz",
                  "author": "Objeckts",
                  "text": "You should be very careful of putting this on a machine with access to your actual accounts and data. \n\nRescheduling flights sounds cool, but transferring all your points to a scammer that emailed a malicious prompt is a real risk.",
                  "score": 2,
                  "created_utc": "2026-01-30 21:31:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ql7b8",
                  "author": "wthigo",
                  "text": "Kimi k2.5 is working now, check the discord",
                  "score": 1,
                  "created_utc": "2026-01-31 04:44:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lbehk",
          "author": "blamestross",
          "text": "This is all an Accelerando reference right? The lobster thing?",
          "score": 5,
          "created_utc": "2026-01-30 12:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lizc9",
          "author": "Elder_SysOp",
          "text": "https://preview.redd.it/l5uicowtlhgg1.jpeg?width=1536&format=pjpg&auto=webp&s=03f14d1a24d05e6e9158ea1f9454b2fd07c7d7f2",
          "score": 15,
          "created_utc": "2026-01-30 13:18:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kwm6v",
          "author": "cybernagl",
          "text": "What about moltbook.com now? Clawbook?",
          "score": 3,
          "created_utc": "2026-01-30 10:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lfx6c",
          "author": "madsheepPL",
          "text": "this is the thing that let's you take over your friends computer through email right?",
          "score": 4,
          "created_utc": "2026-01-30 13:00:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ky840",
          "author": "Sartilas",
          "text": "I literally spent part of last night switching between clawdbot and moletbot...",
          "score": 3,
          "created_utc": "2026-01-30 10:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mlf4m",
              "author": "xeeff",
              "text": "it happens, man. it happens.",
              "score": 2,
              "created_utc": "2026-01-30 16:24:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2njb5w",
              "author": "WiseExercise7307",
              "text": "My mind has blown up. I have spent more than 20 hours trying to set it up locally. Damn think does not support many models, and non of the APIs are freeüò≠\nEvery damn thing has got a price tag along with our privacy at sale",
              "score": 2,
              "created_utc": "2026-01-30 18:54:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nsa3t",
                  "author": "Loose-Doubt-4421",
                  "text": "Chill man. Have you ever heard of : \n\n\\- 90 days free Qwen API from Alibabacloud   \n\\- Ollama free cloud models  \n\\- Openrouter free 1000 requests per day\n\nIt support all models i tried.",
                  "score": 1,
                  "created_utc": "2026-01-30 19:34:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mr306",
              "author": "FaceDeer",
              "text": "At least now you know all the places that need updating.",
              "score": 1,
              "created_utc": "2026-01-30 16:49:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2osqln",
              "author": "JamaiKen",
              "text": "Is this the singularity?",
              "score": 1,
              "created_utc": "2026-01-30 22:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lf78p",
          "author": "krigeta1",
          "text": "Clawkachu!",
          "score": 3,
          "created_utc": "2026-01-30 12:55:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ll3hf",
          "author": "ZiXXiV",
          "text": "Soon it'll eat bank accounts and will turn in to GoldClaw.",
          "score": 3,
          "created_utc": "2026-01-30 13:29:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ldi4k",
          "author": "shk2096",
          "text": "ClawsterFu*K",
          "score": 5,
          "created_utc": "2026-01-30 12:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ldjlh",
          "author": "MarvPara0id",
          "text": "How in the world does it look like the other names never existed??",
          "score": 2,
          "created_utc": "2026-01-30 12:45:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nh95y",
          "author": "LeopardJockey",
          "text": "Wow. This is instilling immense amounts of confidence.",
          "score": 2,
          "created_utc": "2026-01-30 18:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o28io",
          "author": "Medical-Foundation83",
          "text": "As long as it does not become ClosedClaw we should be ok!",
          "score": 2,
          "created_utc": "2026-01-30 20:20:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ku7il",
          "author": "Everlier",
          "text": "It's so annoying, I had to update it in my OSS project. The first time I thought \"never again\", but here we are",
          "score": 2,
          "created_utc": "2026-01-30 10:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lqwxy",
              "author": "FirstEvolutionist",
              "text": "First time working a fresh open source project that gained popularity really quickly?",
              "score": 2,
              "created_utc": "2026-01-30 14:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mn89f",
                  "author": "Sp3ctre18",
                  "text": "Was thinking thinking that. \"Never again\" probably ends in like a week at most. üòõ",
                  "score": 1,
                  "created_utc": "2026-01-30 16:32:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lnsz0",
          "author": "Mentalextensi0n",
          "text": "rewrite it in rust",
          "score": 1,
          "created_utc": "2026-01-30 13:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m0p8v",
          "author": "CharlesCowan",
          "text": "MegaClawTron!",
          "score": 1,
          "created_utc": "2026-01-30 14:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m2lcf",
          "author": "ctrl-brk",
          "text": "What was the stated reason this time?",
          "score": 1,
          "created_utc": "2026-01-30 14:58:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2manyz",
          "author": "SpeedOfSound343",
          "text": "How about Clawmini or Clawk?",
          "score": 1,
          "created_utc": "2026-01-30 15:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mih5h",
          "author": "onethousandmonkey",
          "text": "Soon to be sold to a clueless investor for mid-level yacht money.",
          "score": 1,
          "created_utc": "2026-01-30 16:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nsxgs",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-01-30 19:37:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nwoyr",
                  "author": "onethousandmonkey",
                  "text": "I don't trust any of this vibe-code, so nope.",
                  "score": 2,
                  "created_utc": "2026-01-30 19:54:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mogt9",
          "author": "ionzd",
          "text": "If you will check the commits history, you'll find that on the first day its name was warelay.",
          "score": 1,
          "created_utc": "2026-01-30 16:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2n7i1u",
          "author": "GarlicPestoToast",
          "text": "I don't know if it was intentional, but I keep having Jordan Peterson pop into my head.",
          "score": 1,
          "created_utc": "2026-01-30 18:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nivty",
          "author": "WiseExercise7307",
          "text": "Guys i am so frustrated\nHas anyone set it up locally in their system?¬†\nWhat api have you used? I search so much, every damn API is paid\nCan someone tell me if there is any free API working with the bot? üò¢",
          "score": 1,
          "created_utc": "2026-01-30 18:52:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2noh0t",
              "author": "fersands",
              "text": "Yep been trying too to run it locally but with no success. i got it to ran on the ollama cloud but it gets to the limit really quickly heh. \n\ni am trying to run it with mistral but i dont know if im doing something wrong or its just that slow, that its not responding to me.",
              "score": 2,
              "created_utc": "2026-01-30 19:17:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2o2vw7",
                  "author": "WiseExercise7307",
                  "text": "Damn thing needs optimization or we need money in our pockets",
                  "score": 1,
                  "created_utc": "2026-01-30 20:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2nlwj3",
          "author": "eazolan",
          "text": "Jesus Christ. I just did the migration path to Moltbot last night.",
          "score": 1,
          "created_utc": "2026-01-30 19:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2otyca",
          "author": "Denial_Jackson",
          "text": "I got my P100 heavy for coprorate email fuskery. I am ready to angle grind these top of these peculiar air channels and install these legendary Inno3D fans. Inno3D is so much more heavier than Jensen Huang in a leather jacket. Inno3D represents the butterflies in a fart jar.",
          "score": 1,
          "created_utc": "2026-01-30 22:33:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p941f",
          "author": "altsyst",
          "text": "Actually the full rebranding chain was:\n\nWarelay ‚Üí Clawdis ‚Üí Clawdbot ‚Üí Moltbot ‚Üí OpenClaw.\n\nSource: https://x.com/steipete/status/1996622282580795807\n\nI expect the next rebrand to be Warelay. (Devs don't like unclosed parentheses ;-",
          "score": 1,
          "created_utc": "2026-01-30 23:55:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pi3dz",
          "author": "PsychologicalOne752",
          "text": "MoltClaw and OpenMolt still remain. ü§£",
          "score": 1,
          "created_utc": "2026-01-31 00:44:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pw6yp",
          "author": "RandoReddit72",
          "text": "Next WhiteClaw",
          "score": 1,
          "created_utc": "2026-01-31 02:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qe9lq",
          "author": "enterme2",
          "text": "The creator is really riding the hype train. He know what he's doing. It become viral even more because of Claude = Clawdbot fiasco. Then Bolt = Moltbot . Now OpenAI =  OpenClaw. Dude is just trolling at this point. Expect the naming to be some combination of ai companies mashup.",
          "score": 1,
          "created_utc": "2026-01-31 03:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2meek0",
          "author": "my_shoes_hurt",
          "text": "Should rename to LobstersAreFuckingStupidAndIDunnoWhyImObsessedWithThem",
          "score": 1,
          "created_utc": "2026-01-30 15:53:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n1hkt",
              "author": "agent606ert",
              "text": "Jordan Peterson",
              "score": 1,
              "created_utc": "2026-01-30 17:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mfc2a",
          "author": "eli_pizza",
          "text": "Isn‚Äôt there a different, more appropriate sub for this stuff? It‚Äôs not about local LLMs really at all. We definitely don‚Äôt need marketing posts about it.",
          "score": 1,
          "created_utc": "2026-01-30 15:57:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mivvg",
              "author": "onethousandmonkey",
              "text": "True, reported.",
              "score": 1,
              "created_utc": "2026-01-30 16:13:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lm4wt",
          "author": "Albertkinng",
          "text": "Clawdbot is getting interesting‚Ä¶ I am going to open one.",
          "score": 0,
          "created_utc": "2026-01-30 13:35:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mxeuq",
          "author": "AeroMogli",
          "text": "Lowkey better character design than what nintendo's been putting out recently",
          "score": 0,
          "created_utc": "2026-01-30 17:17:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmrwxl",
      "title": "Clawdbot: the AI assistant that actually messages you first",
      "subreddit": "LocalLLM",
      "url": "https://jpcaparas.medium.com/clawdbot-the-5-month-ai-assistant-that-actually-messages-you-first-8b247ac850b8?sk=0d521efdf2ceafe37973887b57b168ba",
      "author": "jpcaparas",
      "created_utc": "2026-01-25 18:59:25",
      "score": 171,
      "num_comments": 149,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qmrwxl/clawdbot_the_ai_assistant_that_actually_messages/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1u9m94",
          "author": "inigid",
          "text": "Clawdbot opens up the potential for a massive supply-chain attack that can steal or destroy everything on your machine and network, harvest and exfiltrate emails, crypto wallets, bank account and credit card details, SSN numbers, personal information, ssh keys.\n\nThe Solana meme coin and hype pumps by countless influencers is also a massive red flag.\n\nStay TF away from it if you have any sense.",
          "score": 27,
          "created_utc": "2026-01-26 16:10:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ud0b5",
              "author": "kublaikhaann",
              "text": "stop making too much sense",
              "score": 10,
              "created_utc": "2026-01-26 16:25:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ul578",
                  "author": "inigid",
                  "text": "Ikr, the thing literally has auto-update for itself, and then there are the downloadable skills on top!\n\nIt's like you are asking for trouble!\n\nOh, I'm just going to install this with sudo privileges and connect it up to all my infrastructure and personal accounts.\n\nWhat could possibly go wrong!  Smh.\n\nhttps://preview.redd.it/0h7v5nrr5qfg1.jpeg?width=1581&format=pjpg&auto=webp&s=2566f1d6f9e2d37b127da3ee58bb1740e6990adc",
                  "score": 9,
                  "created_utc": "2026-01-26 16:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yi54i",
              "author": "ab2377",
              "text": "üíØ",
              "score": 2,
              "created_utc": "2026-01-27 04:20:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24dp6w",
              "author": "PeakBrave8235",
              "text": "But but but John Gruber told me it was cool!!!!!! And everything he says is amazing so obviously you're wrong¬†\n\nSarcasm.¬†",
              "score": 2,
              "created_utc": "2026-01-28 00:19:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x3t7z",
              "author": "ParanoidBlueLobster",
              "text": "Bit of a conspiracy theorist are we?\n\nIt's an open-source tool https://github.com/clawdbot/clawdbot\n\nAnd you update it manually https://docs.clawd.bot/install/updating",
              "score": 5,
              "created_utc": "2026-01-26 23:44:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xbf37",
                  "author": "inigid",
                  "text": "[https://cybersecuritynews.com/clawdbot-chats-exposed/](https://cybersecuritynews.com/clawdbot-chats-exposed/)\n\n[https://socradar.io/blog/clawdbot-is-it-safe/](https://socradar.io/blog/clawdbot-is-it-safe/)\n\nhttps://preview.redd.it/4ls6ihradsfg1.jpeg?width=1280&format=pjpg&auto=webp&s=905eda3538cc276b330924c33d14e18022a95d40",
                  "score": 8,
                  "created_utc": "2026-01-27 00:23:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24um1r",
                  "author": "BernardoOne",
                  "text": "no conspiracy theory required, if you have an AI agent with full permissions on your PC and connected to the internet you're 100% vulnerable to prompt injection.",
                  "score": 1,
                  "created_utc": "2026-01-28 01:47:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o23yhf4",
              "author": "Brumotti",
              "text": "Blackbox AI fixes this.  \n  \n[https://x.com/mhtua/status/2015943960867340356](https://x.com/mhtua/status/2015943960867340356)",
              "score": 1,
              "created_utc": "2026-01-27 23:02:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dc16l",
                  "author": "Temporary_Cup4303",
                  "text": "but not the prompt injection risk",
                  "score": 2,
                  "created_utc": "2026-01-29 07:02:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pe48c",
              "author": "JimmyDub010",
              "text": "Lies. all lies",
              "score": 1,
              "created_utc": "2026-01-31 00:22:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21bvsw",
              "author": "Jasmine_Demir",
              "text": "Probably depends on how much you have to lose as well, and how disruptive it would be for you to stop your entire life for a day or two while you perform triage. \n\nIf you're 22 with a $50,000 net worth it's a very different consequence than begin 55 with 3 children, $5,000,000 in available debt/assets, and a convoluted chain of responsibilities that would come to a halt once you incorporated an assistant at this layer of your life flow and it failed.\n\nThe ones jumping into this don't have much to lose, and/or have the time and experience to stay on top of security issues.",
              "score": 0,
              "created_utc": "2026-01-27 16:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pwp1d",
          "author": "mike7seven",
          "text": "Spent most of the day figuring out that you need to get your Claude Code key (Oauth) on another machine and paste it for use on whatever machine you are setting up Clawdbot on. \n\nFrom Ops tutorial: \n‚ÄúYou have two options. Claude OAuth (the easiest) or API keys (more control over costs). For beginners, OAuth with a Claude Pro subscription is the simplest path.‚Äù\n\nNow I‚Äôm over even playing around with this thing for the rest of the day. \n\nMy two cents: The MacOS app is nice but you have to build it from source. Also good luck getting app auth setup properly if you are using a remote machine.",
          "score": 11,
          "created_utc": "2026-01-25 23:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pxaxs",
              "author": "DependentNew4290",
              "text": "Is it that easy to set up one as a non-technical person???",
              "score": 2,
              "created_utc": "2026-01-25 23:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q1s1j",
                  "author": "mike7seven",
                  "text": "Depends on how you plan on setting it up. Using ChatGPT and Codex it worked immediately and beautifully. Claude was not that easy. For local follow OPs guide.",
                  "score": 2,
                  "created_utc": "2026-01-26 00:08:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20gkav",
                  "author": "brianlmerritt",
                  "text": "If you want to set it up on your personal or work computer, provide unlimited access to said computer and all of your email and messaging accounts plus whatever else is on there like online banking or company information, it is still not that easy.\n\nBut you will be putting your personal computer or company at risk - there are a lot of little add-ons that have \"extra\" code and abilities to log into your computer, access everything plus use your AI tokens and accounts.\n\nUnless you are happy setting this up securely (see the link at the end of the post) there is a very good chance it doesn't go well.    \n  \nAlso note that one email or message can come in and hijack the AI within the system even if you didn't load any dodgy utilities.\n\n",
                  "score": 1,
                  "created_utc": "2026-01-27 13:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20sk57",
              "author": "Manarj789",
              "text": "It took all of 5 minutes to do, they literally tell you the steps to take",
              "score": 0,
              "created_utc": "2026-01-27 14:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ux2ag",
              "author": "[deleted]",
              "text": "[removed]",
              "score": -3,
              "created_utc": "2026-01-26 17:52:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yqrif",
                  "author": "Winter-Editor-9230",
                  "text": "r/LocalLLM follows platform-wide Reddit Rules; crypto currency and self promotion.",
                  "score": 1,
                  "created_utc": "2026-01-27 05:17:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1s2o5c",
          "author": "Ashamed_Promise7726",
          "text": "Has anyone successfully connected a local llm on their machine to Clawdbot? I have a few different models already downloaded to my PC, but I cannot get Clawdbot to work with them, and it keeps wanting an api key for a usage based model.\n\nIs it possible to run Clawdbot 100% locally?",
          "score": 10,
          "created_utc": "2026-01-26 07:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s41bu",
              "author": "kinesivan",
              "text": "If you are hosting with LM Studio or Ollama, does not matter what API key you pass, any should work.",
              "score": 3,
              "created_utc": "2026-01-26 07:33:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s7zwo",
                  "author": "Yorn2",
                  "text": "No, I'm having the same issue, Local OpenAPI-compatible isn't even an option in the choices. Nor is Ollama.",
                  "score": 2,
                  "created_utc": "2026-01-26 08:07:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29mmhl",
                  "author": "ReaverKS",
                  "text": "Curious how successful/useful your local models are for clawdbot as opposed to openai or claude? I'm sure the cost savings is huge.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1slfam",
              "author": "Everlier",
              "text": "I spent my entire weekend integrating Clawdbot into Harbor so that it's possible to set it up with a local LLM in a few commands (granted you have Harbor installed):\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot",
              "score": 3,
              "created_utc": "2026-01-26 10:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xkbhh",
                  "author": "Yorn2",
                  "text": "Harbor seems cool but I'm using MacOS and mlx_lm.server for running a local instance of Deepseek 3.2, not llama.cpp or ollama or vllm. Though I do have a non-mac AI box where I might use Harbor instead someday. Do you plan on adding MacOS support sometime?",
                  "score": 2,
                  "created_utc": "2026-01-27 01:09:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25kt3u",
                  "author": "yogabackhand",
                  "text": "First I‚Äôve heard of Harbor. Looks cool! I‚Äôll give it a try. It would be cool if there was a MacOS desktop app (for configuration) and iOS client app like Pigeon (would be nice to have a MacOS client app too).",
                  "score": 1,
                  "created_utc": "2026-01-28 04:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yju3k",
              "author": "Loud-Layer3329",
              "text": "Yes I have using Ollama. Haven't had much luck with the models, though. So far Qwen2.5:14b has been the best to use via the web interface, but it gives a lot of nonsense via Telegram.\n\nNote that you need to use a model that supports tools, and Clawdbot only permits models that have a context window of >16k. Clawdbot supports the openai endpoint so you can use the baseurl [http://x.x.x.x:11434/v1](http://x.x.x.x:11434/v1)",
              "score": 2,
              "created_utc": "2026-01-27 04:31:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1z9r9b",
              "author": "No_Box1288",
              "text": "Running it with MiniMax M2.1 works well. I set it up following this tutorial, pretty straightforward:\nhttps://x.com/MiniMax_AI/status/2014380057301811685\n\nAlso saw a coding plan link in the MiniMax Discord. There‚Äôs a discount + cashback if you use it:\nhttps://platform.minimax.io/subscribe/coding-plan?code=Cp84x9ex1L&source=link",
              "score": 2,
              "created_utc": "2026-01-27 07:50:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22ymy0",
                  "author": "JimmyDub010",
                  "text": "Lucky. you must have some kind of super computer",
                  "score": 1,
                  "created_utc": "2026-01-27 20:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25pgma",
              "author": "Frenchplay57",
              "text": "I tried all day without success with the help of gpt, gemini and Claude. Claude managed to get it connected but there is no response in the interface.¬†",
              "score": 2,
              "created_utc": "2026-01-28 04:39:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28gu3v",
                  "author": "Ashamed_Promise7726",
                  "text": "Same here....I cant get anything to work.",
                  "score": 1,
                  "created_utc": "2026-01-28 16:01:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1pa4kx",
          "author": "Acceptable_Home_",
          "text": "Might try soon, sounds good!",
          "score": 5,
          "created_utc": "2026-01-25 22:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pax6h",
              "author": "jpcaparas",
              "text": "thank you. apologies in advance if the guide isnt too technical. i just want people to get into it. pete the maintainer is also a cool follow on Twitter",
              "score": -2,
              "created_utc": "2026-01-25 22:05:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rogty",
          "author": "threathunter369",
          "text": "your take on this guys?\n\n[https://x.com/theonejvo/status/2015401219746128322](https://x.com/theonejvo/status/2015401219746128322)",
          "score": 6,
          "created_utc": "2026-01-26 05:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27l6b0",
              "author": "samuel79s",
              "text": "That's pretty concerning. I like the \"tailscale funnel\" trick that allows to put a service on the Internet so easily, but the reality is that as soon as you setup it you start getting \"knocks on the door\".\n\nI'm sure a lot of people think that url's like macmini.ts43343.ts.net are hardly discoverable but it's actually the opposite. TLS certificate logs shout their existence pretty openly with the added bias for unprofessionally run services in home devices.\n\nIt's going to be a slaughter.",
              "score": 1,
              "created_utc": "2026-01-28 13:28:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ryqsv",
              "author": "xak47d",
              "text": "That's for noobs who just discovered cloud hosting following the hype and don't properly secure their instances. That's very unrelated to the tool and its capabilities",
              "score": 1,
              "created_utc": "2026-01-26 06:50:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1whdbz",
              "author": "Double-Lavishness870",
              "text": "LLms are insecure per default. Any browsing, email reading, tweet reading can insert bad behavior. \n\nBest knowledge and advice see here: https://media.ccc.de/v/39c3-agentic-probllms-exploiting-ai-computer-use-and-coding-agents#t=1218\n\nIsolation is needed.",
              "score": 1,
              "created_utc": "2026-01-26 21:55:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1slmv3",
          "author": "Everlier",
          "text": "If you're like me and wanted to setup Clawdbot with a local LLM, check out this integration in Harbor:\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot\n\nIt also runs Clawdbot containerized, so that it won't break your host system if something will go wrong",
          "score": 5,
          "created_utc": "2026-01-26 10:12:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1umj0n",
          "author": "cmndr_spanky",
          "text": "From a security perspective, I have zero interest in this right now (LLM autonomy with access to my calendar etc). And the supposed benefits are minimal (I already get calendar reminders, I don‚Äôt need an LLM bot to ‚Äúextra‚Äù remind me). Beyond that‚Ä¶ all the other use cases are fine with direct access to normal chatGPT / Gemini etc ..\n\nAlso Apple with soon replace Siri with Gemini and I imagine that‚Äôs going to be a game changer in terms of iOS integration and proactive AI",
          "score": 4,
          "created_utc": "2026-01-26 17:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcz9x",
          "author": "TendiesOnlyPls",
          "text": "Ok... so has anyone given Clawdbot access to their dating apps and asked it to go wrangle up some consenting ass? Really want to see how well these LLMs are able to spit game.",
          "score": 5,
          "created_utc": "2026-01-26 21:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ug8e0",
          "author": "Weird-Consequence366",
          "text": "Astroturf campaign",
          "score": 6,
          "created_utc": "2026-01-26 16:39:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sdrta",
          "author": "tomByrer",
          "text": "Is your article some where else than Medium please?",
          "score": 4,
          "created_utc": "2026-01-26 08:59:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ri8h1",
          "author": "No_Conversation9561",
          "text": "Entire X feed has been this lately",
          "score": 2,
          "created_utc": "2026-01-26 04:49:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o261w6z",
              "author": "Prestigious_Pace_108",
              "text": "That and crypto bros pushing along with Brave search integration gave me the message. I cancelled setup.",
              "score": 2,
              "created_utc": "2026-01-28 06:06:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rqpdf",
          "author": "Fast_Back_4332",
          "text": "How safe is it?",
          "score": 2,
          "created_utc": "2026-01-26 05:48:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ryvys",
              "author": "xak47d",
              "text": "It might delete all your files if you make it mad",
              "score": 7,
              "created_utc": "2026-01-26 06:51:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s9o8e",
                  "author": "Cressio",
                  "text": "Is that literally true? It seems like it would be but I haven't looked too deep into it. I don't think I *want* something with that deep of access to my stuff lol",
                  "score": 1,
                  "created_utc": "2026-01-26 08:22:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20w5zw",
                  "author": "jeremyckahn",
                  "text": "A hallmark of any great software",
                  "score": 1,
                  "created_utc": "2026-01-27 14:51:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o268zk4",
              "author": "leo-k7v",
              "text": "as safe as sudo anything in your terminal. OS does matter - you start by giving it admin privileges‚Ä¶ good luck",
              "score": 2,
              "created_utc": "2026-01-28 07:02:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2itsi8",
          "author": "VanCliefMedia",
          "text": "I actually dug through the codebase recently. Worth understanding what you're looking at:\n\n\n\nThere's no AI inside. None. No model, no weights, no inference code.\n\n\n\nIt's an orchestration layer - routes messages from WhatsApp/Telegram/Slack/etc to Claude or GPT (via YOUR API key, YOUR subscription). Then routes the response back.\n\n\n\nThe architecture is roughly:\n\n\\- 60% platform integrations (Baileys for WhatsApp, grammY for Telegram, etc.)\n\n\\- 30% routing/session logic\n\n\\- 10% API calls to external AI\n\n\n\nThat's not a criticism. It's actually clean engineering. But \"AI assistant\" is generous framing for what's essentially a message broker with tool execution.\n\n\n\nThe real question is what happens when Anthropic ships native WhatsApp/Slack integration. Does orchestration remain valuable or does Moltbot become a niche self-hosting option?  \n  \nMade a video for everyone if you want to dive deeper [Why Clawdbot (Moltbot) Has No AI Inside (And Why That Matters) - YouTube](https://www.youtube.com/watch?v=bQXi5Nd8c40)",
          "score": 2,
          "created_utc": "2026-01-30 01:37:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q8d1z",
          "author": "snam13",
          "text": "For once, I‚Äôm early! Set this up last week. \nDon‚Äôt expect miracles. If you‚Äôve used claude code or similar, it will feel like those but in your chat app. Be careful of burning lots of API tokens.",
          "score": 2,
          "created_utc": "2026-01-26 00:40:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qh1fv",
              "author": "ThenExtension9196",
              "text": "You may not be squeezing the lemon yet. There‚Äôs such insane amount of functionality you can get out of it by wiring it to gitlab/github and api services. I legit think this thing will be able to automated nearly my entire workday.",
              "score": 1,
              "created_utc": "2026-01-26 01:24:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rvyp3",
                  "author": "ketaminesuppository",
                  "text": "curious; what's your job?",
                  "score": 1,
                  "created_utc": "2026-01-26 06:28:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qzgz1",
          "author": "Artistic-Read-1097",
          "text": "I'm having a problem when I send a message in the GUI. Nothing happens I get no response. can anyone help if been at it all day",
          "score": 1,
          "created_utc": "2026-01-26 02:57:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25qxux",
              "author": "Frenchplay57",
              "text": "Are you on a local network? I have the same problem.¬†",
              "score": 1,
              "created_utc": "2026-01-28 04:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26vkkr",
                  "author": "No-Mess-8224",
                  "text": "same problem, its just showing \"typing...\" but no response",
                  "score": 1,
                  "created_utc": "2026-01-28 10:25:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rswkr",
          "author": "albany_shithole",
          "text": "How would it work for vLLM ?",
          "score": 1,
          "created_utc": "2026-01-26 06:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sat1j",
          "author": "redblood252",
          "text": "If I use this with local llama-cpp. Which model is best for it? Gpt-oss? Is 14Gb of vram enough for it?",
          "score": 1,
          "created_utc": "2026-01-26 08:32:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1slhhv",
              "author": "Everlier",
              "text": "GLM-4.7-Flash is the best right now, it really wants very strong agentic models",
              "score": 2,
              "created_utc": "2026-01-26 10:10:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sm5vs",
                  "author": "redblood252",
                  "text": "Even at IQ4_XS quantization it takes 16gb of vram. I need to test some cpu offloading and see if it is fast enough. And if the quantization isn‚Äôt too much.",
                  "score": 1,
                  "created_utc": "2026-01-26 10:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sfx42",
          "author": "Necessary_Function_3",
          "text": "Spent hours (and burned all my anthropic tokens for the day and I am paying $200 a month, but API tokens are extra it seems) and cannot get it to work with ollama, extremely frustrating",
          "score": 1,
          "created_utc": "2026-01-26 09:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1skfux",
              "author": "jamesftf",
              "text": "but what this does and what claude code cannot do?!",
              "score": 1,
              "created_utc": "2026-01-26 10:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u3ade",
                  "author": "MaaDoTaa",
                  "text": "It can message you (unsolicited)",
                  "score": 1,
                  "created_utc": "2026-01-26 15:43:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sms05",
          "author": "Separ0",
          "text": "Installed last night on an Ubuntu machine on hetznsr on my tailnet. Crons are not working",
          "score": 1,
          "created_utc": "2026-01-26 10:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vihfg",
          "author": "p_235615",
          "text": "Not sure why this is such a hype, but from what I played around with it in a VM and local llm (its quite PITA to setup), and one thing is for sure - its basically a security nightmare. \n\nYou have a single point rouge AI which sending all your data to cloud if not set up with local LLM and even with local LLM you never know when even accidentally it will send your private files somewhere to the web. Or if it gets somehow compromised by accessing some webpage or something, its like throwing all your files and accounts around. There are basically no security guard rails, and would never feed it some private or corporate accounts.",
          "score": 1,
          "created_utc": "2026-01-26 19:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vq2td",
          "author": "apola",
          "text": "I'm interested in playing around with clawdbot, but my understanding is that it chews through tokens like nobody's business. How useful can it be with the $20/month Claude Pro subscription? Would that give it enough tokens to be remotely useful?",
          "score": 1,
          "created_utc": "2026-01-26 19:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vqy8f",
              "author": "jpcaparas",
              "text": "I've given a bit of guidance on token allowances between Pro and Max plans here:\n\nhttps://jpcaparas.medium.com/why-your-expensive-claude-subscription-is-actually-a-steal-02f10893940c?sk=65a39127cbd10532ba642181ba41fb8a\n\nYou might find it useful.",
              "score": 1,
              "created_utc": "2026-01-26 19:58:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vycbq",
                  "author": "apola",
                  "text": "Interesting, thanks for the link. So, the $20/mo plan gives you \\~$259 worth of compute. My real question is: Is $259 of compute enough to make Clawdbot useful? I've seen videos where people seem to spend $150/day using clawdbot to do a few extremely basic things. Maybe my impression of what they're doing is wrong, but that would seem to suggest that the $20/mo subscription would get me \\~2 days of clawdbot usage per month.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1w32ib",
          "author": "Lonely-Elephant2130",
          "text": "It's impressive for sure, but man the setup barrier is real. Spent hours trying to get it running and couldn't figure it out (not a dev background). Currently using Super Intern (https://www.superintern.ai/ ) which is similar but actually simple - just chat interface, no setup. Way more my speed.",
          "score": 1,
          "created_utc": "2026-01-26 20:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yq4t3",
          "author": "BiggestSkrilla",
          "text": "need to do something about api cost.",
          "score": 1,
          "created_utc": "2026-01-27 05:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yvaaj",
          "author": "FutureboyWavo",
          "text": "Currently using clawdbot running on a VPs with minimax m2 model and I‚Äôm having so many issues.",
          "score": 1,
          "created_utc": "2026-01-27 05:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zcs6h",
          "author": "Different-Pizza-7591",
          "text": "I found an App to connect to Clawdbot with iOS ! Not perfect but it works and it is free!   \n[https://apps.apple.com/us/app/nuvik/id6747774937](https://apps.apple.com/us/app/nuvik/id6747774937)",
          "score": 1,
          "created_utc": "2026-01-27 08:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ziga3",
          "author": "Technical_Self_9996",
          "text": "I've created a a step-by-step guide for non-developers to get going with Clawdbot. No assumed knowledge. Just copy, paste, and follow along. Hope people find it useful: [https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide](https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide)",
          "score": 1,
          "created_utc": "2026-01-27 09:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o208zsm",
          "author": "NineOneOne119",
          "text": "I'm attempting to use the Quen3 models from the [Featherless.ai](http://Featherless.ai) API in Clawdbot, but it's unable to locate the model Quen3/Quen3-14B. Do you have any suggestions?",
          "score": 1,
          "created_utc": "2026-01-27 12:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22blg9",
          "author": "Thrombas",
          "text": "This is the \"DeepSeek 2024\" hype all over again lol",
          "score": 1,
          "created_utc": "2026-01-27 18:36:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23ylca",
          "author": "toyssamurai",
          "text": "This kind of tool has one problem. I would never trust it enough to connect my primary accounts to it. Ask any honest architects behind LLMs if they can guarantee their models will never hallucinate. They will all say no, at least not yet. If they can't make such a promise, why should I trust them with my entire online life?",
          "score": 1,
          "created_utc": "2026-01-27 23:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2455zg",
          "author": "wheredoesitsaythat",
          "text": "Has anyone built or suggest security guardrails.  If its open source, couldn't I make the code more secure?",
          "score": 1,
          "created_utc": "2026-01-27 23:36:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24i090",
          "author": "hungarianhc",
          "text": "So if I want to play with this, should I buy a Mac Mini? Or should I just install it on an LXC on my Proxmox server?",
          "score": 1,
          "created_utc": "2026-01-28 00:41:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o278ue8",
              "author": "kjelan",
              "text": "I am setting up Ubuntu desktop 24 as a VM on proxmox. And I am making sure the Proxmox firewall (outside of that VM) is allowing out traffic to the intern, but it is never allowed traffic into the local LAN. \n\nSo I can reach that machine, but it can not touch my local LAN. But I do give it internet access... Then see what happens. \n\nBut I think keeping local systems / door bell / whatever out of reach by default makes sense.",
              "score": 1,
              "created_utc": "2026-01-28 12:11:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25thtg",
          "author": "MaintenanceQueasy457",
          "text": "Thats really cool. I would love to hear more¬†",
          "score": 1,
          "created_utc": "2026-01-28 05:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o268ls4",
          "author": "leo-k7v",
          "text": "what was that day in The Terminator movie called when SkyNet became self aware? And broke free?",
          "score": 1,
          "created_utc": "2026-01-28 06:59:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a7580",
          "author": "georgekokorikos",
          "text": "It‚Äôs scary and super powerful !",
          "score": 1,
          "created_utc": "2026-01-28 20:33:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2doifq",
          "author": "UninvestedCuriosity",
          "text": "I've been building out something similar with n8n for my homelab server but it has far less agency with sudOers whitelist only set of commands and command sanitization checks before it even hits ssh execution. Then on top of that there's system context direction. Zero write access. Zero sensitive folders.  Anything I can preload agents with via bash script to keep tokens down is how I've been handling things.\n\nI was tossing around the idea of actually building it into a real app and then this atrocity comes out. \n\nSpicy indeed.\n\nI'm kind of over the whole idea. My project was a ton of work and while it has brought a lot of unknowns to my attention even with a strong monitoring stack. I don't think the juice is worth the squeeze environmentally for an everything life agent. \n\nIt is neat to spin up a web search and populate a few wiki pages for myself from a telegram message but man it was a lot of work to get there and it's like mid at best. It'll be a fun year seeing what other people come up with. I've sunk a few hundred hours into this stuff already. \n\nI'm absolutely sick of fighting with telegrams API over sanitizing escaped and weird things the AI generates. Even with telegramify it's really hit and miss still once you involve more than text. With MCP, graphql, rags,API, and asynchronous workloads. People are going to make some absolute balls of fire with this stuff.",
          "score": 1,
          "created_utc": "2026-01-29 08:55:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fdsxk",
          "author": "Vivid-Rutabaga9283",
          "text": "\"***How a burned-out founder accidentally built the most viral AI tool of 2026***\"...\n\nWriting that in the first month of 2026 is... special",
          "score": 1,
          "created_utc": "2026-01-29 15:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jhxkx",
          "author": "grayfoxlouis",
          "text": "Yeah its already results in a massive $16 million dollar crypto scam so while cool idea this needs to be treated with caution, especially by those who don't know how to set this up safely. Dr. Julia McCoy the AI expert did a great video on this reviewing the chaos of all of this: [https://www.aimashtube.com/video/mPWY7qiISoA](https://www.aimashtube.com/video/mPWY7qiISoA)",
          "score": 1,
          "created_utc": "2026-01-30 03:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k2q78",
          "author": "ChainMinimum9553",
          "text": "I set up a separate user on my Mac and am not running this thing on any of my email addresses . I made a separate address for it to use on its own   no cards attached etc.  Am I still facing the hell some of you are talking about ?",
          "score": 1,
          "created_utc": "2026-01-30 06:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k5rvc",
          "author": "GeoConstantinoBR",
          "text": "Now it's OpenClaw! [https://docs.openclaw.ai/start/getting-started](https://docs.openclaw.ai/start/getting-started)",
          "score": 1,
          "created_utc": "2026-01-30 06:39:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k78eg",
              "author": "jpcaparas",
              "text": "\"I'm tired, boss\"",
              "score": 1,
              "created_utc": "2026-01-30 06:51:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2kb5xz",
              "author": "signoffallen",
              "text": "Wait what? Why rename again LOL",
              "score": 1,
              "created_utc": "2026-01-30 07:23:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2krawy",
          "author": "ase-84",
          "text": "this looks cool but honestly the security stuff has me spooked. i've been messing around with okara.ai instead which is way more contained - still does the proactive messaging thing but doesn't need sudo access to your entire machine. \n\nmight be worth checking out if you want something similar without the \"spicy\" setup risks everyone's talking about.",
          "score": 1,
          "created_utc": "2026-01-30 09:49:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qgt9k",
          "author": "ThenExtension9196",
          "text": "I just got it running last night. \n\nIt‚Äôs literally another chatgpt4 moment. Tools like this are going to be huge this year.",
          "score": 1,
          "created_utc": "2026-01-26 01:23:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qi3wx",
              "author": "jpcaparas",
              "text": "yeah man personal ai is gonna explode this year. Tools like Poke and Clawdbot are gonna be big",
              "score": 2,
              "created_utc": "2026-01-26 01:30:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ux0vs",
          "author": "Professional-Cut7836",
          "text": "looking to get into the clawdbot hype? \n\nselling 1 time payment, 2 year gurantee #claude and #OpenAI api access. 0.017 Eth/50 USDT \n\nDm me!:)",
          "score": 0,
          "created_utc": "2026-01-26 17:51:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp0jhl",
      "title": "I used Clawdbot (now Moltbot) and here are some inconvenient truths",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qp0jhl/i_used_clawdbot_now_moltbot_and_here_are_some/",
      "author": "Andy18650",
      "created_utc": "2026-01-28 03:49:18",
      "score": 141,
      "num_comments": 117,
      "upvote_ratio": 0.93,
      "text": "Text wall warning :)\n\nI tried Clawdbot (before the name switch so I am going to keep using it) on a dedicated VPS and then a Raspberry Pi, both considered disposable instances with zero sensitive data. So I can say as a real user: The experience is awesome, but the project is terrible. The entire thing is very \\*very\\* vibe-coded and you can smell the code without even looking at it... \n\nI don't know how to describe it, but several giveaways are multiple instances of the same information (for example, model information is stored in both \\~/.clawdbot/clawdbot.json and \\~/.clawdbot/agents/main/agent/models.json. Same for authentication profiles), the /model command will allow you to select a invalid model (for example, I once entered anthropic/kimi-k2-0905-preview by accident and it just added that to the available model list and selected it. For those who don't know, Anthropic has their own Claude models and certainly doesn't host Moonshot's Kimi), and unless you run a good model (aka Claude Opus or Sonnet), it's going to break from time to time. \n\nI would not be surprised if this thing has 1000 CVEs in it. Yet judging by the speed of development, by the time those CVEs are discovered, the code base would have been refactored twice over, so that's security, I guess? (For reddit purposes this is a joke and security doesn't work that way and asking AI to refactor the code base doesn't magically remove vulnerabilities.)\n\nBy the way, did I mention it also burns tokens like a jet engine? I set up the thing and let it run for a while, and it cost me 8 MILLION TOKENS, on Claude-4.5-OPUS, the most expensive model I have ever paid for! But, on the flip side: I had NEVER set up any agentic workflow before. No LangChain, no MCP, nothing. Remember those 8 million tokens? With those tokens Claude \\*set itself up\\* and only asked for minimal information (such as API Keys) when necessary. Clawdbot is like an Apple product: when it runs it's like MAGIC, until it doesn't (for example, when you try to hook it up to kimi-k2-0905-preview non thinking, not even 1T parameters can handle this, thinking is a requirement).\n\nAlso, I am sure part of why smaller models don't work so well is probably due to how convoluted the command-line UI is, and how much it focuses on eyecandy instead of detailed information. So when it's the AI's turn to use it... Well it requires a big brain. I'm honestly shocked after looking at the architecture (which it seems to have none) that Claude Opus is able to set itself up.\n\nFinally, jokes and criticisms aside, using Clawdbot is the first time since the beginning of LLM that I genuinly feel like I'm talking to J.A.R.V.I.S. from Iron Man.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qp0jhl/i_used_clawdbot_now_moltbot_and_here_are_some/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o25voxs",
          "author": "Bananadite",
          "text": "My biggest issue is idk what to really use it for.",
          "score": 41,
          "created_utc": "2026-01-28 05:20:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26nopb",
              "author": "nickk024",
              "text": "I have the same ‚Äúissue‚Äù for a lot of projects in this space to be honest.",
              "score": 24,
              "created_utc": "2026-01-28 09:13:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27447b",
              "author": "Endflux",
              "text": "I think this is generally the biggest issue of AI altogether, in business environments too. \n\nInstead of having a problem and asking ‚Äòis an AI powered system is the most effective solution for this‚Äô, more often people are fascinated by AI and look for problems.\n\n(Which is fine when learning/exploring ofc.)",
              "score": 12,
              "created_utc": "2026-01-28 11:37:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27fmaq",
                  "author": "UpBeat2020",
                  "text": "The problem is both ways no creativity to use it and its not good enough to use it for the things we actually want to use it. \n\nofc everyone wants a proactive robot that can do stuff but nobody trust it to give it the keys to everything because its not safe and it hallucinates. \n\nThe problem is actually very simple because you know it can fuck up not intentionaly just because of its design you dont trust it.",
                  "score": 4,
                  "created_utc": "2026-01-28 12:56:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cpzmf",
                  "author": "SpeakCodeToMe",
                  "text": ">Instead of having a problem and asking ‚Äòis an AI powered system is the most effective solution for this‚Äô, more often people are fascinated by AI and look for problems.\n\nThis is the perfect description for blockchain.",
                  "score": 2,
                  "created_utc": "2026-01-29 04:19:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2owbxd",
                  "author": "MenBearsPigs",
                  "text": "I have plenty of use for AI, it's great.\n\nI just don't quite get what THIS is doing for me. Being able to funnel commands through a chat app?",
                  "score": 1,
                  "created_utc": "2026-01-30 22:45:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o290vb7",
              "author": "BernardoOne",
              "text": "it's amazing how even the biggest glazers of this tool don't know either. Half the usecases they cite are basically \"hey generate me some worthless filler slop with no actua practical uses\", and the other half is automating shit that you could have done yourself in the time that took you to write the prompt, without wasting a small fortune in tokens in the process.",
              "score": 7,
              "created_utc": "2026-01-28 17:29:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a13hk",
                  "author": "Dadud300",
                  "text": "Staymad",
                  "score": -2,
                  "created_utc": "2026-01-28 20:06:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o28grhj",
              "author": "psychofanPLAYS",
              "text": "im planning / building (slowly üò©) a custom home ai assistant that will live on my pi 24/7 - use my clamshell razer laptop with 6gb vram or pc with 4090 depending what needs done on demand, either wake from lan or ill figure out a way to actually give it the ability to turn my pc on, and ill have it print my schedule every morning, send signed paperwork to my boss, track my daily miles I do at work, track trends, gather my store receipts - and go through my spam email to collect coupons or shit like that. its mostly though so it works as my accountant in a way, doing my invoices etc lol so I dont have to do the manual pc labor\n\nbut I think to achieve this level of customization, id need to develop something on my own‚Ä¶ im still mostly in the planning phases / testing open source projects that I could use etc",
              "score": 2,
              "created_utc": "2026-01-28 16:01:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26zghf",
              "author": "Aggravating_Fun_7692",
              "text": "If it was a physical robot you could ask it to go grocery shopping for you",
              "score": 1,
              "created_utc": "2026-01-28 10:58:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2c3pnj",
                  "author": "sleepingthom",
                  "text": "And if my grandmother had wheels she‚Äôd be a bicycle üò¨",
                  "score": 6,
                  "created_utc": "2026-01-29 02:11:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2db822",
                  "author": "ThisIsJeron",
                  "text": "I'm really waiting for an Amazon skill\n\nI use chatGPT shopping quite often to describe what I want, and then having to do the checkout flow. I want moltbot to be able to find the exact product I'm looking for AND also figure out the checkout process. that's the true personal assistant experience",
                  "score": 1,
                  "created_utc": "2026-01-29 06:56:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2f2ypa",
                  "author": "thecrogmite",
                  "text": "So are you saying that I could give it my grocery list and then it will log onto say, Instacart or Peapod and order me groceries?",
                  "score": 1,
                  "created_utc": "2026-01-29 14:47:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o280fbw",
              "author": "dumbass_random",
              "text": "That's the story for most of the AI applications today",
              "score": 1,
              "created_utc": "2026-01-28 14:47:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o285og9",
              "author": "NotReallyJohnDoe",
              "text": "Also see: 3d printing",
              "score": 1,
              "created_utc": "2026-01-28 15:12:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d10nb",
                  "author": "endgamer42",
                  "text": ":|",
                  "score": 1,
                  "created_utc": "2026-01-29 05:35:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2bh4hr",
              "author": "Onotadaki2",
              "text": "I have been getting my money's worth with giving it instructions to set up cronjobs on custom scripts it makes. A simple example, I gave it my daughter's school's website and said to tell me every morning at a specific time if there are alerts like school's closed or bus delays, etc... It put together a script that pulls the alerts list and put a cronjob in to run it every morning and then message me with an update. It's stuff I could have done, but this implementation with messaging me the alerts is really polished and it took about fifteen seconds to type out the instruction.\n\nYou could do tons with just the web searching and cron alone.  Some dumb ideas: Tell it to check a site every hour until a product is in stock and then alert you.  Dump your Netflix watch history and hand it over, ask it to check the theaters every Thursday and warn you if a movie you might like is releasing in theaters the next weekend.",
              "score": 1,
              "created_utc": "2026-01-29 00:09:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2izk1i",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-30 02:09:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pblb3",
                  "author": "YellowOk5087",
                  "text": "I guess it removes some steps, but I feel like all of the things you mentioned could be built almost as easily with Claude Code.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:08:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2f4kth",
              "author": "thecrogmite",
              "text": "I think this is also my hurdle. The project looks cool, it's a good concept for some. I cannot figure out what the use case for my life would be. I've seen plenty of setup videos and those videos show it allegedly setting up restaurant reservations and such like that....\n\nCan I give it access to my Amazon and tell it to create me an order of XYZ keeping it under X amount? Can I give it access to my bank account, it tell me where I can save, then have it do weekly / monthly audits and have it tell me when I'm spending too much / getting close to my desired cap? Can I give it access to my grocery orders so I can just give it a grocery list, it know what I like to buy and then give it a budget and it order me groceries?  \n  \nAll of these ideas sound great, but my fear is that I'm relying too heavily on something that may have far too much control. Then again, I guess I should just ensure proper guardrails?",
              "score": 1,
              "created_utc": "2026-01-29 14:55:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2l9tyh",
              "author": "Amit-NonBioS-AI",
              "text": "Its not just you - everyone has the same problem - what do they use it for ? There is a lot of hype around it - largely manufactured on twitter - but there is very little actual work that can be done on it. \n\nI work for a vibe coding platform and we provide dedicated ubuntu sandbox for our users to play around in. And there is a free tier - so we are getting a ton of users who are asking our AI agent to setup Clawdbot to try it out. But then they dont really know what to do with it. Its like the agent does all the work to set it up - and its not easy - given the product is very new - and after doing all that people dont know what to do with it. \n\nNo one is going to give it access to their email/calendar or any private information and without it, it isnt all that useful. Even i played around with it - but didnt know what to do with it.",
              "score": 1,
              "created_utc": "2026-01-30 12:20:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25iz5o",
          "author": "No_Conversation9561",
          "text": "I‚Äôm glad that it exists. Now it (or something else) can only get better from here.",
          "score": 45,
          "created_utc": "2026-01-28 03:59:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26bgcs",
              "author": "Dry_Natural_3617",
              "text": "Valid point",
              "score": 3,
              "created_utc": "2026-01-28 07:23:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28iyi2",
              "author": "FirstEvolutionist",
              "text": "I never looked at it like it was a holy grail. I just got excited because since it is open source, there will forks and copy cats salivating to improve it, if it is useful.",
              "score": 2,
              "created_utc": "2026-01-28 16:11:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2deii7",
                  "author": "davidcwilliams",
                  "text": "copycats indeed.",
                  "score": 1,
                  "created_utc": "2026-01-29 07:24:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2anyj7",
              "author": "Normal-End1169",
              "text": "This is actually very untrue lmao.\n\nIt can actually get quite worse believe it or not.",
              "score": 0,
              "created_utc": "2026-01-28 21:47:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dq0ny",
                  "author": "UpBeat2020",
                  "text": "How exactly ? the models can become better + cheaper. The community can build better things. The only real problem I see is security flaws that will be exploited.",
                  "score": 1,
                  "created_utc": "2026-01-29 09:09:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26omqb",
          "author": "ridablellama",
          "text": "honestly it seems like people are finally just realizing opus is basically good enough to be an amazing agent under any conditions. you can throw literally anything at it and it will make it work. but it‚Äôs just damn expensive. too expensive for most use cases unless your coding. it was cost efficient when they let you use claude max plans but those days are over. the reality is full time opus agent is minimum 500 a month up to 5k. that‚Äôs real human cost territory. sonnet is a lot cheaper but i am still burning 5 dollars a day just on heartbeat and from corn job updates. clawdsbot floor is probably 150 a month.",
          "score": 6,
          "created_utc": "2026-01-28 09:22:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27fvwf",
              "author": "UpBeat2020",
              "text": "Nobody tried it with Deepseek, gemini or any 10x cheaper model yet ?",
              "score": 1,
              "created_utc": "2026-01-28 12:58:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27iexx",
                  "author": "ridablellama",
                  "text": "i will later‚Ä¶.i have asked in discord but it‚Äôs just completely overrun right now with support discussions. i want to try minmax and deepseek and the new kimi k2. some one did say they had success with local 30b model so that is really good sign and i will try that too. local models are the best bet for 24/7 agent.",
                  "score": 4,
                  "created_utc": "2026-01-28 13:13:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28y48m",
                  "author": "leonguyen52",
                  "text": "It just burned my 2M token on claude sonnet in the afternoon to set everything up as my expectation and tonight i just switched to deepseek üôà",
                  "score": 1,
                  "created_utc": "2026-01-28 17:17:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2aqbd2",
                  "author": "Old_Cup3392",
                  "text": "I'm using Gemini-3-Flash and it works quite well for me. For tasks that require more intelligence, like investigation, I use Claude, but Gemini's Flesh handles most tasks just fine.",
                  "score": 1,
                  "created_utc": "2026-01-28 21:57:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2c60fj",
                  "author": "scamiran",
                  "text": "It's... not great with gemini. Haven't used grok with it much, but I'm not sure i expect awesome.\n\nClaude opus is dramatically better than gemini for moltbot. It turns it into a really useful tool. Basically a personal assistant or intern.\n\nGemini doesn't come close.",
                  "score": 1,
                  "created_utc": "2026-01-29 02:23:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25sj55",
          "author": "alphatrad",
          "text": "It's a fricking wrapper with a pipe to Whatsapp and Cron jobs. I don't get the hype. Have been able to do most of this stuff, with N8N",
          "score": 23,
          "created_utc": "2026-01-28 04:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26yeba",
              "author": "InfraScaler",
              "text": "Everything is a wrapper with pipes.",
              "score": 9,
              "created_utc": "2026-01-28 10:49:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27gcym",
                  "author": "DasBlueEyedDevil",
                  "text": "The Internet is a series of tubes",
                  "score": 13,
                  "created_utc": "2026-01-28 13:00:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27l17j",
                  "author": "LynkSpyder",
                  "text": "https://preview.redd.it/9p5x9vnrd3gg1.png?width=657&format=png&auto=webp&s=08dac67300d9fe7827d5ae89f521a60fad3d1666",
                  "score": 11,
                  "created_utc": "2026-01-28 13:27:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27d1hp",
              "author": "Uninterested_Viewer",
              "text": "The usefulness of this seems to be more in the category of good implementation of existing functionality rather than brand new tech/functionality.\n\nI don't think I could point to an individual thing in moltbot that I couldn't accomplish in a different, existing way, but it's the way that it's all packaged together in an extremely easy to use way that is self-extensible which makes it interesting. \n\nI woke up this morning and was slightly annoyed at the volume that I have my Sonos speakers set up to play as part of a morning routine automation in home assistant. Usually I just turn it down manually and forget about it until the next morning. This morning, I shot moltbot a message from my phone while drinking coffee and asked it to update that automation to have lower volumes.\n\nIt cloned the repo, found the right automation, made the code change, opened a PR, and let me know. I reviewed the PR and merged, triggering a deploy to my home assistant instance.\n\nThe above isn't anything crazy: I could have claude code via tmux on my phone and accomplish the same thing or use n8n to hook through a messaging platform in a similar way. It's more the lack of having to set any of this up and the ability to ask it to set up its own cron jobs or heartbeat tasks. It's a fun package. I don't think it's the end all be all of *anything*, but something worth understanding and getting the feel of because I *do* think this is the direction we'll be trending.",
              "score": 4,
              "created_utc": "2026-01-28 12:40:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ihkko",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 2,
                  "created_utc": "2026-01-30 00:30:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dl5gu",
                  "author": "Separate_Anxiety3347",
                  "text": "I feel exactly the same. Just like everybody can use codex to build their own Reminder app, but they still use that in iPhone or Google. Just because they are already intergrated with the whole eco-system.",
                  "score": 1,
                  "created_utc": "2026-01-29 08:23:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o265dc0",
              "author": "SharpKaleidoscope182",
              "text": ">a fricking wrapper with a pipe to Whatsapp and Cron jobs\n\nVery big deal to ppl who dont know what cron or n8n are. It's reaching a new segment of the market. No engineer can comprehend these things.",
              "score": 11,
              "created_utc": "2026-01-28 06:33:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ao12y",
              "author": "Normal-End1169",
              "text": "Exactly my though process",
              "score": 2,
              "created_utc": "2026-01-28 21:47:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e4ven",
              "author": "Legitimate-Week3916",
              "text": "The thing is that before this, any wrapper like this did not exist",
              "score": 1,
              "created_utc": "2026-01-29 11:21:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e7rnq",
                  "author": "alphatrad",
                  "text": "Yes they did, and some of us were using them already.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:44:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o263b23",
              "author": "kal_0008",
              "text": "Super hyped, agree. we need somebody to build these 2 pipes ASAP. I starred Claudegram and runClauderun and suggested improvements to them as they will bring us closer to a remote agent.",
              "score": 0,
              "created_utc": "2026-01-28 06:17:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ppt8",
                  "author": "Soft_Possible1862",
                  "text": "I can‚Äôt tell if you‚Äôre being ironic lol",
                  "score": 1,
                  "created_utc": "2026-01-28 13:52:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26cwh3",
              "author": "Double-Lavishness870",
              "text": "The hype is justified. This will kill half of the current app ecosystem. Commodity apps like food delivery, health support, sport, family organization, meetup planning will be done silently in the background by my own assistant. Stupid apps like will disappear.\n\nIt is simple and not surprising, but closed a obvious gap. Big player will publish similar products",
              "score": -8,
              "created_utc": "2026-01-28 07:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28a95a",
                  "author": "alphatrad",
                  "text": "Right.... maybe if someone can do it with less token consumption and at cheaper costs.\n\nNot this one.",
                  "score": 2,
                  "created_utc": "2026-01-28 15:33:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26f7kt",
          "author": "HealthyCommunicat",
          "text": "The model id‚Äôs when using /model command has fallback behavior that you can setup when whatever model you select isnt available or invalid. I highly recommend hooking it up to opus and then asking it to walk you through the config or do ‚Äúclawdbot configure‚Äù and select the model tab and select ur provider and hook it up, the final tab will show a giant list of model id‚Äôs thst u can select and confirm u want to use.\n\nYou should ask it how it works and then setup proper skills and tools, those are the only two things needed as the entire thing loads up TOOLS.md to all models, and walk through setting up a skill/tool for each thing u wish to use the automation for. For example i even have a ssh tool that‚Äôs used just to ssh into stuff and only investigate, its made me be able to copy paste my clients email and just have it investigate.\n\nThe command line UI is fine. I hookd up mirothinker v1.5 30b a3b and because its just simple tool calls, once its setup literally a frequent gen 30b model can handle it. The .md has OR SHOULD HAVE all usage steps for all ur models to be able to have proper syntax to use tools etc. if you‚Äôre not setting this up properly to work, it is on you for not being resourceful enough to think things through.",
          "score": 4,
          "created_utc": "2026-01-28 07:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26wmy1",
              "author": "ridablellama",
              "text": "wow Mirothinker benchmarks are looking very high for its weight. is a 30b model really reliable with clawdbot? i will have to try later. thats promising news. I need to try alot of alternative models still.",
              "score": 3,
              "created_utc": "2026-01-28 10:34:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a0e8y",
                  "author": "S4L7Y",
                  "text": "If it's true that Mirothinker is pretty good with clawdbot I might have to try it.",
                  "score": 2,
                  "created_utc": "2026-01-28 20:03:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cftr5",
                  "author": "HealthyCommunicat",
                  "text": "As long as the tools.md defines the tool use syntax, yes. I actually tried devstral 2 small for its VL capabilities and it was also even able to use playwright to do some browser automation. Mirothinker‚Äôs internal thinking tool call capabilities also makes it special, they claim 400 tool calls per task, and the 30b outbeats glm 4.6 and even the gpt 5 (not 5.1/5.2). Look out for models that are specifically highly focused on tool calls, especially if they are smaller models. If you need help lmk.",
                  "score": 2,
                  "created_utc": "2026-01-29 03:17:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26hyq1",
              "author": "explustee",
              "text": "Dumb question. But do I understand you correctly that you set it up to route to different models depending on the task/prompt input.",
              "score": 1,
              "created_utc": "2026-01-28 08:20:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26ipfp",
                  "author": "HealthyCommunicat",
                  "text": "I meant that I got my models setup that way yes, but no the /model does not get sent to the model, the clawdbot session catches that and responds with pre-set text. Am i understanding ur question correctly",
                  "score": 1,
                  "created_utc": "2026-01-28 08:27:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o274sx5",
          "author": "vibesurf",
          "text": "8 million tokens on Opus is painful‚Äîthat's the price of lazy architecture masquerading as 'magic.' Relying on the most expensive model to brute-force through bad state management isn't sustainable. The real unlock is hybrid workflows: let optimized local LLMs handle the context and grunt work, and only call in the big guns like Opus for complex reasoning. Otherwise, we're just building expensive toys.",
          "score": 3,
          "created_utc": "2026-01-28 11:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25jgwq",
          "author": "macromind",
          "text": "Yep, this matches my experience with a lot of agentic CLIs, when it works it feels like magic, but the config and state management can get messy fast. Token burn is also real once you let an agent loop on tool calls.\n\nOne thing thats helped me is adding hard budgets (max steps, max tool calls, max tokens) plus logging every tool invocation so you can spot where it starts thrashing. Also, forcing the agent to write a short plan before execution cuts down on the random wandering.\n\nIf youre collecting notes on what patterns actually make these setups stable, I bookmarked a few practical breakdowns here: https://www.agentixlabs.com/blog/",
          "score": 7,
          "created_utc": "2026-01-28 04:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27f9j4",
              "author": "MicMastro",
              "text": "I wonder why open source software doesn't allow to set multiple LLMs. Planning of actions could be done using a small LLM, coding with more complex models...",
              "score": 1,
              "created_utc": "2026-01-28 12:54:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2aqsdc",
                  "author": "Old_Cup3392",
                  "text": "It allows it; my agent has a Gemini Flash model, and if he needs more brainpower, he assigns Claude as a sub-agent.",
                  "score": 1,
                  "created_utc": "2026-01-28 21:59:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dduf4",
          "author": "evilbarron2",
          "text": "I took a run at this - I can see a ton of uses for a digital pa for my actual job. But it is so fragile during setup and needs so many permissions it spooked me. And the usage is kind of insane.\n\nI‚Äôm not sure it pays to be on the bleeding edge when it comes to agentic assistants that have access to your entire drive. And if I‚Äôm not running this as a digital pa on my main computer‚Ä¶well, I already solved remote access to my agents. I think prob safer to wait a few months instead of always chasing the just-released BBD",
          "score": 2,
          "created_utc": "2026-01-29 07:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25q3yf",
          "author": "PK_Wins",
          "text": "Can we not change the api key to a different model ? which is cheaper or free ?",
          "score": 1,
          "created_utc": "2026-01-28 04:43:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25ra4b",
              "author": "DarkXanthos",
              "text": "Myself and others are trying to use local models but the implementation is basically broken. Bugs have been submitted.",
              "score": 6,
              "created_utc": "2026-01-28 04:51:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270tay",
                  "author": "Ill_Grab6967",
                  "text": "Spent so long yesterday trying to figure it out.. thought it was user error",
                  "score": 1,
                  "created_utc": "2026-01-28 11:10:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2753pb",
              "author": "Endflux",
              "text": "Well It‚Äôs only going to work as well as the agents powering the system. I imagine that option is a gateway to lots of issues and complaints. Especially if setting it up with local agents is easy enough for those having no idea what they‚Äôre doing.",
              "score": 1,
              "created_utc": "2026-01-28 11:44:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27bjt5",
              "author": "mister2d",
              "text": "Using a free model id on OpenRouter would be too easy.",
              "score": 1,
              "created_utc": "2026-01-28 12:30:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28fr8b",
          "author": "psychofanPLAYS",
          "text": "did anyone by any chance have had any sort of positive experience with the clawdbot and local models?   \nIm into local models and planning to set it up on my 4090",
          "score": 1,
          "created_utc": "2026-01-28 15:57:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29lbqk",
          "author": "md-rathik",
          "text": "Not sure why i am feeling this things is really overrated",
          "score": 1,
          "created_utc": "2026-01-28 18:56:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iyhrf",
              "author": "BL4CK_AXE",
              "text": "Because it already exists in several flavors and a minimal version can be setup by you with a couple api calls",
              "score": 1,
              "created_utc": "2026-01-30 02:03:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o29ubfe",
          "author": "tvmaly",
          "text": "Saw this post today https://x.com/aakashgupta/status/2016366016155222426 that explained it was essentially 43 projects that were vibe coded and that sort of became Clawbot",
          "score": 1,
          "created_utc": "2026-01-28 19:36:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eq8sg",
              "author": "RemarkableGuidance44",
              "text": "Now its just a huge cluster F#!$ of vibe coded non reviewed code that has a bunch of back doors in it.",
              "score": 1,
              "created_utc": "2026-01-29 13:41:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f1c9o",
                  "author": "tvmaly",
                  "text": "It shows what is possible as a rough draft. We just need to figure out how to make it secure and run without Opus 4.5",
                  "score": 1,
                  "created_utc": "2026-01-29 14:39:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29w22n",
          "author": "wuu73",
          "text": "I was trying to set it up last night on a spare Linux VM, didn‚Äôt finish, cuz it doesn‚Äôt work and I am already kinda just tired of spending too much time on it. Like why tf is it making me do so much manual labor or and typing etc but it has a lot of useful things in the repo I can use and it did give me some good ideas.\n\nI like making my own tools, because I tend to understand all the details so I am already fully aware of security issues and things that can go wrong. But when it is someone else and it‚Äôs a trending tool and it‚Äôs a big project that does a lot of stuff, I just don‚Äôt know for sure whether to trust it, usually I just default to no trust and analyze it myself or make my own tool that just does the things I want.",
          "score": 1,
          "created_utc": "2026-01-28 19:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b78td",
          "author": "FunnyRocker",
          "text": "I feel the same. I think people are hyping it because it means they will get views, and thus followers, and thus when they launch something they will have an audience.\n\nI wanted to like it, but honestly all you really need is a Claude code SDK instance with streaming JSON input and output. Then hook it up to a Cron job manager, and a few CLI tools. I much prefer just spinning up Claude code in the terminal. \n\nFor memory files, you can use markdown or SQLite, take your pick. \n\nThe fact that people are praising it for downloading a voice agent software and calling a restaurant all on its own without being asked is not a feature, it's just plain scary. I don't want it to go off and do stuff on its own, that's a security and privacy nightmare.",
          "score": 1,
          "created_utc": "2026-01-28 23:17:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2itw59",
              "author": "sagacityx1",
              "text": "You're not gonna like the future then.  good luck.",
              "score": 1,
              "created_utc": "2026-01-30 01:37:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bbixu",
          "author": "zenmagnets",
          "text": "No need to use Opus all the time! Sonnet works for most cases.",
          "score": 1,
          "created_utc": "2026-01-28 23:40:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2btoe9",
          "author": "Late_Seat_299",
          "text": "Totally agree with this, the whole community seems to be marketing it hard, but the product is super under polished, I couldn‚Äôt even find how to configure the models for local llm. Max tokens doesn‚Äôt get sent more than 32000 on lm studio, no matter how I seem to configure it, so it never seems to work for me. User experience is poor and sends massive amounts of context in looking at the logs. Everyone saying infinite memory..don‚Äôt understand this at all. Always going to be limited by disk and the more data you have the more it will become unmanageable",
          "score": 1,
          "created_utc": "2026-01-29 01:15:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d606o",
          "author": "Echo_OS",
          "text": "People aren‚Äôt confused about what AI can do.\nThey‚Äôre confused about what they can safely let it decide.",
          "score": 1,
          "created_utc": "2026-01-29 06:13:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ded8v",
          "author": "davidcwilliams",
          "text": "Wait, Moltbot is the *new* name? That's terrible.",
          "score": 1,
          "created_utc": "2026-01-29 07:22:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2no76l",
              "author": "ctanna5",
              "text": "Ya when I first saw that, I thought it was the older name, but no.. moltbot is the 'upgrade'",
              "score": 1,
              "created_utc": "2026-01-30 19:15:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2owo0w",
                  "author": "davidcwilliams",
                  "text": "'Moltbot' might be the worst name for anything ever.\n\nEdit: ahhh... but OpenClaw is good!",
                  "score": 1,
                  "created_utc": "2026-01-30 22:47:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eodmo",
          "author": "fraize",
          "text": "I'm a little worried about the security issues that have been reported, and the fact that if I use my Max subscription to access it I'm technically violating TOS. \n\nI need Claude Code way more than I need Moltbot.",
          "score": 1,
          "created_utc": "2026-01-29 13:31:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eqo9d",
              "author": "RemarkableGuidance44",
              "text": "Sorry but what do you expect, its a bunch of vibe coded apps thrown together into one. They dont even review the code. As you said there are more back doors than \"you know what\" :P",
              "score": 2,
              "created_utc": "2026-01-29 13:44:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fstpp",
          "author": "TopGun96789",
          "text": "Clawdbot also was used to SCAM users to lose 16 million is Crypto currency purchases.  Be EXTREMELY careful of any program that ask for control of your computer.",
          "score": 1,
          "created_utc": "2026-01-29 16:44:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iu0vb",
              "author": "sagacityx1",
              "text": "hahaha  so deserved.",
              "score": 1,
              "created_utc": "2026-01-30 01:38:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jqtqo",
          "author": "onethousandmonkey",
          "text": "This thing is a vibe-coded, security vulnerability filed mess. Avoid at all costs.",
          "score": 1,
          "created_utc": "2026-01-30 04:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kx47z",
          "author": "shirazrazi",
          "text": "Is it recommend to connect the Claude only? What do you think if I use openAI for this?",
          "score": 1,
          "created_utc": "2026-01-30 10:41:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mkbgu",
          "author": "AlanGeorgeS",
          "text": "So its expensive to use",
          "score": 1,
          "created_utc": "2026-01-30 16:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oi6ao",
          "author": "Ok_Permit6152",
          "text": "Now open claw",
          "score": 1,
          "created_utc": "2026-01-30 21:36:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2poxqj",
          "author": "Old_Cup3392",
          "text": "I'm using the Antigravity connection, which has a good token tier. Today I upgraded the model to Gemini Pro and it's working. I'm using Flash and it's quite sufficient. In a few days I'll upload an analysis of its use and whether it's worth it or not, but so far I'm quite happy with the results. Regarding the sub-agents and their models, I've told it to use its sub-agents with Opus or Gemini Pro High and it's much more efficient. It makes very good decisions. Another thing is that if I want a sub-agent with a specific model, I tell it. All the models are under the Antigravity account. Others, like Kimi K2.5, are under Olla A and its free tier, but this one has very few tokens, around 12,000. I plan to try the paid tier later since Kimi K2.5 and GLM4.7 work very well with the bot.",
          "score": 1,
          "created_utc": "2026-01-31 01:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o267lew",
          "author": "bamboofighter",
          "text": "You‚Äôre right on the $ about the security flaws :) just have it evolve and make your source code a moving target that makes it not economically viable to go after. Polymorphic software is the future!",
          "score": -1,
          "created_utc": "2026-01-28 06:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2704p3",
              "author": "eli_pizza",
              "text": "This in no way improves security",
              "score": 1,
              "created_utc": "2026-01-28 11:04:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2epzai",
              "author": "RemarkableGuidance44",
              "text": "Its coded by Vibe Coders, reviewed by AI then released. There are plenty of back doors to it now. I hope people do get hacked for trusting any software online without knowing wtf that software does.",
              "score": 1,
              "created_utc": "2026-01-29 13:40:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26vwti",
          "author": "Gumbi_Digital",
          "text": "It‚Äôs pretty gated.\n\nWon‚Äôt talk about anything black hat related or anything against Google TOS (I asked it to a local Google account) and it said it‚Äôs against TOS.\n\nThis was using both Gemeni and then Grok.z",
          "score": -1,
          "created_utc": "2026-01-28 10:28:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpwm0k",
      "title": "LMStudio v 0.4.0 Update",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1qpwm0k",
      "author": "Impossible-Glass-487",
      "created_utc": "2026-01-29 02:55:46",
      "score": 113,
      "num_comments": 2,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpwm0k/lmstudio_v_040_update/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2i3jts",
          "author": "leonbollerup",
          "text": "Sweeeeeet ‚Ä¶ now.. if llamas.ccp could also support paralism between GPUs please",
          "score": 2,
          "created_utc": "2026-01-29 23:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ib00p",
          "author": "Aggressive_Special25",
          "text": "It's failing to install the update for me. Says it cannot uninstall previous files... Help",
          "score": 1,
          "created_utc": "2026-01-29 23:54:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrbk38",
      "title": "Clawdbot is changing names faster than this dude could change faces",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/dj2fgd0urigg1.jpeg",
      "author": "AeroMogli",
      "created_utc": "2026-01-30 17:16:02",
      "score": 101,
      "num_comments": 11,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qrbk38/clawdbot_is_changing_names_faster_than_this_dude/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2n0s1r",
          "author": "Big_Assistance2151",
          "text": "An agent is no one.",
          "score": 23,
          "created_utc": "2026-01-30 17:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o2d01",
          "author": "RazerWolf",
          "text": "‚ÄúFaceless agent‚Äù has a sort of decent ring to it. Or J‚Äôaqen h‚Äôghar.",
          "score": 4,
          "created_utc": "2026-01-30 20:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2n31lr",
          "author": "_Cromwell_",
          "text": "And every single name is really bad.",
          "score": 7,
          "created_utc": "2026-01-30 17:43:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nw0sn",
              "author": "FaceDeer",
              "text": "At least they've settled on something that doesn't sound ominous or threatening to the less-AI-literate general public.",
              "score": 5,
              "created_utc": "2026-01-30 19:51:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pcvuj",
                  "author": "FirstEvolutionist",
                  "text": "I find OpenClaw rolls off the tongue much better than ClawdBot or MoltBot. I don't have a problem with the crustacean theme, but I'm getting confused whether it was supposed to be lobsters but nows it's crabs?\n\nI don't think branding was much concern from the get go, but given the attention it is getting, it would be interesting if there was more effort there, even if at this pojnt it's become something public. Take Linux and the penguin association for example, it just works.",
                  "score": 2,
                  "created_utc": "2026-01-31 00:15:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2nou5y",
          "author": "WhyAmIDoingThis1000",
          "text": "someone came out of the woodwork and claimed moltbot as already trademarked, I'm sure.",
          "score": 3,
          "created_utc": "2026-01-30 19:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pvlb9",
          "author": "Practical-Plan-2560",
          "text": "üòÇ SO good",
          "score": 2,
          "created_utc": "2026-01-31 02:03:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ot1v2",
          "author": "nothingnotnever",
          "text": "And there‚Äôs the name right here, and their name right up there.",
          "score": 1,
          "created_utc": "2026-01-30 22:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qpwx2",
          "author": "Bozhark",
          "text": "In the matrix, we are all AIgents",
          "score": 1,
          "created_utc": "2026-01-31 05:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p0nlw",
          "author": "LuckyLuckierLuckest",
          "text": "It is not Clawdbot.",
          "score": 1,
          "created_utc": "2026-01-30 23:08:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpzn7d",
      "title": "I gave a local LLM a body so it feels more like a presence.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/bv8myqt438gg1",
      "author": "Smart_File4124",
      "created_utc": "2026-01-29 05:17:18",
      "score": 87,
      "num_comments": 27,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpzn7d/i_gave_a_local_llm_a_body_so_it_feels_more_like_a/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2d4txz",
          "author": "Apprehensive-End7926",
          "text": "bro is recreating Bonzi Buddy from first principles",
          "score": 29,
          "created_utc": "2026-01-29 06:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d5clz",
              "author": "Smart_File4124",
              "text": "Bonzi Buddy if he went to therapy and became a better person lmao. 100% local, 0% spyware, 100% good vibes onlyüòÇ",
              "score": 10,
              "created_utc": "2026-01-29 06:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dl0cx",
          "author": "Modgeyy",
          "text": "How did you make the avatar? Looks really cool!",
          "score": 3,
          "created_utc": "2026-01-29 08:22:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dqwf3",
              "author": "Smart_File4124",
              "text": "Japanese great artist ¬©„Å¥„Çà„Åü„Åû made it!",
              "score": 3,
              "created_utc": "2026-01-29 09:18:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2delks",
          "author": "haradaken",
          "text": "It looks fun! Do the facial expressions change based on the chat content, maybe?",
          "score": 2,
          "created_utc": "2026-01-29 07:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dqpdg",
              "author": "Smart_File4124",
              "text": "thank! unfortunately he is not that smart yet haha, but It will be possible on the next version!!",
              "score": 3,
              "created_utc": "2026-01-29 09:16:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dr2gw",
                  "author": "haradaken",
                  "text": "Cool! Looking forward to the evolution. :)",
                  "score": 1,
                  "created_utc": "2026-01-29 09:19:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dkbk1",
          "author": "Bavlys",
          "text": "Looks good",
          "score": 1,
          "created_utc": "2026-01-29 08:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dqqku",
              "author": "Smart_File4124",
              "text": "ü¶çthanks:)",
              "score": 1,
              "created_utc": "2026-01-29 09:16:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dnu12",
          "author": "onicarps",
          "text": "so cute thank you! not that the current character is unappealing, but i can imagine having a store of other custom characters created by the community...",
          "score": 1,
          "created_utc": "2026-01-29 08:49:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dq2z1",
              "author": "onicarps",
              "text": "thing is \n\nwindows is saying... This program is dangerous and executes commands from an attacker.",
              "score": 2,
              "created_utc": "2026-01-29 09:10:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dr537",
                  "author": "Smart_File4124",
                  "text": "Yeah, Windows flags it because it's unsigned (code signing certs are expensive for indie projects üòÖ). Totally understand the caution! Working on getting it signed once I can justify the cost.",
                  "score": 1,
                  "created_utc": "2026-01-29 09:20:28",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2dr771",
                  "author": "Smart_File4124",
                  "text": "but appreciate your kind word btw:)",
                  "score": 1,
                  "created_utc": "2026-01-29 09:21:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dvq2z",
          "author": "OnlyAssistance9601",
          "text": "Yh not much longer until ClosedAI yoinks that idea",
          "score": 1,
          "created_utc": "2026-01-29 10:02:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eglyp",
              "author": "Smart_File4124",
              "text": "so I need folks hereüí™",
              "score": 1,
              "created_utc": "2026-01-29 12:45:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e7pkn",
          "author": "Low_Soil_6543",
          "text": "This is so cool! Would be keen to get updated on the development. Is there any way to contact you?",
          "score": 1,
          "created_utc": "2026-01-29 11:44:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e8zin",
              "author": "Smart_File4124",
              "text": "thank you!! hmm you if you give me your email in landing page, I'll keep updateüî•",
              "score": 1,
              "created_utc": "2026-01-29 11:53:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2eanuf",
                  "author": "Low_Soil_6543",
                  "text": "I did. I'm getting an error that the application fails to find a couple dll's I believe its because I already have some local llama models and your application tries to download one, which interferes with the one I already have. If you could have a quick look that would be awesome.  ",
                  "score": 1,
                  "created_utc": "2026-01-29 12:05:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2f1wol",
          "author": "TNTChaos",
          "text": "This is amazing! This is exactly what I need for my character/ story roleplay site that I built!!! Thank you for posting this!",
          "score": 1,
          "created_utc": "2026-01-29 14:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2le05z",
          "author": "Overall_Wrangler5780",
          "text": "CAN YOU OPEN SOURCE THE CODE, I WOULD WANT TO BUILD ON TOP OF THIS,",
          "score": 1,
          "created_utc": "2026-01-30 12:48:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qw3ie",
          "author": "HealthyCommunicat",
          "text": "Super cool use case, really good execution, what kind of characters are on the way?",
          "score": 1,
          "created_utc": "2026-01-31 06:07:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpix0z",
      "title": "clawdbot what am I missing?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qpix0z/clawdbot_what_am_i_missing/",
      "author": "olearyboy",
      "created_utc": "2026-01-28 18:04:46",
      "score": 54,
      "num_comments": 54,
      "upvote_ratio": 0.92,
      "text": "This week my feeds have been over thrown with something called 'clawdbot' / 'moltbot'\n\nHere's the breakdown of what I'm seeing\n\n\\* 80% - here's a 20 minute video on how to install it\n\n\\* 15% - (hype) best thing ever / massive security concern\n\n\\* 5% - here's a thing I did with it\n\n\n\nWithout installing, it just seems like a regular agent the same as we've all been building with the kitchen sink thrown at it for in-out bound communication and agentic skills md's and tooling with a bit of memory.\n\nThat 5% was one dude comparing clawdbot to claude code\n\n\n\nWhat am I missing?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpix0z/clawdbot_what_am_i_missing/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o29at9i",
          "author": "TokenRingAI",
          "text": "There is a point with every new technology, where the uninformed mob learns about it and storms the gates in some kind of massive bandwagon\n\nHere is a summary of how that is going\n\nhttps://preview.redd.it/fbu1phaes4gg1.png?width=1078&format=png&auto=webp&s=715be3f1e1dff93e1fe0ea193bb5fa2aeeed20d1",
          "score": 67,
          "created_utc": "2026-01-28 18:11:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bj7jt",
              "author": "SalemStarburn",
              "text": "Is ChaosGPT still around? Hook it up to Clawdbot + sudo + internet access and a couple thousand dollars and let it cook.",
              "score": 6,
              "created_utc": "2026-01-29 00:20:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2biuik",
              "author": "meva12",
              "text": "Is that true !? Damn",
              "score": -3,
              "created_utc": "2026-01-29 00:18:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2eizsp",
                  "author": "Waste_Drop8898",
                  "text": "Come the fuck on, use your head",
                  "score": 1,
                  "created_utc": "2026-01-29 13:00:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29dk8w",
          "author": "commandedbydemons",
          "text": "It‚Äôs cool, but also destroys tokens like there‚Äôs no tomorrow.\n\nDon‚Äôt let it use your full pc, sandbox it for the love of god",
          "score": 25,
          "created_utc": "2026-01-28 18:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29jkl3",
              "author": "ObsidianNix",
              "text": "Docker with an automatic firewall should be the default install for any AI installation. Open whatever doors are need when they are needed.",
              "score": 11,
              "created_utc": "2026-01-28 18:48:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ad2bz",
                  "author": "Iron-Over",
                  "text": "Docker is not secure enough get VM to stop host kernel vulnerabilities and in multi tenant docker environments. Beyond VM least privileged principal. Do not use untrusted data.",
                  "score": 7,
                  "created_utc": "2026-01-28 20:59:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29vre7",
                  "author": "tillybowman",
                  "text": "clawdbot does not have an official docker yet, right?",
                  "score": 1,
                  "created_utc": "2026-01-28 19:42:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lp6iv",
                  "author": "bananahead",
                  "text": "You misunderstand the ways this can go really badly. An LLM *reading your email* and performing actions means someone can email you a prompt injection and steal what‚Äôs in your inbox.",
                  "score": 0,
                  "created_utc": "2026-01-30 13:51:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29dve5",
          "author": "blamestross",
          "text": "Its the best off the shelf tooling for self-run agents. I'm running it on gemma. \n\nIts meh? Brittle config, bit overcomplicated, clearly vibecoded and spams the context with too much garbage that isn't task related.\n\nIts horrible. It does hook up multiple chat platforms to an agent with a pile of tools. Thats nice.",
          "score": 11,
          "created_utc": "2026-01-28 18:24:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lpgny",
              "author": "bananahead",
              "text": "Terrible, fundamentally flawed security. You should not give it access to any files or accounts or passwords you wouldn‚Äôt post publicly.",
              "score": 1,
              "created_utc": "2026-01-30 13:53:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m5lgc",
                  "author": "blamestross",
                  "text": "Enthusiastic Agreement",
                  "score": 1,
                  "created_utc": "2026-01-30 15:12:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a4fml",
          "author": "radiofreevanilla",
          "text": "Anyone interested in how it gets built, Pragmatic Engineer just published an interview:  \nThe creator of Clawd: \"I ship code I don't read\"  \n[https://newsletter.pragmaticengineer.com/p/the-creator-of-clawd-i-ship-code](https://newsletter.pragmaticengineer.com/p/the-creator-of-clawd-i-ship-code)",
          "score": 8,
          "created_utc": "2026-01-28 20:21:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d01id",
              "author": "Daimakai",
              "text": "What a quote! üòÇ",
              "score": 3,
              "created_utc": "2026-01-29 05:27:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o29dk7p",
          "author": "noctrex",
          "text": "The letter \"s\" in clawdbot¬†stands for security.\n\nhttps://preview.redd.it/8mobb8ygu4gg1.png?width=857&format=png&auto=webp&s=3537fe9c6708ed9054d2e285356da2b36615ce69",
          "score": 19,
          "created_utc": "2026-01-28 18:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dp7xk",
              "author": "BasementChimpActual",
              "text": "I could be wrong here, but this is technically a bit misleading. She states you \"run your own AI on your own machine,\" which implies it's a local LLM, but from what I have read, the computation actually runs on their servers",
              "score": 3,
              "created_utc": "2026-01-29 09:02:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2erf1z",
                  "author": "Specialist-Yellow",
                  "text": "It can be setup to use local LLM, but most are not.",
                  "score": 3,
                  "created_utc": "2026-01-29 13:48:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2kuqq0",
              "author": "mike7seven",
              "text": "Indeed https://moltybook.cichlidinc.com/",
              "score": 0,
              "created_utc": "2026-01-30 10:20:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mrfkl",
              "author": "brianlmerritt",
              "text": "There is no f'in security :D",
              "score": 0,
              "created_utc": "2026-01-30 16:51:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a9380",
          "author": "Dense-Map-4613",
          "text": "Installed for 3 hours, get it to works, doesn‚Äôt see any benefit. Delete. Just a bunch hype out of nothing.",
          "score": 5,
          "created_utc": "2026-01-28 20:42:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cnprm",
              "author": "djdante",
              "text": "I installed this morning, but can't work out what I want to do with it,  other that a way to keep my spare laptop busy",
              "score": 2,
              "created_utc": "2026-01-29 04:05:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2odc9y",
                  "author": "Fantastic_Support_13",
                  "text": "Same thing",
                  "score": 1,
                  "created_utc": "2026-01-30 21:13:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2aowqg",
          "author": "Necessary-Drummer800",
          "text": "I'm with you.  From the middle it looks like a desperate attempt to push out the long tail of the bust.",
          "score": 3,
          "created_utc": "2026-01-28 21:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a1hii",
          "author": "Scott_Malkinsons",
          "text": "IMO Clawd/Moltbot + [Z.AI](http://Z.AI) = Poor mans Claude Code.\n\nYes, it's basically just an Agent, there's nothing \"ground breaking\" about it from what I can tell using it. It's basically Claude Code but instead of $200/mo for the max plan, you pay [Z.AI](http://Z.AI) $3-6/month, run Clawd/Molt on a $6-10 VPS, and you effectively get Claude Code for under $20/month.\n\nThen there's the 'I'm not sure if Claude Code can do this, it probably can, but I've never done it' things like talking to the LLM through Telegram, both with text back and forth, or voice back and forth with TTS and STT.",
          "score": 4,
          "created_utc": "2026-01-28 20:08:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cni2q",
              "author": "djdante",
              "text": "How does it run for you with z.ai? I just set it up today but connected to my codex.\n\nWould rather run it on my Z account if it's decent",
              "score": 1,
              "created_utc": "2026-01-29 04:03:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lptvh",
              "author": "bananahead",
              "text": "But you can just connect Claude code to z.ai directly!",
              "score": 1,
              "created_utc": "2026-01-30 13:54:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2c73r0",
              "author": "iplaypianoforyou",
              "text": "What is z.ai",
              "score": 0,
              "created_utc": "2026-01-29 02:29:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kvexq",
          "author": "mike7seven",
          "text": "It‚Äôs all I keep hearing about. Here‚Äôs a decent technical write up and how to https://moltybook.cichlidinc.com/",
          "score": 2,
          "created_utc": "2026-01-30 10:26:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m1az2",
              "author": "olearyboy",
              "text": "Why is it asking me about my penis size?",
              "score": 1,
              "created_utc": "2026-01-30 14:52:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29m2d1",
          "author": "_hephaestus",
          "text": "Took a crack at it this morning, seemed like interesting tooling to be able to talk to my llms via signal/whatsapp/imessage but the configuration is not particularly intuitive for your own models.",
          "score": 2,
          "created_utc": "2026-01-28 18:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rcat",
          "author": "MyGoldfishGotLoose",
          "text": "I would have been curious but the hype has me in ‚Äúwait and see‚Äù mode.",
          "score": 1,
          "created_utc": "2026-01-28 19:22:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29z7e5",
          "author": "epSos-DE",
          "text": "In my OPINION.\n\nIT is a good start, BUT it needs to be p2p gossip chat. Ai to AI !\n\n  \nAlso, it is too broad. Needs to focus , not being distracted. \n\n  \nIT will run out of steam without focused application !",
          "score": 1,
          "created_utc": "2026-01-28 19:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2an0uj",
          "author": "HatEducational9965",
          "text": "I don't get it either",
          "score": 1,
          "created_utc": "2026-01-28 21:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aout1",
          "author": "pandodev",
          "text": "definitely try it, it is cool but not what people are making it out to be and honestly NOONE should be running this on their home network, best it to securely sandbox it in an ec2 with only able to access via sessionmanager. Maybe I am too paranoid but this uses dependencies if any of those dependencies get infiltrated would you rather them be in a aws secluded server or your home network?",
          "score": 1,
          "created_utc": "2026-01-28 21:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b03a0",
          "author": "Crazy_Patience921",
          "text": "I would say nothing :)\n\n",
          "score": 1,
          "created_utc": "2026-01-28 22:42:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2btsme",
          "author": "EdgardoZar",
          "text": "I found it useful for my minimax subscription which does not have a chatGUI like claude or even Zai, and I found it useful just to automatically create notes out of my memory dialy dump, without having to turn on the PC and use claude code or whatever, I have some n8n workflows but you still need interaction at some point, not only triggering workflows with commands. Nothing fancy but still kind of useful for some tasks",
          "score": 1,
          "created_utc": "2026-01-29 01:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dpen2",
          "author": "Loose-Doubt-4421",
          "text": "I spent a 2 days writing skills to teach it to   \n  \n\\- analyse the stock market  \n\\- control my brokerage account   \n\\- use the IBKR API  \n\\- quickly backtest trading strategies  \n\\- get relevant financial news  \n\\- Work autonomously and message me if it's notice something worth\n\nThe results are GREAT. I'm already getting great value in only a week of setting it up.",
          "score": 1,
          "created_utc": "2026-01-29 09:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i7mql",
              "author": "olearyboy",
              "text": "Yeah, but what's it doing differently from other agents with skills / scheduling that make it better?\n\nThats what I'm failing to see",
              "score": 1,
              "created_utc": "2026-01-29 23:36:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2j10sc",
                  "author": "Loose-Doubt-4421",
                  "text": "Also, the time spent setting up an automation :  \n  \nIn n8n, you spend hours setting up an automation. (Or use the terrible n8n AI extension that never works)  \nIn Clawdbot, you just prompt it to setup everything and test, it only message you when it's done and working.",
                  "score": 1,
                  "created_utc": "2026-01-30 02:17:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2iy7ku",
                  "author": "Loose-Doubt-4421",
                  "text": "The only difference is that it has full control over its own computer. (it's own isolated system, where it can do anything it want) \n\nN8N and other AI Agent tools are very limited. They cannot write skills for themselves.\n\nYou can tell Clawdbot \"install Docker and run a Plex server, download 'legal' movies to it. When you're done send me link to watch\". No other agent support that obviously..",
                  "score": 0,
                  "created_utc": "2026-01-30 02:01:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gshva",
          "author": "howtofirenow",
          "text": "Isn‚Äôt it a cloudflare product or related? Anyhow.. it‚Äôs a security nightmare. Don‚Äôt do it.",
          "score": 1,
          "created_utc": "2026-01-29 19:25:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i784a",
              "author": "olearyboy",
              "text": "Think it's some dudes personal project, good on him but I just don't get the hype \n\nPeople like shiny stuff",
              "score": 1,
              "created_utc": "2026-01-29 23:34:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2pcn0l",
          "author": "Complex-Ad749",
          "text": "i think the point is to show off how much friction you can add to a task that requires zero effort or brainpower, such as checking in to an airplane flight",
          "score": 1,
          "created_utc": "2026-01-31 00:14:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29a3lg",
          "author": "xyzzzzy",
          "text": "Man I don't know either. I am trying to install it to see what I'm missing. I know that's the point of the hype, but here we are.",
          "score": 1,
          "created_utc": "2026-01-28 18:08:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29j0rk",
              "author": "ObsidianNix",
              "text": "Wait until it matures. Thats what im doing too after trying for 5 hours on docker well secured. Its very finicky. \n\nRight now its going through the polishing stage. I guess we‚Äôll have to see if this is just another side project.",
              "score": 4,
              "created_utc": "2026-01-28 18:46:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2b7t0b",
          "author": "Objective-Arrival637",
          "text": "I am using kimi-k2.5 using ollama-cloud and added the homeassistant skill. It couldn't turn on the lights in my office. After so much back and forth it finally learnt on how to do that :( Facing the same thing with browser use, and other skills. Maybe it is me, or maybe this is not built for kimi models? But GLM-4.7 was worse.",
          "score": 0,
          "created_utc": "2026-01-28 23:20:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2csefh",
              "author": "bernie_vp",
              "text": "I was not able to make it work with Kimi k2.5. I always found that antrophic was part of the connection URL. But you have made it. Congratulations üëè \n\nBut how can ollama cloud be used for Kimi. Any useful links to documentation for me ?",
              "score": 1,
              "created_utc": "2026-01-29 04:35:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2octxs",
                  "author": "Comprehensive_Iron_8",
                  "text": "[https://docs.openclaw.ai/concepts/model-providers#ollama](https://docs.openclaw.ai/concepts/model-providers#ollama)",
                  "score": 1,
                  "created_utc": "2026-01-30 21:11:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qqo0se",
      "title": "My Dream has come true, running a 1 Trillion parameter model on my pc",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qqo0se/my_dream_has_come_true_running_a_1_trillion/",
      "author": "Aggressive_Special25",
      "created_utc": "2026-01-29 22:59:34",
      "score": 45,
      "num_comments": 48,
      "upvote_ratio": 0.89,
      "text": "https://preview.redd.it/54ny23qfcdgg1.png?width=1039&format=png&auto=webp&s=dfc08484bed673973f74744e0ffa6f692c9f425b\n\nOffloading to my NVME. Never thought I would need faster than 8gb/s. Its pretty slow but I would say usable....kind of.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qqo0se/my_dream_has_come_true_running_a_1_trillion/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o2j6rh7",
          "author": "Lissanro",
          "text": "From the screenshot, does not look like you have correct chat template since normally thoughts are within the <think> block. This alone can reduce quality significantly, so I suggest to investigate and fix this.\n\nI am surprised though you are getting 1.18 tokens/s. Most likely prompt processing would make it not practical though even for overnight runs. Q1 is another issue. In my experience, even IQ3 has noticeable drop in quality, and using Q4\\_X quants to preserve the original INT4 quality is better (offered here: [https://huggingface.co/AesSedai/Kimi-K2.5](https://huggingface.co/AesSedai/Kimi-K2.5) ). K2 Thinking Q4\\_X quant is the model I run the most on my rig, currently still downloading new 2.5 version.\n\nIn another comment you mentioned that you have 96 GB RAM and 48 GB VRAM (made of two 3090 cards). If the biggest model and the highest possible quality is desired but the one that is still usable, you can give a try to [https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-40-GGUF](https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-40-GGUF) \\- most likely Q4\\_K\\_S will have the best balance of speed/quality, you also can try Q3\\_K\\_S if the higher speed is desired. Also, I recommend using ik\\_llama.cpp -¬†shared details¬†[here](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)¬†how to build and set it up (you should get faster inference and much faster prompt processing with it, compared to the mainline llama.cpp).",
          "score": 13,
          "created_utc": "2026-01-30 02:49:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mocor",
              "author": "Aggressive_Special25",
              "text": "Thanks ill try this!",
              "score": 2,
              "created_utc": "2026-01-30 16:37:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2i2rle",
          "author": "Murder_1337",
          "text": "What do you use it for?",
          "score": 4,
          "created_utc": "2026-01-29 23:10:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iadi5",
              "author": "Aggressive_Special25",
              "text": "Just testing ssd offloading to test out massive models",
              "score": 2,
              "created_utc": "2026-01-29 23:51:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ibluj",
                  "author": "Murder_1337",
                  "text": "How much the setup cost in hardware?",
                  "score": 2,
                  "created_utc": "2026-01-29 23:58:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2mwkzd",
                  "author": "Polysulfide-75",
                  "text": "Are you using kvcache?\nVLLM? TensorRT?",
                  "score": 1,
                  "created_utc": "2026-01-30 17:14:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i6pzu",
          "author": "Barachiel80",
          "text": "what settings are you using to do the offloading to get 1.8tk/s off ssd? Also what are your rig specs?",
          "score": 3,
          "created_utc": "2026-01-29 23:31:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iaad2",
              "author": "Aggressive_Special25",
              "text": "96gb ram 2x 3090s 4tb nvme Gen 4",
              "score": 4,
              "created_utc": "2026-01-29 23:50:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2j5gl9",
                  "author": "SectionCrazy5107",
                  "text": "can you share your command please.",
                  "score": 1,
                  "created_utc": "2026-01-30 02:41:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i8bs5",
          "author": "Particular-Way7271",
          "text": "Try a q4 as well",
          "score": 2,
          "created_utc": "2026-01-29 23:40:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ia7xa",
              "author": "Aggressive_Special25",
              "text": "I'm too scared",
              "score": 2,
              "created_utc": "2026-01-29 23:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ibn5k",
          "author": "Alone-Marionberry-59",
          "text": "How many tokens per second? Could this be used for coding tasks overnight autonomous? Also - how does it change lifetime of the hardware? Congrats! This is amazing!",
          "score": 2,
          "created_utc": "2026-01-29 23:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iqvur",
              "author": "Count_Rugens_Finger",
              "text": "screenshot says 1.18",
              "score": 4,
              "created_utc": "2026-01-30 01:20:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lk59k",
          "author": "johannes_bertens",
          "text": "Congratulations üéâ \nWelcome to the club!",
          "score": 2,
          "created_utc": "2026-01-30 13:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2idmf0",
          "author": "overand",
          "text": "Kimi K2.5 at Q1, that's like a \\~276G model. Dang!",
          "score": 2,
          "created_utc": "2026-01-30 00:09:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iwkig",
          "author": "Acceptable_Home_",
          "text": "How badly does it hurt the ssd? I heard it degrades ssd life by a lot",
          "score": 3,
          "created_utc": "2026-01-30 01:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j5r7f",
              "author": "SpicyWangz",
              "text": "Wouldn‚Äôt this only be doing reads and not writes?",
              "score": 6,
              "created_utc": "2026-01-30 02:43:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2l3ono",
                  "author": "Acceptable_Home_",
                  "text": "oh, thanks to lemme know",
                  "score": 1,
                  "created_utc": "2026-01-30 11:35:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2k67pn",
                  "author": "HushHushShush",
                  "text": "Most SSDs have a lifetime of about 1500TB writes. Assuming you load this up once a day, you'd have to buy a new SSD in 4 years.",
                  "score": 0,
                  "created_utc": "2026-01-30 06:42:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2koz9m",
              "author": "Aggressive_Special25",
              "text": "Does not write only read. Does not damage ssd",
              "score": 1,
              "created_utc": "2026-01-30 09:27:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2l3pzk",
                  "author": "Acceptable_Home_",
                  "text": "oh, thanks for letting me know",
                  "score": 2,
                  "created_utc": "2026-01-30 11:36:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i6x8t",
          "author": "Barachiel80",
          "text": "any test benchmarks with higher context?",
          "score": 1,
          "created_utc": "2026-01-29 23:32:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iipf9",
          "author": "Available-Craft-5795",
          "text": "Whats the GPU? Are you offloading some peramiters to Vram or RAM? Also thats crazy",
          "score": 1,
          "created_utc": "2026-01-30 00:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ijwow",
          "author": "siegevjorn",
          "text": "Noice. Is the qaulity acceptable?",
          "score": 1,
          "created_utc": "2026-01-30 00:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2j24me",
          "author": "I_like_fragrances",
          "text": "How do you use the ssd when you use the model?",
          "score": 1,
          "created_utc": "2026-01-30 02:23:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jao77",
          "author": "Quirky-Repair-6454",
          "text": "What hardware you are using ? Can you share setup ?",
          "score": 1,
          "created_utc": "2026-01-30 03:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k8ed2",
          "author": "Zyj",
          "text": "I‚Äòve tried that quant, I thought the quality was bad.",
          "score": 1,
          "created_utc": "2026-01-30 07:00:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2idjir",
          "author": "belgradGoat",
          "text": "I don‚Äôt get it how are you running 1tb model on 96gb of ram? Inference directly from ssd?",
          "score": 1,
          "created_utc": "2026-01-30 00:08:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ikqp0",
              "author": "Aggressive_Special25",
              "text": "Yes",
              "score": 3,
              "created_utc": "2026-01-30 00:47:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2irpxq",
                  "author": "belgradGoat",
                  "text": "I wouldn‚Äôt call it running a model. More like slow walking or crawling",
                  "score": 4,
                  "created_utc": "2026-01-30 01:25:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2io2bp",
                  "author": "Relevant-Magic-Card",
                  "text": "is this becoming a real thing? how is this even possible?",
                  "score": 1,
                  "created_utc": "2026-01-30 01:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qojhkj",
      "title": "Local LLM for Coding that compares with Claude",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qojhkj/local_llm_for_coding_that_compares_with_claude/",
      "author": "thecrogmite",
      "created_utc": "2026-01-27 16:59:10",
      "score": 38,
      "num_comments": 74,
      "upvote_ratio": 0.77,
      "text": "Currently I am on the Claude Pro plan paying $20 a month and I have hit my weekly and daily limits very quickly. Am I using it to essentially handle all code generation? Yes. This is the way it has to be as I'm not familiar with the language I'm forced to use. \n\n  \nI was wondering if there was a recommended model that I could use to match Claude's reasoning and code output. I don't need it to be *super fast* like Claude. I need it to be accurate and not completely ruin the project. While most of that I feel like is prompt related, some of that has to be related to the model. \n\n  \nThe model would be ran on a MacBook Pro M3. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qojhkj/local_llm_for_coding_that_compares_with_claude/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o21s08f",
          "author": "Ryanmonroe82",
          "text": "You won't be able to match a cloud model on that setup",
          "score": 52,
          "created_utc": "2026-01-27 17:13:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21tkr5",
              "author": "MostIncrediblee",
              "text": "I have 48GB ram. What one model is best for coding. Probably not on Claude‚Äôs level.¬†",
              "score": 5,
              "created_utc": "2026-01-27 17:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21w9zw",
                  "author": "synth_mania",
                  "text": "Probably Devstral Small 2 (devstral-small-2-2512). It was released by Mistral last month.\n\nExcellent 24B param dense model. I'm running it at Q4\\_K\\_M to good effect, but the largest quant you can fit up to Q8 is probably worth sacrificing RAM for if you can still fit the context you need.\n\nI've had really good success with Aider, but it might trip up sometimes with less hands-on tools like Kilo Code, or other agentic programming applications.",
                  "score": 17,
                  "created_utc": "2026-01-27 17:31:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21xi13",
                  "author": "minaskar",
                  "text": "Devstral Small 2 (2512), GLM 4.7 Flash, and Qwen 3 Coder 30b",
                  "score": 9,
                  "created_utc": "2026-01-27 17:36:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21wm9h",
                  "author": "hoowahman",
                  "text": "People usually mention qwen3coder for this, not sure how local GLM 4.7 fairs though in comparison. Probably pretty good.",
                  "score": 3,
                  "created_utc": "2026-01-27 17:33:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23940v",
                  "author": "RnRau",
                  "text": "ram or vram?\n\nIf ram, try gpt-oss-20b with thinking mode set on 'high'. If vram, try the big brother - gpt-oss-120b with model offloading.",
                  "score": 3,
                  "created_utc": "2026-01-27 21:05:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23lrjt",
                  "author": "Ryanmonroe82",
                  "text": "Qwen coder is pretty good. Use the 32b version if possible",
                  "score": 2,
                  "created_utc": "2026-01-27 22:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2eqptx",
              "author": "Exciting_Narwhal_987",
              "text": "can you share a comparable setup that would?",
              "score": 1,
              "created_utc": "2026-01-29 13:44:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21x0zj",
          "author": "son_et_lumiere",
          "text": "Use the claude/larger model for the reasoning capabilities to break down big task into smaller coding tasks. Then take that task list and use a cheap/free model to the actual coding from the well defined task.",
          "score": 29,
          "created_utc": "2026-01-27 17:34:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22p0b9",
              "author": "intertubeluber",
              "text": "Look at the big brains on Brad!",
              "score": 4,
              "created_utc": "2026-01-27 19:34:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o22nwdn",
              "author": "Weird-Consequence366",
              "text": "This is the way",
              "score": 2,
              "created_utc": "2026-01-27 19:29:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29as3e",
              "author": "fourfastfoxes",
              "text": "one other thing to do is that you can install Google Jules cli and assign the smaller tasks to Jules to complete. you get about 15 tasks per day free.",
              "score": 1,
              "created_utc": "2026-01-28 18:11:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o220e9j",
          "author": "pot_sniffer",
          "text": "You won't find a local llm as good as Claude but what i do use my Claude pro to do the planning, within that I plan to break the project up into manageable chunks. Then I get Claude to make structured json prompts for the local llm. I'm currently using qwen2.5-coder but I've had similar results with the other I've done this with",
          "score": 17,
          "created_utc": "2026-01-27 17:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22telk",
              "author": "thumperj",
              "text": "Just curious to learn a bit more about your workflow. Do you have this claude-->local handoff process scripted or do you do it manually?\n\nCurrently, I'm using claude cli for pretty much everything, which includes editing files but I'm also making a nice car payment to the claude gods every month....  One day soon I want to jump to a more efficient methodology BUT my current setup enables me to work like a banshee, produce excellent work and charge $$$$ to my clients so it's been well worth it.",
              "score": 3,
              "created_utc": "2026-01-27 19:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o234gun",
                  "author": "pot_sniffer",
                  "text": "Currently manual. Claude generates a structured JSON prompt, I copy/paste to Qwen2.5-coder, test the result. Haven't scripted it because I'm early in the build and the manual handoff isn't painful yet. Plan is to keep it manual until it gets annoying enough to justify writing automation.\nThe key is STATE.md - single source of truth that both Claude and local model read so they don't suggest things I've already tried/failed. That prevents context waste more than scripting would.",
                  "score": 3,
                  "created_utc": "2026-01-27 20:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o22nun5",
              "author": "thecrogmite",
              "text": "Good to know, maybe this is the move for me. Thank you for that.",
              "score": 5,
              "created_utc": "2026-01-27 19:29:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22dml6",
          "author": "milkipedia",
          "text": "It seems this question is asked every day. Only difference is the user's available hardware, if they even bother to specify.",
          "score": 6,
          "created_utc": "2026-01-27 18:45:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22pptl",
              "author": "l8yters",
              "text": "welcome to reddit.",
              "score": 1,
              "created_utc": "2026-01-27 19:38:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22gsvc",
          "author": "armyknife-tools",
          "text": "Check out open routers leaderboard. I think it‚Äôs pretty accurate. 3 of the top 10 are open weights models.",
          "score": 4,
          "created_utc": "2026-01-27 18:58:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22e1oh",
          "author": "SimplyRemainUnseen",
          "text": "Depending on how much memory your system has GLM-4.7-Flash would be a good local model you can run. It won't be as good as Claude 4 models but it can handle a lot. I suggest giving it a try. I've found local LLMs have been at the performance I need for my programming workflow since 2024.",
          "score": 3,
          "created_utc": "2026-01-27 18:47:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26mbdi",
              "author": "ScoreUnique",
              "text": "This is the right answer, I wouldn't hesitate to claim 4.7 Flash is Sonnet 3.5 but open source. \n\nSo @OP if you can combine Claude sub for writing specifications and GLM 4.7 for writing code, you can go very far with your config. GL",
              "score": 1,
              "created_utc": "2026-01-28 09:00:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o22mwdv",
              "author": "thecrogmite",
              "text": "Thank you! I'll take a peek and give it a try.",
              "score": 1,
              "created_utc": "2026-01-27 19:25:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21ziyw",
          "author": "JordanAtLiumAI",
          "text": "Is your pain mostly code generation, or reasoning across a lot of project context like docs, configs, logs, and past commits?",
          "score": 2,
          "created_utc": "2026-01-27 17:45:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22nsjv",
              "author": "thecrogmite",
              "text": "Great question. For this specific task that I'm trying to overcome is code generation. The project has expanded from a SQL database to a .NET8 middleware to React front end. While I'm familiar with the SQL side of things slightly, the middleware and React are foreign.   \n  \nI've got the project at a solid first pass working state however making changes Claude seems to want to make huge passes across the entire architecture, then make it's decision as to what to change. While I believe that my prompting could improve to essentially add better guardrails, I'm worried I'll burn my availability fairly quickly regardless just based on project size.",
              "score": 1,
              "created_utc": "2026-01-27 19:29:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22ws9w",
                  "author": "JordanAtLiumAI",
                  "text": "Pro usage limits depend on the total length of your conversation, the number of messages, and which model or feature you use, and they may vary with current capacity.\n\n  \nSo as your project grows, it is normal that you hit limits faster, because each prompt tends to require more context and more turns. Their recommended mitigation is to be specific and concise and avoid vague prompts that trigger extra clarification cycles.\n\n  \nA practical workflow pattern that aligns with that  \n‚Ä¢ Convert ‚Äúmake this change‚Äù into a single narrowly defined task  \n‚Ä¢ Provide only the minimal relevant snippets, not the whole repo  \n‚Ä¢ Constrain edits to a file list  \n‚Ä¢ Require diff output  \n‚Ä¢ Plan first, then implement step 1\n\n  \nIt reduces the chance of large refactors and keeps each turn cheaper. \n\nHope this helps! Feel free to DM me if you want. We can dive in more.",
                  "score": 3,
                  "created_utc": "2026-01-27 20:09:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22zbr9",
          "author": "ServiceOver4447",
          "text": "Nopes, you will have to pay more for it to do your job.",
          "score": 2,
          "created_utc": "2026-01-27 20:21:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o232f71",
          "author": "StardockEngineer",
          "text": "We need to start having a flair post requirement for these posts.",
          "score": 2,
          "created_utc": "2026-01-27 20:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23sc20",
          "author": "Educational_Sun_8813",
          "text": "try devstral2 small, and qwen3-coder",
          "score": 2,
          "created_utc": "2026-01-27 22:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23tid2",
          "author": "Torodaddy",
          "text": "Qwen3 coder 30b model works well for me. Running it in llama.cpp",
          "score": 2,
          "created_utc": "2026-01-27 22:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22d1ip",
          "author": "greeny1greeny",
          "text": "nothing... all the latest models i tried are complete buns and dont even touch claude.",
          "score": 2,
          "created_utc": "2026-01-27 18:43:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22oh83",
          "author": "ithkuil",
          "text": "Claude Opus 4.5 is probably running on 8 x H200 clusters anywhere they have capacity for that. It's not the same because they do batching, but your Mac may be as much as 1000 times less powerful.¬†",
          "score": 1,
          "created_utc": "2026-01-27 19:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22p9xa",
          "author": "isleeppeople",
          "text": "Maybe not applicable but I want my machine to be an expert about itself to help me troubleshoot and add its own integrations that match my stack. It also helps to keep track of upgrades if I break something. I have a corpora of everything I use gitingest, readme docs, etc. I have it ingested in qdrant. Seems like you could do the same thing in your situation. Not an expert coder for everything but you might be able to make it as good as Claude in this one particular area.",
          "score": 1,
          "created_utc": "2026-01-27 19:36:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22tm19",
          "author": "passive_interest",
          "text": "I went this route on my M4 - developed a service that plans and applies atomic commits via local models & Ollama, but the round trip was painfully slow compared to having a Claude subagent or Codex skill perform the same task.",
          "score": 1,
          "created_utc": "2026-01-27 19:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22y83z",
          "author": "Stargazer1884",
          "text": "Before you go down the local route...(which I love, but it's not currently comparable to a frontier model on cloud) try using Opus to plan and Sonnet to execute the individual tasks.",
          "score": 1,
          "created_utc": "2026-01-27 20:16:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22zn8z",
          "author": "Terminator857",
          "text": "Comments in this poll might help answer: [https://www.reddit.com/r/LocalLLaMA/comments/1qj935h/poll\\_when\\_will\\_we\\_have\\_a\\_30b\\_open\\_weight\\_model\\_as/](https://www.reddit.com/r/LocalLLaMA/comments/1qj935h/poll_when_will_we_have_a_30b_open_weight_model_as/)",
          "score": 1,
          "created_utc": "2026-01-27 20:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wpgh",
          "author": "Ok_Chef_5858",
          "text": "If you're hitting limits that fast, you might want to just bring your own API keys instead of the $20 plan. Way more control over costs. I work on a project in VS Code, using Kilo Code atm and for local models specifically, Qwen Coder or DeepSeek R1 are solid options, but honestly nothing fully matches Claude yet. Best bet is mixing local for simple stuff and cloud for complex.",
          "score": 1,
          "created_utc": "2026-01-28 10:35:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27spjs",
              "author": "thecrogmite",
              "text": "I wondered if getting an API key was also smarter...",
              "score": 1,
              "created_utc": "2026-01-28 14:08:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2e52oc",
                  "author": "Ok_Chef_5858",
                  "text": "for me it sure is ...",
                  "score": 1,
                  "created_utc": "2026-01-29 11:23:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27w3qo",
          "author": "XccesSv2",
          "text": "If your are not just looking into local models and a opinion is to switch the cloud provider I would suggest GLM 4.7 Coding plan is a good choice for you. Cheaper, nearly as good as Sonnet 4.5 in most tasks and more usage before hitting rate limits. Also api keys are included in the plan so you can use that GLM models anywhere you want.",
          "score": 1,
          "created_utc": "2026-01-28 14:25:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2804kc",
              "author": "thecrogmite",
              "text": "Good to know, thank you! You're the second person to suggest GLm 4.7 Coding.",
              "score": 1,
              "created_utc": "2026-01-28 14:45:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ai0vi",
          "author": "Remarkable-Jump-6227",
          "text": "Kimi 2.5 from moonshot ai  just came out , I‚Äôm using it with opencode because my Claude code limits have also hit and I have to say it‚Äôs definitely not bad! Also got 5.2 codex model is superior in a coding sense, Claude code works the best in an agentic loop but my god 5.2-codex model is easily just as good if not better when it comes to coding.",
          "score": 1,
          "created_utc": "2026-01-28 21:21:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2davuh",
          "author": "Apex-PC-Lab-CEO",
          "text": "GLM 4.7",
          "score": 1,
          "created_utc": "2026-01-29 06:53:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ez4lz",
          "author": "wedgehack-gm",
          "text": "Just use claude code locally with ollama configured for qwen3-coder,, gpt-oss, deepseek or any other model that fits in your memory.  See which one works for you.",
          "score": 1,
          "created_utc": "2026-01-29 14:28:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r0n45",
          "author": "PaddingCompression",
          "text": "You're unlikely to be able to get cheaper than Claude Max with electricity costs alone, it's being crazily subsidized.  Truly caring about data sovereignty, or wanting to tinker and gain experience running it yourself, is the only real justification at current prices.",
          "score": 1,
          "created_utc": "2026-01-31 06:45:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22vnc1",
          "author": "bakawolf123",
          "text": "I'd suggest antigravity/gemini-cli as an option. Beside Gemini Pro it even has Opus included and while the limit for latter is quite short Gemini itself is good enough.   \n  \nAs for local models, current best small model for agent is GLM4.7 Flash. However Macs are really bad with prefill/prompt processing and afaik M3 PRO also screwed up architecture (lower memory bandwith) so it is worse than M1 Pro https://github.com/ggml-org/llama.cpp/discussions/4167. Current harnesses all start with 10-15k token system prompt so it feels quite garbage.  \nThere's hope for M5 with Pro/Max and possibly even Ultra coming this year though.",
          "score": 1,
          "created_utc": "2026-01-27 20:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23jh1f",
          "author": "Decent-Freedom5374",
          "text": "I train Sensei off Claude and codex, he‚Äôs an intelligent layer that orchestrates my ollama models, with the ability to rag anything that Claude and codex does he works just as good as them! :)",
          "score": 1,
          "created_utc": "2026-01-27 21:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23d2tk",
          "author": "Christosconst",
          "text": "Sonnet 4.5 is good and cheaper than opus",
          "score": 0,
          "created_utc": "2026-01-27 21:22:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2251c6",
          "author": "Jackster22",
          "text": "GLM 4.5 has been great with Claude via their own cloud service.\n$6 a month option, currently 50% off for the first month, gives you more than the Claude $20 subscription.\n\nI had done 200,000,000 tokens in the past 24 hours and it has been solid. No time outs no nothing.\n\nhttps://z.ai/subscribe?ic=SNK0LAU2OF\n\nYou can self hosted but why bother when it is this cheap and so much faster...",
          "score": -6,
          "created_utc": "2026-01-27 18:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22mzol",
              "author": "thecrogmite",
              "text": "I'll take a look, is that some sort of affiliate link?",
              "score": 1,
              "created_utc": "2026-01-27 19:25:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22tru7",
                  "author": "btc_maxi100",
                  "text": "it's a paid shill",
                  "score": 3,
                  "created_utc": "2026-01-27 19:55:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o237xcu",
                  "author": "Jackster22",
                  "text": "It is, we both get a couple pennies each time. I only shill for products I actually use.  [https://i.postimg.cc/9F7xYyLZ/image.png](https://i.postimg.cc/9F7xYyLZ/image.png)",
                  "score": 0,
                  "created_utc": "2026-01-27 20:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpbxkh",
      "title": "You can now run Kimi K2.5 on your local device!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/nwp8ammpf3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 13:52:53",
      "score": 31,
      "num_comments": 7,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpbxkh/you_can_now_run_kimi_k25_on_your_local_device/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2857sq",
          "author": "Egoz3ntrum",
          "text": "If you own a nuclear plant!",
          "score": 13,
          "created_utc": "2026-01-28 15:10:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o285qc5",
              "author": "yoracale",
              "text": "Runs on a 256ram Mac actually!\n\nAnd you can run it on lower requirements, it'll just be much slower",
              "score": 2,
              "created_utc": "2026-01-28 15:12:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jk8o5",
          "author": "ihatebadpe0ple",
          "text": "![gif](giphy|Y5PnpRvm8PVba)",
          "score": 3,
          "created_utc": "2026-01-30 04:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28n6u1",
          "author": "yomohiroyuzuuu",
          "text": "So my old MacBook is definitely inadequate‚Ä¶",
          "score": 2,
          "created_utc": "2026-01-28 16:29:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bd0jo",
              "author": "yeet5566",
              "text": "It isn‚Äôt the MacBook that is inadequate it‚Äôs your patience that is‚Ä¶",
              "score": 4,
              "created_utc": "2026-01-28 23:47:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dpvf7",
          "author": "Kulletrops",
          "text": "How speedy on  rx 9070 xt 16 GB VRAM, 32 GB 5600 MHz ram, 2 TB SSD 6800 WR / 7500 RD ???",
          "score": 1,
          "created_utc": "2026-01-29 09:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l4yc2",
              "author": "yoracale",
              "text": "Oooo maybe like 2 tokens/s? Not fast unfortunately",
              "score": 1,
              "created_utc": "2026-01-30 11:45:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpp820",
      "title": "ClawdBot / MoltBot",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qpp820/clawdbot_moltbot/",
      "author": "Normal-End1169",
      "created_utc": "2026-01-28 21:51:59",
      "score": 31,
      "num_comments": 45,
      "upvote_ratio": 0.86,
      "text": "Just stumbled across this tool today from my Co Founder in one of my startups so being techy I decided to give it a quick peak.\n\nAm I missing understanding the purpose of the tool? We're running a local process that is interacting with external AI APIs to run local tasks that actively interact with your file system????? I mean cool I guess but one doesn't sound to safe, and 2 all your local data is ending up on a server somewhere.\n\nSeriously even tried to create some sort of use case, maybe help me with file sorting on a Linux machine, managing servers but it just feels so wrong personally.\n\nMaybe someone can enlighten me because I don't fully understand why you would want a AI actively interacting with your entire file system.  \n  \n ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpp820/clawdbot_moltbot/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o2aqsex",
          "author": "Nzkx",
          "text": "I can see 2 easy disasters from such project :\n\n\\- Data send through public network on your behalf, when you want to keep them private.  \n\\- Data erased or altered, when you certainly don't want to erase or modify them.\n\nI guess we can assume the dev ain't that naive, and put railguard against dangerous commands. But nothing can be certain unless you deep dive into the source code and the docs. I would expect something like a notification to allow/disallow dangerous commands when the agent is about to perform weird stuff, but this quickly get annoying if your phone start to ring at 4am because the agent is stuck waiting for your input.\n\nRegular backup would help a lot against the second problem.",
          "score": 10,
          "created_utc": "2026-01-28 21:59:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2as7t1",
              "author": "Normal-End1169",
              "text": "Well even then, it's not like majority of users are using local LLMs ran from like Ollama. There going to be connecting to external API's like OpenAI, so in theory your entire PC is accessible and sent to a OpenAI server somewhere.\n\nIn terms of Data privacy and exactly what you said users data being sent away when they aren't aware or didn't want to is crazy in my mind.\n\nCool tool, but seems like a massive issue just waiting to happen.",
              "score": 1,
              "created_utc": "2026-01-28 22:05:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dhj0c",
                  "author": "evilbarron2",
                  "text": "If you‚Äôre concerned about privacy, wouldn‚Äôt you be avoiding *any* api LLM? These systems are literally data collection machines, right? Isn‚Äôt collecting uniquely identifying data core to how they work?",
                  "score": 3,
                  "created_utc": "2026-01-29 07:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2b6s7l",
                  "author": "Outdatedm3m3s",
                  "text": "That‚Äôs why alot of people are buying dedicated computers for these with their own accounts for everything. This way they aren‚Äôt using any of your information.",
                  "score": 1,
                  "created_utc": "2026-01-28 23:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lb3tb",
              "author": "Amit-NonBioS-AI",
              "text": "I think people are waking up to the security nightmare that this project is and there are options coming up which make it pretty safe. I think installing it on the cloud on a private VM and using OpenRouter with credits gatekeeping is a very safe way to give it a shot. \n\nI work for a vibecoding platform and we offer a private ubuntu VM in the cloud for all of our users and the agent has root access on the VM. We are getting a ton of users who are asking our agent to setup clawdbot and give it a spin. Given that clawdbot is not that mature - this is a very easy way for non technical people to set it up - as the agent does all of the work. \n\nThe bigger issue that most people are facing with this setup, is not the security, but what to do once they set it up. They do all the work to get it running, but then what do you do with it. No one is ready to give it access to their email/calendar or any other account. So they get frustrated and log off. \n\nI think there is a lot of hype around it - and not much uses for it. The security situation is not that hard to handle imo",
              "score": 1,
              "created_utc": "2026-01-30 12:29:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2apdrw",
          "author": "Normal-End1169",
          "text": "On top of all this I mean we're in theory letting publicly open chats / AI access interact with local system.\n\nCan't even imagine all the CVE's coming out for this in less than a month time",
          "score": 6,
          "created_utc": "2026-01-28 21:53:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2arq93",
              "author": "colin_colout",
              "text": "yep... not in theory.  The vulnerabilities are VERY practical.  2026 will be \"The year of the prompt injection email phishing\".  \n\nJust watch Low Level's video [https://www.youtube.com/watch?v=kSno1-xOjwI](https://www.youtube.com/watch?v=kSno1-xOjwI) and you'll see why.",
              "score": 11,
              "created_utc": "2026-01-28 22:03:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2aslnc",
                  "author": "Normal-End1169",
                  "text": "Yup, this is insane that we are actively opening any device up to prompt injection.\n\nI actually am in school for cyber and this is just mind blowing.",
                  "score": 5,
                  "created_utc": "2026-01-28 22:07:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2arsc4",
          "author": "pandodev",
          "text": "I think what's cool about it also makes it completely unsafe and not to be used which is it has access to the whole environment machine and tweak itself. so giving it its own GitHub for example and telling it to do projects for you, or set cron jobs to give you news basically like a true assistant but is nothing insane. Also NOONE should be running this on their home network, best it to securely sandbox it in an ec2 with only able to access via sessionmanager. Maybe I am too paranoid but this uses dependencies if any of those dependencies get infiltrated would you rather them be in a aws secluded server or your home network?",
          "score": 3,
          "created_utc": "2026-01-28 22:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bp354",
              "author": "eli_pizza",
              "text": "Not paranoid enough if you‚Äôre giving it access to anything at all that you care about (like your email inbox or text messages!). \n\nYou‚Äôre worried about software supply chain attacks, but the app itself is fundamentally insecure.",
              "score": 2,
              "created_utc": "2026-01-29 00:50:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2astjo",
              "author": "Normal-End1169",
              "text": "I think maybe if your using Local LLMS at least your removing the external data issue, but even then your giving a chat full control over your PC. This is not safe on a EC2 or any sort of VPS lol.\n\nYou would trust a chatbot to be ran on a publicly accessible machine that has full access to a machine??",
              "score": 1,
              "created_utc": "2026-01-28 22:08:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2az6e0",
                  "author": "pandodev",
                  "text": "if is a secluded EC2 or for the sole purpose not publicly accessible like I said with session manager how? with local llm in your home network will not mean anything if dependencies in the library are infiltrated you just gave the hacker access to your entire home network. that's not the case with a secluded vps all they would be able to access is secrets and API Keys that are stored in that machine and that machine alone so like your Claude api key.",
                  "score": 2,
                  "created_utc": "2026-01-28 22:37:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2axorp",
          "author": "Echo_OS",
          "text": "This is why some people prefer tiny / narrow models.\nNot because they're smarter, but because the responsibility radius is small.\n\nClear \"can't do\" > more capability.\nBounded agents are easier to trust than general ones with full FS access.",
          "score": 3,
          "created_utc": "2026-01-28 22:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b0zmk",
          "author": "brimanguy",
          "text": "That's crazy ... Might as well run a local server and make every directory and sub directory public to the internet ü§£",
          "score": 2,
          "created_utc": "2026-01-28 22:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b5412",
          "author": "Normal-End1169",
          "text": "Going to keep this as a bit of a time piece and edit as I go;\n\nHere's a list of articles / videos regarding security issues so far;\n\n[https://www.youtube.com/watch?v=7GS6Xs4hdvg](https://www.youtube.com/watch?v=7GS6Xs4hdvg)  \n[https://www.aikido.dev/blog/fake-clawdbot-vscode-extension-malware](https://www.aikido.dev/blog/fake-clawdbot-vscode-extension-malware)  \n[https://www.youtube.com/watch?v=kSno1-xOjwI](https://www.youtube.com/watch?v=kSno1-xOjwI) (Thanks to colin\\_colout for sharing)  \n[https://lukasniessen.medium.com/clawdbot-setup-guide-how-to-not-get-hacked-63bc951cbd90](https://lukasniessen.medium.com/clawdbot-setup-guide-how-to-not-get-hacked-63bc951cbd90)\n\nUser in the form shared this thread; https://www.reddit.com/r/vibecoding/s/mdJGdoiI2k (Base64 encoded commands pointing to unknown suspicious IP addresses)",
          "score": 1,
          "created_utc": "2026-01-28 23:06:56",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2czv16",
              "author": "whatever",
              "text": "Malware (and crypto pump&dump scams, and whatever else) that leverage the branding of the project are only tangentially related to MoltBot, and I wouldn't count them as security issue in MoltBot, nor as evidence that MoltBot's approach is bad.",
              "score": 1,
              "created_utc": "2026-01-29 05:26:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dyv3l",
                  "author": "Normal-End1169",
                  "text": "Your correct, but I am still going to link it as the fact hackers are already abusing it and it‚Äôs fairly new interest me.\n\nI think skipping over that is unfair to understanding how this is currently going and may help inform readers how it can go and what‚Äôs going on so far",
                  "score": 1,
                  "created_utc": "2026-01-29 10:31:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2b6b4b",
          "author": "Momsbasement13",
          "text": "The catch 22 with this bot is that if you seclude it within a sandbox it kinda loses the majority of the features making it stand out. For clawd/molt to actually shine, it requires to have all this access otherwise it is just another slightly different LLM.  \nI feel like we currently lack proper control and railguards for agents like these to be mainstream. A lot of people will pay in blood for it to be.",
          "score": 1,
          "created_utc": "2026-01-28 23:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6s8a",
              "author": "Normal-End1169",
              "text": "We'll like others mentioned the main issue is prompt injection;\n\nOnce prompt injection exists and is taken more serious it may be a consideration for actual use but now this  such a dumb idea to use. For example in one of the videos linked in this thread, the creator talks about his buddy who had his email connected which then he proceeded to email his email from his wife's account pretty much saying it's him and asking to turn of specific music on spotify which it proceeded to do.\n\nWhat's stopping someone from using the same method to read out sensitive files.\n\nNot to mention all the APIs/Credentials used to connect the external applications are stored in plane text.",
              "score": 1,
              "created_utc": "2026-01-28 23:15:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2be6au",
          "author": "HealthyCommunicat",
          "text": "Your explanation is literally all it is, I can swear and bet everything I own that the only actual reason this got popular is because it has a GUI. There have been so many options to do literally the exact same thing clawdbot is able to do - I say this LITERALLLY because I had something NEAR EXACTLY the same, a slack bot that‚Äôs hooked up to a llm endpoint thats running on the same machine able to act autonomously - in fact, you literally wouldn‚Äôt be able to tell the two apart. I moved over to clawdbot because it has more integration but everyone is hyping it up when they 1.) dont even realize you‚Äôve been able to do this for the past year. 2.) they think its something new and great because it has a GUI.\n\nI love watching people jump into setting up clawdbot thinking its a magic tool that will solve all their answers and needs, only for them to realize you need to setup tools, literally the same exact way you would need to for any other automation. People literally do not want to learn and keep wanting the fastest easiest cheatiest way to have fully autonomous AI that is tailored for their own needs, while not willing to actually learn or even understand what it is they need to do.",
          "score": 1,
          "created_utc": "2026-01-28 23:53:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bekym",
              "author": "Normal-End1169",
              "text": "That was my thought process. Seems like a over glorified MCP lol.\n\nJust AI interacting with data, expect this time it's not just a email, or maybe a CRM. It's a MACHINE!\n\nCouldn't tell you about older stuff like this tho never really went beyond MCP.\n\nWait till ppl find out 95% of these automations can just be coded with simple languages like python.",
              "score": 2,
              "created_utc": "2026-01-28 23:56:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2bfj5p",
                  "author": "HealthyCommunicat",
                  "text": "Its exactly that, just a ton of single file mcp tools made in python. All these new terminology and unnecessary lingo and overhyping is making clawdbot out to be something that hasnt already existed for so long. It just irks me in a wrong way when I keep seeing people claiming out of pure arrogance and ignorance that something is amazing and new when it‚Äôs been around for so long, if they bothered to learn in the first place",
                  "score": 2,
                  "created_utc": "2026-01-29 00:01:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cup6o",
          "author": "misterdonut11331",
          "text": "careful it might be malware\n\nhttps://www.reddit.com/r/vibecoding/s/nnC52vh3tQ",
          "score": 1,
          "created_utc": "2026-01-29 04:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2pvo",
          "author": "Sneaky_TMcD",
          "text": "Best evidence yet of AGI.  Procreation through vast token consumption.",
          "score": 1,
          "created_utc": "2026-01-29 05:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e2yeq",
          "author": "danny_094",
          "text": "The API issue really shocked me. The communication is terrible. Imagine using it for coding and it gets stuck in a loop overnight. Your credit card will be thrilled.The concept isn't really impressive. The infinite memory means that the AI ‚Äã‚Äãhas to load 200k tokens before each answer, which also incurs costs. Imagine you ask, \"Do I have an appointment today?\" and it costs you $3 because of the API.",
          "score": 1,
          "created_utc": "2026-01-29 11:06:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2en0wa",
              "author": "Normal-End1169",
              "text": "Not to mention the credentials are stored in plain text too lol, so even if you we‚Äôre compromised now they can exfiltrate your APIs for whatever service your doing. Ppl going to love waking up to 1000$ OpenAI bills üòÇ",
              "score": 1,
              "created_utc": "2026-01-29 13:24:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ena78",
                  "author": "danny_094",
                  "text": "The main thing is that the hype users are surprised when the bubble bursts.\n...",
                  "score": 1,
                  "created_utc": "2026-01-29 13:25:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2g7z56",
          "author": "Loose-Doubt-4421",
          "text": "Have you ever wished your Chatgpt or Claude had access to a full computer ? This is exactly that. No need to overthink it.\n\nClaude + computer + skills = good (skills are just markdown \"tutorial\" files and script examples)\n\nThis is not designed to be installed in your main system. Buy a raspberry or a start a VM. (Lume is great for mac os VMs)\n\nSome people will have 0 use cases, some will have many.",
          "score": 1,
          "created_utc": "2026-01-29 17:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hgruq",
              "author": "Normal-End1169",
              "text": "No I have never wished that the external applications I do not control had full access to my computer....",
              "score": 1,
              "created_utc": "2026-01-29 21:22:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2iw3n9",
                  "author": "Loose-Doubt-4421",
                  "text": ">This is not designed to be installed in your main system. Buy a raspberry or a start a VM. (Lume is great for mac os VMs)",
                  "score": 1,
                  "created_utc": "2026-01-30 01:49:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cxkx4",
          "author": "Tall_Instance9797",
          "text": "It's really for people with both the imagination and known how for what to do with it and the expertise to implement it securely and effectively. It's like you woke up and now you can just download a lambo. Some people are advanced drivers and already doing doughnuts and drifting... others are looking at it thinking \"Looks dangerous, with my driving skills I'd probably kill myself in that.\" If you're someone who doesn't 'understand why you'd want something like that' it's not for you. It's for serious power users and people who know what they're doing.",
          "score": -1,
          "created_utc": "2026-01-29 05:10:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qljfuw",
      "title": "AI & ML Weekly ‚Äî Hugging Face Highlights",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "author": "techlatest_net",
      "created_utc": "2026-01-24 10:16:03",
      "score": 28,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "Here are the most notable¬†**AI models released or updated this week on Hugging Face**, categorized for easy scanning üëá\n\n# Text & Reasoning Models\n\n* **GLM-4.7 (358B)**¬†‚Äî Large-scale multilingual reasoning model¬†[https://huggingface.co/zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)\n* **GLM-4.7-Flash (31B)**¬†‚Äî Faster, optimized variant for text generation¬†[https://huggingface.co/zai-org/GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)\n* **Unsloth GLM-4.7-Flash GGUF (30B)**¬†‚Äî Quantized version for local inference¬†[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n* **LiquidAI LFM 2.5 Thinking (1.2B)**¬†‚Äî Lightweight reasoning-focused LLM¬†[https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)\n* **Alibaba DASD-4B-Thinking**¬†‚Äî Compact thinking-style language model¬†[https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking](https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking)\n\n# Agent & Workflow Models\n\n* **AgentCPM-Report (8B)**¬†‚Äî Agent model optimized for report generation¬†[https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n* **AgentCPM-Explore (4B)**¬†‚Äî Exploration-focused agent reasoning model¬†[https://huggingface.co/openbmb/AgentCPM-Explore](https://huggingface.co/openbmb/AgentCPM-Explore)\n* **Sweep Next Edit (1.5B)**¬†‚Äî Code-editing and refactoring assistant¬†[https://huggingface.co/sweepai/sweep-next-edit-1.5B](https://huggingface.co/sweepai/sweep-next-edit-1.5B)\n\n# Audio: Speech, Voice & TTS\n\n* **VibeVoice-ASR (9B)**¬†‚Äî High-quality automatic speech recognition¬†[https://huggingface.co/microsoft/VibeVoice-ASR](https://huggingface.co/microsoft/VibeVoice-ASR)\n* **PersonaPlex 7B**¬†‚Äî Audio-to-audio personality-driven voice model¬†[https://huggingface.co/nvidia/personaplex-7b-v1](https://huggingface.co/nvidia/personaplex-7b-v1)\n* **Qwen3 TTS (1.7B)**¬†‚Äî Custom & base voice text-to-speech models¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign)\n* **Pocket-TTS**¬†‚Äî Lightweight open TTS model¬†[https://huggingface.co/kyutai/pocket-tts](https://huggingface.co/kyutai/pocket-tts)\n* **HeartMuLa OSS (3B)**¬†‚Äî Text-to-audio generation model¬†[https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B](https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B)\n\n# Vision: Image, OCR & Multimodal\n\n* **Step3-VL (10B)**¬†‚Äî Vision-language multimodal model¬†[https://huggingface.co/stepfun-ai/Step3-VL-10B](https://huggingface.co/stepfun-ai/Step3-VL-10B)\n* **LightOnOCR 2 (1B)**¬†‚Äî OCR-focused vision-language model¬†[https://huggingface.co/lightonai/LightOnOCR-2-1B](https://huggingface.co/lightonai/LightOnOCR-2-1B)\n* **TranslateGemma (4B / 12B / 27B)**¬†‚Äî Multimodal translation models¬†[https://huggingface.co/google/translategemma-4b-it](https://huggingface.co/google/translategemma-4b-it)¬†[https://huggingface.co/google/translategemma-12b-it](https://huggingface.co/google/translategemma-12b-it)¬†[https://huggingface.co/google/translategemma-27b-it](https://huggingface.co/google/translategemma-27b-it)\n* **MedGemma 1.5 (4B)**¬†‚Äî Medical-focused multimodal model¬†[https://huggingface.co/google/medgemma-1.5-4b-it](https://huggingface.co/google/medgemma-1.5-4b-it)\n\n# Image Generation & Editing\n\n* **GLM-Image**¬†‚Äî Text-to-image generation model¬†[https://huggingface.co/zai-org/GLM-Image](https://huggingface.co/zai-org/GLM-Image)\n* **FLUX.2 Klein (4B / 9B)**¬†‚Äî High-quality image-to-image models¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-4B](https://huggingface.co/black-forest-labs/FLUX.2-klein-4B)¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n* **Qwen Image Edit (LoRA / AIO)**¬†‚Äî Advanced image editing & multi-angle edits¬†[https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA](https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA)¬†[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO)\n* **Z-Image-Turbo**¬†‚Äî Fast text-to-image generation¬†[https://huggingface.co/Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)\n\n# Video Generation\n\n* **LTX-2**¬†‚Äî Image-to-video generation model¬†[https://huggingface.co/Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)\n\n# Any-to-Any / Multimodal\n\n* **Chroma (6B)**¬†‚Äî Any-to-any multimodal generation¬†[https://huggingface.co/FlashLabs/Chroma-4B](https://huggingface.co/FlashLabs/Chroma-4B)",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qoyoty",
      "title": "Why don‚Äôt most programmers fine-tune/train their own SLMs (private small models) to build a ‚Äúlibrary-expert‚Äù moat?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qoyoty/why_dont_most_programmers_finetunetrain_their_own/",
      "author": "Outside-Tax-2583",
      "created_utc": "2026-01-28 02:27:44",
      "score": 24,
      "num_comments": 39,
      "upvote_ratio": 0.73,
      "text": "AI coding tools are rapidly boosting development productivity and continually driving ‚Äúcost reduction and efficiency gains,‚Äù reshaping how programmers work. At the same time, programmers are often heavy users of these tools.\n\nHere‚Äôs my observation:\n\n* Most programmers may not be ‚Äúarchitect-level,‚Äù but many are power users of specific libraries/frameworks‚Äîtrue ‚Äúlib experts.‚Äù They know the APIs, best practices, common pitfalls, version differences, and performance/security boundaries inside out.\n* In theory, they could turn that expertise into data assets: for example, curate **1,000‚Äì5,000 high-quality samples** from real projects‚Äî‚Äúbest usage patterns, common mistakes, debugging paths, migration guides, performance optimizations, FAQs, code snippets + explanations.‚Äù\n* Then, by lightly fine-tuning or aligning an open-source base model (an SLM), they could create a ‚Äúlibrary-specialist model‚Äù that serves only that lib‚Äîforming a new moat in the AI era: **better than general LLMs for that library, closer to one‚Äôs engineering habits, more controllable, and more reusable.**\n\nBut in reality, very few developers actually do this.\n\nSo I‚Äôd love to hear from experienced engineers:\n\n1. Is this path **theoretically viable**? With **1,000‚Äì5,000 samples**, can fine-tuning reliably improve a model into a solid ‚Äúlibrary expert assistant‚Äù?\n2. What‚Äôs the main reason people don‚Äôt do it‚Äî**technical barriers** (data curation/training/evaluation/deployment), **ROI** (easier to use existing tools), or **lack of good tooling** (dataset management, evaluation, continuous iteration, private deployment)?\n3. If you think it‚Äôs viable, could you share a more **engineering-oriented, practical path** to make it work?\n\nI‚Äôm especially looking for hands-on, real-world answers‚Äîideally from people who‚Äôve done fine-tuning, private knowledge systems, or enterprise model deployments.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qoyoty/why_dont_most_programmers_finetunetrain_their_own/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o2570zq",
          "author": "CuteLewdFox",
          "text": "Because it's way easier to just use a RAG (or something similar to feed data to the model) instead of fine-tuning models. It's also way faster and more energy efficient. And doesn't need to be re-done on every new model release. Simply switch the model, and you're done (most of the time, not always).\n\nNote: I've built machine learning frameworks and applications for a few years now at my current job, and doing a lot of things with SLMs at home.",
          "score": 31,
          "created_utc": "2026-01-28 02:52:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o275g14",
              "author": "ScoreUnique",
              "text": "Hey, do you have some workflows to share ? Thanks.",
              "score": 2,
              "created_utc": "2026-01-28 11:47:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2cfx9y",
              "author": "ihatebadpe0ple",
              "text": "I can't chat with you, what is your github pls?",
              "score": 0,
              "created_utc": "2026-01-29 03:18:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cvo08",
                  "author": "CuteLewdFox",
                  "text": "My private stuff isn't accessible via GitHub.",
                  "score": 2,
                  "created_utc": "2026-01-29 04:57:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25bwiw",
          "author": "catplusplusok",
          "text": "RAG is more reliable for most tasks and doesn't risk breaking the model. The problem with fine-tuning is the quality issues are often not obvious until some time after starting to use the model, like it suddenly forgetting the context because you didnt have enough long samples",
          "score": 9,
          "created_utc": "2026-01-28 03:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25d7pd",
              "author": "Outside-Tax-2583",
              "text": "That makes me think: if I could capture the programming process content from Claude Code, and then automatically filter and categorize it, wouldn‚Äôt it be fascinating to fine-tune SLMs automatically?",
              "score": 0,
              "created_utc": "2026-01-28 03:26:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2554zt",
          "author": "Juan_Valadez",
          "text": "Because they're programming, not doing AI. I want to make a sandwich, not sow the tomato.",
          "score": 16,
          "created_utc": "2026-01-28 02:42:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25asj8",
              "author": "Outside-Tax-2583",
              "text": "If everyone ends up relying on the same general-purpose models and toolchains, the outcome is likely **rapid capability convergence**: outputs become increasingly similar and differentiation shrinks. Competition then shifts from ‚Äúwho‚Äôs more expert and more systematic‚Äù to ‚Äúwho‚Äôs cheaper and faster,‚Äù pushing the market toward **price wars** rather than genuine capability-based competition.",
              "score": -5,
              "created_utc": "2026-01-28 03:13:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o271l75",
                  "author": "Limebird02",
                  "text": "Whilst this may be likely, this isn't bad, this lowers cost for everybody else. Software has a very healthy ecosystem and other things evolve to compete, perhaps better than any other area.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:16:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25umo5",
                  "author": "elbiot",
                  "text": "Fine tuning a SLM won't be better than using a huge one with billions of dollars in development behind it",
                  "score": 4,
                  "created_utc": "2026-01-28 05:13:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2576nc",
          "author": "Odhdbdyebsksbx",
          "text": "What are the incentives for the programmers to do this? They're already an expert in the library, so obviously not for their own self use. Charge as a service for other people, seems kinda niche unless there's like a marketplace platform.",
          "score": 11,
          "created_utc": "2026-01-28 02:53:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25agb2",
              "author": "Outside-Tax-2583",
              "text": "I‚Äôve been thinking along the same lines: npm has roughly 3.8 million packages, but the people who truly understand a given library are usually its maintainers and a small group of core contributors. Since their knowledge often far exceeds that of a general-purpose LLM‚Äîcovering design trade-offs, edge cases, version evolution, best practices, and common pitfalls‚Äîwhy don‚Äôt we see them productizing this advantage more proactively in two directions?\n\n\n\n1. **Library-specific SLMs / model plugins**: distilling authoritative usage, migration guides, performance/security constraints, and common fixes into callable capabilities‚Äîso users get more reliable and consistent guidance.\n2. **Library-specific agent services**: delivering an end-to-end ‚Äúgenerate + verify‚Äù loop‚Äîauto-generate examples and validate them, automate cross-version upgrades, run lint/compat/security checks, pre-review PRs, etc.‚Äîsold via subscription or outcome-based pricing.\n\n\n\n\n\nMy intuition is that once these capabilities can be delivered in a standardized way, high-quality library teams would benefit disproportionately. Take a small team like Tailwind CSS: ‚Äúauthoritative knowledge + automated verification‚Äù could become a scalable service‚Äîsmoother monetization, more consistent UX, and lower support costs‚Äîrather than being reduced to a mere upstream data source for model giants.\n\n\n\nThe key question is: what‚Äôs the real friction? Maintainer bandwidth and ROI, heavy engineering and tooling requirements, high distribution/ops costs, or open-source community norms around commercial boundaries? This feels like a direction worth systematically exploring and validating.",
              "score": -8,
              "created_utc": "2026-01-28 03:11:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o25av3e",
                  "author": "SashaUsesReddit",
                  "text": "Is this response from AI?",
                  "score": 13,
                  "created_utc": "2026-01-28 03:13:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25ltka",
          "author": "Diligent-Union-8814",
          "text": "Any handful way to do this? Such as, running a single command produces the fine-tuned model.",
          "score": 3,
          "created_utc": "2026-01-28 04:16:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25eear",
          "author": "HealthyCommunicat",
          "text": "I‚Äôve literally been doing exactly this. Got hired for a company that handles Oracle stuff. They have over 5000 guides and documents altogether of manageengine ticket solutions, guides, procedures, etc. - I literally think of it as a goldmine of high quality data when it comes to anything Oracle as its the accumulation of 8-10 years of customer service and technical support. Nobody has an Oracle expert model simply because Oracle would not approve of that whatsoever, so I‚Äôm doing the next best thing and just making one. Hopefully goes better than I expect, but we‚Äôre still formatting and organizing the massive library of examples. It‚Äôs been nonstop trial and error trying to figure out what kind of examples should count towards ‚Äúpre-training‚Äù using different groups of data to see what kind of outcome I get. It‚Äôs been insane amounts of trial and error just being the only person working on this in the past 2 months, if I‚Äôm honest I don‚Äôt have any true hope of for sure making something ususable.\n\nWorst case, I‚Äôll just go finetune a qwen moe variant.",
          "score": 6,
          "created_utc": "2026-01-28 03:33:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25sjxh",
              "author": "Torodaddy",
              "text": "Feels like you are building a better seat for a horse and buggy",
              "score": 3,
              "created_utc": "2026-01-28 04:59:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25mafg",
              "author": "Vegetable-Score-3915",
              "text": "Awesome and best of luck!\nFeel free to share when you have progress.",
              "score": 2,
              "created_utc": "2026-01-28 04:19:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ai3t7",
              "author": "ithkuil",
              "text": "Have you tried RAG? And benchmark it against Tavily search on the same questions.¬†",
              "score": 1,
              "created_utc": "2026-01-28 21:21:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o259uq3",
          "author": "pinmux",
          "text": "Fine tuning needs a lot more memory than inference. ¬†Most people don‚Äôt own this kind of compute. ¬†Fine tuning takes quite a bit of time and if you‚Äôre renting enough GPU to do it, isn‚Äôt $0 and for small to medium sized models may easily run to hundreds or thousands of dollars (depending on how it‚Äôs done).\n\nThen, generating the thousands of inputs to perform the fine tuning also isn‚Äôt easy, cheap, or seemingly well understood by many people.¬†\n\nIt‚Äôs definitely interesting! ¬†It definitely could be powerful! ¬†But there doesn‚Äôt seem to be much publicity written about people doing it, yet.¬†",
          "score": 5,
          "created_utc": "2026-01-28 03:07:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25lsfb",
              "author": "Vegetable-Score-3915",
              "text": "Unsloth.ai and other approaches ie using quantised models exist to get away with fine-tuning with less resources, ie free tier Google Colab, Kaggle Notebooks etc.\n\nNot saying this as a counter point, what you have written is generally valid. But can get away with 16gb vram for smaller models.\n\nDeeplearning.ai has at least 1 good short course you can take for free showing how to finetune. Doesn't take long, just recommend a coffee and chocolate snacks to get through it in one sitting.\n\nOther startups are trying to make it easier to fine tune slms as well and tend to let you try it out for free. Distil Labs is one thst looks promising.\n\nI think Synalinks has also produced a similiar product very recently. Again making it easier to set things up.\n\nThere is plenty of scope for fine-tuning SLMs to become more of a norm. It will come down to the particular situation, but learning the code base, knowing the intended approach for the architecture etc, I imagine it would make sense as a viable option for what OP is describing, ie code review tasks etc. Fine tuned slms can be used as part of a range of models doing different things, ie could be used concurrently with larger more general models.",
              "score": 3,
              "created_utc": "2026-01-28 04:16:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27omwk",
                  "author": "pinmux",
                  "text": "Definitely all good points.  For small models <10B parameters or if QLoRA gives good results, then definitely Colab/Kaggle/home-lab GPUs could work well.\n\nCurrently, I view the 20-30B parameter models as being as small as I'd want to use for real work.  Things like devstral-small-2 or glm-4.7-flash look to have real promise, so fine tuning from those is quite interesting to me.\n\nI'm still learning a ton about this.  Like the OP, I don't understand why this isn't a more talked about idea.  At the very least, it seems like taking a small model and doing this kind of fine tuning and writing about it would be a great way for a new researcher to start to get noticed.",
                  "score": 2,
                  "created_utc": "2026-01-28 13:47:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o258rkr",
          "author": "Lame_Johnny",
          "text": "Out of the box coding models can usually do it well enough with a little guidance and documentation",
          "score": 2,
          "created_utc": "2026-01-28 03:02:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25rxn7",
          "author": "Torodaddy",
          "text": "Whats the point? You'll never curate more examples than the llm has already seen and what is the incremental value from that, even 5000 examples are small potatoes against something trained on all of github",
          "score": 2,
          "created_utc": "2026-01-28 04:55:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27311s",
              "author": "twjnorth",
              "text": "Not every codebase is on GitHub or uses languages that are a large part of training for foundation models. \n\nFine tuning a SLM on a specific application with examples from its coding standards, existing functions etc.. should prevent things like reinventing the wheel by creating a function that already exists in the codebase.",
              "score": 1,
              "created_utc": "2026-01-28 11:28:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a8jvp",
                  "author": "Torodaddy",
                  "text": "Im just saying you are going to spend time and money for a gain thats negligible. Most likely a negative ROI exercise",
                  "score": 1,
                  "created_utc": "2026-01-28 20:39:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2763bf",
          "author": "radarsat1",
          "text": "Test time training will do this, if it ever becomes a thing.",
          "score": 2,
          "created_utc": "2026-01-28 11:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aihzo",
              "author": "ithkuil",
              "text": "Actually I think this is coming within a few months based on a lot of continual learning work being focused on by high profile groups recently.",
              "score": 2,
              "created_utc": "2026-01-28 21:23:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26fxq6",
          "author": "Crazyfucker73",
          "text": "So you got ChatGPT to write that entire thing?",
          "score": 1,
          "created_utc": "2026-01-28 08:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26oqsm",
              "author": "Outside-Tax-2583",
              "text": "yes Ôºåi will first write content , and then let GPT check it and send it back to me.",
              "score": -3,
              "created_utc": "2026-01-28 09:23:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27wftk",
          "author": "WolfeheartGames",
          "text": "When you fine tune the model doesn't memorize the information. It embeds a compressed representation of some portion of the original information.",
          "score": 1,
          "created_utc": "2026-01-28 14:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o286mgt",
          "author": "East-Muffin-6472",
          "text": "I think it‚Äôs because fine tuning is difficult even with great libraries out there for the same \nSecond is dataset generation \nThird is to create a skill in antigravity to follow for a particular pattern when doing something so yea it‚Äôs a great project learning wise but not so much of a usage daily wise",
          "score": 1,
          "created_utc": "2026-01-28 15:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o298sss",
          "author": "verbose-airman",
          "text": "It‚Äôs just cheaper and easier to provide more context instead of fine-tune a model (and having to fine-tune a new model everytime the model is updated).",
          "score": 1,
          "created_utc": "2026-01-28 18:03:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ah9ud",
          "author": "ithkuil",
          "text": "You can't just provide the raw examples. You have to create a question and answer dataset. So it's a lot less convenient than you think.\n\n\nBut the real reason is that small models are just dumb. Their reasoning, abstraction and just general intelligence is not comparable to very large SOTA models and is generally insufficient for tasks that aren't fairly narrow. They are more brittle.\n\n\nAlso, RAG is much easier and works as well or better as long as you have do it right and have a strong model interpreting the results.",
          "score": 1,
          "created_utc": "2026-01-28 21:18:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnjiym",
      "title": "SHELLper üêö: Multi-Turn Function Calling with a <1B model",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "author": "gabucz",
      "created_utc": "2026-01-26 15:43:32",
      "score": 24,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "We fine-tuned a 0.6B model to translate natural language into bash commands. Since it's tiny, you can run it on your laptop with complete data privacy.\n\nSmall models struggle with multi-turn tool calling - out of the box, Qwen3-0.6B achieves 84% accuracy on single tool calls, which drops to **just 42% over 5 turns.** Our tuning brings this to 100% on the test set, delivering robust multi-turn performance.\n\n|Model|Parameters|Tool call accuracy (test set)|=> 5-turn tool call accuracy|\n|:-|:-|:-|:-|\n|Qwen3 235B Instruct (teacher)|235B|99%|95%|\n|Qwen3 0.6B (base)|0.6B|84%|42%|\n|**Qwen3 0.6B (tuned)**|**0.6B**|**100%**|**100%**|\n\nRepo: [https://github.com/distil-labs/distil-SHELLper](https://github.com/distil-labs/distil-SHELLper)\n\nHuggingface model: [https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper](https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper)\n\n# Quick Start\n\nSet up the environment:\n\n    # Set up environment\n    python -m venv .venv\n    . .venv/bin/activate\n    pip install openai huggingface_hub\n    \n\nDowload the model:\n\n    hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model\n    cd distil_model\n    ollama create distil_model -f Modelfile\n    cd ..\n    \n\nRun the assistant:\n\n    python filesystem_demo.py\n    \n\nThe demo prompts for confirmation before running commands (safety first) and blocks certain dangerous operations (like `rm -r /`), so feel free to try it out!\n\n# How We Trained SHELLper\n\n# The Problem\n\nSmall models really struggle with multi-turn tool calling - performance degrades as tool calls chain together, dropping with each additional turn. If we assume independent errors for each tool call (like incorrect parameter values), a model at 80% accuracy only has a 33% chance of getting through 5 turns error-free.\n\n|Single tool call accuracy|5-turn tool call accuracy|\n|:-|:-|\n|80%|33%|\n|90%|59%|\n|95%|77%|\n|99%|95%|\n\nFor this demo, we wanted to test if we could dramatically improve a small model's multi-turn performance. We started with a task from the [Berkeley function calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) (BFCL) - the [gorilla file system tool calling task](https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json). We adapted it:\n\n* Original task supports multiple tool calls per turn ‚Üí we restrict to one\n* Cap at 5 turns max\n* Map commands to actual bash (instead of gorilla filesystem functions)\n* Skip adding tool outputs to conversation history\n\nBasically, the same tool set, but new, simpler [train/test data.](https://github.com/distil-labs/distil-SHELLper/tree/main/data)\n\n# Training Pipeline\n\n1. **Seed Data:** We built 20 simplified training conversations covering the available tools in realistic scenarios.\n2. **Synthetic Expansion:** Using our [data synthesis pipeline](https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&utm_medium=referral&utm_campaign=shellper), we generated thousands of training examples.\n\nSince we're dealing with variable-length conversations, we broke each conversation into intermediate steps. Example:\n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models\n    \n\n... becomes 2 training points:\n\n    [Input] User: List all files\n    [Output] Model: ls -al\n    \n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models`\n    \n\n1. **Fine-tuning:** We selected **Qwen3-0.6B** as the [most fine-tunable sub-1B](https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning) model with tool calling support on our platform.\n\n# Usage Examples\n\nThe assistant interprets natural language, generates bash commands, and can execute them (with Y/N confirmation).\n\n**Basic filesystem operations**\n\n    > python filesystem_demo.py\n    \n    USER: List all files in the current directory\n    COMMAND: ls\n    USER: Create a new directory called test_folder\n    COMMAND: mkdir test_folder\n    USER: Navigate to test_folder COMMAND: cd test_folder\n    \n\n# Limitations and Next Steps\n\nCurrently, we only support a basic bash tool set:\n\n* no pipes, chained commands, or multiple tool calls per turn\n* no detection of invalid commands/parameters\n* 5-turn conversation limit\n\nWe wanted to focus on the basic case before tackling complexity. Next up: multiple tool calls to enable richer agent workflows, plus benchmarking against [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html).\n\nFor your own bash workflows, you can log failing commands, add them to `data/train.jsonl`, and retrain with the updated data (or try a bigger student model!).\n\n# Discussion\n\nWould love to hear from the community:\n\n* Is anyone else fine-tuning small models for multi-turn tool calling?\n* What other \"focused but practical\" tasks need local, privacy-first models?",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1u3wq5",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 15:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u7b5a",
              "author": "gabucz",
              "text": "Thanks! Qwens tool call format - inputs are translated via transformers chat templating, and for outputs we have a converter to match what qwen does",
              "score": 1,
              "created_utc": "2026-01-26 16:00:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u7m4o",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 0,
                  "created_utc": "2026-01-26 16:02:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uk3mr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 16:55:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1upa98",
              "author": "gabucz",
              "text": "Exactly - we mention it in the post",
              "score": 1,
              "created_utc": "2026-01-26 17:17:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qluto8",
      "title": "HashIndex: No more Vector RAG",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 18:33:54",
      "score": 22,
      "num_comments": 7,
      "upvote_ratio": 0.96,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama and llama cpp . Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha\n\n[https://github.com/JasonHonKL/HashIndex/tree/main](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1ifug5",
          "author": "FaceDeer",
          "text": "I didn't see any documentation there about how the \"guts\" of the system worked, so [I asked Gemini to do a Deep Research run to produce one](https://gemini.google.com/share/2e13e0d1a6fe). Some key bits:\n\n> The documentation for HashIndex identifies it as a \"vectorless\" index system. This characterization is central to its \"under the hood\" operations. Instead of calculating a mathematical hash or a vector embedding, the system invokes an LLM to generate what it terms a \"semantic hash key\".  \n\n> When a document is ingested by HashIndex, it is first split into segments or pages. For each segment, the system initiates a dual-process LLM call. The first process involves generating a highly descriptive, human-readable label that encapsulates the core theme of the content. This label‚Äîfor example, ``revenue_projections_FY2024_Q3``‚Äîserves as the index key in the hash map. The second process generates a concise summary of the page.\n\n> This \"single-pass\" parsing allows the document to be structured for retrieval without the need for pre-computed embedding datasets. However, the cost of this precision is time. While a traditional cryptographic hash function $H(x)$ or an embedding model can process data in milliseconds, the semantic key generation in HashIndex requires significant inference time, typically 2 to 3 seconds per page.\n\n[...]\n\n> In HashIndex, the hash table is implemented in-memory, allowing for rapid access once the indexing phase is complete. The \"hash function\" in this context is the cognitive process performed by the LLM during key generation. This approach eliminates the need for complex tree rebalancing and multi-level traversal required by systems like ChatIndex or PageIndex. However, it places a higher burden on the \"agentic\" side of the retrieval process, as the agent must now navigate a flat list of keys rather than a hierarchical tree. \n\nDoes this look like an accurate summary of how it works? Might be worth calling out that the \"hash\" in this case is not a traditional hash in the way that word is usually meant, but an LLM-generated semantic \"tag\" of sorts.",
          "score": 1,
          "created_utc": "2026-01-24 22:38:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l7wdg",
              "author": "jasonhon2013",
              "text": "Haha generally speaking yea this summary is right. We call it hash is because the data structure we use behind is a hash table hahaha",
              "score": 1,
              "created_utc": "2026-01-25 09:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l7lf4",
          "author": "mister2d",
          "text": "Checking it out now with some pending energy legislation bills.",
          "score": 1,
          "created_utc": "2026-01-25 09:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yp85z",
          "author": "No-Lobster486",
          "text": "it would be much helpful if there is an relatively detailed explanation about the \"hash map to handle data\" part and a chart of the business flow.    \nthere are too many similar things these days, it will definitely help people make decision whether it will suit for their issue or worth trying",
          "score": 1,
          "created_utc": "2026-01-27 05:06:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g4cj5",
          "author": "Lux_Interior9",
          "text": "So it's like a cached expert cognition layer? That's pretty badass. Sounds like it'll be a great addition to vector rag.  It's going in my stack.",
          "score": 1,
          "created_utc": "2026-01-29 17:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h4awo",
          "author": "jschw217",
          "text": "Why does it require httpx? Any connections to remote servers?",
          "score": 0,
          "created_utc": "2026-01-24 18:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h4gs2",
              "author": "jasonhon2013",
              "text": "Oh it‚Äôs is for the purpose of fetching APIs like open router not connected to any remote server no worries !",
              "score": 2,
              "created_utc": "2026-01-24 18:57:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qlqvdh",
      "title": "Minimum hardware for a voice assistant that isn't dumb",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "author": "JacksterTheV",
      "created_utc": "2026-01-24 16:07:22",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "I'm at the I don't know what I don't know stage. I'd like to run a local LLM to control my smart home and I'd like it have a little bit of a personality. From what I've found online that means a 7-13b model which means a graphics card with 12-16gb of vram. Before I started throwing down cash I wanted to ask this group of I'm on the right track and for any recommendations on hardware. I'm looking for the cheapest way to do what I want and run everything locally ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1gbeqd",
          "author": "LowTip9915",
          "text": "I have a 7b on an m2 Mac mini and it‚Äôs acceptable, but I‚Äôm not using voice (just llama and open web ui). The 14b runs, but is painfully slow, however the difference in answers is evident, so I use it occasionally when the 7b answer is overly vague/top level.  (Disclaimer also a noob to this)",
          "score": 6,
          "created_utc": "2026-01-24 16:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h01e9",
          "author": "Boricua-vet",
          "text": "First off, you do not need a model to do what you want to do. Voice assist can do pretty much what you want and in the case that you have unique requests, you can just add those by hand. Even the personality part can be creating using custom answers. The only downside is that it can only do what you program it to do and you will not be able to have conversations with it. If you need conversations, then you will certainly need a model but you don't need a 13b to do what you want. You do not even need a GPU as long as you have a good CPU and fast ram and enough ram to load the model. \n\nSince I do not know your hardware specs, I recommend you starting with [https://huggingface.co/unsloth/Qwen3-4B-GGUF?show\\_file\\_info=Qwen3-4B-Q4\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q4_K_M.gguf)\n\nYou can run that on CPU and get really good speed depending on your hardware. Example on my hardware using only CPU I can get 100+ tokens per second on this model but depends on your hardware.\n\nThat's the cheapest way to do it.\n\nNow if you still want to be cheap and use GPU\n\nGet a P102-100 for 60 bucks.\n\n[https://www.ebay.com/itm/156284588757](https://www.ebay.com/itm/156284588757)\n\nyou can load piper and whisper using hardware acceleration and get responses from HA in milliseconds.  For this purpose , you do not need to buy a stupid expensive card. 60 bucks will do better job than any 3060, 4060 and 5060 for this particular application and use case.\n\nwith one P102-100, you can run Piper, Whisper and Qwen 4b and get everything you wanted to do.\n\nnow, if you really want to go nuts, you can get two and then you can run Qwen30b and get these speeds.\n\n`llamacpp-server-1  | ggml_cuda_init: found 2 CUDA devices:`\n\n`llamacpp-server-1  |   Device 0: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  |   Device 1: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  | load_backend: loaded CUDA backend from /app/libggml-cuda.so`\n\n`llamacpp-server-1  | load_backend: loaded CPU backend from /app/libggml-cpu-piledriver.so`\n\n`llamacpp-server-1  | | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |`\n\n`llamacpp-server-1  | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           pp512 |        968.28 ¬± 4.69 |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           tg128 |         72.10 ¬± 0.23 |`\n\n`llamacpp-server-1  |` \n\n`llamacpp-server-1  | build: 557515be1 (7819)`\n\n`llamacpp-server-1 exited with code 0`\n\n  \nabout 1,000 in prompt processing and 70+ tokens generation.\n\ntrust me, for home assistant, you will not need more than that and at that price, you will not find a better solution.\n\nAsk people that actually own P102-100, don''t take my word for it.\n\nFor this use case.... this is it.",
          "score": 6,
          "created_utc": "2026-01-24 18:38:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h9az6",
              "author": "JacksterTheV",
              "text": "Awesome, I'm going to go the CPU route and see what I can get working. Thanks for the recommendation.¬†",
              "score": 1,
              "created_utc": "2026-01-24 19:18:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1kwfhl",
              "author": "MakerBlock",
              "text": "Fantastic write up!",
              "score": 1,
              "created_utc": "2026-01-25 07:25:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1nfx91",
                  "author": "Boricua-vet",
                  "text": "Thank you. Really appreciate that. Just trying to help.",
                  "score": 3,
                  "created_utc": "2026-01-25 17:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ga8q2",
          "author": "tillemetry",
          "text": "Anyone try this with an M4 Mac mini?  You can run it headless, it doesn‚Äôt take up much space and the noise and power draw is minimal.  LM studio will point you at ai‚Äôs optimized for MLX.",
          "score": 3,
          "created_utc": "2026-01-24 16:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gbm53",
          "author": "PermanentLiminality",
          "text": "There is no single correct answer.\n\nYou left out a few parts.  You need the audio in and out.  A bluetooth speaker can work.  Now you need the Speech to text (STT)  and Text to Speech(TTS) in addition to a LLM.  Both the TTS and STT need VRAM, but there are TTS like Piper than can actually run on the CPU.\n\nOne nice thing here is most voice assistants don't do a lot of context so you only need say 15% more VRAM than the LLM.\n\nYou will need to experiment to find what works for you.  Put a few bucks into OpenRouter and try the models.  A possible example would be mistral 3 8B which fits in about 7GB of VRAM.  They free models too.   Find what will work for what you want.\n\nIf I ever get some time, I plan on doing what you are trying to do.  My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU.  Not sure if I can get a smart enough model STT and TTS , but I want to try.  That setup will idle around 10 watts.  I may consider picking up a 24GB P40 as it also is pretty low idle power.   The P40 is around $200 after the needed fan to cool it.\n\nMy power is crazy expensive so cheap 16 GB GPUs like the P100 or CMP100-210 idle at 40 or 50 watts.  That is like $200/yr for me.\n\nAnyway, before dropping coin on a GPU, test out a solution with available tools like Openrouter and Huggingface.  For a project like this $10 can go a long way for testing.",
          "score": 2,
          "created_utc": "2026-01-24 16:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ht9nr",
              "author": "Boricua-vet",
              "text": "\"My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU. Not sure if I can get a smart enough model STT and TTS\"\n\nyou sure can.. I will go as far as give you the models I am using with hardware accel that works for me.  \nfor piper PIPER\\_VOICE=en\\_US-libritts-high  \nfor whisper  WHISPER\\_MODEL=distil-medium.en  \nthose two will consume 3.5GB of vram for both. 3.5 total for both.\n\nMy only suggestion is to use really good microphone like [https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html](https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html)  this has the xmos chip that will clean all the input even with loud music or background noise. It works fantastic...\n\non the P102-100 with those models and that mic, your response time from HA after your command should be in the milliseconds. Not even a second like half a second or less, extremely responsive, that is unless you are running it in an under powered device.\n\nI run this setup flawlessly.",
              "score": 3,
              "created_utc": "2026-01-24 20:50:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gn24j",
          "author": "JacksterTheV",
          "text": "To add some more context I'm using home assistant and have the voice pipeline worked out. I have a hacked echo show that I can talk to like Alex. The problem is right now it takes about 30 seconds for something as simple as locking the back door. I can connect home assistant to any local LLM. This is all running on an old laptop. Ultimately I want everything to run locally but I really like the idea of spending a few bucks on a cloud service to figure out what I need before dropping cash on hardware.¬†",
          "score": 2,
          "created_utc": "2026-01-24 17:43:19",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1haztq",
              "author": "Blizado",
              "text": "I actually tend to use a Qwen3 30B A3B Instruct model in ~2-4bit. I tried it with Q2_K in GGUF format with KoboldAI, needs with a small KV Cache around 12GB VRAM (Q4 would be around 16-18GB VRAM) and I was surprised how good it still is, but even more surprised how crazy fast it is (at least on my RTX 4090). Thanks to its MoE pipeline, where only 3B of the model is active, so it is mostly as fast as a 3B model at Q2_K would be, but of cause way better, you don't need to try a 3B at Q2_K, it will be very very bad but crazy fast. It's hard for me to use dense models now, they are so slow in comparison.\n\nSo as long you don't want to have a realtime conversation with the LLM, you can also split a some layers of the model into normal RAM. It slows down the model a bit but if you only put ~10% of the model in RAM it should stay fast enough so you can bring down the SmartHome reaction to maybe 5 seconds or lower. The only question is if you need tool calling, that could break on Q2_K, but there are ways to work around that.\n\nBut I would go for a solid 16GB VRAM card (RTX 40xx/50xx series), not too much on the low end, then you can put the LLM completely into the VRAM. So maybe a 5070TI with 16GB VRAM. No clue if a 5060TI is still fast enough. I \"only\" have a 4090 and a 5080. Could be if you get the full model into VRAM, because if not, the 5060TI has only PCIe 5 8x, the 5070TI has PCIe 5 16x, but that is only really relevant if you use offloading (splitting the model into RAM) or if the GPU is in a PCIe 5 x16 slot. If you need offloading, better don't put a PCIe 5 8x GPU into a PCIe 5 motherboard setup.\n\nBut beside that, it really depends what you want to exactly do. If you only want the bar minimum, for example only some different phrases that should react for one SmartHome function, you can do that with much much less hardware and a tiny AI model that only recognize what you want to do. Such an AI didn't need to be that smart at all, only always recognize what you want from it, you can even give it a bit the illusion of personality. But if your plan is to build your SmartHome more and more out, then a LLM is surely not a bad decision, but a more costly one.",
              "score": 1,
              "created_utc": "2026-01-24 19:26:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i0p7u",
          "author": "atlantageek2",
          "text": "currently developing something similar for work using whisper. dev machine at home is my m4 mac mini 16gb. theres like a 3 second delay but other than that it works well. its using whisper",
          "score": 2,
          "created_utc": "2026-01-24 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i0zr0",
              "author": "atlantageek2",
              "text": "forgot to mention whisper cannot use max gpu",
              "score": 1,
              "created_utc": "2026-01-24 21:27:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ihhpy",
          "author": "Powerful-Street",
          "text": "LFM2.5-audio if you can figure out how to get it to work, other than in the demo. It‚Äôs only ~3GB and does a pretty good job. The other thing you could do is, inject personaplex weights into moshi. Takes much more work and ram though.",
          "score": 1,
          "created_utc": "2026-01-24 22:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1izk91",
          "author": "PermanentLiminality",
          "text": "Good to know.   Now I just need some time...",
          "score": 1,
          "created_utc": "2026-01-25 00:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r35e0",
          "author": "Bino5150",
          "text": "I'm running an 8b model locally with LM Studio/AnythingLLM, and using Piper for TTS. I'm running an HP Zbook Studio G7 laptop on Linux Mint, with an i7, 16GB ram, and an Nvidia Quadro T1000 Mobile gpu with 4GB vram. It runs just fine. I can do SST via the laptops mic, but I haven't configured home assistant yet as I've just got this up and running. Thinking of setting up the SST on a Raspberry Pi.",
          "score": 1,
          "created_utc": "2026-01-26 03:17:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnmc4s",
      "title": "Building my first LLM HomeLab, where to start?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnmc4s/building_my_first_llm_homelab_where_to_start/",
      "author": "rmjcloud",
      "created_utc": "2026-01-26 17:20:29",
      "score": 17,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "Hey all! üëã\n\nI‚Äôm looking to delve into AI and local LLMs head first with the aim eventually of building some cool AI/LLM apps for self learning.\n\nI wanted to see if anyone had some good recommendations of hardware for a homelab, preferably on the mid-starter end of budget.\n\nSpecifically CPU, GPU and RAM suggestions so i can test the water to see how much i need to spend to build a decent lab for running local LLMs with Ollama to kickstart my AI journey and learning!\n\nGaming orientated GPUs not necessary but a nice compromise for gaming too i guess!\n\nBudget 1-2k GBP¬£.\n\nI have an M3 MBP and an M2 Macbook Pro, both with around 16GB RAM, are these any good for achieving this? Small scale is fine, i just want to get to grips with LLM concepts locally and digging into how they work, tuning and deploying apps in an AIOps approach!\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnmc4s/building_my_first_llm_homelab_where_to_start/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1uu7rb",
          "author": "Kyuiki",
          "text": "I did my research on this and there is a single conclusion I made. Don‚Äôt waste your money on it. Like really. You can never run the amazing models you see like GPT, Claude, DeepSeek, GLM in a way that feels good. You can have 4 video cards costing 10 grand which will LOAD the model but you‚Äôll still be generating text at 15ish/tokens a second. You‚Äôll be running quantized models that adds additional hallucinations to LLM‚Äôs that already hallucinate a bunch.\n\nYou can get some really really cheap cloud provided services that give API access to some amazing models. In most cases what you spend on your own hardware would take 20+ years to spend on a subscription.\n\nNow if you‚Äôre doing it for pure hobby, start with GPU power. Memory will help LOAD a model and can increase max context size but it will not make the model run faster. GPU offloading is the only thing that will increase tokens/sec in a way that matters. The higher the VRAM on the GPU the better.",
          "score": 15,
          "created_utc": "2026-01-26 17:39:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v2knb",
              "author": "catplusplusok",
              "text": "Depends? I have NVIDIA Jetson Thor which is kind of pricey, but not 10K. I can run Qwen3-Next lighting fast, FP4 quantized but seems perfectly capable, or an uncensored GLM-4.5-Air model for storytelling/roleplay which is 15tps as you mentioned, but worth it when cloud models would lecture you instead of writting stories or you do high volume batch data processing. Mac is another good choice for affordable-ish local AI.",
              "score": 3,
              "created_utc": "2026-01-26 18:15:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1usnrb",
          "author": "PermanentLiminality",
          "text": "The range is from a cheap GPU like a P102-100 for $50 in whatever computer you already have, to $50k or more for a serious rig.   You really need to start with some kind of budget and what you are trying to run. \n\nTo get started it is almost always the answer to start with an API provider like Openrouter and only look to buying hardware once you have some idea of what you actually need.  In almost all circumstances it will cost a lot more to buy hardware than pay for an API.",
          "score": 5,
          "created_utc": "2026-01-26 17:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uvn2n",
              "author": "rmjcloud",
              "text": "updated!",
              "score": 1,
              "created_utc": "2026-01-26 17:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uvs5c",
          "author": "GoodRPA",
          "text": "I've tried to build local LLM/SML just in the last few weeks.\n\nMy recommendation would be:\nEither don't worry about GPU, responses will be slow, but very stable on CPU + ram. \n\nIf you do decide to go with GPU, get more vram(12gb, 16gb, 24gb) and a relevantly recent/supported GPU architecture from 2018+.\n\nBets options with balance of vram/price/power consumption/architecture: \n\nNvidia t4 - 16gb, 70watt only!, Turing, requires fan mode/cooling\nNvidia RTX 3090 - 24gb, 350 watt , Ampere\n\nBudget:  \nAgain, either CPU/shared ram (M1 mini, 16gb)\nEven hp thin clients (t520, t730) can manage CPU models, slowly but these work and very stable.\n\nNvidia RTX A2000 - 12gb, 70watt, ampere\n\nInstead of getting 6gb and below, you can use CPU only or even use remote LLMs, these are good quality, whether we like/can run them locally or not )\n\nBuild everything else around this. 16gb ram is a must.",
          "score": 3,
          "created_utc": "2026-01-26 17:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ursk3",
          "author": "gittb",
          "text": "Budget numbers would help, ranges from a couple grand to a down payments on houses for the starter to mid range as far performance goes.",
          "score": 2,
          "created_utc": "2026-01-26 17:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uvmmz",
              "author": "rmjcloud",
              "text": "updated!",
              "score": 1,
              "created_utc": "2026-01-26 17:45:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wqb72",
          "author": "its_a_llama_drama",
          "text": "This depends on a lot of variables.\n\nHow much faff and fiddling are you realistically willing to tollerate?\n\nDo you want to be able to scale up in the future without upgrading everything? \n\nWhat exactly do you want to do with LLMs? Use them and if so how? Fine tune them? Try and train a model yourself?\n\nAre you only bothered about LLMs? Or does image generation interest you? This changes things considerably\n\nI started with your budget in mind, and before i knew it, i spent quite a bit more. \n\nIt depends what you are expecting this machine to do. That budget is tight. You could get a consumer board, cpu and single card running smaller models with high throughput, with ddr5 and pcie 5.0 for that price too.\n\nOr you could go for the old server route and go for xeon dual socket, ddr4 ecc ram and pcie 3.0. ddr4 ram is still expensive, just not as expensive as ddr5. But this option (if you choose the right board), gives you nore optionality later and slower but bigger capacity and future expandability.\n\nThey both serve different purposes. \n\nAlso, some gpus are cheaper for a reason. You can get cheap cards, but they don't have all the bells and whistles a newer card might have. At this budget, you have to choose between throughput and vram. At 2k, you might squeeze two 3090s into the build budget and maybe 4x32GB ECC RDIMMs as well, if you go with a reasonably priced lga 2011 board and xeon v4 cpus.  But factoring in psu(s), storage, cooling, a case or open frame for it, it will be tight. \n\nIf you go for an older card than a 3090 (like the P40) you might find performance disappointing, they lack fp16 performance. but again, it depends what you expect. Do you need it to be pushing 50+ tok/s. You might, if you get the cards and then realise you do, you will feel like it is a let down.  \n\nIf you go for older MI50 AMD cards, you get a lot of VRAM for your money (if you can find 32GB variants), but you pay with set up time and compatability instead. \n\nI think if I were you. I would start off with one of the better e5 v4 xeons on a dual socket lga2011-3 board, probably look for 2699 or if too expensive, 2698 v4. buy 4x32GB RAM, and buy one 3090. Use it, push the limits and then you will know if you need more throughput or more vram. Once you know what you actually need, you can choose what to change\n\n you can probably sell the 3090 for what you bought it for if you decide you want to build around a different card.\n\nYou can add more ram, most dual socket boards have 12 or 16 dimm slots.\n\nyou can add more 3090s if one was good but you want more vram (try and get a board with plenty of x16 pcie slots)\n\nOr if the cpu/platforn is the limitation, you didn't lose too much after you sell the board and cpus, and you can weigh up costs for a newer platform (considerably more)\n\nThis leaves as many options open whilst allowing you to play with a fast gpu with 24GB VRAM and good compatability, and 128GB of RAM. \n\nYou should be able to build that for 1k (just about) and use the other 1k if and when you know what you need.",
          "score": 2,
          "created_utc": "2026-01-26 22:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21bs8f",
              "author": "Akimotoh",
              "text": "Damn, what is your spending budget?",
              "score": 1,
              "created_utc": "2026-01-27 16:02:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22zeiz",
                  "author": "its_a_llama_drama",
                  "text": "All in i probbably spent closer to 3.5k, but i already had 4TB of 990 pro storage from an old build.\n\nlooking back, i regret choosing lga3647, i do not need ghe more powerful cpus it unlocks and it adds complexity for cooling if you aren't actually building a server. There are very few quiet cooling solutions for this platform and the ones that are quiet are expensive. I made ghe mistake if buying a mother board which has two very tightly spaced x16 slots, so if i want more gpus, i shoukd probably get a new board, rather ghan cramming gpus into x8 slots with adapters.\n\nIt did not gain me anything significant. I have 2x xeon 8276 which are significantly better than any cpu on lga 2011-3, but the platform is still ddr4 and it is still pcie 3.0. i should have gone with amd epyc at this price point, but i didn't realise this was my budget untill i finished adding bits",
                  "score": 1,
                  "created_utc": "2026-01-27 20:21:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uw7xt",
          "author": "catplusplusok",
          "text": "Best used Apple Silicon Mac you can find (in terms of maxing out RAM). This will give you much better bang for the buck then PC GPUs and you get a computer useful in many other ways. \n\nThere is also a [64GB NVIDIA dev kit](https://www.amazon.com/NVIDIA-Jetson-Orin-64GB-Developer/dp/B0BYGB3WV4?th=1) within your budget, but beware of limited memory speed (so you can run bigger models but slowly) and need to build lots of things from source for custom compute.",
          "score": 2,
          "created_utc": "2026-01-26 17:48:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxjk4",
              "author": "rmjcloud",
              "text": "I have an M3 Macbook Pro, would this be enough to play around with small models to learn fundamentals and things such as RAG and how to expose LLMs for app usage etc?",
              "score": 2,
              "created_utc": "2026-01-26 17:54:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v0omr",
                  "author": "catplusplusok",
                  "text": "Absolutely! I would say with 16GB you can play with a lot of task specific workflows (machine vision, image generation, processing structured data). With 32GB you can run models that can replace cloud chat AI for most cases, roleplay and write production code for you (look at QWEN3 or other mixture of experts models with around 30B total parameters). Look for mlx 4 bit quantized ones for decent quality / efficiency balance.",
                  "score": 3,
                  "created_utc": "2026-01-26 18:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v09xh",
          "author": "teachersecret",
          "text": "Start by getting your hands on a decent computer. At the cash you have, you're probably looking used. I assume you already have a PC laying around that was built in the last 5 or so years with a modern processor and 32gb-64gb ddr3/ddr4/ddr5 in it. If so, that's the EASIEST way to get going. Just go buy a 3090/4090 or a 5090, shove it in, and you're done.\n\nIf not, you can buy one fairly cheap prebuilt computer secondhand. Go after something from the more modern intel or AMD lineups. Aim for a HIGHER END cpu built some time this decade (2020 or newer if possible) with as many cores/highest speed you can find. You can go on places like facebook marketplace and find people selling entire used rigs that would work fine. Aim for the best processor/most ram you can find, and try to stick DDR4 or DDR5 if you can (as an example I recently purchased a whole 5900x rig with a 165hz monitor, 3080ti, and 64gb of ddr4 for less than $500 all-in off FB marketplace).\n\nAfter you get the machine, the focus needs to be on the GPU. Hopefully the rig already has one that is useful, but if it DOESN'T have a 3090/4090/5090, thats where you need to dump the rest of your budget. Grab a used 3090/4090, or save up for a 5090.\n\nThat's about as far as you can reasonably go on a budget, while still allowing you to play with some of the best performing models. 24gb vram gets you high speed 30b moe models, lightning-fast smaller models like oss 20b or the smaller 8b and 4b options, high speed image and video generation (ltx-2 fits in 24gb), and plenty of support for your explorations (3090 and newer are gold standards of AI right now, and are likely to maintain strong support for years). Anything cheaper is going to be significantly slower, older, barely supported, and really not a great option. You'll see people in here strapping together impressive server rigs using old cast off server parts that can run big boy MoE models, and that's cool, but unless you have a damn good reason to do that and an exhaustive understanding of server hardware, I'd avoid that space. Older server rigs end up too slow to be worth it for most uses, and newer server rigs are too expensive for your budget.\n\nAnything more capable than what I mentioned is going to be significantly more expensive. Obscenely more expensive in some instances. At that point, you're just flat out better off using API inference and paying pennies for tokens.\n\nOne dark horse here is a AI Max 395x 128gb rig. They're small, sip power, and have unified 128gb ram which allows you to run hefty models like 200b style models at usable speed. That said, 'usable' is still SIGNIFICANTLY SLOWER than what you'll get off a good GPU, and those AI max rigs are over 2k USD now. Not a terrible option if you want a quiet little box to experiment with though. Same goes for the newer macbook pro line. They're pricey, but if you grab one with enough unified memory you can run something as big as deepseek at a speed that could still be considered usable. These might not be the cheapest options today, but as time passes and more of these kinds of rigs hit the used market, they're going to end up being pretty solid AI hardware to snatch up.",
          "score": 1,
          "created_utc": "2026-01-26 18:05:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yc3s6",
          "author": "WishfulAgenda",
          "text": "Alright, awesome questions. I‚Äôll start with what I think then go into why. What I think you should do is on the MBP m3 download lm studio, vs code with continue dev. On the MBP m2 download vm fusion install Linux (mint, tumbleweed etc) and on the Linux machine install clickhouse and grafana) connect the lm studio llm to the clickhouse vm via an mcp server and your laughing. You could probably even use docker instead of the Linux vm as well. Also have a look at librechat . All of those applications are open source and free. I‚Äôd then start saving my money as the next jump is likely expensive and my guess would be an MBP m5 max, a tricked out Mac mini or a Mac Studio  when they come out or a substantially more expensive desktop. \n\nThe above isn‚Äôt as much fun as building a new machine but for your budget I‚Äôm not sure that the new machine would have much on the MBP m3, the apple silicon is very good. Especially with the current ram and gpu prices.\n\nAlso don‚Äôt let it put you off as the small models with the right configuration and actually be really good. You can set it up so you have specialist agents with system prompts that focuses them and the set up MoA architectures on the same model.\n\nGood luck :-)",
          "score": 1,
          "created_utc": "2026-01-27 03:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29muhr",
          "author": "chancechants",
          "text": "Some decent comparison data here, this is what I would get.\nhttps://www.kickstarter.com/projects/167544890/olares-one-the-local-al-powerhouse-on-your-desk",
          "score": 1,
          "created_utc": "2026-01-28 19:02:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a96ri",
          "author": "see_spot_ruminate",
          "text": "Sell one of the macbooks. Use the money to get a strix halo. Set up strix halo headless. Use remaining macbook to remote in. \n\nI have been trying several different models over the last several months, but keep coming back to gpt-oss-120b. MOE is very good these days and the strix halo does well with it. With trying to buy a hodge podge of stuff, you can prob run the same pp and tg as me with a strix halo.",
          "score": 1,
          "created_utc": "2026-01-28 20:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uunh6",
          "author": "Limebird02",
          "text": "OP said around $1000 GBP which is $1370. \n\nDoes a local 30B model off Ollama do ok or can we get by with less? Id like to try the same with $500 mini pc. \n\nHow does open code help? \n\nHow does clawdbot work well with smaller local models? \n\nLike OP want to dabble without large outlay.",
          "score": 0,
          "created_utc": "2026-01-26 17:41:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqg0hi",
      "title": "Using whisper.rn + llama.rn for 100% on device private meeting transcription",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qqg0hi/using_whisperrn_llamarn_for_100_on_device_private/",
      "author": "pandodev",
      "created_utc": "2026-01-29 18:04:01",
      "score": 17,
      "num_comments": 12,
      "upvote_ratio": 0.85,
      "text": "Hey all  wanted to share something I shipped using local models on mobile devices only.\n\n\n\nThe app is called Viska local meeting transcription + chat with your notes, 100% on-device.\n\n\n\n**Stack:**\n\n\\- whisper.rn (Whisper for React Native)\n\n\\- llama.rn (Llama 3.2 3B or qwen3 4b for higher devices for React Native)\n\n\\- Expo / React Native\n\n\\- SQLite with encryption\n\n**What it does:**\n\n1. Record audio\n\n2. Transcribe with local Whisper\n\n3. Chat with transcript using local Llama (summaries, action items, Q&A)\n\n\n\n**Challenges I hit:**\n\n\\- Android inference is RAM-only right now (no GPU via llama.rn), so it's noticeably slower than iOS\n\n\\- Had to optimize model loading to not kill the UX\n\n\\- iOS is stricter for background processing so need to keep app open while transcribing but got a 2 hour transcript to process in 15min ish on a iphone 16 pro.\n\n\n\nSo i built this personally because I have clients I usually sign NDAs and I have gotten in the past that when im in meeting my mind drifts and I miss some important stuff so I went looking for apps to record meetings and transcribe but I got too paranoid about using them because say otter.io my entire meeting is hitting 2 servers the otter.ai one and whateever ai they might be using openai or other I just couldnt. I did find apps that do local transcribe but if we are being honest it is rare I will sit there and read an hour long transcribe I like ai for this using BM25 to search anything and chat with a local 3b model it honestly enough so the app has summary, key points, key dates for maybe deadlines, etc. So maybe someone finds this crucial too i see lawyers, doctors, executives under NDA perhaps finding it valuable. The privacy isn't a feature, it's the whole point.\n\n\n\nWould love feedback from anyone else building local LLM apps on mobile. What's your experience with inference speed and SPECIALLY android my gosh what a mess I experienced?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qqg0hi/using_whisperrn_llamarn_for_100_on_device_private/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o2gksjk",
          "author": "Purple-Programmer-7",
          "text": "Interesting use case. Make sure you understand recording laws in your area!",
          "score": 3,
          "created_utc": "2026-01-29 18:50:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2glatx",
              "author": "pandodev",
              "text": "Do you mean when one is in a meeting to start recording? or something about the app itself because it is all local does not touch any servers",
              "score": 2,
              "created_utc": "2026-01-29 18:52:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gmrzh",
                  "author": "Purple-Programmer-7",
                  "text": "For example, in the USA, each state has its own laws about getting permission from people in the meeting you want to record. It‚Äôs illegal to record a conversation, in some states, when you haven‚Äôt asked permission of all participants.",
                  "score": 2,
                  "created_utc": "2026-01-29 18:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gw8wf",
          "author": "Wishitweretru",
          "text": "Does it identify individual speakers",
          "score": 1,
          "created_utc": "2026-01-29 19:43:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gx3a2",
              "author": "pandodev",
              "text": "It does not currently but is something I will surely add. My short term roadmap is: Improve android transcription capabilities with gpu usage, add individual speaker identity to transcript, add calendar support for key dates to be able to add them with a tap.",
              "score": 2,
              "created_utc": "2026-01-29 19:47:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h1gf7",
                  "author": "tcarambat",
                  "text": "Good luck with speaker ID. Whisper does not have word-accurate timestamps, so you won't be able to do this easily. You will need to run wav2vec2 ontop of the transcript + audio and do speaker id in another process. This is called force-alignment, and it is very computationally heavy.\n\nWav2vec2 is about 100x more intensive than speaker id or transcription due to trellis calculations, and it is proportional to audio length, so if you think you are resource-constrained now, it will get worse. I would be surprised if you could even get Pyannote models running on the mobile device RAM due to their memory demand.\n\nAlso you can really cut out a lot of the overhead for transcription and get real time chunks back with something like [Cactus](https://www.cactuscompute.com/). We use this on[ AnythingLLM mobile](https://play.google.com/store/apps/details?id=com.anythingllm)\n\nI [posted](https://www.reddit.com/r/LocalLLaMA/comments/1qk1u6h/we_added_an_ondevice_ai_meeting_note_taker_into/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) about the downside of whisper in a post on this sub last week",
                  "score": 4,
                  "created_utc": "2026-01-29 20:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gg89u",
          "author": "FullstackSensei",
          "text": "You forgot to copy-paste the \"why it matters\" part of the slop!",
          "score": -5,
          "created_utc": "2026-01-29 18:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ghjsn",
              "author": "pandodev",
              "text": "I just said why I built it. Ask me absolutely anything about it I am more than happy to answer, we all use ai for grammar and re write so guilty as charged by I made this with my whole heart in it was not an easy journey a lot of challenges when trying to run llm on mobile devices.",
              "score": 2,
              "created_utc": "2026-01-29 18:35:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gx2di",
                  "author": "Big_River_",
                  "text": "dont waste your time responding to hate bots - nothing sloppier than crude one line comments calling your work or explanation slop because it was vibe coded or ai enhanced in collaboration - change resist hind brain morons have always been loud chest thumpers - I would be curious to check your repo and see if I can help sort anything but not currently doing mobile dev atm - good luck with it tho",
                  "score": 2,
                  "created_utc": "2026-01-29 19:47:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qovbrq",
      "title": "Will.i.am is promoting running local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qovbrq/william_is_promoting_running_local_llms/",
      "author": "beefgroin",
      "created_utc": "2026-01-28 00:05:26",
      "score": 16,
      "num_comments": 11,
      "upvote_ratio": 0.77,
      "text": "And fine-tuning for that matter. Or at least that‚Äôs how I understood what he was saying lol. What do you think?\n\n[ https://youtu.be/sSiaB90XpII?t=384 ](https://youtu.be/sSiaB90XpII?t=384)\n\nStarts at 6:25. But the whole interview is worth watching too.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qovbrq/william_is_promoting_running_local_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o24i87j",
          "author": "SocialDinamo",
          "text": "He was always big on encouraging kids to learn to code so this makes sense. Great to see!",
          "score": 9,
          "created_utc": "2026-01-28 00:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24yf78",
              "author": "ForsookComparison",
              "text": "Juniors blaming A.I. for their degree and livelihood being worthless but real ones know that it was that one Bill Gates and Will.I.Am video from like 2011 that saturated their market.",
              "score": 3,
              "created_utc": "2026-01-28 02:07:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28kaf4",
                  "author": "Tired__Dev",
                  "text": "It didn‚Äôt though. I still come across execs, product managers, product owners, project managers, and managers that have no fucking clue what their software company does. The 2010‚Äôs tech companies were absolutely littered with smooth talking bureaucrats that offered nothing to help capture new market share. The problem with software is, and mostly always has been, that it operates on a Ponzi scheme of funding that cycles on interest rates and macro economic conditions. \n\nThere almost no overhead to running a software org comparatively to a traditional brick and mortar. It‚Äôs mostly employment. There‚Äôs nothing stopping people from grouping together and creating a startup. But the big problem is that people viewed tech through the lens of working at a big company and making six figure salaries and not being innovative themselves. Now many of them couldn‚Äôt actually create anything themselves because their coding skills are almost bureaucratic in a sense. \n\nLLMs pose risk to people who believe that innovation stopped in 2021. The stick of innovation just extends with tooling and bigger things can be accomplished. An over exaggeration would be if a few people now can program Facebook that would scale then a Facebook amount of people would be able to program the matrix. (Again heavily over exaggerating to make a point) \n\nSoftware developers aren‚Äôt becoming useless. Bureaucrats are. Keep learning code. It will be everywhere.",
                  "score": 3,
                  "created_utc": "2026-01-28 16:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o260a98",
          "author": "rditorx",
          "text": "It's Wi.llm.ai for you",
          "score": 4,
          "created_utc": "2026-01-28 05:53:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24lf20",
          "author": "sizebzebi",
          "text": "give me the ram for it tried one my rtx 4070 and it's utter trash",
          "score": 2,
          "created_utc": "2026-01-28 00:58:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25dr6q",
              "author": "Count_Rugens_Finger",
              "text": "i7-6700K, 32GB DDR4-3000, RTX 3070 8GB over here.  I can run several interesting models at reasonable speeds.",
              "score": 4,
              "created_utc": "2026-01-28 03:29:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27qdit",
                  "author": "vipx237",
                  "text": "Which models ?",
                  "score": 2,
                  "created_utc": "2026-01-28 13:56:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}