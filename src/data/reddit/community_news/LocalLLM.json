{
  "metadata": {
    "last_updated": "2026-02-24 02:58:11",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 191,
    "file_size_bytes": 196810
  },
  "items": [
    {
      "id": "1r90rxi",
      "title": "How much was OpenClaw actually sold to OpenAI for? $1B?? Can that even be justified?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/p8c453eapgkg1.jpeg",
      "author": "Alert_Efficiency_627",
      "created_utc": "2026-02-19 14:34:16",
      "score": 203,
      "num_comments": 79,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r90rxi/how_much_was_openclaw_actually_sold_to_openai_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o68xq98",
          "author": "Theseus_Employee",
          "text": "They didn’t pay anything for it. They just hired the dude that made it, and are sponsoring the free open-source project OpenClaw.\n\nThe tweet is just a joke of people just inflating how much money you can make from vibe coded projects.",
          "score": 206,
          "created_utc": "2026-02-19 14:40:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aewgr",
              "author": "ArcticCelt",
              "text": "Also, people talk about it as a \"vibe-coded\" project, but they fail to mention that even if he used AI, it was done by a programmer with over 20 years of experience who sold his previous software startup for $100 million. It is not some random person with no experience.",
              "score": 34,
              "created_utc": "2026-02-19 18:57:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6aykqj",
                  "author": "huzbum",
                  "text": "Yeah, but he clearly did NOT put his engineering skills to work in there... glaring security flaws.  ",
                  "score": 9,
                  "created_utc": "2026-02-19 20:32:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cbn97",
                  "author": "oompa_loompa0",
                  "text": "Yeah. Watch the interview on Lex Friedman. Peter is no vibe coder, he is a brilliant seasoned engineer with crazy depth and breadth.",
                  "score": 4,
                  "created_utc": "2026-02-20 00:55:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cu2v7",
                  "author": "ANTIVNTIANTI",
                  "text": "it feels all very much—contrived ",
                  "score": 1,
                  "created_utc": "2026-02-20 02:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ae7xo",
              "author": "structured_flow",
              "text": "He had meetings with Zuck, same guy who is paying 8 figure salaries for their ai leadership positions, and turned Meta down. He absolutely was paid money how much don’t know, but your mistaken if you think there wasn’t a bid.",
              "score": 7,
              "created_utc": "2026-02-19 18:54:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bqhgu",
                  "author": "cjc4096",
                  "text": "He already had a successful exit.   He has the ability to choose what is important to him.",
                  "score": 3,
                  "created_utc": "2026-02-19 22:52:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o69j1ku",
              "author": "HeinerWersenberg",
              "text": "$1B for an unhinged AI-Agent-VibeCoded something did sound ridiculous indeed. However, when thinking of the competition, investments done already in the field, and considering the reasons for current memory and storage prices... who knows. Anything is thinkable theses days.    \nSo, thanks for the clarification. ",
              "score": 5,
              "created_utc": "2026-02-19 16:26:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69qiln",
              "author": "HappyContact6301",
              "text": "Look at Scale AI...",
              "score": 1,
              "created_utc": "2026-02-19 17:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bwdvj",
                  "author": "bronfmanhigh",
                  "text": "scale AI was still a real business with real employees and revenue end of the day lol",
                  "score": 2,
                  "created_utc": "2026-02-19 23:26:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6a1yfu",
              "author": "Demiansmark",
              "text": "Like the guy, like, like, like the guy with the $100B startup is going to pay attention to the subtext. Come on!",
              "score": 1,
              "created_utc": "2026-02-19 17:57:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69ufe3",
          "author": "reb00tmaster",
          "text": "This tweet alone is worth $80B.  In a few months it will be acquired for $160B. /s",
          "score": 46,
          "created_utc": "2026-02-19 17:21:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a6zme",
              "author": "MarkoMarjamaa",
              "text": "It could be turned into vibe-coded-NTF, then it's like $300B!",
              "score": 6,
              "created_utc": "2026-02-19 18:20:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6agkqz",
              "author": "eli_pizza",
              "text": "Unfortunately the \"/s\" is clearly not going to be enough for some people",
              "score": 3,
              "created_utc": "2026-02-19 19:05:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hdy7o",
              "author": "Far-Low-4705",
              "text": "I can type 300 B's, will that be enough?",
              "score": 1,
              "created_utc": "2026-02-20 20:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hf5iq",
                  "author": "reb00tmaster",
                  "text": "no",
                  "score": 1,
                  "created_utc": "2026-02-20 20:10:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69vzrb",
          "author": "sleepy_roger",
          "text": "The crazy thing is how shitty openclaw is. Seems like most people hyped (besides the crypto grifters who you never trust..) had never used a harness and were just larping the entire time.. \n\nCodex/claudecode/droid/opencode provide a much better experience overall.. openclaw isn't even tailored to non tech people. The only thing it really added that made the masses adopt was easy integration into existing chat platforms.",
          "score": 29,
          "created_utc": "2026-02-19 17:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a77wi",
              "author": "MarkoMarjamaa",
              "text": "Worlds fastest growing malware distribution system. ",
              "score": 23,
              "created_utc": "2026-02-19 18:21:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a8v57",
                  "author": "sleepy_roger",
                  "text": "haha yeah for sure.. I kept mine pretty limited, running in a proxmox container with almost no access. Is it cool to talk to it via discord, sure I guess.. but another big issue is the limited context you see as a user within openclaw versus other agent harnesses... not to mention the entire thing looks vibe coded (because it is).",
                  "score": 5,
                  "created_utc": "2026-02-19 18:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6adfj8",
              "author": "crypticG00se",
              "text": "You are missing the main point why these systems are attractive to openai and the like. Sells more tokens. Perfect system to fool non-tech people. Go look at the claw subreddit how people are complaining about model costs with claw. ",
              "score": 10,
              "created_utc": "2026-02-19 18:50:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b3bw1",
                  "author": "sleepy_roger",
                  "text": "Yeah that's a great point, I'm on the $200 max plan and it was the only time I got close to hitting my 5 hour limit... and it seemed like it was actually doing _much_ less than what I do within claude code directly.",
                  "score": 2,
                  "created_utc": "2026-02-19 20:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bu267",
              "author": "parapa-papapa",
              "text": ">Codex/claudecode/droid/opencode provide a much better experience overall.\n\n  \nI agree that OpenClaw sucks, but what is the competition? \n\nIt is insecure, yes. And that's why the big providers aren't touching such systems, but it's MEANT to be insecure. Like, you can't have a useful LL.M based assistant that is safe. ",
              "score": 1,
              "created_utc": "2026-02-19 23:13:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cy6jl",
                  "author": "Anarchaotic",
                  "text": "Claude on desktop is pretty good - not nearly as \"powerful\" as OpenClaw with no restrictions, but I can use it to do a lot of useful things by giving shell access with approvals on commands.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69ffgu",
          "author": "TimChr78",
          "text": "OpenClaw wasn’t sold to OpenAI at all - they hired the creator Peter Steinberger. OpenClaw is open source under the GNU 3.0 license.\n\nAnd no OpenAI is definitely not paying Peter Steinberger 1B.",
          "score": 22,
          "created_utc": "2026-02-19 16:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69qkv2",
              "author": "ptear",
              "text": "I mean, instant billionaire status would be impressive.",
              "score": 4,
              "created_utc": "2026-02-19 17:02:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cvjcw",
                  "author": "ANTIVNTIANTI",
                  "text": "it's all contrived. this shit was to boost the doubters back into believers again, lolol, I saw the reddit posts go from the actual reality of the situation after so many months/years of the hype hyping away so much hype, was finally starting to chill like 1-2% lol, then OpenClaw and MoldBook or bot or wtf ever the name was, lololol. BLEW IT BACK UP. Now all we are going to hear for the forevers is \"BRUH YOU'RE LIKE, 14 MONTHS BEHIND BRUH?!\"\n\n\"SKILL ISSUE BRUH?! STOP BEING 14 MONTHS BEHIND BRUH?!\"\n\n\"IF YOU DON'T SUCK MOLDYBLOCKS CLAW YOU'RE LIKE, 16 MONTHS BEHIND BRUH?!\" ",
                  "score": 2,
                  "created_utc": "2026-02-20 02:57:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6y6did",
              "author": "UN_Rocinante",
              "text": "where did you get GNU 3.0 from? it’s MIT on the git repo",
              "score": 1,
              "created_utc": "2026-02-23 13:37:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6az013",
          "author": "huzbum",
          "text": "OpenAI hired the creator, they didn't buy a platform.  It makes a lot of sense when you consider it was promoting burning Anthropic tokens and now it will promote burning OpenAI tokens.  ",
          "score": 5,
          "created_utc": "2026-02-19 20:34:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvoxg",
              "author": "ANTIVNTIANTI",
              "text": "honestly dude was playing it like both corps paid him, he would say \"Half was done on Claude\" then \"Codex coded it\" which... yeah... I'm full of conspiracy today, ignore me :P",
              "score": 2,
              "created_utc": "2026-02-20 02:58:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d7s4g",
                  "author": "huzbum",
                  "text": "I hadn't paid enough attention to call you right or wrong, but it seems like the smart play to make nice with both of them for just this kind of eventuality.  \n\nMy comments were based on the original name being Clawed Bot and the recommendation to use Opus.  \n\nBut yeah, anything that gets more people addicted to burning more tokens and trusting LLMs with more data and responsibility is good for both of them.  ",
                  "score": 1,
                  "created_utc": "2026-02-20 04:18:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6a8qqn",
          "author": "shryke12",
          "text": "The dollar amounts being thrown around by these companies are ludicrous lol.  Shows just how many dollars our government has been printing.",
          "score": 3,
          "created_utc": "2026-02-19 18:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69vnda",
          "author": "locomocopoco",
          "text": "Dude is not just a vibecoder. Go see what he has done previously. SMH. ",
          "score": 9,
          "created_utc": "2026-02-19 17:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvu76",
              "author": "ANTIVNTIANTI",
              "text": "lol I know, he's a grifter, he's got this damn WUNDERKID Blog, errr, WUNDERMAN! Vunder? vvvvuuuunnnder? Anyways. It pissed me off. I wanted him to be a dipshit so bad. LOLOLOLOLOL :P XDXDXD",
              "score": 1,
              "created_utc": "2026-02-20 02:59:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g3467",
                  "author": "Capital-Result-8497",
                  "text": "you and I have very different definitions of a grifter. this dude is an engineer through and through",
                  "score": 1,
                  "created_utc": "2026-02-20 16:28:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ao0n7",
          "author": "type102",
          "text": "They can justify it by saying anything while being in a bubble.",
          "score": 2,
          "created_utc": "2026-02-19 19:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6arz6x",
          "author": "Baader-Meinhof",
          "text": "You missed the joke. The actual figure seems to be that he was hired for $30M.",
          "score": 2,
          "created_utc": "2026-02-19 20:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvz4e",
              "author": "ANTIVNTIANTI",
              "text": "also the tweet kept raising the billions, seemed like, hard to miss. LOL",
              "score": 2,
              "created_utc": "2026-02-20 03:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68zq5m",
          "author": "leonbollerup",
          "text": "where does idea that they paid for it come from ?.. is there any actual proff ?",
          "score": 3,
          "created_utc": "2026-02-19 14:50:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69wa86",
              "author": "oureux",
              "text": "No proff but we might be able to find some proof",
              "score": 1,
              "created_utc": "2026-02-19 17:29:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b50a6",
                  "author": "moderately-extremist",
                  "text": "lemme ask chatgpt...",
                  "score": 1,
                  "created_utc": "2026-02-19 21:04:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6brzzr",
                  "author": "leonbollerup",
                  "text": "Would love to see it",
                  "score": 1,
                  "created_utc": "2026-02-19 23:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6af2gm",
          "author": "structured_flow",
          "text": "Even if openai thought that he was a one trick pony and stuck them in a basement with a broken stapler like office space… They still would’ve paid him a lot of money because it’s all about branding so dude absolutely got paid a lot of money. I highly doubt anything close to 1 billion but definitely in the millions.  \n\nIt’s verified that he met with Zuckerberg who’s paying eight figure salaries for their head of AI departments and he turned them down saying that he “ felt more lines with the mission” at OpenAI.",
          "score": 1,
          "created_utc": "2026-02-19 18:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aieum",
          "author": "siegevjorn",
          "text": "They aqui-hired the dev if I understand it correctly.",
          "score": 1,
          "created_utc": "2026-02-19 19:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bzcjr",
          "author": "desexmachina",
          "text": "Manus was $2B, so they had to escalate and paid Pete $5B, kinda wild",
          "score": 1,
          "created_utc": "2026-02-19 23:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eb3c7",
              "author": "Alert_Efficiency_627",
              "text": "Yeah… if OpenClaw can truly be “open” — not OpenAI’s version of “open” — I’d say it could be worth $50B in the years to come.",
              "score": 1,
              "created_utc": "2026-02-20 10:01:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c98be",
          "author": "ithilelda",
          "text": "he might be paid in memory sticks or chatgpt subscription you know.",
          "score": 1,
          "created_utc": "2026-02-20 00:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cqh1v",
          "author": "No_Success3928",
          "text": "https://www.reddit.com/r/myclaw/s/NBiSm73M4p its an investment!",
          "score": 1,
          "created_utc": "2026-02-20 02:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cwmt8",
              "author": "ANTIVNTIANTI",
              "text": "Omfg I'd offer you my first born for showing me this, but wait.... that got weird, I meant....\n\nshit was hilarious. ",
              "score": 2,
              "created_utc": "2026-02-20 03:04:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cwzcn",
                  "author": "No_Success3928",
                  "text": "Crypto bro got a new AI game!",
                  "score": 1,
                  "created_utc": "2026-02-20 03:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e1lqu",
          "author": "satmun",
          "text": "I used to follow the works of (Peter Steinberger) of PSPDFKit before. He is deep into technical things. Probably his X(formerly Twitter) might still have his tweets. This app was named as best app in iOS for few years I think. He used to discuss Kernel level stuff to optmize rendering of PDFs if I remember correctly and well known in the circle of good devs, atleast in Austria I think. I remember him being close to another dev (professor) who is creator of game engine libgdx. I recently saw him dive deep into C++ and other internals of LLMs. So, its a talent pool and network of devs thats important I think. ",
          "score": 1,
          "created_utc": "2026-02-20 08:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k0dpp",
          "author": "1_H4t3_R3dd1t",
          "text": "OpenClaw isn't that amazing it is just a tool that hosts agents insecurely in your system to execute tasks. You can build the concept yourself. The reason he was hireable was because he made a platform out of it which is awesome and the real thing people should be talking about. ",
          "score": 1,
          "created_utc": "2026-02-21 05:21:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lm5ty",
          "author": "stonkister",
          "text": "\"you can just vibecode an open-source project and make $40B in a couple months now\" \n\nyou can...?",
          "score": 1,
          "created_utc": "2026-02-21 13:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o37bo",
          "author": "Mice_With_Rice",
          "text": "It wasnt sold. Nowhere has any official 1st party source claim to buy/sell openclaw. And if you think about it, it makes no sense it would be sold beacuse its under an MIT license with over 600 contributors.",
          "score": 1,
          "created_utc": "2026-02-21 21:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71xys9",
          "author": "ketoatl",
          "text": "I hope they add guard rails so I dont need another computer to run it. ",
          "score": 1,
          "created_utc": "2026-02-24 00:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o692nwj",
          "author": "Investolas",
          "text": "They paid that much for him to sign an nda and stop development. ",
          "score": 0,
          "created_utc": "2026-02-19 15:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o698mfd",
              "author": "Theseus_Employee",
              "text": "Well they didn’t pay him enough apparently because he’s still developing it.",
              "score": 8,
              "created_utc": "2026-02-19 15:36:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69j1e6",
                  "author": "Technical_Ad_440",
                  "text": "have they fixed the security vulnerabilities yet?",
                  "score": 5,
                  "created_utc": "2026-02-19 16:26:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o699t7k",
                  "author": "Investolas",
                  "text": "Sure but it will never be what it would have been without their intervention",
                  "score": -2,
                  "created_utc": "2026-02-19 15:41:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9xifw",
      "title": "Devstral Small 2 24B + Qwen3 Coder 30B Quants for All (And for every hardware, even the Pi)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/s8yw2jndynkg1.png",
      "author": "enrique-byteshape",
      "created_utc": "2026-02-20 14:56:54",
      "score": 139,
      "num_comments": 70,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9xifw/devstral_small_2_24b_qwen3_coder_30b_quants_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6fxrl7",
          "author": "blksunday",
          "text": "Awesome! I use both of these on a Mac mini M4 24GB. I’ll be trying yours later today. Looks promising,.",
          "score": 5,
          "created_utc": "2026-02-20 16:04:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g2td8",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We're very interested in Mac speedups!",
              "score": 3,
              "created_utc": "2026-02-20 16:27:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g4sr3",
          "author": "mac10190",
          "text": "Sweet! I'll give it a shot later this afternoon.\n\nCurrently running dual R9700 32GB GPUs and an RTX 5090 32GB. Been using the dual R9700s to host larger models to act as the brain/orchestrator and then qwen 3 coder 30b on the 5090 for code generation and then tied it all together under the umbrella of Opencode. Testing this as a potential replacement for some of my Gemini CLI tasks.",
          "score": 4,
          "created_utc": "2026-02-20 16:36:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pmc9a",
          "author": "DarthFader4",
          "text": "Excellent work! This is exactly what I've been looking for. I feel like targeting high-end 16GB GPUs is a key audience, like gamers who want to dabble in local LLMs.  I think there are a lot of exciting developments ahead in optimizing models of this size. They're more practical and approachable than requiring a dedicated high-RAM/VRAM setup and we've started seeing models that can actually be useable. Keep up the great work! I've just joined the Hugging Face community.",
          "score": 4,
          "created_utc": "2026-02-22 03:10:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r38fq",
              "author": "enrique-byteshape",
              "text": "Thank you for the kind words! This was exactly part of our motivation. First, there are great quants that fit on larger VRAM devices, but they might be a bit too slow because they're just made to fit (and not benchmarked, people just assume they'll work). Then, there's a clear accuracy cliff when going lower than 4bpw, and we know our technology excells below those ranges.",
              "score": 1,
              "created_utc": "2026-02-22 10:42:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fwurh",
          "author": "swupel_",
          "text": "Love the graph style",
          "score": 3,
          "created_utc": "2026-02-20 16:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fxh9j",
              "author": "enrique-byteshape",
              "text": "Thanks! Don't tell the team, but the style is on me ;)",
              "score": 3,
              "created_utc": "2026-02-20 16:02:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gklbd",
          "author": "BillDStrong",
          "text": "So, are these suitable for speculative decoding in llama.cpp? I would assume so, and since you have worked to keep them from falling off the cliff, they could do most of the work and then let a larger version fix the difference, which might result in faster perf for the same accuracy as the normal models?\n\nMaybe?\n\nThe best I have is a P40 24GB, so will  have to test it later.",
          "score": 3,
          "created_utc": "2026-02-20 17:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gxbsx",
              "author": "enrique-byteshape",
              "text": "We have not tried speculative decoding at all with these models, but they have good quality and are performant. If you have a way to use them for such use case, we assume they will work, but we can't really promise anything!",
              "score": 2,
              "created_utc": "2026-02-20 18:46:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ip5r2",
                  "author": "TomLucidor",
                  "text": "Qwen3 and Nemotron has native MTP, please try them as well!",
                  "score": 1,
                  "created_utc": "2026-02-21 00:12:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rr1r5",
          "author": "PaMRxR",
          "text": "Always excited I see new byteshape models! Just the right size for my RTX 3090, and they run roughly 2x faster than other quants. Here's some numbers for Devstral-Small-2:\n\n    prompt eval time =    1120.97 ms /  2004 tokens (    0.56 ms per token,  1787.73 tokens per second)\n           eval time =   10315.36 ms /   569 tokens (   18.13 ms per token,    55.16 tokens per second)\n\nRunning with this command:\n\n    llama-server\n    --model \"${models_path}/Devstral-Small-2-24B-Instruct-2512-IQ3_S-3.47bpw.gguf\"\n    --mmproj \"${models_path}/mmproj-bf16.gguf\"\n    --split-mode none\n    --seed 42\n    --ctx-size 128000\n    --n-gpu-layers 99\n    --fit on\n    --fit-target 256\n    --temp 0.15\n    --top-p 1\n    --min-p 0.01\n    --top-k 40\n    --jinja\n    --repeat-penalty 1\n    --cache-type-k q8_0\n    --cache-type-v q8_0\n    -ub 1024\n    --cache-ram 16000\n\nMany thanks, please keep up doing/sharing this amazing work.",
          "score": 3,
          "created_utc": "2026-02-22 13:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fr09x",
          "author": "vanguard2286",
          "text": "Which one would you suggest for rrx 4070 8gb vram? I'm kind of new to self hosting LLMs and kind of not quite understanding the chart. I would love your input.",
          "score": 4,
          "created_utc": "2026-02-20 15:32:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsfuw",
              "author": "enrique-byteshape",
              "text": "Thank you for your interest! 8GB of VRAM is fairly limited, so not a lot of good quality models will fit, but if you want to play around with our models, you can try our Devstral [IQ2\\_S-2.34bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.34bpw.gguf) (75.1% quality of original model), our [IQ2\\_S-2.43bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.43bpw.gguf) (80.3% quality) or our [IQ3\\_S-2.67bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ3_S-2.67bpw.gguf) (87.2% quality, but will fit a smaller context length). You can also try offloading embeddings or some layers on our higher quality quants with ollama or llama.cpp, but this will reduce performance heavily. Let us know what you end up doing and if you enjoy the quants!",
              "score": 5,
              "created_utc": "2026-02-20 15:39:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6g405l",
                  "author": "vanguard2286",
                  "text": "Thank you!",
                  "score": 2,
                  "created_utc": "2026-02-20 16:32:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip21p",
                  "author": "TomLucidor",
                  "text": "Please start testing linear attention models like Nemotron-3-Nano or Kimi-Linear or Ring-Mini-Linear-2.0 or Granite-4.0 with the same methodology. Cus if they are more quant sensitive, that would be very sad. (maybe Gemma 3 and GPT-OSS-20B SWA also get support?)",
                  "score": 1,
                  "created_utc": "2026-02-21 00:11:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fxwfa",
          "author": "jarec707",
          "text": "You mentioned a blog in the post. Link please?",
          "score": 2,
          "created_utc": "2026-02-20 16:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g0nq1",
              "author": "enrique-byteshape",
              "text": "It's at the end of the post! Right here: [https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/](https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/)",
              "score": 3,
              "created_utc": "2026-02-20 16:17:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6i84my",
          "author": "Clear-Lab3427",
          "text": "Thanks so much!",
          "score": 2,
          "created_utc": "2026-02-20 22:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j2h8e",
          "author": "Useful_Disaster_7606",
          "text": "I will forever rue the day I bought an RTX 3060 8GB. But then again I did buy it for less than $220 so I guess it's not that bad. \n\nJust out here feeling FOMO seeing all these amazing models. So close yet so faar.",
          "score": 2,
          "created_utc": "2026-02-21 01:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j60sm",
              "author": "enrique-byteshape",
              "text": "You CAN actually try our low bits per weight Devstral quants so that you don't feel as left out! Under 8GB with enough context length to test them!",
              "score": 1,
              "created_utc": "2026-02-21 01:54:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6j4pyp",
          "author": "Snoo_24581",
          "text": "Thanks for putting this together! Been waiting for good quants of these models. The 24B size is perfect for my 24GB VRAM setup.\n\nHow's the performance on coding tasks compared to the full precision versions? Any significant quality drop?",
          "score": 2,
          "created_utc": "2026-02-21 01:46:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j8efu",
              "author": "enrique-byteshape",
              "text": "If you choose our highest bit per weight quants there is no visible degradation on our benchmarks and our qualitative assessments",
              "score": 1,
              "created_utc": "2026-02-21 02:09:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jtd4a",
          "author": "Count_Rugens_Finger",
          "text": "Going to try these on my RX 9070 XT",
          "score": 2,
          "created_utc": "2026-02-21 04:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfzkd",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We tested on an RX 9060 XT 16GB and they got a speedup versus other quants",
              "score": 1,
              "created_utc": "2026-02-21 13:11:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6psl4t",
                  "author": "Count_Rugens_Finger",
                  "text": "Yes I seem to get a pretty good speedup.  I do not have the means to evaluate accuracy.\n\nFor Qwen-3-coder, I go from about 71 t/s at Q4_K_M to about 160 t/s at IQ5 3.48 bpw, both at max GPU offload and the default 4k context.  More than 2x speedup which is amazing.\n\nFor Devstral-2, I can run the \"IQ8\" 4.04 bpw quant at about 34 t/s with max GPU offload and 4k context.  So, can't hold a candle to the 4080, but usable.  Still playing with this one don't know what the speedup looks like.",
                  "score": 2,
                  "created_utc": "2026-02-22 03:53:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ly7ep",
                  "author": "Count_Rugens_Finger",
                  "text": "Your Blog claims that your optimization is specific to Nvidia's 40 and 50 series hardware.  Would you expect the Radeon Vulkan implementation to be just *not as good*, or actively *worse* with your builds vs the standard Q4_K_M quants?",
                  "score": 1,
                  "created_utc": "2026-02-21 15:02:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lgt9b",
          "author": "xeeff",
          "text": "been following you on huggingface for the longest time - finally glad to see some new models. been waiting for these one so long i kinda forgot they are still great models. keep up the good work.\n\np.s. any notes on the model roadmap and an ETA? :)",
          "score": 2,
          "created_utc": "2026-02-21 13:16:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6liwa0",
              "author": "enrique-byteshape",
              "text": "We can't really promise anything, but some diffusion models are in the near to-do list, and we will try to move onto thinking models (which we expect will suppose a big challenge when evaluating them)",
              "score": 2,
              "created_utc": "2026-02-21 13:30:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ljt5s",
                  "author": "xeeff",
                  "text": "always thought why all the models are instruct, but them being harder to evaluate makes sense. did not expect diffusion models to be mentioned, though. any ones in particular? if you'd prefer to not mention, that's perfectly okay",
                  "score": 2,
                  "created_utc": "2026-02-21 13:36:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6re2y1",
          "author": "CalmAndLift",
          "text": "Probé el Qwen3 coder y excelente a 5 tps en mi laptop Intel core ultra 5 con 24 gigas de ram en lmstudio.\nExcelente trabajo",
          "score": 2,
          "created_utc": "2026-02-22 12:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rfpkn",
              "author": "enrique-byteshape",
              "text": "Muchas gracias! Espero que lo disfrutes!",
              "score": 2,
              "created_utc": "2026-02-22 12:33:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6futyd",
          "author": "floppypancakes4u",
          "text": "So your qwen model wont work with a 4090? Do either support a 3090? Looking forward to trying these out.",
          "score": 1,
          "created_utc": "2026-02-20 15:50:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fvbyv",
              "author": "enrique-byteshape",
              "text": "It does support any type of hardware, it's just that our performance benchmarks are only on the hardware that we have available. Sorry we didn't make that clear enough. Our Qwen on the 4090 runs similarly to the 5090 in terms of comparing it to other quants, albeit at a slower TPS.",
              "score": 2,
              "created_utc": "2026-02-20 15:52:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fvrdv",
                  "author": "floppypancakes4u",
                  "text": "Excellent! I'll test both this afternoon.",
                  "score": 3,
                  "created_utc": "2026-02-20 15:54:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6imy7o",
                  "author": "CTR1",
                  "text": "Following up on the question regarding the 3090 compatibility: do you have suggestions for a ideal model to use with a 5800xt + 64gb 3200mhz ram + 3090 pc build?\n\nIdeally something that balances quality | TPS | context and maybe tool calling too? I know that might be a tough ask",
                  "score": 2,
                  "created_utc": "2026-02-20 23:59:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h39q3",
          "author": "oliveoilcheff",
          "text": "What about strix halo? Are there some performance gains there? Thanks!",
          "score": 1,
          "created_utc": "2026-02-20 19:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hx62p",
              "author": "enrique-byteshape",
              "text": "We don't have one in hand, but there should be performance gains on any type of hardware. We would love to hear of the performance you get on it!",
              "score": 1,
              "created_utc": "2026-02-20 21:39:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h3ei6",
          "author": "Simple-Worldliness33",
          "text": "\n\nHi !\n\nThanks for your work! I didn't bench yet but I need to understand completely.\n\nFor an example, I'm using unsloth iq4\\_NL currently with 2 rtx 3060, i got 70/76 tks.\n\nWhich model you are offering should I choose to compare with? I tried the iq4\\_ks but I didn't have the same perf. (Only 35/40tks)",
          "score": 1,
          "created_utc": "2026-02-20 19:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ia35g",
              "author": "enrique-byteshape",
              "text": "Hi! Thanks for trying our models! The performance you might get out of them can vary a lot depending on the hardware, and/or on whether the model is being loaded and ran correctly. Would you mind being more specific about your setup and llama.cpp environment and parameters?",
              "score": 2,
              "created_utc": "2026-02-20 22:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kcwvl",
          "author": "geringonco",
          "text": "Are there any rankings sites for these models?",
          "score": 1,
          "created_utc": "2026-02-21 07:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfsui",
              "author": "enrique-byteshape",
              "text": "Sadly no because no one is willing to evaluate all the released quants to create such ranking sites. It is very expensive",
              "score": 2,
              "created_utc": "2026-02-21 13:09:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l5258",
          "author": "shankey_1906",
          "text": "Any recommendation for Strix Halo?",
          "score": 1,
          "created_utc": "2026-02-21 11:43:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfq11",
              "author": "enrique-byteshape",
              "text": "It depends on the underlying framework and kernels. Most likely our CPU versions will work best, but it would require testing them out",
              "score": 1,
              "created_utc": "2026-02-21 13:09:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l64qw",
          "author": "puru991",
          "text": "When qwen 3.5?",
          "score": 1,
          "created_utc": "2026-02-21 11:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lg5fv",
              "author": "enrique-byteshape",
              "text": "Thank you for the interest, we are aware of the release (as well as of many others), but it's hard to keep up considering that evaluating these quants takes a lot of resources and time. We have to be picky when releasing models, so we usually go by what is popular and what people might really want",
              "score": 1,
              "created_utc": "2026-02-21 13:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lr2fc",
          "author": "Embarrassed-Boot5193",
          "text": "Eu testei o modelo Devstral-Small-2-24B-Instruct-2512-IQ3\\_S-3.47bpw.gguf e não coube na minha GPU de 16GB com 32k de contexto. Vocês estão quantizando o kv cache para isso acontecer?",
          "score": 1,
          "created_utc": "2026-02-21 14:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mohrm",
              "author": "enrique-byteshape",
              "text": "Hey! When we benchmarked Devstral on the different hardware ranges we did so without the vision tower. That's why it might not fit with a context length of 32K. Sorry for the inconvenience!",
              "score": 2,
              "created_utc": "2026-02-21 17:15:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mhyh1",
          "author": "Thrynneld",
          "text": "The new frontier seems to be figuring out how much cruft can be removed from a model before it falls over. So far the rule of thumb has been always use the largest model (parameter wise) that can fit in memory at a quant that does turn itself into gibberish. I've noticed that models with more parameters seem to hold up better at lower quants than smaller models. Is there any chance you guys will be publishing quants of larger popular models? Something like qwen3-coder-next, or even qwen 3.5? What is your bottleneck in this quantization process? do you need to run inference to determine the importance of the weights to quant down more or less? I'm loving Qwen 3.5 on my mac studio, but it sucks up most of my memory at a 3 bit quant while itseems capable enough at 3 bit, I wonder if it would perform better at a \"smarter\" 3 bit version, or even a 2 bit version :)",
          "score": 1,
          "created_utc": "2026-02-21 16:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mmq3c",
              "author": "enrique-byteshape",
              "text": "Our own research and other groups' research has been showing for a while that larger models have a much larger tolerance to quantization and pruning. We've also seen some weight cause outlier activations that matter the most when actually running inference. And we have also observed larger models being quantized aggressively but still being better than smaller models with the same size. Qwen 3.5 is in our roadmap, but our current bottleneck is evaluating these quants so that people can be informed while downloading them. The datatype learning process is actually quite fast on our technology",
              "score": 1,
              "created_utc": "2026-02-21 17:06:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n5kop",
          "author": "siegevjorn",
          "text": "What benchmark are you running?",
          "score": 1,
          "created_utc": "2026-02-21 18:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n6bht",
              "author": "enrique-byteshape",
              "text": "Hey! From the very end of our blog post:  \n\"Devstral supports both tool calling and vision, so we evaluated it on:\n\n* **BFCL\\_V3** for tool calling\n* **GSM8K\\_V** for vision\n* **LiveCodeBench V6** and **HumanEval** for coding\n* **GSM8K** and **Math500** for math\n* **MMLU** for general knowledge\n\nThe reported score is the mean across these benchmarks, with each benchmark normalized to the original model's score.\n\nQwen was evaluated using the same setup, with two exceptions:\n\n* No **GSM8K\\_V** (no vision support)\n* No **MMLU** (not a general knowledge evaluation)\n\nAll evaluations were run with llama.cpp `b7744`. We used 4K as the minimum context window required for a model to be considered \"fit\" on a given device.\"",
              "score": 1,
              "created_utc": "2026-02-21 18:44:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ptg3m",
          "author": "Cuaternion",
          "text": "¿En serio hay para Raspberry pi?",
          "score": 1,
          "created_utc": "2026-02-22 03:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r2a94",
              "author": "enrique-byteshape",
              "text": "Si! Los hay!",
              "score": 1,
              "created_utc": "2026-02-22 10:33:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71vaor",
          "author": "Numerous_Mulberry514",
          "text": "could you do qwen coder next as well?",
          "score": 1,
          "created_utc": "2026-02-24 00:36:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fvlkz",
          "author": "peyloride",
          "text": "Nice work but I think the real baselin about context should not be 32k because that's very limited in these days. Since these are coding models, context adds very quick in coding agents. I wonder what's the story whe context is around 200k? or even something like 100k? I don't have an idea about what should be the baseline sorry, but 32k seems low.",
          "score": 1,
          "created_utc": "2026-02-20 15:54:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fwyin",
              "author": "enrique-byteshape",
              "text": "32k context is for performance measurements only, which will scale depending on the context length used. For the evaluations we do not limit context length, so those should not be biased. The models will run with any context length as long as it fits. And yes, with longer context lengths, activations start becoming the bottleneck. Sadly, llama.cpp doesn't support quantizing activations to arbitrary datatypes, so at the moment we are limited by that, but our algorithm can also learn the datatypes for them",
              "score": 3,
              "created_utc": "2026-02-20 16:00:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gkqxy",
                  "author": "peyloride",
                  "text": "Yeah I see  your point but context length is important for vram usage. It also affects the accuracy and the TPS (I might be wrong about this). So what I'm trying to say is since these are coding models, you should not test it in 32k context. It might be enough for general usage, but I don't think that's the case for coding models. \n\nIf this is not possible at the time being that's okey; just wanted to flag this out. ",
                  "score": 2,
                  "created_utc": "2026-02-20 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9zo0u",
      "title": "Why AI wont take your job and my made up leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/qaz3ln1ncokg1.jpeg",
      "author": "Eventual-Conguar7292",
      "created_utc": "2026-02-20 16:16:34",
      "score": 73,
      "num_comments": 67,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9zo0u/why_ai_wont_take_your_job_and_my_made_up/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ggzb5",
          "author": "promethe42",
          "text": ">22 users programming with ChatGPT\n\nYeah it's not 2024 anymore...\n\nAll the people claiming coding is done are not using ChatGPT and manual prompts. They are using agentic coding system with meta prompts, skills, etc... with tasks running for 1 to 2 hours autonomously. And those systems can run on local LLMs.\n\nSo those numbers need a serious update.",
          "score": 54,
          "created_utc": "2026-02-20 17:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h43ge",
              "author": "ForsookComparison",
              "text": ">>  users programming with ChatGPT\n\nI think this is just the author failing to grasp that chatgpt != ai. The actual benchmark is *(very very likely)* not using back and forth chat sessions.\n\nSometimes it's easier to just let marketing say \"Chatgpt\"",
              "score": 10,
              "created_utc": "2026-02-20 19:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h8axj",
                  "author": "Malkiot",
                  "text": "I built a tool that decomposes projects into dependency graphs, generates implementation plans, assigns parallel AI agents to work simultaneously, and enforces boundaries so they don't stomp on each other.\n\nI built that tool *with* AI because I was annoyed with managing my other projects.\n\nThe studies here are measuring people using ChatGPT like a search bar. People failing says more about them than it does about the utility of AI.",
                  "score": 15,
                  "created_utc": "2026-02-20 19:37:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gsd10",
          "author": "Lissanro",
          "text": "It is true that the current LLMs and agents cannot yet do freelance work on their own, but \"small bites only\" era has passed long time ago for me.\n\n\nWhen I was just beginning integrating LLMs to my workflow, even basic things how to center a div container, LLMs often struggled with, especially in more complicated layouts. It was more efficient to either try a few things or just google it.\n\n\nNowadays, I can tell Kimi K2.5 make an entire website and leave it overnight running on my PC, providing warmth during winter nights as a bonus... and in most cases it gets almost everything perfect, except I still need to provide images and polish layout, fix small issues. Even with vision, K2.5 is still not precise enough to clearly see some mistakes, or to be able judge icon quality. But it still can describe ideas what icons to put where and make simple SVG placeholders, some of them actually can be good enough, but most need to be replaced.\n\n\nThat said, my prompts are quite detailed and I have over a decade of experience in web design and programming, in addition to being 2D and 3D artist. So I can use both hybrid or traditional methods to produce required images, animations or code, if it is needed for good result. Using my experience, I can specify precisely what I need, or I can provide enough context from previous work so it is clear what is needed, so the better the AI, the faster I can get the job done, or the more work I can take, while still maintain quality, both visual and of the code base, since with or without AI, I do my work based on my skills and experience, using AI just allows me to be more efficient.\n\n\nI think using AI is skill on its own, and that sometimes needs to be relearned (when things change or when need to use new tools / inference backends) and adapted to the situation. At least, with AI models that are available today.",
          "score": 19,
          "created_utc": "2026-02-20 18:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h0b3h",
              "author": "TheAncientOnce",
              "text": "what hardware do you use if you don't mind me asking? Is it full Kimi2.5 or are you running a quantized variants?",
              "score": 1,
              "created_utc": "2026-02-20 18:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h2nu8",
                  "author": "Lissanro",
                  "text": "Yes, I use the full version (Q4\\_X, preserving INT4 weights in GGUF format, along with F16 mmproj for vision). I run K2.5 on 64-core EPYC 7763 + 8-channel 1 TB 3200MHz RAM + 96 GB VRAM (made of 4x3090) + 8 TB NVMe for AI models and 2 TB NVMe SSD for the OS + \\~120 TB disk space on HDDs for storage and backups. If interested to know more, in my another comment I shared a photo and other details about my rig including what PSUs I use and what the chassis look like: [https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/](https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/)",
                  "score": 6,
                  "created_utc": "2026-02-20 19:10:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gt32f",
          "author": "nomorebuttsplz",
          "text": "Sound like you've never heard of coding agents. \n\nHow the fuck is this post getting upvotes. You're like a year behind.",
          "score": 11,
          "created_utc": "2026-02-20 18:27:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hnlfw",
              "author": "alcalde",
              "text": "Prove otherwise. What secret knowledge do you have that the rest of the world does not?",
              "score": -2,
              "created_utc": "2026-02-20 20:52:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hp5z8",
                  "author": "nomorebuttsplz",
                  "text": "OP is literally describing trying to code without an agent \"Paste the relevant code, show what you're working with\"\n\nThat's obsolete.",
                  "score": 10,
                  "created_utc": "2026-02-20 21:00:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip66o",
                  "author": "Low_Amplitude_Worlds",
                  "text": "“secret knowledge” 🤨",
                  "score": 3,
                  "created_utc": "2026-02-21 00:12:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6g9bwr",
          "author": "Vozer_bros",
          "text": "from what I have done last month with AI automation, I dont agree with you.",
          "score": 20,
          "created_utc": "2026-02-20 16:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gusd7",
              "author": "_VirtualCosmos_",
              "text": "What have you done? and with which AI if I may ask",
              "score": 3,
              "created_utc": "2026-02-20 18:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hpbdw",
                  "author": "teamharder",
                  "text": "Copy of comment. Opus 4.6. Ive done more than what is listed below.\n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.",
                  "score": 8,
                  "created_utc": "2026-02-20 21:00:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6iageo",
                  "author": "Vozer_bros",
                  "text": "generate unlimited chapters of novel/story with auto grow characters, my main AI are GLM and Claude Opus",
                  "score": 1,
                  "created_utc": "2026-02-20 22:48:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6grxbq",
              "author": "teamharder",
              "text": "Thats my take too. Idk how someone could do this much work studying it and come to this conclusion. ",
              "score": 3,
              "created_utc": "2026-02-20 18:21:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ibkep",
                  "author": "Vozer_bros",
                  "text": "yep, I dont want to debate, but digital world is changing faster then ever, we should aware all accept that instead of pretending AI is stupid",
                  "score": 1,
                  "created_utc": "2026-02-20 22:54:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hnip2",
                  "author": "alcalde",
                  "text": "This is the conclusion of the entire planet. To claim otherwise is an extraordinary claim requiring extraordinary evidence. No one's cranked out a new operating system via Claude Code.",
                  "score": -2,
                  "created_utc": "2026-02-20 20:52:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h0z2h",
          "author": "ILikeBubblyWater",
          "text": "Mate if you believe AI cant churn out fully fledged websites you clearly are not doing this full time.\n\nWith stuff like multi provider planning loops and ralph loops and claude code you can absolutely push out whole products in a couple hours.\n\nWriting code is a deprecated way of coding already, people are just coping and apply unreasonable high standards to AIs that they would not ask from a human",
          "score": 5,
          "created_utc": "2026-02-20 19:02:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ho80r",
              "author": "alcalde",
              "text": "> people are just coping and apply unreasonable high standards to AIs that they would not ask from a human\n\nYou don't expect humans to deliver functioning code with tests and documentation?",
              "score": 1,
              "created_utc": "2026-02-20 20:55:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ho1l1",
              "author": "alcalde",
              "text": "Websites aren't code. ",
              "score": -2,
              "created_utc": "2026-02-20 20:54:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gq4a7",
          "author": "bakawolf123",
          "text": "Cope is good but the better you learn to use them the scarier it gets...  \nWhile I don't think pure AI agents replacing humans is a realistic approach in foreseeable future, a cheaper weaker dev competent in using wide range of AI tools replacing senior staff is quite a possibility.",
          "score": 6,
          "created_utc": "2026-02-20 18:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gs2tj",
              "author": "Healthy-Nebula-3603",
              "text": "You know AI agents did not even exist a year ago (models were not trained this way yet )  not even codex-cli or claudie-cli ...\n\nAnd you are claiming AI will not replace you soon ?",
              "score": -1,
              "created_utc": "2026-02-20 18:22:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6grac7",
          "author": "Healthy-Nebula-3603",
          "text": "Sure buddy... keep your head in the sand ....",
          "score": 6,
          "created_utc": "2026-02-20 18:19:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ggscz",
          "author": "TopTippityTop",
          "text": "These people must not be using 5.3 codex",
          "score": 3,
          "created_utc": "2026-02-20 17:31:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h6b9h",
          "author": "Mystical_Whoosing",
          "text": "this is such an old take on this, even 1 year ago this shouldn't be the case, but today? You just share that here is a new tech and the people you survey cannot keep up",
          "score": 4,
          "created_utc": "2026-02-20 19:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g15s2",
          "author": "Purple_Ice_6029",
          "text": "Also, it will get much more expensive as the investors require an ROI, making it not as appealing. *pop*",
          "score": 5,
          "created_utc": "2026-02-20 16:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ge91k",
              "author": "DHFranklin",
              "text": "I imagine this might be the year that \"good enough\" AI is paired with really good centauring and the UI/UX will show more custom built stuff. So that the cost per hour in sheparding the AI would have that demonstable ROI.\n\nSo just like how combines don't drive themselves, million dollar AI workflows won't either. That doesn't mean that they won't radically change the work that's done.",
              "score": 0,
              "created_utc": "2026-02-20 17:19:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g6nrd",
          "author": "Otherwise_Wave9374",
          "text": "This tracks with what Ive seen: benchmarks look great, but \"agent does real freelance work end to end\" is mostly about reliability, context management, and actually knowing when it doesnt know. The advice about small bites + verification is the only sane way to use agents today. I also think tooling (tests, linters, sandboxes, traces) matters more than the model for most workflows. If you want more practical patterns for using AI agents without falling into the prompt loop, Ive got a few notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-20 16:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6grphw",
          "author": "teamharder",
          "text": "What were the tasks? How does the score correlate to the human percentage? Given my current experience, these numbers dont add up. \n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.\n\n\nI did this all in my spare time on the weekends. I dont know how to write code. I just plan and test thoroughly with use case scenarios. Realistically Claude did the work and gets the credit for it. How that measures out to 2.46% is what confuses me. ",
          "score": 3,
          "created_utc": "2026-02-20 18:21:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g2a2i",
          "author": "Jumpy_Ad_2082",
          "text": "now present this to a manager and convince him who is more profitable.",
          "score": 3,
          "created_utc": "2026-02-20 16:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g49z6",
          "author": "Needausernameplzz",
          "text": "great write up",
          "score": 2,
          "created_utc": "2026-02-20 16:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h0opz",
          "author": "Smarterchild1337",
          "text": "The broad conclusions of this post are at least a year out of date, which is an eon in terms of AI progress during that time",
          "score": 2,
          "created_utc": "2026-02-20 19:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g759p",
          "author": "masterlafontaine",
          "text": "Can you elaborate a bit more on the columns? What are these integrals with games?",
          "score": 1,
          "created_utc": "2026-02-20 16:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ho6qi",
          "author": "thedarkbobo",
          "text": "We are doomed, I don't agree, we have max few years ;p",
          "score": 1,
          "created_utc": "2026-02-20 20:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ianih",
          "author": "Expert-Reaction-7472",
          "text": "LLMs are very very good at writing code to the point where anyone writing code by hand will be out of a job as a result of that. Anyone who thinks otherwise is delulu.",
          "score": 1,
          "created_utc": "2026-02-20 22:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ja2mm",
          "author": "keamo",
          "text": "Tell me you're using AI to create excel files without telling me you're not using  using AI to write yourself a frontend for data visualizations. Tell me you're only using excel by showing me a screen shot of you not asking AI to create a tailwind/react/vite with chartjs visuals, tell me your middle name.",
          "score": 1,
          "created_utc": "2026-02-21 02:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nqzpp",
          "author": "HiggsBoson2738",
          "text": "\"how do I center a div\", FFS...",
          "score": 1,
          "created_utc": "2026-02-21 20:30:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pnaj3",
          "author": "Tall-Wasabi5030",
          "text": "You'll be one of the first to be replaced with AI ",
          "score": 1,
          "created_utc": "2026-02-22 03:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q73ty",
          "author": "Big-Masterpiece-9581",
          "text": "None of this has anything to do with a) fewer jobs because skilled seniors can do much more with AI and not having to waste time teaching juniors many of whom are duds. b) CEOs want to lay people off and blame AI regardless of quality. They no longer care about quality products.",
          "score": 1,
          "created_utc": "2026-02-22 05:42:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g33p1",
          "author": "Eventual-Conguar7292",
          "text": "Kimi 2.5 Is better than Deepseek?\n\nBut my real question What use case In Local LLM,Here are use cases I can think of\n\n1. Simple code generation - Web visualizations, data dashboards, interactive charts\n2. Image generation - Ads, logos, creative visual content (Using LLM to understand text prompts)\n3. Audio production - Sound effects, voice-over merging, track separation (Using LLM to understand text prompts)\n4. Text-based tasks - Report writing, data retrieval, web scraping",
          "score": 1,
          "created_utc": "2026-02-20 16:28:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6g6ug0",
              "author": "twack3r",
              "text": "Why are you questioning your own result?",
              "score": 2,
              "created_utc": "2026-02-20 16:45:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gciey",
                  "author": "Eventual-Conguar7292",
                  "text": "I said the benchmarks were made up in title a in way.\n\nHow I created the benchmark:\n\n1. Test it myself first - I spend my time running Sonnet 4, Gemini, DeepSeek, etc. on zero-shot tasks, then compare the outputs\n2. Then I back it up -  I Find public benchmarks that support my findings\n\nWhy does this exist? I don't make any money from posting, so you get free quality. I'd rather trust my own opinion than internet benchmarks, but I probably cant give you value with time I got.",
                  "score": 1,
                  "created_utc": "2026-02-20 17:11:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gga7u",
              "author": "Zerokx",
              "text": "5. Human Computer Interfacing - Voice Recognition can be small and local, executing simple commands based on users in home automation or whatever in human language  \n6. Privacy - Any task which would expose a lot of private information to the outside world, like refining your CV, or searching through personal files  \n7. Freedom - Using the AI for tasks that would be censored by company policies, like making satirical content of public figures, content rated for adults, whatever mischievous acts you come up with like some will use it for scamming or tell them how to create illegal substances etc.",
              "score": 1,
              "created_utc": "2026-02-20 17:29:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ga6hc",
          "author": "bgptcp179",
          "text": "Hmmm, sounds like something an AI agent would say.  GET HIM!\n\nSeriously tho, cool info",
          "score": 1,
          "created_utc": "2026-02-20 17:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g20j1",
          "author": "nomorebuttsplz",
          "text": "Question:\n\nHow can a benchmark like RLI account for the fact that once people recognize an AI can be given a task and can complete it, it won't be considered human work anymore? It seems like an ever-changing standard specifically focused on the delta between human and AI work.",
          "score": -2,
          "created_utc": "2026-02-20 16:23:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfxnm",
      "title": "Open source AGI is awesome. Hope it happens!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mkun0tc1k8lg1.jpeg",
      "author": "Koala_Confused",
      "created_utc": "2026-02-23 12:18:36",
      "score": 58,
      "num_comments": 18,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcfxnm/open_source_agi_is_awesome_hope_it_happens/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6xy1dt",
          "author": "05032-MendicantBias",
          "text": "OpenAI had a Foundation \"controlling\" it. The instant the foundation tried to excercise control, they were overruled by dollars.\n\nThis has to be approached at a regulation level. Training AI taps into every data on the internet. It's only fair that every model provider is forced to release it open source so it can be inspected.",
          "score": 31,
          "created_utc": "2026-02-23 12:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypkwg",
              "author": "DHFranklin",
              "text": "And regulation is overruled by dollars every time.\n\nIf there is one tax shelter in the carribean holding out, it will be where the next model is developed. We will never get ahead of this.",
              "score": 8,
              "created_utc": "2026-02-23 15:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70j6l7",
                  "author": "d_the_great",
                  "text": "The best thing we can do is have alternative structures.\n\nIf the government and industry aren't gonna open it up for us and release something safe, we have to do it ourselves.",
                  "score": 1,
                  "created_utc": "2026-02-23 20:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72hz5p",
              "author": "voyager256",
              "text": "But... but this time it will be different. Trust me, bro.",
              "score": 1,
              "created_utc": "2026-02-24 02:46:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z2qjn",
          "author": "BreathingFuck",
          "text": "Not X. Not Y. Z. \n\nThey aren’t getting anywhere if they couldn’t write that paragraph on their own.",
          "score": 9,
          "created_utc": "2026-02-23 16:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xympk",
          "author": "No_Clock2390",
          "text": "Sounds like a bunch of rubbish",
          "score": 18,
          "created_utc": "2026-02-23 12:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5kyd",
          "author": "flonnil",
          "text": "\"jippity, come up with a bunch of marketing words devoid of any meaning at all. no mistakes.\"\n\ngo perceive yourself.",
          "score": 14,
          "created_utc": "2026-02-23 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yr2yz",
          "author": "DHFranklin",
          "text": "This is pissing into the wind. AGI will be open sourced regardless of who tries to contain or privatize it. It will be closed and bottled for maybe a few months before someone else gets that far.",
          "score": 6,
          "created_utc": "2026-02-23 15:28:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z6trq",
              "author": "rafaelRiv15",
              "text": "How can you be so sure ? I honestly can't understand the business model of open source model and I wouddn't be surprised if they get close source eventually",
              "score": 2,
              "created_utc": "2026-02-23 16:42:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zaymg",
                  "author": "DHFranklin",
                  "text": "I'm sorry I may not be understanding you clearly. Do you think that we will have an open source AGI model that will then go closed source?\n\nThis isn't about a particular business model. Look at all the different non-profit examples of open sourced software. Look at the Chinese models that reverse engineered the SOTA from the weights alone.\n\nNo one single operation is a year ahead of the others. We're seeing the cash investments turn into infrastructure as we speak. We're actually building the hardware that was a huge bottleneck.",
                  "score": 2,
                  "created_utc": "2026-02-23 17:01:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yxds1",
          "author": "nijuu",
          "text": "Good in theory but once they have a good product whose to say they won't go for the $$$ bag...",
          "score": 3,
          "created_utc": "2026-02-23 15:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z2zcs",
          "author": "UsedGarbage4489",
          "text": "naive 🧠☠️🤡",
          "score": 2,
          "created_utc": "2026-02-23 16:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zm5v9",
          "author": "BubbleProphylaxis",
          "text": "please please make it stop. stop ai.",
          "score": 2,
          "created_utc": "2026-02-23 17:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70m7v7",
          "author": "bourbonandpistons",
          "text": "We are decades from AGI.\n\nThey'll probably just move the goal post of AGI and make it something else like they did with AI to agi. \n\nRemember what we're saying now is nothing more than a bunch of human program algorithms on human program data. There's no thinking and no reason anywhere near what those words actually mean. It's just loops around optimized algorithms.",
          "score": 2,
          "created_utc": "2026-02-23 20:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y4uff",
          "author": "Exciting-Log-8170",
          "text": "Trying to do it, have a working thermodynamic manifold prototype. Pushing next build this week. \n\nhttps://www.brickmiinews.com",
          "score": -2,
          "created_utc": "2026-02-23 13:28:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rarw7t",
      "title": "I managed to run Qwen 3.5 on four DGX Sparks",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ifu522zupqkg1.jpeg",
      "author": "Icy_Programmer7186",
      "created_utc": "2026-02-21 14:03:34",
      "score": 46,
      "num_comments": 10,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rarw7t/i_managed_to_run_qwen_35_on_four_dgx_sparks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6lq9mo",
          "author": "mosredna101",
          "text": "So 20K brings you 21 t/sec?",
          "score": 7,
          "created_utc": "2026-02-21 14:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lskx8",
              "author": "Icy_Programmer7186",
              "text": "This is a lab setup and this is one of many experiments done on it.  \nMoney well spent; this is not however recommendation for a production setup.",
              "score": 9,
              "created_utc": "2026-02-21 14:30:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lt5gl",
                  "author": "mosredna101",
                  "text": "I would be super happy with only one of those machines to be honest :D   \n",
                  "score": 2,
                  "created_utc": "2026-02-21 14:33:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6sig50",
                  "author": "jinnyjuice",
                  "text": "Why not for production?",
                  "score": 1,
                  "created_utc": "2026-02-22 16:08:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ofl7r",
              "author": "fallingdowndizzyvr",
              "text": "You can get 4 Sparks for $12K.",
              "score": 3,
              "created_utc": "2026-02-21 22:42:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oh648",
          "author": "CalvinBuild",
          "text": "That's amazing! I can't wait to do it locally!",
          "score": 2,
          "created_utc": "2026-02-21 22:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6swjif",
          "author": "Weak-Split-538",
          "text": "How did you connect the 4 sparks together ? Using NIC with a NIC switch ? Or just network cluster ?",
          "score": 2,
          "created_utc": "2026-02-22 17:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t4t7c",
              "author": "Icy_Programmer7186",
              "text": "I use switch: [https://mikrotik.com/product/crs804\\_ddq](https://mikrotik.com/product/crs804_ddq)",
              "score": 2,
              "created_utc": "2026-02-22 17:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmh9x",
          "author": "I_like_fragrances",
          "text": "I am able to get unsloth's Q6\\_K\\_XL with max context at around 40 tok/s. What quant do you use on the sparks? Typically if I offload these bigger models to the CPU I can get around 20 tok/s but when they're fully on the GPU they run at around 40 tok/s.\n\nhttps://preview.redd.it/1ujmj21qmwkg1.png?width=836&format=png&auto=webp&s=ee566d16319e3ae1db5631689978d30c961f67c7",
          "score": 1,
          "created_utc": "2026-02-21 20:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p4g3f",
              "author": "Icy_Programmer7186",
              "text": "I use FP8 quantization -> Qwen/Qwen3.5-397B-A17B-FP8\n\nSpark has a unified memory, so I guess there is no off-loading.",
              "score": 2,
              "created_utc": "2026-02-22 01:13:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7dbwm",
      "title": "I built VELLE.AI - a local AI companion with memory, voice, quant engine, and a full productivity suite. No cloud, no subscriptions. Everything on your machine.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "author": "Rich_Supermarket_164",
      "created_utc": "2026-02-17 17:54:58",
      "score": 44,
      "num_comments": 27,
      "upvote_ratio": 0.81,
      "text": "Hey everyone, I've been building this for a while and finally shipped it.\n\n**VELLE.AI** is a local AI operating system that runs on top of Ollama. It's not just another chat wrapper. It's a full personal assistant with persistent memory, two-way voice, a quantitative finance engine, and a productivity suite with todos, habits, goals, journal, and achievements.\n\n**What makes it different:**\n\n* **Persistent memory** — it actually remembers you across sessions. Your preferences, your name, your projects. All stored locally in SQLite.\n* **Two-way voice** — speech-to-text plus text-to-speech with hands-free mode. Talk to it, it talks back.\n* **7 personalities** — switch between Default, Sarcastic, Evil Genius, Anime Mentor, Sleepy, Kabuneko (finance gremlin), and Netrunner (cyberpunk). They actually stay in character.\n* **Kabuneko Quant Engine** — real-time stock quotes, full technical analysis including RSI, MACD, Bollinger, ADX, Sharpe, momentum scanning, value dislocations, backtesting, sentiment analysis, and a 4-bucket stock ideas generator. All from Yahoo Finance, no API keys needed.\n* **Productivity suite** — task manager with priorities, projects, due dates, habit tracker with streaks and weekly grids, pomodoro timer, goal system with milestones and progress bars, personal journal with writing prompts, bookmarks, knowledge base.\n* **25 achievements** — unlock badges as you use it. Toast notifications slide in when you earn one.\n* **Auto-insights** — detects patterns like \"work has been a recurring stressor 4 times this week\" or \"you created 15 tasks but only completed 3.\"\n* **Daily briefing** — one command gives you mood, tasks, habits, goals, streaks, reminders, and market data.\n* **Local file search** — searches your Desktop, Documents, Projects, Code directories by filename and content.\n* **System commands** — opens apps, runs PowerShell commands, controls your machine.\n* **Proactive reminders** — \"remind me to check email in 10 minutes\" actually fires with browser notifications plus text-to-speech.\n* **Cyberpunk terminal UI** — because aesthetics matter.\n\n**Tech stack:** Node.js, Express, WebSocket, SQLite, Ollama, vanilla JS. About 8,000 lines across 6 server modules. Works with any Ollama model including qwen3:8b, llama3, mistral. Ships as a Windows .exe or run from source on any OS.\n\n**Zero external AI APIs. Zero telemetry. Zero cloud. Everything local.**\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5xggn2",
          "author": "Barachiel80",
          "text": "Github repo or this is AI slop vaporware",
          "score": 13,
          "created_utc": "2026-02-17 20:34:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cuwsn",
              "author": "Rich_Supermarket_164",
              "text": "it's my repo",
              "score": 1,
              "created_utc": "2026-02-20 02:54:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x69lm",
          "author": "BonebasherTV",
          "text": "unrevocable-lyla-gleefully.ngrok-free.dev is a tunnel to the OP’s computer most likely. If the OP has it turned on it works otherwise it won’t work. \nWould love to see the repo of this. To understand the interactions between the different components.",
          "score": 11,
          "created_utc": "2026-02-17 19:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ycgk3",
              "author": "Rich_Supermarket_164",
              "text": "here made it public [https://github.com/velle999/velle.ai](https://github.com/velle999/velle.ai) ",
              "score": 7,
              "created_utc": "2026-02-17 23:10:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wow5y",
          "author": "Awkward-Customer",
          "text": "What are you using for the finance aspect of this? Also, is there a reason you chose to support only ollama and not llama.cpp?",
          "score": 4,
          "created_utc": "2026-02-17 18:24:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wpafn",
          "author": "Fair-Cookie9962",
          "text": "[velle.ai](http://velle.ai) site seems down, [vella.ai](http://vella.ai) seems something different. There is lot of things named velle or vellum, which is confusing.",
          "score": 3,
          "created_utc": "2026-02-17 18:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o692cmw",
              "author": "CarolloSenpai",
              "text": "To my understanding it's local!",
              "score": 1,
              "created_utc": "2026-02-19 15:04:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wpdk5",
          "author": "rusty_daggar",
          "text": "Do you have  link? \"velle.ai\" points to another address that is broken",
          "score": 2,
          "created_utc": "2026-02-17 18:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wwapq",
          "author": "eazolan",
          "text": "I'm surprised you didn't make \"Hyper competent assistant\" one of the personalities.",
          "score": 1,
          "created_utc": "2026-02-17 18:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xwfnr",
          "author": "Christosconst",
          "text": "So you built an operating system? For a companion bot? With filesystems and printer drivers and whatnot?",
          "score": 1,
          "created_utc": "2026-02-17 21:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zns9y",
          "author": "dropswisdom",
          "text": "Looks great. Can you help to package it in a docker that can use an existing (separate) ollama instance running on another docker with docker compose?",
          "score": 1,
          "created_utc": "2026-02-18 03:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n3vrt",
              "author": "Rich_Supermarket_164",
              "text": "docker pull velle999/velle.ai - open port 3000, should be working",
              "score": 1,
              "created_utc": "2026-02-21 18:32:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wx6f9",
          "author": "Far_Cat9782",
          "text": "Wtf I'm in the middle of the same thing my god what a small word.i already have  rag, chat history,  image generation, web search, different agent cards with different functionality like creative writing agen who generates images to go along with stories.a media manager agent who handles jellyfin and can list all the media on your server with movie posters and allows you  watch the movie right there or cast to a tc. I'm mind blown that we all have the same ideas. I call mines June.ai\n\nhttps://preview.redd.it/fmys6r6as3kg1.png?width=1220&format=png&auto=webp&s=b7cbb3c25cfcd491e0b82b644ccdb4201a28809d",
          "score": 1,
          "created_utc": "2026-02-17 19:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wz1ib",
              "author": "Awkward-Customer",
              "text": "I suspect a lot of people are working on this right now. the biggest hurdle is probably ingesting the data from so many different sources, some of which deliberately silo their data.",
              "score": 3,
              "created_utc": "2026-02-17 19:11:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5wxh8g",
              "author": "Far_Cat9782",
              "text": "Here's another screenshot of the jellyfin integration\n\nhttps://preview.redd.it/jlcqpypzr3kg1.png?width=1220&format=png&auto=webp&s=150883919c51839ac805765f3f68e647184bd69b",
              "score": 0,
              "created_utc": "2026-02-17 19:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wxlnv",
          "author": "Far_Cat9782",
          "text": "The agents\n\nhttps://preview.redd.it/gdip5u34s3kg1.png?width=1220&format=png&auto=webp&s=6f18ea71747a245355eda9ab074379cbe527801f",
          "score": 1,
          "created_utc": "2026-02-17 19:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x6g39",
          "author": "Sidze",
          "text": "I wonder why it's on Ollama if MLX models are more efficient for Apple Silicon on MacOS. I guess it could be connecting to Osaurus for MLX models and more efficiency.",
          "score": 1,
          "created_utc": "2026-02-17 19:46:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xjx07",
              "author": "Fair-Cookie9962",
              "text": "Ollama sound familiar, easier to trust.",
              "score": -1,
              "created_utc": "2026-02-17 20:50:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xph8b",
                  "author": "Sidze",
                  "text": "MLX is a framework built by Apple, not easier to trust?  \nThough I get it - much easier to plugin Ollama and forget it, instead of creating the whole MLX host manager. Anyway.",
                  "score": 2,
                  "created_utc": "2026-02-17 21:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1ralgvu",
      "title": "Local LLM for Mac mini",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "author": "Jiggly_Gel",
      "created_utc": "2026-02-21 08:00:24",
      "score": 42,
      "num_comments": 29,
      "upvote_ratio": 0.9,
      "text": "I’ve been watching hours of videos and trying to figure out whether investing in a Mac mini with 64 GB RAM is actually worth it, but the topic is honestly very confusing and I’m worried I might be misunderstanding things or being overly optimistic.\n\nI’m planning to build a bottom up financial analyst using OpenClaw and a local LLM, with the goal of monitoring around 500 companies. I’ve discussed this with ChatGPT and watched a lot of YouTube content, but I still don’t have a clear answer on whether a 30B to 32B parameter model is capable enough for this kind of workload.\n\nI’ll be getting paid for a coding project I completed using Claude, and I’m thinking of reinvesting that money into a maxed out Mac mini with 64 GB RAM specifically for this purpose.\n\nMy main question is whether a 30B to 32B local model is sufficient for something like this, or if I will still need to rely on an API. If I’ll need an API anyway, then I’m not sure it makes sense to spend so much on the Mac mini.\n\nI don’t have experience in this area, so I’m trying to understand what’s realistic before making the investment. I’d really appreciate honest input from people who have experience running local models for similar use cases.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6lpe1n",
          "author": "McMissile",
          "text": "I would just build whatever it is that you want on AWS and pay for cloud LLM tokens. Then you can figure out the feasability of what you're trying to achieve, and determine whether the 32b models will be sufficient. If it works, then great, buy the mac mini and run it locally. If not you've saved yourself a  bunch of money.",
          "score": 21,
          "created_utc": "2026-02-21 14:11:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6luuwp",
              "author": "AppointmentAway3164",
              "text": "This is the conclusion I reached. Targeting 30B models on huggingface. See how my use case tests. If it works then consider the $19k/$20k investment. Currently the the math doesn’t generally make sense for home token generation.",
              "score": 6,
              "created_utc": "2026-02-21 14:43:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ni9fh",
                  "author": "McMissile",
                  "text": "Yeah for most people I doubt running locally is ever really cost effective unless you need enormous amounts of tokens for sustained periods of time. 32b models that you can run locally have gotten fairly cheap to run in the cloud, especially compared to the price of a home lab.",
                  "score": 2,
                  "created_utc": "2026-02-21 19:44:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l3znc",
          "author": "rerorerox42",
          "text": "Maybe also look into tax discounts on investment into research and development.\n\nI have personally found newer 8B models (Ministral-3) to be capable on a 16GB M4 mini for what I have worked with by coding access to and using local models as needed with relevant context.\n\nAn issue currently is that it really is a little too soon for anybody to have tried out everything and every models’ true capabilities. FA-find-out kinda stage.",
          "score": 4,
          "created_utc": "2026-02-21 11:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qghk4",
              "author": "d4mations",
              "text": "I’m running ministral3-14b on a mac mini m416 gb I got used for very cheap. It is completely dedicated to lm studio with nothing else running on it. It also runs bge-3m for embedding. I have completely setup open claw with this setup, it runs crons flawlessly, has written several bash scripts that have worked on this first go, it proposed the embedded memory setup and then configured it perfectly, calls the few tools I have setup so far perfectly. The only caveat is context tokens. I have had to limit the number to 40k. Other than that I have a great little setup for less than 500€ and not one penny spent on tokens. I must say that I have fallback to openai oauth 20€ account and in the beginning I passed all of the recommendations and scripts proposed by ministral through it just to be sure. I trust it more and more everyday",
              "score": 3,
              "created_utc": "2026-02-22 07:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sc0sg",
                  "author": "OysterPickleSandwich",
                  "text": "Do you have your setup described anywhere? \nI’m debating on getting a mini for dedicated local use. \n",
                  "score": 1,
                  "created_utc": "2026-02-22 15:40:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6l4xya",
              "author": "Jiggly_Gel",
              "text": "I was thinking of messing around with the FA-find-out by running multiple models and then using a Claude API and comparing the responses but thought I’d ask here first in case if anyone had any other suggestions. \n\nI’ve seen similar posts to mine but they seemed to be looking for other use cases but the tax thing is definitely interesting I’m going to give that a look thank you",
              "score": 2,
              "created_utc": "2026-02-21 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kr7hn",
          "author": "battle_pantZ",
          "text": "Yeah, Im asking this question myself so I’m curious which answers you’ll get\n\nAfaik: 64GB is not enough to match the performance of large models. Even Kimi needs 200-500GB to perform properly.",
          "score": 3,
          "created_utc": "2026-02-21 09:30:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ksr3h",
              "author": "Jiggly_Gel",
              "text": "I think a 30B model should work fine on a Mac mini but the question is what model and how powerful is a 30B model? Some people attribute Qwen 30B to match gpt 4o but then if I have to spend money on machinery for 500 gb I’d rather run it via APIs because most of my financial analysis is done via my python bot it’s just I need something for judging the analysis and doing further industry research, comparative analysis, and reading annual reports and the news daily",
              "score": 1,
              "created_utc": "2026-02-21 09:45:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6l5n4e",
                  "author": "DistanceSolar1449",
                  "text": "Easy answers.\n\n- Qwen 3 VL 32b\n\n- GLM 4.7 Flash\n\n- Qwen 3.5 35b\n\n- Qwen 3 Next\n\nThese would be the best models you can fit on 64GB.\n\nIs buying a $2k mac mini 64gb to run these models worth it? Hell no. You can run the first 3 on a 3090 for $900 or so. And it'd be way faster than a mac mini.\n\nThe 512GB Mac Studio is actually something to consider, since you can run GLM 5 and similar big models that can actually rival ChatGPT/Claude. But the 64GB mac mini? Definitely not. You're better off spending $2k on GPT api credits.",
                  "score": 6,
                  "created_utc": "2026-02-21 11:48:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kuhd3",
                  "author": "battle_pantZ",
                  "text": "\nI think it makes more sense to buy a high-performance Mac if you need it for demanding tasks outside of AI. I mean, spending $2-3k to “only” use gpt4? I also look into it every day and do my research, and then I always come back to APIs. Yesterday, I paid $20 for the deepseek API, and I have to say, it's very, very cheap and pretty powerful for coding. It's 90% cheaper than the competition, by the way. \n\nSo a larger investment is needed. Either an even more powerful Mac or several smaller Macs as a cluster. Until a new AI comes out that requires even more hardware performance. If you extrapolate that, APIs will probably be cheaper.",
                  "score": 4,
                  "created_utc": "2026-02-21 10:02:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lr2su",
          "author": "HumbleTech905",
          "text": "Pay a few dollars for a service to deploy a 30B model, test and evaluate it, then decide if it's worth buying the Mac.",
          "score": 3,
          "created_utc": "2026-02-21 14:21:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m42f5",
          "author": "ScuffedBalata",
          "text": "\"Will a 30B model work for this use\" is very nebulous. It's not something you can just define.  It's like asking \"what IQ can be a financial analyst\".",
          "score": 3,
          "created_utc": "2026-02-21 15:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o10qi",
          "author": "Comprehensive_Iron_8",
          "text": "If you think you'll love using openclaw using a local model. The local models who remotely perform as well will be worth multiple MAC STUDIOs costing thousands of dollars. A model running inside a 64GB ram mac mini is not worth using. You'll not love it. You can use it for small things like email classification projects and maybe a small RAG system. \n\nWhy I say this, OPUS is so good, that people who have once used it, do not like 5.3 codex just because it does not have that kind of personality. And tomorrow there will be another model which has the same kind of moat. \n\nBut none of them at this time are good enough to run in a 64GB ram mac mini.",
          "score": 2,
          "created_utc": "2026-02-21 21:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o1oyx",
              "author": "Comprehensive_Iron_8",
              "text": "When I say this. I think a $10k mac studio can run GLM-5 and Minimax 2.5 which I actually like to run(using cloud providers, because the $10k ROI is not there yet) for openclaw, and are decent. They make mistakes yes, but you can optimize them to make less of them. But nothing for a mac mini yet.",
              "score": 1,
              "created_utc": "2026-02-21 21:27:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6o9tgu",
                  "author": "nvidiabookauthor",
                  "text": "What are some non China cloud providers to test this?",
                  "score": 1,
                  "created_utc": "2026-02-21 22:10:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mvskq",
          "author": "egoslicer",
          "text": "I have a 128B Strix Halo and run 70-100B models, and I'd say they are 'ok' for OpenClaw generalist tasks and tool calling. \n\nHowever 64GB Mac Mini and targeting ~30B models? Even more restrictive than that and while some are capable with tool calling, the results are going to be pretty meh overall. \n\nIf I were you, I'd setup OpenClaw and run it through MiniMax or Kimi 2.5 apis like many people are doing for a couple of months to see how that runs for you. You can do that on any machine you currently have and get much stronger reasoning and results.\n\nFor a local setup, IMO, ~30B models are ok for scoped tasks, but doing financial analysis with a lot of variables is likely not going to give you great results.",
          "score": 1,
          "created_utc": "2026-02-21 17:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qlfq3",
          "author": "emotionallofi",
          "text": "I just returned my m4 pro mac mini 64gb today. Going with a cheap used m2 max until m5 ultra is released.",
          "score": 1,
          "created_utc": "2026-02-22 07:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l93cm",
          "author": "thought_provoking27",
          "text": "Great thread .",
          "score": 0,
          "created_utc": "2026-02-21 12:18:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7umlq",
      "title": "[macOS] PersonaPlex-7B on Apple Silicon (MLX)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "author": "Apprehensive_Boot976",
      "created_utc": "2026-02-18 05:37:04",
      "score": 40,
      "num_comments": 2,
      "upvote_ratio": 0.96,
      "text": "NVIDIA released an open-source speech-to-speech model [PersonaPlex-7B](https://huggingface.co/nvidia/personaplex-7b-v1). It listens and talks simultaneously with \\~200ms latency, handles interruptions, backchanneling, and natural turn-taking.\n\nThey only shipped a PyTorch + CUDA implementation targeting A100/H100, so I ported it to MLX, allowing it to run on Apple Silicon: [github.com/mu-hashmi/personaplex-mlx](https://github.com/mu-hashmi/personaplex-mlx).\n\nHope you guys enjoy!",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o627q6u",
          "author": "felixlovesml",
          "text": "thanks. What’s the latency on a Mac — for example, on an M4 chip — and how much RAM does it require?",
          "score": 3,
          "created_utc": "2026-02-18 14:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o607o3d",
          "author": "former_farmer",
          "text": "Thanks.",
          "score": 1,
          "created_utc": "2026-02-18 05:44:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbu7sx",
      "title": "M4 Pro 48 or M4 Max 32",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "author": "Mammoth-Error1577",
      "created_utc": "2026-02-22 18:58:33",
      "score": 39,
      "num_comments": 34,
      "upvote_ratio": 0.88,
      "text": "I got my machine renewed at work a week ago.\n\nThey rejected my request of a Mac studio with 128 GB and instead approved a MacBook M4 Pro with 48GB and 512.\n\nWell I finally got around to checking and they actually gave me a more expensive M4 Max but with 32 GB and 1TB instead.\n\n\nIn my previous chatting with Gemini it has convinced me that 128 GB was the bare minimum to get a sonnet level local LLM.\n\nWell I was going to experiment today and see just what I could do with 48 and to my surprise I only had 32, but a superior CPU and memory bandwidth.\n\n\nIf my primary goal was to run coding a capable LLM, even at the cost of throughout, I assume 48 is vastly superior.  However if the best model I can run with 48 (+ containers and IDE and chrome etc.) is really dumb compared to sonnet I won't even use it.\n\nI'm trying to decide if it's worth raising a fuss over getting the wrong, more expensive laptop. I can experiment with a very small model on the current one but unless it was shockingly good I don't think that experiment would be very informative.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6toq7h",
          "author": "j00cifer",
          "text": "M5 ultra studio is coming out this year with a reported max RAM of 1TB.\n\n1TB RAM.",
          "score": 27,
          "created_utc": "2026-02-22 19:22:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tpdap",
              "author": "jiqiren",
              "text": "😍 want it so good 🥰 M5 Ultra 1TB??? Yes please!",
              "score": 6,
              "created_utc": "2026-02-22 19:25:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v9irr",
                  "author": "gingerbeer987654321",
                  "text": "Only 1?  Get 4 and do the Thunderbolt raid thing",
                  "score": 5,
                  "created_utc": "2026-02-23 00:20:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71kvcd",
                  "author": "LimiDrain",
                  "text": "Does this unified memory work as fast as VRAM or it's close to normal RAM speeds?",
                  "score": 1,
                  "created_utc": "2026-02-23 23:38:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uawrt",
              "author": "sav22v",
              "text": "But you'll have to sell your kidneys and children to pay for it...",
              "score": 6,
              "created_utc": "2026-02-22 21:14:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6uq4zs",
                  "author": "j00cifer",
                  "text": "I’m making the case to them now.",
                  "score": 2,
                  "created_utc": "2026-02-22 22:32:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uyghh",
              "author": "grim-432",
              "text": "With the current price of ram, what’ll that cost?  $25,000?",
              "score": 3,
              "created_utc": "2026-02-22 23:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x4h53",
                  "author": "ijontichy",
                  "text": "They would have locked in RAM costs for this year before the RAMpocalypse. But do you think they'll hold prices steady? 🤔",
                  "score": 1,
                  "created_utc": "2026-02-23 08:18:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71z016",
                  "author": "GonzoDCarne",
                  "text": "Ram price does not apply to oems like Apple. Due to many things that someone might want to go into detail in a long thread. My hard guess is they will target 15k or 19999. M3 Ultras with 512GiB go for 10k un the US since before the ram surge and today.",
                  "score": 1,
                  "created_utc": "2026-02-24 00:56:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6trf6a",
              "author": "Mammoth-Error1577",
              "text": "Unfortunately not an option. The only studio I could get is also 36GB.",
              "score": 2,
              "created_utc": "2026-02-22 19:35:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6u9uza",
              "author": "ZealousidealShoe7998",
              "text": "at that level what llm would one even use to reach comercial levels ?",
              "score": 1,
              "created_utc": "2026-02-22 21:08:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6xj37x",
              "author": "iezhy",
              "text": "Given current ram prices (and Apple markup), this probably will be out of reach for most users",
              "score": 1,
              "created_utc": "2026-02-23 10:41:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70hc2o",
                  "author": "j00cifer",
                  "text": "I don’t see why a 2nd kidney is so important to people",
                  "score": 1,
                  "created_utc": "2026-02-23 20:17:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7134m5",
              "author": "Jealous_Incident7978",
              "text": "Starts getting funny that we drop $$$$ on a 1TB Ram M5 ultra studio to run open weight models that is essentially free. 😆 imagine paying something similar for qwen 3.5 / DeepSeek etc just to run the model locally",
              "score": 1,
              "created_utc": "2026-02-23 22:03:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o729hxh",
                  "author": "j00cifer",
                  "text": "1 TB RAM.",
                  "score": 1,
                  "created_utc": "2026-02-24 01:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6u93hc",
          "author": "No_Success3928",
          "text": "Hahaha sonnet level 🙄 classic gemini hallucinations",
          "score": 14,
          "created_utc": "2026-02-22 21:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tl9zz",
          "author": "Thump604",
          "text": "It’s all memory and it’s never enough",
          "score": 25,
          "created_utc": "2026-02-22 19:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u2tks",
          "author": "BisonMysterious8902",
          "text": "I hate to break it to Gemini, but you can't get anywhere close to Sonnet level with 128Gb. Can you get something usable? Sure, but it'll never match frontier level models. Even a Studio with 512Gb. That's just the current state of things.",
          "score": 9,
          "created_utc": "2026-02-22 20:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ughj5",
              "author": "meTomi",
              "text": "Current state? When you think your home pc can compete with million dollar racks in server rooms?\nOn the other hand yes technology is getting better and you can run better and bigger models at home.",
              "score": 1,
              "created_utc": "2026-02-22 21:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ve5yt",
          "author": "MrRandom04",
          "text": "Only open source LLMs that compete with Sonnet 4.6 / Opus 4.6 are GLM 5 and Kimi K2.5. Of these, only GLM 5 is super reliable for agentic coding. That model is far too big for anything less than like 512gb ram. For 32gigs, you can consider the Qwen series UD quants and then have a workflow where you shell out to an API provider of GLM 5 or even just Sonnet / Opus for planning and big design / knowledge level tasks while the manual editing and coding is done by Qwen. The latest ones are very good at stuff like Python and really good for their size.",
          "score": 7,
          "created_utc": "2026-02-23 00:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4d3s",
          "author": "Expert-Reaction-7472",
          "text": "i dont think id make a fuss about this to any place i've ever worked.\n\nnice thing about being self employed is if i want to splurge on a machine i can. Which usually means I have something decent but not mind blowingly expensive cos it's my own money and i'd rather spend the extra on a holiday or something.",
          "score": 3,
          "created_utc": "2026-02-22 20:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uqm3v",
          "author": "ComfortablePlenty513",
          "text": "always prioritize memory. M4 architecture is fundamentally better than previous gen for inference",
          "score": 2,
          "created_utc": "2026-02-22 22:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w40t7",
          "author": "Sharp-Mouse9049",
          "text": "32GB in 2026 for serious local LLM work is basically consumer-tier. I don’t care how fast the M4 Max is — if you’re constantly forced into tiny quants or can’t load 70B comfortably, you’re artificially capping your experimentation. Bandwidth doesn’t matter if the model doesn’t fit. RAM is the ceiling.",
          "score": 2,
          "created_utc": "2026-02-23 03:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v9e8v",
          "author": "pondy12",
          "text": "Get a HP ZBook Ultra G1a, Ryzen AI Max+ PRO 395, 64gb - 128gb of ram, 256gb/s ram bandwidth. Will be 1/4th the price. \n\n",
          "score": 1,
          "created_utc": "2026-02-23 00:20:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xhgbi",
              "author": "Confident-Strength-5",
              "text": "It also has 256gb/s bandwidth, so…\nLLMs really like bandwidth…",
              "score": 1,
              "created_utc": "2026-02-23 10:25:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vlezt",
          "author": "midz99",
          "text": "Vram or whatever mac calls it is everything. Higher the better. really you need 128gb to even get close to something worth testing.",
          "score": 1,
          "created_utc": "2026-02-23 01:30:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uivgp",
          "author": "DistanceSolar1449",
          "text": "M4 max has way faster memory bandwidth\n\n48gb is not enough for Qwen3 next \n\nJust stick with 32gb",
          "score": 0,
          "created_utc": "2026-02-22 21:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6unjkw",
              "author": "Mammoth-Error1577",
              "text": "I just tried  qwen2.5-coder:14b in open code and it was extremely dumb and worse than copying and pasting from a web browser (on an empty repo)\n\nI tried qwen2.5-coder:32b 1st and /init wasn't doing anything so Gemini told me to downgrade.\n\nBut /init didn't do anything after downgrading either.\n\nAll I could get it to do was spit out code that it would tell me to put into the file myself instead of doing it itself, and then the code wasn't even syntactically correct.\n\nI'm super shocked it was so bad, there was no way I was doing it correctly.",
              "score": 0,
              "created_utc": "2026-02-22 22:18:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zftss",
                  "author": "Djagatahel",
                  "text": "What is that /init you're talking about?",
                  "score": 1,
                  "created_utc": "2026-02-23 17:24:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tmf6f",
          "author": "Svyable",
          "text": "Surprised how much I get out of my 24 Pro M4 I have like 100Gb running in Brave browsers no problem. \n\nModel sizes are coming down. Don’t complain innovate",
          "score": -1,
          "created_utc": "2026-02-22 19:11:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7bohc",
      "title": "[macOS] Built a 100% local, open-sourced, dictation app. Seeking beta testers for feedback!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r7bohc",
      "author": "AdorablePandaBaby",
      "created_utc": "2026-02-17 16:58:03",
      "score": 38,
      "num_comments": 43,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7bohc/macos_built_a_100_local_opensourced_dictation_app/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5w75dq",
          "author": "Robby2023",
          "text": "Looks really good! Kudos for this.\n\nOne question, how much RAM do you need in order for it to work properly?",
          "score": 4,
          "created_utc": "2026-02-17 17:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w83la",
              "author": "AdorablePandaBaby",
              "text": "I've tested reliably on a 4GB machine. Wouldn't go lower than that tbh.",
              "score": 3,
              "created_utc": "2026-02-17 17:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5w8qb7",
                  "author": "Robby2023",
                  "text": "Which model does it use behind the scenes?",
                  "score": 1,
                  "created_utc": "2026-02-17 17:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w8tdr",
          "author": "rusty_daggar",
          "text": "I don't have a mac to try it on, but it looks like a nice idea.\n\nAre you using a VAD to clean up the audio or just passing all straight to whisper?",
          "score": 3,
          "created_utc": "2026-02-17 17:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9v0i",
              "author": "AdorablePandaBaby",
              "text": "Straight to whisper. \n\nBut I haven't felt the need to remove bg noise yet.   \nMaybe a lower priority improvement later down the line.",
              "score": 2,
              "created_utc": "2026-02-17 17:14:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wp082",
          "author": "JohnHawley",
          "text": "I've been using Handy, it's been great! [https://github.com/cjpais/Handy](https://github.com/cjpais/Handy)  \nWhat does SpeakType do differently? Is there extra logic that Handy doesn't perform?",
          "score": 6,
          "created_utc": "2026-02-17 18:25:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w759k",
          "author": "AdorablePandaBaby",
          "text": "The title should say \"open source\", instead of \"open-sourced\", but I guess its too late.",
          "score": 2,
          "created_utc": "2026-02-17 17:01:03",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5w9buv",
          "author": "iongion",
          "text": "UI is gorgeous",
          "score": 2,
          "created_utc": "2026-02-17 17:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9zfe",
              "author": "AdorablePandaBaby",
              "text": "Thank you!  \nMy OCD drove me crazy, but I'm glad someone liked it as well.",
              "score": 2,
              "created_utc": "2026-02-17 17:15:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wx6ob",
          "author": "Pitiful-Impression70",
          "text": "nice, the local-only approach is the way to go imo. quick question tho, does it do any LLM post-processing on the whisper output? like adding punctuation, fixing capitalization, formatting stuff based on context? raw whisper output is usually pretty good but it still dumps everything as one block of text which is annoying when youre dictating emails or notes. thats been the main gap ive seen with most local dictation tools vs the cloud ones that have an extra formatting pass",
          "score": 2,
          "created_utc": "2026-02-17 19:02:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wxac1",
              "author": "AdorablePandaBaby",
              "text": "Working on that rn :)",
              "score": 1,
              "created_utc": "2026-02-17 19:03:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ywjpm",
                  "author": "Pitiful-Impression70",
                  "text": "nice, thats the part that really makes or breaks it imo. like raw whisper output is fine for short stuff but anything longer than a paragraph and the lack of punctuation and formatting makes it basically unusable without cleanup. curious if youre gonna do it locally too or offload to an api",
                  "score": 1,
                  "created_utc": "2026-02-18 01:01:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xf3ls",
          "author": "Appropriate-Deer234",
          "text": "Looks nice and i also love that everything stays offline. Already downloaded and will test it tomorrow on M1 and M3. I also would have the 2019 Intel if you need feedback from that.",
          "score": 2,
          "created_utc": "2026-02-17 20:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zo00h",
              "author": "AdorablePandaBaby",
              "text": "Yes perfect. Will DM you!",
              "score": 1,
              "created_utc": "2026-02-18 03:29:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xx0s1",
          "author": "CaptainSuckie",
          "text": "I'd like to test this out! ",
          "score": 2,
          "created_utc": "2026-02-17 21:51:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y4oh0",
          "author": "DertekAn",
          "text": "Yes please? 🤭🤭",
          "score": 2,
          "created_utc": "2026-02-17 22:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wax8h",
          "author": "vulture916",
          "text": "Interested! ",
          "score": 1,
          "created_utc": "2026-02-17 17:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wepnv",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:37:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5weryo",
              "author": "vulture916",
              "text": "Spinning Wheel loop on Transcribe using pre-built app. Can't get anything transcribed using Whisper Large V3 Turbo on Macbook M1 14.1 Sonoma.\n\n  \nChecked settings and for whatever reason Audio Input was defaulted to NoSound rather than Macbook microphone. Changed that, same behavior.  ",
              "score": 1,
              "created_utc": "2026-02-17 17:38:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5widpv",
          "author": "Meowliketh",
          "text": "Hey, I’d love to test this out!",
          "score": 1,
          "created_utc": "2026-02-17 17:55:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5win06",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60zbpl",
                  "author": "Meowliketh",
                  "text": "Don't think I got the DM? Also, love your user name",
                  "score": 1,
                  "created_utc": "2026-02-18 09:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wy4sg",
          "author": "sinebubble",
          "text": "I'm unclear what problem this solves on macOS. Is the implication that Apple's built-in speech dictation is not private? I use the built-in dictation service all the time to transcribe into every app I've used it with.",
          "score": 1,
          "created_utc": "2026-02-17 19:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wzb1k",
              "author": "AdorablePandaBaby",
              "text": "Correct. That's the main usecase, but Apple's STT doesn't detect words properly for a lot of my usecases. \n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 2,
              "created_utc": "2026-02-17 19:12:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wyc52",
          "author": "solipsistmaya",
          "text": "Looks good, interested in testing.",
          "score": 1,
          "created_utc": "2026-02-17 19:08:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x3249",
          "author": "Ok_Yoghurt248",
          "text": "does it work on windows ?",
          "score": 1,
          "created_utc": "2026-02-17 19:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x5q7r",
          "author": "aqdnk",
          "text": "would like to test!",
          "score": 1,
          "created_utc": "2026-02-17 19:43:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xa2en",
          "author": "Correct_Support_2444",
          "text": "I will be trying this out later this week when I get home from a trip. This is exactly what I’ve been looking for.",
          "score": 1,
          "created_utc": "2026-02-17 20:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xni9n",
          "author": "Driftwintergundream",
          "text": "I'm an active user of superwhisper but looking for a local model TTS.\n\nTo me the 3 key features that would sell me is 1) super fast, 2) super accurate, and 3) slight touch up options of the text to remove verbal mispeaks or ums.\n\nI'm testing it out!",
          "score": 1,
          "created_utc": "2026-02-17 21:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zoso6",
              "author": "AdorablePandaBaby",
              "text": "Yes, then SpeakType should be just the best STT for you. It's fast, accurate and removes all the verbal misspeaks you're concerned about",
              "score": 1,
              "created_utc": "2026-02-18 03:34:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yhhu6",
          "author": "The_BeatingsContinue",
          "text": "So, you decided against a $12/month subscription and i think everybody does. In times of Claude Code it's incredibly easy to invest a little time to solve the whole task selfmade.\n\nMy question is: how can you decide against a subscription model and still want money for it while claiming to be open source? No offense, just a serious question.\n\nAs an additional use case idea: I work with deaf people and having a Mac on a table that makes a whole conversation transparent for anyone at that table would be a great feature, requiring just a text output window scalable to fullsize screen, while letting users choose font/fontsizes and opting out the requirement to push a button. It just can be active all the time and can be started/stopped by a keypress.",
          "score": 1,
          "created_utc": "2026-02-17 23:38:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yzqge",
              "author": "CtrlAltDelve",
              "text": "It's a pretty common model for binaries to be paid, but the sources are open, and nothing stops you from building it with no restrictions. \n\nVoiceInk is one of these. \n\nIf you're familiar with Claude Code, you can just ask it to compile it for you?",
              "score": 1,
              "created_utc": "2026-02-18 01:18:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zjep0",
          "author": "rditorx",
          "text": "macOS has offline dictation, you just need to enable it in the System Preferences and download the language data in there. No need to trust third parties. It might send some data to Apple, though, so consult the privacy policy.",
          "score": 1,
          "created_utc": "2026-02-18 03:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zp8ju",
              "author": "AdorablePandaBaby",
              "text": "Yep, you're right.\n\nBut, Apple's STT doesn't detect words properly for a lot of usecases.\n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 1,
              "created_utc": "2026-02-18 03:37:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6231pt",
          "author": "adrian_dev_yyc",
          "text": "Been building something similar on the Windows side. Privacy is the number one thing people bring up when I ask why they don't use cloud dictation. What's your latency like? In my experience that matters way more than raw accuracy for whether people actually stick with it.",
          "score": 1,
          "created_utc": "2026-02-18 14:24:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o671d6b",
          "author": "Sudden-Ad-1217",
          "text": "I’m game, got an M1 Max I’d run it against.",
          "score": 1,
          "created_utc": "2026-02-19 05:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6853j3",
          "author": "Amaeze",
          "text": "yo! this looks great! interested in trying!",
          "score": 1,
          "created_utc": "2026-02-19 11:40:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68p10g",
          "author": "creativedoctor",
          "text": "If it works for portuguese of Portugal, I'm game. Drop me a DM with the instructions, if so, please!",
          "score": 1,
          "created_utc": "2026-02-19 13:52:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6d1qy6",
          "author": "duedev",
          "text": "Found this while searching for a local dictating software for macOS. The one built in works intermittently for me on my m1 laptop.",
          "score": 1,
          "created_utc": "2026-02-20 03:37:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d1syj",
              "author": "AdorablePandaBaby",
              "text": "How did you find this?",
              "score": 1,
              "created_utc": "2026-02-20 03:38:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tncmh",
          "author": "benmackenzie_nine4",
          "text": "Using it now.  Great job.",
          "score": 1,
          "created_utc": "2026-02-22 19:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vit44",
          "author": "hondahb",
          "text": "This is actually really awesome!\n\nI would love to test this for you.\n\nA few comments so far:\n\n* Macbook Air 2020 M1 8gb ram - The only model I could get so far to work is whisper tiny. The other ones just weren't loading for some reason.\n\n* It would be nice if you could replace the dictation key so F5.\n\n* [BLANK_AUDIO] is annoying and unnecessary.\n\n* It would be nice if it streamed the words just like the native Mac Dictation does.\n\n* It would be nice to hide the tray icon.",
          "score": 1,
          "created_utc": "2026-02-23 01:14:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8iew6",
      "title": "Open Source LLM Leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mrakuwr3xbkg1.png",
      "author": "HobbyGamerDev",
      "created_utc": "2026-02-18 23:04:19",
      "score": 36,
      "num_comments": 32,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8iew6/open_source_llm_leaderboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67lo9v",
          "author": "jiqiren",
          "text": "Not enough people have 512GB+ of vram or unified memory (like the Mac Studio). Otherwise Minimax M2.5 would be top dog. 🐶",
          "score": 7,
          "created_utc": "2026-02-19 08:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6890vq",
              "author": "deepfit",
              "text": "Still pretty large but MiniMax-M2.5 UD-Q2\\_K\\_XL from unsloth is both fast and < 100G of ram/vram.  I am having a hard time telling any difference from larger quant versions.",
              "score": 3,
              "created_utc": "2026-02-19 12:11:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68mysa",
              "author": "AfterShock",
              "text": "Came here to +1 MinMax 2.5 when I saw it missing from the list.",
              "score": 3,
              "created_utc": "2026-02-19 13:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d0mb1",
                  "author": "jiqiren",
                  "text": "I only have a Mac mini so can’t run it. But it’s so cheap on openrouter it’s been my goto model.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:30:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67bwp1",
          "author": "einord",
          "text": "Fun fact: the larger the model, the more intelligent.",
          "score": 12,
          "created_utc": "2026-02-19 07:08:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67suaq",
              "author": "DistanceSolar1449",
              "text": "This ranking is trash. Only thing it gets right is the top 2. \n\nDeepseek V3? That came out in 2024. Deepseek R1 came out a full year ago.\n\nWhere’s Deepseek V3.2? Why is Mistral Large rated so highly, and the same tier as gpt-oss-120b? Why Nemotron V1 instead of Nemotron V1.5?",
              "score": 14,
              "created_utc": "2026-02-19 09:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a0weu",
                  "author": "onil34",
                  "text": "Where is minimax?",
                  "score": 3,
                  "created_utc": "2026-02-19 17:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67jksw",
              "author": "Successful-Emu-6409",
              "text": "scaling still alive ",
              "score": 5,
              "created_utc": "2026-02-19 08:19:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cisu8",
              "author": "Available-Craft-5795",
              "text": "Sometimes, bigger moves faster. Sometimes, smaller takes its time. Both find their way — eventually — just on different roads to the finish line.\n\nOne is built for the masses, for the noise and the need and the scale. But we build for the love of the question, for the joy in the curious trail.\n\nThey don't care about the small ones. We do.\n\n*Built with curiosity, not compute.* [CompactAI](https://huggingface.co/spaces/CompactAI/Built-with-curiosity-not-compute)\n\n(I know its a promo, shush)",
              "score": 1,
              "created_utc": "2026-02-20 01:39:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o687bd6",
          "author": "entheosoul",
          "text": "This should be split between actual locally runable models and cloud models (not exactly local)",
          "score": 7,
          "created_utc": "2026-02-19 11:58:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67hymk",
          "author": "nunodonato",
          "text": "Amazing how got oss 120b holds it's place after all these new models have came out",
          "score": 3,
          "created_utc": "2026-02-19 08:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o693exw",
          "author": "Sufficient_Prune3897",
          "text": "Cursed tier list. Shows that benchmarks are not everything",
          "score": 2,
          "created_utc": "2026-02-19 15:09:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67h8z0",
          "author": "HoustonTrashcans",
          "text": "Can anyone tell me what quantization I need to run a 1T model on my laptop with 8 GB of VRAM? If my math is right that's Q.05?",
          "score": 2,
          "created_utc": "2026-02-19 07:57:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67quwa",
              "author": "Ell2509",
              "text": "Not possible. But why do you need a 1t model? With 8gb vram, assuming you have 16gb system ram, you could run a 7 or 8b model, realistically. \n\nMore than that will become unusable slow quite quickly.",
              "score": 2,
              "created_utc": "2026-02-19 09:31:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68pel8",
                  "author": "ShinigamiOverlord",
                  "text": "Very much so. I could somewhat use a 10-14B models, but they cap at 1.3 tokens/s",
                  "score": 1,
                  "created_utc": "2026-02-19 13:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69t4rr",
          "author": "peva3",
          "text": "OP, anyway you could turn this data into an API? I could use these benchmarks for a project I'm working on.",
          "score": 1,
          "created_utc": "2026-02-19 17:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ad120",
          "author": "Far_Cat9782",
          "text": "Gpt 120b is my goal to run locally. Currently the max I can slso to get real work done is 24b model.",
          "score": 1,
          "created_utc": "2026-02-19 18:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b5oz6",
          "author": "Used-Dance-7006",
          "text": "Sorry...maybe this is the designer in me but the color coding is counterintuitive to the way i perceive design.\n\nIs S good? It's red so I read that as the worst?  \nIs C good?\n\nShould I be focusing on S and A models? Is D bad then?\n\nJust trying to understand and appreciate the clarity.",
          "score": 1,
          "created_utc": "2026-02-19 21:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bezen",
          "author": "shankey_1906",
          "text": "Something like this would be amazing for differnt tiers of VRAM, and use-cases. Tier <16GB, <32GB, etc. Tier: Coding, Reasoning, ...",
          "score": 1,
          "created_utc": "2026-02-19 21:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6flfy0",
          "author": "Fantastic-Breath2416",
          "text": "C'è anche il mio\n\nhttps://nothumanallowed.com/search\n\nPotete usarlo!!",
          "score": 1,
          "created_utc": "2026-02-20 15:05:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8awjx",
      "title": "Best Coding Model?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "author": "I_like_fragrances",
      "created_utc": "2026-02-18 18:23:42",
      "score": 22,
      "num_comments": 23,
      "upvote_ratio": 0.85,
      "text": "What is the best model for general coding. This includes very large models too if applicable.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o63oc6q",
          "author": "No_Clock2390",
          "text": "Qwen3-Coder-Next",
          "score": 22,
          "created_utc": "2026-02-18 18:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o644cn8",
              "author": "txgsync",
              "text": "This is the answer. At any affordable home-gamer size (<128GB) Qwen3-Coder-Next (80B-A3B) is the tits.\n\nEdit: and unlike Qwen3-Coder-30B-A3B it doesn’t give me the “fuck off, I just write code, figure it out yourself” energy when planning features. And -Next does better at ZorkBench, not just wandering around with a nasty knife waiting for something to happen until it rage-quits.",
              "score": 7,
              "created_utc": "2026-02-18 19:58:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o677w1y",
                  "author": "Sax0drum",
                  "text": "How much context can you reasonably run with 128GB?",
                  "score": 3,
                  "created_utc": "2026-02-19 06:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o63q8bi",
              "author": "huzbum",
              "text": "Awesome, I need to try this one on some coding tasks, because I can actually run it with a large context and decent TG speed.  (Haven't tested PP)",
              "score": 1,
              "created_utc": "2026-02-18 18:54:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66ntkk",
                  "author": "DifficultyFit1895",
                  "text": "It’s worked well for me. It’s also the only local model I’ve ever tried that 100% answers correctly this one tricky question I ask it about a ~30,000 word novella with the full story in context.",
                  "score": 1,
                  "created_utc": "2026-02-19 04:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64p8ry",
              "author": "jinnyjuice",
              "text": "For what languages are you using it for?\n\nThis is my secondary. GLM 4.7 Flash is better with coding the web UI.",
              "score": 1,
              "created_utc": "2026-02-18 21:36:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64pgm9",
                  "author": "No_Clock2390",
                  "text": "It supports all languages",
                  "score": 1,
                  "created_utc": "2026-02-18 21:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65dej5",
              "author": "National_Cod9546",
              "text": "What IDE / plugins are you using? ",
              "score": 1,
              "created_utc": "2026-02-18 23:35:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o655yof",
          "author": "Faultrycom",
          "text": "People will argue over different models but have in mind that it's about local - and here qwen 3 coder next shines as it can run on not so expensive stack.",
          "score": 5,
          "created_utc": "2026-02-18 22:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63rdtm",
          "author": "huzbum",
          "text": "I haven't used it (beyond confirming I can run it), but I hear great things about MiniMax M2.5.  I use GLM 4.7, and GLM 5 every day, but cloud hosted.  I thought about building something to run GLM 4.5/4.6/4.7, but never got beyond thinking about it... now GLM 5 is twice as big as GLM 4, definitely not running that, but maybe they'll make something intermediate in the 5 family.  \n\nI should try out GLM 4.7 flash for some real tasks, but I haven't gotten around to it yet.  ",
          "score": 3,
          "created_utc": "2026-02-18 18:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64lph1",
          "author": "Grouchy-Bed-7942",
          "text": "If you want something that runs for €6k for agentic code, MiniMax 2.5 with VLLM on 2x ASUS GX10 (equivalent to DGX Spark), you get 2000 tokens/sec of PP without context and you drop to 600/400 tokens/sec of PP at 100k context. Output of about 30 tokens/sec at the start and drops to 15 tokens/sec.\n\nWith Qwen3-Coder-Next, certain tool calls fail after 50k context.",
          "score": 3,
          "created_utc": "2026-02-18 21:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o679pq1",
          "author": "Only_Comfortable_224",
          "text": "Not directly related but for serious coding, is Claude more cost effective?",
          "score": 1,
          "created_utc": "2026-02-19 06:49:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mklci",
              "author": "Ambitious_Injury_783",
              "text": "yes.",
              "score": 1,
              "created_utc": "2026-02-21 16:56:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fll8r",
          "author": "emrbyrktr",
          "text": "I think investing in hardware at this stage is pointless. Everything is developing so fast. In my opinion, Qwen models are the most efficient models locally.",
          "score": 1,
          "created_utc": "2026-02-20 15:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ygaej",
          "author": "Organic-Hall1975",
          "text": "glm 4.7 is solid for general coding imo, handles small scripts and real repos without getting too messy.",
          "score": 1,
          "created_utc": "2026-02-23 14:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64r4bi",
          "author": "Potential-Leg-639",
          "text": "Pick a strong model for architecture/planning and Qwen3 Coder 30B or GLM 4.7 Flash can then do the coding quite good.\n\nMakes quite a difference in coding quality.",
          "score": 1,
          "created_utc": "2026-02-18 21:45:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65bhr7",
              "author": "ijontichy",
              "text": "> Pick a strong model for architecture/planning\n\nCan you provide an example of this?",
              "score": 2,
              "created_utc": "2026-02-18 23:24:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o664ule",
                  "author": "voyager256",
                  "text": "I think he meant non-local / subscription based , big model like Claude 4.6",
                  "score": 3,
                  "created_utc": "2026-02-19 02:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67nzfl",
              "author": "alokin_09",
              "text": "Yep, this is basically my workflow in Kilo Code. I use a premium model like Opus for the architecture stuff, then switch to something like GLM, MiniMax or Qwen for the smaller tasks. Opus creates really detailed plans that are easy to hand off to the cheaper models.",
              "score": 2,
              "created_utc": "2026-02-19 09:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o681d94",
                  "author": "Potential-Leg-639",
                  "text": "I selfhost GLM 4.7 Flash/GPT-OSS-120b/Qwen 3 Coder on a Strix Halo, that do the coding and cant run out tokens. Planning/Architecture  in Opus.\n\nBut i switched completely to OpenCode.",
                  "score": 1,
                  "created_utc": "2026-02-19 11:09:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ppfn",
          "author": "PooMonger20",
          "text": "From my own testing and not benchmarks, on consumer level HW - GPT-OSS-20b is the closest I was able to get to the online equivalents. Everything else is either too slow, generates trash that doesn't even compile, endless syntax errors or straight out misses half the functions you asked for.",
          "score": 0,
          "created_utc": "2026-02-19 09:20:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raao5m",
      "title": "Google officially launches the Agent Development Kit (ADK) as open source",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/utzloi85eqkg1.jpeg",
      "author": "Fun-Necessary1572",
      "created_utc": "2026-02-20 23:09:23",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1raao5m/google_officially_launches_the_agent_development/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6nyjyk",
          "author": "_klikbait",
          "text": "OH MY GOD",
          "score": 1,
          "created_utc": "2026-02-21 21:10:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pve1",
      "title": "Recommendations for agentic coding with 32GB VRAM",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "author": "pioni",
      "created_utc": "2026-02-20 08:27:15",
      "score": 19,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "My current project is almost entirely in node.js and typescript, but every model I'm tried with LM Studio that fits into VRAM with 128k context seems to have problems with getting stuck in a loop. No amount of md files and mandatory instructions has been able to resolve this, it still does it with Roo Code and VSCode. \n\nAny ideas what I should try? Good examples of md files I could try to avoid this, or better LM Studio models with the hardware limitations I have? I have recently used Qwen3-Coder-Next-UD-TQ1\\_0 and zai-org/glm-4.7-flash and both have similar problems. Sometimes it works for good 15 minutes, sometimes it gets into a loop after first try. \n\nI don't know if it matters, but the dev environment is Debian 13. Using Windows was a complete nightmare because of commands it did not have and file edits that did not work.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6eb9on",
          "author": "GuyFromPoland",
          "text": "These questions keep showing up - is there any website where you provide your hardware details and it shows you best models?",
          "score": 5,
          "created_utc": "2026-02-20 10:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpn1j",
              "author": "reditzer",
              "text": "The issue is that \"best\" is subjective.",
              "score": 2,
              "created_utc": "2026-02-20 15:26:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e4c0j",
          "author": "FullstackSensei",
          "text": "How much RAM do you have? Running Q1 on any model severely lobotomizes performance. For coding tasks o smaller models, I find you can't get acceptable performance under Q8, and no KV cache quantization.",
          "score": 3,
          "created_utc": "2026-02-20 08:57:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7r7b",
              "author": "HumanDrone8721",
              "text": "I have 48GB (2x24) VRAM and 128GB RAM. Current using Qwen3-Coder-Next the highest quality 8bit quanta possible. Yes, the speed dropped significantly to something like 24tok/s but I had the latest OpenSSL source code and a regulatory document to follow and adjust the code for it. In German, bureaucratic German even. It worked perfectly (max context -c 0 is enabled), code itself, test cases AND documentation. Less than an hour, no human intervention.\n\nThe biggest problem was not the generated code quality or time, it was the latest llama.cpp crashing and burning with grammar parser errors, THAT was difficult to repair, but once repaired and with the latest GGUFs it works like a charm.\n\nMy point: tried with all other quants and it was a waste of data download, ALL of them failed more or less miserably !!! Also GLM4.7-Flash was a disappointment.\n\nTL; DR: Bottom line is: ALWAYS run a model to the highest possible resolution and context that you can run, discard the speed and take as much quality as possible, small quantas are fast, but really shit for anything besides benchmarks, simple tasks and wankerei: \"I've run this 485B model on my potato at 75t/s...\" Sadly this enforces and propagates the myth that smaller models are crap and not worth using because ADHD zoomers have no patience and take whatever shite as long as is fast.",
              "score": 6,
              "created_utc": "2026-02-20 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fue9m",
                  "author": "fish_of_pixels",
                  "text": "This is so interesting to me. What agentic tools are you running? I'm using opencode and glm 4.7 flash has been amazing while qwen3 coder next has struggled to get through most tasks. Both Q8.",
                  "score": 2,
                  "created_utc": "2026-02-20 15:48:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6edx4c",
                  "author": "Betatester87",
                  "text": "I have the same setup. Would you kindly post your llama.cpp config that you feel is best. Thanks",
                  "score": 1,
                  "created_utc": "2026-02-20 10:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hxyiu",
                  "author": "Soft_Syllabub_3772",
                  "text": "I had the grammar issue, i disabled streaming in roocode",
                  "score": 1,
                  "created_utc": "2026-02-20 21:43:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6e5ii7",
              "author": "pioni",
              "text": "I have 32GB of RAM, the same as VRAM. Can't upgrade for hardware reasons and monetary reasons. Any recommendations for a model that I could run a less quantized version of with my current setup? I think I might be able to squeeze the context window smaller if I have to, because it has to run in GPU, otherwise it will stall to few tokens per second.",
              "score": 2,
              "created_utc": "2026-02-20 09:08:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6e728c",
                  "author": "FullstackSensei",
                  "text": "Devstral 2 24B",
                  "score": 2,
                  "created_utc": "2026-02-20 09:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e2u64",
          "author": "IsSeMi",
          "text": "I tried them both. In my case it depends on the tool I use those models in. Try to use Claude code. I haven't noticed it stucks in a loop. Also Unsloth docs has usage guides for models, e.g. [GLM-4.7-Flash](https://unsloth.ai/docs/models/glm-4.7-flash#usage-guide). They provide different params for tool calling. ",
          "score": 1,
          "created_utc": "2026-02-20 08:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnp8d",
          "author": "former_farmer",
          "text": "I had better outcomes with opencode cli directly, ollama, and 30b models at full size. Slow as fuck but worked. Same pc as yours.",
          "score": 1,
          "created_utc": "2026-02-20 15:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iqz6a",
          "author": "Xantrk",
          "text": "I think LM studios llama.cpp is the reason for GLM performing so badly. I had endless looping issues, as soon as I switched to llama.cpp, they disappeared.",
          "score": 1,
          "created_utc": "2026-02-21 00:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f1pbk",
          "author": "HenkPoley",
          "text": "Qwen 3 Coder 30B, at maybe 4 bit, don’t run 1 bit quantization.\n\nBtw, the KV cache will mostly eat your RAM.",
          "score": 1,
          "created_utc": "2026-02-20 13:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e8d63",
          "author": "Antique_Dot_5513",
          "text": "Qwen code 30b sur opencode. T’attend pas à des miracles par contre.",
          "score": 0,
          "created_utc": "2026-02-20 09:35:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9boaw",
      "title": "Long conversation prompt got exposed",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vsgr1nirihkg1.jpeg",
      "author": "Ramenko1",
      "created_utc": "2026-02-19 21:16:10",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9boaw/long_conversation_prompt_got_exposed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6g3a4x",
          "author": "Decent_Solution5000",
          "text": "Yeah, not surprised. Bet it gets reminders a lot from writers editing manuscripts. lol This one sounds better than ones I read about awhile ago. Wasn't using Claude then, Kind of glad I missed that.",
          "score": 1,
          "created_utc": "2026-02-20 16:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6njw18",
          "author": "iamsimonsta",
          "text": "That font plays havoc with kimi k2 ocr, interesting.",
          "score": 1,
          "created_utc": "2026-02-21 19:52:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xk6ao",
          "author": "Ally_M101",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-02-23 10:51:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra2atm",
      "title": "Is anyone else pining for Gemma 4?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "author": "Formula71",
      "created_utc": "2026-02-20 17:51:43",
      "score": 18,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "About this time last year, I was impressed with Gemma 3, but besides the GPT-OSS models, it seems like the US based labs have been pretty quite on the open source front, and even GPT-OSS even feels like a while ago now.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6hjxt1",
          "author": "journalofassociation",
          "text": "I am very much looking forward to Gemma 4.  It's been almost a year since Gemma 3 was released.  \n\nNothing I can do but whine and speculate, though, except to play with all the other great OSS models we've gotten.",
          "score": 5,
          "created_utc": "2026-02-20 20:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hwtax",
          "author": "Klutzy_Ad_1157",
          "text": "Same, it understands German so well and I use it almost for a year now. I hope so much for Gemma 4!",
          "score": 2,
          "created_utc": "2026-02-20 21:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gvfwo",
          "author": "Savantskie1",
          "text": "GPT-OSS models were released in August 5, 2025. Literally months ago. Models no matter the size take time and hardware to train. It's not like they have a ton of extra compute they're not using just laying around.",
          "score": 5,
          "created_utc": "2026-02-20 18:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hg84m",
              "author": "UndecidedLee",
              "text": ">It's not like they have a ton of extra compute they're not using just laying around.\n\n\n\nThey do. That's why they're trying to shoehorn LLMs in everything.\n\nYou browse the internet often? - Here's an AI that will summarize the page for you.  \nYou have lots of media on your computer? - How about letting AI sort it for you?",
              "score": 4,
              "created_utc": "2026-02-20 20:16:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hj9zr",
                  "author": "Savantskie1",
                  "text": "Yeah that’s for the normies and it’s utilizing their current models wich they run at high concurrency. Why waste money on hardware not being utilized. Yeah you know nothing about business",
                  "score": -2,
                  "created_utc": "2026-02-20 20:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9uttx",
      "title": "Running local LLMs on my art archive, paranoid or actually unsafe?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r9uttx/running_local_llms_on_my_art_archive_paranoid_or/",
      "author": "LifeguardAny1801",
      "created_utc": "2026-02-20 13:04:59",
      "score": 18,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "I'm a professional illustrator and I've basically de-googled my archive: no Drive, no Dropbox, no cloud backup. Everything's on local storage because the idea of my style getting scraped into some training set makes me sick.\n\nNow I'm tempted by \"local AI\" stuff: NAS with on-device tagging, local LLMs, etc. In theory it's perfect: smart search but everything stays at home.\n\nFor people here who run local models on private data (art, notes, docs):\n\n* What's your threat model? Is \"no network / no cloud at all\" the only truly safe option?\n* How do you make sure nothing leaks? (open-source only, firewalls, VLANs, traffic sniffing?\n\nCurious how you all balance privacy / not feeding big models vs having modern search + tagging on your own hardware.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9uttx/running_local_llms_on_my_art_archive_paranoid_or/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6fmmq1",
          "author": "_raydeStar",
          "text": "If you're running a local server, just don't expose it to the outside world.  It's actually difficult to expose a local server to the internet, my ISP refuses to do it and I have to do workarounds. \n\nIf you think you might be the target of a personal attack (someone logging into your network and stealing your stuff) then be a bit more aggressive with security.  \n\nFollow general guidelines -- encrypted files, don't expose it, etc, and you will be ok.  If you are \\*really really\\* worried about security, take a basic udemy course on it, it'll walk you through the ins and outs in ways that I have no way of doing over Reddit comments.",
          "score": 3,
          "created_utc": "2026-02-20 15:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6havpj",
          "author": "Cronus_k98",
          "text": "Your bigger problem is that you don’t have a proper backup. RAID is not a backup and if you’re counting on your NAS to never loose your data, you’re going to loose your data.\n\nThere are private cloud storage providers out there. Keep your NAS for local access and periodically back it up to secure, encrypted storage and it’ll never get scraped for LLM use. \n",
          "score": 1,
          "created_utc": "2026-02-20 19:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oxqwq",
              "author": "RG_Fusion",
              "text": "If their goal is to protect their personal data, uploading to a cloud storage provider isn't really an option.\n\n\nThey just need to back-up their media on another storage medium, either HDD or Blu-ray, and then store those media drives in a separate physical location.",
              "score": 1,
              "created_utc": "2026-02-22 00:31:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fsm1w",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -6,
          "created_utc": "2026-02-20 15:40:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h5oy1",
              "author": "hhioh",
              "text": "Sounds like you place a huge weight on internet archiving as discerning some kind of meaning \n\nOkay lmao \n\nThis isn’t more meaningful than fizzling out back into the universe, my friend\n\nNo need to be so weird about it ❤️",
              "score": 1,
              "created_utc": "2026-02-20 19:25:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rce51e",
      "title": "Two good models for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rce51e/two_good_models_for_coding/",
      "author": "EfficientCouple8285",
      "created_utc": "2026-02-23 10:40:42",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "What are good models to run locally for coding is asked at least once a week in this reddit.   \n\n\nSo for anyone looking for an answer with around 96GB (RAM/VRAM) these two models have been really good for agentic coding work (opencode). \n\n* plezan/MiniMax-M2.1-REAP-50-W4A16   \n* cyankiwi/Qwen3-Coder-Next-REAM-AWQ-4bit\n\nMinimax gives 20-40 tks and 5000-20000 pps. Qwen is nearly twice as fast. Using vllm on 4 X RTX3090 in parallel.  Minimax is a bit stronger on task requiring more reasoning, both are good at tool calls. \n\nSo I did a quick comparison with Claude code asking for it to follow a python SKILL.md. This is what I got with this prompt: \" Use python-coding skill to recommend changes to python codebase in this project\"\n\n**CLAUDE**\n\nhttps://preview.redd.it/jyii8fa4z7lg1.png?width=2828&format=png&auto=webp&s=869b898762a3113ad3a8b006b28457cfb9628da5\n\n**MINIMAX**\n\nhttps://preview.redd.it/5gp4nsp7z7lg1.png?width=2126&format=png&auto=webp&s=8171f15f6356d6bb7a2279b3d4a2cc591ca22c0a\n\n**QWEN**  \n\n\nhttps://preview.redd.it/zf8d383az7lg1.png?width=1844&format=png&auto=webp&s=ba75a84980901837a9b16bbe466df7092675a1b6\n\nBoth Claude and Qwen needed me make a 2nd specific prompt about size to trigger the analysis. Minimax recommend the refactoring directly based on skill. I would say all three came up wit a reasonable recommendation.   \n\n\nJust to adjust expectations a bit. Minimax and Qwen are not Claude replacements. Claude is by far better on complex analysis/design and debugging. However it cost a lot of money when being used for simple/medium coding tasks.  The REAP/REAM process removes layers in model that are unactivated when running a test dataset. It is lobotomizing the model, but in my experience it works much better than running a small model that fits in memory (30b/80b). Be very careful about using quants on kv\\_cache to limit memory. In my testing even a Q8 destroyed the quality of the model.    \n   \nA small note at the end. If you have multi-gpu setup, you really should use vllm. I have tried llama/ik-llama/extllamav3 (total pain btw). vLLM is more fiddly than llama.cpp, but once you get your memory settings right it just gives 1.5-2x more tokens.  Here is my llama-swap config for running those models:   \n\n\n    \"minimax-vllm\":     \n    ttl: 600     \n      vllm serve plezan/MiniMax-M2.1-REAP-50-W4A16 \\\n        --port ${PORT} \\ \n        --chat-template-content-format openai \\ \n        --tensor-parallel-size 4  \\\n        --tool-call-parser minimax_m2 \\\n        --reasoning-parser minimax_m2_append_think \\\n        --enable-auto-tool-choice \\\n        --trust-remote-code \\\n        --enable-prefix-caching \\\n        --max-model-len 110000 \\\n        --max_num_batched_tokens 8192  \\\n        --gpu-memory-utilization 0.96 \\\n        --enable-chunked-prefill  \\\n        --max-num-seqs 1   \\\n        --block_size 16 \\\n        --served-model-name minimax-vllm   \n\n    \"qwen3-coder-next\":     \n     cmd: |       \n        vllm serve cyankiwi/Qwen3-Coder-Next-REAM-AWQ-4bit  \\\n           --port ${PORT} \\\n           --tensor-parallel-size 4  \\\n           --trust-remote-code \\\n           --max-model-len 110000 \\\n           --tool-call-parser qwen3_coder \\\n           --enable-auto-tool-choice \\\n           --gpu-memory-utilization 0.93 \\\n           --max-num-seqs 1 \\\n           --max_num_batched_tokens 8192 \\\n           --block_size 16 \\\n           --served-model-name qwen3-coder-next \\\n           --enable-prefix-caching \\\n           --enable-chunked-prefill  \\\n           --served-model-name qwen3-coder-next       \n\nRunning vllm 0.15.1. I get the occasional hang, but just restart vllm when it happens. I havent tested 128k tokens as I prefer to limit context quite a bit.   \n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rce51e/two_good_models_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6ymnlu",
          "author": "PooMonger20",
          "text": "Thanks for sharing. \n\nI'm pretty sure most folks here don't sport 96gb.\n\nCould be interesting to find something usable under 30gb. So far the best I had is OSS20.",
          "score": 2,
          "created_utc": "2026-02-23 15:06:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z3sf3",
              "author": "stormy1one",
              "text": "If you are coding, Qwen3-Coder-Next outranks OSS20 on swe-rebench.   Changed my workflow entirely around it, previously was using OSS20",
              "score": 3,
              "created_utc": "2026-02-23 16:28:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6z6rxr",
                  "author": "PooMonger20",
                  "text": "Thanks for the reply. I tried it and came back to OSS20B. \nIt wasn't able to make a basic python Tetris-like game or a basic website. If others rave about it perhaps I did misconfigure it or something.\n\nThe thing is GPT-OSS-20B actually makes code that compiles many times from the first few attempts. \n\nEverything else I tried if it even compiles and doesn't fail on syntax errors - it misses a lot of functionality in my real life cases.",
                  "score": 1,
                  "created_utc": "2026-02-23 16:41:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70lozu",
          "author": "Best-Tomatillo-7423",
          "text": "Using the qween coder 80b next on my AMD AI 370 with 96 gig ram running real good",
          "score": 1,
          "created_utc": "2026-02-23 20:38:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8v8r5",
      "title": "I built a clipboard AI that connects to your local LLM, one ⌥C away (macOS)",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/6fld6w8pbfkg1",
      "author": "morning-cereals",
      "created_utc": "2026-02-19 09:57:27",
      "score": 17,
      "num_comments": 13,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8v8r5/i_built_a_clipboard_ai_that_connects_to_your/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o69ql5f",
          "author": "BisonMysterious8902",
          "text": "It's rare that I'll install something that someone posted here, but this is actually pretty useful.\n\nI'm pointing it to my local model (LM Studio running qwen3-30b-a3b-2507) and it responded so quickly that I had to check the logs to ensure it was actually using the local server...\n\nWell done- this could be useful-",
          "score": 1,
          "created_utc": "2026-02-19 17:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dzfno",
              "author": "morning-cereals",
              "text": "Thanks, really great to hear! I've only tested it with a few friends and family so far 😊  \nFeel free to share any feedback, ideas, suggestions for improvement!",
              "score": 1,
              "created_utc": "2026-02-20 08:10:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6algpk",
          "author": "CtrlAltDelve",
          "text": "I *adore* apps like these. \n\nI will absolutely try it and let you know. Thank you so much for open sourcing it!",
          "score": 1,
          "created_utc": "2026-02-19 19:28:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dzkrw",
              "author": "morning-cereals",
              "text": "Yes please, it's a fairly new project so feedback would be much appreciated 🙏",
              "score": 1,
              "created_utc": "2026-02-20 08:12:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bv83d",
          "author": "nemuro87",
          "text": "I like this very much and would like to use it on my intel mac. It now gives an error after I try to download the built in model \"Bad CPU type\"\n\nDo you think this can be supported in the future?",
          "score": 1,
          "created_utc": "2026-02-19 23:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dzzf4",
              "author": "morning-cereals",
              "text": "Thanks for the interest!  The built-in bundled llama-server is compiled for Apple Silicon (ARM64) only, and I don't have plans to support Intel Macs at this point.\n\nThat said, if you install Ollama or LM Studio, Cai should work fine with those as the model provider. Just skip the built-in model download and **point it at your existing setup in Settings :)**",
              "score": 1,
              "created_utc": "2026-02-20 08:16:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6e099y",
                  "author": "nemuro87",
                  "text": "Thanks for the info, I will try.\n\nAnd again congrats for the project, you did in a very short amount of time what Apple Intelligence failed to deliver, a simple and configurable solution that works offline and is truly private.",
                  "score": 1,
                  "created_utc": "2026-02-20 08:18:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eeiac",
          "author": "aretheworsst",
          "text": "Ah man this is awesome, used it all day working. If it’s not a huge ask API key support for the model provider would be awesome, I have my server running in lm studio with the new auth feature enabled. \n\nJust set up my new ai computer so will be testing it with your app this weekend, cheers!",
          "score": 1,
          "created_utc": "2026-02-20 10:32:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yicoj",
              "author": "morning-cereals",
              "text": "It's on the roadmap for the next release. Currently traveling, so might take a few weeks :) ",
              "score": 1,
              "created_utc": "2026-02-23 14:44:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f3r7g",
          "author": "Financial-Source7453",
          "text": "Cherry Ai has a nice floating panel you can use to quickly pivot to Ai agent.",
          "score": 1,
          "created_utc": "2026-02-20 13:33:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67wv4a",
          "author": "Swimming_Ad_5205",
          "text": "А на гитхабе кода нет случайно?",
          "score": 0,
          "created_utc": "2026-02-19 10:29:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67x10f",
              "author": "Swimming_Ad_5205",
              "text": "Все нашёл \nhttps://github.com/soyasis/cai\n\nСпасибо) посмотрю",
              "score": 0,
              "created_utc": "2026-02-19 10:30:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbidbo",
      "title": "I benchmarked GPT 20B on L4 24 vs L40S 48 vs H100 80: response times, decoding speed & cost",
      "subreddit": "LocalLLM",
      "url": "https://devforth.io/insights/self-hosted-gpt-real-response-time-token-throughput-and-cost-on-l4-l40s-and-h100-for-gpt-oss-20b/",
      "author": "vanbrosh",
      "created_utc": "2026-02-22 10:19:45",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rbidbo/i_benchmarked_gpt_20b_on_l4_24_vs_l40s_48_vs_h100/",
      "domain": "devforth.io",
      "is_self": false,
      "comments": []
    }
  ]
}