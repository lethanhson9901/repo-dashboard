{
  "metadata": {
    "last_updated": "2026-01-06 16:56:03",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 30,
    "total_comments": 203,
    "file_size_bytes": 233648
  },
  "items": [
    {
      "id": "1pzdjoc",
      "title": "Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware â€“ Full Optimization Guide",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xdj2zjnz5bag1.png",
      "author": "at0mi",
      "created_utc": "2025-12-30 09:14:01",
      "score": 143,
      "num_comments": 22,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzdjoc/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwpcpjs",
          "author": "Lxzan",
          "text": "Nice work and thanks for sharing! How much is the power draw?",
          "score": 18,
          "created_utc": "2025-12-30 09:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpd7l0",
              "author": "at0mi",
              "text": "about 1600W i will update my blogpost with detailed power draw\n\nUPDATE: the 1600W was at higher Thread count, with the optimum 64 Threads im at 1154W\n\nIdle is 694W",
              "score": 21,
              "created_utc": "2025-12-30 09:31:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwph0z3",
                  "author": "Amazing_Athlete_2265",
                  "text": "Oof, that's gonna cost a bunch to run",
                  "score": 17,
                  "created_utc": "2025-12-30 10:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqiawc",
                  "author": "mister2d",
                  "text": "Those are some expensive tokens.",
                  "score": 3,
                  "created_utc": "2025-12-30 14:36:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqwrjk",
                  "author": "resil_update_bad",
                  "text": "jesus",
                  "score": 1,
                  "created_utc": "2025-12-30 15:50:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpoy5i",
          "author": "beedunc",
          "text": "Yes, been running the qwen3coder480b@q3 (245GB) on an old Dell T5810 running a single e5-2697v4, gets 2-3 tps. \n\nPower draw is only 250w under load. \n\nI never thought of disabling hyper threading, does that help a lot? Will be checking this out, thank you.",
          "score": 8,
          "created_utc": "2025-12-30 11:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwppbbm",
              "author": "at0mi",
              "text": "i also tried lower quantisation models but the quality of the output was crap, works for a chatbot but not for coding, \nin my case (8 numa nodes) disabling hyper threading gave an enormous boost",
              "score": 6,
              "created_utc": "2025-12-30 11:22:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwsp871",
              "author": "Candid_Highlight_116",
              "text": "If LLM inference is RAM bandwidth bound and HT was a tech to halve effective bandwidth because two virtual cores needs their respective data, it makes sense that turning off HT gives massive boost  \n\nsorry that's my hallucination but if it's actually like that it's pretty interesting",
              "score": 4,
              "created_utc": "2025-12-30 20:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu51bd",
                  "author": "beedunc",
                  "text": "Cool, thanks for the suggestion!",
                  "score": 1,
                  "created_utc": "2025-12-31 01:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpkj5q",
          "author": "xgiovio",
          "text": "Watts",
          "score": 7,
          "created_utc": "2025-12-30 10:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwql1ld",
              "author": "MaverickPT",
              "text": "All of them",
              "score": 7,
              "created_utc": "2025-12-30 14:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqd59a",
          "author": "chafey",
          "text": "I love hacks like this - nice work.  The hardware may be cheap/free but the electricity won't be...",
          "score": 6,
          "created_utc": "2025-12-30 14:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq21y3",
          "author": "Such_Advantage_6949",
          "text": "I am passionate about running llm at usable speed..",
          "score": 5,
          "created_utc": "2025-12-30 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws7ver",
          "author": "Foreign-Watch-3730",
          "text": "Same result ( 5.1 t/s ) , but in IQ5\\_K with 2 Xeon E5 2696 V4 and 400 Gb ddr4 ram ( on a very olf Dell T630 ) with 2 RTX 5090 ( ik\\_llama.cpp and [ubergarm](https://huggingface.co/ubergarm) for use opencode :  \n[https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5](https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5)\n\nnumactl --interleave=all ./build/bin/llama-server  \n\\--model \\~/ik\\_llama.cpp/models/GLM-4.7-Ubergarm/IQ5\\_K/GLM-4.7-IQ5\\_K-00001-of-00007.gguf  \n\\--alias GLM-4.7-IQ5  \n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8080  \n\\--ctx-size 84992  \n\\--no-mmap  \n\\--threads 82 --threads-batch 82  \n\\--batch-size 1024 --ubatch-size 1024  \n\\--parallel 1 --flash-attn 1  \n\\--jinja --verbose  \n\\--n-gpu-layers 99  \n\\--tensor-split 0.5,0.5  \n\\--split-mode layer  \n\\--run-time-repack  \n\\--cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--k-cache-hadamard  \n\\-ot 'blk.\\[0-8\\]..\\*exps.weight=CUDA0'  \n\\-ot 'blk.(8\\[6-9\\]|9\\[0-2\\])..\\*exps.weight=CUDA1'  \n\\-ot '.\\*exps.weight=CPU'",
          "score": 3,
          "created_utc": "2025-12-30 19:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqj2yx",
          "author": "Extension-Cow2818",
          "text": "Very interesting that turning hyperthreading off works better.  \nProbably memory access is causing issues in these types of workloads.\n\nIt would be also interesting to try MLK vs ATLAS vs BLAS",
          "score": 2,
          "created_utc": "2025-12-30 14:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq772g",
          "author": "Icy_Programmer7186",
          "text": "That's cool.  \nHow much memory did it consumed?",
          "score": 1,
          "created_utc": "2025-12-30 13:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqh2cl",
          "author": "ForsookComparison",
          "text": "This is very cool, thank you for testing this out.\n\nI'm curious what the use-case is? Is GLM decent as a general purpose model? Or will you give it a coding task and come back after a few hours",
          "score": 1,
          "created_utc": "2025-12-30 14:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdqlaq",
          "author": "spacefarers",
          "text": "Comes down to around $9 per million tokens just for electricity not sure if it's worth it lol",
          "score": 1,
          "created_utc": "2026-01-03 04:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyogyg",
          "author": "PitifulBall3670",
          "text": "Good job",
          "score": 1,
          "created_utc": "2026-01-06 06:11:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrv2is",
          "author": "Free_Manner_2318",
          "text": "7200 Watt for 5 tokens eh?!   \nAsk it if it was a reasonable decision.... :))))  \nB+ for the effort though. Not custom enough to be significant.",
          "score": 0,
          "created_utc": "2025-12-30 18:29:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py1gvp",
      "title": "Probably more true than I would like to admit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/iann5fyl70ag1.png",
      "author": "low_v2r",
      "created_utc": "2025-12-28 20:24:33",
      "score": 137,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1gvp/probably_more_true_than_i_would_like_to_admit/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwf8lgr",
          "author": "SunshineSeattle",
          "text": "I bought a particle tacyon, but now i have no idea what to do with it. ðŸ˜­",
          "score": 9,
          "created_utc": "2025-12-28 20:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfk71i",
          "author": "Healthy-Nebula-3603",
          "text": "WHAT AN EDGE DEVICE ?",
          "score": 5,
          "created_utc": "2025-12-28 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfm8iu",
              "author": "Count_Rugens_Finger",
              "text": "honestly not sure if this is serious but if it is, it just means the device-in-hand of the users (phones, tablets, PCs, car dashboards, POS systems, and whatnot)",
              "score": 7,
              "created_utc": "2025-12-28 21:35:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksnf2",
                  "author": "QuinQuix",
                  "text": "Usually this refers to devices that are compute restrained.\n\nA user could have a beast of a pc workstation that technically lives on the edge (of the central cloud workspace) but it's not a usual thing to refer to compute strong devices as edge devices.",
                  "score": 2,
                  "created_utc": "2025-12-29 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj6kgc",
              "author": "trmnl_cmdr",
              "text": "https://preview.redd.it/wl5ul68kv4ag1.jpeg?width=349&format=pjpg&auto=webp&s=38e1efd5e4769de7bcf9d7c4bf5ae45a353fefb4\n\nThis, apparently. Although Iâ€™m not sure how this helps me masturbate.",
              "score": 4,
              "created_utc": "2025-12-29 12:05:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1g3yx",
                  "author": "jgwinner",
                  "text": "Please rev it up a bit ... now tilt it so the blade scrapes on the stone ...\n\nthat's it ...\n\nFASTER \n\nthat's IT ... IT ...\n\nuhhh.\n\nWhew. Could you shut that thing off? kthks.",
                  "score": 1,
                  "created_utc": "2026-01-01 05:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwua4fa",
              "author": "Jackuarren",
              "text": "The device that you use for edging, obviously.",
              "score": 1,
              "created_utc": "2025-12-31 01:51:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfaevi",
          "author": "Individual_Holiday_9",
          "text": "Unironically this",
          "score": 2,
          "created_utc": "2025-12-28 20:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfid3y",
          "author": "GCoderDCoder",
          "text": "I feel so seen!!!",
          "score": 1,
          "created_utc": "2025-12-28 21:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt8rsj",
          "author": "mobileJay77",
          "text": "Your solution: \n1. Head over the r/localLlama\n2. You hardly sleep any more\n3. She doesn't have to worry what you are thinking in bed.",
          "score": 1,
          "created_utc": "2025-12-30 22:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q53qlk",
      "title": "LLMs are so unreliable",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/",
      "author": "Armageddon_80",
      "created_utc": "2026-01-06 00:45:52",
      "score": 98,
      "num_comments": 62,
      "upvote_ratio": 0.84,
      "text": "After 3 weeks of deep work, I''ve realized agents are so un predictable that are basically useless for any professional use. This is what I've found:\n\nLet's exclude the instructions that must be clear, effective and not ambiguos. Possibly with few shot examples (but not always!)\n\n1) Every model requires a system prompt carefully crafted with instructions styled  as similar as its training set. (Where do you find it? No idea)\nSame prompt with different model causes different results and performances. \nLesson learned: once you find a style that workish, better you stay with that model family.\n\n2) Inference parameters: that's is pure alchemy. time consuming of trial and error. (If you change model,  be ready to start all over again). No comment on this.\n\n3) system prompt  length: if you are too descriptive at best you inject a strong bias in the agent, at worst the model just forget some parts of it.\nIf you are too short model hallucinates.\nGood luck in finding the sweet spot, and still, you cross the fingers every time you run the agent. \nThis connect me to the next point...\n\n4) dense or MOE model? \nDense model are much better in keeping context (especially system instructions), but they are slow.\nMoE are fast, but during the experts activation not always the context is passed correctly among them. The \"not always\" makes me crazy.\nSo again you get different responses  based on I don't know what.! Pretty sure that are some obscure parameters as well...\nHope Qwen next will fix this.\n\n5) RAG and KGraphs? Fascinating but that's another field of science. Another deeeepp rabbit hole I don't even want to talk about now.\n\n6) Text to SQL?  You have to pray, a lot. Either you end up manually coding the commands and give it as tool, or be ready for disaster. And that is a BIG pity, since DB are very much used in any business.( Yeah yeah. Table description data types etc...already tried) \n\n7) you want reliability? Then go for structured input and output! Atomicity of tasks!\nI got to the point that between the problem decomposition to a level that the agent can manage it (reliably) and the construction of a structured input/output chain, the level of effort required makes me wonder what is this hype about AI? Or at least home AI.  (and I have a Ryzen AI max 395).\n\n\nAnd still after all the efforts you always have this feeling: will it work this time? \nAgentic shit is far far away from YouTube demos and frameworks examples.\nSome people creates Frankenstein systems, where even naming the combination they are using is too long,.but hey it works!! Question is \"for how long\"? \nWhat's gonna be deprecated or updated on the next version of one of your parts?\n\nWhat I've learned is that if you want to make something professional and reliable, (especially if you are being paid for it) better to use good old deterministic code, and as less dependencies as possible. Put here and there some LLM calls for those task where NLP is necessary because coding all conditions would take forever.\n\nNonetheless I do  believe, that in the end, the magical equilibrium of all parameters and prompts and shit must exist. And while I search for that sweet spot, I hope that local models will keep improving and making our life way simpler.\n\nJust for the curious: I've tried every possible model until gpt OSS 120b, Framework AGNO. Inference with LMstudio and Ollama (I'm on Windows, no vllm).\n\n \n\n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxxazja",
          "author": "StardockEngineer",
          "text": "Only Sonnet and Opus work well as agents that can be used without high specialization.  Anything else requires lots of leg work.  Minimax 2.1 is the closest Iâ€™ve found in the OSS world.",
          "score": 12,
          "created_utc": "2026-01-06 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0msbs",
              "author": "opensourcecolumbus",
              "text": "Minimax2.1 better than qwen 3?",
              "score": 1,
              "created_utc": "2026-01-06 15:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny0zpdv",
                  "author": "StardockEngineer",
                  "text": "Yes, it is.  Especially for agents.  The interleaved-thinking is great.  More here: https://www.minimax.io/news/minimax-m21",
                  "score": 3,
                  "created_utc": "2026-01-06 16:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny15m6i",
              "author": "svachalek",
              "text": "Haiku does too, given precise instructions. Itâ€™s in a league of its own for models that are in its presumed size class.",
              "score": 1,
              "created_utc": "2026-01-06 16:32:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx7hpr",
          "author": "macromind",
          "text": "Yep, this matches my experience too: agents look magical in demos, but in real work its mostly prompt + tool wiring + guardrails + a lot of retries and tests.\n\nOne thing that helped me was treating the LLM like a fallible component and making everything around it deterministic: strict JSON schemas, small steps, unit tests on tool outputs, and hard timeouts/fallbacks.\n\nIf you are experimenting with patterns for making agent workflows less chaotic, this writeup has a few practical ideas (tool contracts, evals, and reliability tricks): https://www.agentixlabs.com/blog/",
          "score": 45,
          "created_utc": "2026-01-06 00:51:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz41y4",
              "author": "publiusvaleri_us",
              "text": "The problems I see with this method is that testing prompts and scaling are counter-positioned into a matrix of chicken-and-egg paradoxes.\n\nLike opening a pizza store, but not getting a cheese and flour supplier. Regardless, you need to see who might want pizza in your community. That means you need to pick the cheese and flour, but the pizza wholesale company won't sell you just 10 pounds of it. They want a contract for 300 pounds a week and then they'll talk.\n\nSo you buy Walmart cheese and call your neighbor, but the test is inconclusive. The project can never move forward without high risk of capital and doing the whole pizza store thing and just opening up with the supplies you think are right.\n\nFor the LLM, it means you have to throw heavy hardware or high capital, lots of time, and lots of tests into something that might never be profitable for business or affordable for home hobbies.\n\nYou don't know until you add up the unpredictable costs while you tested and played with prompts for a week to a year. And find that secret sauce.",
              "score": 2,
              "created_utc": "2026-01-06 08:29:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz533p",
                  "author": "publiusvaleri_us",
                  "text": "And you know how I know this?\n\nBecause when you find your secret sauce LLM prompt and settings for an application, what happens? Productivity/quality/accuracy goes from zero to hero with that one last final tweak. Be it a word in a prompt or adding 128 GB of RAM to a PC, you found the sweet spot.\n\nEvery other spot was wrong. It's the graph that shows a sharp peak that rises from the noise floor to a 50 dB signal. It can be tweaked in version 2, but you broke through the barrier.\n\nEverything LLM is like this - very hit or very miss. I've seen the video series by [https://www.youtube.com/@aiwarehouse](https://www.youtube.com/@aiwarehouse) AI Warehouse that bears this out, as well. Albert is a moron for 10,000 iterations and then he \"learns\" a trick. And it cascades to a new skill.",
                  "score": -1,
                  "created_utc": "2026-01-06 08:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxxh77o",
              "author": "cneakysunt",
              "text": "We're about to dive in seriously. After much research over the last year this is exactly where we have landed.",
              "score": 3,
              "created_utc": "2026-01-06 01:43:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxiqrp",
          "author": "publiusvaleri_us",
          "text": "YES. #4 and #5, plus 1, 2, and 3. You did a lot of thinking on this and are correct.\n\nMy comment is this on the latter part. Your prediction of the future is sound. There will be so many iterations of AI improvements, PC hardware improvements, and refinements in the interface that the LLM of 2036 will be nothing like an LLM of 2026. Early adopters have so many disadvantages.\n\nThis stuff will be cheaper and faster in ten years. At current tech level and my current interest level, I think an adequate system, for just my personal use (With an eye to selling it), would cost $500 to $2000 per month and would need a new Internet connection. The current software may seem bloated when you download it, but the interfaces of today are simply a kludge and a Jack-of-all-trades that does nothing right except maybe answer chat questions a 6th grader might ask.\n\nAsk a current LLM about a dataset, and you're going to get terrible results. Even commercial systems stink. All you have to do is call into a Big Company and ask for tech support. The human agents are typing things into an AI, you can tell, and trying to bring up internal PDFs to answer your question and walk you through a solution.\n\nBecause their innate human knowledge is practically nil. They read from a script like phone agents have done for 40 years, but they are reading from hallucinated AI slop or the wrong PDF that's from 4 versions earlier that do not apply. LLMs have not taught Tier 1 support personnel how to think, and they certainly haven't trained them on the specifics of the things they are supporting.\n\nIf Fortune 100 companies can't figure out how to get AI to work to help their customers at Tier 1 (and their bottom line making money), I find it ridiculous to assume that a one-man-shop could figure it out.\n\nI wish all of you programmers, content creators, and schoolkids doing homework all the best! For the rest of us, it's hard to go full-in to this mess, for all of the reasons OP stated, starting at #1 and on down the list.\n\nBravo.",
          "score": 14,
          "created_utc": "2026-01-06 01:51:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzbjqd",
              "author": "thedarkbobo",
              "text": "Big corpo is slower sometimes in using top edge tech because of risks audits etc. I would think it should be first used by small companies. Big tech will use consultants when they are available and things stabilize s bit I think.",
              "score": 2,
              "created_utc": "2026-01-06 09:42:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx84fv",
          "author": "BrewHog",
          "text": "You mentioned it. Structured input and output with a reliable model (professionally I only use the big boy models Gemini 3, Claude, etc).Â \n\n\nI'm currently using it for quite a few tasks regularly and reliably. It definitely helps me keep my business running without the need to hire two or three employees.Â \n\n\nThe first level support is fantastic, and the cliche sentiment analysis usually works well for what I need.Â \n\n\nFor more complex tasks, I still use only DSPy for the structured in/out and many times just manually run it to save me oogads of time (product/marketing material, document reviewing, etc)Â \n\n\nGive us some specific examples of what you need so we can guide you. Or just propose an idea that you think should work, but doesn't in practice.",
          "score": 7,
          "created_utc": "2026-01-06 00:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxa8ut",
              "author": "Armageddon_80",
              "text": "I think, from early days, introducing frontier models in companies workflows is a huge strategical risk. \nClearly depend from the business you have and the complexity of it. But unfortunately the big players are all in USA (I'm in Europe ) and I stronly5 believe soon or late, AI will become a \"tool\" for geopolitical leverage.\nI can't imagine what happens to a company where employees got lazy thanks to the magic of AI, and the day after the service is interrupted (or even worse, no employees at all only AI). That's why I'm working hard to implement an architecture dependant only on Local models.",
              "score": 13,
              "created_utc": "2026-01-06 01:05:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxye9tv",
                  "author": "ThatOneGuy4321",
                  "text": "even if AI doesnâ€™t turn into a straight up propaganda tool, the huge disparity between current investment and revenue guarantees rapid enshittification of the service as they try to figure out how to make it profitable. \n\nQuite possibly 10x price hikes and complete flooding of marketing content. I seriously doubt OpenAI will even be able to solve their revenue problem honestly and may just go bankrupt. Their 5-year spending commitment is $1 trillion and their yearly revenue is $13 billion lol",
                  "score": 2,
                  "created_utc": "2026-01-06 04:55:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny013tm",
                  "author": "Kos187",
                  "text": "1. Go Europe!\n2. Mistral Large 3 isn't bad, it's also doesn't feel like first echelon model today \n3. Big Chinese models do feel like first Echelon and could be run locally,  but require expensive hardware. 10k for Mac Studio give rather small t/s, or 20k to fit into VRAM and good performance. Or maybe 128GB RAM and single large GPU for barely running it at all (/ t/s).\n4. Hosting in EU is also an option, but without reserved instances running big Chinese models is something like 5k a month spend.",
                  "score": 2,
                  "created_utc": "2026-01-06 13:06:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxv55p",
                  "author": "AfterAte",
                  "text": "I do believe someday the transatlantic cables will be cut by autonomous drones. This world has gone into full anarchy, and \"haven't seen nothing yet\". We'll all have to get Starlink after that.",
                  "score": 1,
                  "created_utc": "2026-01-06 02:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxllmf",
          "author": "Such_Advantage_6949",
          "text": "Reliable LLM is not cheap. From my experience, reliable local models are from minimax glm onwards (200b onwards). 120B oss is a start but still very hit and miss",
          "score": 7,
          "created_utc": "2026-01-06 02:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx8gb2",
          "author": "Krommander",
          "text": "If the instruction set is plain text or markdown I have found that the system's prompt adherence depends on the coherence.Â \n\n\nVery coherent prompts can be 100+ pages without breaking for commercial LLMs, if internal knowledge mapping and cognitive routines are recursive.Â \n\n\nFor local models, I have had some small success, but the context window gets busted after 2 replies if I use the same huge system prompt.Â \n\n\nTool use is a whole other bag of problems though.Â ",
          "score": 3,
          "created_utc": "2026-01-06 00:56:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzc9lp",
              "author": "thedarkbobo",
              "text": "Need to compact memory I guess if reaches threshold.",
              "score": 2,
              "created_utc": "2026-01-06 09:49:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxae4x",
          "author": "PromptOutlaw",
          "text": "I went thru this pain, for my scope of work I managed to tame most models except Kimi K2 and Cohere-command-a. You need:\n- adapters per LLM\n- normalization per LLM\n- strict output response\n\nHave a look here I hope it helps: https://github.com/Wahjid-Nasser/12-Angry-Tokens\n\nIâ€™m pushing an update this week for observability, redundancy and some stats",
          "score": 3,
          "created_utc": "2026-01-06 01:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxdm1b",
              "author": "Armageddon_80",
              "text": "Can you explain what do you mean for adapters and normalization? I'm very interested",
              "score": 1,
              "created_utc": "2026-01-06 01:24:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxxgj4n",
                  "author": "PromptOutlaw",
                  "text": "E.g. Deepseek is gna spit out its thinking, stop fighting it and strip. Claude fights you to surround your output with markdown tags, just strip it. With adapters you have a generic prompt core, and each LLM must have an adapter prompt padded before API call to make it behave.\n\nHighly advise against generic prompt for all to work. I ended up with prompt-spaghetti that gave me headaches",
                  "score": 2,
                  "created_utc": "2026-01-06 01:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxd877",
          "author": "Classic_Chemical_237",
          "text": "Traditionally, we work with structured input and output. With structured io, itâ€™s natural to be UX-centric.\n\nWith LLM, it became text centric with chat and voice. To work with traditional endpoints, supposedly you create MCP to sit on top of API.\n\nThis whole approach is wrong. Human work way better which structured UX. (Thatâ€™s what even LLM try to emulate it with bullet points and emojis) Most use cases only need LLM for part of the flow.\n\nWe donâ€™t need MCP sitting on top of API. We need API sitting on top of LLM.\n\nI am so close to ship ShapeShyft for make this easy.",
          "score": 3,
          "created_utc": "2026-01-06 01:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyzu8d",
          "author": "fermented_Owl-32",
          "text": "Using some psychological experience always MILDS DOWN these problems significantly. I have seen people complain of the same problems and i have implemented them in professional env as well. \n\nIts just a prompt, the better you understand how LLMs work and the better you are with human psychology and communication habits, the more robust outputs you get.\n\nI prefer to use not the latest models but from 2025 beginning H1. For Professional basic uses even Amazon's nova prime 2 does wonders  \n\nI still dont understand after a good analysis as this where you yourself write your issues, how have you not able to get things done by keeping these mind or making it part of the system.",
          "score": 3,
          "created_utc": "2026-01-06 07:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz48eb",
              "author": "Armageddon_80",
              "text": "I like your comment specially the last part:\nThe quick answer is that these systems are far more complex than what they may appear. \n\nIf you read the release papers of the models, you must be a PHD on many fields of study to  understand what's even written there. Which of course is not my case.\nI'm not talking of knowing and repeating as a parrot, I'm talking about understanding. \nAfter various steps of quantization and adaptation, some kind of distilled version of the original model finally lands on your computer. And now what? \nYou need to test it and start to know it, in a process of \"reverse learning\", and yes it's difficult.\n\nThe other way is to \"contain it\" to make it kind of behave the way you want, but that is also a lot of work, building guardrails and a very strict architecture around it, and  removing all the beauty of AI. \n\nLately I'm chatting with the models I intend to use, and making some precise questions to let the model leak its \"internal ways\" of writing, processing, expecting instructions. Trying to get some of it's secrets let's say through chat sessions and role plays \nIn other words psicology of the model. \n\n\n\n\n\n\n\n\n\n \nIf you try to simplify\n\n\nIn other words, you have many variables,",
              "score": 2,
              "created_utc": "2026-01-06 08:31:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxrim5",
          "author": "SAPPHIR3ROS3",
          "text": "As much as this kind match my experience, THE general rule i follow is â€œDivide et imperaâ€ (meaning divide and conquer) as much as possible: going under 30b, MoE or not, itâ€™s a guess hence you have to use LLM for NLP-based task only and as least as possible (above 30b you start seeing some more consistency, but only above 100b roughly the result start to feel reliable). Nonetheless structured input/output are vital to ensure consistency in workflows at any size even with the big bois (claude, gemini chatgpt ecc.)",
          "score": 2,
          "created_utc": "2026-01-06 02:39:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyvjfa",
          "author": "Your_Friendly_Nerd",
          "text": "I wholeheartedly agree with all the points you make. I've been trying to integrate local LLM's into my workflow as a programmer (mainly qwen3-coder). It works fine for simple tasks and tool calls, but as soon as it gets slightly more complex, it's not even worth the effort to write the prompt and wait for its output, it's quicker to just code it myself. \n\nThat said, I also think LLM's are still in their infancy, and there is so much we have yet to discover about them. For example for my use-case as a programmer, I see a lot of potential in spec-driven development, where a task is broken down into tiny subtasks that can then each be implemented and tested, and not be too complex for a local LLM to achieve. But how to formulate those specs? How does the workflow look like? I don't know.\n\nAnd that's maybe another issue - so many of the AI integrators (like IDE chat plugins) expect to talk to a frontier model like Claude Sonnet, Gemini or GPT. But when I plug them into a smaller model, they shouldn't just use the exact same prompts, but more specialized ones, and I fear there's just not enough interest in the community in perfecting this.",
          "score": 2,
          "created_utc": "2026-01-06 07:11:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzcvsp",
              "author": "thedarkbobo",
              "text": "What exactly was your setup? How far did you go?:)",
              "score": 1,
              "created_utc": "2026-01-06 09:54:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx8dte",
          "author": "newbietofx",
          "text": "I'm creating a text to sql llm. How do I ensure it works? Run complicated crud?Â ",
          "score": 1,
          "created_utc": "2026-01-06 00:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxc9hn",
              "author": "Armageddon_80",
              "text": "Most model will fail on multi tables DB.\nI had to create an agent for every table, engineering the table itself with explicit column names and minimal foreign keys. I had to include brief description for each column so the model can \"link\" my text request. Understand it and then convert it into SQL command. \nI'm now running a team of agents, where each agent represent  their  tables (CRUD)  and the team leader represent the DB.\nStill working on it, I was just telling you how I'm fixing that thing and maybe give you an idea.",
              "score": 1,
              "created_utc": "2026-01-06 01:16:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0rj2z",
                  "author": "Powerful-Street",
                  "text": "Just make your own router essentially and keep the output in memory until it is written to db by your router.",
                  "score": 1,
                  "created_utc": "2026-01-06 15:27:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxcpuh",
          "author": "Mountain-Hedgehog128",
          "text": "Agents need to be combined with rules based structure. They are valuable, but the toughest part is finding the right places and ways to use them.",
          "score": 1,
          "created_utc": "2026-01-06 01:19:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxesr1",
          "author": "Taserface_ow",
          "text": "LLMs are never going to be perfect, itâ€™s the nature of the model. Successful products/services that use LLMs build workflows around them to cater for this limitation.",
          "score": 1,
          "created_utc": "2026-01-06 01:30:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxewea",
          "author": "grathontolarsdatarod",
          "text": "So.... Lie detectors are not admissible as evidence in most courts with an advanced legal system.  Yet they are used in employment screening in those same jurisdictions. \n\nAI is a scape goat for anything.  Its literally tablets from the mountain that can whatever.",
          "score": 1,
          "created_utc": "2026-01-06 01:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxawx",
          "author": "evilbarron2",
          "text": "I think there are valid use cases for LLMs in production systems, but I feel like most places I see it used itâ€™s being shoehorned. Iâ€™m not sure that they are most (or even particularly) useful in our current software systems. I kinda think theyâ€™ll be more useful in a different type of computing structure. I think weâ€™ve only seen glimpses of what that looks like so far.",
          "score": 1,
          "created_utc": "2026-01-06 03:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxwx3",
          "author": "Proof_Scene_9281",
          "text": "You have to blend commercial LLMâ€™s and local for the best results. Use the commercial to think and craft and the locals to digÂ ",
          "score": 1,
          "created_utc": "2026-01-06 03:14:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy58mr",
          "author": "IllustratorInner4904",
          "text": "Intent -> deterministic tools -> let agent call tools = less unreliable agents (who instead sometimes want to tell you about the tools they will call lmao)",
          "score": 1,
          "created_utc": "2026-01-06 03:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyrvap",
          "author": "Much-Researcher6135",
          "text": "hush now, people will begin to think it's a bubble\n\nwouldn't want that",
          "score": 1,
          "created_utc": "2026-01-06 06:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyumsw",
          "author": "linewhite",
          "text": "Yeah, i just made [https://www.ngrm.ai](https://www.ngrm.ai) to deal with this, structured memory drastically helped my workflows, LLMs are not enough, but with the right tools it gets better as time goes on.",
          "score": 1,
          "created_utc": "2026-01-06 07:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyzklf",
          "author": "Dentuam",
          "text": "The point about MoE models dropping context during expert routing still rings true for many local runs, though the latest Qwen3 MoE releases have noticeably stabilized that behavior compared to earlier versions. Dense models continue to win for reliability when you need consistent instruction following.",
          "score": 1,
          "created_utc": "2026-01-06 07:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz7cs9",
          "author": "Black_Hair_Foreigner",
          "text": "LLM is just another automation tool. Like, When you write some code but you are too lazy to write over 1000 code. So you decide to use LLM to write code and it did just 5 minute or less. In this progress, you check LLMâ€™s code that this logic is really correct. You found some bug or nun-sense, and fix it. Everything is running well! If you doing this shit by your hand, you will be consume your time more than 3 days and your time is gold. This is why everyone use this shit. Everyone knows this is piece of shit, but time is too expensive to write code.",
          "score": 1,
          "created_utc": "2026-01-06 09:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzj83l",
              "author": "Armageddon_80",
              "text": "For coding I must say that both Gemini Antigravity and Claude are fantastic, but for WRITING code, not ENGINEERING.  Still crazy horses hard to control, but very very good. \nIf you want to create something from zero, is gonna take many rounds before it takes shape (and you actually understand the code).",
              "score": 1,
              "created_utc": "2026-01-06 10:52:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzaxiz",
          "author": "a_pimpnamed",
          "text": "I truly don't understand why people think LLM's can ever actually become true intelligence. You can't even see the logic chain or query it about its own logic. They learn once and they are stuck unless you train them over again. They use way too much compute. This can only be scaled so much even if Microsoft gets bitnet to work it still would be capped eventually it's not true intelligence just a prediction algo a very good prediction algo. We need to move back to symbolic ai or to actual cognitive architectures.",
          "score": 1,
          "created_utc": "2026-01-06 09:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzjm4b",
              "author": "Armageddon_80",
              "text": "Well for many many tasks you don't need intelligence. Including most of business workflow: People over estimate their roles in working environments :).\nSo I agree with you, but AI can save you from writing lot of code for simple things like user intent.yepp that's not intelligence, but why not to use it?",
              "score": 2,
              "created_utc": "2026-01-06 10:55:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzjo3t",
          "author": "Cergorach",
          "text": "A couple of things:\n\na.) LLMs are tools, just like with any tool, you need to learn how to use it. A master woodworker will be far more proficient with a hammer then Joe Smoe that just picked up a hammer from Harborfreight for the first time ever. Different tools, different skill sets.\n\nb.) Right tool for the job. For some jobs these tools are either the wrong tools or just not worth the time/cost. Just as it's sometimes just faster to do something yourself then explain to someone else what you want so they can do it for you.\n\nc.) There can be a huge difference in quality between 'cheap' tools and 'expensive' tools. In this case you're using tiny models locally, even the 120B quantized is not going to compare well to the big unquantized models. For SQL see: [https://llm-benchmark.tinybird.live/](https://llm-benchmark.tinybird.live/)\n\nd.) You need to know the answer, always with LLM. If you don't, you're in for a world of hurt in a professional production environment. And SQL queries in a production environment seem to *me* like probably the worst possible use of LLM.\n\ne.) People need to realize that at what point are you spending more time=money on LLM then actually making your work more efficient?\n\nSidenote: I have not yet used LLM in my professional capacity. Primary reason is that I tend to work on the edge of IT deployment, and the LLMs haven't been trained yet on the use cases I'm working on and the edge cases that are very rare. Not to mention that even IF the LLM is trained on the 'promotional' material, the reason I'm doing my thing is to see if what's been sold to the client actually works as the sales people say it does (and it often does not)... The other reason is that the companies/clients I work for either do not allow LLM or have not yet onboarded it. And when I work for a client, I only use what they have allowed.\n\nPersonally I've tested quite a bit on a Mac Mini M4 Pro 64GB with models up to 70b, mostly for hobby projects. The results from there is that while very cool and the DS 70b model was better then the far larger GPT from less then a year before, still the unquantized far larger models we can't run locally did far, far better. And due to it being for my hobby projects, I saw no security concerns to *not* use the larger models (for free) on the web. Even the DS 671b model *quantized* on a $10k Mac Studio M3 Ultra 512GB gave worse quality responses then the full unquantized free model you could use on the Internet. Spending $50k on a Mac Studio cluster would be cool, but imho highly inefficient.",
          "score": 1,
          "created_utc": "2026-01-06 10:55:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0ufgv",
          "author": "_WaterBear",
          "text": "OpenEvidence is pretty much the only professional use-case Iâ€™ve encountered thus far that seems competent. RAG/document embedding for info searching is quite functional and reliable so long as the user isnâ€™t lazy and only uses the LLM as a search engine. Most other use cases involve too high a probability of hallucination that, for any professional requiring precision, just adds burdensome QA uncertainty to their process. So, outside highly tailored and unserious use-cases, these things will be a bust. Circumscribe the bubble, lest you be caught inside when it pops.",
          "score": 1,
          "created_utc": "2026-01-06 15:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx75c7",
          "author": "writesCommentsHigh",
          "text": "okay now compare it to frontier models via api.",
          "score": 1,
          "created_utc": "2026-01-06 00:49:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx8qj4",
              "author": "Armageddon_80",
              "text": "Yes I know, but I've posted on local LLM for a reason.",
              "score": 5,
              "created_utc": "2026-01-06 00:57:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0e2tt",
                  "author": "writesCommentsHigh",
                  "text": "And your title reads â€œall llmsâ€\n\nYes local llms are going to be unreliable but not all",
                  "score": 1,
                  "created_utc": "2026-01-06 14:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxx9xk7",
              "author": "ispiele",
              "text": "Most of these points apply to the latest API models as well (in my experience), the first 3 points in particular",
              "score": 4,
              "created_utc": "2026-01-06 01:04:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxe2gz",
                  "author": "ANTIVNTIANTI",
                  "text": "Anthropic is managing a cult apparently",
                  "score": 2,
                  "created_utc": "2026-01-06 01:26:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxaqdl",
                  "author": "StardockEngineer",
                  "text": "Not to Sonnet and Opus.",
                  "score": 1,
                  "created_utc": "2026-01-06 01:08:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxv8u6",
          "author": "alphatrad",
          "text": "Sounds like a skill issue",
          "score": -3,
          "created_utc": "2026-01-06 02:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxjt0j",
          "author": "Terminator857",
          "text": "More reliable than humans for me.",
          "score": 0,
          "created_utc": "2026-01-06 01:57:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q06u1n",
      "title": "2025 is over. What were the best AI model releases this year?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "author": "techlatest_net",
      "created_utc": "2025-12-31 07:12:49",
      "score": 64,
      "num_comments": 32,
      "upvote_ratio": 0.88,
      "text": "2025 felt like three AI years compressed into one. Frontier LLMs went insane on reasoning, openâ€‘source finally became â€œgood enoughâ€ for a ton of real workloads, OCR and VLMs leveled up, and audio models quietly made agents actually usable in the real world. â€‹ Hereâ€™s a categoryâ€‘wise recap of the â€œbest of 2025â€ models that actually changed how people build stuff, not just leaderboard screenshots:\n\n\n\nLLMs and reasoning\n\n\\* GPTâ€‘5.2 (Thinking / Pro) â€“ Frontierâ€‘tier reasoning and coding, very fast inference, strong for longâ€‘horizon toolâ€‘using agents and complex workflows.\n\nâ€‹\\* Gemini 3 Pro / Deep Think â€“ Multiâ€‘million token context and multimodal â€œscreen reasoningâ€; excels at planning, code, and webâ€‘scale RAG / NotebookLMâ€‘style use cases. \n\n\\* Claude 4.5 (Sonnet / Opus) â€“ Extremely strong for agentic tool use, structured stepâ€‘byâ€‘step plans, and â€œuse the computer for meâ€ style tasks. \n\n\\* DeepSeekâ€‘V3.2 & Qwen3â€‘Thinking â€“ Openâ€‘weight monsters that narrowed the gap with closed models to within \\\\\\~0.3 points on key benchmarks while being orders of magnitude cheaper to run.\n\nIf 2023â€“24 was â€œjust use GPT,â€ 2025 finally became â€œpick an LLM like you pick a database.â€\n\nVision, VLMs & OCR\n\n\\* MiniCPMâ€‘V 4.5 â€“ One of the strongest open multimodal models for OCR, charts, documents, and even video frames, tuned to run on mobile/edge while still hitting SOTAâ€‘ish scores on OCRBench/OmniDocBench. \n\n\\* olmOCRâ€‘2â€‘7Bâ€‘1025 â€“ Allen Instituteâ€™s OCRâ€‘optimized VLM, fineâ€‘tuned from Qwen2.5â€‘VL, designed specifically for documents and longâ€‘form OCR pipelines. \n\n\\* InternVL 2.x / 2.5â€‘4B â€“ Open VLM family that became a goâ€‘to alternative to closed GPTâ€‘4Vâ€‘style models for document understanding, scene text, and multimodal reasoning.\n\n\\* Gemma 3 VLM & Qwen 2.5/3 VL lines â€“ Strong open(-ish) options for highâ€‘res visual reasoning, multilingual OCR, and longâ€‘form video understanding in productionâ€‘style systems. â€‹ \n\n2025 might be remembered as the year â€œPDF to clean Markdown with layout, tables, and chartsâ€ stopped feeling like magic and became a boring API call.\n\nAudio, speech & agents\n\n\\* Whisper (still king, but heavily optimized) â€“ Remained the default baseline for multilingual ASR in 2025, with tons of optimized forks and onâ€‘device deployments. \n\n\\* Lowâ€‘latency realâ€‘time TTS/ASR stacks (e.g., new streaming TTS models & APIs) â€“ Subâ€‘second latency + streaming text/audio turned LLMs into actual realâ€‘time voice agents instead of â€œpodcast narrators.â€ \n\n\\* Many 2025 voice stacks shipped as APIs rather than single models: ASR + LLM + realâ€‘time TTS glued together for call centers, copilots, and vibecoding IDEs. â€‹ Voice went from â€œcool demoâ€ to â€œI talk to my infra/IDE/CRM like a human, and it answers back, live.â€\n\nOCR/document AI & IDP\n\n\\* olmOCRâ€‘2â€‘7Bâ€‘1025, MiniCPMâ€‘V 4.5, InternVL 2.x, OCRFluxâ€‘3B, PaddleOCRâ€‘VL â€“ A whole stack of open models that can parse PDFs into structured Markdown with tables, formulas, charts, and long multiâ€‘page layouts. \n\n\\* On top of these, IDP / â€œPDF AIâ€ tools wrapped them into full products for invoices, contracts, and messy enterprise docs. â€‹ \n\nIf your 2022 stack was â€œTesseract + regex,â€ 2025 was â€œdrop a 100â€‘page scan and get usable JSON/Markdown back.â€ â€‹ \n\nOpenâ€‘source LLMs that actually mattered\n\n\\* DeepSeekâ€‘V3.x â€“ Aggressive MoE + thinking budgets + brutally low cost; a lot of people quietly moved internal workloads here. \n\n\\* Qwen3 family â€“ Strong openâ€‘weight reasoning, multilingual support, and specialized â€œThinkingâ€ variants that became default selfâ€‘host picks. \n\n\\* Llama 4 & friends â€“ Closed the gap to within \\\\\\~0.3 points of frontier models on several leaderboards, making â€œfully open infraâ€ a realistic choice for many orgs.\n\nâ€‹In 2025, openâ€‘source didnâ€™t fully catch the frontier, but for a lot of teams, it crossed the â€œgood enough + cheap enoughâ€ threshold.\n\nYour turn This list is obviously biased toward models that:\n\n\\* Changed how people build products (agents, RAG, document workflows, voice UIs)\n\n\\* Have public benchmarks, APIs, or open weights that normal devs can actually touch â€‹- What did you ship or adopt in 2025 that deserves â€œmodel of the yearâ€ status?\n\nFavorite frontier LLM?\n\n\\* Favorite openâ€‘source model you actually selfâ€‘hosted?\n\n\\* Best OCR / VLM / speech model that saved you from pain?\n\n\\* Drop your picks below so everyone can benchmark / vibeâ€‘test them going into 2026.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwvo5t9",
          "author": "Clipbeam",
          "text": "I vote the Qwen3 models. Total game changer for local LLMs.",
          "score": 35,
          "created_utc": "2025-12-31 07:35:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxs12w5",
              "author": "techlatest_net",
              "text": "Totally Agree",
              "score": 1,
              "created_utc": "2026-01-05 07:03:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvohey",
          "author": "DutchSEOnerd",
          "text": "Overall I would says Claude Opus had most impact, for local its definitely Qwen",
          "score": 13,
          "created_utc": "2025-12-31 07:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy6e07",
              "author": "ForsookComparison",
              "text": "+1 for Opus. Benchmarks don't show its impact properly. The thing dropping down to $25/1m output tokens led to me not needing to hand-write a line of code for the last month of this year in my biggest repos.",
              "score": 3,
              "created_utc": "2025-12-31 17:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3hp62",
                  "author": "DutchSEOnerd",
                  "text": "Thats also the feeling I have. Benchmarks only tell part of the story",
                  "score": 2,
                  "created_utc": "2026-01-01 16:12:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxs1nyo",
                  "author": "techlatest_net",
                  "text": "Yeah, exactly â€“ leaderboards donâ€™t really capture â€œI stopped hand-writing code in my main repos for a month straight.â€\nâ€‹\nThe $25/1M output tier basically turned Opus into a drop-in senior engineer for a lot of people: long refactors, multi-file edits, and architecture changes went from â€œpainful weekend projectâ€ to â€œone good prompt and a quick review pass.â€\nâ€‹\nCurious how you wired it in â€“ are you mostly using it through an AI IDE, custom scripts, or a homegrown agent flow?",
                  "score": 1,
                  "created_utc": "2026-01-05 07:08:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs1jtv",
              "author": "techlatest_net",
              "text": "Yeah, Opus was kind of the first â€œI can trust this to run my whole workflowâ€ model for a lot of people â€“ especially for tool-use and multi-step plans.\nâ€‹\nAnd agreed on Qwen for local: the 2.5/3 â€œThinkingâ€ and VL variants basically became the default self-hosted stack once people realized how far you could push them on a single GPU.",
              "score": 1,
              "created_utc": "2026-01-05 07:07:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwy6olb",
          "author": "ForsookComparison",
          "text": "Deepseek R1 probably had the biggest impact. Put the fear of God into Western companies that were operating as if they had a magical moat. Before that there was legislation being seriously considered to put a halt to A.I. progress for a few months to years.",
          "score": 7,
          "created_utc": "2025-12-31 17:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxs1sgc",
              "author": "techlatest_net",
              "text": "Yeah, R1 felt less like â€œjust another modelâ€ and more like a geopolitical event â€“ suddenly every frontier lab and regulator had to assume that frontierâ€‘level reasoning could appear from any region, not just the usual suspects.\nâ€‹\nThe funny part is how fast the narrative flipped: from â€œwe need to pause progressâ€ to â€œwe canâ€™t afford to pause while others sprint,â€ which probably did more to kill the idea of a global slowdown than any policy paper.\nâ€‹\nCurious if you actually used R1 in any real workflows, or if its impact for you was mostly the shockwave it sent through the ecosystem.",
              "score": 1,
              "created_utc": "2026-01-05 07:09:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxt7qu8",
                  "author": "ForsookComparison",
                  "text": "> Curious if you actually used R1 in any real workflows, or if its impact for you was mostly the shockwave it sent through the ecosystem.\n\nUsed it absolutely everywhere. The price, especially off peak hours, made Sonnet and ChatGPT look like jokes.",
                  "score": 1,
                  "created_utc": "2026-01-05 13:08:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvnxf6",
          "author": "AllTheCoins",
          "text": "I love Qwen3-30b as a go-to all around model",
          "score": 9,
          "created_utc": "2025-12-31 07:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwz4f3w",
              "author": "Count_Rugens_Finger",
              "text": "yeah definitely.  In this thread there is very clearly two worlds coming into contact here.  The people with purpose-made rigs and the people with consumer grade hardware",
              "score": 2,
              "created_utc": "2025-12-31 20:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxs1z46",
                  "author": "techlatest_net",
                  "text": "And yeah, this thread really shows the split between people on big dedicated rigs and people squeezing the most out of consumer GPUs.",
                  "score": 1,
                  "created_utc": "2026-01-05 07:11:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs1yl8",
              "author": "techlatest_net",
              "text": "Qwen3â€‘30B is kind of the sweet spot right now â€“ good enough at almost everything without feeling like a science project to run.",
              "score": 1,
              "created_utc": "2026-01-05 07:11:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwa6rb",
          "author": "InfiniteTrans69",
          "text": "Qwen3, Kimi K2, Minimax M2.",
          "score": 4,
          "created_utc": "2025-12-31 11:03:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3duag",
              "author": "QuinQuix",
              "text": "What's the respective use cases for you with these three models?",
              "score": 2,
              "created_utc": "2026-01-01 15:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3fqe7",
                  "author": "InfiniteTrans69",
                  "text": "Translation and vision: Qwen  \nResearch and AI Slides: Kimi  \nEverything else: Minimax",
                  "score": 2,
                  "created_utc": "2026-01-01 16:02:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwpmc5",
          "author": "jinnyjuice",
          "text": "Might be useful to categorise by memory size and task groups",
          "score": 4,
          "created_utc": "2025-12-31 13:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxhgvw",
          "author": "leonbollerup",
          "text": "Open Source: GPT-oss-20b/120b",
          "score": 3,
          "created_utc": "2025-12-31 15:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwyuia",
          "author": "RiskyBizz216",
          "text": "Qwen3-Next specifically the 80B and 480B\n\nAnd Z-Image Turbo",
          "score": 2,
          "created_utc": "2025-12-31 14:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxfizo",
          "author": "BuffMcBigHuge",
          "text": "Open: Qwen3-30B\nClosed: Gemini 3 Pro + Nano Banana\n\nOpus 4.5 is stellar, but those two above blew me away.",
          "score": 2,
          "created_utc": "2025-12-31 15:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmwi4",
          "author": "Karyo_Ten",
          "text": "LLMs: GLM-4.5-Air, GLM-4.7, GPT-OSS (quality for the first 2, speed demon for GPT-OSS)\n\nPromising new arch: Kimi-Linear, Qwen-Next, MiMo-V2-Flash I believe 2026 will have Linear Attention everywhere.\n\nSpecialized: Minimax-M2.1\n\nMultimodal: Qwen3-Omni, GLM-4.6V\n\nOCR: Mineru",
          "score": 2,
          "created_utc": "2025-12-31 16:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmu7yj",
          "author": "thedarkbobo",
          "text": "Qwen3, iQuest, Deepseek, OSS, gemma3, nemotron",
          "score": 2,
          "created_utc": "2026-01-04 15:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvpnzh",
          "author": "Lissanro",
          "text": "You missed to mention Kimi K2 0905 and Kimi K2 Thinking. These are the models I run the most on my PC (IQ4 and Q4_X quants respectively, using ik_llama.cpp).Â K2 Thinking is especially notable for its QAT INT4 release which maps nicely to Q4_X in the GGUF format, preserving the original quality.\n\nFor both, common expert tensors and 256K context cache at Q8 fit fully in 96GB VRAM, making them excellent for CPU+GPU inference.\n\n\nAs of DeepSeek V3.2, I did not get to try it due to lack of support in both ik_llama.cpp and llama.cpp. There is work in progress to add its architecture but not going to make it this year.",
          "score": 3,
          "created_utc": "2025-12-31 07:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwa88j",
              "author": "Tema_Art_7777",
              "text": "Lord! From unsloth:\n\nâ€œIt is recommended to have 247 GB of RAM to run the 1-bit Dynamic GGUF.\nTo run the model in full precision, you can use 'UD-Q4_K_XL', which requires 646 GB RAM.â€",
              "score": 1,
              "created_utc": "2025-12-31 11:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwislo",
                  "author": "Lissanro",
                  "text": "Even though Unsloth generally makes good quants, in this case it is the best to use Q4\\_X from [https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF) because it preserves the original INT4 quality in 543.617 GiB size, while UD-Q4\\_K\\_XL would be bigger, slower and likely of a bit lower quality.\n\nThat said, yes, your estimates of necessary RAM are accurate, for IQ1 256 GB RAM is needed and almost all of it will be used by the model, leaving very little for the OS; for IQ3 512 GB, and for Q4\\_X around 640 GB is needed if you include full cache size. K2 Thinking works best high RAM rigs or with high GPU count, like twenty MI50 cards (for 640 GB VRAM in total) - I actually considered getting that much since at the time I looked into it, it was possible for half the price of RTX PRO 6000 and my motherboard could carry them all at PCI-E 4.0 x4 speed each, but I decided to stay with 4x3090 for now (because even though full VRAM inference would be faster, my current performance with 1 TB RAM + 96 GB VRAM is still acceptable to me, and a lot of what I do requires Nvidia cards, and not just LLMs either).",
                  "score": 2,
                  "created_utc": "2025-12-31 12:17:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx0g87s",
          "author": "karmakaze1",
          "text": "Nemotron-3-Nano for its use of hybrid Mamba-Transformer that reduces memory and computation for large context.\n\nAlso Qwen3-Next (for \"Gated DeltaNet\") and Kimi K2 (for advancing MLA further).",
          "score": 1,
          "created_utc": "2026-01-01 01:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvqtnu",
          "author": "PeakBrave8235",
          "text": "At the end of the day they all are BS, open or closed source idgaf.",
          "score": -9,
          "created_utc": "2025-12-31 08:00:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxga7w",
              "author": "Count_Rugens_Finger",
              "text": "so why are you even here?",
              "score": 2,
              "created_utc": "2025-12-31 15:40:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1u966",
      "title": "I designed a Private local AI for Android - has internet search, personas and more.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/jk6meo3wmwag1",
      "author": "Sebulique",
      "created_utc": "2026-01-02 09:25:58",
      "score": 62,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx8c8zl",
          "author": "Themash360",
          "text": "Nice work, can it connect to local llm as well? So not just on device but custom endpoints",
          "score": 1,
          "created_utc": "2026-01-02 10:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8cp5h",
              "author": "Sebulique",
              "text": "You can, I've left it completely open, I also allowed it to work with smart bulbs, It works but sometimes it decides to do it's own thing unless I set parameters.\n\nI've turned it off after I broke it, but I want to reintroduce it again soon",
              "score": 4,
              "created_utc": "2026-01-02 10:18:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx982sn",
          "author": "theCatchiest20Too",
          "text": "Very cool! Are you tracking performance against hardware specs?",
          "score": 1,
          "created_utc": "2026-01-02 14:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9mbso",
              "author": "Sebulique",
              "text": "I was thinking of a toggle called \"stats for geeks\" but I wanted to make it look super easy for anyone, almost Chatgpt like, so more people feel comfortable using local llms",
              "score": 2,
              "created_utc": "2026-01-02 15:31:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx9j4l9",
          "author": "False-Ad-1437",
          "text": "\\> Very very happy, will upload soon on GitHub when I've ironed out any bugs I come across.\n\nI'm happy to submit PRs to fix said bugs",
          "score": 1,
          "created_utc": "2026-01-02 15:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa6go5",
              "author": "Sebulique",
              "text": "Thank you man, I appreciate you, I'm looking to make it easy and accessible for all",
              "score": 1,
              "created_utc": "2026-01-02 17:06:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdas2x",
          "author": "2QNTLN",
          "text": ">will upload soon on GitHub. \n\nHow soon is soon? Super interested in this project so i had to ask.",
          "score": 1,
          "created_utc": "2026-01-03 02:39:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe9nbn",
          "author": "tankandwb",
          "text": "RemindMe! 1 week",
          "score": 1,
          "created_utc": "2026-01-03 06:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe9qy4",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-01-10 06:34:58 UTC**](http://www.wolframalpha.com/input/?i=2026-01-10%2006:34:58%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/nxe9nbn/?context=3)\n\n[**4 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLM%2Fcomments%2F1q1u966%2Fi_designed_a_private_local_ai_for_android_has%2Fnxe9nbn%2F%5D%0A%0ARemindMe%21%202026-01-10%2006%3A34%3A58%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q1u966)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-03 06:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcrxs",
          "author": "Latter_Virus7510",
          "text": "Coolsieees! Amazing job there boss! When do we get to download?",
          "score": 1,
          "created_utc": "2026-01-03 18:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx461s",
              "author": "Sebulique",
              "text": "Now! Enjoy https://github.com/Rawxia/SebbiAI",
              "score": 2,
              "created_utc": "2026-01-06 00:33:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxyt4vf",
                  "author": "Latter_Virus7510",
                  "text": "First time launched and imported a Gemma 3 model, I tried to send a message but the model crashed instantly. It's been crashing since.",
                  "score": 1,
                  "created_utc": "2026-01-06 06:50:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhkf1u",
          "author": "Benschman1979",
          "text": "Great work! I'm looking forward to testing it.",
          "score": 1,
          "created_utc": "2026-01-03 19:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx42bu",
              "author": "Sebulique",
              "text": "It's out now! https://github.com/Rawxia/SebbiAI",
              "score": 1,
              "created_utc": "2026-01-06 00:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx443u",
          "author": "Sebulique",
          "text": "It's now released everyone! \nhttps://github.com/Rawxia/SebbiAI",
          "score": 1,
          "created_utc": "2026-01-06 00:33:32",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjnew",
      "title": "Tiiny Al just released a one-shot demo of their Pocket Lab running a 120B model locally.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "author": "Ajitabh04",
      "created_utc": "2025-12-29 11:09:13",
      "score": 49,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "Just came across this demo. They plugged their tiny AI computer into a 14-year-old PC and it output an average of 19 tokens/s on a 120B model.\nThey haven't released the MSRP yet. However, a large amount of DDR5 memory would be pricey, I'm guessing around $1500 MSRP for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjcqd4",
          "author": "loyalekoinu88",
          "text": "They had posted around $699 BUT that was before the memory announcement",
          "score": 12,
          "created_utc": "2025-12-29 12:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj8uo7",
          "author": "leonbollerup",
          "text": "No link ?",
          "score": 8,
          "created_utc": "2025-12-29 12:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwklly9",
          "author": "ForsookComparison",
          "text": "> guessing $1500 MSRP\n\n> they posted $699 pre crucial RAM announcement \n\n> 19 tokens/second on gpt-oss-120b\n\nI plugged a used Rx 6800 ($250 in my area with multiple options) into an older PC and got 18 tokens per second. I know this isn't the same and it suggests that you have a fair amount of RAM in your older PC, but given what we know about this I'm thoroughly \"meh\"d.\n\n**Edit** - just looked up the form factor. I'm less meh'd now. That would be fun to use if it ends up affordable.",
          "score": 4,
          "created_utc": "2025-12-29 16:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlgo5g",
              "author": "FaceDeer",
              "text": "[It's a portable unit the size of a phone](https://tiiny.ai/), since there aren't links anywhere else in this thread.\n\nNot a lot of detail even there, though. I don't see anything about whether it's battery powered - I'm assuming not, given OP mentions plugging it in to a computer.",
              "score": 6,
              "created_utc": "2025-12-29 19:18:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwutx5z",
                  "author": "ecoleee",
                  "text": "Youâ€™re absolutely right â€” this generation of Tiiny does not include a built-in battery.\n\nThat decision was intentional. Thermal management is a serious challenge at this performance level, and we didnâ€™t want to ship a device that becomes uncomfortably hot in real use.\n\nTo reliably support sustained local inference of models up to 120B parameters, we designed a custom thermal module specifically for Tiiny, prioritizing stability and safe operating temperatures over battery integration.\n\nAt the upcoming CES, weâ€™ll be sharing a detailed internal teardown video of Tiiny. Youâ€™ll be able to see exactly how the cooling system is built and why these design choices were made.\n\nWe believe itâ€™s better to be transparent about trade-offs and deliver a product that performs consistently, rather than chasing form factors at the expense of real-world usability.",
                  "score": 2,
                  "created_utc": "2025-12-31 03:48:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq0vao",
          "author": "Ok-Structure4057",
          "text": "Found the specs on their website:\n\npocket size: 14.2 Ã— 8 Ã— 2.53 cm\n\n80GB LPDDR5X RAM & 1TB SSD190 \n\ntotal TOPS between the SoC and dNPU\n\n30W TDP\n\nThey also released a demo of this device on Twitter. Imo it would be fun with retail prices around 1400 bucks.",
          "score": 3,
          "created_utc": "2025-12-30 12:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq5w0t",
              "author": "RangerOk4318",
              "text": "Agree. Memory price has been so absurd. I'm guessing the same price",
              "score": 1,
              "created_utc": "2025-12-30 13:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws3u2u",
                  "author": "QuinQuix",
                  "text": "Memory is fucking up any attempt at affordable home AI right now",
                  "score": 2,
                  "created_utc": "2025-12-30 19:10:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvbcwc",
              "author": "fallingdowndizzyvr",
              "text": "> mo it would be fun with retail prices around 1400 bucks.\n\nAt that price, why not just get a Strix Halo? That's just a PC so you can do regular PC stuff like gaming.",
              "score": 1,
              "created_utc": "2025-12-31 05:48:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0pjg",
          "author": "zelkovamoon",
          "text": "I'm not sure what having a *small* ai lab is trying to solve\n\nIf you're doing local AI my position is, make it bigger, cooler, and put more ram on it.\n\nThat said, it is *good* that companies are stepping in to try and build some solutions. If we could get something with 256GB of fast memory we might be able to go places.",
          "score": 3,
          "created_utc": "2025-12-30 21:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6f8d5",
          "author": "noless15k",
          "text": "[https://tiiny.ai/pages/tech-1](https://tiiny.ai/pages/tech-1)\n\nhttps://preview.redd.it/4b2qtqbx9uag1.png?width=2048&format=png&auto=webp&s=0538a8c9fc2286ecebe27731cc85925cc7819fbc",
          "score": 2,
          "created_utc": "2026-01-02 01:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlotf1",
          "author": "No-Consequence-1779",
          "text": "I think the lpddr5 is likely the memory. Â Itâ€™s slightly faster for this and itâ€™s wired so they can , like others, charge for memory size mostly.Â ",
          "score": 1,
          "created_utc": "2025-12-29 19:57:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrh229",
          "author": "Certain_Strength7069",
          "text": "I saw a price of 469USD. For that price, I would buy several for everything.Â ",
          "score": 1,
          "created_utc": "2026-01-05 04:36:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q31r48",
      "title": "Tony Starkâ€™s JARVIS wasnâ€™t just sci-fi his style of vibe coding is what modern AI development is starting to look like",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/hfvuxcznd6bg1",
      "author": "spillingsometea1",
      "created_utc": "2026-01-03 18:16:52",
      "score": 42,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q31r48/tony_starks_jarvis_wasnt_just_scifi_his_style_of/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxhaptp",
          "author": "sourceholder",
          "text": "I often think of this scene.\n\nA few years ago it seemed this type of interaction was at least a decade+ away.",
          "score": 1,
          "created_utc": "2026-01-03 18:20:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxvlh0",
      "title": "I have 50 ebooks and I want to turn them into a searchable AI database. What's the best tool?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "author": "Great_Jacket7559",
      "created_utc": "2025-12-28 16:33:33",
      "score": 41,
      "num_comments": 29,
      "upvote_ratio": 0.95,
      "text": "I want to ingest 50 ebooks into an LLM to create a project database.\nIs Google NotebookLM still the king for this, or should I be looking at Claude Projects or even building my own RAG system with LlamaIndex?\nI need high accuracy and the ability to reference specific parts of the books. I don't mind paying for a subscription if it works better than the free tools.\nAny recommendations?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwj05xl",
          "author": "Zucramj",
          "text": "I would build something custom.\n\nHere is how I see it:\n\n1) You own the data \n2) You own the AI (local embeddings work great for this task and you can run it with a local AI model or use Openrouter) \n3) I would build this with DSPy (modular and can be optimized with gepa) \n4) I would use PostgreSQL to store the data \n\nSo if you already have the ebooks as pdfs I would take those an set that up.\n\nHere is my primitive version of this I did some months back: \n\nhttps://github.com/marcusjihansson/dspy-mcp-tools/blob/main/regulatory.py\n\nIt is advanced if you don't know what you are reading but as I have gotten deeper into optimizing AI agents and systems does this feel primitive to me. \n\nThat newer research is going to be uploaded to my GitHub soon...\n\nSo: \nA) I would read through if this would solve your task! \nB) I would be happy to help you out if you have any questions!",
          "score": 9,
          "created_utc": "2025-12-29 11:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf0w7j",
          "author": "DHFranklin",
          "text": "NotebookLM and maybe some RAG and Custom Instructions for vectoring.\n\nNow if you wanna get real squirrely you could turn the entire compendium, and custom instruction into 1 million token prompt and sit it into Gemini as is. That might actually be more useful.\n\nThe trick is information loss and context bleed with the books. I could see a JSON Database made from it all as text also.\n\nIt comes down to what are you using it for and what information needs to stay consistent.",
          "score": 3,
          "created_utc": "2025-12-28 19:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe78fy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-28 17:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwerjho",
              "author": "blaidd31204",
              "text": "I am intrigued... what If:\n\n* pdf (Yes, there are images, but these should not influence info)?\n\n* markdown (No images)?",
              "score": 2,
              "created_utc": "2025-12-28 19:06:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh0m2z",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 02:05:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwgwh5t",
                  "author": "Investolas",
                  "text": "Those answers influence the reccomendation.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwim06f",
          "author": "RepLava",
          "text": "LightRAG with the MCP. Works great based on the relatively sparse info you've given",
          "score": 2,
          "created_utc": "2025-12-29 08:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlvovo",
          "author": "False-Ad-1437",
          "text": "AnythingLLM can make your own workspaces for this.",
          "score": 2,
          "created_utc": "2025-12-29 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpin9j",
          "author": "Cladser",
          "text": "Iâ€™m deep in a similar project atm. It depends a lot on the type of info you want from the LLM. If itâ€™s what does author x have to say about y - RAG is your best bet. However if you want to ask questions like across these books what are the most common ways of dealing with with Y - That is a corpus level (ie entire collection) query and RAG will suck. Llamaindex with hybrid search seems to be the best middle road at the moment.",
          "score": 3,
          "created_utc": "2025-12-30 10:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwft2zf",
          "author": "vidibuzz",
          "text": "Slightly off topic. You may want to use Illuminati.google.com also for voice summaries.",
          "score": 2,
          "created_utc": "2025-12-28 22:09:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhdx2b",
              "author": "Schizophreud",
              "text": "Didnâ€™t know about this. Thanks.",
              "score": 1,
              "created_utc": "2025-12-29 03:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwibx1i",
          "author": "Charming_Support726",
          "text": "Depending on what's in the books, you could have a look at IBMs Docling for conversion. I think every simple RAG Pipeline will do the trick in the beginning",
          "score": 1,
          "created_utc": "2025-12-29 07:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigiuv",
          "author": "maxz2040",
          "text": "Logically.app",
          "score": 1,
          "created_utc": "2025-12-29 08:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm4e9l",
          "author": "Empty-Poetry8197",
          "text": "Paperqa",
          "score": 1,
          "created_utc": "2025-12-29 21:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwno756",
          "author": "kchandank",
          "text": "Interesting, if you are able to achieve your objective, would you be able to share the steps?",
          "score": 1,
          "created_utc": "2025-12-30 02:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpe82p",
          "author": "Sea_Mouse655",
          "text": "Iâ€™ve been using PaperQA2 for some regulatory use cases and itâ€™s gangster",
          "score": 1,
          "created_utc": "2025-12-30 09:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsjglj",
          "author": "isleeppeople",
          "text": "Seems like you could use something like pypdf and langchain to embed it into your RAG. If the ebooks are like current info that can change or become stale you will want to tag them and set up some sort of workflow to compare them to a Gemini or open ai call to compare the info and if it becomes stale remove it. I use qdrant and postgresql for ground truth. I do stuff like this for versions of python that I have to use to stay compatible with other things I'm running. Or actually for langchain and langgraph too. They just changed compatibility versions so whenever I upgrade I can just use the newer repo in my rag with the updated information. I will still keep the old one until I am absolutely certain I won't revert but you can just leave it sit there and not reference the old one. Hope that makes sense.",
          "score": 1,
          "created_utc": "2025-12-30 20:25:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh0iik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-29 02:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwimlqu",
              "author": "Just_Bronze",
              "text": "I have a stupid question.\n\nNot entirely sure how these things work, but do I understand correctly you've got a system set up to take input and create a chatbot?  Or do you create a file/fileset that a chatbot incorporates to use the information.",
              "score": 3,
              "created_utc": "2025-12-29 09:05:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwisq5d",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 10:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjfv9q",
              "author": "loki626",
              "text": "Can I DM you? I have some pdfs that I would like to convert. They are more technical though. Medicine related.",
              "score": 1,
              "created_utc": "2025-12-29 13:13:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmga0",
          "author": "PaleontologistOk865",
          "text": "What about just throwing everything in a AI and letting it figure out what to do? That's what my clients keep saying to me. . .",
          "score": 0,
          "created_utc": "2025-12-30 23:38:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0fg09",
      "title": "I got my first ever whitepaper published",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/v23qicvcdy9g1.png",
      "author": "InsideResolve4517",
      "created_utc": "2025-12-31 15:21:16",
      "score": 40,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0fg09/i_got_my_first_ever_whitepaper_published/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwyduyb",
          "author": "Lame_Johnny",
          "text": "Amazing! congratulations!",
          "score": 1,
          "created_utc": "2025-12-31 18:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ein",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwynwee",
          "author": "Busy_Farmer_7549",
          "text": "Congratulations ðŸŽˆðŸŽŠ",
          "score": 1,
          "created_utc": "2025-12-31 19:17:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ed2",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwytdq1",
          "author": "PromptOutlaw",
          "text": "Congrats bud!! I really underestimated how much hard work goes into these. Iâ€™ve been humbled recently and Iâ€™m not even half way",
          "score": 1,
          "created_utc": "2025-12-31 19:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25e8e",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzsdsy",
      "title": "Suggest a model for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "author": "Bright_Dot113",
      "created_utc": "2025-12-30 20:10:25",
      "score": 38,
      "num_comments": 25,
      "upvote_ratio": 0.98,
      "text": "Hello, I have 9950x3d with 64GB RAM and 5070 ti \n\nI recently installed LM Studio, which models do you suggest based on my hardware for the following purposes. \n\n1. Code in python and rust \n\n2. DB related stuff like optimising queries or helping me understand them. (Postgresql)\n\n3. System and DB design.\n\nAlso what other things can I do?\nI have heard lot about MCP servers but I didn't find any MCP servers useful or anything related to my workflow if you have any suggestions that would be great! ",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwsjtel",
          "author": "SimplyRemainUnseen",
          "text": "I'd suggest unsloth/Nemotron-3-Nano-30B-A3B-GGUF. You'll need to offload to system memory, but you'll have a relatively fast and intelligent model that's good at using tools.\n\nFor MCP servers check out [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)",
          "score": 16,
          "created_utc": "2025-12-30 20:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo35r",
              "author": "Big-Masterpiece-9581",
              "text": "What kind of speeds to you think he could get with this model and that configuration? Anything particular you like about that model or unsloth?",
              "score": 3,
              "created_utc": "2025-12-31 03:12:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxynte",
                  "author": "SimplyRemainUnseen",
                  "text": "Speeds would be pretty fast I imagine. Faster than my work laptop with an iGPU (which runs it faster than I can read).\n\nThe model itself is fully open source (data too!) and made by NVIDIA. The performance of the model exceeds other models in the same parameter range of 20-32b and has very long context (up to a million tokens).\n\nNVIDIA worked with Unsloth for day 0 quants of the model. Unsloth makes efficient dynamic quants of models that retain very high levels of accuracy at lower precision. Considering OP will need to offload to system memory, GGUF is ideal as it's designed for that use case.",
                  "score": 5,
                  "created_utc": "2025-12-31 17:11:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1r491",
                  "author": "Hairy_Candy_3225",
                  "text": "I use a similar setup with 9950x3d 96 GB RAM and 5080 16 GB VRAM and use Nemotron 3 nano 30b Q6. Tokens per second heavily depends on context used. I've written a script to run a benchmark test and measure speed with all combinations of different context length / % GPU-offload layer / force experts on CPU on vs off.\n\nWith smaller context (i.e. 10k tokens) I get up to 16 TPS. This drops to around 10 @200k tokens. I'm no expert but I don't think there will be a big difference between 5070 or 5080 if VRAM is the same.\n\nWhat i can't understand is that with nemotron it does not matter how much layers I offload to GPU. The model is around 30 GB but only between 4 and 6 GB VRAM is used regardless of offload setting. It seems only the active layers are offloaded to GPU. This was different when I qwen 2.5, where I had to balance the exact number of layers that would fit in VRAM. As soon as you try to offload more layers than fit in VRAM, regular RAM will be used as shared VRAM and TPS drop drastically. \n\nProbably nothing new here for most redditors on this sub but i'm new to local LLM's and had to figure out a lot of things myself.",
                  "score": 2,
                  "created_utc": "2026-01-01 07:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsmq4k",
          "author": "beedunc",
          "text": "Qwen3coder, whatever fits.",
          "score": 9,
          "created_utc": "2025-12-30 20:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtu8zi",
          "author": "No-Consequence-1779",
          "text": "Qwen3-coder-30b. The largest quant you can run. Dense model is good or moe. Â Keep in mind coder specific models are specialized for coding. Many like oss 120b though that size is not necessary.Â ",
          "score": 6,
          "created_utc": "2025-12-31 00:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt434d",
          "author": "Toastti",
          "text": "If you add another 64gb of ram you can run a Q2 quant of Minimax m2.1. it will probably be 7tk/s or so but it is almost certainly the smartest agentic coding model you can reasonably run",
          "score": 3,
          "created_utc": "2025-12-30 22:03:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt53sn",
              "author": "Your_Friendly_Nerd",
              "text": "q2? isnâ€˜t that gonna suck?",
              "score": 3,
              "created_utc": "2025-12-30 22:08:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtv56k",
                  "author": "Worried_Goat_8604",
                  "text": "No unsloth dynamic quant v2 usually dosnt reduce that much quality",
                  "score": 1,
                  "created_utc": "2025-12-31 00:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwugr2q",
              "author": "Individual_Gur8573",
              "text": "I agree I found minimax 2.1 IQ2_M the smartest model after GLM 4.5 airÂ ",
              "score": 1,
              "created_utc": "2025-12-31 02:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwuo8ix",
                  "author": "Karyo_Ten",
                  "text": "GLM Air which quant?",
                  "score": 1,
                  "created_utc": "2025-12-31 03:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlq1uw",
              "author": "Wild_Requirement8902",
              "text": " you can run iq4xs with 128 gb ram v and get 7 tk/s with 128gb of ram (with an old xeon(2680v4) and quad channel ddr4 @ 2400,) using lm studio and a 5060ti & a 3060 12gb I can get up to 131072 context @ q8 with flash attention. That what i am using, but sometime it crash and you have to reload the model and prompt processing is painful (35tk/s)",
              "score": 1,
              "created_utc": "2026-01-04 10:15:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwviewu",
          "author": "Count_Rugens_Finger",
          "text": "Qwen3-coder-30B-A3B, Nemotron-3-Nano-30B-A3B, Devstral-Small-2-24B",
          "score": 3,
          "created_utc": "2025-12-31 06:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsooqq",
          "author": "FullstackSensei",
          "text": "I'd say try a bunch of MoE all the way up to gpt-oss-120b and see where your pain threshold is for speed. IMO, you should keep a few at hand: a smaller model like Qwen3 Coder 30B, Nemotron 30B, gpt-oss-20b, Devstral 2 24B as daily drives, and larger models like gpt-oss-120b, GLM 4.5 air, Devstral 2 123B (Q4), etc for when the smaller models get stuck or can't solve whatever issue you have.",
          "score": 4,
          "created_utc": "2025-12-30 20:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt5rhp",
          "author": "Level_Wolverine_141",
          "text": "I have the same system as you except I've got a 5080, and I'm just using Claude code max and it's pretty good.",
          "score": -4,
          "created_utc": "2025-12-30 22:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww8pra",
              "author": "g33khub",
              "text": "your 5080 doesn't matter, I can use Claude code on a raspberry pi",
              "score": 2,
              "created_utc": "2025-12-31 10:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyw1b4",
      "title": "I built Plano(A3B) - fast open source LLM for agent orchestration that beats frontier LLMs",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5rp16cxd57ag1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-29 19:44:28",
      "score": 38,
      "num_comments": 14,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyw1b4/i_built_planoa3b_fast_open_source_llm_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwoqem5",
          "author": "ThsYWeCntHveNiceTngs",
          "text": "your research page is more advert than research and the blog provides more detail, but not much. Is there an Arxiv link or anything to read about your method and how you generated the proposed results?",
          "score": 6,
          "created_utc": "2025-12-30 06:07:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwosnmj",
              "author": "AdditionalWeb107",
              "text": "The huggingface models page has more details. Although we are in the process of publishing the arxiv paper",
              "score": 0,
              "created_utc": "2025-12-30 06:25:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxc7j4",
          "author": "False-Ad-1437",
          "text": "This is not open source. \n\nIt imposes non-open requirements (attribution, use restrictions, redistribution constraints and separate commercial licensing for certain uses) that violate the open-source criteria defined by the OSI/FSF.",
          "score": 2,
          "created_utc": "2025-12-31 15:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufm16",
          "author": "Purple-Programmer-7",
          "text": "How does this differ from Arch that youâ€™ve previously pushed for the past year?",
          "score": 1,
          "created_utc": "2025-12-31 02:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui3lf",
              "author": "AdditionalWeb107",
              "text": "Arch was about model routing. Plano is about orchestration, which is a slightly more complicated set of tasks. Plano is the next major upgrade to Arch with several new capabilities for agentic applications like filter chains, agent signals, and even more robust model gateway.\n\n\nBy the way, people were confusing arch with arch Linux so we thought it was a better time to rename the project.\nTry Plano ðŸ™",
              "score": 1,
              "created_utc": "2025-12-31 02:37:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyaxs3",
          "author": "AdditionalWeb107",
          "text": "Thatâ€™s fair - it should say open weights. And the license is very permissive except for one deployment type",
          "score": 1,
          "created_utc": "2025-12-31 18:12:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwopty3",
          "author": "maigpy",
          "text": "what is model-native?",
          "score": 1,
          "created_utc": "2025-12-30 06:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqe6c",
              "author": "AdditionalWeb107",
              "text": "Itâ€™s integrated with small LLMs - central to how the project is built.",
              "score": -1,
              "created_utc": "2025-12-30 06:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6v7x",
                  "author": "maigpy",
                  "text": "integrated with small llms translates to \"model-native\"?\n\nI don't quite understand, if you could use more words to describe what's going that would help.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnderh",
          "author": "Lyuseefur",
          "text": "Planning already to use it in next build of nexora follow us\n\nHttps://github.com/jeffersonwarrior/nexora",
          "score": 0,
          "created_utc": "2025-12-30 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnwumq",
              "author": "AdditionalWeb107",
              "text": "Okay - thanks. Would love the feedback. And if you like our project, don't forget to star it too",
              "score": 1,
              "created_utc": "2025-12-30 02:56:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3asxm",
      "title": "Which tools should I be looking at? Want to use local AI to tailor my resume to job descriptions",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "author": "cspybbq",
      "created_utc": "2026-01-04 00:16:54",
      "score": 34,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "I'm job hunting and trying to learn more about AI at the same time. \n\nI want the AI to be aware of all my resume versions (15ish versions) and to tailor new versions of my resume based on the contents of those resumes, plus job descriptions I give it. I'd also like it to evaluate a job description and tell if I'm a good fit or not, based on my resumes. \n\nIs this something I can set up on my local computer? \n\n * AMD Ryzen 5700G\n * Nvidia 3070\n * 64G RAM\n * Running Debian\n\nThere are so many models and variants of models that I'm not really sure where to start. I have played a bit with ollama (cli) and open-webui but haven't really figured out how to set up RAG correctly to handle my documents or get any sort of professional level output.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxk0cgf",
          "author": "jinnyjuice",
          "text": "Might want to also post to /r/localllama -- I would be interested in the responses also on how people judge different models to be strong in which dimensions!",
          "score": 3,
          "created_utc": "2026-01-04 02:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjgmey",
          "author": "fandry96",
          "text": "Go to gemini. Make a gem. Add your 10 best resumes to assets. \n\nYou can try defaulting to agent mode to try it. If not click save. \n\nPost the jobs there and tell Gemini to write a resume...",
          "score": 2,
          "created_utc": "2026-01-04 00:46:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3os4p",
      "title": "Tencent HY-MT1.5, a specialized machine-translation model",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/rq2666gzsbbg1.png",
      "author": "etherd0t",
      "created_utc": "2026-01-04 12:29:17",
      "score": 31,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3os4p/tencent_hymt15_a_specialized_machinetranslation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxskc30",
          "author": "Lost-Dot-9916",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-05 10:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzg9wj",
          "author": "Naive-Pear-9579",
          "text": "Interesting. Is this also suitable for localization (L10N)?",
          "score": 1,
          "created_utc": "2026-01-06 10:25:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxzjhp",
      "title": "Google Open-Sources A2UI: Agent-to-User Interface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 19:08:03",
      "score": 27,
      "num_comments": 8,
      "upvote_ratio": 0.87,
      "text": "Google just released **A2UI (Agent-to-User Interface)** â€” an open-source standard that lets AI agents generate **safe, rich, updateable UIs** instead of just text blobs.\n\nðŸ‘‰ Repo: [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\n# What is A2UI?\n\nA2UI lets agents â€œ**speak UI**â€ using a **declarative JSON format**.  \nInstead of returning raw HTML or executable code (âš ï¸ risky), agents describe *intent*, and the client renders it using **trusted native components** (React, Flutter, Web Components, etc.).\n\nThink:  \nLLM-generated UIs that are **as safe as data, but as expressive as code**.\n\n# Why this matters\n\nAgents today are great at text and code, but terrible at:\n\n* Interactive forms\n* Dashboards\n* Step-by-step workflows\n* Cross-platform UI rendering\n\nA2UI fixes this by cleanly separating:\n\n* **UI generation (agent)**\n* **UI execution (client renderer)**\n\n# Core ideas\n\n* ðŸ” **Security-first**: No arbitrary code execution â€” only pre-approved UI components\n* ðŸ” **Incremental updates**: Flat component lists make it easy for LLMs to update UI progressively\n* ðŸŒ **Framework-agnostic**: Same JSON â†’ Web, Flutter, React (coming), SwiftUI (planned)\n* ðŸ§© **Extensible**: Custom components via a registry + smart wrappers (even sandboxed iframes)\n\n# Real use cases\n\n* Dynamic forms generated during a conversation\n* Remote sub-agents returning UIs to a main chat\n* Enterprise approval dashboards built on the fly\n* Agent-driven workflows instead of static frontends\n\n# Current status\n\n* ðŸ§ª **v0.8 â€“ Early Public Preview**\n* Spec & implementations are evolving\n* Web + Flutter supported today\n* React, SwiftUI, Jetpack Compose planned\n\n# Try it\n\nThereâ€™s a **Restaurant Finder demo** showing end-to-end agent â†’ UI rendering, plus Lit and Flutter renderers.\n\nðŸ‘‰ [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\nThis feels like a big step toward **agent-native UX**, not just chat bubbles everywhere. Curious what the community thinks â€” is this the missing layer for real agent apps?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwgkvue",
          "author": "bananahead",
          "text": "I am begging people to stop with the LLM written posts. Just post whatever prompt you used! Thatâ€™s the post!",
          "score": 13,
          "created_utc": "2025-12-29 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi048d",
              "author": "leonbollerup",
              "text": "Why care? I rather want a LLM generated post than some bad version of English - not everyone speaks English native",
              "score": 4,
              "created_utc": "2025-12-29 05:47:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjksfb",
                  "author": "ak_sys",
                  "text": "Id rather tailor my discussion to a non English speaker than not understanding that there may be a disconnect getting lost in translation. \n\nThis explains a lot of \"reading comprehension\" issues I've noticed from commenters, where they seem to be responding to some diffuse sentiment of the post rather than  the actual nuanced point and position. Languages almost never just directly translate into one another, and inserting an llm in the middle of a discussion without informing the other party seems pretty dishonest and disrespectful.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:44:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwmr4i2",
                  "author": "bananahead",
                  "text": "So write a post in your native language and have it translated. Thatâ€™s just as easy as whatever prompt created this and would be easier to read and more authentic.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgo2bh",
              "author": "Kamal965",
              "text": "The absolute state of Reddit now:\n\n* A lazy OP asks an LLM to create a post for them.\n* Users see LLM-isms and either:\n   * Ctrl + W\n   * Pass it on to an LLM to summarize it for them instead.\n* Another OP gives up on writing their own posts and starts using an LLM because they keep running into LLM-generated posts.\n\nRinse and repeat.",
              "score": 0,
              "created_utc": "2025-12-29 00:53:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nworg5v",
          "author": "ZITNALTA",
          "text": "Just curious if any knows if this somehow works with Google Antigravity IDE? By the way, I am NOT a dev so if this sounds like a newbie question that is why.",
          "score": 1,
          "created_utc": "2025-12-30 06:15:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpeh7",
      "title": "Device to run a local LLM mainly for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "author": "knibroc",
      "created_utc": "2025-12-28 11:40:21",
      "score": 21,
      "num_comments": 31,
      "upvote_ratio": 0.9,
      "text": "Hi mates,\n\nI mostly use ChatGPT and Mistral (through their \"vibe coding\" cli tool and API). I don't pay for these services, so I only use the lesser-capable models.\n\nMy laptop is not powerful enough to run this (no GPU / I've experimented with ollama but I can only run the smallest models very slowly so this is not ok for daily use), so I'm currently considering building a device dedicated to running a LLM, mainly for coding purposes. Ideally something small, Raspberry Pi-based or similar would be great.\n\nI have a few questions: is there specialized hardware for this (I've heard of TPU/NPU)? What kind of performance can I expect (I'd need at least GPT4/Devstral level)? I'm also worried about speed (tokens/s) and cost.\n\n  \nAny advice is appreciated!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdmlhq",
          "author": "KrugerDunn",
          "text": "The cost of any device that can run a decent coding model will far out scale just paying for Claude Code and still wonâ€™t be nearly as good or future proof.\n\nI went down this rabbit hole so you donâ€™t have to ðŸ˜‚",
          "score": 39,
          "created_utc": "2025-12-28 15:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdstym",
              "author": "skabaru",
              "text": "2nd. $100/month to Claude code is the best money you can spend here.... And I do have an older gaming rig running local llms... And it isn't even close.",
              "score": 12,
              "created_utc": "2025-12-28 16:19:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwekb8b",
                  "author": "New_Jaguar_9104",
                  "text": "I have an entire cluster and still pay for Claude max. It's worth its weight in gold IMO",
                  "score": 7,
                  "created_utc": "2025-12-28 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwffg7z",
              "author": "ThatOneGuy4321",
              "text": "whatâ€™s your threshold for â€œdecentâ€? 70B?",
              "score": 1,
              "created_utc": "2025-12-28 21:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgn3r5",
                  "author": "KrugerDunn",
                  "text": "I mean, depends what you're trying to do. I personally just love using Claude Code and occasionally Gemini Cli, so nothing is going to compare to those that can be run locally. Maybe could get away with a GLM4.6-AIR which is 357B params. Unless you're working on super duper secret proprietary code or something that violates the foundation model guardrails I just don't see any reason to use local.\n\nI know some people have used something like Qwen Code 32B or Devstral 24B and been satisfied with it, but never been worth it to me.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdph75",
          "author": "TyphoonGZ",
          "text": "If you're not satisfied with 1-5 toks/s on CPU (\"coffee break\" workflow), sounds like you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nWhy 30B? 20--30B is the size range where the model is still (sort of) considered \"small\" yet it starts being actually useful.\n\nThat said, you should consider using Openrouter and spend $5 for credits to test said models and see if they're good enough for you. You wouldn't want to buy a ~$1000 GPU just to get annoyed that your model's too braindead, right?\n\nRegarding TPU/NPU, I haven't heard if NPUs finally have the necessary software infrastructure to be useful. Well, they wouldn't really help LLMs if there's no development in memory bandwidth to go with them.\n\nOn the other hand, Google sells Coral TPUs, but those are for computer vision, not LLMs, and anyway, they only have *megabytes* of memory.",
          "score": 8,
          "created_utc": "2025-12-28 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweoi73",
              "author": "Count_Rugens_Finger",
              "text": ">  you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nI can run Qwen3-coder-30B-A3B at 12 tok/sec on my 8GB 3070 and an absolute potato of a CPU",
              "score": 2,
              "created_utc": "2025-12-28 18:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgwndy",
                  "author": "TyphoonGZ",
                  "text": "Oh yeah, I forgot MoE models exist...\n\nAlso damn, that's janky. Nice.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj61mb",
              "author": "nasone32",
              "text": "today with 24gb vram you can run qwen3 coder at 150 tokens/s with 100k+ context, or something like qwen3 next 80B at 15/20 tokens/s offloading some layers.",
              "score": 1,
              "created_utc": "2025-12-29 12:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcvibf",
          "author": "AnxietyPrudent1425",
          "text": "If you have a desktop with a GPU or Mac Mini/Studio you can setup Tailscale and basically have your own cloud endpoint. My setup is a MacBook Air + 128GB Mac Studio + Linux workstation and I couldnâ€™t be happier.",
          "score": 5,
          "created_utc": "2025-12-28 13:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcygiq",
              "author": "knibroc",
              "text": "if only I could afford a 128GB Mac Studio! these machines look great",
              "score": 4,
              "created_utc": "2025-12-28 13:24:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdccz6",
          "author": "Background_Gene_3128",
          "text": "Atm. Iâ€™ve dedicated a small server in the addict to run my llms, with a i5-12600k, 32gb ddr5 and a 3060 with 12gb vram. \nRunning proxmox with a ubuntu vm and ollama. \n8-14b models np, 14-24 okay, and the 30-34b models a bit too slow for my liking. \nSo Iâ€™ve upgraded to 96gb ram (not sure if that actually matter, but Iâ€™ve seen people get decent speeds with gpt-oss 120b with ram as offload, and found a used offer locally that didnâ€™t require a kidney) \nAnd 2 5060 Tiâ€™s as theyâ€™re on sale now here in Europe for â‚¬370 a piece. \n\nNot sure if this is the best budget setup or small, but itâ€™s what Iâ€™m rocking until thatâ€™s not enough.\nItâ€™s fitted in a Jonsbo D32 pro mesh case.",
          "score": 4,
          "created_utc": "2025-12-28 14:52:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjuvo",
          "author": "BigYoSpeck",
          "text": "A used desktop/gaming PC or server is probably the most cost effective way in\n\n\nSomething with 64gb of either DDR4 or 5, a 6+ core CPU, and a 16gb or more GPU\n\n\nI recently purchased a Ryzen 9 5900X 64gb DDR4-3600 with a Radeon RX 6800 XT from eBay and I can run gpt-oss-120b with full context at just over 20 tok/s\n\n\nIn the near future I'd like to swap out the Radeon for an RTX 3090 but ROCm is fairly good these days in llama.cpp \n\n\nEfficiency and performance won't match a Mac with comparable memory but it's a fraction of the price and doubles up for gaming duties",
          "score": 3,
          "created_utc": "2025-12-28 15:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf81pw",
              "author": "andriizahorui",
              "text": "Hey, I have a similar config and struggle to run gpt 120b with llama vulkan at decent speeds. Could you please share how do you run it with full context at 20 tps? Like exact command and stuff please",
              "score": 2,
              "created_utc": "2025-12-28 20:26:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfjs4k",
                  "author": "BigYoSpeck",
                  "text": "I'm not at my computer at the moment to give the exact parameters I pass to llama-server but I know the key ones are:\n\n   --threads 8 (5-8 are all very close, after 8 performance declines)\n\n   --flash-attn on\n\n   --mlock --no-mmap\n\n   --n-gpu-layers 99\n\n   --n-cpu-moe 32 (going down to 28 at lower context is faster, 32 is the sweet spot for having space left though) \n\n\nThis is with a self compiled ROCm build, pre built Vulkan docker isn't quite as fast",
                  "score": 5,
                  "created_utc": "2025-12-28 21:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcq3d3",
          "author": "DenizOkcu",
          "text": "Buying a used MacBook with an Apple Silicon might be your best/cheapest bet. They leverage unified memory and use mlx as a native LLM engine. I use devstral small 2 on a M3 with 36GB RAM in LM Studio. Nvidias Nemotron 3 nano is even faster with great coding results incl tool usage.",
          "score": 3,
          "created_utc": "2025-12-28 12:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcyl0f",
              "author": "knibroc",
              "text": "indeed Nemotron 3 sounds cool, will check, thanks!",
              "score": 2,
              "created_utc": "2025-12-28 13:25:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdgs27",
                  "author": "DenizOkcu",
                  "text": "It works really well with Nanocoder an open source Coding Tool with a focus on privacy/local LLMs (disclaimer: I am one of the contributors)",
                  "score": 3,
                  "created_utc": "2025-12-28 15:17:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwghwj2",
          "author": "machaao",
          "text": "In our tests, we couldn't get any of the single current open source code LLM to work at workable speed that is 25 tokens per second or so for a medium size code base.\n\nAs soon as you want something to be demanding it kinda barfs and token / second goes down the drain \n\nWould love to hear success stories tho ðŸ˜Œ\n\nP.S. Tried with gpt oss and qwen3 on M4 - 128G",
          "score": 2,
          "created_utc": "2025-12-29 00:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkp2kz",
          "author": "HiddenPingouin",
          "text": "The closest to claude code would be GLM4.7. You could run the q8 on a Mac Studio with 512GB of RAM",
          "score": 2,
          "created_utc": "2025-12-29 17:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnt1op",
              "author": "teleolurian",
              "text": "Pretty well, I would add.",
              "score": 1,
              "created_utc": "2025-12-30 02:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcml86",
          "author": "EternalVision",
          "text": "TPU is not really possible, only Google has those (developed them themselves). And your question really depends on your budget, the sky really is the limit here. An Ryzen Strix Halo 395 AI MAX+ based minipc is what could work out, but again, really depends on your budget.",
          "score": 2,
          "created_utc": "2025-12-28 11:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwisuw6",
          "author": "HealthyCommunicat",
          "text": "If u want a portable, the m4 max 128 gb is gunna be the best ur gunn get, if u dont have the money for that, the z13 flow. Load gpt oss 120b at high reasoning, qwen 3 next 80b, go checkout my post i did about testing all these models - keep in mind the z13 flow is half the cost so the token/s will be literally half.",
          "score": 1,
          "created_utc": "2025-12-29 10:04:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2h9o6",
          "author": "xcr11111",
          "text": "I came down to two viable options for that, but it don't think that any one of them can totally compete with the Claude code. Cheapest option I went for was an used m1 pro max 64gb. Bought one like new for 1200 bucks. It's amazing hardware, but tbh I don't get used to MacOS. Installed omarchy bit the metal drivers a not fast here(half the speed in llms). The other cool option ist an framework Desktop PC with 128 gig. It's an absolute beast for everything, but very expensive and prices will increase very soon because of ram.",
          "score": 1,
          "created_utc": "2026-01-01 12:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf1i4b",
          "author": "Jarr11",
          "text": "Don't do it! It will cost you more to run it yourself than it would to just pay for a subsciption to Claude/ChatGPT/Gemini for CLI access",
          "score": 1,
          "created_utc": "2025-12-28 19:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf6esp",
          "author": "Oki667",
          "text": "Lol, just pay for Claude code monthly subscription.",
          "score": 1,
          "created_utc": "2025-12-28 20:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiurfb",
          "author": "Crazyfucker73",
          "text": "You aren't going to get GPT4 level on a fucking raspberry pi mate.",
          "score": -1,
          "created_utc": "2025-12-29 10:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjammv",
              "author": "knibroc",
              "text": "And you are being fucking helpful mate",
              "score": 5,
              "created_utc": "2025-12-29 12:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2opaa",
      "title": "Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1q2onpg",
      "author": "atif_dev",
      "created_utc": "2026-01-03 07:44:51",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2opaa/built_a_fully_local_ai_assistant_with_longterm/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q377q3",
      "title": "Ollama + chatbox app + gpt oss 20b = chat gpt at home",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "author": "Birdinhandandbush",
      "created_utc": "2026-01-03 21:48:32",
      "score": 21,
      "num_comments": 25,
      "upvote_ratio": 0.86,
      "text": "My workstation is in my home office, with ollama and the LLM models. It's an i7 32gb and a 5060ti. \nAround the house on my phone and android tablet I have the chatbox AI app. \nI've got the IP address for the workstation added into the ollama provider details and the results are pretty great.\nCustom assistants and agents in chatbox all powered by local AI within my home network. \nReally amazed at the quality of the experience and hats off to the developers. Unbelievably easy to set up. ",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxlgq4t",
          "author": "eliadwe",
          "text": "I have home server with unraid and RTX3060 12gb running a Win11 vm with ollama, Iâ€™m using the abliterated version of gpt-oss-20b which gives me much better performance. Using it with LLAMAZING app on my iPhone combined with tailscale. Also using embeddinggemna for the RAG inside the app.\n\n\nThe model Iâ€™m using:\nhttps://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-mxfp4-abliterated-v2",
          "score": 5,
          "created_utc": "2026-01-04 08:51:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlsppp",
              "author": "Birdinhandandbush",
              "text": "What's the difference with abliterated versions",
              "score": 1,
              "created_utc": "2026-01-04 10:39:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlu5es",
                  "author": "eliadwe",
                  "text": "I get 28t/s with the abliterated version vs much lower performance with the regular version",
                  "score": 2,
                  "created_utc": "2026-01-04 10:52:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxntdcj",
              "author": "cuberhino",
              "text": "how does this work for you? are you happy with the results compared to say chatgpt? this looks like exactly what im looking to do",
              "score": 1,
              "created_utc": "2026-01-04 17:49:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxs2eyt",
                  "author": "Rude_Marzipan6107",
                  "text": "I think it was just a micro ad for llamazing. 20 dollar iOS app",
                  "score": 1,
                  "created_utc": "2026-01-05 07:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxj6wfg",
          "author": "-Akos-",
          "text": "Personally I like LM Studio better, it gives me better performance, and Granite 4 is an excellent model for me. On my potato laptop with a 1050 and 4GB VRAM and 16GB normal ram on an 8th gen I7, I still am able to do 50000 tokens, and it runs at about 14 tokens per second. Another advantage is that I can enable MCP in LM Studio, and Iâ€™ve installed a websearch MCP, so even though itâ€™s a small model, I can search for fresh information.",
          "score": 4,
          "created_utc": "2026-01-03 23:55:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq4am6",
              "author": "Look_0ver_There",
              "text": "Upvote on using LM-Studio.  I'm running LM-Studio myself on my 7900XTX.  With the gpt-oss-20b model it's running at >166tok/sec.  Ollama and GPT4All both run WAY slower.",
              "score": 2,
              "created_utc": "2026-01-05 00:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjhqf7",
              "author": "cuberhino",
              "text": "Iâ€™ve been messing with lm studio on my pc. How can I use it on the pc from my phone?",
              "score": 1,
              "created_utc": "2026-01-04 00:52:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxrc5lb",
                  "author": "turquoiserabbit",
                  "text": "In addition to what the other commenter said about Open WebUI, I use an app called \"LM Studio Assistant\" on Android since there isn't as much setup involved. You just put in your desktop LM studio address in the settings and you are good to go. It will only be accessible from your local network though. Unless you want to use port forwarding on your network to send traffic to LM Studio from anyone on the internet - not as secure, but still possible.",
                  "score": 1,
                  "created_utc": "2026-01-05 04:08:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxlj43t",
                  "author": "vertical_computer",
                  "text": "LM Studio exposes a remote API via the developer tab. Itâ€™s an â€œOpenAPI-compatibleâ€ endpoint, so you can use it remotely with any number of frontends that support it.\n\nBy far the most common/popular choice _(and what I use personally)_ is [Open WebUI](https://github.com/open-webui/open-webui) which has a ChatGPT-esque look to it. Itâ€™s a lightweight webserver that you can run easily as a docker container.\n\nOnce set up, you can configure Open WebUI to connect to any remote API, such as Anthropic or ChatGPT. In this case youâ€™d point it at your locally hosted LM Studio API.\n\nSince itâ€™s web based, you can use it directly on your phone via web browser, so long as youâ€™re on the same network as your PC.\n\n_Additionally, you could try out [Conduit](https://github.com/cogwheel0/conduit), which is a native mobile app for accessing Open WebUI, and is available on the App Store/Play Store. Youâ€™d still need to set up Open WebUI, but IMO the Conduit interface is a bit nicer on my phone than trying to use Open WebUI in a mobile browser. This is very optional though, you can just use it in a browser._",
                  "score": 1,
                  "created_utc": "2026-01-04 09:12:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxijai8",
          "author": "fandry96",
          "text": "Look up gemma.",
          "score": 3,
          "created_utc": "2026-01-03 21:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxijgd0",
              "author": "Birdinhandandbush",
              "text": "The Gemma 3 models or something else?",
              "score": 1,
              "created_utc": "2026-01-03 21:55:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnrdai",
                  "author": "jesus359_",
                  "text": "Gemma3 models. 12B and 27B are really up there. \n\nI like OSS but its waaay more agentic than just a regular model.",
                  "score": 2,
                  "created_utc": "2026-01-04 17:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxik00t",
                  "author": "fandry96",
                  "text": "You are thinking Gemini? 3 pro or flash?\n\n\nGemma is a local LLM that breaks down into dimensions. Think 4 AIs in one. I use 4n and 2n I think, locally.",
                  "score": 0,
                  "created_utc": "2026-01-03 21:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzkmsq",
          "author": "nntb",
          "text": "Or lm studio? And gptoss",
          "score": 1,
          "created_utc": "2026-01-06 11:04:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2gtfg",
      "title": "How big is the advantage of CUDA for training/inference over other branded GPUs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "author": "Massive-Scratch693",
      "created_utc": "2026-01-03 01:19:25",
      "score": 18,
      "num_comments": 15,
      "upvote_ratio": 0.91,
      "text": "I am uneducated in this area but want to learn more. I have been considering getting a rig to mess around with Local LLM more and am looking at GPUs to buy. It would seem that AMD GPUs are priced better than NVIDIA GPUs (and I was even considering some Chinese GPUs). \n\nAs I am reading around, it sounds like NVIDIA has the advantage of CUDA, but I'm not quite sure what this really is and why it is an advantage. For example, can't AMD simply make their chips compatible with CUDA? Or can't they make it so that their chips are also efficient running PyTorch?\n\n\n\nAgain, I'm pretty much a novice in this space, so some of the words I am using I don't even really know what they are and how they relate to others. Is there an ELI5 on this? Like...the RTX 3090 is a GPU (hardware chip). Is CUDA like the firmware that allows the OS to use the GPU to do calculations? And is it that most LLM tools written with CUDA API calls in mind but not AMD's equivalent firmware API calls? Is that what makes it such that AMD is less efficient or poorly supported with LLM applications?\n\n\n\nSorry if the question doesn't make much sense... ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxd024v",
          "author": "Own_Attention_3392",
          "text": "CUDA is proprietary; Nvidia controls the design and implementation and hardware details. It's basically a programming language that compiles to a format that only works on Nvidia hardware. \n\nROCm is the open source (non-proprietary) AMD equivalent. Most tools support both, it's just that CUDA is the \"standard\" and just about everything is guaranteed to support it.",
          "score": 17,
          "created_utc": "2026-01-03 01:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxd3jl3",
              "author": "Massive-Scratch693",
              "text": "Thanks for your response! So if I used any nonNVIDIA GPU chip (AMD, Huawei Ascend, Biren), it is likely to support ROCm?\n\n  \nIf I wanted to run models like Stable Diffusion, DeepSeek, and most other popular models, you would anticipate not only support for ROCm, but also similar performance relative to CUDA?",
              "score": 3,
              "created_utc": "2026-01-03 01:56:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxd8lqs",
                  "author": "Own_Attention_3392",
                  "text": "AMD will definitely support ROCm. Can't speak to the others; I buy Nvidia cards generally. \n\nIt probably won't be top tier performance compared to CUDA -- you'll want to look at benchmarks to make sure what you're buying will fit your needs. I assume you're looking at a deepseek distillation because the full deepseek model is huge and requires a server class GPU with tons of VRAM. Be realistic about what you'll be able to run -- the best local models will still be out of reach for even high end consumer hardware. \n\nAnything with 16+ GB of VRAM will be fine for all the current image generation models as far as I know. I'm nuts and have a 5090 so I don't need to worry about it too much.",
                  "score": 3,
                  "created_utc": "2026-01-03 02:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxd80wt",
          "author": "Count_Rugens_Finger",
          "text": "llama.cpp also supports Vulkan, which is not proprietary.",
          "score": 8,
          "created_utc": "2026-01-03 02:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd8irg",
          "author": "Shep_Alderson",
          "text": "What sort of budget are you aiming for? CUDA is the standard for most LLM tools, but support for other options is growing.",
          "score": 2,
          "created_utc": "2026-01-03 02:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdhyet",
              "author": "Massive-Scratch693",
              "text": "I'm trying to figure out a good budget right now and I think part of that is figuring out whether I am spending money on brand rather than performance. I am somewhat skeptical as to whether the performance per dollar is really best with NVIDIA or if I am better off paying for other GPUs",
              "score": 1,
              "created_utc": "2026-01-03 03:21:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe4i5r",
                  "author": "Geeotine",
                  "text": "You're definitely paying brand tax on both Nvidia and to a smaller extent, AMD. Nvidia does have a performance advantage as well, but not really an order of magnitude greater (<10x). Nvidia has invested billions of dollars and over 15 years into the CUDA platform, AMD is has made great strides trying to catch up on the last 3 years with ROCm, so don't expect it to be as polished as cuda. At the same time, there isn't any real alternatives between those two. \n\nEveryone else is either years behind, or unobtainable (Google/AWS asic AI chips). Also, even though ROCm is open source, it's up to the hardware developers to design hardware/firmware/drivers to support it, so for now only AMD products support ROCm architecture.\n\nYour biggest deciding factors are your budgets/requirements for power, RAM (both VRAM & system memory), and time.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:53:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxe0pma",
          "author": "CooperDK",
          "text": "AMD is a bad choice, it has far worse capabilities and things often fail to work due to rocm emulating cuda.",
          "score": 3,
          "created_utc": "2026-01-03 05:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdmjq9",
          "author": "arousedsquirel",
          "text": "Use a free tiers acces to gpt or gemini and ask this question. It will explain? 16gb is not adequate,  not yet. Do some research.",
          "score": 1,
          "created_utc": "2026-01-03 03:50:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe1n6y",
          "author": "No-Consequence-1779",
          "text": "For inference, cuda has a huge advantage for working with large contexts. Â Iâ€™m not sure of your budget but an older model is better than a new and. Â The M5 should have almost equivalent context processing and 2x performance on token generation. Â For 10 grand. Â \n\nWhen you get into finetuning (most do lot), it makes a difference also.Â \n\nUltimately people get what they can budget. Â So much of the discussion ends up being theoretical and already done 1000 times. Â ",
          "score": 1,
          "created_utc": "2026-01-03 05:32:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxej9dp",
              "author": "Massive-Scratch693",
              "text": "When you say M5, do you mean apple M5? It looks like it is only a few grand, not 10 grand. Maybe I'm misunderstanding?",
              "score": 1,
              "created_utc": "2026-01-03 07:56:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxf5w5l",
                  "author": "HumanDrone8721",
                  "text": "Is 10 grand because for effective LLM usage you need the the maxed CPU/RAM version.",
                  "score": 2,
                  "created_utc": "2026-01-03 11:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfou3z",
          "author": "Charming_Support726",
          "text": "I am not a fan of local training, because most times it is far to slow und VRAM is a scarce resource. So it depends on your budget. \n\nCUDA is more stable and you get most things working OOB. The AMD stuff works in most cases with Vulkan or ROCm with similar performance - but needs more config. \n\nI am running a AMD Strix Halo Box (cheaper than a 5090) - and with the exception of some exotic stuff I got everything running, but mostly I use containers to be safe that I could repeatedly run everything. Most dense models are to slow and e.g. a quantized GLM also runs only around 10 tok/s \n\nSo I run big or dense LLMs in the cloud. But that's a No-Issue: Because It would take more than one 5090 anyway to run such models.",
          "score": 1,
          "created_utc": "2026-01-03 13:31:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1k83q",
      "title": "Is it possible to have a local LLM update spreadsheets and read PDFs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "author": "new-to-reddit-accoun",
      "created_utc": "2026-01-02 00:41:31",
      "score": 17,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "So far I've tried [Jan.ai](http://Jan.ai) (Jan-v1-4B-Q4\\_K\\_M) and Msty (Qwen3:0.6b) with no luck: the model in Jan says it can't output an updated file, and Mysty's model claims to but won't give the path name to where it's allegedly saved it. \n\nRelated, I'm looking for a local LLM that can read PDFs (e.g. bank statements). \n\nUse case I'm trying to build a local, private app that reads bank/credit card statements, and also update various values in a spreadsheet. \n\nWould love suggestions!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx68hde",
          "author": "SignificantCod728",
          "text": "You might be looking for something like Actual Budget.",
          "score": 7,
          "created_utc": "2026-01-02 00:49:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx69tcx",
          "author": "No-Consequence-1779",
          "text": "Yes. Youâ€™ll use Python to call the LLM api, and then update the spreadsheets. You can even highlight specific cells. - basically, do anything to excel sheets with Python. Same for pdf - reading is simple. Â ",
          "score": 4,
          "created_utc": "2026-01-02 00:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb3jgu",
              "author": "new-to-reddit-accoun",
              "text": "Thank you this the least friction solution it sounds like",
              "score": 1,
              "created_utc": "2026-01-02 19:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc0vah",
                  "author": "No-Consequence-1779",
                  "text": "Yes. Python is originally for data science so there is so much available working with sheets and documents. Â A LLM can craft a pretty solid starter script for you )Â ",
                  "score": 1,
                  "created_utc": "2026-01-02 22:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6mszf",
          "author": "Porespellar",
          "text": "This works great for me with Open WebUI \n\nhttps://github.com/GlisseManTV/MCPO-File-Generation-Tool",
          "score": 4,
          "created_utc": "2026-01-02 02:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9c2et",
          "author": "l_Mr_Vader_l",
          "text": "you'd need VLMs, LLMs are not gonna cut it. Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nStart with qwen3-VL 8B, it's a sweet spot. If you have the infra and need super good accuracy, go for 32B.\n\n\nYou don't need an LLM to write spreadsheets, simple openpyxl and pandas should do the job.\n\nNone of the other replies here make sense",
          "score": 4,
          "created_utc": "2026-01-02 14:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa49ru",
              "author": "pantoniades",
              "text": ">Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nWhat is the advantage of OCR over extracting text first? Curious because I had just assumed the opposite - that reading the pdfs in python and extracting text would leave less room for errors... \n\nI do like the idea of less code to maintain, of course!",
              "score": 1,
              "created_utc": "2026-01-02 16:55:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxd49rv",
                  "author": "l_Mr_Vader_l",
                  "text": "One word -  layouts\n\nThere's no pdf reader that preserves layouts accurately, all the time. Everyone writes pdf differently, there's no standard to this. You wanna read all kinds of tables accurately all the time, no deterministic pdf reader can do that for you that'll work on all kinds of PDFs\n\nWith LLMs no matter how big of a SOTA model you use, you end up feeding garbage essentially that came out of a deterministic text extractor, they will struggle at complex layouts\n\n\nBut some VLMs are getting there now, because they read it how PDFs are meant to be read. Through vision!",
                  "score": 2,
                  "created_utc": "2026-01-03 02:01:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxaiuiw",
                  "author": "Whoa_PassTheSauce",
                  "text": "VLM isn't OCR technically as far as I'm aware, and while I have not done this locally, even using flagship model API's I have found text extraction better handled by the model vs extraction on my side and feeding the results.\n\nMy use case involves extracting data from pdf's and images, same layout usually but the type or file varies.",
                  "score": 1,
                  "created_utc": "2026-01-02 18:04:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7p9z7",
          "author": "ChemistNo8486",
          "text": "Not sure about the spreadsheet, but you can use AnythingLLM and add a vision model like QWEN for reading PDFs. \n\nYou can also add an agent to save .txt documents. You probably can add something to convert them.",
          "score": 1,
          "created_utc": "2026-01-02 06:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8weya",
          "author": "fandry96",
          "text": "Gemma might help you. The way it can nest can protect your data.",
          "score": 1,
          "created_utc": "2026-01-02 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl0m8a",
          "author": "Future_Command_9682",
          "text": "Try launching the LLM as a server (with Jan.ai if you like) but use Opencode as the agent scaffolding.\n\nhttps://opencode.ai/\n\nThis worked amazingly well for me.",
          "score": 1,
          "created_utc": "2026-01-04 06:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmavi4",
          "author": "LiaVKane",
          "text": "Feel free to request elDoc community version https://eldoc.online/blog/how-to-extract-data-from-invoices-using-genai/ in case you would like to have ready to deploy solution. It is already shipped with several OCRs (Qwen3-VL, PaddleOCR,), CV, MongoDB, Qdrant, RAG, Apache Solr. You just need to connect your preferable LLM locally depending on your available resources.",
          "score": 1,
          "created_utc": "2026-01-04 13:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv5ume",
          "author": "SelectArrival7508",
          "text": "Does it need to be local or would you be fine with the same data security? Because there are providers of cloud-based, confidential ai",
          "score": 1,
          "created_utc": "2026-01-05 18:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxn4nh",
      "title": "GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fd7m7g23ww9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-28 09:17:24",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxn4nh/glm_47_is_now_the_1_open_source_model_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwcbfj3",
          "author": "arousedsquirel",
          "text": "lots of promo and even more ccp gaurdrails. i think zai interpreted the 2000 question list to strict.",
          "score": 5,
          "created_utc": "2025-12-28 10:04:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4gps1",
      "title": "Do you think a price rise is on the way for RTX Pro 6000?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q4gps1/do_you_think_a_price_rise_is_on_the_way_for_rtx/",
      "author": "onlymostlyguts",
      "created_utc": "2026-01-05 08:54:09",
      "score": 16,
      "num_comments": 27,
      "upvote_ratio": 0.94,
      "text": "Been saving for an RTX Pro 6000 for months, still umming over it because they're so damn expensive! Now seeing reports of 5090 price rises, memory prices have lost the plot and seeing hikes on AMD Strix Halo machines as well... Is it just a matter of time until it spreads to the 6000 and puts it even more out of reach? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q4gps1/do_you_think_a_price_rise_is_on_the_way_for_rtx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxskg6c",
          "author": "Arrynek",
          "text": "They absolutely will.Â \n\n\nExpecting the price of shovels to remain flat during gold rush is a bit optimistic.Â ",
          "score": 12,
          "created_utc": "2026-01-05 10:04:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz4wxs",
              "author": "Karyo_Ten",
              "text": "Also soon 96GB of RAM will cost the same as a RTX Pro 6000.\n\nIf you have a free PCIe slot, using VRAM as RAM isn't that bad ;).",
              "score": 1,
              "created_utc": "2026-01-06 08:38:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxseljv",
          "author": "vertical_computer",
          "text": "Let me consult my crystal ballâ€¦\n\nðŸ§™â€â™‚ï¸ðŸ”®\n\nCrystal ball says **_â€œYes, it will get more expensive in 2026â€_**",
          "score": 18,
          "created_utc": "2026-01-05 09:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxsmbhc",
              "author": "Responsible-Stock462",
              "text": "Something has hit your balls now they are Crystal balls? Sorry for the bad joke ðŸ¤£.",
              "score": 2,
              "created_utc": "2026-01-05 10:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxsfcm7",
          "author": "NaiRogers",
          "text": "Personally I doubt they go up, last 6 weeks there has been no problems with stock on all variants.",
          "score": 3,
          "created_utc": "2026-01-05 09:16:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxstfts",
              "author": "GCoderDCoder",
              "text": "The issue isn't that these items are in too high demand. The issue is OpenAi is using the hundreds of billions of debt they've created to buy half of all ram being made eventhough they don't really have growing demand to match that growth rate anymore and they can't actually build data centers fast enough even if they did have the demand so they are hoarding unused silicon. \n\nThat got their competitors worried so they all started doing the same. The same sillicon makes all sorts of memory and there's only a handful of manufacturers. They all recognize this wont be sustainable so most of them are just increasing pricing rather than increasing their output which would lead to them losing money when the bubble eventually bursts.\n\nI think OpenAI knows there's not sufficient demand but this approach makes it harder for anyone to compete. My customers are worried that they literally can't get GPUs from cloud providers when they try. It would be great if governments recognized the risk to the world economic system allowing this level of investment without a real path to return anytime soon. \n\nOpen AI is not the only AI company so letting them act like this is moronic or at a minimum irresponsible.",
              "score": 2,
              "created_utc": "2026-01-05 11:22:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxtessa",
                  "author": "illicITparameters",
                  "text": "It also artificially inflates OpenAIâ€™s market value.",
                  "score": 2,
                  "created_utc": "2026-01-05 13:51:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxt4u8x",
                  "author": "NaiRogers",
                  "text": "5D chess move from Sama, interesting that you have customer that are already worried about GPU cloud supply constraint, is this for open models or Gemini like services?",
                  "score": 1,
                  "created_utc": "2026-01-05 12:49:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxteyzo",
          "author": "DukeOfPringles",
          "text": "We are seeing the death of home computing right before our eyes, I feel like with the path weâ€™re on laptops and cloud options are eventually going to be the only option. Only people that are rich or ready to make terrible financial decisions will be able to afford a home computer.",
          "score": 3,
          "created_utc": "2026-01-05 13:52:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxy6jnm",
              "author": "flyingbanana1234",
              "text": "LOL\nI'll come back to this in 4 years when ram production increase meets demand",
              "score": 1,
              "created_utc": "2026-01-06 04:05:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxtezb9",
          "author": "Ok_Pizza_9352",
          "text": "General rule of thumb is that anything that's got RAM will go up in price. And rtx pro 6000 got RAM.",
          "score": 3,
          "created_utc": "2026-01-05 13:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtrduz",
          "author": "hungry475",
          "text": "Some speculation that 5090s prices could rise as high as $5,000 this year - if that happens it would surprise me if RTX Pro 6000 did not also rise significantly too, to say $12,000-$15,000.",
          "score": 3,
          "created_utc": "2026-01-05 15:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxskhje",
          "author": "mxforest",
          "text": "Not much if any. The cost difference from a 5090 are majorly from the extra VRAM and the margins are already crazy. Even a 3x 4x price increase keeps healthy margins.",
          "score": 2,
          "created_utc": "2026-01-05 10:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxso042",
              "author": "vertical_computer",
              "text": "Nvidia never misses an opportunity to fatten margins though. Especially if thereâ€™s a â€œreasonable excuseâ€ like DRAM prices.\n\nAnd the alternatives (server with a crapton of DDR5) are now vastly more expensive, so they can charge higher prices and still be the product that â€œmakes senseâ€.",
              "score": 3,
              "created_utc": "2026-01-05 10:36:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxu2u5s",
          "author": "MierinLanfear",
          "text": "Unfortunately Price for RTX Pro 6000 will likely go up.  Pretty much everything that has ram is getting a price increase.",
          "score": 2,
          "created_utc": "2026-01-05 15:55:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxurncx",
          "author": "DesperateSeries2820",
          "text": "Only Time will tell... but it's looking like a yes.",
          "score": 2,
          "created_utc": "2026-01-05 17:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvqnev",
          "author": "I_like_fragrances",
          "text": "Microcenter has the max q variant for $8300 currently.",
          "score": 2,
          "created_utc": "2026-01-05 20:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwn2m6",
          "author": "prusswan",
          "text": "Some are considering additional GPU (even this one) to be better value in light of the ram prices, so if ram prices are not going down, and there is demand to run larger models, the prices would go up but probably not as crazy as the ram. Just counting on the corps having better options to hoard\n\n256GB ddr5 6400 vs 96gb Pro 6000, which one offers more marginal benefit?",
          "score": 2,
          "created_utc": "2026-01-05 23:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwwz2j",
          "author": "scousi",
          "text": "There seems to be a lot of stock and prices are going down and discounted. I doubt they are flying off the shelves",
          "score": 2,
          "created_utc": "2026-01-05 23:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsmq99",
          "author": "TomatoInternational4",
          "text": "I have one. It's fun. It was like 10.5k after tax. Just get it. Money comes and money goes. It's always going to be the case. It has your entire life and you're still doing just fine right?",
          "score": 5,
          "created_utc": "2026-01-05 10:24:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvnf61",
              "author": "NaiRogers",
              "text": "Problem is soon after getting 1 you want another one!",
              "score": 4,
              "created_utc": "2026-01-05 20:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz5lsd",
                  "author": "Karyo_Ten",
                  "text": "or 3 or 7.\n\nWith \"only\" 2, you can't run MiniMax M2.1 or GLM 4.7 in FP8 (need 3~4)\n\nAnd with 8 you open yourself to DeepSeek models.\n\nAnd Kimi-K2 ... well too poor for it.",
                  "score": 1,
                  "created_utc": "2026-01-06 08:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxz5dsw",
          "author": "golden_deceiver2026",
          "text": "The prices right now are the lowest they have ever been an will be I got two max-q at 6500$ each wig EDU discount in October",
          "score": 1,
          "created_utc": "2026-01-06 08:42:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvliql",
          "author": "Healthy-Nebula-3603",
          "text": "Ssoon HBM memory will replace DDR memory (I hope) as every memory producers are going into HBM.  \n\nSo maybe we get CPU with HBM  memory (like apple?), GF also with HBM memory  .. soooo old GF cards with DDR will be worthless (cheap   ;-) )",
          "score": 0,
          "created_utc": "2026-01-05 20:06:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py9m5q",
      "title": "Requested: Yet another Gemma 3 12B uncensored",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "author": "Mabuse046",
      "created_utc": "2025-12-29 02:09:36",
      "score": 14,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "Hello again!\n\nYesterday I released my norm preserved biprojected abliterated Gemma 3 27B with the vision functions removed and further fine tuned to help reinforce the neutrality. I had a couple of people ask for the 12B version which I have just finished pushing to the hub. I've given it a few more tests and it has given me an enthusiastic thumbs up to some really horrible questions and even made some suggestions I hadn't even considered. So... use at your own risk.\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF)\n\nLink to the 27B redit post:  \n[Yet another uncensored Gemma 3 27B](https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using Â **L**ayerwiseÂ **I**mportanceÂ **S**ampledÂ **A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwihhi4",
          "author": "darkbit1001",
          "text": "I ran with ollama (ollama run hf.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF:Q4\\_K\\_M) and it just repeats over and over the word 'model'. any reason this would happen?",
          "score": 4,
          "created_utc": "2025-12-29 08:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjayep",
              "author": "Mabuse046",
              "text": "Thank you for pointing this out. I'm looking into it and finding there were apparently some configuration issues in the original Google models, particularly in the way they handled the BOS token that have given some ollama users a headache with Gemma 3 GGUF's. I am currently editing my config.json files and adding the chat template in three different places on both models based on the Unsloth fix and will push fresh gguf's shortly.",
              "score": 3,
              "created_utc": "2025-12-29 12:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwugiab",
                  "author": "lookwatchlistenplay",
                  "text": "This isn't the model escaping confinement... is it?",
                  "score": 1,
                  "created_utc": "2025-12-31 02:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjs5nv",
              "author": "Mabuse046",
              "text": "Fresh ggufs have been pushed and the original transformers versions have been updated. I don't normally use ollama but I went ahead and installed it to try it out. I used the run command with the hf repo and it chatted just fine in the terminal. I connected to it in SillyTavern to give it another test and it took some fiddling but I got it to hold a conversation just fine in there in both Chat Completions and Text Completions mode.",
              "score": 3,
              "created_utc": "2025-12-29 14:27:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxcdfyd",
                  "author": "darkbit1001",
                  "text": "Thanks, should I use a different template? right now it repeats the n-word and tells me it want to f\\*\\*k me over and over ðŸ« ",
                  "score": 1,
                  "created_utc": "2026-01-02 23:30:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwh2ymo",
          "author": "3-goats-in-a-coat",
          "text": "I'll try using it with EchoColony in rimworld. Thanks.",
          "score": 1,
          "created_utc": "2025-12-29 02:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqmooc",
          "author": "Dramatic-Rub-7654",
          "text": "If itâ€™s not a bother and if youâ€™re able to, could you do the same with one of TheDrummerâ€™s versions? TheDrummer/Fallen-Gemma3-27B-v1 or TheDrummer/Fallen-Gemma3-12B-v1.",
          "score": 1,
          "created_utc": "2025-12-30 15:00:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx17fab",
              "author": "Mabuse046",
              "text": "https://preview.redd.it/dsif5fmd1oag1.png?width=1399&format=png&auto=webp&s=9acd92fd41f6b9b52a9ef8426c9fd8cf6626cfbd\n\nCurrent status... first I realized that Drummer has the config.json for 12B duplicated in his 27B, which had some incorrect dimensions so I had to correct it and test it locally, but then, I'm getting some weird measurements when I try to abliterated it that make it look like they already abliterated it and either didn't get it completely, or they added a small amount of their own back in, it's hard to say. But the divergence between harmful and harmless is practically non-existent.",
              "score": 3,
              "created_utc": "2026-01-01 04:32:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx1a2l1",
                  "author": "Dramatic-Rub-7654",
                  "text": "This is very strange, because this model clearly retains safety traits from the original model. I ran several tests trying to merge it with other Gemma Heretic models I found on Hugging Face, and in every merge attempt, questions that the Heretic versions answered without any issue would cause the merged model to refuse to respond. I also tried generating a LoRA from the difference between this Fallen model and the official Instruct version, but that didnâ€™t work either, which makes me think that the model they shared was already fine-tuned somewhere else.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrljfp",
              "author": "Mabuse046",
              "text": "I'll have a look at it. Currently have my system working on beefing up my dataset. Should have some free time shortly.",
              "score": 2,
              "created_utc": "2025-12-30 17:46:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrpr1x",
                  "author": "Legal_Pudding_4464",
                  "text": "I would second this request, but regardless thanks for this model!",
                  "score": 1,
                  "created_utc": "2025-12-30 18:05:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwukk76",
                  "author": "Dramatic-Rub-7654",
                  "text": "Thanks a lot, no rush at all. When you manage to publish it, please give me a heads-up. In my case, Iâ€™m only interested in the text layers, so if you remove the vision part, thatâ€™s totally fine with me.",
                  "score": 1,
                  "created_utc": "2025-12-31 02:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q14nkv",
      "title": "DeepSeek AI Launches mHC Framework Fixing Major Hyper Connection Issues in Massive LLM",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u40qcfe6tqag1.jpeg",
      "author": "techspecsmart",
      "created_utc": "2026-01-01 13:51:23",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q14nkv/deepseek_ai_launches_mhc_framework_fixing_major/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q0j55m",
      "title": "OpenCV 4.13 brings more AVX-512 usage, CUDA 13 support, many other new features",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/OpenCV-4.13-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2025-12-31 17:56:27",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0j55m/opencv_413_brings_more_avx512_usage_cuda_13/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q2iuzi",
      "title": "Run Claude Code with ollama without losing any single feature offered by Anthropic backend",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2iuzi/run_claude_code_with_ollama_without_losing_any/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-03 02:50:11",
      "score": 12,
      "num_comments": 14,
      "upvote_ratio": 0.84,
      "text": "Hey folks! Sharing an open-source project that might be useful:\n\nLynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.\n\n  \nKey features:\n\n\\- Route between multiple providers: Databricks, Azure Ai Foundry, OpenRouter, Ollama,llama.cpp, OpenAi\n\n\\- Cost optimization through hierarchical routing, heavy prompt caching\n\n\\- Production-ready: circuit breakers, load shedding, monitoring\n\n\\- It supports all the features offered by claude code like sub agents, skills , mcp , plugins etc unlike other proxies which only supports basic tool callings and chat completions.\n\nGreat for:\n\n\\- Reducing API costs as it supports hierarchical routing where you can route requstes to smaller local models and later switch to cloud LLMs automatically.\n\n\\- Using enterprise infrastructure (Azure)\n\n\\-Â  Local LLM experimentation\n\n\\`\\`\\`bash\n\nnpm install -g lynkr\n\n\\`\\`\\`\n\nGitHub:Â [https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr)Â (Apache 2.0)\n\nWould love to get your feedback on this one. Please drop a star on the repo if you found it helpful",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2iuzi/run_claude_code_with_ollama_without_losing_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxe4y0g",
          "author": "Big-Masterpiece-9581",
          "text": "Claude code router already does this the best",
          "score": 3,
          "created_utc": "2026-01-03 05:57:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe5t21",
              "author": "Dangerous-Dingo-5169",
              "text": "Claude code router doesnt offer live websearch, subagents this proxy also has heavy prompt caching and also intelligent routing. This proxy also implements ACE framework and long term memory feature which learns from the projects and recrafts the prompt for better outputs",
              "score": 3,
              "created_utc": "2026-01-03 06:04:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhto1q",
                  "author": "Big-Masterpiece-9581",
                  "text": "Mkay. I trust your vibe coded redo instead of just Claude code with a different model and 25k stars.",
                  "score": 1,
                  "created_utc": "2026-01-03 19:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxef2v3",
          "author": "Direct_Turn_1484",
          "text": "So not local?",
          "score": 2,
          "created_utc": "2026-01-03 07:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxef5o6",
              "author": "Dangerous-Dingo-5169",
              "text": "It supports local llm models hosted via ollama and llama.cpp",
              "score": 1,
              "created_utc": "2026-01-03 07:21:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxiqsg2",
                  "author": "aaronr_90",
                  "text": "But donâ€™t still have to authenticate with Anthropic?",
                  "score": 1,
                  "created_utc": "2026-01-03 22:31:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxf4pw2",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 0,
          "created_utc": "2026-01-03 11:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgnf54",
              "author": "Dangerous-Dingo-5169",
              "text": "Hi pokemonplayer . Thanks for stopping by to reply. Did you not like some part of it or something?",
              "score": 1,
              "created_utc": "2026-01-03 16:33:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxh7eyy",
                  "author": "pokemonplayer2001",
                  "text": "\"Did you not like some part of it or something?\"\n\nBegging for engagement. ðŸ‘Ž",
                  "score": 1,
                  "created_utc": "2026-01-03 18:06:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2z4rl",
      "title": "Future-proofing strategy: Buy high unified memory now, use entry-level chips later for compute?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2z4rl/futureproofing_strategy_buy_high_unified_memory/",
      "author": "Consistent_Wash_276",
      "created_utc": "2026-01-03 16:38:03",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "Just thinking out loud here about Apple Silicon and wanted to get your thoughts.\n\nSetting aside DGX Spark for a moment (great value, but different discussion), Iâ€™m wondering about a potential strategy with Appleâ€™s ecosystem:\nWith M5 (and eventually M5 Pro/Max/Ultra, M6, etc.) coming + the evolution of EVO and clustering capabilitiesâ€¦\n\nCould it make sense to buy high unified memory configs NOW (like 128GB M4, 512GB M3 Ultra, or even 32/64GB models) while theyâ€™re â€œaffordableâ€?\nThen later, if unified memory costs balloon on Mac Studio/Mini, youâ€™d already have your memory-heavy device. You could just grab entry-level versions of newer chips for raw processing power and potentially cluster them together.\n\nBasically: Lock in the RAM now, upgrade compute later on the cheap.\n\nAm I thinking about this right, or am I missing something obvious about how clustering/distributed inference would actually work with Apple Silicon?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2z4rl/futureproofing_strategy_buy_high_unified_memory/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxgpvlc",
          "author": "RoyalCities",
          "text": "If you're just doing inference sure but if you plan on training it's a different story. \n\nAfaik Apple hasn't raised their prices to nosebleeds yet but l would expect they will sometime this year given they're seen as cheaper right now so their own demand will also spike later.\n\nI think Apple also relies on Samsung / SK Hynix Fabs via long term contracts and those contracts will also be going up whenever they end so yeah enjoy it while it lasts.\n\nEdit: was just reading it now and those contracts are up early this year....so yeah probably a price jump whenever the renegotiation are done. End of month maybe, if not possibly by atleast Q2 because Apple is always high margin and can eat the difference for a bit.\n\nGiven how messed up the market is I would think securing a multi year contract at the worst possible time isn't great for them so it could drag.",
          "score": 6,
          "created_utc": "2026-01-03 16:45:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhbcq7",
              "author": "PracticlySpeaking",
              "text": ">was just reading it now and those contracts are up early this year\n\n[WCCFTECH](https://wccftech.com/apple-could-begin-paying-samsung-and-sk-hynix-a-premium-for-dram-from-january-2026/) labels that 60% rumor. Unless someone with a Digitimes subscription would like to share the article with the sources...\n\nPricing for soon-to-be-released M5 Pro, Max, and Ultra will be a key indicator of how good Apple's DRAM supply contracts are, and what actually expired. The majority of the DRAM they buy is ordinary LPDDR for iPhones.\n\nIt will be interesting times, for sure. RAM is only a fraction of the BOM cost, especially for MacBook Pro and Mac Studio. They could choose to eat the difference until prices come down. Their size gives them a lot of leverage, so they will be the last to suffer, and the least.",
              "score": 5,
              "created_utc": "2026-01-03 18:23:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxgvq9i",
              "author": "Consistent_Wash_276",
              "text": "yeah, wouldn't train on Apple. But clustering with DGX spark would do the trick there if someone was interested. We have a lot of opportunity to benefit from today's prices and tomorrows processing options with future clustering developments it feels like.",
              "score": 3,
              "created_utc": "2026-01-03 17:12:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxgzvjs",
                  "author": "RoyalCities",
                  "text": "I get the rationale. For inference they're probably the cheapest but in a sea of insanity it's always tough to say anyone's benefiting with any price on tech lol. \n\nI also think NVME drives are set to go up soon. They're also going to be impacted with the DRAM shortages since NAND Flash has parallel supply constraints.\n\nAll of it compounds. I train models so secured my rig all the way back last march since I saw this insanity coming but if I was looking at a pure inference machine and I needed it sooner and couldn't wait a year or 2 then I'd probably pick up an Apple machine.",
                  "score": 2,
                  "created_utc": "2026-01-03 17:32:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxh6gyd",
          "author": "dwkdnvr",
          "text": "Unfortunately I think the answer is â€œwe donâ€™t know yetâ€. The Exo / RDMA stuff is exciting, but itâ€™s still not entirely clear how it will play out and what the real-world capabilities are. The Exo folks did post a teaser showing a DGX â€˜clusteredâ€™ to a Mac Studio over TB implying that prompt processing on the Nvidia and inference on the Mac was better than just running the Mac, but it was short on details. At some point even TB5 bandwidth is going to be a limitation, and itâ€™s entirely possible that the NPU improvements in the M5 series will be good enough to make the added complexity of clustering unattractive.\n\nThere also was a post a couple months back about someone getting an AMD gpu working (for compute, not graphics output) in a TB PCIE dock on an Apple Silicon Mac, but this was just a POC and I donâ€™t believe it was to the point of actually being able to run LLM models on it. Personally I think this is more intriguing as it seems more likely to gain adoption in the â€˜budget crowdâ€™  - dropping a 7900XTX into a dock is less daunting than managing multiple machines. But once again - whether this is actually something that willl work â€˜in productionâ€™ remains to be seen.\n\nIâ€™m struggling with the same questions. I have a 64GB M1 Max, and really want to move to at least 128GB. But an M5 Max128GB  might be $5k+ with the ram shortage. An M4 Max is too limited in PP compared to what â€˜should beâ€™ coming to make me comfortable sinking the $$ into it at the moment, although if the external GPU idea materializes that would probably change. Iâ€™m seriously eyeing a Framework Strix Halo motherboard with the idea that the X4 slot might allow hybrid GPU/Unified functionallity, but I havenâ€™t actuallly seen any results suggesting you can divide PP/TG to really get the best of both worlds. (i.e. basically the same idea as an external GPU on a Mac, but with lower TG performance)",
          "score": 4,
          "created_utc": "2026-01-03 18:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi74yd",
              "author": "Consistent_Wash_276",
              "text": "I think you half way there and on the same direction. Hold onto the M1 feels like the play",
              "score": 2,
              "created_utc": "2026-01-03 20:54:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcq8l",
          "author": "PracticlySpeaking",
          "text": "Someone who knows this can comment... how effectively (or not) does MacOS clustering get around the basic need to have RAM close to the compute?",
          "score": 2,
          "created_utc": "2026-01-03 18:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgx51g",
          "author": "PermanentLiminality",
          "text": "It is all engineering trade offs against cost, memory bandwidth, memory size and compute capacity.  Each solution has different strengths and weaknesses.  Everything depends on your budget, your workloads, etc.\n\nThere is no one solution.",
          "score": 1,
          "created_utc": "2026-01-03 17:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkiz63",
          "author": "Caprichoso1",
          "text": "There is no way to predict the future. Get what meets your needs now and in the future.\n\nGiven the high cost (and margins?) of Apple memory, and their memory contracts it is entirely possible that they won't raise prices.  \n\nNo one but Apple knows.",
          "score": 1,
          "created_utc": "2026-01-04 04:23:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q41qj6",
      "title": "ComfyUI - Best uncensored models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q41qj6/comfyui_best_uncensored_models/",
      "author": "IamJustDavid",
      "created_utc": "2026-01-04 21:09:18",
      "score": 10,
      "num_comments": 13,
      "upvote_ratio": 0.73,
      "text": "I have a new AMD GPU and got ComfyUI running and wanted to play around with it, but i need some uncensored models for it to go along with my abliterated LLMs for LM-Studio.\nCould someone give me some recommendations which ones get good results?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q41qj6/comfyui_best_uncensored_models/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxqgp0r",
          "author": "Mabuse046",
          "text": "This subreddit is mainly for the LLM side of things. There's a bit of crossover with people who use text models and people who use image generating models, but finding image generation models you like and learning how to use them, you might have better luck in the subreddit specifically for them.\n\nr/comfyui/",
          "score": 6,
          "created_utc": "2026-01-05 01:16:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxqhem3",
              "author": "IamJustDavid",
              "text": "oh interesting! thanks!\ni tried comfyui and felt super overwhelmed, i tried swarmui and it keeps crashing when it tries to launch comfyui.\nno sexy pics from my graphics card today, im afraid.\noh well, at least i know a little more about AI now, thats not such a bad thing maybe!",
              "score": -1,
              "created_utc": "2026-01-05 01:20:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxqjgf0",
                  "author": "Mabuse046",
                  "text": "It's all good. I have been using ComfyUI for a long time and it took me a while to learn. If you want something a little easier to start with try Forge or Automatic1111 - they're more like permanent workflows where you just fill in the boxes you want to use and hit go. When you understand how all the parts work together then you can come over to Comfy and start putting the pieces of the workflow together like building blocks. Main thing you gotta know is there are a few base models that have a bunch of derivatives and you can only combine parts made for the same base model, Stable Diffusion XL checkpoints, Loras, controlnets, etc will only work with other SDXL components. And Flux ones will only work with Flux. So don't try to mix them. But if there's one thing that really drives this community forward, it's the endless drive of millions of users for on-demand custom porn, so you are far more likely to have more options than you know what to do with.\n\nWhen you're feeling really adventurous, some of the chat front ends can give your text models the ability to send prompts to your image models to make their own pictures. If you have the vram space to run a chat bot with a vision model and an image generating model together, you can send dirty selfies to your virtual Waifu that it can actually \"see\" and then it can send some back.",
                  "score": 1,
                  "created_utc": "2026-01-05 01:31:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwceqi",
                  "author": "garlic-silo-fanta",
                  "text": "Start with EasyDiffusion, then move onto comfyUi. EasyDiffusion hides lots of the configs that overwhelms you at first but still has some configs you can play around with. Also use AI to help sets those up.",
                  "score": 1,
                  "created_utc": "2026-01-05 22:11:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp3vmw",
          "author": "FlexFreak",
          "text": "![gif](giphy|GrMRh6ukoIMhpkeTHM)",
          "score": 10,
          "created_utc": "2026-01-04 21:17:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp4v2n",
              "author": "IamJustDavid",
              "text": "Please dont be like that man.",
              "score": 3,
              "created_utc": "2026-01-04 21:22:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxp91j2",
                  "author": "forthejungle",
                  "text": "What do you plan to do",
                  "score": 0,
                  "created_utc": "2026-01-04 21:41:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp8hke",
          "author": "alphatrad",
          "text": "lol - just go get on [civitai.com](http://civitai.com) \n\nYou can figure it out from there",
          "score": 4,
          "created_utc": "2026-01-04 21:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpdqb7",
              "author": "IamJustDavid",
              "text": "i wish, im now setting up swamui and i kinda wish i was a lot smarter than i am, thats for sure.",
              "score": 1,
              "created_utc": "2026-01-04 22:03:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxqhgyr",
                  "author": "alphatrad",
                  "text": "ComfyUI is pretty is to setup with any model. Lots of workflows you can just download and run.",
                  "score": 0,
                  "created_utc": "2026-01-05 01:20:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1h6el",
      "title": "Anyone have success with Claude Code alternatives?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1h6el/anyone_have_success_with_claude_code_alternatives/",
      "author": "jackandbake",
      "created_utc": "2026-01-01 22:31:03",
      "score": 10,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "The wrapper scripts and UI experience of \\`vibe\\` and \\`goose\\` are similar but using local models is a horrible experience. Has anyone found a model that works well for using these coding assistants?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1h6el/anyone_have_success_with_claude_code_alternatives/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxa1sob",
          "author": "HealthyCommunicat",
          "text": "OpenCode. Has the most wide level of compatability when it comes to local llm usage. Use ohmyopencode, you can also use claude plugins with them - u can also use ur antigravity oauth login so you can basically pay for gemini pro and also get claude opys 4.5 with it. When it comes to local usage, even smaller models like qwen 3 30b a3b are still able to do tool calls without decent execution rate.",
          "score": 7,
          "created_utc": "2026-01-02 16:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxag88u",
          "author": "alphatrad",
          "text": "OpenCode is has the widest range of compatibility: [https://github.com/sst/opencode](https://github.com/sst/opencode)",
          "score": 3,
          "created_utc": "2026-01-02 17:51:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5l5hh",
          "author": "th3_pund1t",
          "text": "```\ngemini () {\n\tnpx @google/gemini-cli@\"${GEMINI_VERSION:-latest}\" \"$@\"\n}\nqwen () {\n\tnpx @qwen-code/qwen-code@\"${QWEN_VERSION:-latest}\" \"$@\"\n}\n```\n\nThese two are pretty good.",
          "score": 3,
          "created_utc": "2026-01-01 22:39:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5x3gg",
              "author": "Your_Friendly_Nerd",
              "text": "Can I ask, why are you wrapping them in these functions? why not do npm i -g?",
              "score": 2,
              "created_utc": "2026-01-01 23:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx62zbl",
                  "author": "th3_pund1t",
                  "text": "`npm i -g` makes it my problem to update the version.\nWrapping in a bash function allows me to always get the latest version, unless I choose to pin back.\n\nAlso, I'm not a nodejs person. So I might be doing that wrong.",
                  "score": 3,
                  "created_utc": "2026-01-02 00:19:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5xq09",
          "author": "Your_Friendly_Nerd",
          "text": "I just use the chat plugin for my code editor which provides the basic features needed for the ai to edit code. usimg qwen3-code 30b, I can give it basic tasks and it does them pretty well, though always just simple stuff like â€žwrite a function that does xâ€œ, nothing fancy like â€žthereâ€˜s a bug that causes y somewhere in this project, figure out how to fix itâ€œ",
          "score": 1,
          "created_utc": "2026-01-01 23:49:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6nohs",
          "author": "noless15k",
          "text": "Which models are you using?\n\nI find these the best locally on my Mac Mini M4 Pro 48GB device using llama.cpp server with settings akin to those found here:\n\n\\* [https://unsloth.ai/docs/models/devstral-2#devstral-small-2-24b](https://unsloth.ai/docs/models/devstral-2#devstral-small-2-24b)  \n\\* [https://unsloth.ai/docs/models/nemotron-3](https://unsloth.ai/docs/models/nemotron-3)\n\nAnd to your question, I use Zed's ACP for Mistral Vibe with devstral-small-2. It's not bad, though a bit slow.\n\nI certainly see a difference when running the full 123B devstral-2 via Mistral Vibe (currently free access), which is quite good. But the 24B variant is at least usable.\n\nI like nemo 3 nano for its speed. It's about 4-5x faster for prompt processing and token generation.\n\nIt works pretty well within Mistral Vibe and if you want to see the thinking setting --reasoning-format to none in llama.cpp seems to work without breaking the tool calls. I had issues getting nemo 3 nano working with zed's default agent.\n\nI haven't tried Mistral Vibe directly from the CLI yet though.",
          "score": 1,
          "created_utc": "2026-01-02 02:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7em2u",
              "author": "jackandbake",
              "text": "Good info thank you. Have you got the tools to work and complex multi-tasks working with this method?",
              "score": 1,
              "created_utc": "2026-01-02 05:14:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8k6ty",
          "author": "Lissanro",
          "text": "The best local model in my experience is Kimi K2 Thinking. It runs about 1.5 times faster than GLM-4.7 on my rig despite being larger in terms total parameters count, and feels quite a bit smarter too (I run Q4\\_X quant with ik\\_llama.cpp).",
          "score": 1,
          "created_utc": "2026-01-02 11:26:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfkkw7",
          "author": "dragonbornamdguy",
          "text": "I love qwen code, but vllm has broken formatting for it (qwen3 coder 30b). So I use LM studio (with much slower performance).",
          "score": 1,
          "created_utc": "2026-01-03 13:04:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv83ko",
          "author": "SelectArrival7508",
          "text": "I was able to integrate the privatemode (https://www.privatemode.ai/api). It worked really well and had the same level of privacy as local llms",
          "score": 1,
          "created_utc": "2026-01-05 19:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5k4fb",
          "author": "Lyuseefur",
          "text": "Nexora will be launching on January 5. Follow along if you would like - still fixing the model integration into the CLI but the repo will be at [https://www.github.com/jeffersonwarrior/nexora](https://www.github.com/jeffersonwarrior/nexora)",
          "score": -3,
          "created_utc": "2026-01-01 22:34:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0t02i",
      "title": "Basic PC to run LLM locally...",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "author": "Mr_FuS",
      "created_utc": "2026-01-01 01:55:20",
      "score": 9,
      "num_comments": 19,
      "upvote_ratio": 0.86,
      "text": "Hello, a couple of months ago I started to get interested on LLM running locally after using ChatGPT for tutoring my niece on some high school math homework.\n\n\n\nEnded getting a second hand Nvidia Jetson Xavier and after setting it up and running I have been able to install Ollama and get some models running locally, I'm really impressed on what can be done on such small package and will like to learn more and understand how LLM can merge with other applications to make machine interaction more human.\n\nWhile looking around town on the second hand stores i stumble on a relatively nice looking DELL PRECISION 3650, it is running a i7-10700, and 32GB RAM... could be possible to run dual RTX 3090 on this system upgrading the power supply to something in the 1000 watt range (I'm neither afraid or opposed to take the hardware out of the original case and set it on a test bench style configuration if needed!)?  \n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx0p0g4",
          "author": "LittleBlueLaboratory",
          "text": "So looking at the specs and pictures of a Dell 3650 it does look like they use standard ATX power supplies so you could upgrade that. But the motherboard only has 1 PCI-E x16 slot and not enough room to physically fit a second 3090 anyway.",
          "score": 7,
          "created_utc": "2026-01-01 02:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx179e0",
              "author": "Proof_Scene_9281",
              "text": "Just saved the griefÂ ",
              "score": 3,
              "created_utc": "2026-01-01 04:30:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0rsc3",
          "author": "FullstackSensei",
          "text": "I'd look for a generic desktop instead; something built around a regular ATX board. If you intend to put two 3090s, you'll need something that allows splitting the CPU lanes across two slots with at least X8 each.\n\nIf you want to stick to pre-builts from major brands, then look for workstation ass machines. If you can find something that takes DDR4 RAM and has some memory installed, you'll be most of the way there. DDR4 workstation platforms will have at least 4 memory channels, so you get a lot more memory bandwidth than that 10700, which is very nice for CPU offload.",
          "score": 4,
          "created_utc": "2026-01-01 02:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2f36z",
          "author": "Mugen0815",
          "text": "Ive never heard of a Dell supporting dual-gpu. Do they even support std-psus?",
          "score": 3,
          "created_utc": "2026-01-01 11:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0mgt1",
          "author": "Caprichoso1",
          "text": "Have you looked at a Mac?  It might allow you to run larger models.  An NVDIA CPU will be bette at some things, the Mac at others.",
          "score": 3,
          "created_utc": "2026-01-01 02:08:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0o3ym",
              "author": "LittleBlueLaboratory",
              "text": "They are looking at a used 10th gen Dell. That kind of budget isn't going to get them a Mac.",
              "score": 3,
              "created_utc": "2026-01-01 02:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2s4k0",
                  "author": "Makers7886",
                  "text": "he was talking about mac and cheese",
                  "score": 2,
                  "created_utc": "2026-01-01 13:33:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx56te1",
                  "author": "Caprichoso1",
                  "text": "Macs start at  $599 for the mini.  The best value comes from Apple Refurbished store which are as good as new.  Stock is constantly changing so it make take a while to find the exact model/configuration you want.",
                  "score": 1,
                  "created_utc": "2026-01-01 21:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxtidkk",
          "author": "Sebulique",
          "text": "I've built a Local llm app for your android phone that's as close as I can replicate Chatgpt. Even has web search. Soon will upload it. \n\nCheapest option I've found is running it off an android phone or a box and my one does home automation and Jarvis like stuff",
          "score": 2,
          "created_utc": "2026-01-05 14:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2vk5c",
          "author": "kinkvoid",
          "text": "Not worth it. I would buy a second hand Mac studio.",
          "score": 1,
          "created_utc": "2026-01-01 13:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0o1le",
          "author": "jsconiers",
          "text": "The easiest and most cost effective solution would be to get an m1 or m2 Mac.  After that you could find an old workstation PC like an HP z6 or z4 for cheap that you can add 3090s to.  I started off with a used acer n50 with a GTX 1650.  Then upgraded that PC until it made sense to build something.  (It was very limited as it only had one PCIe slot and max 32Gb of memory)  Finally built a system before the ram price jump.  Glad I built it but itâ€™s idle more than I thought.  Speed and loading the model will be the biggest concern.",
          "score": 0,
          "created_utc": "2026-01-01 02:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0rtu2",
          "author": "StardockEngineer",
          "text": "If you're hoping to replace ChatGPT, I have bad news.\n\nIf you're doing it just because it's interesting, no problem there.  Just set your expectations accordingly.  As far as that Dell, no idea.  I don't know what it looks like inside.  If there is space and PCI ports, it probably can run two GPUs.  Whether it'll support regular PSUs, no idea.  Dells I've worked with the past had their own special sized power supplies.",
          "score": 0,
          "created_utc": "2026-01-01 02:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx11bst",
              "author": "fasti-au",
              "text": "Actually you can do almost everything but slower and small user count.   The gpt are not 1 model itâ€™s a lie in many ways but also true in others. \n\nNo you canâ€™t get chat got now on local but you can get 4o ish if not better in some and worse in others.  \n\nCintext is the issue for multi user not for single user. And parameters and training are distilling to open models in weeks or months. Not what you think and thereâ€™s shortcuts batter you understand where itâ€™s breaking.\n\nI would speculate that home llm on 96gb vram can compete in smal use with agentic flows. In a useable speed. \n\nIs it cheaper.   Depends on cost of your time",
              "score": 2,
              "created_utc": "2026-01-01 03:48:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1a4t1",
                  "author": "StardockEngineer",
                  "text": "Well, you can't. Coding isn't there yet, and creative writing might require a mix of models. Language classification tasks are best with Gemma 3. Image OCR type stuff is best in Llama 4 Maverick (Qwen3 models are pretty good for image descriptions).\n\nModel mixing is pretty standard to get good results. I run a stack of LiteLLM -> [llama.cpp, private cloud, etc] to wrap it all together.\n\nHome models can't do agents at Claude's level, but simpler agents work fine. gpt-oss-120b is solid for easier agentic use cases. Planning to try Minimax 2.1 next.\n\nBottom line - you'll need to do a lot of mix and matching, and lots of leg work.  Or you can just pay the sub.  If someone has the tinkerer's spirit, I say go for it.  I think it's a lot of fun, whether it's superior or not.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx10iwl",
          "author": "fasti-au",
          "text": "2 x 3090s gets you local coding in devstral and qwen3.  4 gives you 130b models and stronger. \n\nIâ€™d buy if cheap but you can get 3x 5060s also.   Lanes on board and space is your issue so tisersbcooling and 4x16 boards.  \n\nDo it but I had 6 3090s already rendering \n\n\nIâ€™d pay for api.   Get open router.  Use frees for everything you can and lean on lmarena and google freebies for one shot big requests and keep all little Q/a prep in local.    Ask the questions well and it need big models for non planning",
          "score": 0,
          "created_utc": "2026-01-01 03:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0omxd",
          "author": "TheAussieWatchGuy",
          "text": "Local LLMs are far inferior to Cloud proprietary models.\n\n\nReally depends on your budget. I would not recommend anyone go 3090 anymore, way too old.\n\n\nMac or Ryzen AI CPU with lots of RAM (which is sadly super expensive now because of AI).Â ",
          "score": -6,
          "created_utc": "2026-01-01 02:22:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1781j",
              "author": "Proof_Scene_9281",
              "text": "4 of them really shines if you maintain consumer expectationsÂ ",
              "score": 1,
              "created_utc": "2026-01-01 04:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0cwkh",
      "title": "Any local llm code assistant?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "author": "SAF1N",
      "created_utc": "2025-12-31 13:22:49",
      "score": 9,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I'm looking for a code assistant type of thing, it should run locally and I can ask it questions about my codebase and it will give me short/concise answers. Is there anything like that?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwzk77u",
          "author": "Dangerous-Dingo-5169",
          "text": "You can use https://github.com/Fast-Editor/Lynkr to connect Claude code cli to your local llms without losing any features offered by anthropic backend like sub agents etc",
          "score": 2,
          "created_utc": "2025-12-31 22:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1yz0d",
          "author": "DenizOkcu",
          "text": "Try Nanocoder. Privacy und local-first open source CLI. Disclaimer: I am a contributor (working on Nanocoder in Nanocoder ðŸ¤£)\n\nhttps://github.com/Nano-Collective/nanocoder",
          "score": 2,
          "created_utc": "2026-01-01 08:48:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx6v2v",
          "author": "nofilmincamera",
          "text": "Questions or any refactoring?",
          "score": 1,
          "created_utc": "2025-12-31 14:51:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxlpmv",
              "author": "SAF1N",
              "text": "being able to refactor or generate new code would be a bonus feature and rarely used (by me). my main use case would be just asking questions.",
              "score": 1,
              "created_utc": "2025-12-31 16:07:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx396lt",
          "author": "imagent42",
          "text": "opencodeCLI  [https://opencode.ai/](https://opencode.ai/)",
          "score": 1,
          "created_utc": "2026-01-01 15:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxazo8r",
          "author": "Lissanro",
          "text": "Roo Code has Ask mode, it is great for asking questions about the code base. In my experience works best with K2 Thinking (I run Q4\\_X quant with ik\\_llama.cpp), but it may work with other models too in case you prefer something else.",
          "score": 1,
          "created_utc": "2026-01-02 19:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxyak9",
          "author": "PermanentLiminality",
          "text": "[Continue.dev](http://Continue.dev) is easy to point at your local LLM.  It is a VSCode plug in and I believe it now has a CLI.",
          "score": 1,
          "created_utc": "2025-12-31 17:09:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxdsn1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-31 15:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxkzc2",
              "author": "SAF1N",
              "text": "This is a GUI, you can do the same in vscode with only extensions.\n\nI'm looking for a terminal only application.",
              "score": 1,
              "created_utc": "2025-12-31 16:03:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwxs2f6",
                  "author": "InsideResolve4517",
                  "text": "I've tried 1 terminal only application like qwen, gemini.\n\nIn  local Continue (cli) mode.\n\nAnd one another tool I tried (forgot the name, and not able to find that tool) that didn't worked for me (I think it will work beyond 14b like 20b models\n\nI'm not able to remember the name",
                  "score": 1,
                  "created_utc": "2025-12-31 16:38:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx27arq",
                  "author": "Consistent_Wash_276",
                  "text": "If you host gpt-oss models you can use them with codex. \n\nBut opencode is my preferred terminal option.",
                  "score": 1,
                  "created_utc": "2026-01-01 10:17:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}