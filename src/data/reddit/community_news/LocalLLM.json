{
  "metadata": {
    "last_updated": "2026-02-12 17:15:22",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 250,
    "file_size_bytes": 323371
  },
  "items": [
    {
      "id": "1r229ay",
      "title": "GLM thinks its Gemini",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/q4ikh1sn8wig1.jpeg",
      "author": "dolo937",
      "created_utc": "2026-02-11 16:39:08",
      "score": 179,
      "num_comments": 65,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r229ay/glm_thinks_its_gemini/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4twyys",
          "author": "Ninthjake",
          "text": "Day 1537 of telling people they cannot ask ask the LLM what model it is. It doesn't know...",
          "score": 186,
          "created_utc": "2026-02-11 17:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u9700",
              "author": "spudzo",
              "text": "How come when I ask the Internet text prediction machine a question and it produces the most likely text on the Internet instead of becoming sentient? /s",
              "score": 50,
              "created_utc": "2026-02-11 18:12:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4v8hqm",
              "author": "tofagerl",
              "text": "Negative. I am a meat popsicle.",
              "score": 10,
              "created_utc": "2026-02-11 20:59:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u2hhd",
              "author": "stinky_binky3",
              "text": "i seriously don’t understand why anyone takes any of what an LLM says as fact, especially with regards to things “about” the LLM. i challenge OP to ask the model this 10-15 times and see how many different answers they get",
              "score": 41,
              "created_utc": "2026-02-11 17:41:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ut0e3",
                  "author": "eli_pizza",
                  "text": "It makes sense if you don’t really know how they work",
                  "score": 20,
                  "created_utc": "2026-02-11 19:44:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4w63dp",
              "author": "HenkPoley",
              "text": "Well yes, but GLM is also finetuned on a lot of traces of other commercial chatbots. It talks quite like them.\n\nClick the (i) buttons in the Slop column to see matches: [https://eqbench.com/creative\\_writing\\_longform.html](https://eqbench.com/creative_writing_longform.html)",
              "score": 8,
              "created_utc": "2026-02-11 23:50:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yxpvn",
                  "author": "cheechw",
                  "text": "Yes, we know. \n\nThe training data is all stolen (not just GLM, but all models from every maker). The architecture is what sets models apart at this point.",
                  "score": 2,
                  "created_utc": "2026-02-12 12:29:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4y5842",
              "author": "singh_taranjeet",
              "text": "Day 1538 of explaining that “what model are you?” is just another prompt. If you didn’t hardcode the answer in the system prompt, you are sampling vibes from the training.. It is autocomplete, not introspection",
              "score": 5,
              "created_utc": "2026-02-12 08:11:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vmh2n",
              "author": "markeus101",
              "text": "Try asking claude that. An llm knows which model it is unless trained on the output of other models",
              "score": 8,
              "created_utc": "2026-02-11 22:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vz13s",
                  "author": "claythearc",
                  "text": "It knows which model it is, when they put it in the system prompt.",
                  "score": 23,
                  "created_utc": "2026-02-11 23:11:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w4nmw",
                  "author": "YoAmoElTacos",
                  "text": "If you ask an api versiom claude what it is,it wont know.",
                  "score": 3,
                  "created_utc": "2026-02-11 23:42:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w48sc",
                  "author": "Baldur-Norddahl",
                  "text": "I asked Claude that and it says it knows because it is in the system prompt. It does claim that it was trained to know it is an Anthropic model, but not which one exactly.\n\nhttps://claude.ai/share/80b38695-fd74-41fc-a3d8-bfd294395e42",
                  "score": 2,
                  "created_utc": "2026-02-11 23:40:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xxbkc",
              "author": "dotdd",
              "text": "Not true!\n\nhttps://preview.redd.it/grdxp42oh0jg1.png?width=1575&format=png&auto=webp&s=52516040e17b75096306fe6fa072936bc403358a\n\n",
              "score": 1,
              "created_utc": "2026-02-12 06:56:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4y7mjv",
              "author": "old_mikser",
              "text": "Somehow GPT and Claude models know who they are. It's not hard to include it in system prompt to prevent uncertainty.",
              "score": 1,
              "created_utc": "2026-02-12 08:35:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yydcy",
                  "author": "cheechw",
                  "text": "All that shows is that Z.ai didn't bother including it in the system prompt. It doesn't mean that GPT and Claude are smarter or have more self awareness or anything.",
                  "score": 2,
                  "created_utc": "2026-02-12 12:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wq3bw",
              "author": "memorial_mike",
              "text": "It might not know, but it is clearly using output from other models and violating their ToS. This is the interesting piece here.",
              "score": 0,
              "created_utc": "2026-02-12 01:49:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ydsca",
                  "author": "droptableadventures",
                  "text": "Even if you just scraped the internet you'd pick up a bunch of training data from other models, because the internet is now full of AI slop.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:36:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uamao",
          "author": "Own-Potential-2308",
          "text": "Lmao same happened to me\n\nAsked it if it had an Android app and it linked me to Gemini",
          "score": 11,
          "created_utc": "2026-02-11 18:18:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u5doo",
          "author": "iPharter",
          "text": "I get the following answer for the same question \n\nI am Qwen, a large language model developed by Alibaba Cloud. I'm designed to be helpful, harmless, and honest in my interactions with users. My training involves learning from a diverse range of internet text data, allowing me to assist with various tasks like answering questions, providing explanations, and engaging in conversations.\n\nIs there something specific you'd like to know about my capabilities or how I can assist you today?",
          "score": 9,
          "created_utc": "2026-02-11 17:54:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u7s0z",
          "author": "Recent_Apricot_517",
          "text": "Likely used training data sets from Gemini that wasn't scrubbed",
          "score": 5,
          "created_utc": "2026-02-11 18:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvg0r",
          "author": "NoobMLDude",
          "text": "Interesting. \nSo is GLM distilled from Gemini outputs ? \nOr is Gemini used in generating synthetic data ?\nVery curious to learn.",
          "score": 26,
          "created_utc": "2026-02-11 17:08:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2wks",
              "author": "Distinct-Target7503",
              "text": ">So is GLM distilled from Gemini outputs ? \nOr is Gemini used in generating synthetic data ?\n\n\nthose are basically the same thing. gemini is not OS, so we don't have access to the raw logits distribution... so the only distillation you can do is supervised fine tuning on a synthetic dataset (or using gemini as scorer for RL, if you consider that distillation, but that wouldn't likely make the model think it is Gemini)",
              "score": 17,
              "created_utc": "2026-02-11 17:43:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u6o7t",
                  "author": "Feztopia",
                  "text": "Actually it wasn't but they got mixed up and now we have the mess. Synthetic data is just output input. Distillation was supposed to contain all the possible next tokens and their individual probabilities.",
                  "score": 3,
                  "created_utc": "2026-02-11 18:00:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u5jqd",
              "author": "WolfeheartGames",
              "text": "I had 4.7 call itself Claude. I think that it doesn't have a set personality or constitution but knows it's Ai, so it reaches for the name it most identifies with Ai.\n\nThere are quite a few models that haven't been taught what model they are. You see it a lot with the test releases on open router, but even gemini only know \"I'm made by Google\" it doesn't know if it's Gemma or gemini or what version. (I think they've reinforced gemini as a name but still not version).",
              "score": 8,
              "created_utc": "2026-02-11 17:55:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4waufz",
                  "author": "Metsatronic",
                  "text": "I experienced this as well. But the response was identical to the internal system classifier for Claude. So either it was trained on enough output that included Claude's response to the same prompt... Or... They could be routing to an Anthropic endpoint when their servers get hammered?",
                  "score": 2,
                  "created_utc": "2026-02-12 00:18:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w20h0",
                  "author": "3spky5u-oss",
                  "text": "Most identifying with Gemini is not a good thing, hah. ",
                  "score": -1,
                  "created_utc": "2026-02-11 23:27:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4usmze",
              "author": "eli_pizza",
              "text": "It doesn’t mean anything. There was Gemini output in the training data somewhere, but of course there was, it’s on the internet. Maybe they also distilled from Gemini but this isn’t strong evidence of anything. \n\nLLMs are not capable of introspection. At best you can get it to repeat something from the system prompt about what it is and how it works. But often it’s just hallucination.",
              "score": 3,
              "created_utc": "2026-02-11 19:43:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tqcum",
          "author": "Worth_Rabbit_6262",
          "text": "You are cheating because the context of the model is not empty",
          "score": 12,
          "created_utc": "2026-02-11 16:44:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4trear",
              "author": "dolo937",
              "text": "This was my first query and 2nd is the query in the post\n\nhttps://preview.redd.it/aapcmya9awig1.jpeg?width=828&format=pjpg&auto=webp&s=f693e2fb3c0e4221b356cc6a8d1288c2390a184a",
              "score": 8,
              "created_utc": "2026-02-11 16:48:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uefm1",
                  "author": "StackSmashRepeat",
                  "text": "Without a system prompt; The next thing you feed into the context window will effectively act as a system prompt. So you can tell it that its Obama. And it will be Obama. It doesn't know Jack shit. This happens with kimi 2.5 too. \n\nHowever. I don't know why this happened.",
                  "score": 1,
                  "created_utc": "2026-02-11 18:36:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wv2a7",
          "author": "Scott_Malkinsons",
          "text": "It doesn't know what it is, as you can also ask for a comparison of GLM-4.7 and GLM-5 and it'll tell you the newest version is GLM-4 and both 4.7 and 5 don't exist.",
          "score": 3,
          "created_utc": "2026-02-12 02:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tqyry",
          "author": "dolo937",
          "text": "https://preview.redd.it/jt12u651awig1.jpeg?width=828&format=pjpg&auto=webp&s=77879f88468600b9235f6439574a91661c1537a0",
          "score": 6,
          "created_utc": "2026-02-11 16:46:58",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4trpru",
              "author": "Worth_Rabbit_6262",
              "text": "Man I was joking ",
              "score": 5,
              "created_utc": "2026-02-11 16:50:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u0b7d",
          "author": "Witty_Mycologist_995",
          "text": "Garbage in garbage out",
          "score": 2,
          "created_utc": "2026-02-11 17:30:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvjdy",
          "author": "blownawayx2",
          "text": "I’ve wondered if all of the companies are just stealing one another’s models and tweaking them… I’m not sure that it should surprise us given their training data is entirely stolen in the first place.\n\nWould there be any way for one company to call out another and prove this? I’d think not. It’s not like Anthropic wasn’t built by people from OpenAI.",
          "score": 2,
          "created_utc": "2026-02-11 17:08:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u186c",
              "author": "05032-MendicantBias",
              "text": "Considering they all stole the total sum of the internet, I would laugh at the concept they steal from each other.",
              "score": 7,
              "created_utc": "2026-02-11 17:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4yze49",
              "author": "cheechw",
              "text": "They're not stealing each other's models (especially not Gemini, it's closed source). But data? Yes they're all doing that.\n\nBut just having the data doesn't mean you can put out a good model though - far from it. If that were the case, Meta would have put out a top model long ago. What makes it work better comes down to architecture, training techniques, and other technological innovations used to develop the model.",
              "score": 1,
              "created_utc": "2026-02-12 12:40:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vphg4",
          "author": "Alone-Marionberry-59",
          "text": "Ah… this is incriminating!!",
          "score": 2,
          "created_utc": "2026-02-11 22:21:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w68zy",
              "author": "FaceDeer",
              "text": "Howso? Most LLMs are trained on synthetic data these days.",
              "score": 0,
              "created_utc": "2026-02-11 23:51:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vznna",
          "author": "UnionCounty22",
          "text": "Let’s hope they didn’t train on Gemini",
          "score": 2,
          "created_utc": "2026-02-11 23:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wb015",
          "author": "Deepeye225",
          "text": "Well, someone was distilling from Gemini. Naughty, naughty...",
          "score": 2,
          "created_utc": "2026-02-12 00:18:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wmsxx",
          "author": "ad_rojo75",
          "text": "My deepseek believes is Claude",
          "score": 2,
          "created_utc": "2026-02-12 01:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uy47p",
          "author": "Last_Track_2058",
          "text": "Shills going mad",
          "score": 1,
          "created_utc": "2026-02-11 20:09:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v7sg8",
          "author": "macumazana",
          "text": "aw shit here we go again",
          "score": 1,
          "created_utc": "2026-02-11 20:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vm4er",
          "author": "Hefty_Development813",
          "text": "This has been common since awhile ago, all the open source models will say they are someone else. It just comes down to the output of those models and their distribution in the training data. An LLM doesn't have any hardcoded identity info",
          "score": 1,
          "created_utc": "2026-02-11 22:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xkfz6",
          "author": "ScuffedBalata",
          "text": "So did the old Gemma3 from several years ago.",
          "score": 1,
          "created_utc": "2026-02-12 05:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ye2bf",
          "author": "No_Mango7658",
          "text": "Omfg is glm just distilled Gemini!  Fucking incredible!!!",
          "score": 1,
          "created_utc": "2026-02-12 09:39:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9ngd",
          "author": "Torodaddy",
          "text": "Was it built on Gemma?",
          "score": 1,
          "created_utc": "2026-02-12 13:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvl27",
          "author": "exaknight21",
          "text": "i think they use numerous models to train and supervise the data, it becomes impossible to actually make sure the training data is true to its intent. It hallucinates, spits out some garbage facts like this and now its essentially saying its Gemini when its not, because its training parameters have that data.\n\nIf I understand it correctly, LLM is essentially a superfast database with condensed vectors that load inside a safetensor, so it only knows about the training data etched into its safetensor files.",
          "score": 1,
          "created_utc": "2026-02-11 17:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvbi2",
          "author": "kaanivore",
          "text": "It doesn’t think it’s Gemini, it IS Gemini with a face lift",
          "score": -2,
          "created_utc": "2026-02-11 17:07:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0swmh",
      "title": "Is Local LLM the next trend in the AI wave?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r0swmh/is_local_llm_the_next_trend_in_the_ai_wave/",
      "author": "G3grip",
      "created_utc": "2026-02-10 06:02:01",
      "score": 91,
      "num_comments": 140,
      "upvote_ratio": 0.89,
      "text": "Suddenly I've been seeing a lot of content and videos centred around the cost of running LLMs vs paying subscriptions.\n\nCouple of months back it was all about Claude Code, very recently it is OpenClaw, now I feel, that by the coming week, everyone would be talking hardware and local LLM setups.\n\nIt will start with people raving about \"how low is the cost of local AI over time\", \"privacy\", \"freedom\", only to be followed by gurus saying \"why did I not do this earlier?\" and dropping crazy money into hardware setups. Then there will be an influx of 1-click setup tools and guides.\n\nHonestly, I've been loving all the exploration and learning with the past couple of trends, but I'll admit, it's a bit much to keep up with. I don't know, maybe I'm just crazy at this point.\n\nThoughts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r0swmh/is_local_llm_the_next_trend_in_the_ai_wave/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4kqfeh",
          "author": "jpinbn",
          "text": "I can't afford to sell an arm and a leg for RAM, so no LLM until prices come back to earth.",
          "score": 18,
          "created_utc": "2026-02-10 07:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kqxdw",
              "author": "corelabjoe",
              "text": "Uses Vram mostly not normal ram. I've run adorable little LLMs on even an old GTX1660 but presently run them on an RTX3060 12gb.",
              "score": 6,
              "created_utc": "2026-02-10 07:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mfjlw",
                  "author": "Prestigious_Ad572",
                  "text": "Running LLMs on RAM with CPU for inference is surprisingly cheap and with good enough performance that it becomes possible to use big models locally.",
                  "score": 3,
                  "created_utc": "2026-02-10 14:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ksm01",
              "author": "G3grip",
              "text": "Don't even get me started on the hardware cost these days. The market has constantly cursed ever since the crypto boom.\n\nFrom there, I've never seen \"normal\" pricing across the right components. It's always something or the other that jacks up the price of memory, GPUs, and whatnot. Then they rarely get down to the regular price.",
              "score": 5,
              "created_utc": "2026-02-10 07:20:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kqdhs",
          "author": "civil_politics",
          "text": "I mean local is always going to be limited by your gear - to have a ‘good’ local LLM you’re talking 5k minimum to have something reasonable and really 10k+ to get something that is significant value add - and at these prices you can get access to good online models for a long period of time. \n\nFor some usecases, and I think something like OpenClaw is one, there will be people building good local setups, but writ large I don’t see this trend taking off. I mean even with OpenClaw there are a ton of reviews basically saying it is near useless without hooking it up to one of the large models.",
          "score": 31,
          "created_utc": "2026-02-10 07:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kqxxn",
              "author": "G3grip",
              "text": "Cost is a big barrier to entry. Agree.\n\nUnfortunately, this type of first principle thinking only comes to sense after the wave settles.\n\nHaving that said, I think there's real value in budget, small and specialised LLMs. Use cases that we are yet to see. \n\nThere are always going to trade offs and advantages on both sides.",
              "score": 10,
              "created_utc": "2026-02-10 07:05:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4l0a45",
                  "author": "pot_sniffer",
                  "text": "For coding I've had good results with qwen2.5-coder 32b q4, using a rag with chromadb that contains all the info about the project.\n\nI've also had good results. Maybe better actually using Qwen3-Coder-Next 80b q6 without the rag as it caused it to be too slow. \n\nI'm running a 7950x, 64gb ddr5 and a 9060XT 16gb. Using llama.cpp. the 7950x isn't essential but it definitely helps, a 9070XT would've been better if I had the money at the time but this is working well enough I don't regret getting the 9060. Cost me £1200 just before the ram crisis. Would be a few hundred more now but not out of reach",
                  "score": 8,
                  "created_utc": "2026-02-10 08:33:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n6im1",
              "author": "PeanutButterApricotS",
              "text": "Local ai isn’t to far from being able to be a quality agent. A lot of it is the process.\n\nI was able to build my own personal Ai agent app. It has worked better then openclaw because openclaw restricts quality items/features behind paywalls.\n\nI was able to make a agent\n\nI told the agent to have a way to check my weather locally\n\nThe agent made a python script after scraping the web and finding an api. It took the script and provided the weather.\n\nIt provided the wrong time zone (defaults to ust)\n\nI told it I needed CST\n\nIt went and fixed the time zone and updated the script.\n\n\n\nI couldn’t get openclaw to do that because it’s restricted to paying users.",
              "score": 2,
              "created_utc": "2026-02-10 16:59:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n71n8",
                  "author": "civil_politics",
                  "text": "I mean OpenClaw is just a wrapper - what models are you running locally? \n\nGimmicky things like single page websites or small little single API integration tasks are fun and neat, but they hardly move the needle on being practically viable. That being said getting that built on a local setup that costs less than 5k would be pretty impressive",
                  "score": 1,
                  "created_utc": "2026-02-10 17:01:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nqk7y",
                  "author": "cheechw",
                  "text": "What exactly are you talking about that's behind a paywall? I'm using Openclaw and there's nothing to even pay for right now. Am I missing something huge?\n\nAlso, Openclaw is open source. You can just fork it and change it to do what you want. Frankly, I don't even know if we're talking about the same thing here.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mjsry",
              "author": "danny_094",
              "text": "The situation will change once RAM prices recover. Current development efforts are also focused on allocating RAM to VRAM. AI is not even five years old in this form. Anything concerning the future is speculation.",
              "score": 1,
              "created_utc": "2026-02-10 15:12:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o44et",
                  "author": "stuffitystuff",
                  "text": "RAM manufacturers are selling unbuilt manufacturing capacity for a couple years from now. It's going to be a bit.",
                  "score": 3,
                  "created_utc": "2026-02-10 19:33:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mpnzs",
                  "author": "quantgorithm",
                  "text": "It’s not estimated to recover until 27 or 28.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:41:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4p02u9",
              "author": "nntb",
              "text": "The thing is I'm quite surprised how many things were able to overcome even though we have small gear or something like that. I never thought I'd be able to run you know one of the super big models without having an entire farm of gpus but then to see that I can not only run it but run it pretty decently using hard drive space on a solid state that's kind of amazing.",
              "score": 1,
              "created_utc": "2026-02-10 22:02:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4p1c24",
                  "author": "civil_politics",
                  "text": "Oh I completely agree it’s pretty incredible and we will move more and more towards a world where local is accessible, but I’m not sure if we get to a point where local can run truly impressive models that aren’t hyper specialized and even then for a decent chunk of change. How often do you need to leverage something that costs 10k to build?",
                  "score": 1,
                  "created_utc": "2026-02-10 22:08:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4pwr9q",
              "author": "alias454",
              "text": "I think using local models can solve the good enough problem very well though. A lot of things don't need SOTA models to perform decently well. The quality of relatively small local models is also getting better.",
              "score": 1,
              "created_utc": "2026-02-11 01:00:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l5pb5",
          "author": "truthputer",
          "text": "I think we’re coming up on a tipping point where free small local models make more sense than paid cloud models.\n\nThe big cloud models will always be better than the local models, but the gap between them is closing as the big models have diminishing returns on their improvements.\n\nIe: Local models are getting better faster than the cloud models are improving.\n\nLocal models will still be taking big jumps forward in the next 12 months - especially with the next generation of AI trained small LLMs giving similar result to models 10x bigger.\n\nWhereas cloud models might be a bit better? Like how Opus 4.6 is a bit better than 4.5 but some people have decided to stick with 4.5?\n\nWe’re already at the point where if you care about image generation, you’re probably running it locally. While cloud stuff is still “better” and usually faster, it’s not as flexible with usage limits and content restrictions.\n\nLike you’re crazy to pay for online image generation with usage restrictions when you can just download Ollama or llmstudio and make as many images as you want, even tho it might take a few minutes for a high quality result.\n\nI can see in the next 12 months it might make more financial sense to have a $2000 mini PC with 128GB RAM running a local model that you connect Claude Code or KiloCode to vs paying $200 per month for a Claude Code subscription.",
          "score": 9,
          "created_utc": "2026-02-10 09:26:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8ijh",
              "author": "G3grip",
              "text": "Solid perspective. The way you placed it, I think it really makes sense.\n\nI just hope that the hardware pricing comes to its senses soon.",
              "score": 2,
              "created_utc": "2026-02-10 09:54:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4no0uu",
              "author": "chimph",
              "text": "Can local image gen really compare to nano banana pro tho?\nI’ve gone deep into comfy ui in the past and tbf haven’t revisited for a while but recently have been using nano banana for help designing and renovating a house and it’s been superb. Nails the images. I feel like I would have to do a lot of tweaking for local and still get mixed results. But yes, not having limitations is nice.",
              "score": 2,
              "created_utc": "2026-02-10 18:19:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4owfhg",
              "author": "NaiveAccess8821",
              "text": "Very interesting take, I wondered what Local LMs are on top of your useage list",
              "score": 1,
              "created_utc": "2026-02-10 21:44:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kv114",
          "author": "BeatTheMarket30",
          "text": "Local LLMs are much less capable with 16-24GB GPU memory limit. We need cheap GPUs with like 128GB memory.",
          "score": 7,
          "created_utc": "2026-02-10 07:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l7y8h",
              "author": "Look_0ver_There",
              "text": "There's already machines we can buy today that have dedicated AI inference acceleration with that much memory for less than the price of a 5090.\n\nI would say that 256 or 512GB may be a better memory target for a very strong capable local machine that would get within spitting distance of the \"big boys\".\n\nThe thing is, that would democratize strong local AI compute to the masses, and that would break this artificial monopolistic bubble that has formed.\n\nI suspect that is the real reason why the tech companies have aligned to push prices sky high. To me, this is remiscent of the 80's to 90's, when vast compute power was in the hands of the few while the masses got scraps. The strong personal PCs of the time cost tens of thousands of dollars in today's value.\n\nThen PC\"s started to actually get good, and now everyone has on their desktop machines that cost a few thousand dollars that outperform machines that used to cost milliona.\n\nTo my mind, it's inevitable that this artificial bubble will break and access to wide scale high quality personal and private AI will start to become available to all. The big boys know it, and they're doing all the can to gate keep it and make money while the sun is shining.",
              "score": 4,
              "created_utc": "2026-02-10 09:48:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ozvul",
                  "author": "NaiveAccess8821",
                  "text": "If we, as a community, (or someone who built a company) can train a collection of agents by using a shared pool of individual data (with consent first), or experiences sampled from such small agents. \n\nThen with such agent swarm work coordinatively, don't we have a path beating those big guys? ",
                  "score": 1,
                  "created_utc": "2026-02-10 22:01:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4l1qvr",
              "author": "G3grip",
              "text": "One can only hope. RAM is expensive as is, V-RAM more so, plus we're at the mercy of OEMs to bundle it within GPUs. So you only get a decent amount alongside beefy, expensive GPUs.",
              "score": 2,
              "created_utc": "2026-02-10 08:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4klbmh",
          "author": "oldboi",
          "text": "I think the maturity of self-hosted LLM's is starting to reach a point of actual usefulness to those with user-end hardware. Tiny, effective MoE models, models under 20gb that match or outperform SOTA models from 12 months ago, more apps to use them with, and then the odd viral use-case like OpenClaw... \n\nI have been tinkering with them for a while but they weren't really that useful or effective until recently. Now I actually *use* them instead of *testing* them.",
          "score": 6,
          "created_utc": "2026-02-10 06:16:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l43l7",
              "author": "RnRau",
              "text": "Yup models have been getting better over the last year. \n\nThere is a paper out there that reckons that the capability per parameter doubles every 3.5 months.\n\nhttps://arxiv.org/abs/2412.04315",
              "score": 3,
              "created_utc": "2026-02-10 09:11:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lbvj4",
                  "author": "G3grip",
                  "text": "2x every 3.5 months is insane growth.",
                  "score": 1,
                  "created_utc": "2026-02-10 10:25:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4klwn0",
              "author": "G3grip",
              "text": "The most useful things that I hope we get out of this is simplified self hosting LLMs and practically skilled local LLMs under budget.",
              "score": 2,
              "created_utc": "2026-02-10 06:21:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kqrla",
                  "author": "corelabjoe",
                  "text": "Simplified? [Oh it's super simplified!](https://corelab.tech/private-chatgpt-local-llm/)\n\nYou can just fire a docker up with access to your gpu, load an LLM module and BAM! Start chatting with it...\n\nIt's been like this for AWHILE already ;)",
                  "score": 2,
                  "created_utc": "2026-02-10 07:03:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kukao",
          "author": "SebastianOpp",
          "text": "Remember to factor in electricity too ⚡️",
          "score": 4,
          "created_utc": "2026-02-10 07:38:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l25fx",
              "author": "G3grip",
              "text": "And fire safety.\n\nPeople underestimate the effort that goes into uptime. Entire businesses run on maintaining hardware and redundancy.",
              "score": 2,
              "created_utc": "2026-02-10 08:51:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4nozcc",
              "author": "chimph",
              "text": "Again why Mac Minis are great. 5w idle",
              "score": 1,
              "created_utc": "2026-02-10 18:24:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mbj9n",
          "author": "Hector_Rvkp",
          "text": "A Strix halo for 2100$ (128) starts to be capable with large MoE models. It's slower and dumber than a frontier model, but it doesn't have to be slow or dumb. Moe models will obviously keep getting better. Meanwhile, models are running out of data to train on, synthetic data is often but helpful, and model improvement curve looks logarithmic, not linear, let alone exponential. Also, killing a fly with a bazooka isn't a sign of intelligence, so most people don't need a 1tb dense model to find a recipe. \nPrivacy concern is real. \nSkilling/ reskilling concern is real too. \nIt would be intelligent to develop skills and work flows to know when to use local compute, and maybe, when you really do need speed or compute, use the cloud. But running a dense pro model for every prompt sounds dumb, wasteful, and only sensible from the perspective of the cloud provider that's charging you per token irrespective of the slop you make. \nOne fear I have: letting a model running on the cloud overnight working on some multi step task to wake up to a large bill with garbage to show for. If that ran locally, all I've done is heat my place a little bit.",
          "score": 5,
          "created_utc": "2026-02-10 14:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kkk5r",
          "author": "hchen25",
          "text": "Yes, looking forward to build a machine to run local LLM.",
          "score": 3,
          "created_utc": "2026-02-10 06:09:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4krnlc",
              "author": "goobervision",
              "text": "Mac M1 Max Pro 64gb second hand isn't a bad shout on a budget.",
              "score": 3,
              "created_utc": "2026-02-10 07:11:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l4pft",
                  "author": "ScuffedBalata",
                  "text": "I found one of those, busted up Macbook with bad screen (i wasn't using it as a laptop anyway) for $700 recently.",
                  "score": 1,
                  "created_utc": "2026-02-10 09:16:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4klndk",
              "author": "G3grip",
              "text": "Me too! But I'm terrified of being so excited about every new thing all the time.",
              "score": 4,
              "created_utc": "2026-02-10 06:18:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kqpaj",
                  "author": "MrScotchyScotch",
                  "text": "I don't think you need to be scared of being excited",
                  "score": 1,
                  "created_utc": "2026-02-10 07:03:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l699a",
          "author": "hydropix",
          "text": "Local LLM are actually quite expensive.\n\nIn data centers, the hardware is used almost 100% of the time and is much better optimized. If you calculate precisely what it costs to amortize a configuration capable of running AI and its obsolescence after three years. You need to perform inference for at least 6 hours continuously per day (7 days a week) for the local option to be profitable. I advise you to ask an AI to give you the details of the calculations and depreciation costs; it's quite easy to compare different scenarios.\n\nThe difference with GPUs for gaming, where latency is critical, does not apply to the usual use of AI, so again, this does not argue in favor of local processing.\n\nOn the other hand, for those who want to work with AI, having the hardware to perform all the tests is very valuable. For others, unless you have very intensive use or data protection needs, the cloud is a better option.",
          "score": 3,
          "created_utc": "2026-02-10 09:32:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfn1k",
              "author": "profcuck",
              "text": "You're right about all that of course but despite the current blip in RAM prices (and SSDs) a broad version of Moore's Law still very much holds.\n\nIn 2016 \"Intel Core i7 Extreme Edition is the Most Powerful Desktop PC Processor Ever\" - https://www.thurrott.com/hardware/67574/computex-2016-intel-core-i7-extreme-edition-powerful-desktop-pc-processor-ever\n\nIt is slower than an Intel N150 which is now considered appropriate for a very basic mini pc.\n\nWe could dig a bit deeper:\nhttps://medium.com/@cli_87015/the-evolution-of-gpu-pricing-a-deep-dive-into-cost-per-fp32-flop-for-hyperscalers-cbf072b85bb5\n\nMy overall point?  While it is always going to be true that near 100% usage of datacenter hardware means that it will be cheaper to go that way, it won't be long until it's all plenty cheap enough to run good stuff locally.",
              "score": 2,
              "created_utc": "2026-02-10 10:59:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o5pus",
                  "author": "stuffitystuff",
                  "text": "Even now you can get an Nvidia A100 80GB on eBay for \\~$7k and they were around twice that a year ago, IIRC.",
                  "score": 2,
                  "created_utc": "2026-02-10 19:40:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l2g1j",
          "author": "NoobMLDude",
          "text": "LocalLLM is not “the next trend” , it’s already “the CURRENT TREND” practiced by many (including me) for few years .\n\nI’ve been[trying to inform and educate](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) the general public that they do not need to give their money and personal data to big AI companies. We can get Local LLMs running and it might be enough for 90% or users. \n\nWhy would you share your Personal information with AI companies who will use it against you to show you/ manipulate you with Ads and other products !??\n\n\nNot just data, the offerings of AI APIs and services are very questionable:\n1. Customers of Perplexity paid for Pro account (advertised to have limit of 2000 research questions) noticed they have reduced the limit now tor 20. How a company can change the limits of a product after customers paid for  it is still unclear, but it’s happening.\n\n2. companies change the underlying model running behind the Chat Apps to lower quality model without telling us to save costs. Customers notice degradation in models few weeks after the model has generated buzz and captured attention.\n\nAll of this is happening and  the only way to have some control over your AI experience/access is to run your own model locally.\n\n\nHere’s some ideas for Private and Local AI uses:\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 5,
          "created_utc": "2026-02-10 08:54:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l5t0q",
              "author": "G3grip",
              "text": "Of course, I know many are doing it and even swear by it. \n\nWhen I say 'next trend,' I’m talking about a total takeover. I mean the kind of buzz where my phone is literally drowning in notifications from Google and X every single day. The same way everyone (myself included) is obsessed with OpenClaw right now.",
              "score": 2,
              "created_utc": "2026-02-10 09:27:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lolim",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-10 12:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m2dv8",
              "author": "G3grip",
              "text": "Hey, this looks great. \n\nCan I ask, what sort of a model / use case would I be able to run on a simple macbook (if any at all)?",
              "score": 2,
              "created_utc": "2026-02-10 13:40:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4mhm53",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-02-10 15:01:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lqs6l",
          "author": "techlatest_net",
          "text": "Totally feel you on the hype cycle overload—Claude coding buzz to OpenClaw blowing up overnight, now everyone's eyeing local setups. Local LLMs are def picking up steam for privacy and no-sub fees long-term (like $1/day amortized vs $20 cloud), but that hardware drop (GPUs, rigs hitting $1k-3k) is gonna suck people in hard. I'm all in on the freedom angle, but yeah, it's exhausting keeping pace—stick to what works for your workflow and ignore the FOMO gurus!",
          "score": 2,
          "created_utc": "2026-02-10 12:27:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3aug",
              "author": "G3grip",
              "text": "Finally, someone shares my pain!",
              "score": 1,
              "created_utc": "2026-02-10 13:45:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lzay2",
          "author": "pot4scotty20",
          "text": "next trend? if getting squeezed by market to pay the maximum price is something that interests you? the time to build was 12 months ago, best to wait for prices to stabilize after these artificially inflated hype rates come crashing down",
          "score": 2,
          "created_utc": "2026-02-10 13:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3i6z",
              "author": "G3grip",
              "text": "😭",
              "score": 1,
              "created_utc": "2026-02-10 13:46:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mirj9",
          "author": "danny_094",
          "text": "I don't see locale execution as a trend. It's more like the logical next step. Locale LLM has existed since GPT. It's just becoming more user-friendly. It will eventually be possible to run large models on less powerful hardware. Not because magic happens, but because AI is still quite new and its development toward greater efficiency is still in its infancy. Tools like Clawbot simply make AI more appealing to the local market, which in turn generates more interest. OpenAI's first AI model was open source and available to everyone back then. So, locally.",
          "score": 2,
          "created_utc": "2026-02-10 15:07:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mk3fl",
          "author": "beedunc",
          "text": "It would have been, but not with these ram prices. \n\nMy $180 96GB sticks (needed to run big local models) are now almost  $1200. That’s more than I spent on everything else.",
          "score": 2,
          "created_utc": "2026-02-10 15:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mnbld",
          "author": "mr__smooth",
          "text": "The issue is memory. If there wasnt a supply constraint created by that company last year it would have been much more realizable. But now there is a shortage of memory that will impact what the average consumer can buy on their own",
          "score": 2,
          "created_utc": "2026-02-10 15:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n97f2",
              "author": "G3grip",
              "text": "This RAM pricing thing resonates with everyone.",
              "score": 1,
              "created_utc": "2026-02-10 17:11:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4oqs2p",
              "author": "VaporwaveUtopia",
              "text": "The RAM shortage is shitty, but it won't last forever. There's demand and money to be made, so it won't be long before existing manufacturers scale up, or new players get into the space.",
              "score": 1,
              "created_utc": "2026-02-10 21:18:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4msbgv",
          "author": "robbro9",
          "text": "Nah, it's quantum blockchain ai..... With COVID.  Seriously i how your right.  I get the feeling the ai cos are booking up all the hardware needed to run ai locally but they can't sustain that.  Imagine memory prices from a year ago, ai would be so much more accessible with that locally.",
          "score": 2,
          "created_utc": "2026-02-10 15:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4na20n",
              "author": "G3grip",
              "text": "I feel like I should have bulk-ordered RAM before the price hike; I would have been rich. The return on these things has turned out to be better than gold... lol...",
              "score": 1,
              "created_utc": "2026-02-10 17:15:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4msq8t",
          "author": "AppointmentAway3164",
          "text": "Yes. Privacy is important. But also I feel like the cloud based models have hit a wall and aren’t advancing sufficiently to make their position a requirement for ai. I can get similar results locally, and so the opportunity is present for local models to gain popularity.\n\nAs a dev I really am not interested in constantly improving Anthropic’s models, which is what you’re doing when using them. You’re paying them to reinforcement train their models. And at the end of the day, I want my source code private. \n\nThe nvidia DGX Spark is really nice, and apple would be smart to position themselves as local LLM hosts. We will see if Tim Apple can see the forest for the trees.",
          "score": 2,
          "created_utc": "2026-02-10 15:55:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nbxlz",
              "author": "G3grip",
              "text": "Good points. And with models like Kimi 2.5 available for free, the hardware is the only limitation. However, if you want to run something like that locally and do not already own the hardware to support it, then the monthly subscriptions start to look good.\n\nOnly if you are actually dropping 1000s of dollars on the subs every month, you should then get a machine so powerful (and expensive).\n\n  \nBut my hope is that in time we will have small, specialised and efficient models that most people can run decent personal machines. Leaving the subscription-based ones on for heavy-duty work that not everyone needs all the time.",
              "score": 1,
              "created_utc": "2026-02-10 17:24:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mt8nj",
          "author": "locai_al-ibadi",
          "text": "Local LLMs (and AI in general) has been improving massively with lower computing resources/specs. It is costly to have a \"good\" local LLM but not as much as it used to be 6+ month ago. Honestly, I think for a lot of the daily average use cases of LLMs, people do not need cloud servers running those massive models. ",
          "score": 2,
          "created_utc": "2026-02-10 15:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc367",
              "author": "G3grip",
              "text": "I second this.",
              "score": 2,
              "created_utc": "2026-02-10 17:24:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtaes",
          "author": "mpw-linux",
          "text": "The question is do we really want to have a local super computer to run high-end LLM's? Cost of hardware, cost of power to run the machines. For price of buying all the hardware one can just pay the 30-50 USD a. month to let the cloud computers do all the work for us. Local LLM's are a great for experimentation, learning models, training, etc. \n\nI think that the future will be more powerful quantized versiions of powerful LLM's for local usage like what they are doing at Liquid AI. ",
          "score": 2,
          "created_utc": "2026-02-10 15:58:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ncj27",
              "author": "G3grip",
              "text": "Intrusting stuff, this Liquid AI. First time I got to know. Thanks.\n\nI agree.",
              "score": 1,
              "created_utc": "2026-02-10 17:26:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nmv6a",
          "author": "chimph",
          "text": "Local LLM will become a bigger part of people’s toolsets for sure. And I think the whole Clawdbot + Mac Mini has made people more aware of local LLM. This years Mac M5 will be known for driving good local models",
          "score": 2,
          "created_utc": "2026-02-10 18:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o9vdq",
          "author": "darkestvice",
          "text": "Given that said AI buildup has skyrocketed the cost of all PC parts required to run not-so-garbage local LLMs, I highly doubt Local LLMs will become trendy any time soon. It's still squarely in an up front cost range that's out of reach for the majority of people.",
          "score": 2,
          "created_utc": "2026-02-10 20:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v1ldh",
              "author": "G3grip",
              "text": "Fair enough, I keep telling myself ”Prices have to come down, technology is supposed to get cheaper with time, these are just temporary bumps.\", BUT... they never do!\n\nHow we are going backwards in technology, it's beyond me. I mean, ya, I know why the costs are high, I understand the technical reasons but, like, as a whole, I just can't comprehend it at this point.",
              "score": 1,
              "created_utc": "2026-02-11 20:26:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ozqni",
          "author": "nntb",
          "text": "You're going to find a lot of videos comparing the cost of local versus software as a solution because there's a lot of money being poured into software as a solution right now. And a lot of companies have to recoup that cost so they may be fighting each other and they may be offering stuff for inexpensive for now until the user base gets big enough it can warrant them increasing costs. I've been on this subreddit for a really long time and I've been involved in locally running AI for years I would say that it's not a new thing that local is a popular option. And I would say it's the best option to be 100% honest. Now I know those who don't have hardware of their own will go and use things like Google collab or other services to host theirs and that's fine but really the heart of this entire suburb has been locally running AI",
          "score": 2,
          "created_utc": "2026-02-10 22:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p3i2y",
          "author": "billytryer",
          "text": "I was thinking of buying a M4 Mac mini £900new new with 24gb of ram is that good for ollama and Some of the smaller LLMs , I saw here that the M1 Max has a better performance ? I just don't know",
          "score": 2,
          "created_utc": "2026-02-10 22:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v51ds",
              "author": "G3grip",
              "text": "Ya, one dude did mention that, I think 2 days back. While I don't know much about the M1 and M4 comparison, many folks have told me that smaller models on Apple silicon Macs are no issues.",
              "score": 1,
              "created_utc": "2026-02-11 20:43:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pcf1d",
          "author": "epSos-DE",
          "text": "Yes. Logical Bit Operators on the CPU make it 100% possible.\n\nGPU vector calualtions can assist.\n\nIT is possible to have local LLMs that reduce the decision space with binery trees and run logical operators , instead of vector search. LLM with logical bit operators and some GPU offload can run on one CPU core , if it can prune the decision space correctly.\n\nTraining the Ai is still hard tho ! Needs server farms or p2p bot swarms that share computational load.\n\n\n\nBit Logical LLMs will need to be trained for logical bit operators or the vector space LLM traiing data will need to be converted to bit operators and binary decision trees  for look ups of data.\n\n\n\nTechnically it is possible, but NVIDIA wants to sell more GPUs !!! SO they train vectors all day long, because Bit operations would harm their profits !",
          "score": 2,
          "created_utc": "2026-02-10 23:05:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v5msc",
              "author": "G3grip",
              "text": "I feel dumb after reading your comment. I gotta learn so much about these LLMs.",
              "score": 1,
              "created_utc": "2026-02-11 20:46:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pqcjk",
          "author": "HealthyCommunicat",
          "text": "i was pretty dissapointed because even with 350+ gb of VRAM clustered, it felt hard making any LLM have a fast enough pp/tgs to feel really useful, (20 token/s is not realistically useful for someone who uses a agentic cli tool literally 8+ hrs a day) - but the most recent qwen 3 coder next gives me hope, i really wish companies would focus more on speed, minimax and stepfun are also two that come to mind, but the 200b models seem to be the perfect golden spot for those that want to spend 5-10k to run models from home.",
          "score": 2,
          "created_utc": "2026-02-11 00:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v6hw5",
              "author": "G3grip",
              "text": "With how much VRAM!?!?!  😵",
              "score": 1,
              "created_utc": "2026-02-11 20:50:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v0ogy",
          "author": "Hydroskeletal",
          "text": "If you are writing software; maybe? \n\nBut if you're doing stuff where you are ingesting lots and lots of tokens it becomes quite easy to see where the token pricing model becomes prohibitive. So that leaves you with using instance pricing. Well now you need to make sure you're not leaving the thing idle. Even then not hard to see a weekly bill in the hundreds. At that point ownership vs renting is a real discussion and you can see break-even in months not years. \n\nBut the real hinge is if spot instance pricing remains stable. I am skeptical about how much runway there really is for the accelerating hunger.",
          "score": 2,
          "created_utc": "2026-02-11 20:21:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l1fsp",
          "author": "beryugyo619",
          "text": "Of course it would be great if you could just run Claude Opus(any) locally for free, if that could be done at all. It's not the matter of demand but feasibility.",
          "score": 1,
          "created_utc": "2026-02-10 08:44:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1yzb",
              "author": "G3grip",
              "text": "What if we don't need Opus? What if there's real use in much smaller but specialised and practical models that may run on much cheaper hardware?\n\nI mean ya, it's all speculation but the possibilities are exciting.",
              "score": 1,
              "created_utc": "2026-02-10 08:50:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l4f46",
          "author": "ScuffedBalata",
          "text": "No, unless there's a breakthrough that makes the ever-larger scaling of models breakdown somehow.\n\nThe cloud-based \"frontier\" models will always be (at least in the short/medium term) SIGNIFICANTLY more capable than the local ones.",
          "score": 1,
          "created_utc": "2026-02-10 09:14:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l6pg4",
              "author": "G3grip",
              "text": "Yes, but do we always need the best model at all times? \n\nI mean, think of it this way, I can rent a ridiculously powerful machine on AWS. They will always have servers more powerful than any personal computer that I can go out and buy. \n\nI still own a very low-powered laptop for my personal day-to-day tasks, as I don't need a 128-core CPU for running chrome and checking email (RAM is a different story for chrome... lol...).",
              "score": 2,
              "created_utc": "2026-02-10 09:36:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4mpqk5",
                  "author": "ScuffedBalata",
                  "text": "For Coding?  I feel like \"often\".\n\nI mean even when you have a better model, i find it makes more elegant code, it plans better and architects better for future upgrades and cleaner code.  \n\nA lower quality model might make it work, but it may do it with spaghetti code, or choose a suboptimal library that doesn't account for something.\n\nJust my feeling doing a lot of AI-backed development.\n\nIf you're just chatting or asking about muffin recipies, then no, no you don't.  But even if you're asking about research, the actual concreteness and reduction of \"AI buzz\" (aka mild hallucination and exageration) is better with larger models.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:41:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4li35j",
          "author": "Soft-Dragonfruit6447",
          "text": "Local LLMs feel inevitable long-term, but hardware cost is still the real gate. Once RAM/VRAM prices drop and setup gets easier, adoption will explode. Right now it’s a power-user game, not mainstream yet.",
          "score": 1,
          "created_utc": "2026-02-10 11:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lmx4e",
          "author": "tcoder7",
          "text": "I do not thunk so. The cartels colluded to price gouge [RAM.So](http://RAM.So) the next waves of PC will have lower RAM. Also the best LLM models are still not available to download locally. There is still some low proba chance of local private LLM winning the race if there is an innovation to dramatically reduce RAM and VRAM needs.  ",
          "score": 1,
          "created_utc": "2026-02-10 11:59:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m1guj",
              "author": "G3grip",
              "text": "Wait, doesn't the RAM pricing impact the \"cartel\" too?",
              "score": 1,
              "created_utc": "2026-02-10 13:35:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m2o3f",
                  "author": "tcoder7",
                  "text": "Not if you are the first buyer.",
                  "score": 1,
                  "created_utc": "2026-02-10 13:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lsne4",
          "author": "LeaderBriefs-com",
          "text": "The hardware you would need to run a competent LOCAL LLM is really cost prohibitive for the majority. \n\nYou will always trade quality for time and money. \nThat doesn’t really scale in my mind.",
          "score": 1,
          "created_utc": "2026-02-10 12:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3408",
              "author": "G3grip",
              "text": "I think for now, yes.\n\nIn time, smaller models will get better and small setups would be viable.",
              "score": 1,
              "created_utc": "2026-02-10 13:44:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nrdcz",
          "author": "No_Vehicle7826",
          "text": "If they ever make local LLMs efficient enough to run good models of basic hardware, closed ai will just be government ai \n\nUntil then, there's Venice for us broke peeps without a rig lol\n\nLeChat is pretty cool too. There's too much \"safety\" on the ChatGPT, Claude and Gemini... then Grok just seems dumb to me. Hoping Grok 4.20 is cool though, supposed to be 6 trillion parameters",
          "score": 1,
          "created_utc": "2026-02-10 18:34:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nwmyp",
          "author": "Budget-Length2666",
          "text": "Not right now - it will cost you 20k+ to run a similar high end model locally in some cluster just to be sort of on equal playing field with the frontier foundational models. No guarantees your hardware you buy today will be able to compete with the frontier models of 2 years from now. And tps is generally slower locally, even with latency considered. inference is just hard and expensive.",
          "score": 1,
          "created_utc": "2026-02-10 18:58:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o7dqm",
          "author": "Zestyclose_Paint3922",
          "text": "Local will always be behind. I think this is more for corporate \"limited\" applications.",
          "score": 1,
          "created_utc": "2026-02-10 19:48:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4och8n",
          "author": "abe-azam",
          "text": "If you use high end models, local won’t do.\n\nIf you want something basic the it will pass.",
          "score": 1,
          "created_utc": "2026-02-10 20:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pl2sj",
          "author": "Apprehensive_Gap3673",
          "text": "I don't think it will honestly ever be a trend.  Local LLM can't compete with frontier models in terms of performance and most people wouldn't be able to set them up or be willing to pay the upfront costs",
          "score": 1,
          "created_utc": "2026-02-10 23:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v5v1y",
              "author": "G3grip",
              "text": "I would put my money on the other post. But I guess only time will tell.",
              "score": 1,
              "created_utc": "2026-02-11 20:47:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qtpvx",
          "author": "Shoddy_Bed3240",
          "text": "Two Mac Ultra M3 machines with 512GB of memory are probably the best option if you want to run very large local models at a quality level comparable to paid subscriptions — but the total cost is around $30,000.",
          "score": 1,
          "created_utc": "2026-02-11 04:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v6rqr",
              "author": "G3grip",
              "text": "The Mac mini studios (or whatever they call them) are cheaper, no?",
              "score": 1,
              "created_utc": "2026-02-11 20:51:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4vcpx1",
                  "author": "Shoddy_Bed3240",
                  "text": "It’s cheaper, but only the M3 Ultra comes with 512 GB of unified memory.",
                  "score": 1,
                  "created_utc": "2026-02-11 21:20:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sj36n",
          "author": "FinancialBandicoot75",
          "text": "With cost as a factor, no.  It’s fun for small llm but not too practical.  Yes you can use a 3060ti and be ok, but if you want better you have to spend a buck on vid cards and ram and with ram nuts today, good luck.",
          "score": 1,
          "created_utc": "2026-02-11 13:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nf0xi",
          "author": "One-Distribution-376",
          "text": "its not the future they want, you'll own nothing and you will be happy!",
          "score": 1,
          "created_utc": "2026-02-10 17:38:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nh713",
              "author": "G3grip",
              "text": "I heard Linus say the same thing today.",
              "score": 1,
              "created_utc": "2026-02-10 17:48:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4knwms",
          "author": "woundedkarma",
          "text": "Yeah... I think it could be a business for a while. I just don't know how to find clients. LOL  There's bound to be people who want local llms but don't know how to do it. \n\nI posted in a local sub and got zero interest. I need more outreach but lol I'm an introvert.",
          "score": 0,
          "created_utc": "2026-02-10 06:38:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kt9o3",
              "author": "Low_Amplitude_Worlds",
              "text": "My advice:\n1) Use LLMs to help with your marketing and outreach strategy, find leads, etc. and maybe even generate copywriting for you, though that can turn many people off.\n2) Avoid that toxic sub you posted to at all costs. Not only did they reject your post, but made up misinformation about why it wouldn’t work. That’s not your target market or audience.",
              "score": 2,
              "created_utc": "2026-02-10 07:26:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kv9ys",
              "author": "Wide_Brief3025",
              "text": "Finding clients for niche tech like local LLMs can be tough, especially if cold outreach is not your thing. Try looking for communities where small businesses or devs vent about AI challenges and jump in to share your knowledge. Tools like ParseStream can help by tracking those discussions across multiple platforms so you do not miss any real opportunities to help.",
              "score": 2,
              "created_utc": "2026-02-10 07:45:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ko9jn",
              "author": "G3grip",
              "text": "These days you can get outreach white sitting in your room. Try posting at multiple places maybe? Also, care to share that post? I'm intrigued to see.",
              "score": 2,
              "created_utc": "2026-02-10 06:41:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kp59l",
                  "author": "woundedkarma",
                  "text": "[https://www.reddit.com/r/kzoo/comments/1qvavf9/fully\\_local\\_ai/](https://www.reddit.com/r/kzoo/comments/1qvavf9/fully_local_ai/) ",
                  "score": 1,
                  "created_utc": "2026-02-10 06:49:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxrvq9",
      "title": "My $250 24gb of VRAM setup (still in 2026)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qxrvq9/my_250_24gb_of_vram_setup_still_in_2026/",
      "author": "Jack_at_BrewLedger",
      "created_utc": "2026-02-06 19:49:08",
      "score": 78,
      "num_comments": 50,
      "upvote_ratio": 0.95,
      "text": "What I'm running is a nvidia Tesla p40, a server compute accelerator card from 2016 which just so happens to have 24 gigs of VRAM on the highest end version. They can be found on ebay for about $250 bucks right now.\n\nThe card is passively cooled and designed for a server rack, so I made a custom cooling shroud to force air into the back and through it like it would work in a server race. On the back is a PWM high pressure fan, controlled by my motherboard, and the speed is directly bound to the tesla's temperature through nvidia-smi and FanControl on Windows.\n\nBought a big ass PC case, cut a big chunk out of the back. Got myself an 8 pin server card adapter to dual 6 pin GPU power outputs from a PSU, and got myself a nice big ass PSU. Fired the whole thing up as a Frankenstein design.\n\nI wouldn't call it fast by any means, but in 4bit quant I can fit gpt-oss 20b in there with 32k+ context length, all on the GPU. The speeds are fast enough to be used as a local chatbot, so works well as my AI assistant. Also, works with CUDA 12 if you pick the right driver.\n\n  \nOh, I forgot to mention, this thing has no video output, as it's a server accelerator card. I have a Ryzen 5700G as my processor, with integrated graphics. The Tesla is driver hacked into registering as a nVidia Quadro in workstation mode, and so I can run games on the Tesla using the windows settings for high performance graphics (meant to be used on gaming laptops with GPUs) and it gets relayed through my integrated GPU. The actual die on this card is a clone of the 1080ti, so I get 1080ti performance gaming too, just with 24 gigs of VRAM, and it'll run anything as long as I put the game's exe in a list. I'm most proud of that part of the setup.\n\n[The TESLA running in my rig](https://preview.redd.it/xodlgmmphxhg1.png?width=4624&format=png&auto=webp&s=7adaaf3f9eaed0dee176343d18e12a534af33bd8)\n\n[Underside View](https://preview.redd.it/u2ftaa3shxhg1.png?width=4624&format=png&auto=webp&s=642d7b83a028dd6b268a16c6a6241f2dcd4feb05)\n\n[closer look at the cooling solution and power adapter](https://preview.redd.it/up5colythxhg1.png?width=3472&format=png&auto=webp&s=0d15f0ddcf217b939ca2f806dad6d97043afd36e)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxrvq9/my_250_24gb_of_vram_setup_still_in_2026/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3yjnbi",
          "author": "Used_Chipmunk1512",
          "text": "Kudos to you, seriously this looks great, do post more of your adventures here",
          "score": 15,
          "created_utc": "2026-02-06 19:56:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxga0",
              "author": "Jack_at_BrewLedger",
              "text": "thanks man :)",
              "score": 1,
              "created_utc": "2026-02-06 21:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yni06",
          "author": "According_Study_162",
          "text": "How many tps?",
          "score": 8,
          "created_utc": "2026-02-06 20:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yoy6u",
              "author": "Jack_at_BrewLedger",
              "text": "30ish",
              "score": 6,
              "created_utc": "2026-02-06 20:22:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zi62r",
                  "author": "Themash360",
                  "text": "At 15GB MoE I'd be expecting a lot more. How much context is that?",
                  "score": 2,
                  "created_utc": "2026-02-06 22:50:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41lajy",
                  "author": "MrScotchyScotch",
                  "text": "that is wild... i have a thinkpad from 2023 and i get 30 t/s with qwen3 coder (q4_k_m). the newer chips really make a difference!",
                  "score": 1,
                  "created_utc": "2026-02-07 07:11:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3zi4o4",
                  "author": "ClimateBoss",
                  "text": "bruh can it run vLLM ?",
                  "score": 1,
                  "created_utc": "2026-02-06 22:50:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zi3tv",
              "author": "Themash360",
              "text": "https://www.techpowerup.com/gpu-specs/tesla-p40.c2878\n\n350GB/s vram, so uh at 15GB MoE I'd be expecting a lot more.",
              "score": 1,
              "created_utc": "2026-02-06 22:50:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zj3k7",
                  "author": "Jack_at_BrewLedger",
                  "text": "It's the pascal architecture that's the limitation, not the ram speed",
                  "score": 3,
                  "created_utc": "2026-02-06 22:55:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ynel2",
          "author": "TMuel1123",
          "text": "Small Tipp. Try a radial fan with an adapter shroud. They can produce more pressure and higher airflow. ",
          "score": 5,
          "created_utc": "2026-02-06 20:14:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ypbvg",
              "author": "Jack_at_BrewLedger",
              "text": "heard. I considered that originally, but this was cheaper to make. That's definitely better, but this with the high pressure fan will keep the thing under 70C at full load constantly, which is all I need,",
              "score": 1,
              "created_utc": "2026-02-06 20:24:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z272p",
          "author": "Klutzy_Ad_1157",
          "text": "Good old P40 never disappoints for LLM :)",
          "score": 3,
          "created_utc": "2026-02-06 21:28:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ykz2z",
          "author": "Purrsonifiedfip",
          "text": "Question because I'm new to the hardware stuff. Just built my first pc. Put a 5070ti in it and finding out its enough to chat, but if I want heavy tasks, I'll need more. Was looking at adding an older 3060 or something of the sort.\n\nDo older gpus need aggressive cooling for LLMs? or would high powered fans on the psu shroud be adequate?",
          "score": 2,
          "created_utc": "2026-02-06 20:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yp57n",
              "author": "Jack_at_BrewLedger",
              "text": "The fans built into the GPUs should work fine, I only have this set up because the Tesla has 0 cooling setup on it. It was designed to be used in a data center with massive fans so it just has a heatsink, you have to design a solution.",
              "score": 2,
              "created_utc": "2026-02-06 20:23:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ypgwo",
                  "author": "Purrsonifiedfip",
                  "text": "ahhh, thanks",
                  "score": 1,
                  "created_utc": "2026-02-06 20:25:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yska7",
          "author": "Decent_Solution5000",
          "text": "Come to my house and make us one. We'll feed you and everything! Seriously, that thing looks like it seriously rocks! Congrats on your genius innovation!",
          "score": 2,
          "created_utc": "2026-02-06 20:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxfk7",
              "author": "Jack_at_BrewLedger",
              "text": "made me smile",
              "score": 1,
              "created_utc": "2026-02-06 21:05:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zmxqt",
                  "author": "Decent_Solution5000",
                  "text": "Mad me a little jelly, ngl, but also made me happy for you. :)",
                  "score": 2,
                  "created_utc": "2026-02-06 23:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yuxac",
          "author": "UnlikelyPotato",
          "text": "I believe P100s are faster with their HBM2 bandwidth. $80 for 16GB cards. 32GB for $160.",
          "score": 2,
          "created_utc": "2026-02-06 20:52:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxd99",
              "author": "Jack_at_BrewLedger",
              "text": "im pretty sure it doesnt come in 32, maybe you're thinking of something else?\n\n",
              "score": 2,
              "created_utc": "2026-02-06 21:04:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yyjg5",
                  "author": "UnlikelyPotato",
                  "text": "Sorry. Two for $80 each is $160. P100s have twice the memory bandwidth of the P40. If you can fit multiple GPUs in your system then P100s are hands down better.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:10:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3zhe2v",
          "author": "No-Leopard7644",
          "text": "Kudos to innovation, great idea and execution",
          "score": 2,
          "created_utc": "2026-02-06 22:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ztvmy",
          "author": "apVoyocpt",
          "text": "I have the p40 too. Under linux (with enough ram) the oss 120b ran surprisingly well!",
          "score": 2,
          "created_utc": "2026-02-06 23:57:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zzthc",
              "author": "Jack_at_BrewLedger",
              "text": "do you mean 20b? or did you get the 120b to run? Iv'e got 48gb of ram to work with\n\n",
              "score": 1,
              "created_utc": "2026-02-07 00:31:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41vdf8",
                  "author": "apVoyocpt",
                  "text": "the 120b. I have 64GB of ddr4 and the p40. It worked really well!",
                  "score": 1,
                  "created_utc": "2026-02-07 08:47:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o419h35",
          "author": "hw999",
          "text": "Put a brace under than card or the fan, gravity is not your friend.",
          "score": 2,
          "created_utc": "2026-02-07 05:29:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45qs1y",
              "author": "MyTwitterID",
              "text": "This.",
              "score": 1,
              "created_utc": "2026-02-07 22:50:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o46ytot",
          "author": "singlebit",
          "text": "very nice! why dont I think the same!",
          "score": 2,
          "created_utc": "2026-02-08 03:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o479x4i",
          "author": "Fit-Rub3325",
          "text": "I am ashamed to call myself as engineer after reading things here on reddit :) ",
          "score": 2,
          "created_utc": "2026-02-08 04:39:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43a6ra",
          "author": "milkipedia",
          "text": "this is worthy of r/homelab",
          "score": 1,
          "created_utc": "2026-02-07 15:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43d4wq",
          "author": "AlexGSquadron",
          "text": "How does this compare to Intel b60 pro?",
          "score": 1,
          "created_utc": "2026-02-07 15:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48nvtp",
          "author": "LaunchAgentHQ",
          "text": "This is the kind of setup that makes local inference practical. The P40s are a steal for the VRAM you get - my only suggestion would be to look into vLLM or llama.cpp with tensor parallelism to actually use both cards together. Single card 24GB is great for 13B-30B models quantized, but if you can get both cards working in tandem you could potentially run 70B models at decent quality. The passive cooling is going to be your main challenge in a standard case though - those cards were designed for server airflow.",
          "score": 1,
          "created_utc": "2026-02-08 12:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dnxgm",
              "author": "Jack_at_BrewLedger",
              "text": "This setup works fine, but id get a sideways blower style fan next time.",
              "score": 1,
              "created_utc": "2026-02-09 04:19:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4eusj5",
          "author": "harktron",
          "text": "That's an awesome use for old hardware. Might have to steal this idea.",
          "score": 1,
          "created_utc": "2026-02-09 10:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rgpan",
              "author": "Jack_at_BrewLedger",
              "text": "DM me if you want and I can give you information about how I did it",
              "score": 1,
              "created_utc": "2026-02-11 07:34:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1jdq3",
      "title": "My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r1jdq3/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/",
      "author": "BetaOp9",
      "created_utc": "2026-02-11 01:14:08",
      "score": 55,
      "num_comments": 41,
      "upvote_ratio": 0.89,
      "text": "I didn't want to buy two systems. That was the whole thing.\n\nI needed a NAS. I also wanted to mess around with local LLMs. And I really didn't want to explain to my wife why I needed a second box just to talk to a chatbot that sometimes hallucinates, I have my father-in-law for that. So when I was specing out my NAS build, I went a little heavier than most people would and crossed my fingers that the system could pull double duty down the road.\n\nHonestly? I was prepared to be wrong. Worst case I'd have an overpowered NAS that never breaks a sweat. I could live with that.\n\nBut it actually worked. And way better than I expected.\n\n**The Build**\n\n* Minisforum N5 Pro\n* AMD Ryzen AI 9 HX PRO 370 (12c/24t, 16 RDNA 3.5 CUs)\n* 96GB DDR5-5600 (2x 48GB SO-DIMMs)\n* 5x 26TB Seagate Exos in RAIDZ2 (\\~70TB usable)\n* 2x 1.92TB Samsung PM983 NVMe (ZFS metadata mirror)\n* TrueNAS SCALE\n\nDay to day it runs Jellyfin with VAAPI hardware transcoding, Sonarr, Radarr, Prowlarr, qBittorrent, FlareSolverr, Tailscale, and Dockge. It was already earning its keep before I ever touched LLM inference.\n\n**The Experiment**\n\nThe model is Qwen3-Coder-Next, 80 billion parameters, Mixture of Experts architecture with 3B active per token. I'm running the Q4\\_K\\_M quantization through llama.cpp with the Vulkan backend. Here's how it actually went:\n\n**3 tok/s** \\- First successful run. Vanilla llama.cpp and Qwen3-Coder-Next Q8 quantization, CPU-only inference. Technically working. Almost physically painful to watch. But it proved the model could run.\n\n**5 tok/s** \\- Moved to Q4\\_K\\_M quantization and started tuning. Okay. Nearly double the speed and still slow as hell...but maybe usable for an overnight code review job. Started to think maybe this hardware just won't cut it.\n\n**10 tok/s** \\- Ran across a note in a subreddit that someone got Vulkan offloading and doing 11 tok/s on similar hardware but when I tried it...I couldn't load the full model into VRAM despite having plenty of RAM. Interesting. I tried partial offload, 30 out of 49 layers to the iGPU. It worked. Now it actually felt usable but it didn't make sense that I had all this RAM and it wouldn't load all of the expert layers.\n\n**15 tok/s** \\- Then the dumb breakthrough. I discovered that `--no-mmap` was quietly destroying everything. On UMA architecture, where the CPU and GPU share the same physical RAM, that flag forces the model to be allocated twice into the same space. Once for the CPU, once for GPU-mapped memory, both pulling from the same DDR5 pool. I couldn't even load all 49 layers without OOM errors with that flag set. Dropped it. All 49 layers loaded cleanly. 46GB Vulkan buffer. No discrete GPU.\n\n**18 tok/s** \\- Still I wanted more. I enabled flash attention. An extra 3 tok/s, cut KV cache memory in half, and significantly boosted the context window.\n\n3 → 5 → 10 → 15 → 18. Each step was one discovery away from quitting. Glad I didn't.\n\n**Results (Flash Attention Enabled)**\n\n* Up to 18 tok/s text generation\n* 53.8 tok/s prompt processing\n* 50% less KV cache memory\n* Fully coherent output at any context length\n* All while Jellyfin was streaming to the living room for the kids\n\nCouldn't I just have bought a box purpose built for this? Yep. For reference, a Mac Mini M4 Pro with 64GB runs $2,299 and gets roughly 20-25 tok/s on the same model. Apple's soldered LPDDR5x gives it a real bandwidth advantage. But then it wouldn't run my media stack, store 70TB of data in RAIDZ2. I'm not trying to dunk on the Mac at all. Just saying I didn't have to buy one AND a NAS.\n\nWhich was the whole point.\n\nNo exotic kernel flags. No custom drivers. No ritual sacrifices. Vulkan just works on RDNA 3.5 under TrueNAS.\n\n**Still On the Table**\n\nI've barely scratched the surface on optimization, which is either exciting or dangerous depending on your relationship with optimizing. Speculative decoding could 2-3x effective speed. EXPO memory profiles might not even be enabled, meaning I could be leaving free bandwidth sitting at JEDEC defaults. Thread tuning, KV cache quantization, newer Vulkan backends with RDNA 3.5 optimizations landing regularly, UMA buffer experimentation, different quant formats.\n\nOn top of all that, the model wasn't even designed to run on standard transformer attention. It was built for DeltaNet, a linear attention mechanism that scales way better at long context. There's an active PR implementing it and we've been helping test and debug it. The fused kernel already hits 16 tok/s on a single CPU thread with perfect output, but there's a threading bug that breaks it at multiple cores. When that gets fixed and it can use all 12 cores plus Vulkan offloading, the headroom is significant. Especially for longer conversations where standard attention starts to choke.\n\n18 tok/s is where I am but I'm hopeful it's not where this tops out.\n\n**The Takeaway**\n\nI'm not saying everyone should overbuild their NAS for an LLM machine or that this was even a good idea. But if you're like me, enjoy tinkering and learning, and are already shopping for a NAS and you're curious about local LLMs, it might be worth considering specing a little higher if you can afford it and giving yourself the option. I didn't know if this would work when I bought the hardware, a lot of people said it wasn't worth the effort. I just didn't want to buy two systems if I didn't have to.\n\nTurns out I didn't have to. If you enjoyed the journey with me, leave a comment. If you think I'm an idiot, leave a comment. If you've already figured out what I'm doing wrong to get more tokens, definitely leave a comment.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r1jdq3/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4qdcjw",
          "author": "qwen_next_gguf_when",
          "text": "96GB DDR5 5600.",
          "score": 15,
          "created_utc": "2026-02-11 02:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rwka4",
              "author": "Ill_Shelter4127",
              "text": "he has ram! get him! ",
              "score": 13,
              "created_utc": "2026-02-11 10:04:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t8yd9",
                  "author": "BetaOp9",
                  "text": "Grab the pitch forks!",
                  "score": 4,
                  "created_utc": "2026-02-11 15:22:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r4kwt",
          "author": "Firestorm1820",
          "text": "Could you post the exact flags you’re using with llama.cpp? Curious to see what I can get on my setup.",
          "score": 7,
          "created_utc": "2026-02-11 05:48:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s7nrm",
              "author": "BetaOp9",
              "text": "Yeah, I'll try to remember in the morning to look to see what I stopped at. What hardware are you running? Welcome to message me.",
              "score": 3,
              "created_utc": "2026-02-11 11:41:39",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4trdon",
              "author": "BetaOp9",
              "text": "vanilla llama.cpp, Vulkan backend\n\nllama-server \\\\\n\n\\-m /models/Qwen3-Coder-Next-Q4\\_K\\_M-00001-of-00004.gguf \\\\\n\n\\-t 12 \\\\\n\n\\-c 4096 \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\\\\n\n\\--port 8080 \\\\\n\n\\-ngl 99 \\\\\n\n\\-fa on",
              "score": 3,
              "created_utc": "2026-02-11 16:48:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4usi3i",
                  "author": "Firestorm1820",
                  "text": "Excellent, thank you! My main rig is a RTX 5000 Ada 32GB + 256GB RAM, I’ve also got a variety of mini PCs with Intel iGPU and 4GB NVIDIA GPUs. Smaller models are becoming more and more intelligent particularly when it comes to tool calling, so offloading inference tasks from my main PC to the mini PCs where possible would be awesome. Your post was exactly what I was looking for to start down that path, ty again.",
                  "score": 2,
                  "created_utc": "2026-02-11 19:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qzx2h",
          "author": "leonbollerup",
          "text": "Try running gpt-oss-20b, set up a server.dev mcp search.. you will find that it’s properly 2x as fast and with search it becomes quite smart",
          "score": 8,
          "created_utc": "2026-02-11 05:11:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r25f9",
              "author": "Icy_Distribution_361",
              "text": "Or possibly even the 120b with this amount of memory. From what I've read the token generation isn't much slower compared to 20b.",
              "score": 2,
              "created_utc": "2026-02-11 05:29:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4t203a",
              "author": "BetaOp9",
              "text": "I'll have to check this out. I may ping you when I do if that's okay.",
              "score": 1,
              "created_utc": "2026-02-11 14:47:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qmaa3",
          "author": "HealthyCommunicat",
          "text": "Its cool to see that the HX chips are decently affordable and are being used everywhere. We’re boutta all be able to inference on our phones in a few years.",
          "score": 3,
          "created_utc": "2026-02-11 03:36:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rdh1y",
              "author": "cramyzarc",
              "text": "Try MNN Chat, works already quite nicely with small models",
              "score": 2,
              "created_utc": "2026-02-11 07:05:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4rvz9c",
                  "author": "HealthyCommunicat",
                  "text": "i had a ai max 395+ and had to return it, too slow for me, i got the dgx spark and returned that too.",
                  "score": 1,
                  "created_utc": "2026-02-11 09:59:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4s8f00",
              "author": "strosz",
              "text": "With ChatterUI on Android I can already use GPT-OSS-20B with about 5-7 tok/s on 16GB ram phone. Probably good to have if stranded somewhere. Got to try other apps as well",
              "score": 1,
              "created_utc": "2026-02-11 11:47:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4sckps",
              "author": "No_Clock2390",
              "text": "Already can on S25 Ultra",
              "score": 1,
              "created_utc": "2026-02-11 12:18:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4taej7",
          "author": "singh_taranjeet",
          "text": "The --no-mmap point is the real PSA here. UMA + Vulkan is exactly where people shoot themselves in the foot by accidentally doubling allocations.\n\nAlso, 18 tok/s on an 80B MoE while the box is doing NAS and Jellyfin duty is a legit “one box to rule them all” flex. Post your final llama.cpp flags when you can, everyone’s going to try to replicate this",
          "score": 5,
          "created_utc": "2026-02-11 15:29:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tvewl",
              "author": "BetaOp9",
              "text": "llama.cpp, Vulkan backend\n\nllama-server \\\\\n\n\\-m Qwen3-Coder-Next-Q4\\_K\\_M-00001-of-00004.gguf \\\\\n\n\\-t 12 \\\\\n\n\\-c 4096 \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\\\\n\n\\--port 8080 \\\\\n\n\\-ngl 99 \\\\\n\n\\-fa on",
              "score": 2,
              "created_utc": "2026-02-11 17:07:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tjap0",
          "author": "Kuusj",
          "text": "I have the normal N5 NAS with 32GB’s, do you still think it can run some models? Not specifically the RAM but the chip overall. I’m sad I chose 32GB half a year back because why ‘would I ever need 96GB’ lol.. \n\nAlso, do you not have problems with the hardware fault in the SATA controller?",
          "score": 2,
          "created_utc": "2026-02-11 16:11:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q435h",
          "author": "timbo2m",
          "text": "Good stuff! I did something similar yesterday. I have different hardware but maybe this will get a few extra tokens per second:\n\n\n--threads 16 --batch-size 512\n\n\nThat gave me 2 tokens per second extra. Small, but it means a lot when it's sub 20 tokens.  Of course, fix threads to 1 or 2 less cores than you have available for llama server. In my case flash attention didn't help so I left it off.\n\nAlso, I wonder if the 2 bit XL quant is accurate enough, it will probably give you 10 tokens per second extra over the 4. Play with your context size too, I find 64k is good middle ground.",
          "score": 1,
          "created_utc": "2026-02-11 01:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs7ou",
              "author": "Shoddy_Bed3240",
              "text": "Try running it with 8 threads and pin the process to the performance cores only using taskset.\n\n\n\nOn Intel CPUs, this makes a huge difference when you restrict the workload to P-cores instead of mixing P-cores and E-cores. I suspect it should work similarly on AMD systems with Zen 5 performance cores as well.\n\n\n\nIt’s definitely worth testing and comparing the results.",
              "score": 2,
              "created_utc": "2026-02-11 04:16:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4qvbu7",
                  "author": "timbo2m",
                  "text": "I'm actually running 2 combos with the same qwen coder next models - an old amd server with a couple of nvidia T4 Tesla cards, comparing with my own gaming machine which is intel i9/4090/32GB, my gaming machine is actually 10 tokens per second faster but it's nice to offload agents to the server. Ideally I can get some Openclaw thing happening when  I get some firewalls in place.",
                  "score": 1,
                  "created_utc": "2026-02-11 04:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4r4303",
              "author": "colin_colout",
              "text": " 768 batch size was the sweet spot on my 780m when i was maining it. \n\nit matches the number of shader cores and improves prompt processing significantly. \n\ni think 16 threads shouldn't be needed since it uses vulkan amd no cpu offload if you set GTT correctly in Linux",
              "score": 1,
              "created_utc": "2026-02-11 05:44:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qqttc",
          "author": "Shoddy_Bed3240",
          "text": "If you want to maximize performance, memory bandwidth is critical. Improving effective memory bandwidth can have a significant impact on overall speed, especially for large models.\n\nThe best tool I’ve found for measuring memory bandwidth is STREAM:  \n[https://github.com/jeffhammond/STREAM](https://github.com/jeffhammond/STREAM)\n\nYou can compile it with:\n\n    gcc -O3 -fopenmp -mcmodel=large -DSTREAM_ARRAY_SIZE=200000000 stream.c -o stream\n    \n\nThen run it to determine your maximum memory bandwidth:\n\n    export OMP_NUM_THREADS=<max_threads>\n    taskset -c <list_of_cores> ./stream\n    \n\nOn my system, I was able to reach around 96–97 MB/s, which allowed me to run a Q8 model at about 10 tokens per second.\n\nIf you're trying to optimize inference speed, it’s definitely worth benchmarking your memory bandwidth first.",
          "score": 1,
          "created_utc": "2026-02-11 04:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r20gd",
              "author": "Icy_Distribution_361",
              "text": "And then you find your memory bandwidth, and then what?",
              "score": 2,
              "created_utc": "2026-02-11 05:28:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r2f5v",
                  "author": "Shoddy_Bed3240",
                  "text": "Next, find a way to maximize your memory bandwidth. LLM is sensitive to memory bandwidth—the more you have, the faster it can generate tokens.",
                  "score": 1,
                  "created_utc": "2026-02-11 05:31:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r3he1",
          "author": "EbbNorth7735",
          "text": "It's a 3B model. Speculative decoding likely wont get you much.",
          "score": 1,
          "created_utc": "2026-02-11 05:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rqszf",
              "author": "BetaOp9",
              "text": "You're probably right that speculative decoding gains won't be as dramatic as they would on a dense 70B. The active parameter count does limit how much a draft model can accelerate things. I'll be honest, I listed it as an optimization I haven't tried yet, not a guaranteed win. I haven't even looked into if anyone's actually benchmarked spec decode on MoE models and has real numbers.\n\nBut...it is 80B total parameters with 3B active per token. The routing layer is still selecting from the full 80B every forward pass. It's not the same workload as a dense 3B model. The memory bandwidth requirements alone make that obvious since a dense 3B doesn't need 46GB of VRAM. Each step has been about trying it and debugging and seeing what we end up with. 🤷‍♂️",
              "score": 1,
              "created_utc": "2026-02-11 09:10:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ry0n7",
                  "author": "EbbNorth7735",
                  "text": "Yeah, I guess if you dump a 1B in vram you may see a few tps",
                  "score": 0,
                  "created_utc": "2026-02-11 10:17:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u3z6a",
          "author": "beedunc",
          "text": "This is the future, especially with panther lake coming. Congrats!",
          "score": 1,
          "created_utc": "2026-02-11 17:48:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wltry",
          "author": "__rtfm__",
          "text": "Congrats! That was my dream and I ran into hardware issues with my n5 pro and returned it. Glad it’s working out for you",
          "score": 1,
          "created_utc": "2026-02-12 01:23:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r6z5d",
          "author": "RiskyBizz216",
          "text": "Absolute beast of a setup, but I agree - a Mac Studio would have been a better value - higher throughput and you would have gotten access to MLX models and 0-day releases.",
          "score": 1,
          "created_utc": "2026-02-11 06:08:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rswjk",
              "author": "BetaOp9",
              "text": "Six months ago if someone said \"just buy a Mac for local inference\" I would have agreed with them. Unified memory, great bandwidth, MLX ecosystem. Nothing else was close.\n\nBut that's not really true anymore. AMD's Strix Halo chips are going toe to toe with Apple Silicon on inference performance. Competitive bandwidth, 128GB of unified memory, and the software stack is catching up fast. My chip is the budget little brother of that architecture and it's still pushing 18 tok/s on an 80B MoE while running my other stuff. I would have spent more, gotten a bit more performance and ended up with a lot less function baked in had I gone with the Mac.",
              "score": 1,
              "created_utc": "2026-02-11 09:30:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rvjhn",
                  "author": "RiskyBizz216",
                  "text": "Since you're going \"NAS first\" I guess that makes sense\n\nBut one correction - if you wanted better LLM performance the mac would hands down be the better solution. That's based on recent benchmarks and test cases.\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1r082v1/qwen3codernext\\_performance\\_on\\_mlx\\_vs\\_llamacpp/](https://www.reddit.com/r/LocalLLaMA/comments/1r082v1/qwen3codernext_performance_on_mlx_vs_llamacpp/)\n\nhttps://preview.redd.it/vd6nbulg7uig1.png?width=1080&format=png&auto=webp&s=bba9bf928b8b1103fb5ef8ea677b5d3285afaeec\n\n  \nAlso there is limited AMD/ROCm support in the LLM community - users are seeing poor performance across the board with this model in particular.\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1qwg58c/qwen3\\_coder\\_next\\_poor\\_performance\\_on\\_r9700s/](https://www.reddit.com/r/LocalLLaMA/comments/1qwg58c/qwen3_coder_next_poor_performance_on_r9700s/)\n\nIf you're just \"tinkering and learning\" and shopping for a NAS, then yeah - a Mac is overkill. Go with Dr. Frankenstein :) ",
                  "score": 2,
                  "created_utc": "2026-02-11 09:55:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qfs5y",
          "author": "UpbeatDraw2098",
          "text": "your a smart dude",
          "score": 1,
          "created_utc": "2026-02-11 02:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q50pi",
          "author": "greatwilt",
          "text": "Nice! This model works well with open claw too, if you can get the context size up above  200 K. Good luck in your optimization journey.",
          "score": -1,
          "created_utc": "2026-02-11 01:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qsixu",
              "author": "timbo2m",
              "text": "I was going to use Kimi K2.5 as the brain and outsource coding and heartbeats to my local qwen 3 coder next 80B. Are you suggesting the brain could be just the local LLM too?!",
              "score": 1,
              "created_utc": "2026-02-11 04:18:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r220hc",
      "title": "ACE Step 1.5 is here: beats Suno on common eval metrics",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/vkrplb696wig1",
      "author": "North-Jeweler-8699",
      "created_utc": "2026-02-11 16:30:07",
      "score": 55,
      "num_comments": 30,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r220hc/ace_step_15_is_here_beats_suno_on_common_eval/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4u2wkg",
          "author": "sleepy_roger",
          "text": "Everyone saying it beats Suno clearly doesn't use Suno. This is great to have locally and I'm excited for 2.0 but saying it beats Suno is just dumb honestly.",
          "score": 35,
          "created_utc": "2026-02-11 17:43:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4udnfp",
              "author": "Decaf_GT",
              "text": "It's just like all the other LinkedIn style posts here...\"LLM #243 just came out and it BEATS OPUS 4.6 no seriously\"\n\nThe *fuck* it does. As fun as local models are, pretending for a moment that the garbage, anime-style generic pop song is anything close to what Suno can produce is utterly deluded. \n\nAs much fun as I have tinkering with local models, I've become so *tired* of the hyperbolic claims here from people who *desperately* want to show that \"local models are always better and they're the future\". \n\nMaking up some bullshit about \"evals\" (really? evals...for something as subjective as music?) and then using an AI to create a bunch of nonsense \"questions\" and throwing it into a Reddit post...ugh.",
              "score": 10,
              "created_utc": "2026-02-11 18:32:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wjoxq",
                  "author": "ptear",
                  "text": "+1 let's definitely stay realistic in this sub, we're all supportive of local models and this is an amazing constantly iterating space.\n\n\nRight now I'm all about right sizing use cases by model. If I can do something locally I love it since I can just depend on my own systems. We can't do everything as great local today, and I totally use hosted models and also have fun experimenting with products like Suno too!",
                  "score": 3,
                  "created_utc": "2026-02-12 01:10:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uek8x",
                  "author": "sleepy_roger",
                  "text": "Yeah it's definitely tiring. I sound paranoid I know but it seems as if a few of these model providers have a marketing and or affiliate scheme setup so you see reddit subs hit with all these stupid posts. Kimi, minimax, and z.ai are the biggest offenders in the llm space by far.",
                  "score": 2,
                  "created_utc": "2026-02-11 18:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u4nen",
              "author": "mintybadgerme",
              "text": "True that.",
              "score": 6,
              "created_utc": "2026-02-11 17:51:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ucu7a",
          "author": "SanDiegoDude",
          "text": "lol no it doesn't, and it's not even close. Don't get me wrong, AceSTEP 1.5 is great for the latest best in breed OSS music generator, but even it's very best sounds like poo compared to just the average Suno output.",
          "score": 6,
          "created_utc": "2026-02-11 18:29:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4udtto",
              "author": "Decaf_GT",
              "text": "It sounds like Suno V3 at best, and Suno is now on V5.",
              "score": 5,
              "created_utc": "2026-02-11 18:33:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uede7",
                  "author": "SanDiegoDude",
                  "text": "Yea, mostly robotic voices, very simple layering of instruments and sounds, and it will still sometimes get lost and lose the key, and good lord I don't think it's ever actually honored the BPM I set for it. I like AceSTEP, it's a lot of fun to take lyrics from songs you like and twist them into new genres, but it's still a toy - nobody is going to release ANYTHING out of AceSTEP over what you can get from the same prompts from the modern cloud music generators, the quality gap is just too wide.",
                  "score": 2,
                  "created_utc": "2026-02-11 18:36:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tqn6v",
          "author": "tim_dude",
          "text": "It's good for making pop. That's it.",
          "score": 1,
          "created_utc": "2026-02-11 16:45:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uymda",
              "author": "nntb",
              "text": "Out of the box It's crazy good at Japanese enka. It's not good at cat sounds,unless you train a Lora which is quite easy then it's amazing at it. There is nothing it does bad when trained.",
              "score": 1,
              "created_utc": "2026-02-11 20:11:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4twswq",
              "author": "FaceDeer",
              "text": "I've been able to get a wide variety of genres out of it.",
              "score": 1,
              "created_utc": "2026-02-11 17:14:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tygrt",
                  "author": "tim_dude",
                  "text": "Yes but is it good?",
                  "score": 2,
                  "created_utc": "2026-02-11 17:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4unopc",
          "author": "raysar",
          "text": "so benchmark suck. Zero humain prefer ace quality against suno v5.",
          "score": 1,
          "created_utc": "2026-02-11 19:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vsej6",
          "author": "MrWeirdoFace",
          "text": "It's a little rough around the edges, to be honest, but I look forward to it improving with future versions.",
          "score": 1,
          "created_utc": "2026-02-11 22:36:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wf3z8",
          "author": "uti24",
          "text": ">Speed: full song under 2s on A100\n\nIt would be great for images, or for text (where you can utilize speed that is faster than information consuming) but what's the point to generate music in under 2s? Can we have like 10 second or 30, but better quality?\n\n>Quality: beats Suno on common eval scores\n\nWhat does it mean? It sounds not better than Suno, not even better than older versions, like, 2 years ago versions? Realisticly? Like Suno 2. (I thing actual version of Suno is 4 or 4.5, or something like that and every new version sonds better than previous, so much better one would not want to return to previous version)\n\nDon't get me wrong, I still love that we have Suno at home, though, that's a lot, it don't have to beat Suno to be great.",
          "score": 1,
          "created_utc": "2026-02-12 00:42:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wkqtf",
              "author": "Creepy-Bell-4527",
              "text": "The reason the generation speed is so important is because you need to generate batches of 10 just to get 1 track which follows the lyrics and doesn't skip entire verses.",
              "score": 3,
              "created_utc": "2026-02-12 01:16:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ylz0i",
                  "author": "uti24",
                  "text": "And to listen to a single generation I need like 30 seconds, so how am I utilizing 2 seconds song generation?",
                  "score": 1,
                  "created_utc": "2026-02-12 10:54:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wjsk0",
          "author": "Technical_Ad_440",
          "text": "not anywhere close i have used it. after 2 generations of the same prompt but random seed it has almost 0 variation. maybe when it gets variation. i used it for 1 night then gave up on it",
          "score": 1,
          "created_utc": "2026-02-12 01:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wwans",
          "author": "Mods_Are_Fatties",
          "text": "What was the prompt used for this? sounds good to me\n\nWould be interesting to see what suno generates with the same prompt",
          "score": 1,
          "created_utc": "2026-02-12 02:26:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o500b23",
          "author": "emperorofrome13",
          "text": "It is fun to use. Trolled patriots fans at the superbowl party",
          "score": 1,
          "created_utc": "2026-02-12 15:59:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50dnmv",
          "author": "AppealThink1733",
          "text": "Does he already have his gguf file to use in comfyui?",
          "score": 1,
          "created_utc": "2026-02-12 17:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vrmmd",
          "author": "Aggravating_Fun_7692",
          "text": "Maybe make your own music, how about that?",
          "score": -1,
          "created_utc": "2026-02-11 22:32:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wklvs",
              "author": "Creepy-Bell-4527",
              "text": "You must be new here",
              "score": 2,
              "created_utc": "2026-02-12 01:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yzuxz",
                  "author": "Aggravating_Fun_7692",
                  "text": "Sorry I hate soulless music, sue me",
                  "score": 1,
                  "created_utc": "2026-02-12 12:43:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxuwh7",
      "title": "Super-light, 90ms latency, runs locally on Apple Silicon. More expressive and prosodic than Elevenlabs.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/gbb6ro2d2yhg1",
      "author": "EmbarrassedAsk2887",
      "created_utc": "2026-02-06 21:43:54",
      "score": 53,
      "num_comments": 18,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxuwh7/superlight_90ms_latency_runs_locally_on_apple/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3zg0hd",
          "author": "LSU_Tiger",
          "text": "Underwhelming technology and that may have been the most boring video I've ever seen. No one wants to watch someone click around a screen for 6 minutes.",
          "score": 9,
          "created_utc": "2026-02-06 22:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zi4gw",
              "author": "EmbarrassedAsk2887",
              "text": "sure! we are not here to please but show the exciting possibilities of a silicon. ",
              "score": -1,
              "created_utc": "2026-02-06 22:50:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zgrco",
          "author": "ProfMooreiarty",
          "text": "The onboarding seems like quite a bit to do.",
          "score": 4,
          "created_utc": "2026-02-06 22:42:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhuys",
              "author": "EmbarrassedAsk2887",
              "text": "its a few mins setup if you choose the hardcore mode! you can reach me out if theres any other issue!\n\nthanks :)",
              "score": 2,
              "created_utc": "2026-02-06 22:48:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o408wmt",
          "author": "TopTippityTop",
          "text": "Very cool! Wish it supported more languages (Portuguese, at least), but I get it. I take that it's open source, right? Any way to fine tune it to other languages? Can it clone voices?",
          "score": 3,
          "created_utc": "2026-02-07 01:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d9rmn",
              "author": "EmbarrassedAsk2887",
              "text": "yes it can clone voices as well. in the upcoming days, we gonna push it to production as well and before easter we will release the model weights as well. \n\nyes we did train it on 14+ languages but specifically for english and french, then spanish. \n\nbut yeah!! here you go, you can be the judge here :) \n\nhere’s one example i generated for you. \n\nhttps://youtu.be/Kyd3TL5p-lQ?si=TvTUUAjsyZhPEYL9",
              "score": 2,
              "created_utc": "2026-02-09 02:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dh6rg",
                  "author": "TopTippityTop",
                  "text": "Very cool! I think the Portuguese needs more training, but it's a start!",
                  "score": 1,
                  "created_utc": "2026-02-09 03:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43zifx",
          "author": "Maximum-Wishbone5616",
          "text": "what ???\n\n# Desktop Only Experience\n\nHey there! BodegaOS is currently only available on desktop. Please visit this downloads page on your desktop computer to experience BodegaOS and all its features.\n\nWTF  \non 2x 5090, 1x 7950x3d, 5x 4k monitors and you still using some crap that cannot recognize desktop? \n\nThat is 2001 knowledge...",
          "score": 1,
          "created_utc": "2026-02-07 17:19:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d64vr",
              "author": "EmbarrassedAsk2887",
              "text": "hey there! we relied on the browser user agent for detecting the device type. apologies for that, now you can visit again— you won’t have a problem now.\n\napologies for any inconvenience. please dm or hit me up anytime with any query you might have.",
              "score": 1,
              "created_utc": "2026-02-09 02:36:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40w9en",
          "author": "somethingdangerzone",
          "text": "More competition in the voice space is great! I look forward to seeing how this develops.\n\nI tried going to your srswti.com/download site above, but I keep getting a black, blank page. It could be me, or maybe there is a site issue.",
          "score": 1,
          "created_utc": "2026-02-07 03:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41kaxl",
              "author": "EmbarrassedAsk2887",
              "text": "hey there, thanks! the post mentions https://www.srswti.com/downloads\n\nyou can try visiting it again? please hit me up or dm if any other queries.",
              "score": 0,
              "created_utc": "2026-02-07 07:03:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o432knh",
                  "author": "somethingdangerzone",
                  "text": "I tried it again on both LibreWolf and Brave, and I got a black blank page both times.if anything else, i will reach out, thanks",
                  "score": 1,
                  "created_utc": "2026-02-07 14:33:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzsynx",
      "title": "Qwen3 Coder Next on M3 Ultra v.s. GX10",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "author": "Imaginary_Ask8207",
      "created_utc": "2026-02-09 03:11:13",
      "score": 51,
      "num_comments": 22,
      "upvote_ratio": 0.96,
      "text": "https://preview.redd.it/rg9mxsm7zdig1.png?width=2294&format=png&auto=webp&s=7400c7c54428910158a160e5e407022cbba24947\n\n[Qwen3-Coder-Next served on GX10](https://reddit.com/link/1qzsynx/video/cfad1dooxdig1/player)\n\n[Qwen3-Coder-Next served on M3 Ultra 512GB](https://reddit.com/link/1qzsynx/video/sawsb3kpxdig1/player)\n\nI'm currently exploring CLI-based coding tools as an alternative to GitHub Copilot.\n\nRight now I'm testing **opencode** (seems to be the most popular open-source option at the moment), and I paired it with the new **Qwen3-Coder-Next** model. It's decent!\n\nThe tasks I tested were pretty simple, just some small refactoring in my toy project and fixing one bug. But that's exactly the point, for these kind of everyday coding tasks, you ain't gonna need Opus 4.6 level of intelligence, a \"small and sweet\" open source model is usually good enough.\n\nI feel like 80b model is a sweet spot for GX10 with 128GB of GPU memory, 8-bit quantized models could be comfortably loaded into the GPU. For comparison between the two devices, you can clearly see M3 ultra gives a higher throughput, but it's 3x the price of GX10.\n\nDo you think going local will be the coming trend?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4df22x",
          "author": "Thump604",
          "text": "This is my agent pick of the day with Step as the planner",
          "score": 7,
          "created_utc": "2026-02-09 03:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4diz60",
              "author": "Imaginary_Ask8207",
              "text": "Great suggestion! I don't think GX10 will be able to load Step-3.5-Flash, I'll try on M3 ultra.",
              "score": 2,
              "created_utc": "2026-02-09 03:47:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dr3e2",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-02-09 04:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dmfxb",
          "author": "NoobMLDude",
          "text": "Going local is already the current trend !!\nYou are in the right subreddit for it. 😉\n\n\nI’ve gone Local since a long time for most of my tasks that are Private.\n\nI’m also trying to educate the community that you don’t need to give your money and personal data to big AI companies. You can download Free and OpenSource models and they are good enough for 90% of users. \n\n\nHere are few examples of Local AI workflows that I use:\n- [Local Meeting Assistant](https://youtu.be/cveV7I7ewTA)\n- [Local Talking Assistant](https://youtu.be/2VHzYy45kPw)\n- [Terminal with AI Context support](https://youtu.be/_sDJBosDznI)\n\n\nI added a few more in the [LocalAI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)\n\nOwen3-Coder-Next looks like a solid model for Coding if you can run a 80B model on your hardware.",
          "score": 5,
          "created_utc": "2026-02-09 04:10:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4e5mlg",
          "author": "HumanDrone8721",
          "text": "As I know someone using a similar DGX Spark setup, they use a specialized replacement for the otherwise excellent nvtop that is not really optimized for Sparks that is called dgxtop:\n\nhttps://github.com/GigCoder-ai/dgxtop\n\nIs vibe coded, but one of the good ones IMHO:",
          "score": 3,
          "created_utc": "2026-02-09 06:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e7gw8",
              "author": "Imaginary_Ask8207",
              "text": "Thanks for sharing! This definitely looks better than nvtop. Is it stable and well-tested?",
              "score": 1,
              "created_utc": "2026-02-09 06:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e884x",
                  "author": "HumanDrone8721",
                  "text": "AFAIK they have terminals open running it on both of their Spark cluster nodes as it shows also the network traffic over the high-speed interfaces (these are indeed FAST), along with the CPU and GPU temps, for me is fascinating how they correlate while doing complex inference. So far I've heard no complains, but I'm a poor so I don't own a Spark or two ;).",
                  "score": 2,
                  "created_utc": "2026-02-09 06:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4eorv5",
              "author": "eleqtriq",
              "text": "Nvtop has been updated and works now",
              "score": 1,
              "created_utc": "2026-02-09 09:38:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h70nx",
                  "author": "HumanDrone8721",
                  "text": "Super, thanks, I will let the \"sparkers\" know about it-",
                  "score": 1,
                  "created_utc": "2026-02-09 18:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gqa1r",
          "author": "kingcodpiece",
          "text": "I can say that it's running a fair bit faster than that Mac example on my GX10 using an FP4MX quant which the Grace Blackwell machines love (I don't have the exact numbers but it was like 65 t/s outout and 370ish prefill) . That's using llama.cpp BTW. It's the first model that's had an acceptable speed vs quality tradeoff on this machine for me.",
          "score": 2,
          "created_utc": "2026-02-09 17:21:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jskzh",
              "author": "Imaginary_Ask8207",
              "text": "How does llama.cpp compare with vLLM on GX10?   \nI am used to serving models through vLLM, but nvfp4 MoE models will crash intermittently, which is quite annoying.",
              "score": 1,
              "created_utc": "2026-02-10 02:54:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k9i9t",
                  "author": "kingcodpiece",
                  "text": "vLLM was a pain to get running. Llama.cpp was much easier to get running and it's faster and so far very stable.\n\nEdit: For anybody interested, the architecture isn't properly implemented in stable pytorch yet (CUDA version is too high, plus ARM CPU is a pain to workaround).",
                  "score": 1,
                  "created_utc": "2026-02-10 04:44:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m0a9i",
          "author": "techlatest_net",
          "text": "M3 Ultra 512GB outpacing GX10 on Qwen3-Coder-Next throughput is no surprise—Apple's unified memory crushes fragmented GPU loads, especially for 80B 8-bit quant that saturates 128GB cleanly.\n\n**Hardware showdown**:\n- **M3 Ultra wins**: Higher t/s across refactors/bug fixes (your vids prove it). Unified 512GB = zero swapping, CLI tools like opencode scream.\n- **GX10 value king**: 1/3 price for \"good enough\" 80B coding. nVidia ecosystem + CUDA upgrades future-proof it over Apple's walled garden.\n\n**Local coding trend locked in**:\n- Opencode + Qwen3-Coder-Next already beats Copilot for toy projects/real work—your \"small and sweet\" thesis is spot on.\n- 80B sweet spot confirmed: Everyday tasks don't need Opus-level IQ, just reliable context handling.\n\n**Stick with GX10** if budget > peak perf. M3 Ultra's premium is for Mac diehards who refuse Docker. Local CLI stacks (opencode/aider) are obliterating SaaS subscriptions—your benchmarks prove the economics work today.",
          "score": 2,
          "created_utc": "2026-02-10 13:28:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qqxcg",
          "author": "catplusplusok",
          "text": "Just saying, NVIDIA Thor Dev Kit is slightly cheaper than DGX Spark and seems to be faster both on paper (twice the compute speed) and practice (less crusty NVFP4 support now after a rough start). Now idea about vs SOTA Mac Studio, but those are expensive.",
          "score": 2,
          "created_utc": "2026-02-11 04:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4el8of",
          "author": "HallTraditional2356",
          "text": "Amazing! How did you host it on the Apple Silicon M3 Ultra to Connect opencode to it ?",
          "score": 1,
          "created_utc": "2026-02-09 09:02:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4es3zv",
              "author": "Imaginary_Ask8207",
              "text": "I server the model with LM Studio, which gives OpenAI compatible endpoints.",
              "score": 3,
              "created_utc": "2026-02-09 10:11:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jk42v",
                  "author": "nickollie_",
                  "text": "How are you finding the latest version of LM Studio’s server?",
                  "score": 1,
                  "created_utc": "2026-02-10 02:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fnvqr",
          "author": "bakawolf123",
          "text": "You are comparing GX10 to maxed M3 Ultra though, you can get one with 256GB (alas there's no 128GB option, only 96GB which seems just shy of decent space for context), it's more like 2x price then and you get an actual working environment on top of being able to run a model in background.\n\nOr you can go even cheaper with AMD AI Max+ 395 (TG is about same as gx10 according to llama.cpp benchmarks, but prefill is slower)",
          "score": 1,
          "created_utc": "2026-02-09 14:09:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g5a24",
          "author": "Vozer_bros",
          "text": "I actually just need a good architecture, and a local model with good tool calling and SWE bench a bit better than 60%.",
          "score": 1,
          "created_utc": "2026-02-09 15:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kj5uo",
          "author": "taiphamd",
          "text": "Should try to run the DGX spark with flashinfer as the attention backend using vLLM. Furthermore vLLM also has MoE tuning which might be able to squeeze out some more tps. See this issue : https://github.com/vllm-project/vllm/issues/17619 but you basically launch benchmark_moe.py and remember to specify max batch_size or else it will try to tune up to 4096 and that might take a week to run.",
          "score": 1,
          "created_utc": "2026-02-10 05:58:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0x3kn",
      "title": "What would a good local LLM setup cost in 2026?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r0x3kn/what_would_a_good_local_llm_setup_cost_in_2026/",
      "author": "Lenz993",
      "created_utc": "2026-02-10 10:20:12",
      "score": 50,
      "num_comments": 75,
      "upvote_ratio": 0.95,
      "text": "If you had a budget of around $5,000 and wanted to set up a local LLM, what hardware would you go for? Would you go for a good PC with a powerful graphics card like an RTX 5090 or a Mac like the M3 Ultra? And why?\n\nI would like to connect and use my own local LMM with OpenClaw in the course of the year (hopefully more powerful local models will come onto the market in 2026).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r0x3kn/what_would_a_good_local_llm_setup_cost_in_2026/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4mil5k",
          "author": "PraxisOG",
          "text": "Honestly 4x 3090 is a solid way to go as a balance of memory capacity and speed, and run something like Qwen coder next super fast. You could also get more vram with an obscure option like 7x AMD V620 for full gpu offload using GLM4.7 or a lot of context with minimax 2.1 and soon 2.2. Though if you don’t want a super loud mining rig type thing get a strix halo box for 2k and call it a day, same vram capacity as 4x3090 but slower and whisper quiet in comparison. ",
          "score": 12,
          "created_utc": "2026-02-10 15:06:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndwvb",
              "author": "Endhaltestelle",
              "text": "Where can you buy 4 x 3090 for $5000 ?",
              "score": 3,
              "created_utc": "2026-02-10 17:33:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ni0oh",
                  "author": "Doug_Fripon",
                  "text": "How much is a 3090 nowadays? A few weeks ago in Europe you could easily get one for 400€. Now 600.",
                  "score": 5,
                  "created_utc": "2026-02-10 17:52:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nf8gt",
                  "author": "AnonymousCrayonEater",
                  "text": "Ebay?",
                  "score": 2,
                  "created_utc": "2026-02-10 17:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4qtw1d",
              "author": "WonderfulEagle7096",
              "text": "Depending on where you live, 4x 3090 could end up being very costly operationally. Assuming 8 hours per day operation (average work day), 4 × 350 W = 1.40 kW or roughly 5000 kWh annually.\n\nWhere I live this will translate to \\~$1800/year in electricity alone, then you'll have another chunk in depreciation. An M3/M4 chip (Macbook Pro/Mac Studio) only consumes <100W and depreciates a lot more gracefully.",
              "score": 2,
              "created_utc": "2026-02-11 04:27:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ozxjv",
              "author": "Citizen_Edz",
              "text": "I don’t think 4x 3090s is a very realistic setup for 5k? That’s atleast 3k in gpus. Then you probably need a thredripper, or other workstation/server ship to support that many pcie devices.",
              "score": 3,
              "created_utc": "2026-02-10 22:01:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pl382",
                  "author": "PraxisOG",
                  "text": "You can throw it in a 1k X299 platform and mining frame and save a grand if you want ",
                  "score": 3,
                  "created_utc": "2026-02-10 23:54:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wchw6",
              "author": "Small_Bee_4655",
              "text": "Is it 24g version of 3090?",
              "score": 1,
              "created_utc": "2026-02-12 00:27:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xzopm",
                  "author": "PraxisOG",
                  "text": "All models of the 3090 are 24g, some are modded for 48g though but are more expensive ",
                  "score": 1,
                  "created_utc": "2026-02-12 07:18:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n21rh",
          "author": "Zyj",
          "text": "2x Strix Halo (2x $2000) + 2 used infiniband network cards (2x $200) + 2x Oculink and a small PSU. Probably around $4500-4600 in\ntotal. Gives you the ability to run 470b models at q4 etc. **with tensor parallelism**\nSpeed won’t be great but good enough probably.",
          "score": 10,
          "created_utc": "2026-02-10 16:38:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q0nzy",
              "author": "flyingbanana1234",
              "text": "i could be wrong but waiting for the 256 gb m5 mac studio for 5500 seems to be a better move\nhigher bandwidth 🫠🫠🫠",
              "score": 2,
              "created_utc": "2026-02-11 01:24:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4szrwq",
                  "author": "Zyj",
                  "text": "Sure, that sounds pretty good too. If the cost doesn‘t go up.",
                  "score": 2,
                  "created_utc": "2026-02-11 14:35:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4opwh4",
              "author": "Lenz993",
              "text": "That's a great idea too! Thank you very much.",
              "score": 1,
              "created_utc": "2026-02-10 21:14:52",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4youd8",
              "author": "alfons_fhl",
              "text": "And why do you prefer \"AMD Strix Halo\" and not from Nvidia? Like \"Asus GX10\"?",
              "score": 1,
              "created_utc": "2026-02-12 11:19:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4msiyb",
          "author": "SimplyRemainUnseen",
          "text": "I'd probably cluster 2 128gb ryzen ai max+ systems. 256gb of unified memory is crazy good. Really good 4bit performance on those systems for LLMs and image generation.\n\nYou can finetune a QAT LoRA with unsloth's workflows to shrink the performance gap for int4 quants. For reference you can get usable speeds on models like GLM 4.7 with int4 quantization.\n\nA friend of mine runs a ComfyUI API on one system and GPT OSS 120B on the other as a house AI system that can do image & video gen. \n\nThat amount of unified memory really opens up a lot of possibilities.",
          "score": 18,
          "created_utc": "2026-02-10 15:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n2dak",
              "author": "Zyj",
              "text": "If you add RDMA capable network cards\nyou speed things up with tensor parallelism",
              "score": 5,
              "created_utc": "2026-02-10 16:40:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o0ken",
                  "author": "jstormes",
                  "text": "Do you know of any posts about this?  I am looking into it for my setup?",
                  "score": 1,
                  "created_utc": "2026-02-10 19:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4q0fs7",
              "author": "flyingbanana1234",
              "text": "i would totally get 2 ryzens if they just had higher bandwidth as i want to mess around with video generation, i might just wait for the m5 256 gb mac studio but im so impatient",
              "score": 3,
              "created_utc": "2026-02-11 01:22:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4njtm0",
          "author": "Own_Atmosphere9534",
          "text": "Macbook M4 PRO MAX 128 gb comes in around $5K.  Best models for this config:\n\n**Llama 3.3 70B Instruct (MLX / GGUF 4-bit/8-bit**  \nstrong reasoning, writing, and instruction following. Supposedly GPT 4 Class inference on local machine\n\n**Qwen3** coder variants (\\~80B MLX)  \n**Qwen3-235B-A22B (MLX 3-bit or 4-bit)**  \nGood coding\n\nand others I’m sure. \n\nRTX 5090 32 GB VRAM  come in new around this price point ($5000).\n\n As others have said here you could stack multiple RTX 3090s and get a pretty good system with large VRAM for \\~5K probably more.\n\ni’ve heard that the new Mac chips perform in 3090 class RTX range (irrespective of  ram size).  Maybe someone can confirm or give a better indication of  performance is \n\nThe real question is how large of model  do you want to  run and how comfortable are you with screwing around with hardware. \n\nUntil recently, I was looking at Nvidia based solutions but did some testing on my 24 gig M4  MacBook,  and found that things like GPT-OSS-20B run extremely well for my usage. So I went ahead and bought a M4  PRO MAX 128 gig.\n\n\n\n\n\n\n\n",
          "score": 7,
          "created_utc": "2026-02-10 18:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qgy12",
              "author": "Boring-Attorney1992",
              "text": "what kind of tokens/s are we ranging here with your Mac 128GB?",
              "score": 3,
              "created_utc": "2026-02-11 03:02:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r5b1d",
                  "author": "Own_Atmosphere9534",
                  "text": "Just ordered it. The MAX MacBooks are on backorder. It’s going to be mid March. There is pretty strong likelyhook that the M5 MAX MacBooks are going to be announced early March. Something to consider if you are looking in this direction. I’m considering canceling but in all likely hood the M5 MAX will be shipping a month later once they are announced so that would be at least April..",
                  "score": 2,
                  "created_utc": "2026-02-11 05:54:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4q9mgv",
          "author": "BetaOp9",
          "text": "People don't always realize clustering is good for parallel tasks, not single-stream inference. Splitting one model across multiple nodes means every layer boundary ships activations over the network. On 10GbE that's about 1.25 GB/s. Your nodes spend more time waiting on each other than doing math. You end up slower than a single fat node. \n\nThat said someone in this thread mentioned InfiniBand between two Strix Halo boxes and that actually offloads this bottleneck significantly. Used ConnectX-4/5 EDR cards run about $150-200 each and give you 100 Gbps, roughly 11 GB/s actual throughput with RDMA. That's 9x faster than 10GbE and makes tensor parallelism viable. 256GB unified memory across two nodes, 470B models at Q4, about $4,500 total. Interesting build. \n\nBut for a single $5K box? Mac Studio M4 Max. 128GB. $3,499.\n\n400 GB/s memory bandwidth, 128GB unified so no VRAM ceiling, silent, low power. Q3CN (80B MoE) would do roughly 40-60 tok/s. Most open models through 2026 will fit in 128GB quantized.\n\nThe RTX 5090 has insane bandwidth (about 1,800 GB/s) but only 32GB VRAM. Anything that fits in 32GB will scream. Anything that doesn't, and that's most of the interesting models, won't run at all without spilling to system RAM and tanking performance.\n\nThe M3 Ultra is last gen. Wait for the M4 Ultra if you want 256GB+ unified but that'll blow past $5K.\n\nFor context I'm running Q3CN on a Minisforum N5 Pro (Ryzen AI 9 HX PRO 370, 96GB DDR5, 16 RDNA 3.5 CUs) and hitting 18 tok/s on the iGPU via Vulkan at about 95 GB/s bandwidth. No discrete GPU. Token gen is bandwidth-bound so it scales roughly linear. A Strix Halo box at 212 GB/s would be around 40 tok/s. The M4 Max at 400 GB/s would push 75+.\n\nSave the $1,500 left over for when the M4 Ultra drops. Sell the Max and upgrade to 256GB at 800 GB/s. That's the real next step for local inference in a tight little box.",
          "score": 5,
          "created_utc": "2026-02-11 02:18:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mb83w",
          "author": "I_like_fragrances",
          "text": "At the moment the rtx 5090 is around 4-4.5k and a decent amount of ram bumps it up higher. For around $7k-$8k id do that. With $4-$5k id do a mac studio or dgx spark.",
          "score": 5,
          "created_utc": "2026-02-10 14:28:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mc2ox",
              "author": "eribob",
              "text": "I would do 4xRTX 3090 in an AM4 system",
              "score": 6,
              "created_utc": "2026-02-10 14:32:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4pen8q",
              "author": "Kaokien",
              "text": "I got a pre-built from cyber power for 4.2k so definitely doable, within the 5k range 64gb ram",
              "score": 1,
              "created_utc": "2026-02-10 23:17:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nyhya",
          "author": "Hector_Rvkp",
          "text": "A very clean option would be a mac studio with 128gb ram, 1 or 2tb SSD, and you max out the chip / bandwidth to fit your budget. I don't think 256gb ram makes sense on apple because the bandwidth is 1000gb/s max, which makes models 128+gb too slow to be useful. (Bandwidth/model size). But a good MoE model fitting inside 128gb would be very usable I believe.",
          "score": 5,
          "created_utc": "2026-02-10 19:07:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ojcmq",
              "author": "Lenz993",
              "text": "So you mean the Apple M4 Max (16-core CPU), 128 GB RAM, 1024 GB SSD?",
              "score": 2,
              "created_utc": "2026-02-10 20:44:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4p8f7j",
                  "author": "mikestanley",
                  "text": "That’s what I bought last week. Setup changedetection.io and pointed it at the Apple Certified Refurb store. Bought that config 19 hours later. With the additional Veteran discount I paid $2825 before tax.",
                  "score": 3,
                  "created_utc": "2026-02-10 22:44:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4o15np",
          "author": "NoobMLDude",
          "text": "Start FREE FIRST.\n\nIf you have not tried it yet, get a taste of Local LLM on whichever device you have.\n\nOnly if you notice issues in the model you run should you put money for hardware. For most users they won’t notice a huge difference. \n\nFor OpenClaw you might need a big model.\nHowever there are some Local LLM pipelines you could try first which have some assistant features like OpenClaw:\n- [Personal Meeting Assistant](https://youtu.be/cveV7I7ewTA)\n- [Personal Talking Assistant](https://youtu.be/2VHzYy45kPw)\n- Tools for coders- Code Assistant, Terminal, Local API, etc\n\nTry out some of the above setups first as a way to check if you could utilize the expensive hardware.",
          "score": 4,
          "created_utc": "2026-02-10 19:19:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o6ckj",
          "author": "Lenz993",
          "text": "Most AI systems recommended the Mac M4 Max with 128 GB because it fits perfectly into the budget and can run large LLMs with 128 GB. That sounds logical to me. In addition, Apple's RAM is supposed to be particularly fast.",
          "score": 4,
          "created_utc": "2026-02-10 19:43:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4mkoka",
          "author": "mpw-linux",
          "text": "# what about a NVIDIA DGX Spark, 3,999.00 ? There was a review of it I saw on YT it looking pretty impressive, runs a version of LInux",
          "score": 7,
          "created_utc": "2026-02-10 15:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mtwsb",
              "author": "InfraScaler",
              "text": "You can get AMD AI MAX+ 395 with 128GB of unified RAM for half that... But also, someone else mentioned 4x3090 and that's probably the way to go to maximise your 5k",
              "score": 8,
              "created_utc": "2026-02-10 16:01:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4njthm",
                  "author": "Look_0ver_There",
                  "text": "Adding to your comment, there's some benchmarks here: [https://github.com/lhl/strix-halo-testing/](https://github.com/lhl/strix-halo-testing/)  \nThis guy compares the DGX Spark to the Strix Halo.  For Prompt Processing the Spark runs at about twice the speed of the Strix Halo, but for token generation the Spark falls back to just \\~10% faster.  This closely reflects both the compute speed differences (ARM vs AMD64) and the memory speed differences between the two machine.   At $3999 vs $2000, you'd want to mostly be doing prompt processing full-time to justify paying twice as much.",
                  "score": 3,
                  "created_utc": "2026-02-10 18:00:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4n8adp",
                  "author": "Expert_Mulberry9719",
                  "text": "be aware that the AMD ones can only use 50% of the memory for the GPU, Apple ones can use 75% of the memory for the GPU.",
                  "score": -5,
                  "created_utc": "2026-02-10 17:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nflsy",
              "author": "SpecialistNumerous17",
              "text": "Asus Ascent GX10 is a clone of the DGX Spark that is $1000 cheaper. I bought this and am very happy with it. The internals are mostly identical to the Spark, except for smaller disk, and so I just attached an external 2TB disk that I had already and it's been great so far.",
              "score": 3,
              "created_utc": "2026-02-10 17:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q0y75",
                  "author": "flyingbanana1234",
                  "text": "have you tried any video generation ?",
                  "score": 1,
                  "created_utc": "2026-02-11 01:26:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ypjmv",
                  "author": "alfons_fhl",
                  "text": "And why do you bought from Nvidia and not from AMD like \"Halo Strix\"?",
                  "score": 1,
                  "created_utc": "2026-02-12 11:25:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n5rtg",
              "author": "AppointmentAway3164",
              "text": "Yep I’m shopping these. Really great solution from nvidia.",
              "score": 2,
              "created_utc": "2026-02-10 16:55:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4no0e0",
              "author": "Financial-Source7453",
              "text": "$3k only for Asus Gx10. 1TB drive instead of 4TB. Copper radiator and plastic body instead of vapor chamber and metal foam case, but totally worth it.",
              "score": 2,
              "created_utc": "2026-02-10 18:19:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4r27hc",
              "author": "J673hdudg",
              "text": "I love my DGX Spark!! I do training runs (e.g. nanochat) as well as inference. Power usage is 90W max. The GB10 GPU does about \\~55 tok/s (over 1000 tok/s batched) for Qwen3-VL-30B and is my workhorse. Same Spark is running 3 other models and Flux for image gen. I still have a 2x3090 rig that can only do a fraction of that and uses 10x as much power.   \n  \nI also have a Strix Halo, but it came pre-installed with Windows that took a few hours to set up w/ a monitor (puke!) and haven't tried to re-image it with Linux yet. The Spark is Linux native and will setup headless out of the box. Amazing.",
              "score": 2,
              "created_utc": "2026-02-11 05:29:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ypp0v",
                  "author": "alfons_fhl",
                  "text": "And why from Nvidia and not from amd like halo Strix?",
                  "score": 1,
                  "created_utc": "2026-02-12 11:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n91h9",
          "author": "LSU_Tiger",
          "text": "Apple M4 Mac Studio Max with 128GB of ram.",
          "score": 4,
          "created_utc": "2026-02-10 17:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q138a",
              "author": "flyingbanana1234",
              "text": "this maybe the 256 gb option. I have an m4 128 gb ready for pickup but seriously debating waiting for the mac studio m5 for the higher bandwidth",
              "score": 2,
              "created_utc": "2026-02-11 01:26:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4svijy",
                  "author": "LSU_Tiger",
                  "text": "I was in the same boat but decided against waiting for the M5. I didn't want to wait an undetermined amount of time for an unknown better level of performance at an unknown price point, so I pulled the trigger and have been enjoying my M4 Max / 128GB for months now. It's a beast.",
                  "score": 2,
                  "created_utc": "2026-02-11 14:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4p9a01",
          "author": "mikestanley",
          "text": "This is what I bought last week. Paid $2825 from Apple as certified refurb with veteran discount.\n\nhttps://preview.redd.it/4pd30mfixqig1.jpeg?width=1219&format=pjpg&auto=webp&s=7e094fd32183651e064d0b82aa1062d570fd2091\n\nI’m still getting started but it can run my large models than my M1 Max 32GB and faster. And I wanted a new Mac anyway.",
          "score": 2,
          "created_utc": "2026-02-10 22:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q1kvb",
              "author": "flyingbanana1234",
              "text": "dang anytime i check it out they only have m1 and m2 in stock !!!",
              "score": 1,
              "created_utc": "2026-02-11 01:29:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q8qor",
                  "author": "mikestanley",
                  "text": "That's what happened to me for a week. The I setup [changedetection.io](http://changedetection.io) in a Docker container on my NAS and pointed it at the Apple Certified Refurb store. Bought that config 19 hours later. I left the query running for a few days - that config shows up regularly, it is just bought within 1-2 hours.",
                  "score": 2,
                  "created_utc": "2026-02-11 02:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4spv36",
              "author": "Remote-Reception-958",
              "text": "Thatsndo damn cheap, in south america is at least 1000 more ",
              "score": 1,
              "created_utc": "2026-02-11 13:41:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n5kk5",
          "author": "AppointmentAway3164",
          "text": "Nvidia DGX Spark.",
          "score": 2,
          "created_utc": "2026-02-10 16:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mspx8",
          "author": "MimosaTen",
          "text": "I would think about 5/6.000 €",
          "score": 1,
          "created_utc": "2026-02-10 15:55:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n6k5u",
          "author": "esmurf",
          "text": "Gfx card alone 1-2000 usd. More if you have the money.",
          "score": 1,
          "created_utc": "2026-02-10 16:59:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4skl5b",
          "author": "Glad_Middle9240",
          "text": "DGX Spark.  128gb unified ram.  nvidia stack so you don’t have to mess with Mac/rocm.  Room to grow with 200gb infiniband networking built in.   Add a second when you want more capability.",
          "score": 1,
          "created_utc": "2026-02-11 13:11:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ypsgg",
              "author": "alfons_fhl",
              "text": "why from Nvidia an not from AMD?",
              "score": 1,
              "created_utc": "2026-02-12 11:27:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4z6611",
                  "author": "Glad_Middle9240",
                  "text": "To each their own but:  \n\\#1 CUDA > ROCm all day every day.  Support is just more mature  \n\\#2 Built in 200GB infiniband networking for clustering  \n\\#3 Complete ecosystem preinstalled + playbooks at [build.nvidia.com/spark](http://build.nvidia.com/spark)  \n\\#4 Prompt processing on the GB10 chip blows away Max 395+.  Becomes important when you get beyond simple chats.\n\nThe AMD product is great for the price, but it has some limitations.  Spark does as well -- but at least you are dealing with with NVIDIA hardware so compatibility is better.",
                  "score": 1,
                  "created_utc": "2026-02-12 13:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4te167",
          "author": "TheRiddler79",
          "text": "My decision would come down to what model I wanted to run.\n\nPersonally, if you shop it exactly correct, you could get between 5 and 8 of the 32 GB V100, on a server board that doesn't require pcie, just the board is where they sit.\n\nThe reason I would go that route is because for the money, there's probably not a way to get more vram, and the limitations of what you run are basically entirely dependent upon how much fits into the ram or vram.\n\nIf you shop exactly right and get a little lucky and find eight of those in a board, you can run Minimax 2.1, you would feel like you had the most powerful AI on Earth. And it'd be able to run 50 tokens a second to 10 users at once",
          "score": 1,
          "created_utc": "2026-02-11 15:46:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yvnuv",
              "author": "running101",
              "text": "where is a good place to procure the 32gb v100 at a good price? \n\nAlso I thought it is better to get one big card vs many small cards for inference?",
              "score": 1,
              "created_utc": "2026-02-12 12:14:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n4m5x",
          "author": "fasti-au",
          "text": "Depend on uostres.  Naked you want qwen 30b with decent cintext as a bench but it was different when you had uostreamnipen ai etc not allowing agentvaccess  with subs",
          "score": 0,
          "created_utc": "2026-02-10 16:50:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4naja7",
          "author": "That-Cost-9483",
          "text": "No, the best thing for LLM solely would be to have the GPU/CPU/systemRAM/vRAM all tied together. Just one big soldered heap. This is kind of the direction apple has been going for awhile but obviously for AI you want cuda, so the spark is the best option. Other options would be to build a something… you have quite a large budget. I’ve seen wild 3090 builds, these things are zip tied to racks and feeding into servers to utilize the enterprise ram and power supplies.\n\n5090 in a gaming PC is way more practical. We will have to see what bolt graphics (Zeus) brings us… being able to add memory to a graphics processor is either the greatest thing that will ever happen or those guys are stealing everyone’s money. Then we will have to wait for nvida to steal their research and give us the RTX7095 vRAM 64gb extendable to 256 🙏",
          "score": 0,
          "created_utc": "2026-02-10 17:17:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r26mw9",
      "title": "Getting ready to send this monster to the colocation for production.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r26mw9",
      "author": "Ok_Stranger_8626",
      "created_utc": "2026-02-11 19:14:58",
      "score": 50,
      "num_comments": 16,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r26mw9/getting_ready_to_send_this_monster_to_the/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4us5jd",
          "author": "MinimalDistance",
          "text": "Awesome! Thanks for sharing details of the stack. I wonder about estimate costs of building and running a box like this - can you provide some numbers? Thanks!",
          "score": 5,
          "created_utc": "2026-02-11 19:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v179v",
              "author": "Ok_Stranger_8626",
              "text": "All told, it's about $25K worth of gear at today's prices(largely due to the RAM and disk). We assembled it from some spare parts we had, and the rest was purchased from The Server Store. All in all, we figure $12K-ish when we first assembled it.",
              "score": 3,
              "created_utc": "2026-02-11 20:24:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uz652",
          "author": "Anarchaotic",
          "text": "You've built this for a client I assume? Time to rake in that sweet sweet \"ongoing support\" $ to make sure everything keeps working properly and you can update containers/libraries as needed. \n\nHow easily could you redeploy something like this if you had to? From a business perspective it could make sense to have the projects/scripts all relatively automated so you can turn-key another one of these.",
          "score": 3,
          "created_utc": "2026-02-11 20:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v2rmm",
              "author": "Ok_Stranger_8626",
              "text": "We built it initially to do a proof-of-concept for the \"Experts\". It was purely a technical exercise to begin with. \n\nBut after the last few weeks, we decided to make it a multi-tenant system(we have it tied into our SSO system and realms so clients can log into it and have their employees grouped so data doesn't cross-domains.\n\nBut we have all the setup scripts, python code and injectable prompts safely stored away in our GitLab host so we could easily replicate the setup at any time.\n\nThe big ticket is the all the Federal Statutes we ingested and the related instructions(CFRs) from the federal registry. It took us a couple weeks to get them all in properly, and we update them every weekend now, so it's a pretty valuable asset that we can sell/subscribe to law firms, CPAs or compliance consultants.\n\nWe're also working with a Medical compliance officer to develop a Medical Compliance Expert(HIPAA/GDPR/State Level) at the moment, we hope to have that one done in a month or so.",
              "score": 3,
              "created_utc": "2026-02-11 20:31:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v20oq",
          "author": "Hydroskeletal",
          "text": "for something like Federal Law expert are you basically doing a lot of prompt engineering by sticking the US Code into the vector DB? super curious if you can talk about the high level concepts you are using",
          "score": 3,
          "created_utc": "2026-02-11 20:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v3upn",
              "author": "Ok_Stranger_8626",
              "text": "So, we use some pretty tight prompts, yeah. We basically force the model to ignore it's own \"helpful assistant\" role, and behave more like a professional expert. That helped us get around the hallucination problem by a lot. The other thing we do is only allow each expert to research in their specific \"Title\" of the federal code, which really helps, as it limits their expertise to just that ONE particular statue/instruction \"manual\". For example, the \"Tax Expert\" only has access to Title 26 and the IRS CFR.",
              "score": 7,
              "created_utc": "2026-02-11 20:37:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4x7uiw",
                  "author": "Hefty_Development813",
                  "text": "How do you train one specific expert with an MOE?",
                  "score": 1,
                  "created_utc": "2026-02-12 03:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v4so2",
          "author": "dave-tay",
          "text": "Beautiful. Price?",
          "score": 2,
          "created_utc": "2026-02-11 20:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v57iy",
              "author": "Ok_Stranger_8626",
              "text": "We estimate now that prices have gone up, somewhere between $20K and $25K US. Back when we did it last summer, probably closer to $12k to $16K US.\n\nEDIT: That would be our cost for just the hardware. It would definitely not get sold for that, considering the customized containers, python code, specialty prompts and the RAG database. We've put hundreds of hours into building the software stack, so I'd say, if we were to retail it, we'd probably price the box around $65K US for a functional \"turn-key\" device.",
              "score": 6,
              "created_utc": "2026-02-11 20:43:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4we6s7",
                  "author": "Much-Researcher6135",
                  "text": "Thanks, I was wondering what the market was like for all this stuff. Are customers needy or pretty hands-off? Or do you guys not even do service contracts?\n\nNeat post!",
                  "score": 3,
                  "created_utc": "2026-02-12 00:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xsxy9",
          "author": "ridablellama",
          "text": "very cool post. i love to see real business applications of local",
          "score": 2,
          "created_utc": "2026-02-12 06:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wqm22",
          "author": "chrisbliss13",
          "text": "How much you paying for colo",
          "score": 1,
          "created_utc": "2026-02-12 01:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wtfn0",
              "author": "Ok_Stranger_8626",
              "text": "Right now, it's about $325/mo for the 6U and low power for my other gear. \n\nThis one, when it goes down there will add about $550/mo because it needs way more energy.",
              "score": 2,
              "created_utc": "2026-02-12 02:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zeqpj",
          "author": "Lonely_Love4287",
          "text": "so cool which i had the funds to do  this",
          "score": 1,
          "created_utc": "2026-02-12 14:11:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyl7ac",
      "title": "What 's a realistic comparison betwee what you can run on a 512GB ram M3 Ultra vs frontier models",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qyl7ac/what_s_a_realistic_comparison_betwee_what_you_can/",
      "author": "rwvyf",
      "created_utc": "2026-02-07 18:22:06",
      "score": 49,
      "num_comments": 26,
      "upvote_ratio": 1.0,
      "text": "I’m looking for real-world impressions from the \"high-RAM\" club (256GB/512GB M3 Ultra owners). If you've been running the heavyweights locally, how do they actually stack up against the latest frontier models (Opus 4.5, Sonnet 4.5, Geimini 3 pro etc)\n\n* Coding in a relative large codebase, python backend, javascript front end \n* Best-quality outputs (not speed) for RAG over financial research/report + trading idea generation, where I focus on:\n   * (1) data quality + retrieval, \n   * (2) verification/compute\n   * (3) a multi-pass reasoning pipeline.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qyl7ac/what_s_a_realistic_comparison_betwee_what_you_can/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o44qdz3",
          "author": "TokenRingAI",
          "text": "Dual M3 Ultra 512GB will allow you to run Kimi 2.5 at a reasonable speed, which is 100% a frontier model.\n\nIf M5 ultra shows up with a bump in compute, memory bandwidth, 768G ram, and a 15K price tag, which seem like reasonable possibilities, then we will have frontier level performance on a single mac",
          "score": 41,
          "created_utc": "2026-02-07 19:33:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o455aeg",
              "author": "SpicyWangz",
              "text": "768 M5 ultra would go so hard. That could start seriously eating into nvidia’s prosumer market share. ",
              "score": 16,
              "created_utc": "2026-02-07 20:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45tpnn",
                  "author": "TokenRingAI",
                  "text": "If the M5 Ultra utilizes the memory speeds of the M5 generation while maintaining the 1024-bit bus width of the M3 Ultra, it would achieve a theoretical memory bandwidth of approximately 1,224 GB/s",
                  "score": 13,
                  "created_utc": "2026-02-07 23:07:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45gipz",
                  "author": "segmond",
                  "text": "NOT could, but WOULD",
                  "score": 5,
                  "created_utc": "2026-02-07 21:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45zeep",
              "author": "mewnor",
              "text": "Don’t get me excited like that.",
              "score": 4,
              "created_utc": "2026-02-07 23:43:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ytow",
          "author": "false79",
          "text": "I would not allow any LLM to do backtesting. You're just asking for trouble when it's real-time and the results are so far apart.",
          "score": 5,
          "created_utc": "2026-02-07 20:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44z57p",
              "author": "rwvyf",
              "text": "That’s for sure. Thats what the codebase is for.",
              "score": 5,
              "created_utc": "2026-02-07 20:19:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o478wqg",
          "author": "Technical_Buy_9063",
          "text": "I run a single m3 ultra v3 512gb - I run Qwen-3-coder-next as a virtual assistant - clearly some delay, but for now I'm getting all of my LLM use out of it. I use ChatGPT 5.3 to craft some prompts and such for it, but this is more of a expediting measure than anything. Not exactly what you're asking but also I'm not fully optimized at all - this is just a raw setup for now. ",
          "score": 4,
          "created_utc": "2026-02-08 04:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45ggcx",
          "author": "segmond",
          "text": "Kimi K2.5 & DeepSeekv3.2 are just as good as SOTA and can 100% serve as substitutes.\n\ngpt-oss-120b, GLM4.7, MiniMax2.1 will handle 90% of SOTA needs.",
          "score": 6,
          "created_utc": "2026-02-07 21:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46olbb",
          "author": "desexmachina",
          "text": "Do you guys trust it without web context?",
          "score": 2,
          "created_utc": "2026-02-08 02:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49cjz6",
              "author": "AnxietyPrudent1425",
              "text": "That’s why you add web context in LM Studio",
              "score": 2,
              "created_utc": "2026-02-08 14:49:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48gmqo",
          "author": "Hector_Rvkp",
          "text": "I've been comparing hardware options from Strix halo to stuff costing 15k$. I don't think 256+gb apple vram makes sense currently because the bandwidth is too low, even at c.1000gb/s. Token speed is bandwidth/model size, so you can't run a dense model, obviously, but even MoE models that take 128gb+ call layers so big that token speed is still unusable. Afaik, sweet spot is moe model fitting on 128gb ram. The penalty seems to be 5-25pc performance, and what that means in practice is anybody's guess, there's no standard. But comparing quality without factoring in speed / usability is somewhat irrelevant.",
          "score": 2,
          "created_utc": "2026-02-08 11:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ael0o",
              "author": "JonasTecs",
              "text": "So what is best from your comparison?",
              "score": 2,
              "created_utc": "2026-02-08 17:58:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l84je",
                  "author": "Hector_Rvkp",
                  "text": "\"best\" depends on budget. Strix halo 128gb @2k$ is cheapest entry point for legit LLM work. \"Legit\" isn't without caveats. From 3-4k$ upwards, it gets less clear. Apple is great, Nvidia dgx spark CAN make a lot of sense, and Nvidia GPUs can destroy everything in some use cases. There's also a time function (N1X might come out, new M5 configs will come out).",
                  "score": 1,
                  "created_utc": "2026-02-10 09:50:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44gz5w",
          "author": "Powerful-Street",
          "text": "It’s shit compared to Claude or Gemini. Context is limited so much, that a large codebase gets hallucinated. Nothing local comes close.",
          "score": -2,
          "created_utc": "2026-02-07 18:45:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44vnh9",
              "author": "redditor_420_69_lol",
              "text": "Claude and Gemini are just doing context compression and pretending to have super long context, which you can totally do locally",
              "score": 11,
              "created_utc": "2026-02-07 20:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o454vhq",
                  "author": "XccesSv2",
                  "text": "How can i Do this locally?",
                  "score": 1,
                  "created_utc": "2026-02-07 20:50:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o49vapy",
                  "author": "Powerful-Street",
                  "text": " When you have a program with 10k lines of spec, local rag, doesn’t have near enough information for the model to find the correct context. I have tried other things and all will get lost if you don’t know what you are pointing them to. Also, Gemini Ultra has much more context, and it still gets it wrong, most times. I now use one instance of Gemini or Claude to plan a debug session and will use another instance, to follow in the past of the last—if the knowledge base is clean. None are perfect in large codebase, which is what the OP asked about. I have found ways to work with Claude code or Gemini on its ultra plan. Local with RAG doesn’t even touch them yet.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:25:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44i403",
              "author": "rwvyf",
              "text": "That’s sad to hear",
              "score": 0,
              "created_utc": "2026-02-07 18:51:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44d18i",
          "author": "ComfyUser48",
          "text": "Following",
          "score": 0,
          "created_utc": "2026-02-07 18:26:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx51zc",
      "title": "OpenClaw with local LLMs - has anyone actually made it work well?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qx51zc/openclaw_with_local_llms_has_anyone_actually_made/",
      "author": "FriendshipRadiant874",
      "created_utc": "2026-02-06 02:14:48",
      "score": 48,
      "num_comments": 135,
      "upvote_ratio": 0.8,
      "text": "I’m honestly done with the Claude API bills. OpenClaw is amazing for that personal agent vibe, but the token burn is just unsustainable. Has anyone here successfully moved their setup to a local backend using Ollama or LM Studio?\n\nI'm curious if Llama 3.1 or something like Qwen2.5-Coder is actually smart enough for the tool-calling without getting stuck in loops. I’d much rather put that API money toward more VRAM than keep sending it to Anthropic. Any tips on getting this running smoothly without the insane latency?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qx51zc/openclaw_with_local_llms_has_anyone_actually_made/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3u6x0y",
          "author": "regjoe13",
          "text": "I just played with it,  taking over Signal to my local gpt-oss-120b in lmstudio\nI installed openclaw under nologin user on my Linux, locking permissions to a particular folder.  \nIt was fun to play with it, but nothing it does is really worth the risk of having it for me.",
          "score": 9,
          "created_utc": "2026-02-06 03:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uapf9",
          "author": "DataGOGO",
          "text": "Yes, but I wouldn’t run it until they fix the code / massive security holes.\n\nVibecoded slop.",
          "score": 40,
          "created_utc": "2026-02-06 03:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3urjku",
              "author": "onethousandmonkey",
              "text": "This is the correct answer.",
              "score": 10,
              "created_utc": "2026-02-06 05:38:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4plg7t",
              "author": "FindingAwake",
              "text": "What LLM did you use?",
              "score": 1,
              "created_utc": "2026-02-10 23:56:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4qq38v",
                  "author": "DataGOGO",
                  "text": "Hu?",
                  "score": 1,
                  "created_utc": "2026-02-11 04:01:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vsmvv",
              "author": "betahost",
              "text": "I believe it has to do more that local models are more prone to prompt injection than SOTA models such as Claude or GPT",
              "score": 1,
              "created_utc": "2026-02-11 22:38:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3yp2b6",
              "author": "Decent-Freedom5374",
              "text": "Why don’t you just build a model that circulates and isolates, protecting your code only but still allowing quantum gain and make it derive any malicious intent for subsequent gain",
              "score": -2,
              "created_utc": "2026-02-06 20:23:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3yvv87",
                  "author": "DataGOGO",
                  "text": "What?\n\nOh…. Sorry:\n\nGLM-47-flash NVFP4 and qwen3-30b instruct-2507-NVFP4 ",
                  "score": 2,
                  "created_utc": "2026-02-06 20:57:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zzjxa",
              "author": "HealthyCommunicat",
              "text": "the security flaws that they have are the same exact security flaws we've always had. if you aren't doing ur due dilligence to check every layer of the application to make sure that it just technologically and physically cannot do xyz then you shouldn't really have any security concerns.\n\n  \nin short, you can address the security concerns yourself.",
              "score": -4,
              "created_utc": "2026-02-07 00:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o407pfo",
                  "author": "DataGOGO",
                  "text": "You can address the issues yourself, but that involves a rewriting a LOT of the source code…. Most of it in fact.\n\nThere is no way to fix them without recoding. ",
                  "score": 2,
                  "created_utc": "2026-02-07 01:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3v4y4r",
              "author": "Boring-Attorney1992",
              "text": "why do you think it's vibecoded? or is that just your way of criticizing the app?",
              "score": -20,
              "created_utc": "2026-02-06 07:31:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3v6fj7",
                  "author": "Thomdesle",
                  "text": "Look up statements of the creator and check out the  source code.",
                  "score": 12,
                  "created_utc": "2026-02-06 07:44:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3vlpfq",
                  "author": "andrewfenn",
                  "text": "The creator literally says on social media that he doesn't even look at the code. When someone demanded he fixed the security issues he just put his hands up 🤷‍♂️ and said he doesn't know what to do. So yes, it's 100% slop and you can't trust that guy at all.",
                  "score": 8,
                  "created_utc": "2026-02-06 10:10:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wo1wf",
                  "author": "noctrex",
                  "text": "Here you go: [https://www.youtube.com/watch?v=8lF7HmQ\\_RgY](https://www.youtube.com/watch?v=8lF7HmQ_RgY)\n\n\"I ship code I don't read\"",
                  "score": 3,
                  "created_utc": "2026-02-06 14:30:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3weldn",
                  "author": "DataGOGO",
                  "text": "…. Did you look at the code?",
                  "score": 1,
                  "created_utc": "2026-02-06 13:40:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vgthg",
          "author": "dragonbornamdguy",
          "text": "Using it with qwen3 coder 30b, its awesome. Setup was undocumented hell. Works very well. He can create own skills only by telling him.",
          "score": 13,
          "created_utc": "2026-02-06 09:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhrso",
              "author": "Technical_Buy_9063",
              "text": "can you share your setup? is it LM Studio?",
              "score": 1,
              "created_utc": "2026-02-06 22:48:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41icxd",
                  "author": "GreaseMonkey888",
                  "text": "I actually told OpenClaw to configure local LMstudio and Ollama by testing the endpoints of the providers. After some iterations it worked and I could switch over to local providers. At some point I tried to use the working configuration in another VM with OpenClaw, but it had to almost start over configuring it self, although I gave it the config snippets of the previous working one… However, I have a Mac Studio M4 with 64GB, but prefill phase is slow, OpenClaw seems to push so much context into the LLM that it takes very long for every response, no matter how small the model is.",
                  "score": 1,
                  "created_utc": "2026-02-07 06:45:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mq2ez",
              "author": "PaulShroom",
              "text": "I got it working on a 5090 laptop 24gb vr but it was slow, but when I run the same llm on lm studio it super nippy think I need to do a different setup. ",
              "score": 1,
              "created_utc": "2026-02-10 15:43:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0qbq",
                  "author": "dragonbornamdguy",
                  "text": "I run mainly vllm because of nvlinked 3090s. But when new models are released best out of box experience is indeed with LM Studio.",
                  "score": 1,
                  "created_utc": "2026-02-10 16:32:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3tywva",
          "author": "NoobMLDude",
          "text": "> I’d much rather put that API money toward more VRAM than keep sending it to Anthropic.\n\nThis is the right way !! 🫡 \nI’m trying to educate more users to realize this and run their own models for free than pay some company that is going to use your data against you in few months.\n\nQwen3Coder or Qwen3Coder-Next is decent for tool calling and agentic uses.\n\n[https://qwen3lm.com/coder-next/](https://qwen3lm.com/coder-next/)\n\nI’ve not used OpenClaw due to the security loopholes discovered.\n\nHowever if you wish to try other more secure uses for Local LLMs, here are a few simple examples\n- Private Meeting Assistant \n- Private Talking Assistant \n- The usual Coding Assistants\n- terminal with AI support \n\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 17,
          "created_utc": "2026-02-06 02:29:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uicyy",
              "author": "Electronic_Muffin218",
              "text": "Alright, I'll bite - what's the best way to get adequate hardware for these things? Is there some sort of good - better - best (with ballpark prices or not) for nominally consumer-available GPUs (and whatever else matters)? I'm wondering specifically if 48GB is a useful sweet spot, and if so, is there a meaningful performance difference between buying two 24GB cards and just one 48GB card.\n\nIs there a guide to these things that folks keep up to date a la the NUC buyer's guide/spreadsheet? I could of course ask (and have asked) the commercial LLMs themselves, but I'm never sure what they're wrong about or leaving out.",
              "score": 5,
              "created_utc": "2026-02-06 04:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uut8l",
                  "author": "NoobMLDude",
                  "text": "TLDR; You can run with whatever device you have available to try it out. \n\nDisclaimer: I’ve not tried OpenClaw, all comments below is for agent workflows that do similar things locally.\n\nAll of the above tools currently run on my MacBook M2 Max 32GB laptop without any additional GPUs.\n\nI was considering upgrading to bigger GPUs but the rate at which open Source models are improving, I think i might not even need to upgrade. \n\nThe smaller models are already decent enough for those tasks. Of course the huge models would perform better for tool-calling, but for me the marginal improvements does not justify the huge costs of hardware.\n\n- 2x24GB VRAM can run the same models as single 48GB VRAM.\n- generally higher the VRAM the larger models you can run\n\nPrices are skyrocketing. So don’t buy before you have tried cheaper alternatives. You might not even notice huge differences.",
                  "score": 2,
                  "created_utc": "2026-02-06 06:03:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wrg90",
                  "author": "HuckSauce",
                  "text": "Get a AMD Strix Halo mini pc or laptop (Ryzen AI Max) - 128 GB VRAM for 2-3k",
                  "score": 2,
                  "created_utc": "2026-02-06 14:48:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xb72u",
                  "author": "unique-moi",
                  "text": "One thing to keep in mind is that PCs running one GPU are a commodity, while PCs with two high speed PCI slots and a powerful power supply are specialist.",
                  "score": 2,
                  "created_utc": "2026-02-06 16:24:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45rqrz",
              "author": "Dispo96",
              "text": "When you say security loopholes what do you mean?",
              "score": 1,
              "created_utc": "2026-02-07 22:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o477sqy",
                  "author": "NoobMLDude",
                  "text": "A lot of issues have been published.  \nTLDR; Giving access to all your accounts (telegram, email, etc) to anyone human or AI is not a good idea. Main issues: Private Data access + exposure to Unsecured content + Ability to externally communicate  \n  \n [https://www.paloaltonetworks.com/blog/network-security/why-moltbot-may-signal-ai-crisis/](https://www.paloaltonetworks.com/blog/network-security/why-moltbot-may-signal-ai-crisis/)",
                  "score": 1,
                  "created_utc": "2026-02-08 04:24:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o401q80",
              "author": "HealthyCommunicat",
              "text": "the security loopholes in openclaw are the same security loopholes in any agentic bot that has access to a bunch of tools and high autonomy. if you lack the knowledge to see that then you needa go all the way back to the basics and make sure u are fully capable of going through the code and files and making sure that there just isnt anything that can physically be taken advantage of.\n\n  \nexample, if you make sure that it is physically not possible for ur model to run the command \"rm\" or \"rm -rf\" then you wouldnt be worrying about it being able to delete things. if you dont have ur bot able to be reached whatsoever to public internet, then u truly dont have to worry about anything.\n\n  \nlets stop talking about security flaws like they cant be fixed with really easy steps.",
              "score": 1,
              "created_utc": "2026-02-07 00:42:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o47w0xh",
                  "author": "NoobMLDude",
                  "text": "Of course they can be fixed. \n\nIt’s just that people are running these agents without understanding the risks they present.\n\nSo it is the responsibility of the people familiar with the tech to educate and inform those who are not familiar with it, and the responsibility of people running it to understand the risks and apply the fixes.",
                  "score": 1,
                  "created_utc": "2026-02-08 07:47:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u152z",
          "author": "Antique_Juggernaut_7",
          "text": "I got glm4.6v to behave relatively well so far -- been trying it for the past 24h on a dual DGX Spark setup and vLLM. It weirds out at times, but is generally helpful and functional.\n\nI chose this particular model for its image processing capabilities and overall model size. It works for openclaw with a slight change on its chat template.",
          "score": 4,
          "created_utc": "2026-02-06 02:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub6d9",
              "author": "DataGOGO",
              "text": "Try with my 4.6V-NVFP4 quant, it works really well\n\nhttps://huggingface.co/GadflyII/GLM-4.6V-NVFP4",
              "score": 2,
              "created_utc": "2026-02-06 03:43:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uk2bs",
                  "author": "Antique_Juggernaut_7",
                  "text": "Great stuff! Can you share your vllm serve command? I've been having trouble getting NVFP4 to run well in my cluster due to some GB10 shenanigans...\n\nEDIT: wrote before actually checking the HF page. Thanks for adding it there. Are you running this in a DGX Spark?",
                  "score": 1,
                  "created_utc": "2026-02-06 04:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3vcgo7",
              "author": "edmerf",
              "text": "I work on DGX Spark with vLLM and made it work with LLama4-Scout-17b-16e-instruct-NVFP4. However I still couldn't manage to find a perfect chat template. Chat flow is really digsusting. What kind of template do you use and how do you derive it to make it work with OpenClaw?",
              "score": 2,
              "created_utc": "2026-02-06 08:41:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vm01u",
                  "author": "Antique_Juggernaut_7",
                  "text": "The issue with running GLM4.6 is that OpenClaw expects a \"developer\" role, but GLM4.6's chat template only accepts \"system\". So you just need to change that particular line in the chat template to make it run.",
                  "score": 1,
                  "created_utc": "2026-02-06 10:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3x9w9v",
              "author": "unique-moi",
              "text": "How about Claude code self-hosted (I mean pointing at a self-hosted LLM)?",
              "score": 1,
              "created_utc": "2026-02-06 16:18:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xo6yq",
                  "author": "Antique_Juggernaut_7",
                  "text": "Haven't tried yet. Will try and see how it behaves.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44n7dp",
              "author": "scottybowl",
              "text": "Can I ask how you got vllm working? Struggling with the configuration (also using a DGX Spark)",
              "score": 1,
              "created_utc": "2026-02-07 19:16:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o44qz1d",
                  "author": "Antique_Juggernaut_7",
                  "text": "Do you know if you are you having trouble strictly with vLLM, or with getting openclaw to work nice with vLLM?\n\nI'm running vLLM using the community Docker container from eugr: [https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker)  \nIf you only wish to run GLM4.5/4.6V on vLLM (ie not running with openclaw), you can try the following command -- note that the vllm serve command is baked in, so you can also try that directly if you already have vllm installed somewhere (note also: if you're not running a cluster of Sparks, adapt accordingly):  \n\\`\\`\\`  \n./launch-cluster.sh  \\\\\n\nexec vllm serve zai-org/GLM-4.6V-FP8 \\\\\n\n\\--port 8000 \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\\\\n\n\\--gpu-memory-utilization 0.75 \\\\\n\n\\-tp 2  \\\\\n\n\\--distributed-executor-backend ray  \\\\\n\n\\--max-model-len 128000  \\\\\n\n\\--load-format fastsafetensors  \\\\\n\n\\--enable-auto-tool-choice \\\\\n\n\\--tool-call-parser glm45  \\\\\n\n\\--reasoning-parser glm45  \\\\\n\n\\--trust-remote-code \\\\\n\n\\--allowed-local-media-path / \\\\\n\n\\--mm-encoder-tp-mode data  \n\\`\\`\\`\n\nBUT!\n\nOpenclaw at this point requires a chat template that accepts the \"developer\" role. GLM4.5 and 4.6 follow a chat template that only accepts the \"system\" role. So you will need to provide vLLM with a custom chat template for this to work.\n\nEDIT: went ahead and pushed to Github the instructions on how to make GLM4.5/4.6 on vLLM to talk to OpenClaw: [https://github.com/fidecastro/fix\\_glm46v/](https://github.com/fidecastro/fix_glm46v/)",
                  "score": 1,
                  "created_utc": "2026-02-07 19:36:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u1kyv",
          "author": "SillyLilBear",
          "text": "I run it with M2.1 4 bit locally, works well.",
          "score": 5,
          "created_utc": "2026-02-06 02:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u32xg",
          "author": "shigeru777",
          "text": "Try qwen3-coder-next, better inference speed than GLM-4.7-FLASH,  but still too hard to use tool/skill calling. I only use openclaw for chat and weather information / brave api search.",
          "score": 3,
          "created_utc": "2026-02-06 02:53:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o451aki",
              "author": "chimph",
              "text": "why not just run open webui with web search then?\n\n",
              "score": 2,
              "created_utc": "2026-02-07 20:31:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xnv6k",
              "author": "cashedbets",
              "text": "What do you have qwen3-coder-next running on?",
              "score": 1,
              "created_utc": "2026-02-06 17:23:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4026ou",
                  "author": "shigeru777",
                  "text": "Mac Studio M3 ultra 256GB",
                  "score": 1,
                  "created_utc": "2026-02-07 00:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3tzbru",
          "author": "piddlefaffle12",
          "text": "Spent a few days on this with my 5090 and M4 Max 128GB.\n\nOnly model that kinda worked is glm-4.7-flash. Prompt pre-processing is going to be the performance killer for self hosted agentic in my experience.  \n",
          "score": 3,
          "created_utc": "2026-02-06 02:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uayjz",
              "author": "DataGOGO",
              "text": "Depends on your hardware and hosting configuration. ",
              "score": 1,
              "created_utc": "2026-02-06 03:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3u3n9g",
          "author": "FinancialMoney6969",
          "text": "I keep fucking mine up. I’ve tried everything even LM studio…",
          "score": 3,
          "created_utc": "2026-02-06 02:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub9q7",
              "author": "DataGOGO",
              "text": "LMstudio is pretty trash dude. ",
              "score": 0,
              "created_utc": "2026-02-06 03:44:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ufq1z",
                  "author": "FinancialMoney6969",
                  "text": "What’re you using?",
                  "score": 2,
                  "created_utc": "2026-02-06 04:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3xg80c",
              "author": "DarkZ3r0o",
              "text": "Try with ollama the difference is huge",
              "score": 0,
              "created_utc": "2026-02-06 16:47:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4001jm",
                  "author": "FinancialMoney6969",
                  "text": "That’s where I’m having the problem… I am trying on windows tho",
                  "score": 1,
                  "created_utc": "2026-02-07 00:32:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ud8xk",
          "author": "kdd123456789",
          "text": "If we setup kimi on hetzner vps running openclaw locally, what kind of costs would be involved, as the cost of the hardware to run a descent llm locally is pretty expensive.",
          "score": 3,
          "created_utc": "2026-02-06 03:57:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v7h9x",
          "author": "Gargle-Loaf-Spunk",
          "text": "You mean it's supposed to work at all? ",
          "score": 3,
          "created_utc": "2026-02-06 07:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y9ame",
          "author": "Professional_Owl5603",
          "text": "I have a question, I know Claw is a security nightare but I dont need it to do half the things people say it could. I essentaill want a bot that can help me to research on thing. Example: I'll talk to Grok (yea I know, but if I need spicy, I go there, everythign else is Gemini for anythign serious) and will discuss something I saw on youtube, like a new LLM or API or whatever. Like the new Nvidia Personoplex. I woudl like to have the bot go and research it for me, check the gitgub and see if it can be intergrated into itself. Obviously, this is an extreme situation, but along these lines. \n\n  \nThe reason why I thought this was possible was becasue I was tryign to get it to work with discord so I can talk to it that way, and when I was testing it via Claude Opus, I asked it to help me configure it so it would work the way I wanted to. It just did it. And when it hit problems, it kept trying things, which is GREAT, however, the openwebui credits I have for over a year of 4.35c that I've been using that lasted me forever, was drained in minutes to .35c apparently soakign though hundreds of thousands of tokens. Which is nuts.\n\n  \nSo my take is, claude is great and works as advertised, at the cost of a liver and partial kidney per hour. I realize there isnt a comparable model that's open source, but I'm wondering if I can get close? With those abilities? My rig is pretty basic, I have an older Gigabyte X99P-sli Motherbaord with 225gb of ram, that has pci 3x slots and dual rtx 5090's that I use for minecraft, with ollama, so I have 64gb of pooled vram using ollama. I get about 30tps usign a 70b model. Which im guessign inhundreds if times slower than the cloud API.\n\n  \nAm I just dreaming here? Would a machien like DGX spark make a better machine? I'm guess it probably wouldnt as it just has x2 the vram and nothing would change other than the model and maybe a lower tps even. And yes I knwo giving it access to this machine is dangerous, Ive installed it closed wsl enviorment. I dont plan to give it access to anything and stricly want to use it as a chat bot springboard research assistant. I manage my own calendar.\n\nAm I wasting my time? Thanks for the advice in advance.\n\n![gif](giphy|1D7ryE8SDYuq8kGGGQ)",
          "score": 3,
          "created_utc": "2026-02-06 19:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xfpom",
          "author": "DarkZ3r0o",
          "text": "I tested it with glm-4.7-flash with 35k context and gpt-oss-20b with 120k context and am really satisfied with results. I have 3090ti",
          "score": 2,
          "created_utc": "2026-02-06 16:45:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40he1v",
          "author": "Long_Complex_4395",
          "text": "I used LMStudio with Qwen2.5 instruct. I wrote on how to set it up\n\nhttps://medium.com/@nwosunneoma/how-to-setup-openclaw-with-lmstudio-1960a8046f6b",
          "score": 2,
          "created_utc": "2026-02-07 02:17:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u0w0b",
          "author": "Battle-Chimp",
          "text": "All these OpenClaw posts just prove that smart people still do really, really dumb things.\n\nDon't install OpenClaw. ",
          "score": 7,
          "created_utc": "2026-02-06 02:41:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u4hcc",
              "author": "actadgplus",
              "text": "All these OpenClaw posts just prove that smart people still post really, really dumb things.\n\nDo your research and install OpenClaw.",
              "score": -1,
              "created_utc": "2026-02-06 03:02:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3u5dpr",
                  "author": "Momo--Sama",
                  "text": "I have it running on a separate mini pc with a kimi sub, and its definitely fun to mess around with, but there's not a lot I can actually do with it while refusing to give it access to any of my personal accounts. Maybe I'm just not being creative enough, idk",
                  "score": 2,
                  "created_utc": "2026-02-06 03:07:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3uavlu",
                  "author": "DataGOGO",
                  "text": "I wouldn’t. ",
                  "score": 2,
                  "created_utc": "2026-02-06 03:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3uy3lz",
          "author": "grumpycylon",
          "text": "I tried OpenClaw with Llama 3.1 and it was spewing nonsense. I typed hi in the chat and it kept typing giant paragraphs of garbage.",
          "score": 2,
          "created_utc": "2026-02-06 06:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3up97b",
          "author": "IngwiePhoenix",
          "text": "Really want to try it myself to see how far it can go - but I fear my singular 4090 is not going to go that far... x)\n\nI hear Qwen3-Coder (and it's -Next variant) are really good. In general, tool-call optimized models like the recent GLMs and such should do well.\n\nIn theory, anyway.",
          "score": 1,
          "created_utc": "2026-02-06 05:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v0oia",
          "author": "SEND_ME_YOUR_ASSPICS",
          "text": "Any recommendations for 32 RAM and 16 VRAM?",
          "score": 1,
          "created_utc": "2026-02-06 06:53:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wm2rq",
              "author": "mzinz",
              "text": "Gpt-oss:20b. One of the recommended models straight from the ollama docs",
              "score": 1,
              "created_utc": "2026-02-06 14:20:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vjo6c",
          "author": "ifheartsweregold",
          "text": "Yeah working really well I just got it set up with dual DGX Sparks running Minimax 2.1",
          "score": 1,
          "created_utc": "2026-02-06 09:51:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tamx1",
              "author": "jtbzr92",
              "text": "Considering getting a second Spark, is your setup still running well?",
              "score": 1,
              "created_utc": "2026-02-11 15:30:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vm0il",
          "author": "prusswan",
          "text": "I wanted a tool like this but only as a guidance rather than having broad executive powers - it is too much of a security burden (can't give it free reign, and whatever it does needs to have audit trail). Open to suggestions",
          "score": 1,
          "created_utc": "2026-02-06 10:13:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vndrj",
          "author": "Zevatronn",
          "text": "I run qwen 8b with openclaw and qwebcoder 30b local models are used ny sub agent while the 'conductor' runs on a chatgpt sub it works fine",
          "score": 1,
          "created_utc": "2026-02-06 10:25:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yasxu",
          "author": "w3rti",
          "text": "I made it work once, it was perfect, write codes, write apps, adjust setup, performance,. Clawd just did everything for me. After graphic card update and some changes it went garbage. I hat 5 days of fun, still keeping those .mds and sessesion, when he will work with the llm like this again, we can continue",
          "score": 1,
          "created_utc": "2026-02-06 19:12:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yoeod",
          "author": "Decent-Freedom5374",
          "text": "I use 8 ram and use the new release ollama gave of awencoder inside for free. Works great same project multiple terminals. Why",
          "score": 1,
          "created_utc": "2026-02-06 20:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zz2qe",
          "author": "HealthyCommunicat",
          "text": "yes it is. and its easy.\n\nif ur looking at qwen 2.5 and llama 3.1 you do not have the required level of information throughput. this is a space that is ever changing at a pace no other field has moved in before. the models we had a year ago (qwen 2.5 as you say) are leagues leagues less capable than the model that just came out, qwen 3 coder next 80b (when comparing to 72b or 70b qwen 2.5) literally feels like an entirely different kind of tech. one can write files and access emails and search the web, one cant even run a simple find command.\n\nif u put in the work and learn ground up instead of trying to rush in and expect results, then you'd come to see very easily that this field requires really high levels of information intake on a daily basis. to top it off, this is a niche that requires u to have a minimum of 5-10k to even touch a model that feels somewhat capable.\n\n  \nwhat makes it even worse is reading that u do in fact have a claude subscription. if u were dilligent you would've used it in combination with ur own local models to better learn how you can utilize these models. if you cared you would've already asked claude to help you with this setup.",
          "score": 1,
          "created_utc": "2026-02-07 00:27:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43b20q",
          "author": "FrankWanders",
          "text": "Anyone here who has experience with Ministral 3 14B? It supports up to 256K context, but with 128K context it seems to be just small enough into one RTX 5090 without offloading, so this would make it doable.\n\nCompared to Flash 4.7 I think ministral 3 14B is the winner,  or am I wrong? To be fair I don't see any other option to run it on local hardware? Open to suggestions!",
          "score": 1,
          "created_utc": "2026-02-07 15:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4onwyb",
          "author": "playingdrumsonmars",
          "text": "couldn't get it to run in any reasonable way until I found lfm2.5-thinking today.  \nTry out this model  \n  \n\"ollama pull lfm2.5-thinking:latest\"\n\nIt is the only model I found that would not hallucinate like crazy even with simple prompts",
          "score": 1,
          "created_utc": "2026-02-10 21:05:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pyh8s",
              "author": "InterestingFly9566",
              "text": "I can’t get it to work at all with ollama, have you got a tutorial how to set it up?",
              "score": 1,
              "created_utc": "2026-02-11 01:11:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3uff1h",
          "author": "HenkPoley",
          "text": "Even the gigantic open weights models are 9 months behind the closed source models (\"always have been\"). The models of Anthropic and OpenAI only recently got to the level where they can work autonomously for a bit. Claude Opus 4.5 was released on 24 November 2025, GPT-5.1 on November 12, 2025.\n\nYou'll have to wait till mid august, and have a very beefy machine by then.\n\nBtw, clawdbot(-descendents) are conceptually fun, but in reality not that interesting.",
          "score": 1,
          "created_utc": "2026-02-06 04:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vmrtq",
          "author": "RevealIndividual7567",
          "text": "I would highly recommend not running openclaw, or if you have to then running it in a sandbox with very limited external websites and resources allowed. It is a security nightmare due to things like website prompt injection.",
          "score": 1,
          "created_utc": "2026-02-06 10:20:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ekq5u",
          "author": "Pains_asistant",
          "text": "Yo lo estoy configurando actualmente y llevaba unos dias pegandome porque al parecer cado modelo cambia un poco en el tema del JSON de openclaw (o eso me parece a mi) pero encontre un comando (solo para configurar el LLM local) luego configurar claw ya es otra historia.   \nUna vez tienes configurado e instalado el bot puedes ejecutar este comando en la bash:  \nollama launch openclaw  \nCon ese comando ya te manda seleccionar el modelo de ollama que quieres de los que trienes instalados o de los que te recomeinda y se configura automatico (cosa que me vi negro para hacer manual yo mismo).  \nPor otro lado tengo en local el llama 3.1 8b y no consigo que ese  modelo me responda bien... pero es que por lo menos reponde que el dolphin o el mannix no responden y no los consigo configurar (manualmente todo hasta ahora que acabo de descubir el comando de ollama y de momento solo pude probar ese modelo utilizable).  \nTengo miedo a que sea por la capacidad del modelo lo que me esta pasando porque no me salen logs de error en el dashboard ni en el status de claw\n\nhttps://preview.redd.it/9g4kctlvnfig1.png?width=962&format=png&auto=webp&s=27edfb69558ff2ad7191172b45f2527fb45fed5c\n\nY con este modelo probando a que me respondiese mal (se desinstalo completamente del equipo a si mismo osea que imaginate el poder que tiene y lo loco que esta jaja) recomiendo hacerlo en un pc aislado y con sus propios recursos de redes y correo.  \nPor cierto acabo de comprobar que despues se te va a lanzar automaticamente con el modelo que selecciones la primera vez que hagas el lauch, estoy mirando como cambiarlo porque esta config de modelo auto es mil veces mas comoda, peleandome con el json no di hecho ",
          "score": 0,
          "created_utc": "2026-02-09 08:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u581c",
          "author": "actadgplus",
          "text": "I’m have a really powerful Mac Studio M3 Ultra with 256GB RAM so testing out various models via LM Studio.  I haven’t leaned on anything yet.\n\nIn parallel I have been exploring also leveraging Synthetic.  Has anyone given it a try?  Thoughts?\n\nhttps://synthetic.new",
          "score": -2,
          "created_utc": "2026-02-06 03:06:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyjztp",
      "title": "I built a local‑first RPG engine for LLMs (beta) — UPF (Unlimited Possibilities Framework)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qyjztp/i_built_a_localfirst_rpg_engine_for_llms_beta_upf/",
      "author": "Gohzio",
      "created_utc": "2026-02-07 17:36:54",
      "score": 43,
      "num_comments": 45,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nI want to share a hobby project I’ve been building: **Unlimited Possibilities Framework (UPF)** — a local‑first, stateful RPG engine driven by LLMs.\n\nI’m **not a programmer by trade**. This started as a personal project to **help me learn how to program**, and it slowly grew into something I felt worth sharing. It’s still a **beta**, but it’s already playable and surprisingly stable.\n\n# What it is\n\nUPF isn’t a chat UI. It’s an **RPG engine** with actual **game state** that the LLM can’t directly mutate. The LLM proposes changes; the engine applies them via structured events. That means:\n\n* **Party members, quests, inventory, NPCs, factions, etc.** are tracked in state.\n* Changes are applied through JSON events, so the game doesn’t “forget” the world.\n* It’s local‑first, inspectable, and designed to stay coherent as the story grows.\n\n# Why you might want it\n\nIf you love emergent storytelling but hate losing context, this is the point:\n\n* The engine **removes reliance on context** by keeping the world in a structured state.\n* You can **lock fields** you don’t want the LLM to overwrite.\n* It’s built for long‑form campaigns, not just short chats.\n* You get RPG‑like continuity without writing a full game.\n\n# Backends\n\nMy **favourite backend is LM Studio**, and that’s why it’s the priority in the app, but you can also use:\n\n* **text-generation-webui**\n* **Ollama**\n\n# Model guidance (important)\n\nI’ve tested with models **under 12B** and I **strongly recommend not using them**. The whole point of UPF is to reduce reliance on context, not to force tiny models to hallucinate their way through a story. You’ll get the best results if you use your **favorite 12B+ model**.\n\n# Why I’m sharing\n\nThis has been a learning project for me and I’d love to see other people build worlds with it, break it, and improve it. If you try it, I’d love feedback — especially around model setup and story quality.\n\nIf this sounds interesting, this is my repo  \n[https://github.com/Gohzio/Unlimited\\_possibilies\\_framework](https://github.com/Gohzio/Unlimited_possibilies_framework)\n\nThanks for reading.\n\nEdit: I've made a significant update to the consistency of RPG output rules. I strongly recommend you use the JSON schema in LM studio. I know Ollama has this functionality to but I have not tested it.\n\nModels, I have found instruction models ironically fail to follow instructions and actively try to fight the instructions from my framework. Thinking models are also pretty unreliable.\n\nThe best models are usually compound models made for roleplay 12-14b parameters with massive single message context lengths. I recommend uncensored models not because of it's ability to create lewd stories but because they have fewer refusals (none mostly). You can happily play a Lich and suck the souls out of villagers without the model having a conniption.  \nI am hesitant to post a link to a NSFW model because it's not actual the reason I made the app. Feel free to message me for some recommendations.\n\nhttps://preview.redd.it/8817b4fqwnig1.png?width=1784&format=png&auto=webp&s=1e83f62b646baff084265731b8bc2b926b993f48\n\n",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qyjztp/i_built_a_localfirst_rpg_engine_for_llms_beta_upf/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o446o91",
          "author": "ComprehensiveFun3233",
          "text": "It always make me laugh at myself, but even when I'm in a LLM sub, and the explicit interesting tool is using LLM, I just glaze over and stop reading trhw second I realize the explanation is made/highly tuned by a LLM itself.",
          "score": 2,
          "created_utc": "2026-02-07 17:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o447bwc",
              "author": "Gohzio",
              "text": "Or perhaps I'm not the most eloquent and grammatically correct person out there so, to make it comprehensible I took my notes and put it into LM studio and asked for a comprehensible post 🤷🏿‍♂️",
              "score": 5,
              "created_utc": "2026-02-07 17:58:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45tosj",
                  "author": "nachohk",
                  "text": "If you couldn't be bothered to write it, why should you expect anyone else to be bothered to read it? Using an LLM to help improve and refine your own words is one thing. Using them to inflict slop upon the rest of us is another.",
                  "score": 4,
                  "created_utc": "2026-02-07 23:06:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o448m1a",
                  "author": "ComprehensiveFun3233",
                  "text": "Exactly, I know you did that.  I do it as well.  But it still just makes my eyes glaze over. Interesting.",
                  "score": 1,
                  "created_utc": "2026-02-07 18:04:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o44rts6",
                  "author": "FirstEvolutionist",
                  "text": "I fault people for using AI to summarize or structure text only when the content is already bad: promotion, a disguised ad, AI psychosis or non sense.\n\nFor pruposeful use of AI formatting, summarizing and structure I've actually began to appreciate it: I no longer need to tune in to the style of the person writing to make sure I don't kisunderstand anything or having to deal with unclear text. The same has started happening for audio (podcasts) and video.\n\nWhen I want info, I rather have it done by AI. When I want connection, or thinking, I'm ok with old fashioned content as long as it is well done.\n\nI have a feeling that a lot of people will quickly adopt a similar preference over time.",
                  "score": -1,
                  "created_utc": "2026-02-07 19:40:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o444m8w",
          "author": "dmitryplyaskin",
          "text": "https://preview.redd.it/vvx2zxhn04ig1.png?width=1256&format=png&auto=webp&s=9826e5995f16fa5328ca3db135866a66049c260d\n\nThe repository is not available at this link.",
          "score": 1,
          "created_utc": "2026-02-07 17:44:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o444qiq",
              "author": "Gohzio",
              "text": "It would be a good idea for me to open it. thanks!",
              "score": 3,
              "created_utc": "2026-02-07 17:45:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o445dap",
              "author": "Gohzio",
              "text": "fixed",
              "score": 3,
              "created_utc": "2026-02-07 17:48:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o445jok",
              "author": "drumzalot_guitar",
              "text": "Agreed, same result.  Any amount of searching doesn’t turn anything up.",
              "score": 1,
              "created_utc": "2026-02-07 17:49:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o447mz7",
                  "author": "Gohzio",
                  "text": "fixed",
                  "score": 2,
                  "created_utc": "2026-02-07 17:59:44",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o444q0i",
          "author": "ghostd93",
          "text": "The link does not work",
          "score": 1,
          "created_utc": "2026-02-07 17:45:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o445cos",
              "author": "Gohzio",
              "text": "fixed",
              "score": 3,
              "created_utc": "2026-02-07 17:48:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o445r25",
                  "author": "ghostd93",
                  "text": "Thanks. Looks promising. I will definitely check it out",
                  "score": 1,
                  "created_utc": "2026-02-07 17:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44axih",
          "author": "Oshden",
          "text": "Awesome work man!!!",
          "score": 1,
          "created_utc": "2026-02-07 18:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44n7vj",
          "author": "Aionion",
          "text": "curious, does it allow api's for other services such as OpenAi, Anthropic, NateGPT, Moonshot, etc.?\n\n",
          "score": 1,
          "created_utc": "2026-02-07 19:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44nd65",
              "author": "Gohzio",
              "text": "It's fully OpenAI compatible.",
              "score": 2,
              "created_utc": "2026-02-07 19:17:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45hsvv",
          "author": "DHFranklin",
          "text": "Cool. I did something similar, I've been experimenting in AI Studio since April doing a lot of this.\n\nI have Custom Instructions for the game play and found that systems it's really familiar with like d20 work better than bespoke ones. I then drag and drop RAG files as plain text and JSON and haven't had much trouble with one more than the other. I tell it to rely on python for the math and then relate it narratively for \"critical failure, failure, success, critical success\" However it doesn't really work out. It won't let a character lose.\n\nI have had tones of problem with things like tone and consistent character \"voice\". As it plays out it changes format for game state. It has problems with context bleed and context rot especially at token lengths over 150-200k tokens or so.\n\nHow does this compare?",
          "score": 1,
          "created_utc": "2026-02-07 22:00:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45qfwa",
              "author": "Gohzio",
              "text": "Yes, I know this well. My first 2 tries ended similarly. My first try just shuffled things in and out of context but the LLM just went insane. Second I tried splitting the processing to a back end server type thing but the processing power needed started to ramp up and basically only would work if you connected it to chat gpt or something and had massive context lengths (literally the opposite to what I wanted)\n\nWith this I started with 100% Rust and tried to make the LLM output tags for everything so the engine could read them and process them accordingly. This was the best yet but my keywords list went straight over 300 words and still missed stuff due to how conversations work.\n\nThat's when I read about context requests  in the open ai format. I pivoted to that and it works amazingly. The LLM doesn't need to remember a damn thing it just requests the information it needs to build an output. So then the world data is basically the system prompt and the engine combines the players input and relevant data from the player tab and if the LLM needs to know something like how loot drops work it will just request that data adding delay but removing context needs and so far the LLM has stayed sane.\n\nI've gotten as far as I can in solo testing and as I neither know anyone on Linux or with a GPU as powerful as mine (5080) I decided to let Reddit destroy me 😂.",
              "score": 1,
              "created_utc": "2026-02-07 22:48:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45ymob",
                  "author": "DHFranklin",
                  "text": "Oh geez. Yeah that sounds like a hell of a work around. No I don't have a GPU that hefty. I wanted to ask the speed of turn around on prompts, but I guess that answer wouldn't be terribly useful.\n\nDo you do anything that needs a RNG? How are you handling that? What I'm getting out of Gemini 2.5 and 3 (admittedly all I've tried) is that it will see the instruction to run python for random numbers, do math sometimes and sometimes not, then just auto-win anyway.",
                  "score": 1,
                  "created_utc": "2026-02-07 23:38:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47k6lx",
          "author": "TeamDman",
          "text": "Would be good to add some screenshots to the post and readme",
          "score": 1,
          "created_utc": "2026-02-08 06:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lspnx",
              "author": "Gohzio",
              "text": "Screenshot added. Readme in the link :)",
              "score": 1,
              "created_utc": "2026-02-10 12:41:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48cb8x",
          "author": "AnotherFuckingSheep",
          "text": "I don't get it. Do you have a demo?",
          "score": 1,
          "created_utc": "2026-02-08 10:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48gqeg",
              "author": "Gohzio",
              "text": "It's fully usable in the git.",
              "score": 1,
              "created_utc": "2026-02-08 11:03:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48zmb7",
                  "author": "AnotherFuckingSheep",
                  "text": "This is totally not my domain. From the post you did it sounds like this is something that can actually run and give users results. \n\nFor me running this would probably take a couple of days.\n\nMaybe you can post a chat, a game text, or something else to just show off what it actually does.\n\nAgain not my domain so if this is actually impossible you can just tell me",
                  "score": 1,
                  "created_utc": "2026-02-08 13:33:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aco5t",
          "author": "Narrow_Operation7252",
          "text": "Tried it out, but I keep getting a completion error when I try to send a prompt to my local LLM (with both LM Studio and Ollama). It says “LLM error: error sending request for url”. Any idea what that might be?",
          "score": 1,
          "created_utc": "2026-02-08 17:49:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ao0u7",
              "author": "Gohzio",
              "text": "Did you have a model loaded? And was it connected. I've only seen this error when I've forgotten to load a model or press the connect button",
              "score": 1,
              "created_utc": "2026-02-08 18:42:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4apc1n",
                  "author": "Narrow_Operation7252",
                  "text": "Yeah, it’s kind of strange. The UPF program can connect and see the model. And occasionally I’ll even get a response or two back if I set it to 127.0.0.1 instead of Localhost. But then it will fail on the next prompt I send, and I’ll keep getting the error messages again after that. \n\nIt might be something weird with my setup, but just wanted to see if it was happening to anyone else.",
                  "score": 1,
                  "created_utc": "2026-02-08 18:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hubjv",
          "author": "fungnoth",
          "text": "It's so hard to get into. I've set it up and struggle to think of that much info to put in.\n\nCan anyone share their world building?",
          "score": 1,
          "created_utc": "2026-02-09 20:33:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hw5vp",
              "author": "Gohzio",
              "text": "Everything is filled out for you in world tab except the description and world name. The only things I would select is the quest rules I keep them all selected. ChatGPT can give you an easy description but here's one I use.\n\n>!Elysium Prime is an immense world, approximately 139,820 times the mass of Earth, with a diameter roughly equal to Jupiter's. Its surface area spans over 65 billion square kilometers, consisting of five major continents and countless smaller islands scattered across vast oceans that cover nearly 75% of its surface.!<\n\nFor player tab I just usually keep the same one that I used from before I made the app, the same I try to play every first run of a new RPG. A melee tank with a shield and warhammer",
              "score": 2,
              "created_utc": "2026-02-09 20:42:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4icb8j",
          "author": "PeaceLoveorKnife",
          "text": "Does it work with tabbyapi?",
          "score": 1,
          "created_utc": "2026-02-09 22:03:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4il8qr",
              "author": "Gohzio",
              "text": "I've literally never heard of it. Until I made this app I actually didn't know so many backends existed. If it supports openai connections it will work otherwise I doubt it.   \nI strongly recommend LM studio or Ollama as both support JSON schema which significantly improves reliability. I work for neither of these companies I just love Local LLM.",
              "score": 2,
              "created_utc": "2026-02-09 22:48:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ko25",
          "author": "ghostd93",
          "text": "Did you consider adding an option for generating images (sending a request to comfyui api)? For imagine last scene etc?",
          "score": 0,
          "created_utc": "2026-02-07 19:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lsmt2",
              "author": "Gohzio",
              "text": "I did think about it but decided not to. It really was just about a way to circumvent the context restrictions in LLM Roleplay. I am looking into adding a 'create prompt' for NPC's that character cards so you can upload images for immersion like the main character can.",
              "score": 1,
              "created_utc": "2026-02-10 12:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxun8z",
      "title": "How many mac mini are equal to 1 mac studio?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/snkxawxo0yhg1.jpeg",
      "author": "Far-Stretch5237",
      "created_utc": "2026-02-06 21:33:56",
      "score": 27,
      "num_comments": 21,
      "upvote_ratio": 0.7,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxun8z/how_many_mac_mini_are_equal_to_1_mac_studio/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3z5yvs",
          "author": "xXprayerwarrior69Xx",
          "text": "https://preview.redd.it/r7fnmpf33yhg1.jpeg?width=1290&format=pjpg&auto=webp&s=10b9e1e12ba7a91d88e5197b1c86094034d5b8c3\n\nHope this helps",
          "score": 41,
          "created_utc": "2026-02-06 21:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z644z",
              "author": "Far-Stretch5237",
              "text": "It doesn't work like that 🥴",
              "score": -31,
              "created_utc": "2026-02-06 21:48:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zjp7g",
                  "author": "yunarivay",
                  "text": "![gif](giphy|10hfegXGKVRVNm)",
                  "score": 12,
                  "created_utc": "2026-02-06 22:58:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3z7pzk",
          "author": "AnonymousCrayonEater",
          "text": "Telling us what you are trying to accomplish might help. At a basic level you can divide the ram of the studio over the mini, but i don’t think that answers your question",
          "score": 12,
          "created_utc": "2026-02-06 21:56:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z81rp",
              "author": "Far-Stretch5237",
              "text": "I AM just curious \nA guy on twitter said he will get mac studio for his openclaw bot\n\nSo i thought....\n\nHow many mac mini are equal to 1 mac studio",
              "score": -9,
              "created_utc": "2026-02-06 21:57:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zaoyp",
                  "author": "AnonymousCrayonEater",
                  "text": "Based on your numbers you are comparing a maxed out studio to a base mini so:\n\n512 / 16 = 32 mac minis",
                  "score": 5,
                  "created_utc": "2026-02-06 22:11:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o48exgv",
                  "author": "TBT_TBT",
                  "text": "If this „guy on twitter“ does not use local LLMs but API keys, it is completely irrelevant if this OpenClaw instance runs on a Studio or Mini. If used with local LLMs, no Mini setup will ever reach a Studio with a lot of ram.",
                  "score": 1,
                  "created_utc": "2026-02-08 10:46:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4174m0",
          "author": "Such_Advantage_6949",
          "text": "How many cars is equal to one plane",
          "score": 9,
          "created_utc": "2026-02-07 05:11:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o442wu5",
              "author": "nanotothemoon",
              "text": "Mostly depends on the turn signals, fuselage, and whether they have full size spares or not",
              "score": 2,
              "created_utc": "2026-02-07 17:36:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zf8vt",
          "author": "Momo--Sama",
          "text": "You'd also have to price in the equipment to connect all of these Minis together to work in tandem (that's an option, right?). as opposed to an independent plug and play Mac Studio but I'm not knowledgeable enough to expand on that.",
          "score": 7,
          "created_utc": "2026-02-06 22:34:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zibie",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-02-06 22:51:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zk600",
                  "author": "Buddhabelli",
                  "text": "thunderbolt5. exo.",
                  "score": 3,
                  "created_utc": "2026-02-06 23:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o407vpt",
          "author": "deeddy",
          "text": "42.",
          "score": 6,
          "created_utc": "2026-02-07 01:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40n0hk",
          "author": "SebastianOpp",
          "text": "At least tree fiddy",
          "score": 4,
          "created_utc": "2026-02-07 02:52:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40u5zm",
          "author": "jwrunge",
          "text": "About 3 and a half to 5, depending on if you are keeping the rigid structure or allowing for redistribution of volume (eg melting one down)",
          "score": 3,
          "created_utc": "2026-02-07 03:39:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o406pd1",
          "author": "TimLikesAI",
          "text": "About five by volume from the looks of it.",
          "score": 2,
          "created_utc": "2026-02-07 01:12:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r05z64",
      "title": "Claude Code vs OpenCode vs Cline vs Qwen Coder",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r05z64/claude_code_vs_opencode_vs_cline_vs_qwen_coder/",
      "author": "ShowTimezz",
      "created_utc": "2026-02-09 14:42:28",
      "score": 27,
      "num_comments": 25,
      "upvote_ratio": 0.92,
      "text": "Hi all, \n\n  \nI'm curious if anyone knows of or if anyone here did a deep dive with different agentic AI tooling based on concrete, measurable metrics? \n\nFor example, let's say someone gave the same prompt to the same coding model, across several different tools. \"Create a flappy bird clone\" to GLM-4.7-Flash running in Claude Code, GLM running in OpenCode, GLM running in Cline, etc. Some more important prompts and requests could be: \"Here's an attached CSV file with unstructured data. Extrapolate meaningful entries in the file, and create a dashboard using NextJS that displays that data in a meaningful format\".\n\nI did try to find something along these lines, but the vast majority of the results have just been slop, comparing features of the tooling without actually going into a deep dive on how the tooling performs for concrete, measurable instructions, or just displaying the \n\nSomething like this could allow us to actually have a tooling comparison instead of a model comparison per se. \n\nMy gut is telling me that this analysis will have a surprising conclusion, with clear winners on what model compared with what tool provides the best results. i.e GLM4.7-flash works best when paired with Claude Code and worst when paired with Cline based on <metric that was used to evaluate all model/tooling combos>. Or, gpt-oss works best when paired with OpenCode and worst with Qwen Coder.\n\n  \nI'm open to doing the research myself, but was wondering if anyone before bothered doing something like this before I spend the time. \n\n  \nThank you",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r05z64/claude_code_vs_opencode_vs_cline_vs_qwen_coder/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4g44g8",
          "author": "see_spot_ruminate",
          "text": "Don't forget our baguette shaped, cheese dealing partners with mistral-vibe. Maybe not what most people use, but tool calls work well with all the models I put with it.\n\nedit (a joke), can it really be called \"vibe\" coding if it is not from the vibe region in france?",
          "score": 11,
          "created_utc": "2026-02-09 15:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gifub",
              "author": "cmndr_spanky",
              "text": "And Cursor (which is pretty great in my experience).",
              "score": 1,
              "created_utc": "2026-02-09 16:43:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gcxyw",
          "author": "qubridInc",
          "text": "There’s no serious, metric-driven comparison of **agentic tools** yet and most write-ups compare features, not outcomes. Tooling behavior (planning loops, retries, permissions, context handling) often matters more than the model itself, so the same model can perform very differently across tools. Your intuition is likely right, but this gap is still largely unexplored.",
          "score": 4,
          "created_utc": "2026-02-09 16:17:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gi17m",
              "author": "cmndr_spanky",
              "text": "If that’s true, what a missed opportunity. Are you telling me with all the hapless and desperate content creators scrambling to remain relevant, with all the noise they are vomiting out to social media… nobody has through to compare coding agent wrappers for the same model??\n\nWhat an idiotic missed opportunity. \n\nFWIW I agree with op that this is a critical thing. It’s literally the only reason Cursor (to pick my go-to tool) would even hope to compete or have any relevance.  They should (in theory) have some secret sauce that manages code base and context better than opus being used inside something free like VSCode + Roocode for example.",
              "score": 1,
              "created_utc": "2026-02-09 16:41:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fvpnm",
          "author": "calben99",
          "text": "This is a really interesting research idea! I haven't seen anyone do a systematic comparison like this across the same model with different tooling. I'd be curious to see the results if you end up running the tests. You might also want to track things like token usage and execution time as secondary metrics. Good luck with the research!",
          "score": 2,
          "created_utc": "2026-02-09 14:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g984e",
          "author": "Deep_Traffic_7873",
          "text": "I have a private benchmark like this, but it don't do it on full projects, just questions i can measure",
          "score": 2,
          "created_utc": "2026-02-09 15:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gz0q2",
          "author": "i-eat-kittens",
          "text": "Qwen Coder is their model. The tool name doesn't have an 'r'.",
          "score": 2,
          "created_utc": "2026-02-09 18:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i5iub",
          "author": "Crafty-Diver-6948",
          "text": "you are forgetting pi.\n\npi is better than all of them.",
          "score": 2,
          "created_utc": "2026-02-09 21:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fw0sy",
          "author": "Total-Context64",
          "text": "I'd love to have [CLIO](https://github.com/SyntheticAutonomicMind/CLIO) grouped into this if that's interesting to you, it's a small project with very few users but I'd still like to know how it stacks up.  No worries if it isn't interesting enough, figured it never hurts to try.  :D",
          "score": 1,
          "created_utc": "2026-02-09 14:54:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fy7kd",
          "author": "xLRGx",
          "text": "Probably not. Most everyone prefers the slight intelligence boost of the frontier models to power their coding agents and then muscle memory + the UX just brings us to pair them with each other. Most people using Claude code are just using opus. \n\nIt’d be valuable research but I’m not sure how useful it would really be in the AI landscape. It would deprecate quickly due to the pace of change the big 3 are shipping new models. By the time you finish your first rigorous benchmark openAI, Google, and Anthropic will probably have a new model ready to be launched. \n\nI would add a caveat, I think you’re burying the lead a little bit or maybe you haven’t considered it - the memory costs of AI and AI token generation of enterprise and consumers is going to likely 100x in 2026 compared to just last year. That’s a real problem and finding efficiency gains like this might matter more than you think. Right it’s not just humans increasing the cost of AI now it’s Agents who can out produce humans in terms of pure token generation with workflows. It could honestly be 1000x in the aggregate. The real question becomes how do we train the Agents to operate in the “efficient frontier” of 2026 and in the future. And it seems like the big 3 are aware that’s what needs to happen it’s just that they CANT STOP SCALING even if it’s looking flat now.",
          "score": 1,
          "created_utc": "2026-02-09 15:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g9gxm",
          "author": "Corosus",
          "text": "I'm confused why this isn't already being done by other people publicly, thats essentially what I do for when I'm trying to figure out good agent/model combos that aren't broken out of the box or just unusably slow.\n\n> \"Here's an attached CSV file with unstructured data. Extrapolate meaningful entries in the file, and create a dashboard using NextJS that displays that data in a meaningful format\".\n\nLiterally what I've been doing too but with a dump of discord message payload json objects.\n\nIf we could have a matrix comparison with agents on the top, and models on the left, where you can get a gist of the thuroughness of their solution + being able to quickly browse the results it'd be amazing.\n\nI guess where it gets a bit more complicated is what quants do you use, I guess try popular ones, ones at various VRAM/RAM tiers. And then on top of that the params you feed into llama.cpp etc. I guess again cherry picking common sensible usage might help there.\n\nI guess like others have said the info would deprecate quickly, the space moves so fast theres not much time for long standing data, so maybe people don't find it worth doing.",
          "score": 1,
          "created_utc": "2026-02-09 16:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gjusi",
              "author": "cmndr_spanky",
              "text": "Slapping a dashboard ontop of a data structure is such a pedestrian task these days, I don’t think this will be an affective test. \n\nThe secret sauce of these different agentic wrappers is in how efficiently they use context through multiple loops of reasoning, and they index and manage / read a large code base, and the “bag of tricks” / bonus features like rules, skills, MCP access, built-in AI controllable browser etc.\n\nYou’d be better off finding a sizable code base on GitHub, prompt the agent to understand it well enough to implement feature x and validate its working correctly.  And again, avoid simple “dashboard on data” use cases.. it’s too common, too small and too easy",
              "score": 2,
              "created_utc": "2026-02-09 16:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h1f78",
                  "author": "Corosus",
                  "text": "Agreed yeah, I guess a couple reasons why I am using the test case I mentioned is because \n\n1. It was part of something I already had claude setup for me (viewing a feed of as many discord channels I want on a single page you can look at without having to navigate around to each channel)\n\n2. It doesn't take a very long time to find out if the results of the test case I am trying is total garbage or not, even that simple task it can range from 5 minutes to 30 minutes depending on context size/model distribution in ram/vram. So in a way its main purpose is an initial filter to see if it's worth testing further at all\n\nMy next step after that was basically what you said, got a lot of work that involves making use of a specific API that isn't used widely and seeing if it can implement it correctly.",
                  "score": 1,
                  "created_utc": "2026-02-09 18:13:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gfovi",
          "author": "MrMisterShin",
          "text": "Qwen3-coder-next did something similar in there tech report (look for the scaffolding table).  https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf?spm=a2ty_o06.30285417.0.0.3bdec921sAzKJK&file=qwen3_coder_next_tech_report.pdf",
          "score": 1,
          "created_utc": "2026-02-09 16:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gj0zz",
              "author": "cmndr_spanky",
              "text": "You completely missed the point. He wants to compare the quality of different agentic wrappers for the same model.  Nobody needs another dumb benchmark comparing the LLMs",
              "score": 1,
              "created_utc": "2026-02-09 16:46:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gkv6m",
                  "author": "MrMisterShin",
                  "text": "That’s in the paper if you READ it. It literally measures the same model across various scaffolding. It also measure many models against them also.\n\nThe only thing it doesn’t do is explicitly detail which scaffolding achieved the highest marks for the models.\n\nhttps://preview.redd.it/tjhxjiv81iig1.jpeg?width=1027&format=pjpg&auto=webp&s=7b2a4e50dff2f2821fa86fad92f34946fa8c603e",
                  "score": 4,
                  "created_utc": "2026-02-09 16:55:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l5tlh",
          "author": "Charming_Support726",
          "text": "My Opinion is: (There are rarely real benchmarks out there)\n\n1 . Overall it simply doesn't matter at large. They are human interfaces.\n\n2. Harness and Scaffold don't have large impact on the model - But depending on the infrastructure they can make models perform only worse, never better.\n\n3. Many of these scaffolds have got terrible prompts (T - E - R - R - I - B - L - E). E.g look at the discussion in Opencode about the codex prompt - and their new codex prompt when OpenAI entered the game. To many contradictionary hints, restriction, example and so on. And far to long.\n\n4. Either prompts or tools will degrade performance.\n\nIf that is more or less o.k. you could switch scaffold without notice. I am using multiple of them mostly Zed, Opencode and Codex ( sometimes Antigravity ). I manage the access to the models using CLIProxy - so every harness gets the same provider chain. I feel no difference .",
          "score": 1,
          "created_utc": "2026-02-10 09:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lyscw",
          "author": "techlatest_net",
          "text": "No comprehensive head-to-head benchmarks exist yet comparing those exact tool+model combos on concrete tasks like Flappy Bird or CSV→NextJS dashboard. \n\n**Gut ranking from scattered tests**:\n- **Claude Code + Sonnet 4.5/Opus**: Still king for complex reasoning+autonomy. Best at \"think → plan → code → test → fix\" loops on unfamiliar stacks. SWE-bench leader.\n- **OpenCode + GLM-4.7**: Closest open-source contender. 73% SWE-bench (vs Claude's 72%) + zero lock-in. Great for multi-model routing.\n- **Cline**: Solid but tool-heavy—shines on file ops, weaker on pure generation. Agent scaffolding overhead kills simple prompts.\n- **Qwen Coder**: Fastest raw speed, surprisingly good UI/dashboard code, but needs tight scaffolding for full autonomy.\n\n**Surprise winner**: OpenCode+GLM-4.7 often beats Claude Code on price/performance for production pipelines. The tooling matters more than raw model for agentic flows.\n\n**Your test methodology is perfect**—same prompt/model across tools reveals the orchestration gap. I'd run:\n1. Flappy Bird (game logic + Canvas)\n2. CSV→Recharts dashboard (data parsing + React)\n3. GitHub issue reproduction (multi-file edit)\n\nTrack: **task completion rate, edit iterations, token usage, wall-clock time**. \n\nDo it—community needs this data desperately. My bet: Claude Code edges out, but OpenCode closes gap 80%.",
          "score": 1,
          "created_utc": "2026-02-10 13:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nl1gq",
          "author": "Lonely-Ad-3123",
          "text": "this is a really interesting question and I've been wondering the same thing about agentic tooling benchmarks. The reality is most comparisons are just feature checklists without actually measuring output quality or success rates on real tasks. If you're going to do this research yourself, here's how I'd approach it: 1.\n\n\nPick 3-5 concrete tasks with different complexity levels (the flappy bird clone is good for basic generation, the CSV dashboard is good for multi-file coordination, maybe add something like refactoring a legacy codebase or fixing a subtle bug across multiple repos) 2. Define your success metrics upfront. Not just does it run but things like: lines of code changed, build errors introduced, how many iterations needed, time to completion, whether it actually solves the stated problem vs going off track 3.\n\n\nFor the validation step, you might want to look at something like Zencoder Zenflow since it has spec-driven workflows with built-in verification loops that can measure drift between requirements and implementation. That could give you actual quantifiable data on how far each tool/model combo strays from the original prompt 4. Document everything in a spreadsheet with timestamps and iteration counts.\n\n\nScreenshots help too for when things go sideways 5. Run each combo at least 3 times since LLMs can be inconsistent The hardest part is gonna be keeping variables controlled. Same prompt wording, same model temp settings, same time of day even (API performance varies).\n\n\nBut yeah if you do this please share the results because the community desperately needs this kinda analysis instead of just vibes-based recommendations.",
          "score": 1,
          "created_utc": "2026-02-10 18:06:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gw0te",
          "author": "Ok_Rough5794",
          "text": "There are evals and benchmarks and youtubers doing this.. all over the place",
          "score": 0,
          "created_utc": "2026-02-09 17:48:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1g5m0",
      "title": "Best model to run local on a \"normal pc\"",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r1g5m0/best_model_to_run_local_on_a_normal_pc/",
      "author": "MaxsBosch",
      "created_utc": "2026-02-10 22:59:11",
      "score": 25,
      "num_comments": 19,
      "upvote_ratio": 0.88,
      "text": "Hey guys, I'm pretty sure that this post has been made multiple times by other people. But with the fast pace of LLM's right now I'm getting overwhelmed in all the options. Im getting back into coding and I want to experiment with using AI in my workflow.\n\nI do have the gemini pro subscription (I get it for free from work) but I notice that it lacks alot in coding\n\nI recently bought a new pc with a Ryzen 7 9700X, rxt 5070 and 32gb of ram. I'm fascinated by the models from claude but I'm not willing to pay 200 euros a month right now. I also read about other models like kimi k2.5 and GLM. But as I said I'm overwhelmed with how many options there are.\n\nWhat models would you guys recommend for me to run locally right now? Or will I get worse results than Gemini when running locally?\n\nI am new to all of this, but I'm sure I will be able to figure out how to set everything up correcty as soon as I know whichone I should go for. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r1g5m0/best_model_to_run_local_on_a_normal_pc/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4pd4xc",
          "author": "SimilarWarthog8393",
          "text": "You'll only come close to Gemini results when running huge models like Kimi K2.5 (1T parameters) but your PC can't handle models bigger than maybe 30B parameters (Qwen3 Coder 30B A3B). Reduce your expectations and enjoy the privacy you gain from running locally~",
          "score": 13,
          "created_utc": "2026-02-10 23:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pd653",
          "author": "seedsofchaos",
          "text": "My suggestion?  Download LM Studio.  Join huggingface.co.  Search this sub-Reddit for your specific use case (ex. Python coding).  Download those models off huggingface.co.  Try those models with your use cases and see what happens and which you like better.  As someone who just started down the same path on Friday morning, that'd be my suggestion.  Now that I'm that far, I'm looking into what options I have to train a RAG for my particular use cases but haven't figured those steps out yet..\n\nYour rig is about the same specs as mine so it'll do just fine.  ",
          "score": 16,
          "created_utc": "2026-02-10 23:09:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q0ut1",
              "author": "low_v2r",
              "text": "I'm in somewhat of the same boat, although I think I will give llamma.cpp a shot on my strix machine (128Gb unified memory).  \n\nStill have to learn how to do the RAG part - likely will just ask gemini LOL",
              "score": 2,
              "created_utc": "2026-02-11 01:25:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pd2qf",
          "author": "ai_hedge_fund",
          "text": "You will absolutely get worse results than Gemini - but that should not stop you\n\nThe first model should be something small so you can get all your other little pieces in place to be able to run it successfully\n\nMaybe a small Granite gguf formatted model\n\nFrom there you can start pushing hardware and testing capabilities",
          "score": 6,
          "created_utc": "2026-02-10 23:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q925w",
          "author": "ScuffedBalata",
          "text": "You won't get anything anywhere close to Gemini (not even in the ballpark) on such a machine. \n\nKimi 2.5 requires a TERABYTE of high-speed unified memory.  You can get a quantized (shrunk) version of it to run on a ultra-high-spec Mac Studio M4 Ultra with 512GB of unified RAM, but that's a $15k machine.\n\nThe closest normal human model that's kinda/sorta in the ballpark of Gemini might be the new **Qwen3-Coder-Next-80B** that came out last week.  It requires 60-80GB of VRAM.  I think your RTX5070 has 12GB.  The system ram on AMD/Intel systems is too slow for AI work, so it's just not going to work on anything short of a $4k machine.  My friend has a dual-3090 GPU setup and it's basically minimum tolerable speed on that.\n\nI mean if you had 64GB of system RAM, it might RUN, but it'll be AWFUL (like waiting 5 minutes for a simple prompt).\n\nI have it running on a Mac with a M1 Max processor and 64GB of unified memory.  That's the minimum-minimum spec, I have to keep the system stripped down and running nothing but the LLM to have enough RAM, but it works.\n\nFor the most capable local \"coder\" that will work in your system as a hobby/plaything would be maybe Qwen3-Coder 30B with heavy CPU offload.\n\nIt's pretty easy to try.  Download LMStudio, it's point and click.  Then search for \"coder\" models and stay below 30b.  It'll bitch and moan about even that big a model (that's a 20gb download I think), but it will probably work if you slide the offload slider a bunch.\n\nI think as fast a system as you have, it'll run... slow but maybe tolerably well.  But won't sniff gemini for quality and capability.\n\nIf you want something quick, stick to like a 8b model on your system that will fit fully in VRAM with decent context.  Problem is those models aren't that smart and will be a bit forgetful and not as skilled at error-free code.",
          "score": 3,
          "created_utc": "2026-02-11 02:14:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4peyck",
          "author": "aPenologist",
          "text": "Ask Gemini. With a 5070ti I can run a 20B model on vram v nicely. If i was looking for a new model, qwen would be in the equation but id definitely want to make sure whether the code is clean after some 'news' I brushed over today. \n\nFor a first try, just follow instructions to the letter, it will ask all the relevant questions.",
          "score": 2,
          "created_utc": "2026-02-10 23:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pm48i",
              "author": "snozzberrypatch",
              "text": "What 'news' did you brush over today?",
              "score": 1,
              "created_utc": "2026-02-10 23:59:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4po6gh",
                  "author": "aPenologist",
                  "text": "I put it in quotes because it was the title of a reddit post, which i cant now find, and I didnt read beyond the possibly anti-hype bait title. Up-to-date Due dilligence is always a good idea though regardless.",
                  "score": 1,
                  "created_utc": "2026-02-11 00:11:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pcs0f",
          "author": "tomopenworldai",
          "text": "You'll get worse results running locally than with Gemini. But Gemini's pretty good for coding in my opinion.",
          "score": 4,
          "created_utc": "2026-02-10 23:07:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pe1is",
          "author": "rerorerox42",
          "text": "Ministral-3:8b is one that i prefer for non coding, also as someone else mentioned Qwen 3.\n\nOtherwise there are other older coding models. Unsure how for instance a devstrall small or similar will fit as a coding agen in an IDE or something.",
          "score": 1,
          "created_utc": "2026-02-10 23:14:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ph7e1",
          "author": "former_farmer",
          "text": "Cursor has 20 usd per month plans. Sometimes you get free usage of composer model after you run out. There are also other providers of qwen or deepseek that cost little.\n\nRunning locally you will suffer a lot of quality degradation probably. You will be able to run models from 1B to 20B, maybe 30B at very slow speed. Anything above 10B might be slow. Like.. 5% the speed you are used to.\n\nThese things demand a lot of infrastructure.",
          "score": 1,
          "created_utc": "2026-02-10 23:32:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4punks",
          "author": "boyobob55",
          "text": "Download Opencode, they have a handful of free really capable models right now",
          "score": 1,
          "created_utc": "2026-02-11 00:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rbmvw",
          "author": "andrew-ooo",
          "text": "For coding on your 5070 (12GB VRAM), Qwen 2.5 Coder 14B at Q4_K_M quantization is probably your sweet spot — fast inference and genuinely useful for code completion and refactoring. If you want to push it, you can offload some layers to RAM and run the 32B version, but expect slower speeds. Start with Ollama or LM Studio for easy setup; both handle model downloads and GPU offloading automatically.",
          "score": 1,
          "created_utc": "2026-02-11 06:48:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4s12dy",
          "author": "Hefty-Significance91",
          "text": "Any llama model",
          "score": 1,
          "created_utc": "2026-02-11 10:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4suvck",
          "author": "eXl5eQ",
          "text": "I think Qwen 30B-A3B is the best you can get for 32GB. At least that's what I'm playing with.",
          "score": 1,
          "created_utc": "2026-02-11 14:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tk8zt",
          "author": "Express_Ad3626",
          "text": "Have you tried using AntiGravity, Gemini CLI, or the Gemini VScode extension? These (I would assume) will have a slightly different LoRA for coding tasks vs standard Gemini App. I would take a swing at this and see the results you get. \n\nI dont think you're going to be able to run Nemotron or Qwen Code on the 5070. You might be able to run GPT-OSS 20B but might spill over. It's no where close to other models, but I like for simple tasks, that provide concise responses. ",
          "score": 1,
          "created_utc": "2026-02-11 16:15:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qj7kb",
          "author": "No_Foot_7465",
          "text": "Hello guys !, tengo una pregunta similar al del post , que modelos enfocados a codigo puedo correr de manera local ?\n\nexpec:  \nRyzen 5 5600G , Nvidia 3060 12Gb y 16Gb de ram",
          "score": 0,
          "created_utc": "2026-02-11 03:16:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rlt7j",
              "author": "DertekAn",
              "text": "In english nowwwwwwww",
              "score": 1,
              "created_utc": "2026-02-11 08:23:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy9kg1",
      "title": "Best local model for Apple Silicon through MLX",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qy9kg1/best_local_model_for_apple_silicon_through_mlx/",
      "author": "PerpetualLicense",
      "created_utc": "2026-02-07 09:32:32",
      "score": 20,
      "num_comments": 9,
      "upvote_ratio": 0.95,
      "text": "I am only a user, I am not an expert in AI. I use mostly Claude and I pay now for the Claude Max plan. Now that is a large amount of money in a year (>1000 USD) and I want to cancel that subscription. For this purpose I would like to use my MacBook Pro M4 Max/128 GB for running a good enough local LLM for Swift and Python coding and optionally learning German. Ideally it should also have web searching capabilities and it should store the context long term, but I don't know if that is possible. I have experimented with mlx and it seems that mlx supports only dense models, but again I am not sure. What would be the best current LLM for my setup and use case. Basically I am looking at an assistant which will help me in day to day activities which runs 100% locally.\n\nSorry if my post does not fit here, but I just could not find a better forum to ask, it seems reddit is the best when it comes to AI discussions  \nThanks!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qy9kg1/best_local_model_for_apple_silicon_through_mlx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o424645",
          "author": "BABA_yaaGa",
          "text": "For 128gb UM, qwen 3 coder next 80B",
          "score": 15,
          "created_utc": "2026-02-07 10:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45prnh",
              "author": "GreaseMonkey888",
              "text": "Even for 64GB!",
              "score": 2,
              "created_utc": "2026-02-07 22:44:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4288tp",
          "author": "bakawolf123",
          "text": "gptoss120b - via llama.cpp, it's not correctly supported on MLX, but is lightning fast on llama.cpp  \nfor running on MLX stack I guess latest Qwen3 Coder Next or GLM4.7 Flash/Air is worth to try  \n  \noverall having 2 engines is a good idea, as lately MLX was mostly focusing on distributed performance, which is for clustering macs over TB5 while llama recently added ngram drafting that further improves inference performance",
          "score": 6,
          "created_utc": "2026-02-07 10:54:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o446ly3",
              "author": "GCoderDCoder",
              "text": "I agree with nearly everything but my mlx gpt-oss120b seemed fine when I last tested. I used the lm studio mlx mxfp4 version with no issues in the 75-80t/s range on my m3 ultra. What do you mean about not correctly supported on mac? Genuinely curious since I usually use larger models on my mac studio and 3090s for my gpt-oss-120b so q4kxl for that...",
              "score": 3,
              "created_utc": "2026-02-07 17:54:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48glxa",
                  "author": "bakawolf123",
                  "text": "I've meant base mlx-lm library, LM studio has at least proper support for harmony format and tool parsing I guess.  \nHowever there's also big performance issue - at least on my M1Pro I'm getting up to 400 tps prefill and 37 tps token generation on llama.cpp, and only 245/33 on mlx running 4 bit gptoss20b. The difference of TG is kinda small, I wouldn't even notice it, but PP difference is huge for large agentic prompts.",
                  "score": 1,
                  "created_utc": "2026-02-08 11:02:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o424db5",
          "author": "East-Suggestion-8249",
          "text": "you can try all types of models by installing lm studio, I have an M4 pro with 48GB the model speed and context size is important to me so I am just settling with GPT OSS",
          "score": 1,
          "created_utc": "2026-02-07 10:17:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45s277",
          "author": "Useful_University574",
          "text": "Local modals are nowhere near claude",
          "score": 1,
          "created_utc": "2026-02-07 22:57:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47dtno",
              "author": "catplusplusok",
              "text": "There are also a lot of good cloud models cheaper than Claude, I never seem to run out of personal Gemini plan although I am using Google Antigravity quite heavily.",
              "score": 1,
              "created_utc": "2026-02-08 05:09:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o47orx4",
              "author": "PerpetualLicense",
              "text": "of course, I am looking for a good enough only local model",
              "score": 1,
              "created_utc": "2026-02-08 06:41:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r17kxq",
      "title": "Qwen “just works” while GLM doesn’t.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r17kxq/qwen_just_works_while_glm_doesnt/",
      "author": "JTN02",
      "created_utc": "2026-02-10 17:45:46",
      "score": 19,
      "num_comments": 8,
      "upvote_ratio": 0.95,
      "text": "I use LLMs as a tool. I do not care to fiddle with them. I need instruction based responses with quick prompt processing. \n\nMy king: Qwen3 30B 2507 instruct Q8\n\nWhat I want my next king to be: GLM4.7 flash Q8 no thinking \n\nQwen takes no configuration. I’m on older AMD so using Vulcan as a backend, with what I found to be the most reliable and needing the least configuration (kobold.ccp). \n\nWith openwebui as a front end. \n\nI want to use GLM4.7 flash. But wtf is this buggy mess. I’ve spent a good few hours tinkering with it. I can’t get it to consistently not think. \n\nThis would be kind of ignorable if it at least consistently thought but it refuses to put a think tag at the beginning of its thinking process. Only putting a think tag at the end of it stop process. Nothing at the beginning. \n\nI can’t seem to figure out how to get it to not think though. It seems almost random. I’ll ask it a variety of questions and get good responses. Then I’ll ask it to list off five things and it will go infinitely.  Listing off 100+ before I stop it. Repeating over and over again. I heard it sensitive to repeat penalty so I turned it to zero and that only made it worse. \n\nAny suggestion or help would be greatly appreciated. How tf do I use this buggy mess? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r17kxq/qwen_just_works_while_glm_doesnt/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4njxyb",
          "author": "iMrParker",
          "text": "Try the recommended settings:\n\n[https://huggingface.co/zai-org/GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)\n\nFor general use:\n\n* temperature: `1.0`\n* top-p: `0.95`\n\n\n\nYou could also try disabling repeat penalty or setting it to 1.\n\n  \nAs for the missing thinking tags in OWUI, the prompt template is bugged and I haven't found a way to fix it yet. It's pretty annoying. I do miss the /nothink option with previous GLM releases ",
          "score": 4,
          "created_utc": "2026-02-10 18:01:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nnrus",
              "author": "JTN02",
              "text": "Thank you so much for the response and help. \n\nIs there no way of stopping it from thinking at all?",
              "score": 1,
              "created_utc": "2026-02-10 18:18:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4riw5h",
                  "author": "lol-its-funny",
                  "text": "You can disable thinking at the model level. One example: https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/4#696fc582b8fb31975da04130",
                  "score": 2,
                  "created_utc": "2026-02-11 07:55:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nos03",
                  "author": "iMrParker",
                  "text": "Can you add this line to the top of the jinja prompt template:\n\n**{%- set enable\\_thinking = false %}**\n\nAll together it'll look like:\n\n`[gMASK]<sop>`\n\n`{%- set enable_thinking = false %}`\n\n`{%- if tools -%}`\n\n`<|system|>`\n\nI'm not certain if this will make responses worse but it's worth testing!",
                  "score": 1,
                  "created_utc": "2026-02-10 18:23:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4nwofw",
          "author": "Decent_Solution5000",
          "text": "Freaking love Qwen. Deepseek too. But I've got to say, the GLM models pretty much rock too. Try the Oobabooga UI instead. You might like it better than OWUI. I use both.",
          "score": 3,
          "created_utc": "2026-02-10 18:58:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p9eh9",
          "author": "andrew-ooo",
          "text": "GLM4 models are notoriously picky about stopping conditions. The infinite listing loop you're seeing is classic - it's not respecting the stop tokens properly.\n\nTry adding these stop sequences in kobold.cpp: `<|user|>`, `<|endoftext|>`, and `<|end|>`. GLM uses different tokens than Qwen.\n\nFor disabling thinking entirely, you need to set the system prompt to explicitly say \"Do not use thinking tags or internal reasoning. Respond directly.\" Sometimes that helps more than any parameter.\n\nThe repeat penalty sensitivity is real - keep it at 1.0-1.05 max. Higher values cause the model to avoid common tokens it needs for lists.",
          "score": 3,
          "created_utc": "2026-02-10 22:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xdyof",
          "author": "track0x2",
          "text": "Op, any luck? I have the same issue with GLM, Nemotron, and many others. I'm using OpenWebUI with a LM Studio backend. My temp and top-p settings match the specs, and I've tried different versions of the models too (zai vs unsloth)",
          "score": 1,
          "created_utc": "2026-02-12 04:19:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xhdhq",
              "author": "JTN02",
              "text": "Nope…. Whenever I get time, I’m going to check out new front ends. But for now it’s back to Gwen.",
              "score": 1,
              "created_utc": "2026-02-12 04:44:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2t35r",
      "title": "Tutorial: Run GLM-5 on your local device!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/1047rus1c2jg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-12 13:13:49",
      "score": 19,
      "num_comments": 20,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r2t35r/tutorial_run_glm5_on_your_local_device/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4z7vu1",
          "author": "No_Clock2390",
          "text": "So you need like a 10K PC to run this?",
          "score": 8,
          "created_utc": "2026-02-12 13:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zgxbe",
              "author": "UseMoreBandwith",
              "text": "yes. Does that surprise you?",
              "score": 1,
              "created_utc": "2026-02-12 14:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zjgue",
                  "author": "No_Clock2390",
                  "text": "I guess not",
                  "score": 1,
                  "created_utc": "2026-02-12 14:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zle3w",
              "author": "Salt-Willingness-513",
              "text": "Just alot of ram should work somewhat too i guess. At least if you dont need high speed. Ima try it on my 850gb ram server",
              "score": 1,
              "created_utc": "2026-02-12 14:47:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zngmg",
                  "author": "No_Clock2390",
                  "text": "What kind of server is it? How much did it cost?",
                  "score": 1,
                  "created_utc": "2026-02-12 14:57:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zuwuq",
              "author": "Prudent-Ad4509",
              "text": "Let's see... $700 per 24Gb vram GPU, and you need about 12 of them to run dynamic 2-bit with a little bit of context. That is already $8400. The epyc system with 128 PCI lanes and PCIe 4.0 will set you back another $1000 in current prices, and that is the price \\*without\\* ram or ssd. Well, add another $200 for whatever low ram stick you can find just to run the thing. The remaining $600 will go on connecting all that with x8 bifurcation. This is where you are out of money, but you need 2-3 of more good PSUs and at least one SSD drive.\n\nSo no, 10K PC will not run this. But 11K-12K PC could. Or you can try to run it in ram, but I would not call it \"running\".",
              "score": 1,
              "created_utc": "2026-02-12 15:34:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zn4wi",
          "author": "not-really-adam",
          "text": "I wonder if running this in 1-bit, would provide better local coding results than qwen3-next-coder in 8-bit?",
          "score": 2,
          "created_utc": "2026-02-12 14:56:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50c61w",
              "author": "entr0picly",
              "text": "That’s genuinely an open question in the field. The quantization vs parameterization curve has suggested that larger models at lower quant may perform better than smaller models at larger (or no) quant. There isn’t a one size fits all answer. It’s at the frontier, and you have to test your own use cases yourself. Personally, testing 2bit deepseek R1, I found it generally did better with scientific work than qwen3, however it also tended to drift more quickly and maybe struggle a little more with memory.",
              "score": 2,
              "created_utc": "2026-02-12 16:54:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zvp8o",
          "author": "Jumpy-Requirement389",
          "text": "So.. if I have 192GB of ddr5 and a 5090. I’ll be able to run this?",
          "score": 2,
          "created_utc": "2026-02-12 15:38:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwgzl",
              "author": "robertpro01",
              "text": "Probably, try it and share results :)",
              "score": 1,
              "created_utc": "2026-02-12 15:41:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zbsfr",
          "author": "TimWardle",
          "text": "I wonder if additional languages except from English REAP’ed from the model can reduce the size further while maintaining usability.",
          "score": 2,
          "created_utc": "2026-02-12 13:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zlvi5",
          "author": "rookan",
          "text": "Which bit do you recommend for software development to be as smart as Claude Opus 4.6?",
          "score": 1,
          "created_utc": "2026-02-12 14:49:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zmec4",
          "author": "dropswisdom",
          "text": "Yeah.. No. You basically need a server farm to run this locally.",
          "score": 1,
          "created_utc": "2026-02-12 14:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zzakp",
          "author": "separatelyrepeatedly",
          "text": "honestly what even is the point with such small quants?",
          "score": 1,
          "created_utc": "2026-02-12 15:55:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50al9g",
          "author": "Kubas_inko",
          "text": "2-bit and 1-bit are gonna be absolutely worthless. 3-bit might be somewhat usable.",
          "score": 1,
          "created_utc": "2026-02-12 16:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50eksi",
              "author": "silenceimpaired",
              "text": "I have found 2 bit acceptable for my use for GLM 4.7. I suspect for some use cases 2 bit on GLM 5 will beat models at around the same size or a little lower. I prefer GLM 4.7 to GLM Air.",
              "score": 1,
              "created_utc": "2026-02-12 17:05:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50fa24",
          "author": "squachek",
          "text": "1 bit quant? GTFOH",
          "score": 1,
          "created_utc": "2026-02-12 17:09:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r26ena",
      "title": "GLM 5 is out now.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u9z1nezwywig1.png",
      "author": "Cultural-Arugula-894",
      "created_utc": "2026-02-11 19:06:37",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r26ena/glm_5_is_out_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qx6wz8",
      "title": "🔧 MLX Said No to Mixed Precision. We Did It Anyway.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qx6wz8/mlx_said_no_to_mixed_precision_we_did_it_anyway/",
      "author": "Concert_Dependent",
      "created_utc": "2026-02-06 03:41:14",
      "score": 16,
      "num_comments": 9,
      "upvote_ratio": 0.8,
      "text": "Running Qwen3-MoE-32B locally on Apple Silicon hit a wall: MLX's quantization only supports uniform precision. All experts at FP16? 180GB+. All at 4-bit? Quality tanks on coding tasks.\n\nWe needed 9 coding experts at FP16, 119 others at 4-bit. MLX's tools said impossible.\n\nThe breakthrough? MLX's primitives didn't care about the restriction.\n\n🎯 The Architecture:  \n\\- Split 128 experts into TWO blocks (9 FP16 + 119 4-bit)  \n\\- Map router indices on-the-fly (expert 21 → local ID 0 in FP16 block)  \n\\- Run both blocks in parallel (gather\\_mm + gather\\_qmm)  \n\\- mx.where selects the right output\n\nThe entire \"hack\"? \\~15 lines of conditional routing.\n\nThe lesson: When workflows don't fit, trust the primitives.\n\nMLX's high-level tools said \"one precision only.\" But gather\\_mm, gather\\_qmm, and mx.where were always capable of more.\n\n🔗 Full technical breakdown: [Blog Link](https://open.substack.com/pub/prasannakanagasabai126786/p/mlx-said-no-to-mixed-precision-we?r=40juy&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\n🤗 Quantized model (HF): [PKSGIN/qwen3-30b-selective-quant-MixedMPW-mlx](https://huggingface.co/PKSGIN/qwen3-30b-selective-quant-MixedMPW-mlx)",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qx6wz8/mlx_said_no_to_mixed_precision_we_did_it_anyway/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3vbgzz",
          "author": "BrilliantArmadillo64",
          "text": "Any chance of applying this to qwen3-coder-next?",
          "score": 3,
          "created_utc": "2026-02-06 08:31:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vrf29",
              "author": "bobby-chan",
              "text": "If you don't want to wait.\n```\nmlx_lm.convert \\\n    --model Qwen/Qwen3-Coder-Next \\\n    -q \\\n    --quant-predicate mixed_3_6\n```\n\nMixed quant have been in mlx-lm for about a year now.",
              "score": 2,
              "created_utc": "2026-02-06 11:01:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vzmz6",
                  "author": "Concert_Dependent",
                  "text": "This way does not allow you to choose the Experts you want to have higher FP and which we can degrade. ",
                  "score": 2,
                  "created_utc": "2026-02-06 12:07:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vxvfm",
          "author": "Concert_Dependent",
          "text": "This way of convert does not allow you to quantize layers which light up for experts we want like I choose keep higher precision for security rest lower. \n\nThe approach I do allows me do this via mx.where",
          "score": 1,
          "created_utc": "2026-02-06 11:54:03",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3w2jg3",
              "author": "ChocomelP",
              "text": "What does it do? Assume I don't know anything.",
              "score": 1,
              "created_utc": "2026-02-06 12:27:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w8b6z",
                  "author": "Concert_Dependent",
                  "text": "once Router picks a expert, mlx.where allows us to run a condition and the outcome of the condition can be user to pick different experts one Qunatized  other one full FP.   \n  \nI wrote more in this blog link.   \n  \n[https://open.substack.com/pub/prasannakanagasabai126786/p/mlx-said-no-to-mixed-precision-we?r=40juy&utm\\_campaign=post&utm\\_medium=web&showWelcomeOnShare=true](https://open.substack.com/pub/prasannakanagasabai126786/p/mlx-said-no-to-mixed-precision-we?r=40juy&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true) ",
                  "score": 1,
                  "created_utc": "2026-02-06 13:04:55",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}