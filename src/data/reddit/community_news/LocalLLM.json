{
  "metadata": {
    "last_updated": "2026-02-18 17:29:57",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 207,
    "file_size_bytes": 231006
  },
  "items": [
    {
      "id": "1r4mcwb",
      "title": "Built a 6-GPU local AI workstation for internal analytics + automation â€” looking for architectural feedback",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r4mcwb",
      "author": "shiftyleprechaun",
      "created_utc": "2026-02-14 14:43:58",
      "score": 191,
      "num_comments": 99,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4mcwb/built_a_6gpu_local_ai_workstation_for_internal/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5cjx4r",
          "author": "jhenryscott",
          "text": "Greta job! Taking control of your own LLMs without giving all your data to Scam Altman Is a good practice.  \n\nI donâ€™t have any insight into building your system except to recommend the Level1Techs forum. Youâ€™ll find lot other people doing this there who can answer your Questions",
          "score": 28,
          "created_utc": "2026-02-14 15:01:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cnwti",
              "author": "shiftyleprechaun",
              "text": "awesome man, appreciate it, i will check out the sub!",
              "score": 3,
              "created_utc": "2026-02-14 15:23:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gltck",
                  "author": "ThinkPad214",
                  "text": "Damn man. Nice build, I've got a old gaming PC with a 3060 12gb and 5060 ti 16gb learning how to properly set up smaller Agentic AI for personal and professional use.",
                  "score": 2,
                  "created_utc": "2026-02-15 05:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5clpbg",
          "author": "KnownAd4832",
          "text": "1 Usually the bottleneck is always vram then hardware support (multi-gpu/cluster inference is usually hard to set up with little documentation or its gatekept). If that makes sense, then usually third comes in Storage :)\n\n2 Long term is same as running a single gpu.\n\n3 It all depends on your case, my ROI was done in 2 months from buying\n\n4 multiple smaller nodes - the models are going stronger while being smaller. In 2 years there will be Kimi K2.5 level in 70B without a doubt. So it only depends on you if you need inference speed or variety if models\n\n5 They dont test on rented servers before buying imo. Did this mistake myself with first rig",
          "score": 10,
          "created_utc": "2026-02-14 15:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cox9m",
              "author": "shiftyleprechaun",
              "text": "Really appreciate the detailed breakdown.\n\n1. Makes total sense, VRAM is the hard ceiling and everything else is optimization after that. The documentation gap for multi-GPU is real, I've spent more time piecing together config from GitHub issues than actual docs.\n\n2. Good to hear. I'm mixing card types but pinning models deliberately so each card basically runs independently.\n\n3. That's a fast ROI, are you running inference as a service or using it for internal workloads?\n\n4. This is the point I keep coming back to. The trend toward stronger smaller models is exactly why I went with a distributed multi-card approach rather than dumping everything into one or two massive GPUs. If a 70B model can do what a 400B does today, having flexible GPU allocation matters more than raw single-card VRAM.\n\n5. Haha noted. I did a lot of benchmarking and testing before committing to the full build, but I can see how easy it would be to overbuy on hardware that doesn't match your actual workload.  I also spent around $5-6k using open AI API (Using Chat GPT 4o) via Azure in December.\n\n",
              "score": 4,
              "created_utc": "2026-02-14 15:28:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v65jb",
              "author": "nakedspirax",
              "text": "How did you get ROI if you don't mind me asking",
              "score": 1,
              "created_utc": "2026-02-17 13:52:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vm9i5",
                  "author": "KnownAd4832",
                  "text": "I was paying together.ai for inference. So I just bought the rig and replaced that cost :)",
                  "score": 1,
                  "created_utc": "2026-02-17 15:16:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dc1vo",
          "author": "FinancialMoney6969",
          "text": "Looks great man, I like the open air aspect",
          "score": 6,
          "created_utc": "2026-02-14 17:26:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dcdqe",
              "author": "shiftyleprechaun",
              "text": "Thank you. I went with the open air concept, bc I found it difficult to find an enclosure that would work.",
              "score": 2,
              "created_utc": "2026-02-14 17:27:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e049q",
          "author": "chafey",
          "text": "Your build is awesome, I am doing something very similar.  Here are some improvements you may want to consider:\n\n1. Upgrade your processor to improve your memory bandwidth.  You have a Threadripper Pro CPU and Motherboard which is better than the non pro Threadripper systems for two reasons: a) more PCIe lanes and b) 8 memory channels.  Unfortunately the 9955wx CPU only has two CCDs so can't utilize all 8 memory channels - you need a CPU with more CCDs (such as the 9965wx) to make use of the 8 memory channels.  [https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa\\_the\\_new\\_threadripper\\_pros\\_9000\\_wx\\_are\\_still/](https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/)\n2. PCIe 5.0 has benefits for AI.  The 3090tis are good bang/buck but they are running over PCIe 4.0 so can't take advantage of the new PCIe 5.0 features.  The actual benefit of an all PCIe 5.0 solution depends on your use case and model but it is more than just twice the bandwidth. [https://www.graniteriverlabs.com/en-us/technical-blog/pcie-gen-5-ai-ml](https://www.graniteriverlabs.com/en-us/technical-blog/pcie-gen-5-ai-ml)\n3. The up coming Apple M5 systems may very well be the best bang/buck due to their recently released RDMA over TB5, MLX AI acceleration and its high speed unified memory architecture.  I am really looking forward to seeing how a cluster of M5 mac minis does.  Check out exo: [https://github.com/exo-explore/exo](https://github.com/exo-explore/exo)",
          "score": 5,
          "created_utc": "2026-02-14 19:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5easi7",
              "author": "shiftyleprechaun",
              "text": "I have throught about upgrading the CPU. And also possibly replacing the 3090s entirely . However, I can never bring myself to get a Mac/apple product for anything , yeah I'm one of those guys hah.",
              "score": 3,
              "created_utc": "2026-02-14 20:23:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5e6k0h",
              "author": "starkruzr",
              "text": "my understanding is that what really counts for inference is keeping latency between cards as low as possible rather than super high bandwidth. does this mean PCIe 5's latency is naturally lower than 4's?\n\nthat note about M5 is interesting. I keep waiting for M5 Ultra to be announced!",
              "score": 2,
              "created_utc": "2026-02-14 20:00:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5e8clz",
                  "author": "chafey",
                  "text": "Yes, PCIe 5 has about 50% lower latency compared to PCIe 4.  \n\nHere is a good video on mac clustering with exo: [https://www.youtube.com/watch?v=4l4UWZGxvoc](https://www.youtube.com/watch?v=4l4UWZGxvoc)",
                  "score": 5,
                  "created_utc": "2026-02-14 20:10:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5fqs11",
                  "author": "BillDStrong",
                  "text": "Since speed and latency are properties of the underlying physics, going twice the bandwidth has a 2x latency speedup.\n\nThis is also true of internet, so 10G is 10x speedup latentency than 1G, 40G is 40x, and 100G is 100x.\n\nThis is why Nvidia is  able to use 200G and 400G in their Enterprise AI offerings to sub for PCIe, the latency is so close to not make a difference at those speeds.",
                  "score": 2,
                  "created_utc": "2026-02-15 01:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gqqv7",
          "author": "OwnPomegranate5906",
          "text": "I run 6 rtx 3060s (12GB each) on a dual processor xeon 2690 v4 and 128GB of ram, also in an open air mining case.  I only run Debian 12, one instance of ollama and only one model at a time.\n\nI spent a fair amount of time on PCIe and power tuning.  A lot of performance problems that I had was because when running a larger model that spanned multiple cards, the latency caused by traversing the PCIe bus (and sometimes Numa node) was just long enough that the cards power management would bump it from P2 to P5+, then jump back up to P2 when it actually needed to do something, which added even more latency, which caused cascading slowdown across all the cards where I'd start off really fast, then as the response typed out, it would progressively slow down to the point of just a couple tokens a second.  My setup is not PCIe x16 for every card, so that also exacerbated the issue.\n\nThe solution was multi-fold: \n1. set the power management so that the cards idle at P3 and jump to P2 for inference.  This fixed a huge amount of performance problems with larger contexts and responses.  \n2. Put the cards that actually have PCIe x16 connections on the ends of the chain and the x8 cards in the middle.  Also, order the cards (in ollama.service) so that all the cards on each numa node are together to minimize the number of cross numa jumps during inference.  \n3. Also in the bios, set the PCI max request packet size to 4096.  On my particular system, it defaulted to 128.  Turns out the 3060s won't do more than 256, but it's worth more than a couple tokens a second.\n4. Also in the BIOS, turn rebar on so the system can see and address the entire PCIe VRAM instead of addressing it in 256MB chunks, also worth a few more tokens per second.",
          "score": 4,
          "created_utc": "2026-02-15 05:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cnveg",
          "author": "King_Kasma99",
          "text": "I never thought about looking for wireless dp or hdmi adaptors, Thank you!",
          "score": 3,
          "created_utc": "2026-02-14 15:23:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cpk3u",
              "author": "shiftyleprechaun",
              "text": "The adapters aren't wireless, they are \"Dummy Plugs\" so i can remote in and use dual monitors.  here is link to one i bought.\n\n[https://www.amazon.com/Headless-Emulator-Compatible-Multiple-Resolutions/dp/B0B5XVQVJ9/ref=sxts\\_b2b\\_sx\\_fused\\_v3\\_desktop\\_ref-tab-0?content-id=amzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f%3Aamzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f&crid=3PB8RH0689B4E&cv\\_ct\\_cx=hdmi%2Bdummy%2Bplug&keywords=hdmi%2Bdummy%2Bplug&pd\\_rd\\_i=B0B5XVQVJ9&pd\\_rd\\_r=d68b8781-951e-4d4e-8ac6-d9ddc6db8114&pd\\_rd\\_w=c37v9&pd\\_rd\\_wg=PdjXx&pf\\_rd\\_p=820daa87-0f14-4ace-aeae-d9224f14cf8f&pf\\_rd\\_r=TB8XZ8AJ3PZ9GVKVH0YF&qid=1771083053&sbo=RZvfv%2F%2FHxDF%2BO5021pAnSA%3D%3D&sprefix=hdmi%2Bdummy%2Bpolug%2Caps%2C99&sr=1-6-c3caa9c6-537b-4b39-bbc5-5db9f871bef5&th=1](https://www.amazon.com/Headless-Emulator-Compatible-Multiple-Resolutions/dp/B0B5XVQVJ9/ref=sxts_b2b_sx_fused_v3_desktop_ref-tab-0?content-id=amzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f%3Aamzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f&crid=3PB8RH0689B4E&cv_ct_cx=hdmi%2Bdummy%2Bplug&keywords=hdmi%2Bdummy%2Bplug&pd_rd_i=B0B5XVQVJ9&pd_rd_r=d68b8781-951e-4d4e-8ac6-d9ddc6db8114&pd_rd_w=c37v9&pd_rd_wg=PdjXx&pf_rd_p=820daa87-0f14-4ace-aeae-d9224f14cf8f&pf_rd_r=TB8XZ8AJ3PZ9GVKVH0YF&qid=1771083053&sbo=RZvfv%2F%2FHxDF%2BO5021pAnSA%3D%3D&sprefix=hdmi%2Bdummy%2Bpolug%2Caps%2C99&sr=1-6-c3caa9c6-537b-4b39-bbc5-5db9f871bef5&th=1)",
              "score": 3,
              "created_utc": "2026-02-14 15:32:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d6lf7",
          "author": "Senior-Okra-6325",
          "text": "Ver tanta GPU junta es hipnotico. Solo quiero imaginarme el nivel de ruido. Mi rig es modesto porque solo lo uso para analisis cualitativo de transcripciones biograficas (3090 + 4070). Te envidio tu config, un saludo!",
          "score": 3,
          "created_utc": "2026-02-14 16:58:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dc2jl",
              "author": "shiftyleprechaun",
              "text": "If I didn't have two additional external fans blowing on it, it wouldn't be that loud tbh.",
              "score": 3,
              "created_utc": "2026-02-14 17:26:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dknvg",
          "author": "Character-Ad-2048",
          "text": "This is an awesome build. Funny I just bought the same frame myself. Do you mind sharing your CPU/Motherboard specs and why you went with your decision? Iâ€™m looking to upgrade from my current setup from 4 GPUs and need to find a new CPU/Motherboard",
          "score": 2,
          "created_utc": "2026-02-14 18:09:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dufeh",
              "author": "shiftyleprechaun",
              "text": "Sure. I went with my own setup, because for what I am trying to accomplish, I didn't want to be beholden to the giant corporations. And I did already create everything that I am now doing locally through azure and open AI APIs and I spent thousands of dollars on it.\n\nFor what i ran all night last night, would have cost me $3-4k with open AI chat gpt 4oz vs $6 in electricity. Of course I have spent a lot of money on the hardware, but the roi will hopefully prove itself over the coming years.\n\nUpdated â€“ February 13, 2026\r\nOS - Ubuntu 24.04 LTS Desktop\r\nMotherboard - ASUS WRX90E-SAGE Pro WS SE AMD sTR5 EEB\r\nCPU - AMD Ryzen Threadripper PRO 9955WX Shimada Peak 4.5GHz 16-Core sTR5\r\nSDD â€“ (2x4TB) Samsung 990 PRO 4TB Samsung V NAND TLC NAND PCIe Gen 4 x4 NVMe M.2 Internal\r\nSSD\r\nSSD - (1x8TB) Samsung 9100 PRO 8TB Samsung V NAND TLC NAND (V8) PCIe Gen 5 x4 NVMe M.2\r\nInternal SSD with Heatsink\r\nPSU #1 - SilverStone HELA 2500Rz 2500 Watt Cybenetics Platinum ATX Fully Modular Power Supply - ATX\r\n3.1 Compatible\r\nPSU #2 - MSI MEG Ai1600T PCIE5 1600 Watt 80 PLUS Titanium ATX Fully Modular Power Supply - ATX 3.1\r\nCompatible\r\nPSU Connectors â€“ Add2PSU Multiple Power Supply Adapter (ATX 24Pin to Molex 4Pin) and Daisy Chain\r\nConnector-Ethereum Mining ETH Rig Dual Power Supply Connector\r\nUPS - CyberPower PR3000LCD Smart App Sinewave UPS System, 3000VA/2700W, 10 Outlets, AVR, Tower\r\nRam - Kingston FURY Renegade Pro 256GB (8 x 32GB) DDR5-5600 PC5-44800 CL28 Quad Channel ECC\r\nRegistered Memory Modules KF556R28RBE2K4-128\r\nCPU Cooler - Thermaltake WAir CPU Air Cooler\r\nGPU Cooler â€“ (6x) Arctic P12 PWM PST Fans (externally mounted)\r\nCase Fan Hub â€“ Arctic 10 Port PWM Fan Hub w SATA Power Input\r\nCase/Build â€“ Open air Rig\r\nGPU 1 - PNY RTX 6000 Pro Blackwell\r\nGPU 2 â€“ PNY RTX 6000 Pro Blackwell\r\nGPU 3 â€“ FE RTX 3090 TI\r\nGPU 4 - FE RTX 3090 TI\r\nGPU 5 â€“ EVGA RTX 3090 TI\r\nGPU 6 â€“ EVGA RTX 3090 TI\r\nUninstalled \"Spare GPUs\":\r\nGPU 7 - Dell 3090 (small form factor)\nGPU 8 - Zotac Geforce RTX 3090 Trinity\r\nPossible Expansion of GPUs â€“ Additional RTX 6000 Pro Maxwel",
              "score": 2,
              "created_utc": "2026-02-14 18:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5eebyy",
                  "author": "RedditNoob001",
                  "text": "Hey amazing build! Do you mind sharing what riser cables (brand/length) worked for you? Trying to figure out which pcie 4.0 cables I should get for my 3090s. ",
                  "score": 2,
                  "created_utc": "2026-02-14 20:43:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ff1rj",
                  "author": "Character-Ad-2048",
                  "text": "Thatâ€™s smart to have done that early test to ensure. I just want to say thank you for a really detailed breakdown of the components! Really appreciate it and can see a lot of thought went into it!",
                  "score": 2,
                  "created_utc": "2026-02-15 00:14:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ftahr",
                  "author": "Metrix1234",
                  "text": "Did you use any guides making your local LLM build?  Iâ€™m very curious the use cases of building one vs API vs CLI.  Do you actually need to train your local LLM?\n\nIâ€™m pretty new to even using the big production models but the more I use them the more I fear eventually they will be gatekeeped for only rich/enterprise consumers. \n\nWould love to learn more about the cost of your setup and what itâ€™s capable of.",
                  "score": 2,
                  "created_utc": "2026-02-15 01:45:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fcoof",
          "author": "Just3nCas3",
          "text": "Level1 just put out a [video recently about bifurcation](https://www.youtube.com/watch?v=Dd6-BzDyb4k), something to look into if you want to add gpus without replacing any and a quick search of your motherboard says it supports it.",
          "score": 2,
          "created_utc": "2026-02-15 00:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fct75",
              "author": "shiftyleprechaun",
              "text": "Awesome I'll check it out ty",
              "score": 2,
              "created_utc": "2026-02-15 00:00:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5i783u",
          "author": "greg-randall",
          "text": "Have you figured out your software stack?",
          "score": 2,
          "created_utc": "2026-02-15 13:39:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5irm8p",
              "author": "shiftyleprechaun",
              "text": "Not quite yet. \n\nBut for This first phase I am running:\n4 separate instances of llama 3.1 8B (1 on each the 3090's)\n2 separate instances of llama 3.3 70B (1 on each of the Maxwell 6000 pros)\n\nAfter I have the initial data analysis complete, I still need to figure out the next steps, which I am working on now.",
              "score": 1,
              "created_utc": "2026-02-15 15:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5igm1e",
          "author": "Fair_Permission_9005",
          "text": "Power consumption? \n\n",
          "score": 2,
          "created_utc": "2026-02-15 14:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ish9u",
              "author": "shiftyleprechaun",
              "text": "With all gpus running Max around 3.35kwh",
              "score": 1,
              "created_utc": "2026-02-15 15:37:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kc6ay",
          "author": "basedgewner",
          "text": "Iâ€™m coming.",
          "score": 2,
          "created_utc": "2026-02-15 20:10:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ki0xq",
              "author": "shiftyleprechaun",
              "text": "Come on over ðŸ˜‚",
              "score": 1,
              "created_utc": "2026-02-15 20:40:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5meuv7",
          "author": "lcjanke2020",
          "text": "That is one sick rig!",
          "score": 2,
          "created_utc": "2026-02-16 03:21:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nyrxh",
              "author": "shiftyleprechaun",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-16 11:17:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5t0dqo",
          "author": "Physical-Scholar3176",
          "text": "I don't think most of us can spend new car money for this. Still very cool. What is your job?\n\n",
          "score": 2,
          "created_utc": "2026-02-17 03:35:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ujai9",
              "author": "shiftyleprechaun",
              "text": "I own and run a small logistics company.",
              "score": 1,
              "created_utc": "2026-02-17 11:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wkf7r",
          "author": "hetarthvader",
          "text": "So cool!",
          "score": 2,
          "created_utc": "2026-02-17 18:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xprfh",
          "author": "tachyi",
          "text": "amazing!",
          "score": 2,
          "created_utc": "2026-02-17 21:18:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60wa0i",
          "author": "Aware_Photograph_585",
          "text": "I have: 2x 3090, 3x 4090 48gb, 2x pro 6000\n\n1) I don't trust pcie riser cables. If you have any errors during training: timeout, gpu fell off (I forget the exact wording), weird training speeds, etc, it's most likely your cables.  You need pcie retimer/redriver card with those cable lengths.  Especially with pcie5.  Check any professional training servers, they have retimer/redriver if the gpu is not plugged into the pcie slot.  Heck, my HBA cards have a retimer to connect to my HDD/NVME.  On the other hand, inference tends to be fine with pcie cables.  Have you done any serious multi-day/week training yet? \n\n2) CPU doesn't matter as much as people think.  I use an 8 core AMD 7002 cpu, never maxed it out once during training.  Just cache everything (latents/embeds/etc) before training.  Which you should be doing anyway to save vram during training.\n\n3) PCIE speed also doesn't matter as much as people think. Going from x16 to x8 only decreases training speed a couple percent.  There isn't a large amount of data exchanged between gpus during training, assuming you wrote a good trainer.  PCIE latency is the real killer.\n\n4) RAM is awesome.  You can do so much more with more ram.  GPU offset to ram, keeping everything in ram, using ram to write temp files to, EMA can done on cpu, etc.  Having more ram makes everything easier.  I just upgraded to 1TB for dataset curation use.\n\n5) HDD space is also awesome.  I have \\~400TB, it's not enough.  Just being able keeping a copy of fully prepped, ,ready to train, cached dataset on disk with the original data is so nice.   Then you can store your embeddings, and whatever else on disk with the original data.   \n  \nRam and disk space used properly can reduce vram usage during training, and significantly speed up training, especially for multi-run training.",
          "score": 2,
          "created_utc": "2026-02-18 09:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61ufrc",
              "author": "shiftyleprechaun",
              "text": "Thanks for the feedback! A lot of good info here for me to take in, I appreciate the time you took.\n\nI haven't began any training phases yet. I have just been analyzing massive amounts of company data to build into a combo of duckdb, neo4j and embedding graph. I will deploy a chatbot integrated into teams for the users to start using and store all of the query info so I can use that info to them finetune the models for better output.\n\nThe training aspect, is something I really have zero experience with, so I will learn about it while I am attempting to build out that pipeline, but I am still a bit away from that point.\n\nAnother commenter posted a good video about pcie retimer/Redriver cards, honestly was the first time I heard about them. I have to look into it more, from what I gathered they are pretty pricy, but at this point, if it will improve things, then I will make the investment. I have to get a better/larger open air rig, bc the one I have is too small.\n\nSo CPU I can leave as is then , I mean it's not the top of the line, but it's up there.\n\nDo you think I should get more ram? The prices of course are out of control. I initially bought 128 GB for 1600, then decided a week later I wanted 256 total, so when I went to buy another 128 gb kit the price was already up to $2k and when I check yesterday that same kit was now $3k. Nuts!\n\nI only have the 3 nvme ssds. I am storing all of the models on the 9100 and everything else on the 990 pros. I was thinking about getting some mechanical sata HDDs for long term storage and backups maybe 8-10TB.\n\nThanks again. I also do not yet know how to optimize/comfigure the HDD and the ram as per your recommendations, but I will add all of this to the seemingly endless list of things I need to figure out - hah. \n\nThanks again! \n\nBtw - I ended up adding a 3rd blackwell. So now I have 3x 6000 pro Blackwells and 4 x 3090 tis. All 7 pcie slots are used.",
              "score": 1,
              "created_utc": "2026-02-18 13:38:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o62vdlw",
                  "author": "Aware_Photograph_585",
                  "text": "I have a pretty similar open air rig.  Started with 2x 3090 and slowly built up from there.  \nI have zero experience with training LLMs, but I have written my own fully custom multi-gpu text-to-image trainers.  I assume my training experience with transfer over to what you're doing, but maybe LLM training has it's own quicks.\n\nPCIe retimer/redriver:   \nWhen I first started learning programming, I lost 6 months of training time due to pcie cables.  Gpu, fell off the bus, time outs, really strange training speeds (one run would be 2-3x the speed of the next)  Asked everyone everywhere, no one had any idea what was wrong.  Re-wrote my scripts, bought the most expensive cables I could find, thought I was going crazy, and nearly gave up.  Tried a retimer card, problem solved.  I learned a lot about multi-gpu training during that time, but that's not the best way to learn.  I also burnt the retimer chip on my first card, so use fans on your retimer/redrivers.   \n  \nFor me, retimer/redriver card is non-optional for training.  I won't even consider training without one.  If an problem shows up, how do I know it's not the cables?  I bought my cards for \\~$150 each.  They do x16 or 2 x8.  Make sure you ask the seller if they support 2x 8/x16.  A lot of cards support 4 x4 by default, and the seller needs to flash the card to support 2x 8/x16. You'll also need the cables and the pcie boards.\n\nAlso, pcie cables seem fine for inference.  I never had a problem with inference when using pcie cables.\n\nRam:  \nRam prices are stupid, I know.  I just bought 1TB DDR4 2666mhz ram for $3800, I think I did OK for current market.  3200mhz would have been double the price, I don't even want to think about what ddr5 costs.  The 1tb ram is to be used for the image dataset sorting: embedding clustering and such.  \n  \nRam really benefits for fast gpus with low vram.  When I used 2x 3090 to train the sdxl unet, I had to use FSDP GRAD\\_SHARD\\_OP to get the model to fit into memory.  But there still wasn't enough vram room for a large enough batch size to saturate the gpu core.  So I used just enough ram via cpu\\_offset to saturate the core.  I lost some speed due to the cpu\\_offset, but I gained more speed via  increased batch size for core utilization.  Ram will help your 3090s more than the pro 6000.\n\nStandard training advice is ram = 2x total vram, and on my two 3090s, I was using \\~100GB ram during training.  A lot of the open source trainers focus on being conservative with vram/ram, so it won't matter as much.  But if you're writing your own code, there is just so much you can do with more ram: holding extra models, prefetch batches, ema, etc.    \n  \nFor standard consumer gpus, I highly suggest 2x total training gpu vram.  But maybe LLM training can use less?  And the pro6000s aren't vram starved. So, who knows? Regardless, 256 is enough to get started.    \n  \nFor ram (and vram): size is more important than speed.  Speed is nice, but if you don't have enough size, you're screwed.  So if you see a good deal on slightly slower speed, but a larger amount, buy it.\n\nHDDs:  \nI do blocks of 8 HDDs.  Most motherboards have 8 sata ports, HBA cards have 8/16/24, so 8 is a good number.  ZFS is the file system of choice, it's near impossible to lose your data.  I use raidz2 with a L2ARC metadata cache partition on a cheap sdd.  The ssd cache significantly speeds up the HDDs. It's fast enough for text-to-image training, don't know about LLM.  I buy used enterprise HDDs, no problems so far. Buy from a reputable seller and you're good.\n\nYou're cpu is most likely fine, unless LLM training has crazy cpu requirements.  Ram and HDDs will benefit you more.\n\nAlso, consider what you want to do with the pro6000s & 3090s.  Should they really be in the same machine?  I can't imagine they're be used together, unless you're running some huge model just for fun.  Maybe buy an old AMD 7002/7003 board and split off the 3090s?  \n  \nI will use my pro6000s for training with a 4090 48GB for support (validation/ema/etc) and inference. All the other gpus will be used for dataset curation.  So I'm considering moving 2x 4090 48GB and 2x 3090 to the data server, and leaving the 2x pro6000 and a 4090 48GB in the training server.",
                  "score": 1,
                  "created_utc": "2026-02-18 16:37:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dv7np",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-14 19:01:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eux2w",
              "author": "shiftyleprechaun",
              "text": "I definitely will.",
              "score": 2,
              "created_utc": "2026-02-14 22:13:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dve5w",
          "author": "chafey",
          "text": "what frame and motherboard are you using?  I have a ASUS WRX90E-SAGE Pro WS SE AMD sTR5 EEB Motherboard and am having trouble finding a frame which can hold the EEB motherboard",
          "score": 1,
          "created_utc": "2026-02-14 19:02:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dzm1b",
              "author": "shiftyleprechaun",
              "text": "I ended up settling with this \"mining frame\" from amazon, it's not perfect, but it works. \n\nhttps://a.co/d/0ard2K7F\n\nI have the same.motherboard and it doesn't fit perfectly with the predrilled holes, so I aligned it best I could with the pre-existing hole and only had to drill a few.\n\nThe two PSUs are abble to (just barely) fit on each side of the mb.",
              "score": 2,
              "created_utc": "2026-02-14 19:24:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e135s",
          "author": "Citizen_Edz",
          "text": "Very cool! Few 3090tis that i can see, what are the other cards? ",
          "score": 1,
          "created_utc": "2026-02-14 19:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5es05w",
              "author": "shiftyleprechaun",
              "text": "I bought LINKUP PCIE 5.0 Riser Cable | for Vertical GPU Mount.\n\nI bought a few 30 cm and a few 60cm.\n\nThese are a bit pricey, but they are compatible with pcie 4.0 as well.",
              "score": 1,
              "created_utc": "2026-02-14 21:57:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5etnxx",
              "author": "shiftyleprechaun",
              "text": "2x6000 pro maxwell & 4x3090ti",
              "score": 1,
              "created_utc": "2026-02-14 22:06:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5peamy",
                  "author": "Citizen_Edz",
                  "text": "Awsome! Very cool setup",
                  "score": 2,
                  "created_utc": "2026-02-16 16:19:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ecmbz",
          "author": "Proteus356",
          "text": "Iâ€™ve thought about using old mining rigs for this but all the risers are 1x, which isnâ€™t effective for LLM? How are you getting x16 or at least x8 connectivity to those GPUs?",
          "score": 1,
          "created_utc": "2026-02-14 20:33:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5emf9s",
              "author": "Brilliant-Suspect433",
              "text": "From what i can see, he didnt use risers but extension cablesâ€¦",
              "score": 1,
              "created_utc": "2026-02-14 21:27:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eswc9",
                  "author": "shiftyleprechaun",
                  "text": "No I used risers: LINKUP PCIE 5.0 Riser Cable | for Vertical GPU Mount.\n\nCombo of 30 & 60 cm.\n\nI realized today that I need to move the last 3090 into slot 7, rather than 6.\n\nhttps://preview.redd.it/1tgi89f49jjg1.jpeg?width=1034&format=pjpg&auto=webp&s=0fb36e7f733fa61358a83dbc9ac69f51e22a865b",
                  "score": 1,
                  "created_utc": "2026-02-14 22:02:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5etayx",
              "author": "shiftyleprechaun",
              "text": "Also, nothing I bought has been used for mining. The only used components are the 3090s , which of course I cannot guarantee weren't used for mining, but based on when I met each person (buying them off of FB marketplace), they all told me they wernt using for mining, just gaming or photo/video and they upgraded to 5090.\n\nAside from the 3090s everything is brand new and assembled together myself.",
              "score": 1,
              "created_utc": "2026-02-14 22:04:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5eedx0",
          "author": "Weird-Abalone-1910",
          "text": "What did this cost you to put together? I want to build one to try to run kimi2.5",
          "score": 1,
          "created_utc": "2026-02-14 20:43:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etgzn",
              "author": "shiftyleprechaun",
              "text": "Total all in, including new dedicated electrical circuits installed, around $50k.",
              "score": 1,
              "created_utc": "2026-02-14 22:05:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5eu9ag",
                  "author": "Weird-Abalone-1910",
                  "text": "Wow, thanks for sharing.",
                  "score": 2,
                  "created_utc": "2026-02-14 22:10:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5esbr3",
          "author": "ApprehensiveView2003",
          "text": "what did you do to fix when it originally doesnt allow P2P?",
          "score": 1,
          "created_utc": "2026-02-14 21:59:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eufa7",
              "author": "shiftyleprechaun",
              "text": "Enable IOMMU (SVM Mode), set PCIe ACS override, disable ASPM/SR-IOV/C-States, force PCIe Gen 4, and if you're still having issues add pcie_acs_override=downstream,multifunction to your kernel boot parameters. The WRX90 workstation boards need all of this dialed in for clean multi-GPU P2P.",
              "score": 1,
              "created_utc": "2026-02-14 22:11:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ew17n",
          "author": "Correct_Lead_2418",
          "text": "Can I ask your budget for this? For 10k you could get the highest tier Mac studio with 512GB unified memory and 4tb storage",
          "score": 1,
          "created_utc": "2026-02-14 22:20:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f3m5w",
              "author": "shiftyleprechaun",
              "text": "I spent around $50k. But there is no way a mac with 512gb unified memory could do what this can. I can also continue to add to this as needed. Not so easy with Mac.\n\nAlso I hate apple and macs ðŸ˜‰.",
              "score": 1,
              "created_utc": "2026-02-14 23:04:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g7w46",
                  "author": "lenjet",
                  "text": "For that kind of money you could have got circa 8-10 DGX Spark or an OEM and ended up with 1024-1280gb of unified VRAMâ€¦ coupled with the smaller footprint and much lower power consumption.\n\nDid you consider that?",
                  "score": 1,
                  "created_utc": "2026-02-15 03:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5g1tlg",
          "author": "Prigozhin2023",
          "text": "Is it cheaper than 2 x Dgx Spark?",
          "score": 1,
          "created_utc": "2026-02-15 02:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g2ce3",
              "author": "shiftyleprechaun",
              "text": "What I built? No.",
              "score": 1,
              "created_utc": "2026-02-15 02:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g357d",
                  "author": "Prigozhin2023",
                  "text": "Trying got understand which is the more cost effective build. One like yours, vs DGX Spark vs Mac Studio.",
                  "score": 1,
                  "created_utc": "2026-02-15 02:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gbrpv",
          "author": "Big_Championship1291",
          "text": "I know nothing about building an ai workstation and Iâ€™m planning on building my own. What is the highest model you can use with this station with reasonable speed for heavy software development?",
          "score": 1,
          "created_utc": "2026-02-15 03:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gdn91",
              "author": "shiftyleprechaun",
              "text": "With what I built?",
              "score": 1,
              "created_utc": "2026-02-15 04:07:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ge92n",
                  "author": "Big_Championship1291",
                  "text": "Yes",
                  "score": 1,
                  "created_utc": "2026-02-15 04:12:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jpftt",
          "author": "kidflashonnikes",
          "text": "something to consider: with this motherboard, if you want to expand compute, get PCIe lane spitters, going from 16x per one lane (exluding the one pcie 8x lane on this motherboard) and split every 16x pcie gen 5 lane in pcie 8x, which will double your compute. I currently use 4 RTX PRO 6000s, and I am getting another 8 RTX PROs delivered in the next few weeks, and will be doing this. ",
          "score": 1,
          "created_utc": "2026-02-15 18:17:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jyhk4",
              "author": "shiftyleprechaun",
              "text": "And by doing this, you find it running faster? What are you typically doing? Training?",
              "score": 1,
              "created_utc": "2026-02-15 19:01:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jza42",
                  "author": "kidflashonnikes",
                  "text": "I run a team at one of the worlds largest privately funded AI labs. My team works on inserting BCI (brain computer interface) threads into live test subjects (willingly of course) with brain damage to compress brainvwave data in real time via LLMs due to the data being too large and variate with current software ect. I work on personal compute tasks. One of my personal projects is using AI to predict bad thoughts/crime thoughts. I am currently using my set up for trainining and inference. I am almost done with my research, effectively I need more training compute to complete the version 1 of the model where I will be able to successfully predict negative behaviour of humans based on AI throughput. Some people will hate me for doing this, I dont care, I need to feed my family and you would not believe the prices certain US companies are willing to pay for this new LLM ",
                  "score": 2,
                  "created_utc": "2026-02-15 19:05:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5jzsqx",
                  "author": "kidflashonnikes",
                  "text": "as for the motherboard - you can split the 16x lanes into 8x lanes (2 each from 1 16x lane), to double your compute. Dropping from 16x to 8x does not impact inference that much - it does impact training substantially more. My current set up (all housed in a phanteks serve rpro 2 TG case): 1 TB of RAM, 16 TB of NVMe, 96 core CPU (thread ripper pro), 2000 Watt PSU, AIO for the CPU, regular fans to for cooling, 4 RTX PRO 6000s (6 more on the way before I transition to an open rig case), Linux (ubuntu)",
                  "score": 2,
                  "created_utc": "2026-02-15 19:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mwytb",
          "author": "Common_Heron4002",
          "text": "Gotta ask, what do you do to ty all the GPU's together? allow the sharing of information of them or how to utilize them effectively? \n\n",
          "score": 1,
          "created_utc": "2026-02-16 05:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nyrhl",
              "author": "shiftyleprechaun",
              "text": "So far I've been running models independently on each one.\n\nI've tested a bit spreading llama 3 across the 4 3090s, but I haven't had to run anything larger (yet) than what can fit on a single 6000 pro.",
              "score": 1,
              "created_utc": "2026-02-16 11:17:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ogm03",
                  "author": "Common_Heron4002",
                  "text": "interesting so do you do anything with tying each model to help with processing or getting better results or you just one per type of process your running etc? \n\nI didnt think you could spread a model across multiple in vram...\n\n",
                  "score": 2,
                  "created_utc": "2026-02-16 13:27:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gfbve",
          "author": "Prize_Wolf_5859",
          "text": "Iâ€™m new to this AI game Brodie how you built this and how much did it cost",
          "score": 0,
          "created_utc": "2026-02-15 04:20:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r659df",
      "title": "Qwen3.5 is released!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xtgnyvb2stjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-16 09:34:21",
      "score": 106,
      "num_comments": 11,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r659df/qwen35_is_released/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5rs0pq",
          "author": "DiligentRanger007",
          "text": "How much vram needed ???",
          "score": 3,
          "created_utc": "2026-02-16 23:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sqnpj",
              "author": "ubrtnk",
              "text": "Yes",
              "score": 14,
              "created_utc": "2026-02-17 02:35:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5t9j3m",
              "author": "yoracale",
              "text": "Depends on your ram. I'd say at least 16gb. See the guide: https://unsloth.ai/docs/models/qwen3.5",
              "score": 2,
              "created_utc": "2026-02-17 04:36:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ts1sf",
          "author": "pghqdev",
          "text": "what non-mac setup would be equivalent to 512 M3 Ultra config?",
          "score": 3,
          "created_utc": "2026-02-17 07:04:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q7bfe",
          "author": "I_like_fragrances",
          "text": "I didn't know, just started downloading it now.",
          "score": 2,
          "created_utc": "2026-02-16 18:33:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qn83k",
              "author": "yoracale",
              "text": "Awesome, let us know how it goes!",
              "score": 1,
              "created_utc": "2026-02-16 19:48:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vx60z",
          "author": "phoenixfire425",
          "text": "Make sad sounds with 2 x RTX3090ti  Wish I could run this.  I love 2.5-coder, maybe a few weeks ill be able to get a version of this i can run on my hardware.",
          "score": 1,
          "created_utc": "2026-02-17 16:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61kcik",
          "author": "barkdender",
          "text": "Can I even use the 1 bit quantized version on my RTX 3080 12 GB without offloading or am I in for a bad time? ",
          "score": 1,
          "created_utc": "2026-02-18 12:39:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61uwbj",
              "author": "yoracale",
              "text": "Wil work but will be extremely slow, how much RAM do you have?",
              "score": 1,
              "created_utc": "2026-02-18 13:41:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o623u9s",
                  "author": "barkdender",
                  "text": "I had 256gb but sold some because well to offset cost. Down to 64gb",
                  "score": 1,
                  "created_utc": "2026-02-18 14:28:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62phc0",
          "author": "slyticoon",
          "text": "Why did we pick 3 shades of grey for the comparisons models...",
          "score": 1,
          "created_utc": "2026-02-18 16:10:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2t35r",
      "title": "Tutorial: Run GLM-5 on your local device!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/1047rus1c2jg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-12 13:13:49",
      "score": 99,
      "num_comments": 65,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r2t35r/tutorial_run_glm5_on_your_local_device/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4z7vu1",
          "author": "No_Clock2390",
          "text": "So you need like a 10K PC to run this?",
          "score": 22,
          "created_utc": "2026-02-12 13:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zgxbe",
              "author": "UseMoreBandwith",
              "text": "yes. Does that surprise you?",
              "score": 5,
              "created_utc": "2026-02-12 14:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zjgue",
                  "author": "No_Clock2390",
                  "text": "I guess not",
                  "score": 2,
                  "created_utc": "2026-02-12 14:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zuwuq",
              "author": "Prudent-Ad4509",
              "text": "Let's see... $700 per 24Gb vram GPU, and you need about 12 of them to run dynamic 2-bit with a little bit of context. That is already $8400. The epyc system with 128 PCI lanes and PCIe 4.0 will set you back another $1000 in current prices, and that is the price \\*without\\* ram or ssd. Well, add another $200 for whatever low ram stick you can find just to run the thing. The remaining $600 will go on connecting all that with x8 bifurcation. This is where you are out of money, but you need 2-3 of more good PSUs and at least one SSD drive.\n\nSo no, 10K PC will not run this. But 11K-12K PC could. Or you can try to run it in ram, but I would not call it \"running\".",
              "score": 5,
              "created_utc": "2026-02-12 15:34:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o513tiu",
                  "author": "Particular-Way7271",
                  "text": "Or in ssd or hdd",
                  "score": 1,
                  "created_utc": "2026-02-12 19:03:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zle3w",
              "author": "Salt-Willingness-513",
              "text": "Just alot of ram should work somewhat too i guess. At least if you dont need high speed. Ima try it on my 850gb ram server",
              "score": 2,
              "created_utc": "2026-02-12 14:47:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zngmg",
                  "author": "No_Clock2390",
                  "text": "What kind of server is it? How much did it cost?",
                  "score": 1,
                  "created_utc": "2026-02-12 14:57:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5264zx",
                  "author": "bLackCatt79",
                  "text": "what speed did you got?",
                  "score": 1,
                  "created_utc": "2026-02-12 22:07:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o514gfg",
              "author": "fallingdowndizzyvr",
              "text": "Nope. I run GLM on my little Strixy with a little help from a Mac and a 7900xtx. I might have to add another 7900xtx for GLM-5 since it's a bit bigger than 4.7.",
              "score": 2,
              "created_utc": "2026-02-12 19:07:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o514qtq",
                  "author": "No_Clock2390",
                  "text": "You're running the 1-bit version?",
                  "score": 1,
                  "created_utc": "2026-02-12 19:08:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bd58k",
                  "author": "Sociedelic",
                  "text": "Really? And how much ram  you'll need?",
                  "score": 1,
                  "created_utc": "2026-02-14 09:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o54t6e2",
              "author": "JacketHistorical2321",
              "text": "No, I can run 4bit on a server I paid a total of $2k. Between 6-8t/s depending on ctx. People just like to be dramatic",
              "score": 1,
              "created_utc": "2026-02-13 08:47:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zn4wi",
          "author": "not-really-adam",
          "text": "I wonder if running this in 1-bit, would provide better local coding results than qwen3-next-coder in 8-bit?",
          "score": 7,
          "created_utc": "2026-02-12 14:56:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50c61w",
              "author": "entr0picly",
              "text": "Thatâ€™s genuinely an open question in the field. The quantization vs parameterization curve has suggested that larger models at lower quant may perform better than smaller models at larger (or no) quant. There isnâ€™t a one size fits all answer. Itâ€™s at the frontier, and you have to test your own use cases yourself. Personally, testing 2bit deepseek R1, I found it generally did better with scientific work than qwen3, however it also tended to drift more quickly and maybe struggle a little more with memory.",
              "score": 7,
              "created_utc": "2026-02-12 16:54:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54wxa7",
              "author": "Septimus4_FR",
              "text": "I canâ€™t test it myself since I donâ€™t have the hardware for that setup, but in general 1-bit (and usually 2-bit) quants degrade too much to be great for coding.\n\nOnce you drop that low, models tend to hallucinate more, lose consistency and are not very usable in practice. For coding, that usually shows up as wrong APIs, subtle logic bugs, or broken refactors, invalid json generation. In practice, 4-bit is often considered the lowest â€œcomfortableâ€ range for usable quants.\n\nThat said, it really depends on the quantization method and how well itâ€™s done. A very good 3-bit quant of GLM-5 could actually be interesting to try. But Iâ€™d be very skeptical that a typical 1-bit GLM-5 would outperform an 8-bit Qwen coder for real coding work.",
              "score": 2,
              "created_utc": "2026-02-13 09:22:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zvp8o",
          "author": "Jumpy-Requirement389",
          "text": "So.. if I have 192GB of ddr5 and a 5090. Iâ€™ll be able to run this?",
          "score": 5,
          "created_utc": "2026-02-12 15:38:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwgzl",
              "author": "robertpro01",
              "text": "Probably, try it and share results :)",
              "score": 1,
              "created_utc": "2026-02-12 15:41:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5143aj",
                  "author": "Ell2509",
                  "text": "I think they might be mocking",
                  "score": 1,
                  "created_utc": "2026-02-12 19:05:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bdvkh",
              "author": "Sociedelic",
              "text": "Does DDR4 vs DDR5 matter when running LLM locally?",
              "score": 1,
              "created_utc": "2026-02-14 09:40:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zzakp",
          "author": "separatelyrepeatedly",
          "text": "honestly what even is the point with such small quants?",
          "score": 5,
          "created_utc": "2026-02-12 15:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o544kd5",
              "author": "yoracale",
              "text": "You can see benchmarks we did for 1-bit DeepSeek-V3.1 which is smaller than GLM: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\n3-bit is very good and surprisingly near full precision \n\nAlso if you don't want to use lower precision, just use higher precision",
              "score": 1,
              "created_utc": "2026-02-13 05:13:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50al9g",
          "author": "Kubas_inko",
          "text": "2-bit and 1-bit are gonna be absolutely worthless. 3-bit might be somewhat usable.",
          "score": 5,
          "created_utc": "2026-02-12 16:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50eksi",
              "author": "silenceimpaired",
              "text": "I have found 2 bit acceptable for my use for GLM 4.7. I suspect for some use cases 2 bit on GLM 5 will beat models at around the same size or a little lower. I prefer GLM 4.7 to GLM Air.",
              "score": 3,
              "created_utc": "2026-02-12 17:05:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50jn4p",
                  "author": "Kubas_inko",
                  "text": "From my own testing, and for my purpose, Q2 GLM 4.7 is worse than Q6 GLM 4.5 Air.",
                  "score": 1,
                  "created_utc": "2026-02-12 17:29:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o514k9j",
              "author": "fallingdowndizzyvr",
              "text": "That's not true at all. I run TQ1, 1 bit, and find it pretty darn usable.",
              "score": 2,
              "created_utc": "2026-02-12 19:07:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o544tmo",
              "author": "yoracale",
              "text": "You can see benchmarks we did for DeepSeek-V3.1 which is smaller than GLM: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\n1-bit is absolutely useable since it's dynamic. And 3-bit is much better though.\n\nAlso if you don't want to use lower precision, just use higher precision",
              "score": 2,
              "created_utc": "2026-02-13 05:15:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57lcgq",
          "author": "dreamer2020-",
          "text": "Many thanks master! \n\nI have couple questions, I have maxed out Mas studio, so 512gb. What I really found difficult is to test the models from unsloth against like glm 4.7 6-bit. I need to dive into how and what it means to be unsloth dynamic. \n\nMaybe stupid question, what is the best model in terms of agentic coding ? Like using it for openclaw ? What should you use ?",
          "score": 2,
          "created_utc": "2026-02-13 18:48:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zbsfr",
          "author": "TimWardle",
          "text": "I wonder if additional languages except from English REAPâ€™ed from the model can reduce the size further while maintaining usability.",
          "score": 2,
          "created_utc": "2026-02-12 13:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52o7x3",
          "author": "minilei",
          "text": "Dam what even is the performance to run this locally with actual consumer hardware.",
          "score": 1,
          "created_utc": "2026-02-12 23:43:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54rhcc",
              "author": "yoracale",
              "text": "With Mac maybe 15 tokens/s. With RAM + VRAM you can get 20 tokens/s. With GPU pure, then 100 tokens/s",
              "score": 1,
              "created_utc": "2026-02-13 08:31:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54ued2",
          "author": "lol-its-funny",
          "text": "I can save you $10k â€¦ just run the 0b quant. Itâ€™s incredibly fast, as if nothingâ€™s going on! Must try!",
          "score": 1,
          "created_utc": "2026-02-13 08:58:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54uy4m",
              "author": "yoracale",
              "text": "You can see benchmarks we did for 1-bit DeepSeek-V3.1 which is smaller than GLM: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs)\n\n3-bit is very good and surprisingly near full precision\n\nIf you don't want to use lower precision, just use higher precision.\n\nIf you don't want to run it at all, jsut run smaller models",
              "score": 1,
              "created_utc": "2026-02-13 09:03:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56bn0t",
          "author": "Dr-Coktupus",
          "text": "Makes zero sense to run locally, just run it from a cloud provider",
          "score": 1,
          "created_utc": "2026-02-13 15:09:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56qrna",
              "author": "yoracale",
              "text": "If you have the compute requirements, why not local?",
              "score": 1,
              "created_utc": "2026-02-13 16:22:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5pc7l6",
              "author": "stokdam",
              "text": "Makes zero sense to comment like this on r/LocalLLM ",
              "score": 1,
              "created_utc": "2026-02-16 16:09:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pcm52",
                  "author": "Dr-Coktupus",
                  "text": "It makes perfect sense, someone can get a different perspective. You think outside views have no places in subreddits and only single pov should be discussed? Lolololol",
                  "score": 1,
                  "created_utc": "2026-02-16 16:11:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o579yr3",
          "author": "lol-its-funny",
          "text": "Why do you guys never publish the K-L divergence of your quants against the ***unquantized model***???",
          "score": 1,
          "created_utc": "2026-02-13 17:54:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59gag8",
              "author": "yoracale",
              "text": "We did do benchmarks for many models previously here: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\nRunning kl divergence benchmarks are expensive and time consuming",
              "score": 1,
              "created_utc": "2026-02-14 00:40:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bfafb",
          "author": "emrbyrktr",
          "text": "Qwen 3 Coder does the same job as next 80b.",
          "score": 1,
          "created_utc": "2026-02-14 09:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bjxi5",
              "author": "Medical_Farm6787",
              "text": "So why this is under GLM post?",
              "score": 1,
              "created_utc": "2026-02-14 10:40:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5rhegf",
          "author": "throwaway5006001",
          "text": "where is GLM 5 flash gguf?",
          "score": 1,
          "created_utc": "2026-02-16 22:16:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zmec4",
          "author": "dropswisdom",
          "text": "Yeah.. No. You basically need a server farm to run this locally.",
          "score": 1,
          "created_utc": "2026-02-12 14:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50fa24",
          "author": "squachek",
          "text": "1 bit quant? GTFOH",
          "score": 1,
          "created_utc": "2026-02-12 17:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o514xn8",
              "author": "fallingdowndizzyvr",
              "text": "Try it. I think it runs fine.",
              "score": 3,
              "created_utc": "2026-02-12 19:09:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o544qzo",
              "author": "yoracale",
              "text": "You can see benchmarks we did for DeepSeek-V3.1 which is smaller than GLM: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\n1-bit is absolutely useable since it's dynamic. And 3-bit is much better though.\n\nAlso if you don't want to use lower precision, just use higher precision",
              "score": 2,
              "created_utc": "2026-02-13 05:15:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o549ihy",
                  "author": "squachek",
                  "text": "Just use higher precision he says! Sheesh. Iâ€™ll just will 256gb of VRAM into existence! ðŸ˜­",
                  "score": 0,
                  "created_utc": "2026-02-13 05:52:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zlvi5",
          "author": "rookan",
          "text": "Which bit do you recommend for software development to be as smart as Claude Opus 4.6?",
          "score": 0,
          "created_utc": "2026-02-12 14:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52hnrz",
              "author": "zekrom567",
              "text": "None locally will get there unless you have a lot of money to fork over. I'm liking the gpt-oss-120b so far agentic programming",
              "score": 1,
              "created_utc": "2026-02-12 23:07:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6e4h4",
      "title": "Anyone else spending more time tweaking than actually using their model?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r6e4h4/anyone_else_spending_more_time_tweaking_than/",
      "author": "Weirdboy212",
      "created_utc": "2026-02-16 16:24:28",
      "score": 83,
      "num_comments": 43,
      "upvote_ratio": 0.97,
      "text": "I swear Iâ€™ve spent 10x more time:  \n\\-comparing quants  \n\\-adjusting context size  \n\\-testing different system prompts  \n\\-watching tokens/sec\n\nthan actually asking it useful questions\n\nFeels like building a gaming PC and then only running benchmarks",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6e4h4/anyone_else_spending_more_time_tweaking_than/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5prvku",
          "author": "beisenhauer",
          "text": "\"Give me six hours to get a useful answer from an LLM, and I'll spend the first four tuning the model.\" -- Abraham Lincoln",
          "score": 19,
          "created_utc": "2026-02-16 17:22:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t7ynj",
              "author": "ANTIVNTIANTI",
              "text": "Ummmm Aktuallly I believe that was Thomas Jefferson",
              "score": 3,
              "created_utc": "2026-02-17 04:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5u77ri",
                  "author": "singh_taranjeet",
                  "text": "Common misconception. Jefferson preferred longer context windows but worse latency. Lincoln was more of a prompt engineer.\n\nBoth terrible at quant selection though...",
                  "score": 5,
                  "created_utc": "2026-02-17 09:27:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5t7z4z",
                  "author": "ANTIVNTIANTI",
                  "text": ":D",
                  "score": 2,
                  "created_utc": "2026-02-17 04:25:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5zn1od",
              "author": "No-Television-7862",
              "text": "\"The last 20% takes longer than the first 80%.\"",
              "score": 2,
              "created_utc": "2026-02-18 03:24:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pfyvq",
          "author": "Medium_Chemist_4032",
          "text": "Oh for sure. Plus recompiliing llama.cpp and ik\\_llama. \n\nRecently it has been more: \"why tool calling breaks after few times\" often, so I guess... progress?",
          "score": 9,
          "created_utc": "2026-02-16 16:27:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pl0km",
          "author": "Look_0ver_There",
          "text": "Having a 128GB shared memory machine, and with the recent release of all these models in the 190-240B parameter range has made searching for that elusive \"quantization that isn't brain-dead and fits\" take a massive chunk of my time.  Every time I think my task is done, another model drops and the search continues...",
          "score": 6,
          "created_utc": "2026-02-16 16:50:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pocs4",
              "author": "Hector_Rvkp",
              "text": "I was assuming / hoping that 128gb is actually the sweet spot to get MoE models that sound intelligent enough to be useful and fast enough to be usable. Is that not the case? What are you trying to achieve? I'm considering buying a Strix halo w 128gb ram.",
              "score": 2,
              "created_utc": "2026-02-16 17:05:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5prb3w",
                  "author": "Look_0ver_There",
                  "text": "It is. Qwen3-Coder-Next, which is 80B, fits fine at full Q8, and you can run it at full context at ~35tg/sec.  Speeds go up if you use a smaller quantization but there's no need to sacrifice quality.\n\nI've been messing about with Step-3.5-Flash and MiniMax-M2.5 though, which are 196B and 229B respectively.  Both fit okay with an IQ3 based quantization, but Step seems to suffer a fair bit at that level, while MiniMax however seems to be just fine, or at least, it's hasn't really put a foot wrong in the coding tasks I've asked of it.\n\nBasically I'm refining an IQ3 quant of MiniMax, as well as tuning the VM parameters of the kernel, to get it running as smooth as possible.  It seems to be ticking along at 30tg/sec as of this morning, and it generally emits better quality output that works the first or second time, whereas Qwen Coder Next makes a lot of small mistakes that I really need to keep an eye on.\n\nI also found out today how to fix a chat template bug that was in the original release of MiniMax, and I now have it working really well (again, as if today).",
                  "score": 3,
                  "created_utc": "2026-02-16 17:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pgatg",
          "author": "iz_raymond",
          "text": "Haha that hits home. Honestly, I just want my AI to not sound corporate ðŸ˜” all of them sounds like that, I just want an AI that's unhinged like Grok or at least \"human-like\" Gemini",
          "score": 8,
          "created_utc": "2026-02-16 16:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pmcm8",
              "author": "GetMeThePresident",
              "text": "any luck or tips at getting closer to that?",
              "score": 3,
              "created_utc": "2026-02-16 16:56:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5w50yw",
                  "author": "iz_raymond",
                  "text": "Nope, tried Qwen,Mistral,Deepseek,Llama..all still sound very corporated. But I stick to Qwen for a bit more balance than the rest",
                  "score": 1,
                  "created_utc": "2026-02-17 16:50:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5sexpp",
              "author": "sgamer",
              "text": "go on the character card sites and look for assistant characters, you can add a lot of flavor with just frontloading a json onto chat mode",
              "score": 2,
              "created_utc": "2026-02-17 01:24:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t86yy",
                  "author": "ANTIVNTIANTI",
                  "text": "Oh yeah I 2nd this! SO much can be done just in the sys prompt! Also prompt/write intentionallyâ€”like, really recognize the mirroring effect or how you lead the models by way of your own words, also I'm not sure if I'm replying to the right person to make it so that I appear replying to both of you LOLOL",
                  "score": 2,
                  "created_utc": "2026-02-17 04:27:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5phps1",
          "author": "ptear",
          "text": "Yes, I'm still searching for the most valuable local uses that actually are worth it,Â other than write python script to solve this problem (which I don't end up doing with a local model because I have midrange hardware).",
          "score": 3,
          "created_utc": "2026-02-16 16:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pghpc",
          "author": "sn2006gy",
          "text": "Yeah...\n\nReality is, the real work is being done with layers and models of models/retrievers/planers/routers like:\n\nUser Input  \nÂ  â†“  \nRetriever (patterns, code, history, embeddings)  \nÂ  â†“  \nPlanner / Router  \nÂ  â†“  \nLLM (reasoning)  \nÂ  â†“  \nTool Calls (search, code execution, APIs)  \nÂ  â†“  \nEvaluator / Critic  \nÂ  â†“  \nFinal Output\n\n\n\nThat's why claude code is kicking the butt of whatever isolated model we kick the tires on... but also why claude is so fragile vs competition because it's not rocket science to build this onion layer pattern with tooling.",
          "score": 5,
          "created_utc": "2026-02-16 16:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5po44m",
              "author": "SubstantialPoet8468",
              "text": "Is this akin to the â€œarrâ€ stack for media, compared to say something like Netflix?",
              "score": 1,
              "created_utc": "2026-02-16 17:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5qqiyv",
              "author": "StaysAwakeAllWeek",
              "text": ">it's not rocket science to build this onion layer pattern with tooling.\n\nIt definitely is rocket science to do it profitably though. You need highly optimised models designed specifically for being stacked like this if you ever want to break even",
              "score": 1,
              "created_utc": "2026-02-16 20:04:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5r54ml",
                  "author": "sn2006gy",
                  "text": "It's why claude is going all in on development... but it's not hard to pull OSS models together to build something similar - my point is less about finding the perfect model and tinkering to infinity but about finding models that fit the retriver/reasoning/tool call/evaluator paradigm and if that's coding - fits in the current tooling that has that concept as part of its workflow.",
                  "score": 1,
                  "created_utc": "2026-02-16 21:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ryg0f",
              "author": "m94301",
              "text": "Is this the secret behind Claude?  It's so good, it's really the only one I pay for, but I've never tried to understand what's going on under the hood",
              "score": 1,
              "created_utc": "2026-02-16 23:48:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sr51e",
                  "author": "sn2006gy",
                  "text": "pretty much",
                  "score": 1,
                  "created_utc": "2026-02-17 02:37:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rs6wj",
          "author": "ElijahKay",
          "text": "Skyrim Modders.\n\n  \nFirst time meme.",
          "score": 2,
          "created_utc": "2026-02-16 23:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5plnbg",
          "author": "HomsarWasRight",
          "text": "Welcome to every â€œdo it yourselfâ€ technical hobby.",
          "score": 3,
          "created_utc": "2026-02-16 16:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pn66j",
          "author": "throwaway5006001",
          "text": "Yes downloaded tweaked many models but learned a lot using ollama, lm studio, koboldcpp, openclaw",
          "score": 1,
          "created_utc": "2026-02-16 17:00:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pnaf3",
          "author": "live4evrr",
          "text": "Spent several hours yesterday trying to play with llama.cpp settings to improve the minimax 2.5 performance. Itâ€™s not much difference from the dopamine reward as gaming.",
          "score": 1,
          "created_utc": "2026-02-16 17:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5prk3a",
          "author": "SKirby00",
          "text": "I've been working on building a utility that automatically does some of this stuff for me.\n\nFor context, I have multiple GPUs with varying amounts of VRAM, and I run models with SGLang to get the most performance. The idea is that I'll run this utility with a specified model name, and it:\n- Fetches the model's info from huggingface (in particular, the number of layers that I'll need to distribute across my GPUs)\n- Checks my current hardware to see how many GPUs are installed and how much VRAM is in each one\n- Runs an optimization script that:\n    - Gradually increases the memory allocation and context length until I start running into OOM errors\n    - Runs a hill-climbing algorithm on the distribution of model layers between GPUs to make sure I'm getting the most out of each one\n    - Once it's identified the maximum stable configuration, it pulls it back a bit to add some safety margin\n- Saves the discovered optimal configuration for my exact combination of hardware and model selection to a JSON config file that I can use with another script to run the server\n\nI'll still have to mess around with different prompts and prompt templates etc., but once this is working reliably it should significantly cut down on the time and energy that it takes to figure out exactly how hard I can push my system with a given model and how much context I can fit.",
          "score": 1,
          "created_utc": "2026-02-16 17:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ql006",
          "author": "AdOne8437",
          "text": "No. I mostly try a bit with a few testcases I have and then use for weeks and months the same models.",
          "score": 1,
          "created_utc": "2026-02-16 19:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qsud9",
          "author": "esmurf",
          "text": "Only in the beginning.Â ",
          "score": 1,
          "created_utc": "2026-02-16 20:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ruaub",
          "author": "steezy13312",
          "text": "I'm not alone!\n\nI think part of the challenge is being in the performance limbo of \"it's just passable enough to run what I need\" but at the end of the day not good enough to actually use for real work compared to cloud providers.",
          "score": 1,
          "created_utc": "2026-02-16 23:25:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t7pfw",
          "author": "ANTIVNTIANTI",
          "text": "lololololol it's why we do what we do tho",
          "score": 1,
          "created_utc": "2026-02-17 04:24:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t7rdp",
              "author": "ANTIVNTIANTI",
              "text": "we're inherent tweakers ",
              "score": 1,
              "created_utc": "2026-02-17 04:24:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tvvt5",
          "author": "Gargle-Loaf-Spunk",
          "text": "Wait you mean Iâ€™m supposed to use this crap?Â ",
          "score": 1,
          "created_utc": "2026-02-17 07:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ulp7q",
          "author": "Least-Platform-7648",
          "text": "Same here. Now I use agents to help me, e.g. Codex will try different llama.cpp settings and run benchmarks in a loop.",
          "score": 1,
          "created_utc": "2026-02-17 11:38:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wqbge",
          "author": "Ok-Measurement-1575",
          "text": "Yep because when the shit hits the fan, you gotta know you can rely on it.Â ",
          "score": 1,
          "created_utc": "2026-02-17 18:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zmqhg",
          "author": "No-Television-7862",
          "text": "Trying to get that custom open-weight llm just right takes time.",
          "score": 1,
          "created_utc": "2026-02-18 03:22:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rhhmu",
          "author": "tiga_94",
          "text": "Its not like local llms are really useful, so far I was unable to make any one of them to work as an agent, all fail to use simplest apis \n\nAnd then in chat mode they hallucinate too much\n\nModels in question are below 16 gigs",
          "score": 1,
          "created_utc": "2026-02-16 22:17:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rx38f",
              "author": "remainderrejoinder",
              "text": "You just didn't realize you should be asking it to hallucinate.",
              "score": 1,
              "created_utc": "2026-02-16 23:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sfzs4",
                  "author": "tiga_94",
                  "text": "I ask to open a file and output its content, se as using same agent with a cloud api \n\nCloud- does it, local - hallucinates \n\nGpt oss 20b q4 in question",
                  "score": 1,
                  "created_utc": "2026-02-17 01:30:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5s5csk",
              "author": "segmond",
              "text": "your lack of skills is your own problem.",
              "score": 1,
              "created_utc": "2026-02-17 00:28:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qme6p",
          "author": "Decaf_GT",
          "text": "Unpopular opinion: it's because most people don't actually know what they want a local LLM for, and those who do know that the local LLMs that they're actually *able* to run still don't compare to frontier cloud models. \n\nAs we gravitate further and further to MoE and go from \"no local no care\" to \"well, as long as its hosted in the US\" so that you can run GLM-5, we're going to watch local LLM usage collapse because the models that are worth running require hardware that is incredibly expensive, hardware that we *can't even get* anymore due to the RAM apocalypse.",
          "score": -1,
          "created_utc": "2026-02-16 19:44:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r26mw9",
      "title": "Getting ready to send this monster to the colocation for production.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r26mw9",
      "author": "Ok_Stranger_8626",
      "created_utc": "2026-02-11 19:14:58",
      "score": 66,
      "num_comments": 16,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r26mw9/getting_ready_to_send_this_monster_to_the/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4us5jd",
          "author": "MinimalDistance",
          "text": "Awesome! Thanks for sharing details of the stack. I wonder about estimate costs of building and running a box like this - can you provide some numbers? Thanks!",
          "score": 8,
          "created_utc": "2026-02-11 19:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v179v",
              "author": "Ok_Stranger_8626",
              "text": "All told, it's about $25K worth of gear at today's prices(largely due to the RAM and disk). We assembled it from some spare parts we had, and the rest was purchased from The Server Store. All in all, we figure $12K-ish when we first assembled it.",
              "score": 6,
              "created_utc": "2026-02-11 20:24:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uz652",
          "author": "Anarchaotic",
          "text": "You've built this for a client I assume? Time to rake in that sweet sweet \"ongoing support\" $ to make sure everything keeps working properly and you can update containers/libraries as needed. \n\nHow easily could you redeploy something like this if you had to? From a business perspective it could make sense to have the projects/scripts all relatively automated so you can turn-key another one of these.",
          "score": 4,
          "created_utc": "2026-02-11 20:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v2rmm",
              "author": "Ok_Stranger_8626",
              "text": "We built it initially to do a proof-of-concept for the \"Experts\". It was purely a technical exercise to begin with. \n\nBut after the last few weeks, we decided to make it a multi-tenant system(we have it tied into our SSO system and realms so clients can log into it and have their employees grouped so data doesn't cross-domains.\n\nBut we have all the setup scripts, python code and injectable prompts safely stored away in our GitLab host so we could easily replicate the setup at any time.\n\nThe big ticket is the all the Federal Statutes we ingested and the related instructions(CFRs) from the federal registry. It took us a couple weeks to get them all in properly, and we update them every weekend now, so it's a pretty valuable asset that we can sell/subscribe to law firms, CPAs or compliance consultants.\n\nWe're also working with a Medical compliance officer to develop a Medical Compliance Expert(HIPAA/GDPR/State Level) at the moment, we hope to have that one done in a month or so.",
              "score": 6,
              "created_utc": "2026-02-11 20:31:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v20oq",
          "author": "Hydroskeletal",
          "text": "for something like Federal Law expert are you basically doing a lot of prompt engineering by sticking the US Code into the vector DB? super curious if you can talk about the high level concepts you are using",
          "score": 4,
          "created_utc": "2026-02-11 20:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v3upn",
              "author": "Ok_Stranger_8626",
              "text": "So, we use some pretty tight prompts, yeah. We basically force the model to ignore it's own \"helpful assistant\" role, and behave more like a professional expert. That helped us get around the hallucination problem by a lot. The other thing we do is only allow each expert to research in their specific \"Title\" of the federal code, which really helps, as it limits their expertise to just that ONE particular statue/instruction \"manual\". For example, the \"Tax Expert\" only has access to Title 26 and the IRS CFR.",
              "score": 7,
              "created_utc": "2026-02-11 20:37:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4x7uiw",
                  "author": "Hefty_Development813",
                  "text": "How do you train one specific expert with an MOE?",
                  "score": 1,
                  "created_utc": "2026-02-12 03:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v4so2",
          "author": "dave-tay",
          "text": "Beautiful. Price?",
          "score": 3,
          "created_utc": "2026-02-11 20:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v57iy",
              "author": "Ok_Stranger_8626",
              "text": "We estimate now that prices have gone up, somewhere between $20K and $25K US. Back when we did it last summer, probably closer to $12k to $16K US.\n\nEDIT: That would be our cost for just the hardware. It would definitely not get sold for that, considering the customized containers, python code, specialty prompts and the RAG database. We've put hundreds of hours into building the software stack, so I'd say, if we were to retail it, we'd probably price the box around $65K US for a functional \"turn-key\" device.",
              "score": 5,
              "created_utc": "2026-02-11 20:43:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4we6s7",
                  "author": "Much-Researcher6135",
                  "text": "Thanks, I was wondering what the market was like for all this stuff. Are customers needy or pretty hands-off? Or do you guys not even do service contracts?\n\nNeat post!",
                  "score": 3,
                  "created_utc": "2026-02-12 00:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xsxy9",
          "author": "ridablellama",
          "text": "very cool post. i love to see real business applications of local",
          "score": 2,
          "created_utc": "2026-02-12 06:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wqm22",
          "author": "chrisbliss13",
          "text": "How much you paying for colo",
          "score": 1,
          "created_utc": "2026-02-12 01:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wtfn0",
              "author": "Ok_Stranger_8626",
              "text": "Right now, it's about $325/mo for the 6U and low power for my other gear. \n\nThis one, when it goes down there will add about $550/mo because it needs way more energy.",
              "score": 2,
              "created_utc": "2026-02-12 02:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zeqpj",
          "author": "Lonely_Love4287",
          "text": "so cool which i had the funds to do  this",
          "score": 1,
          "created_utc": "2026-02-12 14:11:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4vp1d",
      "title": "Best program and model to make this an actual 3d model?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/41y95nmiwijg1.png",
      "author": "Kolpus",
      "created_utc": "2026-02-14 20:56:37",
      "score": 54,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4vp1d/best_program_and_model_to_make_this_an_actual_3d/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5euie8",
          "author": "chevellebro1",
          "text": "Hunyuan3D is the best local model Iâ€™ve found. I run using the template workflow in ComfyUI. If you want best quality, Iâ€™d recommend looking at Meshy.ai. The quality is impressive from their newer models",
          "score": 10,
          "created_utc": "2026-02-14 22:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gyxri",
              "author": "Dadda9088",
              "text": "Yeah, use an img2img model to get different views and hunyuan3d to make it in 3D is the way to go locally. The limitation could be missing textures and output format.",
              "score": 2,
              "created_utc": "2026-02-15 07:06:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5eitp3",
          "author": "sumane12",
          "text": "You tried the latest trellis?",
          "score": 7,
          "created_utc": "2026-02-14 21:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eljwl",
              "author": "o5mfiHTNsH748KVq",
              "text": "This is the answer\n\nhttps://microsoft.github.io/TRELLIS.2/",
              "score": 11,
              "created_utc": "2026-02-14 21:22:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fshzh",
                  "author": "o5mfiHTNsH748KVq",
                  "text": "for funsies, i put your source image into trellis\n\nhttps://preview.redd.it/hraavaeybkjg1.png?width=1684&format=png&auto=webp&s=27403c19c2b8a52ed6259611a6eff59d591fd6b7\n\nsettings can probably be tweaked",
                  "score": 6,
                  "created_utc": "2026-02-15 01:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ew2ko",
              "author": "Kolpus",
              "text": "Thank you I just tried it with default settings and result is way better than other models I tried. Will try again with maxed settings.\n\nFor other that are interested I used this easy installer: [https://github.com/IgorAherne/trellis-stable-projectorz/releases/tag/latest](https://github.com/IgorAherne/trellis-stable-projectorz/releases/tag/latest)\n\n",
              "score": 11,
              "created_utc": "2026-02-14 22:20:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ewasu",
                  "author": "runlinux",
                  "text": "Oooo? Iâ€™m interested in this too! Thanks for finding this.",
                  "score": 1,
                  "created_utc": "2026-02-14 22:21:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ehsc4",
          "author": "Dramatic_Entry_3830",
          "text": "I want to know too",
          "score": 3,
          "created_utc": "2026-02-14 21:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gryya",
          "author": "an80sPWNstar",
          "text": "Trellis 2 is the way. Be sure to run it in a repair app first to make sure all is well. There's a free one that's popular with the 3d modeling world",
          "score": 2,
          "created_utc": "2026-02-15 06:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f4rfq",
          "author": "KiwiNFLFan",
          "text": "Build it from scratch in Blender?",
          "score": 3,
          "created_utc": "2026-02-14 23:10:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f61mm",
              "author": "frogsarenottoads",
              "text": "https://preview.redd.it/dlee5c0rmjjg1.jpeg?width=236&format=pjpg&auto=webp&s=dcde1aa0f34cab3a20d8e506fb6937bae0ebc3b4\n\nFeel free to fill this in yourself I'm far too lazy",
              "score": 17,
              "created_utc": "2026-02-14 23:18:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5i9ug9",
              "author": "MixeroPL",
              "text": "Learn a new skill? Nah we got ai slop",
              "score": -3,
              "created_utc": "2026-02-15 13:55:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pgfu7",
          "author": "Kolpus",
          "text": "Just learned you can use the prompt \"show from an isometric angle\" to get an image well suited to turn into a 3d model. \n\nWould be even better if it could generate 2 isometric pictures from opposite angles but am not sure this is actually possible. Trellis can be fed multiple images from different angles.",
          "score": 1,
          "created_utc": "2026-02-16 16:29:23",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5sy4nw",
          "author": "Fuzzy-Rooster8548",
          "text": "this https://3d.hunyuan.tencent.com/. mesh and colors will be better than the rest. then you can reduce polycount with instantmeshes, or blender's decimation modifier. but the plants quality wont be too good. i would use ai to remove the plants, generate the model, and then add some actual modelled plants in the scene. but if it is for a top down game, model viewed at distance, just yolo it.",
          "score": 1,
          "created_utc": "2026-02-17 03:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6061nq",
          "author": "sectorix",
          "text": "if you are ok paying for api calls, i would do this:   \n1. ask nano-banana-pro to generate more angles of this model,   \n2. ask rodin to make it into a 3d model.  \nboth available pay-as-you-go on replicate",
          "score": 1,
          "created_utc": "2026-02-18 05:31:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3u81s",
      "title": "GLM5",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/1p89zxhfjajg1.png",
      "author": "I_like_fragrances",
      "created_utc": "2026-02-13 16:45:37",
      "score": 53,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r3u81s/glm5/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o58vv89",
          "author": "FuzzeWuzze",
          "text": "Lol i want to meet the chad downloading a 1.51 Terabyte file from HF.\n\nOnly to get to the end get a checksum error...",
          "score": 19,
          "created_utc": "2026-02-13 22:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nl7u6",
              "author": "diffore",
              "text": "Nvidia could have made a \\~50  5090 for us to play.... but instead theme gigabytes of vram are now sitting in some server closet, spinning the BF16 version of GLM5. Yeah, still have hard feelings about the consumer market suffering from the AI boom. ",
              "score": 2,
              "created_utc": "2026-02-16 09:11:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q8gul",
                  "author": "I_like_fragrances",
                  "text": "It has gotten bad, especially when just a GPU for gaming is becoming inaccessible to people outside the AI community.",
                  "score": 2,
                  "created_utc": "2026-02-16 18:38:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d0zma",
              "author": "Aggressive-Fan-3473",
              "text": "I believe HF uses tls for model downloads so that wouldnâ€™t happen. The tls would not be able to decrypt and would request the corrupted packet again",
              "score": 1,
              "created_utc": "2026-02-14 16:30:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dak87",
                  "author": "FuzzeWuzze",
                  "text": "Maybe it's a lm studio issue then because it most definitely happens",
                  "score": 1,
                  "created_utc": "2026-02-14 17:18:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o591yqc",
          "author": "LA_rent_Aficionado",
          "text": "I assume this is unslothâ€™s repo?\n\nâ€œXLâ€ on unsloth is usually how they categorize their UD dynamic quants, not because they are XL but because there is no separate/better way to fit them into the standard categories.",
          "score": 8,
          "created_utc": "2026-02-13 23:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58k43j",
          "author": "Pixer---",
          "text": "I found that the M model performs better at longer context. The XL variant falls of quicker for programming. Itâ€™s quite noticeable in longer conversations",
          "score": 5,
          "created_utc": "2026-02-13 21:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58tb32",
          "author": "qwen_next_gguf_when",
          "text": "Can't run the smallest\n\n![gif](giphy|uqmtVo5zcIBXJq1rGX)",
          "score": 3,
          "created_utc": "2026-02-13 22:26:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b9qgl",
          "author": "beefgroin",
          "text": "I need Q0.1",
          "score": 2,
          "created_utc": "2026-02-14 08:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o577p96",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 3,
          "created_utc": "2026-02-13 17:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58kuc1",
              "author": "inevitabledeath3",
              "text": "Quantisation doesn't change things like model structure, number of layers, embedding size, or parameters. Llama is lying to you.",
              "score": 5,
              "created_utc": "2026-02-13 21:44:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b92g6",
          "author": "emrbyrktr",
          "text": "Is it better than Qwen 3 Coder Next?",
          "score": 1,
          "created_utc": "2026-02-14 08:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zyfyc",
              "author": "I_like_fragrances",
              "text": "I like it to be honest. But my overall favorite is still Deepseek v3.1 terminus q4_k_xl",
              "score": 1,
              "created_utc": "2026-02-18 04:36:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2vcgi",
      "title": "Mac M4 vs. Nvidia DGX vs. AMD Halo Strix",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r2vcgi/mac_m4_vs_nvidia_dgx_vs_amd_halo_strix/",
      "author": "alfons_fhl",
      "created_utc": "2026-02-12 14:48:28",
      "score": 47,
      "num_comments": 109,
      "upvote_ratio": 0.91,
      "text": "Has anyone experiences or knowledge about:\n\n**Mac M4** vs. **Nvidia DGX** vs. **Amd Halo Strix**\n\n\n\n\\-each with **128gb**\n\n\\-to **run LLM's**\n\n\\-**not** for tune/train\n\n\n\nI cant find any good reviews on youtube, reddit...\n\n\n\nI heard that Mac is much faster (t/s), but not for train/tune (so fine for me)\n\nIs it true?\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r2vcgi/mac_m4_vs_nvidia_dgx_vs_amd_halo_strix/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4zpv10",
          "author": "Miserable-Dare5090",
          "text": "Look up alex ziskind on youtube. He has tested all 3 head to head recentlyâ€”which is important bc the benchmarks from october are stale now.\n\nBy the way, you donâ€™t want to run LLMs. You want to use coding agents. And that is a different factor to add. \n\nThe prompts on coding agents are very large, itâ€™s not just what you want to code but also instructions. I suggest you look into it with that in mind, and with agentic use, concurrencies matter too (Number of requests in parallel).",
          "score": 20,
          "created_utc": "2026-02-12 15:09:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ztvzf",
              "author": "alfons_fhl",
              "text": "Okay, I'm planning to use up to 256k context tokens. So its very large.\n\nYes I heard about the agents.\n\nSo with a good (software/tool) like ClaudeCode/OpenClaude...\n\n  \nWich software/tool and wich device you would recommend?",
              "score": 3,
              "created_utc": "2026-02-12 15:29:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o505cxc",
                  "author": "Longjumping-Boot1886",
                  "text": "in case of Macs, everyone waiting M5 Pro/Max/Ultra for that, because their main change - improved prompt processing time, and thats the main thing when you are putting in 100k context message.",
                  "score": 5,
                  "created_utc": "2026-02-12 16:23:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50aqf4",
                  "author": "jinnyjuice",
                  "text": "The context token size is a bit more model dependent. For example, official max digits may be in the 6 digits for GPT OSS 120B, but its performance degrades after about 30k or so.",
                  "score": 3,
                  "created_utc": "2026-02-12 16:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o59u1qr",
              "author": "DopePedaller",
              "text": "I think Alex's video about concurrency (on DGX and other platforms) is a must watch for anyone interested in local llms. I was a bit underwhelmed by the benchmarks until seeing what can be achieved with the right HW + concurrent use on vLLM. [LINK](https://youtu.be/Ze5XLooTt6g)",
              "score": 2,
              "created_utc": "2026-02-14 02:07:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54s0tx",
              "author": "alfons_fhl",
              "text": "\"alex ziskind\", tested an M4 Pro Mini with 128gb... But the M4 Pro Mini with 128gb isn't available...\n\n128gb only works with Mac Studio, like the Mac Studio M4 Max...\n\nOr does I understand something wrong?.. :/ ",
              "score": 1,
              "created_utc": "2026-02-13 08:36:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50a5dh",
          "author": "Grouchy-Bed-7942",
          "text": "Iâ€™ve got a Strix Halo and 2x GB10 (Nvidia DGX Spark, but the Asus version).\n\nFor pure AI workloads, Iâ€™d go with the GB10. For example, on GPT-OSS-120B Iâ€™m hitting \\~6000 pp and 50â€“60 t/s with vLLM, and I can easily serve 3 or 4 parallel requests while still outperforming my Strix Halo, which struggles to reach \\~700 pp and \\~50 t/s with only a single concurrent request!\n\nExample with vLLM on the GB10 ([https://github.com/christopherowen/spark-vllm-mxfp4-docker](https://github.com/christopherowen/spark-vllm-mxfp4-docker)):\n\n|model|test|t/s|peak t/s|peak t/s (req)|ttfr (ms)|est\\_ppt (ms)|e2e\\_ttft (ms)|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|gpt-oss-120b|pp512|2186.05 Â± 17.36|||235.38 Â± 1.85|234.23 Â± 1.85|284.43 Â± 2.11|\n|gpt-oss-120b|tg32|63.39 Â± 0.07|65.66 Â± 0.08|65.66 Â± 0.08||||\n|gpt-oss-120b|pp512|2222.35 Â± 10.78|||231.55 Â± 1.12|230.39 Â± 1.12|280.76 Â± 1.14|\n|gpt-oss-120b|tg128|63.44 Â± 0.07|64.00 Â± 0.00|64.00 Â± 0.00||||\n|gpt-oss-120b|pp2048|4888.74 Â± 36.61|||420.10 Â± 3.13|418.95 Â± 3.13|469.42 Â± 2.85|\n|gpt-oss-120b|tg32|62.38 Â± 0.08|64.62 Â± 0.08|64.62 Â± 0.08||||\n|gpt-oss-120b|pp2048|4844.62 Â± 21.71|||423.90 Â± 1.90|422.75 Â± 1.90|473.38 Â± 2.10|\n|gpt-oss-120b|tg128|62.65 Â± 0.08|63.00 Â± 0.00|63.00 Â± 0.00||||\n|gpt-oss-120b|pp8192|6658.41 Â± 30.91|||1231.51 Â± 5.73|1230.35 Â± 5.73|1283.13 Â± 5.97|\n|gpt-oss-120b|tg32|60.39 Â± 0.14|62.56 Â± 0.14|62.56 Â± 0.14||||\n|gpt-oss-120b|pp8192|6660.84 Â± 38.83|||1231.08 Â± 7.13|1229.92 Â± 7.13|1281.95 Â± 6.97|\n|gpt-oss-120b|tg128|60.76 Â± 0.03|61.00 Â± 0.00|61.00 Â± 0.00||||\n|gpt-oss-120b|pp16384|5920.87 Â± 13.29|||2768.33 Â± 6.23|2767.18 Â± 6.23|2821.06 Â± 6.16|\n|gpt-oss-120b|tg32|58.12 Â± 0.13|60.21 Â± 0.13|60.21 Â± 0.13||||\n|gpt-oss-120b|pp16384|5918.04 Â± 8.14|||2769.65 Â± 3.81|2768.49 Â± 3.81|2823.16 Â± 3.66|\n|gpt-oss-120b|tg128|58.14 Â± 0.08|59.00 Â± 0.00|59.00 Â± 0.00||||\n|gpt-oss-120b|pp32768|4860.07 Â± 8.18|||6743.46 Â± 11.34|6742.30 Â± 11.34|6800.08 Â± 11.34|\n|gpt-oss-120b|tg32|54.05 Â± 0.14|55.98 Â± 0.14|55.98 Â± 0.14||||\n|gpt-oss-120b|pp32768|4858.40 Â± 5.92|||6745.77 Â± 8.22|6744.62 Â± 8.22|6802.72 Â± 8.15|\n|gpt-oss-120b|tg128|54.18 Â± 0.09|55.00 Â± 0.00|55.00 Â± 0.00||||\n\nllama-benchy (0.3.0) date: 2026-02-12 13:56:46 | latency mode: api\n\nNow the Strix Halo with llama.cpp (the GB10 with llama.cpp is also faster, around 1500â€“1800 pp regardless of context):\n\nggml\\_cuda\\_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n\n|model|size|params|backend|ngl|type\\_k|type\\_v|fa|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp512|649.94 Â± 4.23|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp2048|647.72 Â± 1.88|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp8192|563.56 Â± 8.42|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp16384|490.22 Â± 0.97|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp32768|388.82 Â± 0.02|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|tg32|51.45 Â± 0.05|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|tg128|51.49 Â± 0.01|\n\nbuild: 4d3daf80f (8006)\n\nThe noise difference is also very noticeable: the GB10 at full load is just a light whoosh, whereas the Strix Halo (MS S1 Max in my case) spins up quite a bit.\n\nSo if youâ€™ve got â‚¬3k, get a GB10. If you donâ€™t want to spend that much, a Bossgame at â‚¬1500â€“1700 will also do the job, just with lower performance. But if youâ€™re looking to run parallel requests (agents or multiple users), the GB10 will be far more capable. Same thing if you want to run larger models: you can link two GB10s together to get 256 GB of memory, which can let you run MiniMax M2.1 at roughly Q4 equivalent without issues using vLLM.\n\nI donâ€™t have a Mac, but in my opinion itâ€™s not worth it, except for an M3 Ultra with 256 GB / 512 GB of RAM.",
          "score": 13,
          "created_utc": "2026-02-12 16:45:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51592c",
              "author": "fallingdowndizzyvr",
              "text": "> outperforming my Strix Halo, which struggles to reach ~700 pp and ~50 t/s with only a single concurrent request!\n\nIf you are only getting 700PP on your Strix Halo. Then you are doing something wrong. I get ~1000PP.\n\n    ggml_cuda_init: found 1 ROCm devices:\n      Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | n_batch | n_ubatch | fa | mmap |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -------: | -: | ---: | --------------: | -------------------: |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | ROCm,Vulkan |  99 |    4096 |     4096 |  1 |    0 |          pp4096 |       1012.63 Â± 0.63 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | ROCm,Vulkan |  99 |    4096 |     4096 |  1 |    0 |           tg128 |         52.31 Â± 0.05 |",
              "score": 3,
              "created_utc": "2026-02-12 19:10:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o535puz",
                  "author": "Grouchy-Bed-7942",
                  "text": "Do you have the details (OS, ROCM version, llamacpp arguments? I'm interested. I was able to reach 1000 PP with Rocm 6.4.4, here the benchmark is with 7.2).",
                  "score": 1,
                  "created_utc": "2026-02-13 01:26:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o54cdq4",
                  "author": "alfons_fhl",
                  "text": "If you have the choice again, will you buy the Strix halo again?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-13 06:16:05",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5kzhb7",
                  "author": "Miserable-Dare5090",
                  "text": "I see your posts often. I know you are a big fan and I also got a bosgame m5. But the honest response is that the GB10 chip is superior, hands down. I have both and i wish that I could say AMD made a better price Spark, butâ€¦did you notice the difference at 4096 tokens? 1000 vs 4888. \n\nNow, for the actual question being asked, of agentic coding, which needs long contexts. What is your strix halo doing at 32000 tokens? Is it still processing 1000 tokens per second?",
                  "score": 1,
                  "created_utc": "2026-02-15 22:10:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50p8d8",
              "author": "NeverEnPassant",
              "text": "There must be some mistake with your PP numbers. They are close to what you would see with a RTX 6000 Pro.",
              "score": 1,
              "created_utc": "2026-02-12 17:56:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51p9en",
                  "author": "Grouchy-Bed-7942",
                  "text": "That repo ships a pre-tuned Docker image for DGX Spark/GB10 that bundles the right vLLM build + MXFP4/CUTLASS kernels (and attention stack like FlashInfer) so you donâ€™t fall back to slower generic paths.\nBecause the whole software stack is locked (libs, flags, backends, caching), you get near-best-case throughput out of the box, which can make Spark numbers look surprisingly close to an RTX 6000 Pro on the same low-precision inference workload.",
                  "score": 2,
                  "created_utc": "2026-02-12 20:46:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50chqb",
              "author": "alfons_fhl",
              "text": "Thanks!\n\nIts really helpful.\n\nSo the Asus is around 15% better... But paying the 1,8x for it... idk...\n\nBut you told the pp... I guess for big context like 200k it is a big problem for \"halo Strix\" and only work on Nvidia, right?\n\n",
              "score": 0,
              "created_utc": "2026-02-12 16:56:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50fs2u",
                  "author": "Grouchy-Bed-7942",
                  "text": "Nope, you didnâ€™t read everything.\n\nWith vLLM, the Asus is about 5Ã— faster at prompt processing (pp). vLLM on Strix Halo is basically a non-starter, performance is awful. Itâ€™s also roughly 15% faster on token generation/writing (tg).\n\nTo make it concrete: if youâ€™re coding with it using opencode, opencode injects a 10,000-token preprompt up front (tooling, capabilities, etc.). Add ~5,000 input tokens for a detailed request plus one or two context files, and youâ€™re quickly at ~15,000 input tokens.\n\nOn that workload, the Asus GB10 needs under ~3 seconds to process the ~15k-token input and then starts generating at roughly 55â€“60 tok/s. The Strix Halo, meanwhile, takes just under ~30 seconds before it even begins generating, at around ~50 tok/s. You see the difference?\n\nIn other words, the GB10 can read 15,000 tokens and generate a bit more than 1500 tokens of output before the Strix Halo has started writing anything.\n\nAnd thatâ€™s where the GB10 really shines with VLLM : if, for example, someone is chatting with GPT-OSS-120B while youâ€™re coding, performance doesnâ€™t get cut in half. It typically drops by only a few percent.",
                  "score": 4,
                  "created_utc": "2026-02-12 17:11:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zpr9y",
          "author": "Creepy-Bell-4527",
          "text": "Wait for the M5 max / ultra. That will eat the others lunch.",
          "score": 6,
          "created_utc": "2026-02-12 15:09:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zquai",
              "author": "alfons_fhl",
              "text": "But I guess this one cost much much more... ",
              "score": 0,
              "created_utc": "2026-02-12 15:14:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zrp4b",
                  "author": "Creepy-Bell-4527",
                  "text": "I would hope it would cost as much as m3 max and ultra but you never know, with memory pricing being what it is.",
                  "score": 2,
                  "created_utc": "2026-02-12 15:18:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o507b8f",
          "author": "flamner",
          "text": "Honest opinion: if you need an agent to assist with coding, itâ€™s not worth spending money on hardware to run local models. They will always lag behind cloud models or tools like Claude, or Codex. Anyone claiming otherwise is fooling themselves. Local models are fine for simpler tasks like data structuring or generating funny videos.",
          "score": 2,
          "created_utc": "2026-02-12 16:32:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51yjf5",
              "author": "AshKetchum1011",
              "text": "I totally agree with this. I have a M4 Max.",
              "score": 1,
              "created_utc": "2026-02-12 21:30:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54b25j",
              "author": "alfons_fhl",
              "text": "not really... plan is to do a 24/7 automatically coding system... So with cloud LLM its to expensive... for 24/7... ",
              "score": 1,
              "created_utc": "2026-02-13 06:05:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o54fah1",
                  "author": "flamner",
                  "text": "Mate, youâ€™re operating on some unrealistic expectations. Companies invest billions of dollars in infrastructure to run systems like this, and you think you can achieve similar results on a local home computer? Donâ€™t focus only on token generation speed, in coding, thatâ€™s not a meaningful metric. What really matters is the quality of the code the model produces. Have you even checked that? Have you looked at what kind of code the model you want to use actually generates? And have you calculated the electricity costs this setup will rack up?",
                  "score": 2,
                  "created_utc": "2026-02-13 06:40:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4znlj4",
          "author": "Look_0ver_There",
          "text": "This page here measures the Strix Halo vs the DGX Spark directly. [https://github.com/lhl/strix-halo-testing/](https://github.com/lhl/strix-halo-testing/)\n\nMind you, that's about 5 months old now and things have improved on the Strix Halo since, and likely on the DGX Spark too.  The DGX is going to always be faster for token processing due to the ARM based architecture, but the DGX only has about 7% faster memory than the Strix Halo, so token generation speeds are always going to be at about that ratio of a difference.\n\nFrom what I've read, the 128GB M4 Mac has the same memory bandwidth as the DGX Spark, so it's also going to generate at about the same speed as the Spark (with both being \\~7% faster than the Strix on average).  I don't know what the processing speeds are like on the Max though.  Both the Max and the DGX costs around twice as much as the Strix Halo solutions though, and if you ever plan to play games on your boxes, then the Strix Halo's are going to be better for that.",
          "score": 2,
          "created_utc": "2026-02-12 14:58:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zoqew",
              "author": "alfons_fhl",
              "text": "The biggest problem is, every video, say/show different results...\n\nThanks for the GitHub \"test\".\n\n\n\nRight now idk, what I should buy... \n\n\n\nSo AMD Halo Strix only 7% slower... is it more worth than an DGX or a Mac  \n\n\nPrices in EURO:\n\nMac 3.400â‚¬\n\nNvidia DGX (or Asus Ascent GX10): 2.750â‚¬\n\nAMD 2.200â‚¬",
              "score": 2,
              "created_utc": "2026-02-12 15:04:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zq1j9",
                  "author": "Look_0ver_There",
                  "text": "The big thing to watch out for is whether or not the tester is using a laptop based Strix Halo, vs a MiniPC one, and even then, which MiniPC exactly.  Pretty much all the laptops, and some of the MiniPC's won't give a Strix Halo the full power budget that it wants, and so one review may show it running badly, while another shows it running well.\n\n2.200â‚¬ for a Strix Halo seems surprisingly high priced.  You should be able to get the 128GB models for around the US$2000 mark, so whatever that converts to in Euro (\\~1700EUR)",
                  "score": 1,
                  "created_utc": "2026-02-12 15:10:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51a26z",
                  "author": "fallingdowndizzyvr",
                  "text": "> AMD 2.200â‚¬\n\nYou are overpaying if you pay that much for a Strix Halo. 1.770â‚¬\n\nhttps://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395",
                  "score": 1,
                  "created_utc": "2026-02-12 19:33:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zp93y",
              "author": "Miserable-Dare5090",
              "text": "This is very old. Iâ€™d say as an owner of both, not representative of current optimizations on each system. \n\nI also have a mac ultra chip studio. the spark is by far much faster prompt processing, which makes sense bc PP is compute bound, Inference is bandwidth bound. But no question that even the mac will choke after 40k tokens, as will the strix halo, but the spark is very good even at that size of context.",
              "score": 1,
              "created_utc": "2026-02-12 15:06:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zt15u",
                  "author": "alfons_fhl",
                  "text": "Okay yea up to 256k context would be perfect. So than you recommend the Spark.\n\nIs it still fast with the spark? ",
                  "score": 1,
                  "created_utc": "2026-02-12 15:25:12",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o508u1t",
                  "author": "alfons_fhl",
                  "text": "Is the context token problem only on m3 (ultra), what about the m4 max?",
                  "score": 1,
                  "created_utc": "2026-02-12 16:39:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50b52m",
              "author": "ScuffedBalata",
              "text": "> 128GB M4 Mac\n\nSpecifying this without specifying whether its Pro/Max/Max+/Ultra is weird.\n\nBecause the memory bandwidth of those are (roughly)...  240/400/550/820 GB/s  \n\nThe Ultra is double the Max and nearly 4x the Pro.",
              "score": 1,
              "created_utc": "2026-02-12 16:49:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50rfjo",
                  "author": "rditorx",
                  "text": "It's unlikely to be an M4 Ultra, and I think the only M4 with 128GB RAM is an M4 Max which would have 546 GB/s, so 2x the DGX Spark (273 GB/s) and more than 2x the Strix Halo (256 GB/s) and also faster than M3 Max (300 GB/s for the lower-core, 400 GB/s for the higher-core variant)",
                  "score": 5,
                  "created_utc": "2026-02-12 18:06:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51klf4",
                  "author": "po_stulate",
                  "text": "We kinda just assume it's M4 Max when we see people refer to 128GB M4 as it's the only M4 variant that has 128GB RAM configurations. Also, M4 Ultra isn't a thing.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:24:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zmft8",
          "author": "eleqtriq",
          "text": "My Mac M4 Pro is substantially slower than my DGX.  The prefill rate is atrocious.   My Max wasnâ€™t really any better.   Iâ€™ve been waiting patiently for the M5 Pro/Max. \n\nIf youâ€™re just chatting, itâ€™s fine. But if you want to upload large documents or code, Mac isnâ€™t the way to go.",
          "score": 1,
          "created_utc": "2026-02-12 14:52:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zncar",
              "author": "alfons_fhl",
              "text": "My plan is to use it for **coding**.\n\nSo for example **qwen3-coder-next-80b.**\n\nAnd why do you bought the DGX? Do you use it for train LLM or to tune? or only to run?\n\nWhy do you not bought an AMD Strix Halo? :) ",
              "score": 1,
              "created_utc": "2026-02-12 14:57:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4znyzi",
                  "author": "spaceman_",
                  "text": "Strix Halo user here. Strix Halo's party trick is big memory. Prefill rates are terrible, decode (token generation) rates are below average. Long, agentic, tool using chains are pretty unusable in a lot of cases.",
                  "score": 3,
                  "created_utc": "2026-02-12 15:00:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5138p7",
                  "author": "ConspiracyPhD",
                  "text": "Is it worth it though?  Have you used qwen3-coder-next-80b for coding anything yet?  If you haven't, you might want to try build.nvidia.com's Qwen3-Coder-480B-A35B-Instruct (which is the larger version of it) with something like opencode or kilocode and see if it's worth investing in local hardware (which I have a feeling might be obsolete in a short time) versus just paying $10 a month for something like a github copilot pro plan for 300 requests a month (and then $0.04 per additional request).  That goes a long way.",
                  "score": 2,
                  "created_utc": "2026-02-12 19:01:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4zsq8o",
                  "author": "eleqtriq",
                  "text": "I fine tune and inference.  I also like to run image generation.  Since itâ€™s nvidia, a lot of stuff just works.  Not everything, but a lot.",
                  "score": 1,
                  "created_utc": "2026-02-12 15:23:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o55u8sx",
                  "author": "Professional_Mix2418",
                  "text": "I went DGX spark as well bouw strix halo is so much more expensive. The difference in price isnâ€™t as much and you get actually cuda cores.",
                  "score": 1,
                  "created_utc": "2026-02-13 13:38:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zwx7d",
          "author": "Soft_Syllabub_3772",
          "text": "Which iz the best for coding? :)",
          "score": 1,
          "created_utc": "2026-02-12 15:43:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50doob",
              "author": "Grouchy-Bed-7942",
              "text": "DGX Spark/GB10",
              "score": 2,
              "created_utc": "2026-02-12 17:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54m2nh",
                  "author": "alfons_fhl",
                  "text": "But why?...\n\nLike for example the LLM qwen3-coder-next-80b is very good for coding.\n\nBut why the DGX Spark, and not Mac or \"Halo Strix\"?",
                  "score": 1,
                  "created_utc": "2026-02-13 07:41:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5577pe",
          "author": "Osi32",
          "text": "My Mac M1 Max doesnâ€™t â€œdoâ€ fp8, fp16 only. Which is a bit of a complexity in some situations. It means you canâ€™t efficiently use all models. Nobody else has said this here. If youâ€™re using LM Studio or something similar you wonâ€™t notice much of an issue, but if youâ€™re in comfyui and youâ€™re selecting and configuring your models it can be a bit annoying when youâ€™re using a template and itâ€™s all fp8.\nOf course, everything I just said applies to pytorch, other libraries may not have the same challenges.",
          "score": 1,
          "created_utc": "2026-02-13 10:58:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55b1cc",
              "author": "alfons_fhl",
              "text": "Yes, nobody has said this before.\n\nAhh okay so it is a software based problem..\n\n",
              "score": 2,
              "created_utc": "2026-02-13 11:30:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o58fllw",
                  "author": "Osi32",
                  "text": "Actually, its hardware. Nvidia does fp8, but not fp16 (floating point 16 bit). Apple does, but most of the hardware out in the wild is nvidia... so yes, the software manifesting the limitation, but its actually hardware driving the problem.",
                  "score": 1,
                  "created_utc": "2026-02-13 21:18:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5879eh",
          "author": "catplusplusok",
          "text": "I have an NVIDIA Thor which is slightly cheaper and faster, I think ASUS has an even cheaper SB10 box. Bottom line all of them will get similar performance if you go NVIDIA unified memory route. Great for quantized sparse MOE models, not enjoyable for big dense ones.",
          "score": 1,
          "created_utc": "2026-02-13 20:37:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h6822",
          "author": "fallingdowndizzyvr",
          "text": "Did you see this?\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/",
          "score": 1,
          "created_utc": "2026-02-15 08:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i6blu",
              "author": "alfons_fhl",
              "text": "90% of people say CUDA is very good and an very good option. So yea... :/ ",
              "score": 1,
              "created_utc": "2026-02-15 13:34:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5k6u0p",
                  "author": "fallingdowndizzyvr",
                  "text": "LOL. So you didn't see that? Since that says this.\n\n\"NVIDIA DGX Spark has **terrible CUDA** & software compatibility\"",
                  "score": 1,
                  "created_utc": "2026-02-15 19:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4yim5",
      "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4yim5/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "author": "Educational_Cry_7951",
      "created_utc": "2026-02-14 22:56:06",
      "score": 39,
      "num_comments": 13,
      "upvote_ratio": 0.99,
      "text": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n>**Please think of giving the Github repo a STAR if you like it :)**\n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n>\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations**\n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4yim5/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5fabnd",
          "author": "PomeloSweet926",
          "text": "Just tried this with Gemma3-27B-it (the NVFP4 version) and shit it fits and runs smoothly this is genuinely useful, nice work OP",
          "score": 3,
          "created_utc": "2026-02-14 23:45:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fao32",
              "author": "Educational_Cry_7951",
              "text": "Thank you, Next step for me is to integrate Qwen-Next and make sure it fits and runs fast on a low VRAM budget",
              "score": 1,
              "created_utc": "2026-02-14 23:47:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f4p9m",
          "author": "DataGOGO",
          "text": "Really interesting, what weight format does it expect?",
          "score": 1,
          "created_utc": "2026-02-14 23:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f62bk",
              "author": "Educational_Cry_7951",
              "text": "Thank you! NVFP4 using [https://github.com/NVIDIA/Model-Optimizer.git](https://github.com/NVIDIA/Model-Optimizer.git) \n\nhere is an example:\n\n    python hf_ptq.py \\\n        --pyt_ckpt_path google/gemma-3-27b-it \\\n        --qformat nvfp4 \\\n        --kv_cache_qformat fp8 \\\n        --export_fmt hf \\\n        --export_path ./gemma-3-27b-it-nvfp4-fp8kv \\\n        --calib_size 512 \\\n        --trust_remote_code",
              "score": 2,
              "created_utc": "2026-02-14 23:18:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5f6ffz",
                  "author": "DataGOGO",
                  "text": "Does this use the compressed tensors? Does it use modeloptÂ format? Fused scales?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:21:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5femea",
          "author": "Suitable-Still8379",
          "text": "neat, how does output quality compare to running the same base model in Q4\\_K\\_M via llama.cpp?",
          "score": 1,
          "created_utc": "2026-02-15 00:12:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6hmxl",
      "title": "The Mac Studio vs NVIDIA Dilemma â€“ Best of Both Worlds?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r6hmxl/the_mac_studio_vs_nvidia_dilemma_best_of_both/",
      "author": "JournalistShort9886",
      "created_utc": "2026-02-16 18:29:39",
      "score": 38,
      "num_comments": 32,
      "upvote_ratio": 0.91,
      "text": "Hey, looking for some advice here.\n\nIâ€™m a person who runs local LLMs and also trains models occasionally. Iâ€™m torn between two paths:\n\nOption 1: Mac Studio â€“ Can spec it up to 192gb(yeah i dont have money for 512gb) unified memory. Would let me run absolutely massive models locally without VRAM constraints. But the performance isnâ€™t optimized for ML model training as to CUDA, and the raw compute is weaker. Like basic models would tale days\n\nOption 2: NVIDIA GPU setup â€“ Way better performance and optimization (CUDA ecosystem is unmatched), but Iâ€™m bottlenecked by VRAM. Even a 5090 only has 32GB,.\n\nIdeally I want the memory capacity of Mac + the raw power of NVIDIA, but that doesnâ€™t exist in one box.\n\nHas anyone found a good solution? Hybrid setup? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6hmxl/the_mac_studio_vs_nvidia_dilemma_best_of_both/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5r1ap1",
          "author": "Karyo_Ten",
          "text": "What are the sizes of models you want to train?\n\nBest is probably to train on runpod, rent a B200 or H100x8 for 8hours and be done with it.\n\nNow for inference 192GB gets you interesting models (Qwen, MiniMax, StepFun) but not \"absolutely massive\" models like DeepSeek, GLM, Kimi K2.\n\nYou didn't say your use case. For chatting/RP Macs will be good. For agentic coding you'll wait forever when you dump large files or large webpages / documentation into it.",
          "score": 10,
          "created_utc": "2026-02-16 20:57:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u9u1m",
              "author": "TrendPulseTrader",
              "text": "+1 train on runpod or similar",
              "score": 1,
              "created_utc": "2026-02-17 09:53:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o60pthd",
              "author": "cibernox",
              "text": "This may not be the case anymore in a few weeks when the M5 family chips land. They have ML accelerators similar to cuda cores and promo processing might be 4x what current models get (at least the base M5 runs circles around base M4)",
              "score": 1,
              "created_utc": "2026-02-18 08:22:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60s1xz",
                  "author": "Karyo_Ten",
                  "text": "Fair point. But Apple may also x5 the prices to follow the RAM premium.",
                  "score": 1,
                  "created_utc": "2026-02-18 08:43:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q9hiv",
          "author": "HealthyCommunicat",
          "text": "I have a 5090 workstation and 378 gb of mac unifed memory. \n\nUSE of the model is going to always be so much more important and will only be a such tiny part of your time compared to TRAINING or other CUDA things in real world cases.\n\nTwo dgx sparks canâ€™t even beat the m3 ultra in terms of t/s, and the prefix cache fixes the prmpt processing issues if you are using coding loops or normal use case of conversations and not massive massive data processing isnâ€™t your #1 requirement - but inferencing the biggest models at the best speed is ALWAYS going to be yohr main use case and need, and youâ€™re kidding yourself if you say otherwise as the time and use of the things that are needed in CUDA are super niche and such a dramatic portion of your time will be spent on inferencing and using models itself.\n\nIf your on mac check this out for the fastest server / plug and play agentic coding tool: https://vmlx.net/",
          "score": 14,
          "created_utc": "2026-02-16 18:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qm4d0",
              "author": "DataGOGO",
              "text": "Sparks are not intended to be fast local inference machines. They are development consoles that run the exact same hardware and software stack as the massive clusters, meaning you dev and test on the cheap little spark before you push big jobs to the datacenter full of clusters. If that isnâ€™t you, donâ€™t buy sparks.Â \n\nIf you are just running a personal use chatbox, and want to mess around with running larger models (albeit slowly), then I mostly agree with you.\n\nBut anything beyond that, CUDA isnâ€™t niche, it is THE industry standard in which everything is built on.Â ",
              "score": 10,
              "created_utc": "2026-02-16 19:43:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rmw8o",
                  "author": "luix93",
                  "text": "A spark is not much slower in pure t/s than an m3 ultra, but is much faster in prompt processing. It is also twice or more faster at anything that deals with image or video generation, and an Asus gx10 can be had for less than 3k. If one is looking for a little box that they can hide anywhere with low power consumption then that makes it also a good pick for inferencing imho, as long as you like to thinker with stuff. I love mine personally.",
                  "score": 2,
                  "created_utc": "2026-02-16 22:44:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5tmail",
              "author": "NeverEnPassant",
              "text": "Prefix caching doesn't solve slow prompt processing with coding models.",
              "score": 1,
              "created_utc": "2026-02-17 06:14:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5trgm9",
                  "author": "HealthyCommunicat",
                  "text": "It doesnâ€™t completely yeah but for a good wide range of use cases that is actual more home use automation and not RAG of needing to constantly scrape a crap ton of text, going through a properly structed project should be decent. I work on full sites with a custom cli agent and its been pretty nice so far.",
                  "score": 1,
                  "created_utc": "2026-02-17 06:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5qq64k",
          "author": "Proof_Scene_9281",
          "text": "I think it depends on the use-case. Initially i thought building codes through the commercial API's was going to be cost prohibitive and painful. But now I've pretty much built everything that was needed with a claude Max subscription and ChatGPT pro. It's not even close to the cost of local hardward, especially in todays pricing. \n\ni'm still looking for a good use-case for my 4x3090 machine. ",
          "score": 4,
          "created_utc": "2026-02-16 20:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rya8u",
              "author": "bac2qh",
              "text": "Vibevoice asr to record meetings and transcribe. Thatâ€™s what I am doing now lol",
              "score": 1,
              "created_utc": "2026-02-16 23:47:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r8p4g",
          "author": "wouldntthatbecool",
          "text": "Read the recommendations for Kimi K2.5 yesterday, and it is 2x4090's and 1.92TB of RAM.",
          "score": 2,
          "created_utc": "2026-02-16 21:33:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rb80m",
          "author": "Creepy-Bell-4527",
          "text": "Macs are good at inference, not training.\n\nIn fact the RTX 5090 won't get you far on training either.",
          "score": 2,
          "created_utc": "2026-02-16 21:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ret1o",
          "author": "Zen-Ism99",
          "text": "Will MLX not work for you?",
          "score": 2,
          "created_utc": "2026-02-16 22:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3niu",
              "author": "JournalistShort9886",
              "text": "It does my initial models were trained on mlx on a macbook m2 ,though it is not as optimized and slower than nvidia  \n Plus im not a enterprise level model trainer,im more like a â€œenthusiast â€ level who adjusts scale according to hardware currently i have rtx5080 and i trained 600m from scratch ,if i have more i will train more,that said maybe mac studio is the only option",
              "score": 1,
              "created_utc": "2026-02-17 08:53:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5rfr17",
          "author": "Zen-Ism99",
          "text": "Yup, Iâ€™m looking forward to the M5 Ultraâ€¦",
          "score": 2,
          "created_utc": "2026-02-16 22:08:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3p2h",
              "author": "JournalistShort9886",
              "text": "Fr same",
              "score": 2,
              "created_utc": "2026-02-17 08:53:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s4hd8",
          "author": "SDusterwald",
          "text": "For Nvidia vs Mac - main question would be if you want to use any diffusion models alongside the LLMs. Macs are okay at LLM inference, but for image/video gen I highly recommend the Nvidia route at this time.\n\nMore importantly, if you do decide on the Mac route I highly recommend waiting for the M5 Ultra MacStudio. It should be coming later this year and will be far better for all AI workloads than the previous gen Macs due to the built in matmul acceleration in the M5 GPU. Spending that much money now when a huge upgrade is just around the corner makes no sense (if you can't wait I'd probably just go for Nvidia - not going to see any new Nvidia GPUs for at least a year, maybe two).",
          "score": 2,
          "created_utc": "2026-02-17 00:23:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u45wn",
              "author": "JournalistShort9886",
              "text": "Right ,yes i can wait its not like i was going to buy it tomm,i was planning for future\nThanks for your suggestion!",
              "score": 1,
              "created_utc": "2026-02-17 08:58:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5rh562",
          "author": "hermjohnson",
          "text": "Have you considered one of the Nvidia GB10 devices (ie DGX Spark)? I just ordered the Asus version for $3k. 128GB of shared memory.",
          "score": 1,
          "created_utc": "2026-02-16 22:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3zo0",
              "author": "JournalistShort9886",
              "text": "Yeah heard it is good ;though for your use case is the unified memory gb/s enough,like isnt it 200-300gb/s,that said 128gb is still impressive and 1000tflops on fp4 is great for training models like in 1.5b range \nGuess we cant be too greedyðŸ˜…",
              "score": 1,
              "created_utc": "2026-02-17 08:56:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sgdso",
          "author": "clwill00",
          "text": "Yeah, I have a large Mac Studio and played around. Ugh. Decided to go all in, built a monster AI rig running Windows. AMD Threadripper, 128gb DDR5 ram, Samsung 8tb 9100 ssd, and RTX 6000 workstation with 96gb vram. Your â€œdoesnâ€™t exist in one boxâ€ you mentioned above. It rocks.",
          "score": 1,
          "created_utc": "2026-02-17 01:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sy86v",
          "author": "midz99",
          "text": "This is how nividia controls the market. wait for the new mac studio. coming from someone who owns 4 nvidia 6000 adas",
          "score": 1,
          "created_utc": "2026-02-17 03:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tcup6",
          "author": "syndorthebore",
          "text": "I have 4 RTX pro blackwell 6000 max-q on a workstation.\n\nThis feels barely ok to train, a mac won't do for training at all.\n\nIt depends on use case, I'll be honest, just rent clusters it's way better price/output ratio.\n\n\n\nI also do video music and image generation, if you want to dip your toes in this, the mac won't do either. ",
          "score": 1,
          "created_utc": "2026-02-17 05:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ttsa4",
          "author": "Chlorek",
          "text": "I burned myself a few times on seemingly good hardware only to discover subpar or even nonexistent software support for it. I felt bad about it and it was not even a big investment. Therefore I see Mac the same way vs CUDA on nvidia. I would be very careful with pumping big sums of money into systems I am not sure of. As for Macs I read you need to go with top models as memory bandwidth is not that great on lower ones.",
          "score": 1,
          "created_utc": "2026-02-17 07:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qky31",
          "author": "DataGOGO",
          "text": "If you are doing any training at all, the mac is not really an option.\n\nIf you are just serving models the Mac works pretty well.\n\nIn terms of local hardware, you are not going to do any real training with consumer gaming GPUâ€™s you will need at least an RTX Pro BW, but even then you only have 96GB of VRAM; realistically, you would need 4 or 8; or buy 4 H200 NVLâ€™s (~$130k), and that is an entry point.Â \n\nThe real answer for occasional training is you rent the clusters by the hour.Â \n\nThat said, if you are just learning, a RTX 5090 willÂ work just fine for labs / making very small models.",
          "score": 0,
          "created_utc": "2026-02-16 19:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q9bkw",
          "author": "Antique_Dot_5513",
          "text": "Yes, it's called an API. Otherwise, rent a more powerful GPU online.",
          "score": -2,
          "created_utc": "2026-02-16 18:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5resv4",
              "author": "Ryanmonroe82",
              "text": "Or buy the compute and not be locked in to api costs.  I made a dataset this passed week and final result was 280 million tokens.  That's many thousands of dollars in api costs right there, cheaper to buy something in the long run",
              "score": 5,
              "created_utc": "2026-02-16 22:03:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7dbwm",
      "title": "I built VELLE.AI - a local AI companion with memory, voice, quant engine, and a full productivity suite. No cloud, no subscriptions. Everything on your machine.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "author": "Rich_Supermarket_164",
      "created_utc": "2026-02-17 17:54:58",
      "score": 35,
      "num_comments": 20,
      "upvote_ratio": 0.79,
      "text": "Hey everyone, I've been building this for a while and finally shipped it.\n\n**VELLE.AI** is a local AI operating system that runs on top of Ollama. It's not just another chat wrapper. It's a full personal assistant with persistent memory, two-way voice, a quantitative finance engine, and a productivity suite with todos, habits, goals, journal, and achievements.\n\n**What makes it different:**\n\n* **Persistent memory** â€” it actually remembers you across sessions. Your preferences, your name, your projects. All stored locally in SQLite.\n* **Two-way voice** â€” speech-to-text plus text-to-speech with hands-free mode. Talk to it, it talks back.\n* **7 personalities** â€” switch between Default, Sarcastic, Evil Genius, Anime Mentor, Sleepy, Kabuneko (finance gremlin), and Netrunner (cyberpunk). They actually stay in character.\n* **Kabuneko Quant Engine** â€” real-time stock quotes, full technical analysis including RSI, MACD, Bollinger, ADX, Sharpe, momentum scanning, value dislocations, backtesting, sentiment analysis, and a 4-bucket stock ideas generator. All from Yahoo Finance, no API keys needed.\n* **Productivity suite** â€” task manager with priorities, projects, due dates, habit tracker with streaks and weekly grids, pomodoro timer, goal system with milestones and progress bars, personal journal with writing prompts, bookmarks, knowledge base.\n* **25 achievements** â€” unlock badges as you use it. Toast notifications slide in when you earn one.\n* **Auto-insights** â€” detects patterns like \"work has been a recurring stressor 4 times this week\" or \"you created 15 tasks but only completed 3.\"\n* **Daily briefing** â€” one command gives you mood, tasks, habits, goals, streaks, reminders, and market data.\n* **Local file search** â€” searches your Desktop, Documents, Projects, Code directories by filename and content.\n* **System commands** â€” opens apps, runs PowerShell commands, controls your machine.\n* **Proactive reminders** â€” \"remind me to check email in 10 minutes\" actually fires with browser notifications plus text-to-speech.\n* **Cyberpunk terminal UI** â€” because aesthetics matter.\n\n**Tech stack:** Node.js, Express, WebSocket, SQLite, Ollama, vanilla JS. About 8,000 lines across 6 server modules. Works with any Ollama model including qwen3:8b, llama3, mistral. Ships as a Windows .exe or run from source on any OS.\n\n**Zero external AI APIs. Zero telemetry. Zero cloud. Everything local.**\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5x69lm",
          "author": "BonebasherTV",
          "text": "unrevocable-lyla-gleefully.ngrok-free.dev is a tunnel to the OPâ€™s computer most likely. If the OP has it turned on it works otherwise it wonâ€™t work. \nWould love to see the repo of this. To understand the interactions between the different components.",
          "score": 9,
          "created_utc": "2026-02-17 19:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ycgk3",
              "author": "Rich_Supermarket_164",
              "text": "here made it public [https://github.com/velle999/velle.ai](https://github.com/velle999/velle.ai) ",
              "score": 8,
              "created_utc": "2026-02-17 23:10:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xggn2",
          "author": "Barachiel80",
          "text": "Github repo or this is AI slop vaporware",
          "score": 9,
          "created_utc": "2026-02-17 20:34:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wow5y",
          "author": "Awkward-Customer",
          "text": "What are you using for the finance aspect of this? Also, is there a reason you chose to support only ollama and not llama.cpp?",
          "score": 5,
          "created_utc": "2026-02-17 18:24:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wpafn",
          "author": "Fair-Cookie9962",
          "text": "[velle.ai](http://velle.ai) site seems down, [vella.ai](http://vella.ai) seems something different. There is lot of things named velle or vellum, which is confusing.",
          "score": 3,
          "created_utc": "2026-02-17 18:26:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wpdk5",
          "author": "rusty_daggar",
          "text": "Do you have  link? \"velle.ai\" points to another address that is broken",
          "score": 2,
          "created_utc": "2026-02-17 18:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wwapq",
          "author": "eazolan",
          "text": "I'm surprised you didn't make \"Hyper competent assistant\" one of the personalities.",
          "score": 1,
          "created_utc": "2026-02-17 18:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x6g39",
          "author": "Sidze",
          "text": "I wonder why it's on Ollama if MLX models are more efficient for Apple Silicon on MacOS. I guess it could be connecting to Osaurus for MLX models and more efficiency.",
          "score": 1,
          "created_utc": "2026-02-17 19:46:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xjx07",
              "author": "Fair-Cookie9962",
              "text": "Ollama sound familiar, easier to trust.",
              "score": 0,
              "created_utc": "2026-02-17 20:50:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xph8b",
                  "author": "Sidze",
                  "text": "MLX is a framework built by Apple, not easier to trust?  \nThough I get it - much easier to plugin Ollama and forget it, instead of creating the whole MLX host manager. Anyway.",
                  "score": 1,
                  "created_utc": "2026-02-17 21:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xwfnr",
          "author": "Christosconst",
          "text": "So you built an operating system? For a companion bot? With filesystems and printer drivers and whatnot?",
          "score": 1,
          "created_utc": "2026-02-17 21:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zns9y",
          "author": "dropswisdom",
          "text": "Looks great. Can you help to package it in a docker that can use an existing (separate) ollama instance running on another docker with docker compose?",
          "score": 1,
          "created_utc": "2026-02-18 03:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wx6f9",
          "author": "Far_Cat9782",
          "text": "Wtf I'm in the middle of the same thing my god what a small word.i already have  rag, chat history,  image generation, web search, different agent cards with different functionality like creative writing agen who generates images to go along with stories.a media manager agent who handles jellyfin and can list all the media on your server with movie posters and allows you  watch the movie right there or cast to a tc. I'm mind blown that we all have the same ideas. I call mines June.ai\n\nhttps://preview.redd.it/fmys6r6as3kg1.png?width=1220&format=png&auto=webp&s=b7cbb3c25cfcd491e0b82b644ccdb4201a28809d",
          "score": 1,
          "created_utc": "2026-02-17 19:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wz1ib",
              "author": "Awkward-Customer",
              "text": "I suspect a lot of people are working on this right now. the biggest hurdle is probably ingesting the data from so many different sources, some of which deliberately silo their data.",
              "score": 3,
              "created_utc": "2026-02-17 19:11:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5wxh8g",
              "author": "Far_Cat9782",
              "text": "Here's another screenshot of the jellyfin integration\n\nhttps://preview.redd.it/jlcqpypzr3kg1.png?width=1220&format=png&auto=webp&s=150883919c51839ac805765f3f68e647184bd69b",
              "score": 0,
              "created_utc": "2026-02-17 19:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wxlnv",
          "author": "Far_Cat9782",
          "text": "The agents\n\nhttps://preview.redd.it/gdip5u34s3kg1.png?width=1220&format=png&auto=webp&s=6f18ea71747a245355eda9ab074379cbe527801f",
          "score": 1,
          "created_utc": "2026-02-17 19:04:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7bohc",
      "title": "[macOS] Built a 100% local, open-sourced, dictation app. Seeking beta testers for feedback!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r7bohc",
      "author": "AdorablePandaBaby",
      "created_utc": "2026-02-17 16:58:03",
      "score": 33,
      "num_comments": 36,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7bohc/macos_built_a_100_local_opensourced_dictation_app/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5w75dq",
          "author": "Robby2023",
          "text": "Looks really good! Kudos for this.\n\nOne question, how much RAM do you need in order for it to work properly?",
          "score": 4,
          "created_utc": "2026-02-17 17:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w83la",
              "author": "AdorablePandaBaby",
              "text": "I've tested reliably on a 4GB machine. Wouldn't go lower than that tbh.",
              "score": 3,
              "created_utc": "2026-02-17 17:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5w8qb7",
                  "author": "Robby2023",
                  "text": "Which model does it use behind the scenes?",
                  "score": 1,
                  "created_utc": "2026-02-17 17:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w8tdr",
          "author": "rusty_daggar",
          "text": "I don't have a mac to try it on, but it looks like a nice idea.\n\nAre you using a VAD to clean up the audio or just passing all straight to whisper?",
          "score": 3,
          "created_utc": "2026-02-17 17:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9v0i",
              "author": "AdorablePandaBaby",
              "text": "Straight to whisper. \n\nBut I haven't felt the need to remove bg noise yet.   \nMaybe a lower priority improvement later down the line.",
              "score": 2,
              "created_utc": "2026-02-17 17:14:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wp082",
          "author": "JohnHawley",
          "text": "I've been using Handy, it's been great! [https://github.com/cjpais/Handy](https://github.com/cjpais/Handy)  \nWhat does SpeakType do differently? Is there extra logic that Handy doesn't perform?",
          "score": 6,
          "created_utc": "2026-02-17 18:25:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w759k",
          "author": "AdorablePandaBaby",
          "text": "The title should say \"open source\", instead of \"open-sourced\", but I guess its too late.",
          "score": 2,
          "created_utc": "2026-02-17 17:01:03",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5w9buv",
          "author": "iongion",
          "text": "UI is gorgeous",
          "score": 2,
          "created_utc": "2026-02-17 17:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9zfe",
              "author": "AdorablePandaBaby",
              "text": "Thank you!  \nMy OCD drove me crazy, but I'm glad someone liked it as well.",
              "score": 2,
              "created_utc": "2026-02-17 17:15:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wx6ob",
          "author": "Pitiful-Impression70",
          "text": "nice, the local-only approach is the way to go imo. quick question tho, does it do any LLM post-processing on the whisper output? like adding punctuation, fixing capitalization, formatting stuff based on context? raw whisper output is usually pretty good but it still dumps everything as one block of text which is annoying when youre dictating emails or notes. thats been the main gap ive seen with most local dictation tools vs the cloud ones that have an extra formatting pass",
          "score": 2,
          "created_utc": "2026-02-17 19:02:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wxac1",
              "author": "AdorablePandaBaby",
              "text": "Working on that rn :)",
              "score": 1,
              "created_utc": "2026-02-17 19:03:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ywjpm",
                  "author": "Pitiful-Impression70",
                  "text": "nice, thats the part that really makes or breaks it imo. like raw whisper output is fine for short stuff but anything longer than a paragraph and the lack of punctuation and formatting makes it basically unusable without cleanup. curious if youre gonna do it locally too or offload to an api",
                  "score": 1,
                  "created_utc": "2026-02-18 01:01:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xf3ls",
          "author": "Appropriate-Deer234",
          "text": "Looks nice and i also love that everything stays offline. Already downloaded and will test it tomorrow on M1 and M3. I also would have the 2019 Intel if you need feedback from that.",
          "score": 2,
          "created_utc": "2026-02-17 20:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zo00h",
              "author": "AdorablePandaBaby",
              "text": "Yes perfect. Will DM you!",
              "score": 1,
              "created_utc": "2026-02-18 03:29:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xx0s1",
          "author": "CaptainSuckie",
          "text": "I'd like to test this out! ",
          "score": 2,
          "created_utc": "2026-02-17 21:51:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y4oh0",
          "author": "DertekAn",
          "text": "Yes please? ðŸ¤­ðŸ¤­",
          "score": 2,
          "created_utc": "2026-02-17 22:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wax8h",
          "author": "vulture916",
          "text": "Interested! ",
          "score": 1,
          "created_utc": "2026-02-17 17:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wepnv",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:37:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5weryo",
              "author": "vulture916",
              "text": "Spinning Wheel loop on Transcribe using pre-built app. Can't get anything transcribed using Whisper Large V3 Turbo on Macbook M1 14.1 Sonoma.\n\n  \nChecked settings and for whatever reason Audio Input was defaulted to NoSound rather than Macbook microphone. Changed that, same behavior.  ",
              "score": 1,
              "created_utc": "2026-02-17 17:38:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5widpv",
          "author": "Meowliketh",
          "text": "Hey, Iâ€™d love to test this out!",
          "score": 1,
          "created_utc": "2026-02-17 17:55:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5win06",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60zbpl",
                  "author": "Meowliketh",
                  "text": "Don't think I got the DM? Also, love your user name",
                  "score": 1,
                  "created_utc": "2026-02-18 09:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wy4sg",
          "author": "sinebubble",
          "text": "I'm unclear what problem this solves on macOS. Is the implication that Apple's built-in speech dictation is not private? I use the built-in dictation service all the time to transcribe into every app I've used it with.",
          "score": 1,
          "created_utc": "2026-02-17 19:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wzb1k",
              "author": "AdorablePandaBaby",
              "text": "Correct. That's the main usecase, but Apple's STT doesn't detect words properly for a lot of my usecases. \n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 2,
              "created_utc": "2026-02-17 19:12:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wyc52",
          "author": "solipsistmaya",
          "text": "Looks good, interested in testing.",
          "score": 1,
          "created_utc": "2026-02-17 19:08:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x3249",
          "author": "Ok_Yoghurt248",
          "text": "does it work on windows ?",
          "score": 1,
          "created_utc": "2026-02-17 19:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x5q7r",
          "author": "aqdnk",
          "text": "would like to test!",
          "score": 1,
          "created_utc": "2026-02-17 19:43:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xa2en",
          "author": "Correct_Support_2444",
          "text": "I will be trying this out later this week when I get home from a trip. This is exactly what Iâ€™ve been looking for.",
          "score": 1,
          "created_utc": "2026-02-17 20:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xni9n",
          "author": "Driftwintergundream",
          "text": "I'm an active user of superwhisper but looking for a local model TTS.\n\nTo me the 3 key features that would sell me is 1) super fast, 2) super accurate, and 3) slight touch up options of the text to remove verbal mispeaks or ums.\n\nI'm testing it out!",
          "score": 1,
          "created_utc": "2026-02-17 21:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zoso6",
              "author": "AdorablePandaBaby",
              "text": "Yes, then SpeakType should be just the best STT for you. It's fast, accurate and removes all the verbal misspeaks you're concerned about",
              "score": 1,
              "created_utc": "2026-02-18 03:34:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yhhu6",
          "author": "The_BeatingsContinue",
          "text": "So, you decided against a $12/month subscription and i think everybody does. In times of Claude Code it's incredibly easy to invest a little time to solve the whole task selfmade.\n\nMy question is: how can you decide against a subscription model and still want money for it while claiming to be open source? No offense, just a serious question.\n\nAs an additional use case idea: I work with deaf people and having a Mac on a table that makes a whole conversation transparent for anyone at that table would be a great feature, requiring just a text output window scalable to fullsize screen, while letting users choose font/fontsizes and opting out the requirement to push a button. It just can be active all the time and can be started/stopped by a keypress.",
          "score": 1,
          "created_utc": "2026-02-17 23:38:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yzqge",
              "author": "CtrlAltDelve",
              "text": "It's a pretty common model for binaries to be paid, but the sources are open, and nothing stops you from building it with no restrictions. \n\nVoiceInk is one of these. \n\nIf you're familiar with Claude Code, you can just ask it to compile it for you?",
              "score": 1,
              "created_utc": "2026-02-18 01:18:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zjep0",
          "author": "rditorx",
          "text": "macOS has offline dictation, you just need to enable it in the System Preferences and download the language data in there. No need to trust third parties. It might send some data to Apple, though, so consult the privacy policy.",
          "score": 1,
          "created_utc": "2026-02-18 03:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zp8ju",
              "author": "AdorablePandaBaby",
              "text": "Yep, you're right.\n\nBut, Apple's STT doesn't detect words properly for a lot of usecases.\n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 1,
              "created_utc": "2026-02-18 03:37:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6231pt",
          "author": "adrian_dev_yyc",
          "text": "Been building something similar on the Windows side. Privacy is the number one thing people bring up when I ask why they don't use cloud dictation. What's your latency like? In my experience that matters way more than raw accuracy for whether people actually stick with it.",
          "score": 1,
          "created_utc": "2026-02-18 14:24:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4x41x",
      "title": "Qwen3 8b-vl best local model for OCR?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4x41x/qwen3_8bvl_best_local_model_for_ocr/",
      "author": "BeginningPush9896",
      "created_utc": "2026-02-14 21:56:19",
      "score": 33,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "For all TLDR:\n\n Qwen3 8b-vl is the best in its weight class for recognizing formatted text (even better than Mistral 14b with OCR).\n\nFor others:\n\nHi everyone, this is my first post. I wanted to discuss my observations regarding LLMs with OCR capabilities.\n\nWhile developing a utility for automating data processing from documents, I needed to extract text from specific areas of documents. Initially, I thought about using OCR, like Tesseract, but I ran into the issue of having no control over the output. Essentially, I couldn't recognize the text and make corrections (for example, for surnames) in a single request.\n\nI decided to try Qwen3 8b-vl. It turned out to be very simple. The ability to add data to the system prompt for cross-referencing with the recognized text and making corrections on the fly proved to be an enormous killer feature. You can literally give it all the necessary data, the data format, and the required output format for its response. And you get a response in, say, a JSON format, which you can then easily convert into a dictionary (if we're talking about Python).\n\nI tried Mistral 14b, but I found that its text recognition on images is just terrible with the same settings and system prompt (compared to Qwen3 8b-vl). Smaller models are simply unusable. Since I'm sending single requests without saving context, I can load the entire model with a 4k token context and get a stable, fast response processed on my GPU.\n\nIf people who work on extracting text from documents using LLMs (visual text extraction) read this, I'd be happy to hear about your experiences.\n\nFor reference, my specs:\nR7 5800X\nRTX 3070 8GB\n32GB DDR4\n\nUPD: Forgot to mention. I work with Cyrillic text recognition, so everyone from the CIS segment reading this post can be sure that it applies to Cyrillic alphabets as well.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4x41x/qwen3_8bvl_best_local_model_for_ocr/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5f4nji",
          "author": "sinan_online",
          "text": " Not only do I agree , I also wrote an evaluation script, evaluated a bunch of models, and Qwen3 VL 8B came out on top, passing even Pixtral. I have a Medium article about it.",
          "score": 9,
          "created_utc": "2026-02-14 23:10:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ff6up",
          "author": "International-Lab944",
          "text": "I agree , itâ€™s my go-to local VL/OCR model. When I was doing my evaluations I tested many models and Qwen 8B VL was the top model. I also tested Gemma 27B and Qwen 32B and few more and the small 8B came on top. When itâ€™s not good enough I normally use Qwen3 235B VL through Openrouter.",
          "score": 6,
          "created_utc": "2026-02-15 00:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fcqa4",
          "author": "beedunc",
          "text": "Shhhh. Donâ€™t tell anybody. \n\nIâ€™ve gotten even the 4B/4q Ti do things that no other VL model can. Thereâ€™s none better.",
          "score": 5,
          "created_utc": "2026-02-15 00:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ffrww",
              "author": "BeginningPush9896",
              "text": "What do you think could be the reason that large models perform worse at text recognition than Qwen3-VL?",
              "score": 3,
              "created_utc": "2026-02-15 00:19:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fk054",
                  "author": "beedunc",
                  "text": "I donâ€™t know enough to comment on that, I just run â€˜em. Enjoy!",
                  "score": 3,
                  "created_utc": "2026-02-15 00:45:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ffujr",
          "author": "greatwilt",
          "text": "ZwZ 8B has entered the chat.\n\nhttps://huggingface.co/inclusionAI/ZwZ-8B",
          "score": 4,
          "created_utc": "2026-02-15 00:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mzagx",
              "author": "BeginningPush9896",
              "text": "Dude, just set up the ZwZ 8b. From the initial tests, it's absolutely killer, thanks for the tip. I'm going to keep experimenting with it. If anything changes, I'll post a full review about switching to the ZwZ 8b.",
              "score": 3,
              "created_utc": "2026-02-16 05:52:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5hpnnj",
              "author": "BeginningPush9896",
              "text": "I will test this model later and be sure to write about my impressions. Besides, it says it can automatically recognize areas. Currently, I'm doing the document cropping myself.",
              "score": 1,
              "created_utc": "2026-02-15 11:23:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5y3na7",
              "author": "SpicyWangz",
              "text": "Pretty impressive model honestly",
              "score": 1,
              "created_utc": "2026-02-17 22:24:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5frrzj",
          "author": "boyobob55",
          "text": "Yes it kicks ass I used it in this project too!: https://github.com/boyobob/OdinsList",
          "score": 2,
          "created_utc": "2026-02-15 01:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fgqs3",
          "author": "nunodonato",
          "text": "4B here :)",
          "score": 1,
          "created_utc": "2026-02-15 00:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gbpjs",
          "author": "michael_p",
          "text": "Currently using pixtral 12b 4bit mlx and going to swap to qwen 3 8b-vl per your rec! I love the qwen family. Qwen3 32b mlx is my favorite local model. Feed it good prompts and configure it right and get (imo) opus quality thinking",
          "score": 1,
          "created_utc": "2026-02-15 03:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ijruo",
          "author": "gabriel0123m",
          "text": "For a customer I have made a similar use case like yours and tried multiple models and ways to run them (ollama, vllm) but with qwen 2.5 VL 7B and qwen 3 vl 4B i got the better quality / performance / price to run, but using multiple agents (think like a workflow) to handle some steps and errors with text extraction + using ocr as a fallback step / comparator and if i had evaluated it was possible to be used extract and use text directly (for txt or pdf that are computer generated...). From v2.5 to 3  I had seen better results for OCR in general with the 3 version, but only using a 4B model to make it follow some complex instructions (like following some rules to handle json structured output) wasn't always correct... So using right now the 8B version in prod! I am evaluating as a side GLM-ocr but I have not used as much as qwen vl, ...",
          "score": 1,
          "created_utc": "2026-02-15 14:52:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j4xa0",
          "author": "damirca",
          "text": "for HA it's not so good I'd say\n\nhttps://preview.redd.it/qf321rf3sojg1.png?width=1190&format=png&auto=webp&s=76ed1e8e195c4a44138448104cebb904215f39f9\n\n",
          "score": 1,
          "created_utc": "2026-02-15 16:38:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j73qq",
              "author": "BeginningPush9896",
              "text": "In my workflow, I work with drawings where I can manually crop all the fields I need, so I don't need to rely on searching for a field with specific text.\n\nI would be very interested to know if there are models that can match surnames with signature fields in documents automatically, without prior cropping.",
              "score": 1,
              "created_utc": "2026-02-15 16:48:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jegwf",
          "author": "l_Mr_Vader_l",
          "text": "If you want custom extraction from documents go with qwen 3 vl, but if you just wanna do really good ocr (proper page to markdown, tables and everything) there are smaller dedicated ones which are better even, than the 8b qwen3 vl\n\nMineru2.5 ocr\nLighton ocr\nPaddleOCR vl \n\n and these are just ~1B models just trained to do ocr, they're definitely better in all ways (size, speed and accuracy) \n\nGlm ocr also does custom extraction, but it's main ocr pipeline was pretty underwhelming",
          "score": 1,
          "created_utc": "2026-02-15 17:24:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5msb5d",
          "author": "QuanstScientist",
          "text": "Paddle is as good as qwen and very fast: https://github.com/BoltzmannEntropy/batch-ocr",
          "score": 1,
          "created_utc": "2026-02-16 04:57:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q5b7c",
          "author": "dradik",
          "text": "What about the 30B MoE model? I get like 170 tokens a second and seems accurate.",
          "score": 1,
          "created_utc": "2026-02-16 18:24:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sa1k2",
          "author": "shankey_1906",
          "text": "Noob here, is this considered the best for handwriting recognition too, or would something like this be better? ZwZ 8B - [https://huggingface.co/inclusionAI/ZwZ-8B](https://huggingface.co/inclusionAI/ZwZ-8B)",
          "score": 1,
          "created_utc": "2026-02-17 00:55:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r26ena",
      "title": "GLM 5 is out now.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u9z1nezwywig1.png",
      "author": "Cultural-Arugula-894",
      "created_utc": "2026-02-11 19:06:37",
      "score": 30,
      "num_comments": 4,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r26ena/glm_5_is_out_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o51j942",
          "author": "Donmeusi",
          "text": "IÂ´m using it with Openclaw since a few hours. Kimi-k2.5 was good. GLM-5 is awesome. ",
          "score": 1,
          "created_utc": "2026-02-12 20:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5aun2e",
              "author": "Sociedelic",
              "text": "What hardware do you have?",
              "score": 1,
              "created_utc": "2026-02-14 06:36:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5c8fpj",
              "author": "Donmeusi",
              "text": "I have a Mac Mini 4 with 16GB. Its not possible to run this Model local so I use the Cloud Version with Ollama.",
              "score": 1,
              "created_utc": "2026-02-14 13:54:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52bbzt",
          "author": "IllFirefighter4079",
          "text": "Been using it in kilo code with great success. Can't wait for it to be available through Claude code on my Z.AI plan",
          "score": 1,
          "created_utc": "2026-02-12 22:33:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7umlq",
      "title": "[macOS] PersonaPlex-7B on Apple Silicon (MLX)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "author": "Apprehensive_Boot976",
      "created_utc": "2026-02-18 05:37:04",
      "score": 30,
      "num_comments": 2,
      "upvote_ratio": 0.98,
      "text": "NVIDIA released an open-source speech-to-speech model [PersonaPlex-7B](https://huggingface.co/nvidia/personaplex-7b-v1). It listens and talks simultaneously with \\~200ms latency, handles interruptions, backchanneling, and natural turn-taking.\n\nThey only shipped a PyTorch + CUDA implementation targeting A100/H100, so I ported it to MLX, allowing it to run on Apple Silicon: [github.com/mu-hashmi/personaplex-mlx](https://github.com/mu-hashmi/personaplex-mlx).\n\nHope you guys enjoy!",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o627q6u",
          "author": "felixlovesml",
          "text": "thanks. Whatâ€™s the latency on a Mac â€” for example, on an M4 chip â€” and how much RAM does it require?",
          "score": 2,
          "created_utc": "2026-02-18 14:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o607o3d",
          "author": "former_farmer",
          "text": "Thanks.",
          "score": 1,
          "created_utc": "2026-02-18 05:44:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2xb8j",
      "title": "QLoRA - Fine Tuning a Model at Home",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/t0b4vtn473jg1.png",
      "author": "mac10190",
      "created_utc": "2026-02-12 16:03:01",
      "score": 29,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r2xb8j/qlora_fine_tuning_a_model_at_home/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o50rrp6",
          "author": "Ryanmonroe82",
          "text": "I use Easy Dataset to make the datasets and Transformer Lab or LlamaFactory to fine tune and have had great results.  I'd say go for it.",
          "score": 2,
          "created_utc": "2026-02-12 18:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50zgjv",
          "author": "mp3m4k3r",
          "text": "It might be worth tinkering with the chat templates as well, since at least i am able to make tool calls with nemotron a lot and it seems to adapt a bit via its template vs training.\n\nThis for example i run with tool_calls and seems to work well other than with continue.dev at the moment. https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF\n\nThe chat template this model comes with is kinda interestingly formatted",
          "score": 2,
          "created_utc": "2026-02-12 18:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51cdvj",
              "author": "mac10190",
              "text": "Good to know! I'll look into chat templates for Nemotron. I was using the unsloth Nemotron and for the life of me I couldn't get it to stop putting XML tags around the JSON tool calls which caused n8n to not recognize the call.",
              "score": 1,
              "created_utc": "2026-02-12 19:45:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o52rjuz",
              "author": "mac10190",
              "text": ">The chat template this model comes with is kinda interestingly formatted\n\nThis \\^\\^ it was exactly this. Went digging into the unsloth version of Nemotron and it turns out unsloth included a tool call template in their tokenizer.chat\\_template. You were spot on! Thank you kindly. Going to see about creating my own model file and building customized version of that model.\n\nThank you u/mp3m4k3r !!!",
              "score": 1,
              "created_utc": "2026-02-13 00:03:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o52t97u",
                  "author": "mp3m4k3r",
                  "text": "Sounds fun! Iirc I had issues with one of the templates but as i am using llama.cpp you can just have the chat templates in a file and launch it as a parameter (or in my case an environment variable)",
                  "score": 2,
                  "created_utc": "2026-02-13 00:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6ao8o",
      "title": "Alibabaâ€™s Qwen team just released Qwen3.5-397B-A17B, the first open model in the Qwen3.5 family â€” and itâ€™s a big one.",
      "subreddit": "LocalLLM",
      "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B",
      "author": "techlatest_net",
      "created_utc": "2026-02-16 14:12:59",
      "score": 29,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6ao8o/alibabas_qwen_team_just_released_qwen35397ba17b/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r4oava",
      "title": "Hardware constraints and the 10B MoE Era: Where Minimax M2.5 fits in",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4oava/hardware_constraints_and_the_10b_moe_era_where/",
      "author": "Fragrant_Occasion276",
      "created_utc": "2026-02-14 16:03:02",
      "score": 27,
      "num_comments": 44,
      "upvote_ratio": 0.84,
      "text": "We need to stop pretending that 400B+ models are the future of local-first or sustainable AI. The compute shortage is real, and the \"brute force\" era is dying. I've been looking at the Minimax M2.5 architecture - it's a 10B active parameter model that's somehow hitting 80.2% on SWE-Bench Verified. That is SOTA territory for models five times its size. This is the Real World Coworker we've been waiting for: something that costs $1 for an hour of intensive work. If you read their RL technical blog, it's clear they're prioritizing tool-use and search (76.3% on BrowseComp) over just being a \"chatty\" bot. For those of us building real systems, the efficiency of Minimax is a far more interesting technical achievement than just adding more weights to a bloated transformer.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4oava/hardware_constraints_and_the_10b_moe_era_where/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5d0ni9",
          "author": "Panometric",
          "text": "At what quantization?",
          "score": 8,
          "created_utc": "2026-02-14 16:28:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e3w93",
              "author": "DataGOGO",
              "text": "FP8, it degrades rapidly at anything below thatÂ ",
              "score": 7,
              "created_utc": "2026-02-14 19:46:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ektea",
                  "author": "SpicyWangz",
                  "text": "Thatâ€™s good to know. I hope we can get some high quality quants soon, but it might just be that theyâ€™re maximizing the most out of every parameter and thereâ€™s no way to achieve a good quant on this model",
                  "score": 2,
                  "created_utc": "2026-02-14 21:18:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d11ol",
          "author": "Tema_Art_7777",
          "text": "\nFor those building real products, why are you so focused on cost of tokens? Frontier models are amazing, and they really do not cost much. If you are worried $10 vs $1 for an hour of work, what is your business model that is so sensitive to that? Genuinely want to know.",
          "score": 6,
          "created_utc": "2026-02-14 16:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d68gi",
              "author": "sooodooo",
              "text": "Iâ€™ll point out the â€œsustainableâ€ part first. We know the big providers all burn through investor money by providing AI at the current price. Itâ€™s 10$ now, but without more efficient models thatâ€™s not sustainable and investors want to see some honey at some point.\n\nSecond I donâ€™t know what metric â€œ$10 per hour workâ€ is, but Iâ€™d rather pay $10 and have the same work done in a minute. I donâ€™t understand how price could not be THE deciding factor in the age of cloud computing, if I can get the same work done at 1/10 the of the price then you can scale 10x bigger and faster.",
              "score": 10,
              "created_utc": "2026-02-14 16:56:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dx06y",
                  "author": "Karyo_Ten",
                  "text": "Companies are often bottlenecked by humans, within or outside, say verification, quality control, certification, a signature.\n\nProducing meh output fast is not interesting because work will be sent back.\n\nIt's interesting for iteration/prototyping but a model that does well from the get go might end up costing less even if bigger / more expensive per token",
                  "score": 1,
                  "created_utc": "2026-02-14 19:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ei0r9",
              "author": "evilbarron2",
              "text": "Are you building a product thatâ€™s only good for business use cases? Thatâ€™s fine, but it isnâ€™t where the majority of ai stuff is gonna happen. Instead of focusing on what the user is or isnâ€™t doing, just make clear what the requirements/ costs of *your* product is. Users arenâ€™t gonna use your product the way you think they will.",
              "score": 1,
              "created_utc": "2026-02-14 21:03:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eu8mi",
                  "author": "Tema_Art_7777",
                  "text": "It is a mix, both business and personal use cases like home automation. to expand further, $10 for mm tokens donâ€™t move the needle for business and I donâ€™t need mm of tokens for personal stuffâ€¦. So still not sure why everyone is hyped up about 50cents for mm tokens. So I must be missing a use-case people haveâ€¦",
                  "score": 1,
                  "created_utc": "2026-02-14 22:10:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5fi1kd",
              "author": "former_farmer",
              "text": "You are on LocalLLM my brother. Not on \"consume LLMs on the cloud for cheap\" subreddit.",
              "score": 1,
              "created_utc": "2026-02-15 00:33:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fkh5q",
                  "author": "Tema_Art_7777",
                  "text": "Yes except the OPâ€™s post said: â€œThis is the Real World Coworker we've been waiting for: something that costs $1 for an hour of intensive work.â€. So Iâ€™m questioning the use-cases for the intensive focus on price.",
                  "score": 1,
                  "created_utc": "2026-02-15 00:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ew29b",
          "author": "Dependent-Example930",
          "text": "There is a string of these style posts. Feels spam/fake pushing minimax 2.5",
          "score": 3,
          "created_utc": "2026-02-14 22:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d0btt",
          "author": "Pixer---",
          "text": "What local system do you have ?",
          "score": 2,
          "created_utc": "2026-02-14 16:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e5n6a",
              "author": "starkruzr",
              "text": "unfortunately chances are good OP is just promoting.",
              "score": 1,
              "created_utc": "2026-02-14 19:55:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e0bjy",
          "author": "No-Leopard7644",
          "text": "ðŸ’¯ My focus and expertise is local ai with open source models and tools - for enterprises as well as personal use. That being said, the element of tokenomics, sovereign data and model freedom is paramount for highly regulated industries. I spend my day job straddling cloud as well as architecting sovereign local AI infrastructure with the ability to burst to the cloud.",
          "score": 1,
          "created_utc": "2026-02-14 19:27:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gc6bq",
          "author": "Euphoric_Emotion5397",
          "text": "  \nSo, in actual practise, my local setup that can get me the same output every single time is temp <= 0.3, tool use, searxng, playwright, local context memory to get real world updates in multistage workflow with different prompts at different stage. \n\nWe just need a good enough multimodal LLM (using qwen 3 VL 30b) and a good enough embedding model (using Gemma embedding) to understand what we are feeding it and then use the available data (we give them) to come out with an analysis in json output format.\n\nThen i feed the output to Gemini Pro to verify and it's rated highly against their own.\n\nYou need to have a workflow usecase that you use frequently which you can vibecode first.\n\nFor other things generic , then it's better to just go online and use the frontier models to chat and get your answer.",
          "score": 1,
          "created_utc": "2026-02-15 03:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h18yx",
          "author": "Exciting_Square_4593",
          "text": "The obsession with \"more parameters = more better\" is just cope for companies that can't figure out efficient architecture. M2.5 hitting 80.2% on SWE-Bench Verified with only 10B active parameters is a slap in the face to every bloated 400B model out there. If you're still paying for compute-heavy giants that lag every time you try to run a simple agentic loop, you're literally just subsidizing bad engineering.",
          "score": 1,
          "created_utc": "2026-02-15 07:28:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h8696",
          "author": "Monty756",
          "text": "$1 an hour for a real-world coworker that doesn't hallucinate like a high schooler on caffeine. Finally, a model that respects my time and my wallet.",
          "score": 1,
          "created_utc": "2026-02-15 08:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h8aym",
          "author": "Critical-Raccoon-926",
          "text": "It's the difference between a junior dev who yaps and a senior who just pushes the fix.",
          "score": 1,
          "created_utc": "2026-02-15 08:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hi7lw",
          "author": "Previous-Shop6033",
          "text": "We are hitting the physical limits of H100 clusters and people still think scaling is the only way up. M2.5 is proof that you can reach SOTA by being smart about tool-use and search (that 76.3% on BrowseComp is no joke) rather than just throwing more VRAM at the problem.",
          "score": 1,
          "created_utc": "2026-02-15 10:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i0nlw",
          "author": "Nervous-Dig-6383",
          "text": "Imagine being a \"frontier\" model that's 5x the size and still losing to a 10B active MoE on actual engineering tasks. Embarrassing.",
          "score": 1,
          "created_utc": "2026-02-15 12:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i63m8",
          "author": "BetterInternet63",
          "text": "Finally, someone mentions the BrowseComp score. Everyone fixates on SWE-Bench, but if your \"coworker\" can't navigate documentation or search for a specific library error without getting lost, it's useless. M2.5 actually feels like it knows how to use a browser.",
          "score": 1,
          "created_utc": "2026-02-15 13:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i9ckk",
          "author": "Greedy_Sandwich_1839",
          "text": "100 TPS at 1/10th the cost of Claude. If you're not using this for your CI/CD pipelines yet, you're basically just lighting money on fire.",
          "score": 1,
          "created_utc": "2026-02-15 13:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ic5f6",
          "author": "MarinaCER87",
          "text": "Tired of these \"chatty\" bots that spend 500 tokens apologizing before they even look at the code. M2.5's focus on grounded tool-use over \"personality\" is exactly what the industry needs right now. We don't need a friend; we need an engineer.",
          "score": 1,
          "created_utc": "2026-02-15 14:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5idwlo",
          "author": "n521n",
          "text": "Everyone is mourning the \"compute shortage\" while MiniMax is just out here building the most sustainable architecture of the 10B MoE era. This is the future, whether the hardware snobs like it or not.",
          "score": 1,
          "created_utc": "2026-02-15 14:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r8w1o",
          "author": "HealthyCommunicat",
          "text": "Lol its funny cuz qwen 3.5 cane out today and is 400+ b params AND is around 10b active making it amazinfg. Low active param models are the future.",
          "score": 1,
          "created_utc": "2026-02-16 21:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ak4s",
          "author": "wwy413",
          "text": "The 400B+ model craze is just VC bait at this point. 10B active params hitting those SWE-Bench numbers is the actual engineering flex.",
          "score": 1,
          "created_utc": "2026-02-18 06:07:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60c5by",
          "author": "Letitiahappy",
          "text": "$1/hr for SOTA performance is the only metric that matters if you're actually running a business and not just playing with a chatbot.",
          "score": 1,
          "created_utc": "2026-02-18 06:20:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62fqgb",
          "author": "Huong101",
          "text": "I've been saying this - efficiency > scale. If M2.5 can maintain that 80.2% on verified benchmarks with a smaller footprint, the \"bigger is better\" era is officially over.",
          "score": 1,
          "created_utc": "2026-02-18 15:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62gidn",
          "author": "HarlanWJK",
          "text": "Just read the RL blog mentioned. The way they've optimized for tool-use instead of just conversational fluff is exactly what we need for real dev workflows.",
          "score": 1,
          "created_utc": "2026-02-18 15:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62gwh7",
          "author": "Anders0725",
          "text": "Honestly, the \"bloated transformer\" phase was getting exhausting. Glad to see someone actually prioritizing the compute-to-output ratio.",
          "score": 1,
          "created_utc": "2026-02-18 15:31:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62ia5r",
          "author": "vvkkrr",
          "text": "76.3% on BrowseComp is the real sleeper stat here. Most of these \"huge\" models are surprisingly bad at actually navigating the web to find documentation.",
          "score": 1,
          "created_utc": "2026-02-18 15:38:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62iqg8",
          "author": "lxneay",
          "text": "Finally, a model that doesn't feel like it's burning a forest just to help me debug a React component.",
          "score": 1,
          "created_utc": "2026-02-18 15:40:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62k1ij",
          "author": "TTRY01",
          "text": "Is anyone else seeing the same consistency on the 10B MoE? If it's actually hitting those benchmarks, my API costs are about to drop 80%.",
          "score": 1,
          "created_utc": "2026-02-18 15:46:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62kki3",
          "author": "Llywelynerd",
          "text": "The \"Real World Coworker\" framing is spot on. I don't need a philosopher; I need a tool that can use other tools efficiently without timing out.",
          "score": 1,
          "created_utc": "2026-02-18 15:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62lmop",
          "author": "CNMBUS",
          "text": "The brute force era dying is the best thing that could happen to local-first dev. 10B is actually manageable.",
          "score": 1,
          "created_utc": "2026-02-18 15:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62m10d",
          "author": "nbcb333",
          "text": "I've been testing M2.5 against some of the larger legacy models this morning. The logic density in a 10B parameter setup is honestly impressive.",
          "score": 1,
          "created_utc": "2026-02-18 15:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62m97o",
          "author": "wzl1985",
          "text": "M2.5 seems to be the sweet spot. Itâ€™s small enough to be fast but smart enough to actually handle task decomposition without getting lost in its own weights.",
          "score": 1,
          "created_utc": "2026-02-18 15:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62mkvb",
          "author": "deerrek",
          "text": "It's about time we focused on RL for correctness rather than just RLHF for politeness. Minimax feels like it was built for engineers, not HR departments.",
          "score": 1,
          "created_utc": "2026-02-18 15:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62o3rv",
          "author": "MarinaCER87",
          "text": "$1 for an hour of intensive work? That's basically the price of a cheap coffee for a senior-level coding assistant. Hard to argue with that ROI.",
          "score": 1,
          "created_utc": "2026-02-18 16:04:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62pkoo",
          "author": "19880331",
          "text": "If M2.5 is the benchmark for the \"10B MoE Era,\" then the future of sustainable AI actually looks promising for once.",
          "score": 1,
          "created_utc": "2026-02-18 16:11:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5f8ym",
      "title": "Tutorial: Run MiniMax-2.5 locally! (128GB RAM / Mac)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/61b97oryxnjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-15 13:57:07",
      "score": 26,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r5f8ym/tutorial_run_minimax25_locally_128gb_ram_mac/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5thtsz",
          "author": "Euphoric_Emotion5397",
          "text": "tough luck. I think models can spread faster if they try to downsize to something that fits 16gb GPU and 64GB RAM.   Right now, I'm using Qwen 3 VL 30B. Very Good! :D",
          "score": 1,
          "created_utc": "2026-02-17 05:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5izi0k",
          "author": "Hitchhiker2TheFuture",
          "text": ">",
          "score": 0,
          "created_utc": "2026-02-15 16:12:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3lx38",
      "title": "Google Releases Conductor",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r3lx38/google_releases_conductor/",
      "author": "techlatest_net",
      "created_utc": "2026-02-13 10:39:03",
      "score": 26,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "# Google Releases Conductor: a context-driven Gemini CLI extension that stores knowledge as Markdown and orchestrates agentic workflows\n\nLink: [https://github.com/gemini-cli-extensions/conductor](https://github.com/gemini-cli-extensions/conductor)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r3lx38/google_releases_conductor/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o55c7pi",
          "author": "Super-Jackfruit8309",
          "text": "about time really, having to do this manually made no sense and often got in the way of work. Looking forward to trying it out.",
          "score": 3,
          "created_utc": "2026-02-13 11:40:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b7x86",
          "author": "trentard",
          "text": "conductor we have a problem\nconductor we have a problem",
          "score": 1,
          "created_utc": "2026-02-14 08:41:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}