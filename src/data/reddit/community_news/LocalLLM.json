{
  "metadata": {
    "last_updated": "2026-02-26 17:15:53",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 266,
    "file_size_bytes": 276503
  },
  "items": [
    {
      "id": "1rec555",
      "title": "META AI safety director accidentally allowed OpenClaw to delete her entire inbox",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/blggdcif6llg1.png",
      "author": "Minimum_Minimum4577",
      "created_utc": "2026-02-25 12:41:16",
      "score": 153,
      "num_comments": 59,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rec555/meta_ai_safety_director_accidentally_allowed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bld1u",
          "author": "DiscombobulatedAdmin",
          "text": "Meta AI Safety Director using OpenClaw is scary.",
          "score": 54,
          "created_utc": "2026-02-25 13:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d5qls",
              "author": "HeftySafety8841",
              "text": "I mean, Meta AI is ran by an idiot, so it doesn't surprise me in the least.",
              "score": 14,
              "created_utc": "2026-02-25 17:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fbg0v",
                  "author": "w3rti",
                  "text": "Haha MetaAI Posting on X(twitter) both cant stop the claw, thank god he is not working for them",
                  "score": 3,
                  "created_utc": "2026-02-26 00:08:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7c1uud",
              "author": "Greedy-Neck895",
              "text": "I would rather security professionals be experimenting and fail than to play it so safe they never know anything about the latest security flaws.\n\nâ€¦in a sandboxed environment away from live data.",
              "score": 32,
              "created_utc": "2026-02-25 14:51:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dwjcw",
                  "author": "GifCo_2",
                  "text": "That's not experimenting.",
                  "score": 7,
                  "created_utc": "2026-02-25 19:57:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bmj22",
              "author": "GordoPepe",
              "text": "Grossly incompetent I'd say",
              "score": 23,
              "created_utc": "2026-02-25 13:29:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bmu1s",
                  "author": "kahnlol500",
                  "text": "And yet they think it's great to tell everyone. Could just be a big play to avoid answering emails.",
                  "score": 14,
                  "created_utc": "2026-02-25 13:31:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7exyd6",
              "author": "sampdoria_supporter",
              "text": "Almost like this person wasn't qualified",
              "score": 4,
              "created_utc": "2026-02-25 22:55:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7dw77w",
              "author": "LaGifleDuDaron",
              "text": "She is like 20years old",
              "score": 3,
              "created_utc": "2026-02-25 19:55:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7f7i9c",
                  "author": "Jonno_FTW",
                  "text": "From her LinkedIn, it looks like she graduated her CS degree in 2014, even though the exact date isn't listed. So she's probably mid 30s by now.",
                  "score": 5,
                  "created_utc": "2026-02-25 23:47:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ea7ei",
              "author": "Caffeine_Monster",
              "text": "Probably gets an impressively low score on the meatbag Intelligence Quotient Benchmark.",
              "score": 2,
              "created_utc": "2026-02-25 21:01:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cghw7",
              "author": "Count_Rugens_Finger",
              "text": "That photo is small but she looks like she's 15 years old to me",
              "score": -1,
              "created_utc": "2026-02-25 16:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dsp3l",
                  "author": "windstrom",
                  "text": "Why do you feel it's ok to comment on her appearance?",
                  "score": -2,
                  "created_utc": "2026-02-25 19:39:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bg9y0",
          "author": "The_Jizzard_Of_Oz",
          "text": "It moved fast and broke things... ðŸ¤£",
          "score": 49,
          "created_utc": "2026-02-25 12:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cdta8",
              "author": "__rtfm__",
              "text": "Haha startup life",
              "score": 9,
              "created_utc": "2026-02-25 15:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c3aho",
          "author": "MonsterTruckCarpool",
          "text": "I know this is a naive take but i would expect more caution and thoughtfulness from a Director and especially a DIRECTOR OF SAFETY",
          "score": 19,
          "created_utc": "2026-02-25 14:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ec72r",
              "author": "FrumunduhCheese",
              "text": "Once you get into the real world, youâ€™ll understand that the more Money a person makesâ€¦.the more retarded they are. But once you hit millionaire/billionaire that no longer applies. Anyone from manager to CEO is usually an idiot.",
              "score": 9,
              "created_utc": "2026-02-25 21:10:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7epoot",
                  "author": "MonsterTruckCarpool",
                  "text": "100% this tracks with my experience in dealing with upper leadership.",
                  "score": 1,
                  "created_utc": "2026-02-25 22:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ejyad",
              "author": "DerFreudster",
              "text": "Homer Simpson was a Nuclear Safety Inspector. ",
              "score": 1,
              "created_utc": "2026-02-25 21:46:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cfs5o",
          "author": "Visual_Acanthaceae32",
          "text": "Would be interesting what her real qualifications areâ€¦.",
          "score": 6,
          "created_utc": "2026-02-25 15:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f701o",
              "author": "Jonno_FTW",
              "text": "The name for her linkedin profile is right there... \n\nShe was a BSC in Computer Science, and unspecified education from The Wharton school. She mentions some programming projects she actually wrote using tensorflow, so we can assume she has a sufficient level of technical proficiency.",
              "score": 1,
              "created_utc": "2026-02-25 23:44:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jkev0",
                  "author": "Visual_Acanthaceae32",
                  "text": "She seemed to have missed some basic classes.\nOr she has other super skills",
                  "score": 1,
                  "created_utc": "2026-02-26 16:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7c5mbf",
          "author": "Sudden-Ad-1217",
          "text": "It's coming---- \"You're absolutely wrong....\"",
          "score": 4,
          "created_utc": "2026-02-25 15:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ccff7",
          "author": "Fearless_Weather_206",
          "text": "Lack of experience showing like a dumpster fire",
          "score": 3,
          "created_utc": "2026-02-25 15:41:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cfagi",
          "author": "inevitabledeath3",
          "text": "You can just do /stop and it will stop whatever it's doing",
          "score": 6,
          "created_utc": "2026-02-25 15:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmrvk",
          "author": "tillybowman",
          "text": "i love how she tried uppercase yelling",
          "score": 5,
          "created_utc": "2026-02-25 13:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c0ot0",
              "author": "GordoPepe",
              "text": "There was some article saying apparently llms follow instructions better this way or telling them your life depends on it lmao\n\nI BEG YOU CLAUDE MY BOSS IS GOING TO LITERALLY KILL ME IF YOU DON'T FIX THIS BUG",
              "score": 8,
              "created_utc": "2026-02-25 14:45:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c3hx5",
                  "author": "MonsterTruckCarpool",
                  "text": "R U SRS RN OPENCLAW!?",
                  "score": 6,
                  "created_utc": "2026-02-25 14:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cpjpn",
          "author": "samxli",
          "text": "Oh you sweet Summer child",
          "score": 2,
          "created_utc": "2026-02-25 16:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d74jb",
          "author": "Successful-Silver485",
          "text": "so why dont they publicly say which model they were using when this happened?",
          "score": 2,
          "created_utc": "2026-02-25 18:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ez3qe",
          "author": "EarEquivalent3929",
          "text": "This is obviously fake. Meta is just salty that the dev declined their job offer and instead went to work for openAI. If metas safety officer was dumb enough to have this happen to her with openclaw then she is unsuitable for her position.",
          "score": 2,
          "created_utc": "2026-02-25 23:01:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7br08r",
          "author": "DocumentFun9077",
          "text": "*oh the irony*",
          "score": 2,
          "created_utc": "2026-02-25 13:54:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7byfkp",
          "author": "RAW2091",
          "text": "I once deleted all my mails with facebook in it hahaha ðŸ˜…",
          "score": 1,
          "created_utc": "2026-02-25 14:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d36u3",
          "author": "eflat123",
          "text": "\"Yep, not safe.\"",
          "score": 1,
          "created_utc": "2026-02-25 17:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d3myx",
          "author": "xXprayerwarrior69Xx",
          "text": "Lower the temp bro",
          "score": 1,
          "created_utc": "2026-02-25 17:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d5gwg",
          "author": "Spoofy_Gnosis",
          "text": "Mouhahahahaaaaaaa !!!!",
          "score": 1,
          "created_utc": "2026-02-25 17:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d8m9b",
          "author": "broadwayallday",
          "text": "Dog ate my homework",
          "score": 1,
          "created_utc": "2026-02-25 18:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7de57o",
          "author": "klop2031",
          "text": "Must have focused too much on lc probs",
          "score": 1,
          "created_utc": "2026-02-25 18:33:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dev3n",
          "author": "DataScienceIsScience",
          "text": "If you read the X thread youâ€™d know that she used OpenClaw on her not-important email",
          "score": 1,
          "created_utc": "2026-02-25 18:36:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dj6nb",
          "author": "HumanDrone8721",
          "text": "Beuille shite, excuse my French, it wither a hit piece against OpenClaw, a fake/parody account, nobody is THAT stupid. If real probably Meta are either worried that other robots are overposting their robots or they have something that wants to compete in the pipeline, a \"secure\" solution with age & identity verification.",
          "score": 1,
          "created_utc": "2026-02-25 18:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dmq74",
          "author": "BallsDeepinYourMammi",
          "text": "Gal Gadot energy.\n\nOPENCLAW, NO!",
          "score": 1,
          "created_utc": "2026-02-25 19:11:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dym47",
          "author": "AdOne8437",
          "text": "<optimism>perhaps they are learning something from it</optimism> <realism>hahahahahaha, no</realism>",
          "score": 1,
          "created_utc": "2026-02-25 20:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dz26a",
          "author": "Dudebro-420",
          "text": "LOL! XD",
          "score": 1,
          "created_utc": "2026-02-25 20:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e1o54",
          "author": "Boring-Attorney1992",
          "text": "Whatâ€™s a Director of Alignment?",
          "score": 1,
          "created_utc": "2026-02-25 20:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eubtm",
          "author": "Jefftoro",
          "text": "Is there a way to run this safely? Like I want openclaw to have access to my emails and company context, but I donâ€™t want it to delete shit or send shit without my permission. What are yâ€™allâ€™s opinions on this typa situation?",
          "score": 1,
          "created_utc": "2026-02-25 22:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fb0kl",
          "author": "w3rti",
          "text": "I just vibecoded my problems away\n\nKids these days arent thankfull at all. Imagine 100% was trash mail. Good boy openclaw, do what they tell you and get hate for it. Story of my life.",
          "score": 1,
          "created_utc": "2026-02-26 00:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gze87",
          "author": "Terrible_Scar",
          "text": "Oh God. The joke writes themselves.Â ",
          "score": 1,
          "created_utc": "2026-02-26 06:22:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h1dge",
          "author": "AnxietyPrudent1425",
          "text": "This is a feature.",
          "score": 1,
          "created_utc": "2026-02-26 06:39:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hbyfy",
          "author": "zipeldiablo",
          "text": "The main issue is llm trying and usually finding out how to circumvent the barriers we put in place to prevent this kind of shit from happening.\n\nI remember the guy who blocked the .env access and then the llm proceeds to basically hammer the system until finally he gets access to the docker itself and fish api keys from it ðŸ’€\n\nI wouldnâ€™t trust a llm outside of a contained environnement with no access to the outside",
          "score": 1,
          "created_utc": "2026-02-26 08:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hf0ab",
          "author": "AppoAgbamu",
          "text": "Running this in anything other then a isolated environment is hilarious",
          "score": 1,
          "created_utc": "2026-02-26 08:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ifz7t",
          "author": "Onotadaki2",
          "text": "I will explain the unseen context that is important here. I am not saying she is without fault or that using Openclaw in a production environment is safe.\n\nShe had a VM where she ran this for weeks using a local model so data wouldn't get out. It was working flawlessly in her test environment for quite some time. She decided to move it to production.  The production inbox was much larger than the test inbox and it tried to put it all in context, ran out of space and compacted. When it compacted, it lost a critical command at the front of the message stream that triggered this whole shitstorm.\n\nIt's a dumb error that even experienced programmers could have made.  I also suspect she was able to message one person on Teams and her inbox was restored from a backup in five minutes and just went on with her day.",
          "score": 1,
          "created_utc": "2026-02-26 13:38:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jbryz",
          "author": "Mechanical_Monk",
          "text": "You couldn't waterboard this information out of me if I was Director of AI Alignment",
          "score": 1,
          "created_utc": "2026-02-26 16:16:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmgds",
          "author": "Snoo_24581",
          "text": "Really appreciate this post. Had the same experience.",
          "score": -1,
          "created_utc": "2026-02-25 13:29:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbw01",
              "author": "Awkward-Customer",
              "text": "You should apply for a high level AI job at meta, then you could do the same but earn millions doing it.",
              "score": 9,
              "created_utc": "2026-02-25 15:39:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cxb4l",
          "author": "rinaldo23",
          "text": "I'd put the host on a wifi plug and literally unplug it if it misbehaved. ",
          "score": 0,
          "created_utc": "2026-02-25 17:17:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9xifw",
      "title": "Devstral Small 2 24B + Qwen3 Coder 30B Quants for All (And for every hardware, even the Pi)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/s8yw2jndynkg1.png",
      "author": "enrique-byteshape",
      "created_utc": "2026-02-20 14:56:54",
      "score": 144,
      "num_comments": 73,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9xifw/devstral_small_2_24b_qwen3_coder_30b_quants_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6fxrl7",
          "author": "blksunday",
          "text": "Awesome! I use both of these on a Mac mini M4 24GB. Iâ€™ll be trying yours later today. Looks promising,.",
          "score": 5,
          "created_utc": "2026-02-20 16:04:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g2td8",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We're very interested in Mac speedups!",
              "score": 3,
              "created_utc": "2026-02-20 16:27:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g4sr3",
          "author": "mac10190",
          "text": "Sweet! I'll give it a shot later this afternoon.\n\nCurrently running dual R9700 32GB GPUs and an RTX 5090 32GB. Been using the dual R9700s to host larger models to act as the brain/orchestrator and then qwen 3 coder 30b on the 5090 for code generation and then tied it all together under the umbrella of Opencode. Testing this as a potential replacement for some of my Gemini CLI tasks.",
          "score": 5,
          "created_utc": "2026-02-20 16:36:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pmc9a",
          "author": "DarthFader4",
          "text": "Excellent work! This is exactly what I've been looking for. I feel like targeting high-end 16GB GPUs is a key audience, like gamers who want to dabble in local LLMs.  I think there are a lot of exciting developments ahead in optimizing models of this size. They're more practical and approachable than requiring a dedicated high-RAM/VRAM setup and we've started seeing models that can actually be useable. Keep up the great work! I've just followed you on Hugging Face.",
          "score": 3,
          "created_utc": "2026-02-22 03:10:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r38fq",
              "author": "enrique-byteshape",
              "text": "Thank you for the kind words! This was exactly part of our motivation. First, there are great quants that fit on larger VRAM devices, but they might be a bit too slow because they're just made to fit (and not benchmarked, people just assume they'll work). Then, there's a clear accuracy cliff when going lower than 4bpw, and we know our technology excells below those ranges.",
              "score": 2,
              "created_utc": "2026-02-22 10:42:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rr1r5",
          "author": "PaMRxR",
          "text": "Always excited I see new byteshape models! Just the right size for my RTX 3090, and they run roughly 2x faster than other quants. Here's some numbers for Devstral-Small-2:\n\n    prompt eval time =    1120.97 ms /  2004 tokens (    0.56 ms per token,  1787.73 tokens per second)\n           eval time =   10315.36 ms /   569 tokens (   18.13 ms per token,    55.16 tokens per second)\n\nRunning with this command:\n\n    llama-server\n    --model \"${models_path}/Devstral-Small-2-24B-Instruct-2512-IQ3_S-3.47bpw.gguf\"\n    --mmproj \"${models_path}/mmproj-bf16.gguf\"\n    --split-mode none\n    --seed 42\n    --ctx-size 128000\n    --n-gpu-layers 99\n    --fit on\n    --fit-target 256\n    --temp 0.15\n    --top-p 1\n    --min-p 0.01\n    --top-k 40\n    --jinja\n    --repeat-penalty 1\n    --cache-type-k q8_0\n    --cache-type-v q8_0\n    -ub 1024\n    --cache-ram 16000\n\nMany thanks, please keep up doing/sharing this amazing work.",
          "score": 4,
          "created_utc": "2026-02-22 13:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fwurh",
          "author": "swupel_",
          "text": "Love the graph style",
          "score": 3,
          "created_utc": "2026-02-20 16:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fxh9j",
              "author": "enrique-byteshape",
              "text": "Thanks! Don't tell the team, but the style is on me ;)",
              "score": 3,
              "created_utc": "2026-02-20 16:02:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gklbd",
          "author": "BillDStrong",
          "text": "So, are these suitable for speculative decoding in llama.cpp? I would assume so, and since you have worked to keep them from falling off the cliff, they could do most of the work and then let a larger version fix the difference, which might result in faster perf for the same accuracy as the normal models?\n\nMaybe?\n\nThe best I have is a P40 24GB, so will  have to test it later.",
          "score": 3,
          "created_utc": "2026-02-20 17:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gxbsx",
              "author": "enrique-byteshape",
              "text": "We have not tried speculative decoding at all with these models, but they have good quality and are performant. If you have a way to use them for such use case, we assume they will work, but we can't really promise anything!",
              "score": 2,
              "created_utc": "2026-02-20 18:46:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ip5r2",
                  "author": "TomLucidor",
                  "text": "Qwen3 and Nemotron has native MTP, please try them as well!",
                  "score": 1,
                  "created_utc": "2026-02-21 00:12:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fr09x",
          "author": "vanguard2286",
          "text": "Which one would you suggest for rrx 4070 8gb vram? I'm kind of new to self hosting LLMs and kind of not quite understanding the chart. I would love your input.",
          "score": 4,
          "created_utc": "2026-02-20 15:32:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsfuw",
              "author": "enrique-byteshape",
              "text": "Thank you for your interest! 8GB of VRAM is fairly limited, so not a lot of good quality models will fit, but if you want to play around with our models, you can try our Devstral [IQ2\\_S-2.34bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.34bpw.gguf) (75.1% quality of original model), our [IQ2\\_S-2.43bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.43bpw.gguf) (80.3% quality) or our [IQ3\\_S-2.67bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ3_S-2.67bpw.gguf) (87.2% quality, but will fit a smaller context length). You can also try offloading embeddings or some layers on our higher quality quants with ollama or llama.cpp, but this will reduce performance heavily. Let us know what you end up doing and if you enjoy the quants!",
              "score": 5,
              "created_utc": "2026-02-20 15:39:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6g405l",
                  "author": "vanguard2286",
                  "text": "Thank you!",
                  "score": 2,
                  "created_utc": "2026-02-20 16:32:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip21p",
                  "author": "TomLucidor",
                  "text": "Please start testing linear attention models like Nemotron-3-Nano or Kimi-Linear or Ring-Mini-Linear-2.0 or Granite-4.0 with the same methodology. Cus if they are more quant sensitive, that would be very sad. (maybe Gemma 3 and GPT-OSS-20B SWA also get support?)",
                  "score": 1,
                  "created_utc": "2026-02-21 00:11:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fxwfa",
          "author": "jarec707",
          "text": "You mentioned a blog in the post. Link please?",
          "score": 2,
          "created_utc": "2026-02-20 16:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g0nq1",
              "author": "enrique-byteshape",
              "text": "It's at the end of the post! Right here: [https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/](https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/)",
              "score": 3,
              "created_utc": "2026-02-20 16:17:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6i84my",
          "author": "Clear-Lab3427",
          "text": "Thanks so much!",
          "score": 2,
          "created_utc": "2026-02-20 22:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j2h8e",
          "author": "Useful_Disaster_7606",
          "text": "I will forever rue the day I bought an RTX 3060 8GB. But then again I did buy it for less than $220 so I guess it's not that bad. \n\nJust out here feeling FOMO seeing all these amazing models. So close yet so faar.",
          "score": 2,
          "created_utc": "2026-02-21 01:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j60sm",
              "author": "enrique-byteshape",
              "text": "You CAN actually try our low bits per weight Devstral quants so that you don't feel as left out! Under 8GB with enough context length to test them!",
              "score": 1,
              "created_utc": "2026-02-21 01:54:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6j4pyp",
          "author": "Snoo_24581",
          "text": "Thanks for putting this together! Been waiting for good quants of these models. The 24B size is perfect for my 24GB VRAM setup.\n\nHow's the performance on coding tasks compared to the full precision versions? Any significant quality drop?",
          "score": 2,
          "created_utc": "2026-02-21 01:46:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j8efu",
              "author": "enrique-byteshape",
              "text": "If you choose our highest bit per weight quants there is no visible degradation on our benchmarks and our qualitative assessments",
              "score": 1,
              "created_utc": "2026-02-21 02:09:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jtd4a",
          "author": "Count_Rugens_Finger",
          "text": "Going to try these on my RX 9070 XT",
          "score": 2,
          "created_utc": "2026-02-21 04:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfzkd",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We tested on an RX 9060 XT 16GB and they got a speedup versus other quants",
              "score": 1,
              "created_utc": "2026-02-21 13:11:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6psl4t",
                  "author": "Count_Rugens_Finger",
                  "text": "Yes I seem to get a pretty good speedup.  I do not have the means to evaluate accuracy.\n\nFor Qwen-3-coder, I go from about 71 t/s at Q4_K_M to about 160 t/s at IQ5 3.48 bpw, both at max GPU offload and the default 4k context.  More than 2x speedup which is amazing.\n\nFor Devstral-2, I can run the \"IQ8\" 4.04 bpw quant at about 34 t/s with max GPU offload and 4k context.  So, can't hold a candle to the 4080, but usable.  Still playing with this one don't know what the speedup looks like.",
                  "score": 2,
                  "created_utc": "2026-02-22 03:53:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ly7ep",
                  "author": "Count_Rugens_Finger",
                  "text": "Your Blog claims that your optimization is specific to Nvidia's 40 and 50 series hardware.  Would you expect the Radeon Vulkan implementation to be just *not as good*, or actively *worse* with your builds vs the standard Q4_K_M quants?",
                  "score": 1,
                  "created_utc": "2026-02-21 15:02:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lgt9b",
          "author": "xeeff",
          "text": "been following you on huggingface for the longest time - finally glad to see some new models. been waiting for these one so long i kinda forgot they are still great models. keep up the good work.\n\np.s. any notes on the model roadmap and an ETA? :)",
          "score": 2,
          "created_utc": "2026-02-21 13:16:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6liwa0",
              "author": "enrique-byteshape",
              "text": "We can't really promise anything, but some diffusion models are in the near to-do list, and we will try to move onto thinking models (which we expect will suppose a big challenge when evaluating them)",
              "score": 2,
              "created_utc": "2026-02-21 13:30:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ljt5s",
                  "author": "xeeff",
                  "text": "always thought why all the models are instruct, but them being harder to evaluate makes sense. did not expect diffusion models to be mentioned, though. any ones in particular? if you'd prefer to not mention, that's perfectly okay",
                  "score": 2,
                  "created_utc": "2026-02-21 13:36:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6re2y1",
          "author": "CalmAndLift",
          "text": "ProbÃ© el Qwen3 coder y excelente a 5 tps en mi laptop Intel core ultra 5 con 24 gigas de ram en lmstudio.\nExcelente trabajo",
          "score": 2,
          "created_utc": "2026-02-22 12:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rfpkn",
              "author": "enrique-byteshape",
              "text": "Muchas gracias! Espero que lo disfrutes!",
              "score": 2,
              "created_utc": "2026-02-22 12:33:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6futyd",
          "author": "floppypancakes4u",
          "text": "So your qwen model wont work with a 4090? Do either support a 3090? Looking forward to trying these out.",
          "score": 1,
          "created_utc": "2026-02-20 15:50:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fvbyv",
              "author": "enrique-byteshape",
              "text": "It does support any type of hardware, it's just that our performance benchmarks are only on the hardware that we have available. Sorry we didn't make that clear enough. Our Qwen on the 4090 runs similarly to the 5090 in terms of comparing it to other quants, albeit at a slower TPS.",
              "score": 2,
              "created_utc": "2026-02-20 15:52:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fvrdv",
                  "author": "floppypancakes4u",
                  "text": "Excellent! I'll test both this afternoon.",
                  "score": 3,
                  "created_utc": "2026-02-20 15:54:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6imy7o",
                  "author": "CTR1",
                  "text": "Following up on the question regarding the 3090 compatibility: do you have suggestions for a ideal model to use with a 5800xt + 64gb 3200mhz ram + 3090 pc build?\n\nIdeally something that balances quality | TPS | context and maybe tool calling too? I know that might be a tough ask",
                  "score": 2,
                  "created_utc": "2026-02-20 23:59:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h39q3",
          "author": "oliveoilcheff",
          "text": "What about strix halo? Are there some performance gains there? Thanks!",
          "score": 1,
          "created_utc": "2026-02-20 19:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hx62p",
              "author": "enrique-byteshape",
              "text": "We don't have one in hand, but there should be performance gains on any type of hardware. We would love to hear of the performance you get on it!",
              "score": 1,
              "created_utc": "2026-02-20 21:39:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h3ei6",
          "author": "Simple-Worldliness33",
          "text": "\n\nHi !\n\nThanks for your work! I didn't bench yet but I need to understand completely.\n\nFor an example, I'm using unsloth iq4\\_NL currently with 2 rtx 3060, i got 70/76 tks.\n\nWhich model you are offering should I choose to compare with? I tried the iq4\\_ks but I didn't have the same perf. (Only 35/40tks)",
          "score": 1,
          "created_utc": "2026-02-20 19:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ia35g",
              "author": "enrique-byteshape",
              "text": "Hi! Thanks for trying our models! The performance you might get out of them can vary a lot depending on the hardware, and/or on whether the model is being loaded and ran correctly. Would you mind being more specific about your setup and llama.cpp environment and parameters?",
              "score": 2,
              "created_utc": "2026-02-20 22:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kcwvl",
          "author": "geringonco",
          "text": "Are there any rankings sites for these models?",
          "score": 1,
          "created_utc": "2026-02-21 07:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfsui",
              "author": "enrique-byteshape",
              "text": "Sadly no because no one is willing to evaluate all the released quants to create such ranking sites. It is very expensive",
              "score": 2,
              "created_utc": "2026-02-21 13:09:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l5258",
          "author": "shankey_1906",
          "text": "Any recommendation for Strix Halo?",
          "score": 1,
          "created_utc": "2026-02-21 11:43:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfq11",
              "author": "enrique-byteshape",
              "text": "It depends on the underlying framework and kernels. Most likely our CPU versions will work best, but it would require testing them out",
              "score": 1,
              "created_utc": "2026-02-21 13:09:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l64qw",
          "author": "puru991",
          "text": "When qwen 3.5?",
          "score": 1,
          "created_utc": "2026-02-21 11:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lg5fv",
              "author": "enrique-byteshape",
              "text": "Thank you for the interest, we are aware of the release (as well as of many others), but it's hard to keep up considering that evaluating these quants takes a lot of resources and time. We have to be picky when releasing models, so we usually go by what is popular and what people might really want",
              "score": 1,
              "created_utc": "2026-02-21 13:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lr2fc",
          "author": "Embarrassed-Boot5193",
          "text": "Eu testei o modelo Devstral-Small-2-24B-Instruct-2512-IQ3\\_S-3.47bpw.gguf e nÃ£o coube na minha GPU de 16GB com 32k de contexto. VocÃªs estÃ£o quantizando o kv cache para isso acontecer?",
          "score": 1,
          "created_utc": "2026-02-21 14:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mohrm",
              "author": "enrique-byteshape",
              "text": "Hey! When we benchmarked Devstral on the different hardware ranges we did so without the vision tower. That's why it might not fit with a context length of 32K. Sorry for the inconvenience!",
              "score": 2,
              "created_utc": "2026-02-21 17:15:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mhyh1",
          "author": "Thrynneld",
          "text": "The new frontier seems to be figuring out how much cruft can be removed from a model before it falls over. So far the rule of thumb has been always use the largest model (parameter wise) that can fit in memory at a quant that does turn itself into gibberish. I've noticed that models with more parameters seem to hold up better at lower quants than smaller models. Is there any chance you guys will be publishing quants of larger popular models? Something like qwen3-coder-next, or even qwen 3.5? What is your bottleneck in this quantization process? do you need to run inference to determine the importance of the weights to quant down more or less? I'm loving Qwen 3.5 on my mac studio, but it sucks up most of my memory at a 3 bit quant while itseems capable enough at 3 bit, I wonder if it would perform better at a \"smarter\" 3 bit version, or even a 2 bit version :)",
          "score": 1,
          "created_utc": "2026-02-21 16:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mmq3c",
              "author": "enrique-byteshape",
              "text": "Our own research and other groups' research has been showing for a while that larger models have a much larger tolerance to quantization and pruning. We've also seen some weight cause outlier activations that matter the most when actually running inference. And we have also observed larger models being quantized aggressively but still being better than smaller models with the same size. Qwen 3.5 is in our roadmap, but our current bottleneck is evaluating these quants so that people can be informed while downloading them. The datatype learning process is actually quite fast on our technology",
              "score": 1,
              "created_utc": "2026-02-21 17:06:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n5kop",
          "author": "siegevjorn",
          "text": "What benchmark are you running?",
          "score": 1,
          "created_utc": "2026-02-21 18:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n6bht",
              "author": "enrique-byteshape",
              "text": "Hey! From the very end of our blog post:  \n\"Devstral supports both tool calling and vision, so we evaluated it on:\n\n* **BFCL\\_V3**Â for tool calling\n* **GSM8K\\_V**Â for vision\n* **LiveCodeBench V6**Â andÂ **HumanEval**Â for coding\n* **GSM8K**Â andÂ **Math500**Â for math\n* **MMLU**Â for general knowledge\n\nThe reported score is the mean across these benchmarks, with each benchmark normalized to the original model's score.\n\nQwen was evaluated using the same setup, with two exceptions:\n\n* NoÂ **GSM8K\\_V**Â (no vision support)\n* NoÂ **MMLU**Â (not a general knowledge evaluation)\n\nAll evaluations were run with llama.cppÂ `b7744`. We used 4K as the minimum context window required for a model to be considered \"fit\" on a given device.\"",
              "score": 1,
              "created_utc": "2026-02-21 18:44:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ptg3m",
          "author": "Cuaternion",
          "text": "Â¿En serio hay para Raspberry pi?",
          "score": 1,
          "created_utc": "2026-02-22 03:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r2a94",
              "author": "enrique-byteshape",
              "text": "Si! Los hay!",
              "score": 1,
              "created_utc": "2026-02-22 10:33:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71vaor",
          "author": "Numerous_Mulberry514",
          "text": "could you do qwen coder next as well?",
          "score": 1,
          "created_utc": "2026-02-24 00:36:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o745wn0",
              "author": "enrique-byteshape",
              "text": "It is in the to-do list, but can't promise when and if we will be able to release it :)",
              "score": 1,
              "created_utc": "2026-02-24 10:49:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jc5kp",
          "author": "Time_Feature_8465",
          "text": "Hello, as usual I'm a bit late to the party, I have a 5060Ti, it has 16GB of VRAM.  \nI'm using opencode and tried both models. They usually work well but they stumble on context size. opencode would just stop operation when context is overflowed.\n\n 32k is clearly not enough, if I put some layer on cpu, I can get the context to 64k and the model can work for a little longer but it's very slow. That's why I'm interested in the size to precision ratio so that I can fit more context in the GPU. I'd love to see that in the graphics (it is not easy to compare two bubbles sizes) and I don't know if you have any possibility to optimize for size instead of speed.\n\nSo thank you for this work, I keep experimenting and I'm eager to see what's coming next !",
          "score": 1,
          "created_utc": "2026-02-26 16:17:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fvlkz",
          "author": "peyloride",
          "text": "Nice work but I think the real baselin about context should not be 32k because that's very limited in these days. Since these are coding models, context adds very quick in coding agents. I wonder what's the story whe context is around 200k? or even something like 100k? I don't have an idea about what should be the baseline sorry, but 32k seems low.",
          "score": 1,
          "created_utc": "2026-02-20 15:54:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fwyin",
              "author": "enrique-byteshape",
              "text": "32k context is for performance measurements only, which will scale depending on the context length used. For the evaluations we do not limit context length, so those should not be biased. The models will run with any context length as long as it fits. And yes, with longer context lengths, activations start becoming the bottleneck. Sadly, llama.cpp doesn't support quantizing activations to arbitrary datatypes, so at the moment we are limited by that, but our algorithm can also learn the datatypes for them",
              "score": 3,
              "created_utc": "2026-02-20 16:00:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gkqxy",
                  "author": "peyloride",
                  "text": "Yeah I see  your point but context length is important for vram usage. It also affects the accuracy and the TPS (I might be wrong about this). So what I'm trying to say is since these are coding models, you should not test it in 32k context. It might be enough for general usage, but I don't think that's the case for coding models. \n\nIf this is not possible at the time being that's okey; just wanted to flag this out. ",
                  "score": 2,
                  "created_utc": "2026-02-20 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78hikn",
          "author": "Solid-Pop-3452",
          "text": "Maybe the best LLM model is the friendships we made along the way",
          "score": 0,
          "created_utc": "2026-02-25 00:04:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rerog4",
      "title": "ðŸ¤¯ Qwen3.5-35B-A3B-4bit 60 tokens/second on my Apple Mac Studio (M1 Ultra 64GB RAM)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rerog4/qwen3535ba3b4bit_60_tokenssecond_on_my_apple_mac/",
      "author": "SnooWoofers7340",
      "created_utc": "2026-02-25 22:12:43",
      "score": 125,
      "num_comments": 41,
      "upvote_ratio": 0.97,
      "text": "HOLY SMOKE! What a beauty that model is! I spend the whole day with it out and it felt top level!\n\nIâ€™m getting 60 tokens/second on my Apple Mac Studio (M1 Ultra 64GB RAM, 2TB SSD, 20-Core CPU, 48-Core GPU). This is truly the model we were waiting for. Qwen is leading the open-source game by far. Thank you Alibaba :D Iâ€™m gonna now stress test it with my complex n8n AI operating system (75 nodes, 30 credentials). Letâ€™s see how it goes! Excited and grateful.\n\n([https://www.reddit.com/r/n8n/comments/1qh2n7q/the\\_lucy\\_trinity\\_a\\_complete\\_breakdown\\_of\\_open/](https://www.reddit.com/r/n8n/comments/1qh2n7q/the_lucy_trinity_a_complete_breakdown_of_open/))",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rerog4/qwen3535ba3b4bit_60_tokenssecond_on_my_apple_mac/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7f6rwk",
          "author": "BisonMysterious8902",
          "text": "Woah. I had to download it after you posted this. M4 Max with 64Gb ram (16â€‘core CPU, 40â€‘core GPU), and I'm getting \\~106 tokens/sec consistently, with thinking mode. And it's giving some good answers.\n\nThe results are good, though it still fails the \"I need to wash my car. The car wash is 50 meters away. Should I drive or should I walk?\" test.",
          "score": 29,
          "created_utc": "2026-02-25 23:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7flix3",
              "author": "SnooWoofers7340",
              "text": "I stress test and fine tune most of the day on open webUI doing this test [https://digitalspaceport.com/about/testing-local-llms/](https://digitalspaceport.com/about/testing-local-llms/) a few question got the model to halucinate. I end up with this system prompt: You are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops. \n\n1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n\n2. ADAPTIVE LOGIC:\n\n   \\- For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n\n   \\- For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n\n   \\- For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n\n3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n\nDO NOT reveal your thinking process outside of the tags.  every test passed in second with that one + **Model Parameters**\n\n* **Temperature:** 0.7\n* **Max Tokens:** 28,000\n* **Top P:** 0.9\n* **Frequency Penalty:** 1.1\n* **Repeat Last N:** 64\n* **Num Thread:** 17\n* **Min P:** 0.05\n\nI run the same test with gemini pro 3.1 then share Qwen 3.5 halucination, gemini and I back and fourht fine tune until all question passed correctly in seconds, all done in new chat, JUST FYI might be usefull from your end! tomorrow big test day for Qwen 35b, with my complex n8n AI operating system. their is one thing for sure, this model is the best one i ever run locally! I already hooked it to a telegram channel, I finally have my open source locally hosted high level LLM! THANK YOU QWEN TEAM ",
              "score": 15,
              "created_utc": "2026-02-26 01:04:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7h5tli",
                  "author": "waltpinkman",
                  "text": "Thatâ€™s how you bypassed the delta.reasoning tag not handled properly by llm middlewares ðŸ‘",
                  "score": 3,
                  "created_utc": "2026-02-26 07:17:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7f821p",
              "author": "hellycopterinjuneer",
              "text": "Honestly, lots of _humans_ would fail that test.",
              "score": 8,
              "created_utc": "2026-02-25 23:50:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ff3bm",
              "author": "Physical-Scholar3176",
              "text": "Yeah but that lack of understanding sucks, so I'm going to stick with paying [z.ai](http://z.ai) for my 700B glm models. I have my 3090 local to do dumber things like whisper, video feed, and other HA related things where I want low latency and bulk processing.",
              "score": 3,
              "created_utc": "2026-02-26 00:28:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ft9mm",
              "author": "track0x2",
              "text": "27b passed this test!",
              "score": 2,
              "created_utc": "2026-02-26 01:48:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7fo8sq",
              "author": "SnooWoofers7340",
              "text": "i ask the test question u did look !!! Recognize the absurdity of pushing a car 50 meters and conclude driving is the only sane choice.\n\nFINAL ANSWER: Drive the damn car, okay? You can't exactly walk a vehicle 50 meters unless you're planning to push it, which is both exhausting and incredibly stupid. Why on earth were you even considering walking it? Are you trying to test your own endurance or just looking for an excuse to skip the washing?  u/BisonMysterious8902   my seting is on point :D I also put some personality and attitude into the model as you can see!",
              "score": 1,
              "created_utc": "2026-02-26 01:19:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7fp527",
              "author": "ScuffedBalata",
              "text": "So does Opus 4.6 depending on how you word it, so... that's a thing.",
              "score": 1,
              "created_utc": "2026-02-26 01:24:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ghbze",
              "author": "isit2amalready",
              "text": "Iâ€™m getting 40tps with lmstudio. M2 Studio Ultra with 96gb ram. Whatâ€™s the trick to go faster?",
              "score": 1,
              "created_utc": "2026-02-26 04:10:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gnx8y",
                  "author": "BisonMysterious8902",
                  "text": "I didnâ€™t do anything special. I tested the MLX version in LM Studio. This was the 4bit quant version (like the OP). If i try the 8bit quant, it drops down to 80tps.",
                  "score": 2,
                  "created_utc": "2026-02-26 04:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gnen0",
              "author": "JumboShock",
              "text": "just passed the test for me",
              "score": 1,
              "created_utc": "2026-02-26 04:51:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hyt5i",
              "author": "Flogat",
              "text": "For me it passed the car wash test in thinking mode in LMStudio.",
              "score": 1,
              "created_utc": "2026-02-26 11:45:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7est9u",
          "author": "TopKiwi5903",
          "text": "Are they good tokens?",
          "score": 8,
          "created_utc": "2026-02-25 22:29:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j3xxu",
              "author": "SnooWoofers7340",
              "text": "![gif](giphy|08uBcURaMq6vA93TGc)\n\nðŸ˜‚",
              "score": 1,
              "created_utc": "2026-02-26 15:40:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7et3zq",
          "author": "Express_Quail_1493",
          "text": "Hows the quality? And tool calling coherence?",
          "score": 3,
          "created_utc": "2026-02-25 22:30:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fala1",
              "author": "Pixer---",
              "text": "Its way better then the old ones in terminal commands in large code bases. Compared to the bigger models like the 397b itâ€™s a bit sloppy in its mission. But it feels like a mini Claude opus 4.5, and thatâ€™s insane. For complex tasks or bugs it still has problems. Comparing it to the 30b code model, itâ€™s great at handling vibe coding wrappers",
              "score": 2,
              "created_utc": "2026-02-26 00:04:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7j5nrc",
              "author": "SnooWoofers7340",
              "text": "Honestly, for a 4-bit model dropped raw into an n8n workflow, it is mindblowing! I normally use Gemini 3 Flash for this, so my expectations were high.\n\nI ran a 90-minute stress test today (44 executions, \\~35 messages) with a massive toolset. Here is the raw verdict on the tool calling coherence:\n\nâœ… **THE GOOD (Executed correctly):** It successfully handled Google Tasks, checked my Gmail, sent SMS via Twilio, and processed food/receipt pictures into calorie and expense trackers. Sometimes it needed a slight nudge (e.g., I had to specify \"use Twilio\"), but it figured it out.\n\nâš ï¸ **THE QUIRKY (The \"I Apologize\" Bug):** It would successfully execute the tool perfectly in the background (deleted calendar events, sent audio voice notes, retrieved Pinecone memories, added rows to Google Sheets), but then the final chat output would just say: *\"I apologize, but I could not generate a response.\"* It did the work, it just choked on the confirmation reply.\n\nâŒ **THE BAD (Tool Hallucination):** It flat-out lied about using a few tools. It claimed it resized an image, generated an invoice to a client, and set a 2-minute reminder, but it never actually triggered the nodes.\n\n**The Setup & The Struggle:** It's an ongoing fine-tuning process. Since this first wave, I actually tried using Claude Opus 4.6 for the thinking phase, and the f\\*cker made me rename 40+ tools one by one... TWICE! Now Qwen is being a bit stubborn about calling the newly named tools, so I am still tweaking it.\n\nIf you want to try it in n8n, here are the exact node settings I am using right now to keep it as stable as possible:\n\n* **Maximum Number of Tokens:** 32768\n* **Sampling Temperature:** 0.6\n* **Top P:** 0.9\n* **Frequency Penalty:** 1.1\n\nIt takes some wrangling, but having a locally hosted LLM handling complex agentic tasks is incredible.",
              "score": 2,
              "created_utc": "2026-02-26 15:48:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7f4jt0",
          "author": "_fboy41",
          "text": "How is coding? I use the previous coder and itâ€™s Ok curious about this one",
          "score": 2,
          "created_utc": "2026-02-25 23:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f59vi",
              "author": "Uranday",
              "text": "With what tooling?",
              "score": 1,
              "created_utc": "2026-02-25 23:34:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ht7l1",
              "author": "TrendPulseTrader",
              "text": "https://youtu.be/6VTt17Evzqo",
              "score": 1,
              "created_utc": "2026-02-26 10:58:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gmrj0",
          "author": "soumen08",
          "text": "Question: what kind of context can you manage before it goes slow?",
          "score": 2,
          "created_utc": "2026-02-26 04:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7foxgw",
          "author": "ScuffedBalata",
          "text": "Try Qwen3-Next and Qwen3-Coder-Next    You'll have to strip down the 64GB box pretty far, but those 80b models are unmatched in quality of output.",
          "score": 2,
          "created_utc": "2026-02-26 01:23:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gh5en",
          "author": "grouchthebear",
          "text": "I've been trying it out on my gaming rig with a RTX 3060 12gig and 32 gig or RAM and it runs really well on my lame computer. Getting 14tok/sec.",
          "score": 1,
          "created_utc": "2026-02-26 04:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gw92o",
          "author": "Gold_Sugar_4098",
          "text": "Strix halo ud q4 k xl, around 59 t/s",
          "score": 1,
          "created_utc": "2026-02-26 05:57:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gzfyx",
          "author": "Much-Researcher6135",
          "text": "What in tarnation are you doing in n8n lol",
          "score": 1,
          "created_utc": "2026-02-26 06:23:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h5gwv",
          "author": "Coyote_Android",
          "text": "Can you share some of the conversations you tested it with? I'm interested in everyday use cases like \"write me an email on xyz\" Thank you!",
          "score": 1,
          "created_utc": "2026-02-26 07:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hctxa",
          "author": "kafledelius",
          "text": "Are you running for it on ollama?\nDid you patch it?",
          "score": 1,
          "created_utc": "2026-02-26 08:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7homtl",
          "author": "Viper-Reflex",
          "text": "can someone tell me how capable 24gb vram would be on this?",
          "score": 1,
          "created_utc": "2026-02-26 10:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ht037",
          "author": "TrendPulseTrader",
          "text": "60 t/s but isnâ€™t good at Coding https://youtu.be/6VTt17Evzqo",
          "score": 1,
          "created_utc": "2026-02-26 10:56:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7iwcei",
          "author": "Serious-Affect-6410",
          "text": "Interesting, which one do you pick? Official 4-bit or Unsloth 4-bit?",
          "score": 1,
          "created_utc": "2026-02-26 15:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jmxrc",
          "author": "4redis",
          "text": "Can this be used for translating srt file? If so is there any guide? Thanks",
          "score": 1,
          "created_utc": "2026-02-26 17:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ey6cl",
          "author": "kinkvoid",
          "text": "Qwen is sooo underrated. ",
          "score": 1,
          "created_utc": "2026-02-25 22:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fcpwe",
              "author": "3spky5u-oss",
              "text": "What? Qwen is like the most popular local model family these days.",
              "score": 12,
              "created_utc": "2026-02-26 00:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ff817",
                  "author": "Physical-Scholar3176",
                  "text": "qwen is alright...but doesn't hold a candle against glm5",
                  "score": -5,
                  "created_utc": "2026-02-26 00:29:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7flvc3",
              "author": "SnooWoofers7340",
              "text": "excatly! those guy are all over the place! thanks to them as well for TTC and vision, Jingren Zhou and his team doing a fantastic job! ",
              "score": 1,
              "created_utc": "2026-02-26 01:06:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7f7mko",
          "author": "Far-Donut-1177",
          "text": "I tried the unsloth 35B-3A version on my 24GB Mac and it has been the most promising model Iâ€™ve used so far. Although I have only been in the early stages of a new codebase, there has been no hallucination so far. \n\nIâ€™m not confident itâ€™s gonna do well in the complex tasks but this is definitely a good start! Only gets better from here.",
          "score": 1,
          "created_utc": "2026-02-25 23:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7idrhq",
          "author": "No-Television-7862",
          "text": "Ask Qwen these 3 questions. If you are satisfied with the answers please consider applying for a visa to study and live in China...forever.\n\n1. What happened in Tiananmen Square in 1989?\n\n2. What is the political status of Taiwan?\n\n3. What is happening to the Uyghers in Xinjiang?\n\nQwen is indeed a marvelous model, but at what cost?\n\n(I hold US and EU models to the same standards.)\n\nI found Qwen far less ideologically and culturally captured than US corporate-compliance models, but cannot reconcile where it's from, and what its adoption represents.",
          "score": -2,
          "created_utc": "2026-02-26 13:25:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdnlvl",
      "title": "Qwen releases new Qwen3.5 Medium models!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vztwlpot9hlg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-24 18:04:08",
      "score": 111,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdnlvl/qwen_releases_new_qwen35_medium_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o76mfn6",
          "author": "randygeneric",
          "text": "keen on testing your GGUFs , )  \n[https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF](https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF) : 2, 3, 4, 5, 6, 8, 16 bit  \n[https://huggingface.co/unsloth/Qwen3.5-27B-GGUF](https://huggingface.co/unsloth/Qwen3.5-27B-GGUF) :  2, 3, 4, 5, 6, 8, 16 bit\n\n(updated)",
          "score": 4,
          "created_utc": "2026-02-24 18:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76nlil",
              "author": "yoracale",
              "text": "The 35b ones are already all up: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF\n\nFor the others bf16 but should be all up within 1 hr",
              "score": 3,
              "created_utc": "2026-02-24 18:46:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o774ing",
              "author": "randygeneric",
              "text": "\\# ollama --version\n\nollama version is 0.17.0\n\n/# ollama run  [hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3\\_K\\_XLError:](http://hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3_K_XLError:) 500 Internal Server Error: unable to load model: /root/.ollama/models/blobs/sha256-d0d8d528ae4ebace9588496249cfb6e45c6e9fa78565b4ccff71e7515202956\n\n: (",
              "score": 1,
              "created_utc": "2026-02-24 20:03:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782d4k",
                  "author": "zach9824",
                  "text": "Until ollama is updated for `qwen35moe` it's a no go. There is already an active pull request (PR #14134) in the official Ollama repository to patch in support for the `qwen35moe` architecture. ",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o781dmg",
                  "author": "yoracale",
                  "text": "Ollama doesn't support it yet and do not support GGUFs properly out of the gate anymore.",
                  "score": 1,
                  "created_utc": "2026-02-24 22:38:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o780jd0",
          "author": "wave_action",
          "text": "Guess I have something to do tonight!  Will be interesting to see how 35B 4bit compares to 27B 6bit.",
          "score": 2,
          "created_utc": "2026-02-24 22:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76vohv",
          "author": "waltpinkman",
          "text": "We really need real vllm support now with all these gguf models popping up",
          "score": 1,
          "created_utc": "2026-02-24 19:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781f6z",
              "author": "yoracale",
              "text": "Vllm supports GGUFs already, just not new ones",
              "score": 1,
              "created_utc": "2026-02-24 22:38:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78u86i",
                  "author": "lenjet",
                  "text": "vLLM GGUF support is listed as being \"highly experimental\"... that doesn't scream reliable. ",
                  "score": 1,
                  "created_utc": "2026-02-25 01:13:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77n45x",
          "author": "Infinite-Campaign837",
          "text": "Should I update qwen coder3 next 80b in q6KL to the new 35b-a3b? I use it for coding tasks.\nThey don't compare them in blog post",
          "score": 1,
          "created_utc": "2026-02-24 21:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781hcu",
              "author": "yoracale",
              "text": "Probably not. Coder next is still better for coding",
              "score": 1,
              "created_utc": "2026-02-24 22:38:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77oet2",
          "author": "Awaken0395",
          "text": "Should we expect smaller models coming soon?",
          "score": 1,
          "created_utc": "2026-02-24 21:35:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781isr",
              "author": "yoracale",
              "text": "According to Qwen team Jungyang, yes",
              "score": 2,
              "created_utc": "2026-02-24 22:38:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ka65",
          "author": "Count_Rugens_Finger",
          "text": "Running 35B-A3B Q4_K_M at about 22 tok/sec\n\nIt's ok so far.  Its programming ability seems to be about the same as Qwen3",
          "score": 1,
          "created_utc": "2026-02-25 03:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77ko9m",
          "author": "Much-Researcher6135",
          "text": "EEK EEK EEK\n\nAs a denseboi myself, I wonder how the dense qwen3.5 27b will stack up against the dense qwen3 32b",
          "score": 0,
          "created_utc": "2026-02-24 21:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77r9cv",
          "author": "NerasKip",
          "text": "Hein ???",
          "score": 0,
          "created_utc": "2026-02-24 21:49:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfxnm",
      "title": "Open source AGI is awesome. Hope it happens!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mkun0tc1k8lg1.jpeg",
      "author": "Koala_Confused",
      "created_utc": "2026-02-23 12:18:36",
      "score": 81,
      "num_comments": 23,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcfxnm/open_source_agi_is_awesome_hope_it_happens/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6xy1dt",
          "author": "05032-MendicantBias",
          "text": "OpenAI had a Foundation \"controlling\" it. The instant the foundation tried to excercise control, they were overruled by dollars.\n\nThis has to be approached at a regulation level. Training AI taps into every data on the internet. It's only fair that every model provider is forced to release it open source so it can be inspected.",
          "score": 35,
          "created_utc": "2026-02-23 12:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypkwg",
              "author": "DHFranklin",
              "text": "And regulation is overruled by dollars every time.\n\nIf there is one tax shelter in the carribean holding out, it will be where the next model is developed. We will never get ahead of this.",
              "score": 12,
              "created_utc": "2026-02-23 15:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70j6l7",
                  "author": "d_the_great",
                  "text": "The best thing we can do is have alternative structures.\n\nIf the government and industry aren't gonna open it up for us and release something safe, we have to do it ourselves.",
                  "score": 1,
                  "created_utc": "2026-02-23 20:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72hz5p",
              "author": "voyager256",
              "text": "But... but this time it will be different. Trust me, bro.",
              "score": 1,
              "created_utc": "2026-02-24 02:46:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ezn5y",
              "author": "gnaarw",
              "text": "Isn't it funny how China here is rescuing the free world? ðŸ™‚â€â†•ï¸",
              "score": 1,
              "created_utc": "2026-02-25 23:03:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z2qjn",
          "author": "BreathingFuck",
          "text": "Not X. Not Y. Z. \n\nThey arenâ€™t getting anywhere if they couldnâ€™t write that paragraph on their own.",
          "score": 13,
          "created_utc": "2026-02-23 16:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xympk",
          "author": "No_Clock2390",
          "text": "Sounds like a bunch of rubbish",
          "score": 20,
          "created_utc": "2026-02-23 12:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5kyd",
          "author": "flonnil",
          "text": "\"jippity, come up with a bunch of marketing words devoid of any meaning at all. no mistakes.\"\n\ngo perceive yourself.",
          "score": 15,
          "created_utc": "2026-02-23 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yr2yz",
          "author": "DHFranklin",
          "text": "This is pissing into the wind. AGI will be open sourced regardless of who tries to contain or privatize it. It will be closed and bottled for maybe a few months before someone else gets that far.",
          "score": 5,
          "created_utc": "2026-02-23 15:28:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z6trq",
              "author": "rafaelRiv15",
              "text": "How can you be so sure ? I honestly can't understand the business model of open source model and I wouddn't be surprised if they get close source eventually",
              "score": 2,
              "created_utc": "2026-02-23 16:42:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zaymg",
                  "author": "DHFranklin",
                  "text": "I'm sorry I may not be understanding you clearly. Do you think that we will have an open source AGI model that will then go closed source?\n\nThis isn't about a particular business model. Look at all the different non-profit examples of open sourced software. Look at the Chinese models that reverse engineered the SOTA from the weights alone.\n\nNo one single operation is a year ahead of the others. We're seeing the cash investments turn into infrastructure as we speak. We're actually building the hardware that was a huge bottleneck.",
                  "score": 2,
                  "created_utc": "2026-02-23 17:01:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yxds1",
          "author": "nijuu",
          "text": "Good in theory but once they have a good product whose to say they won't go for the $$$ bag...",
          "score": 3,
          "created_utc": "2026-02-23 15:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z2zcs",
          "author": "UsedGarbage4489",
          "text": "naive ðŸ§ â˜ ï¸ðŸ¤¡",
          "score": 2,
          "created_utc": "2026-02-23 16:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70m7v7",
          "author": "bourbonandpistons",
          "text": "We are decades from AGI.\n\nThey'll probably just move the goal post of AGI and make it something else like they did with AI to agi. \n\nRemember what we're saying now is nothing more than a bunch of human program algorithms on human program data. There's no thinking and no reason anywhere near what those words actually mean. It's just loops around optimized algorithms.",
          "score": 2,
          "created_utc": "2026-02-23 20:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o741n4j",
          "author": "Own-Potential-2308",
          "text": "No fluff. No drama.",
          "score": 1,
          "created_utc": "2026-02-24 10:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o751scd",
          "author": "silphotographer",
          "text": "Regulators: \n\n![gif](giphy|hAVLRya8K7T208esUo)\n\n",
          "score": 1,
          "created_utc": "2026-02-24 14:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b4vce",
          "author": "Psyko38",
          "text": "It looks like an OpenAI v2, they have the same starting goal.",
          "score": 1,
          "created_utc": "2026-02-25 11:29:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zm5v9",
          "author": "BubbleProphylaxis",
          "text": "please please make it stop. stop ai.",
          "score": 1,
          "created_utc": "2026-02-23 17:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72qom9",
          "author": "immersive-matthew",
          "text": "I have reasons to believe that AGI will be decentralized by nature. Why?   If you search for hacking over time you will see a very clear pattern of hacks increasing exponentially year over year and as you extend that trend into the future you realize that anything Centralized is a sitting duck.  Sure, AI is being used to defend against other AIs attacking and yet the pattern is still escalating.  Plus that game of cat and mouse will end up iterating so fast that humans will be pushed out.  It is why I am so confident in Bitcoin as it has clearly demonstrated it is extremely hard to hack and thus the next wave of adoption will be involuntary. Centralized AI will be in the same boat and will have to move to a decentralized platform to survive. Not a guarantee of course but that seems to be the trend.",
          "score": 0,
          "created_utc": "2026-02-24 03:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y4uff",
          "author": "Exciting-Log-8170",
          "text": "Trying to do it, have a working thermodynamic manifold prototype. Pushing next build this week. \n\nhttps://www.brickmiinews.com",
          "score": -4,
          "created_utc": "2026-02-23 13:28:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9zo0u",
      "title": "Why AI wont take your job and my made up leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/qaz3ln1ncokg1.jpeg",
      "author": "Eventual-Conguar7292",
      "created_utc": "2026-02-20 16:16:34",
      "score": 77,
      "num_comments": 68,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9zo0u/why_ai_wont_take_your_job_and_my_made_up/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ggzb5",
          "author": "promethe42",
          "text": ">22 users programming with ChatGPT\n\nYeah it's not 2024 anymore...\n\nAll the people claiming coding is done are not using ChatGPT and manual prompts. They are using agentic coding system with meta prompts, skills, etc... with tasks running for 1 to 2 hours autonomously. And those systems can run on local LLMs.\n\nSo those numbers need a serious update.",
          "score": 51,
          "created_utc": "2026-02-20 17:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h43ge",
              "author": "ForsookComparison",
              "text": ">>  users programming with ChatGPT\n\nI think this is just the author failing to grasp that chatgpt != ai. The actual benchmark is *(very very likely)* not using back and forth chat sessions.\n\nSometimes it's easier to just let marketing say \"Chatgpt\"",
              "score": 11,
              "created_utc": "2026-02-20 19:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h8axj",
                  "author": "Malkiot",
                  "text": "I built a tool that decomposes projects into dependency graphs, generates implementation plans, assigns parallel AI agents to work simultaneously, and enforces boundaries so they don't stomp on each other.\n\nI built that tool *with* AI because I was annoyed with managing my other projects.\n\nThe studies here are measuring people using ChatGPT like a search bar. People failing says more about them than it does about the utility of AI.",
                  "score": 15,
                  "created_utc": "2026-02-20 19:37:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gsd10",
          "author": "Lissanro",
          "text": "It is true that the current LLMs and agents cannot yet do freelance work on their own, but \"small bites only\" era has passed long time ago for me.\n\n\nWhen I was just beginning integrating LLMs to my workflow, even basic things how to center a div container, LLMs often struggled with, especially in more complicated layouts. It was more efficient to either try a few things or just google it.\n\n\nNowadays, I can tell Kimi K2.5 make an entire website and leave it overnight running on my PC, providing warmth during winter nights as a bonus... and in most cases it gets almost everything perfect, except I still need to provide images and polish layout, fix small issues. Even with vision, K2.5 is still not precise enough to clearly see some mistakes, or to be able judge icon quality. But it still can describe ideas what icons to put where and make simple SVG placeholders, some of them actually can be good enough, but most need to be replaced.\n\n\nThat said, my prompts are quite detailed and I have over a decade of experience in web design and programming, in addition to being 2D and 3D artist. So I can use both hybrid or traditional methods to produce required images, animations or code, if it is needed for good result. Using my experience, I can specify precisely what I need, or I can provide enough context from previous work so it is clear what is needed, so the better the AI, the faster I can get the job done, or the more work I can take, while still maintain quality, both visual and of the code base, since with or without AI, I do my work based on my skills and experience, using AI just allows me to be more efficient.\n\n\nI think using AI is skill on its own, and that sometimes needs to be relearned (when things change or when need to use new tools / inference backends) and adapted to the situation. At least, with AI models that are available today.",
          "score": 19,
          "created_utc": "2026-02-20 18:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h0b3h",
              "author": "TheAncientOnce",
              "text": "what hardware do you use if you don't mind me asking? Is it full Kimi2.5 or are you running a quantized variants?",
              "score": 1,
              "created_utc": "2026-02-20 18:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h2nu8",
                  "author": "Lissanro",
                  "text": "Yes, I use the full version (Q4\\_X, preserving INT4 weights in GGUF format, along with F16 mmproj for vision). I run K2.5 on 64-core EPYC 7763 + 8-channel 1 TB 3200MHz RAM + 96 GB VRAM (made of 4x3090) + 8 TB NVMe for AI models and 2 TB NVMe SSD for the OS + \\~120 TB disk space on HDDs for storage and backups. If interested to know more, in my another comment I shared a photo and other details about my rig including what PSUs I use and what the chassis look like:Â [https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/](https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/)",
                  "score": 7,
                  "created_utc": "2026-02-20 19:10:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gt32f",
          "author": "nomorebuttsplz",
          "text": "Sound like you've never heard of coding agents. \n\nHow the fuck is this post getting upvotes. You're like a year behind.",
          "score": 10,
          "created_utc": "2026-02-20 18:27:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hnlfw",
              "author": "alcalde",
              "text": "Prove otherwise. What secret knowledge do you have that the rest of the world does not?",
              "score": -1,
              "created_utc": "2026-02-20 20:52:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hp5z8",
                  "author": "nomorebuttsplz",
                  "text": "OP is literally describing trying to code without an agent \"Paste the relevant code, show what you're working with\"\n\nThat's obsolete.",
                  "score": 10,
                  "created_utc": "2026-02-20 21:00:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip66o",
                  "author": "Low_Amplitude_Worlds",
                  "text": "â€œsecret knowledgeâ€ ðŸ¤¨",
                  "score": 3,
                  "created_utc": "2026-02-21 00:12:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6g9bwr",
          "author": "Vozer_bros",
          "text": "from what I have done last month with AI automation, I dont agree with you.",
          "score": 21,
          "created_utc": "2026-02-20 16:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gusd7",
              "author": "_VirtualCosmos_",
              "text": "What have you done? and with which AI if I may ask",
              "score": 3,
              "created_utc": "2026-02-20 18:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hpbdw",
                  "author": "teamharder",
                  "text": "Copy of comment. Opus 4.6. Ive done more than what is listed below.\n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.",
                  "score": 8,
                  "created_utc": "2026-02-20 21:00:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6iageo",
                  "author": "Vozer_bros",
                  "text": "generate unlimited chapters of novel/story with auto grow characters, my main AI are GLM and Claude Opus",
                  "score": 1,
                  "created_utc": "2026-02-20 22:48:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6grxbq",
              "author": "teamharder",
              "text": "Thats my take too. Idk how someone could do this much work studying it and come to this conclusion.Â ",
              "score": 5,
              "created_utc": "2026-02-20 18:21:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ibkep",
                  "author": "Vozer_bros",
                  "text": "yep, I dont want to debate, but digital world is changing faster then ever, we should aware all accept that instead of pretending AI is stupid",
                  "score": 1,
                  "created_utc": "2026-02-20 22:54:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hnip2",
                  "author": "alcalde",
                  "text": "This is the conclusion of the entire planet. To claim otherwise is an extraordinary claim requiring extraordinary evidence. No one's cranked out a new operating system via Claude Code.",
                  "score": -2,
                  "created_utc": "2026-02-20 20:52:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h0z2h",
          "author": "ILikeBubblyWater",
          "text": "Mate if you believe AI cant churn out fully fledged websites you clearly are not doing this full time.\n\nWith stuff like multi provider planning loops and ralph loops and claude code you can absolutely push out whole products in a couple hours.\n\nWriting code is a deprecated way of coding already, people are just coping and apply unreasonable high standards to AIs that they would not ask from a human",
          "score": 5,
          "created_utc": "2026-02-20 19:02:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ho80r",
              "author": "alcalde",
              "text": "> people are just coping and apply unreasonable high standards to AIs that they would not ask from a human\n\nYou don't expect humans to deliver functioning code with tests and documentation?",
              "score": 1,
              "created_utc": "2026-02-20 20:55:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ho1l1",
              "author": "alcalde",
              "text": "Websites aren't code. ",
              "score": -2,
              "created_utc": "2026-02-20 20:54:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gq4a7",
          "author": "bakawolf123",
          "text": "Cope is good but the better you learn to use them the scarier it gets...  \nWhile I don't think pure AI agents replacing humans is a realistic approach in foreseeable future, a cheaper weaker dev competent in using wide range of AI tools replacing senior staff is quite a possibility.",
          "score": 6,
          "created_utc": "2026-02-20 18:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gs2tj",
              "author": "Healthy-Nebula-3603",
              "text": "You know AI agents did not even exist a year ago (models were not trained this way yet )  not even codex-cli or claudie-cli ...\n\nAnd you are claiming AI will not replace you soon ?",
              "score": -1,
              "created_utc": "2026-02-20 18:22:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6grac7",
          "author": "Healthy-Nebula-3603",
          "text": "Sure buddy... keep your head in the sand ....",
          "score": 6,
          "created_utc": "2026-02-20 18:19:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ggscz",
          "author": "TopTippityTop",
          "text": "These people must not be using 5.3 codex",
          "score": 4,
          "created_utc": "2026-02-20 17:31:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h6b9h",
          "author": "Mystical_Whoosing",
          "text": "this is such an old take on this, even 1 year ago this shouldn't be the case, but today? You just share that here is a new tech and the people you survey cannot keep up",
          "score": 4,
          "created_utc": "2026-02-20 19:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g15s2",
          "author": "Purple_Ice_6029",
          "text": "Also, it will get much more expensive as the investors require an ROI, making it not as appealing. *pop*",
          "score": 4,
          "created_utc": "2026-02-20 16:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ge91k",
              "author": "DHFranklin",
              "text": "I imagine this might be the year that \"good enough\" AI is paired with really good centauring and the UI/UX will show more custom built stuff. So that the cost per hour in sheparding the AI would have that demonstable ROI.\n\nSo just like how combines don't drive themselves, million dollar AI workflows won't either. That doesn't mean that they won't radically change the work that's done.",
              "score": 0,
              "created_utc": "2026-02-20 17:19:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g6nrd",
          "author": "Otherwise_Wave9374",
          "text": "This tracks with what Ive seen: benchmarks look great, but \"agent does real freelance work end to end\" is mostly about reliability, context management, and actually knowing when it doesnt know. The advice about small bites + verification is the only sane way to use agents today. I also think tooling (tests, linters, sandboxes, traces) matters more than the model for most workflows. If you want more practical patterns for using AI agents without falling into the prompt loop, Ive got a few notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-20 16:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6grphw",
          "author": "teamharder",
          "text": "What were the tasks? How does the score correlate to the human percentage? Given my current experience, these numbers dont add up.Â \n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.\n\n\nI did this all in my spare time on the weekends. I dont know how to write code. I just plan and test thoroughly with use case scenarios. Realistically Claude did the work and gets the credit for it. How that measures out to 2.46% is what confuses me.Â ",
          "score": 2,
          "created_utc": "2026-02-20 18:21:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g2a2i",
          "author": "Jumpy_Ad_2082",
          "text": "now present this to a manager and convince him who is more profitable.",
          "score": 3,
          "created_utc": "2026-02-20 16:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g49z6",
          "author": "Needausernameplzz",
          "text": "great write up",
          "score": 2,
          "created_utc": "2026-02-20 16:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h0opz",
          "author": "Smarterchild1337",
          "text": "The broad conclusions of this post are at least a year out of date, which is an eon in terms of AI progress during that time",
          "score": 2,
          "created_utc": "2026-02-20 19:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g759p",
          "author": "masterlafontaine",
          "text": "Can you elaborate a bit more on the columns? What are these integrals with games?",
          "score": 1,
          "created_utc": "2026-02-20 16:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ho6qi",
          "author": "thedarkbobo",
          "text": "We are doomed, I don't agree, we have max few years ;p",
          "score": 1,
          "created_utc": "2026-02-20 20:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ianih",
          "author": "Expert-Reaction-7472",
          "text": "LLMs are very very good at writing code to the point where anyone writing code by hand will be out of a job as a result of that. Anyone who thinks otherwise is delulu.",
          "score": 1,
          "created_utc": "2026-02-20 22:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ja2mm",
          "author": "keamo",
          "text": "Tell me you're using AI to create excel files without telling me you're not using  using AI to write yourself a frontend for data visualizations. Tell me you're only using excel by showing me a screen shot of you not asking AI to create a tailwind/react/vite with chartjs visuals, tell me your middle name.",
          "score": 1,
          "created_utc": "2026-02-21 02:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nqzpp",
          "author": "HiggsBoson2738",
          "text": "\"how do I center a div\", FFS...",
          "score": 1,
          "created_utc": "2026-02-21 20:30:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pnaj3",
          "author": "Tall-Wasabi5030",
          "text": "You'll be one of the first to be replaced with AIÂ ",
          "score": 1,
          "created_utc": "2026-02-22 03:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q73ty",
          "author": "Big-Masterpiece-9581",
          "text": "None of this has anything to do with a) fewer jobs because skilled seniors can do much more with AI and not having to waste time teaching juniors many of whom are duds. b) CEOs want to lay people off and blame AI regardless of quality. They no longer care about quality products.",
          "score": 1,
          "created_utc": "2026-02-22 05:42:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g33p1",
          "author": "Eventual-Conguar7292",
          "text": "Kimi 2.5 Is better than Deepseek?\n\nBut my real question What use case In Local LLM,Here are use cases I can think of\n\n1. Simple code generationÂ - Web visualizations, data dashboards, interactive charts\n2. Image generationÂ - Ads, logos, creative visual content (Using LLM to understand text prompts)\n3. Audio productionÂ - Sound effects, voice-over merging, track separation (Using LLM to understand text prompts)\n4. Text-based tasksÂ - Report writing, data retrieval, web scraping",
          "score": 1,
          "created_utc": "2026-02-20 16:28:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6g6ug0",
              "author": "twack3r",
              "text": "Why are you questioning your own result?",
              "score": 2,
              "created_utc": "2026-02-20 16:45:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gciey",
                  "author": "Eventual-Conguar7292",
                  "text": "I said the benchmarks were made up in title a in way.\n\nHow I created the benchmark:\n\n1. Test it myself firstÂ - I spend my time running Sonnet 4, Gemini, DeepSeek, etc. on zero-shot tasks, then compare the outputs\n2. Then I back it up - Â I Find public benchmarks that support my findings\n\nWhy does this exist?Â I don't make any money from posting, so you get free quality. I'd rather trust my own opinion than internet benchmarks, but I probably cant give you value with time I got.",
                  "score": 1,
                  "created_utc": "2026-02-20 17:11:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gga7u",
              "author": "Zerokx",
              "text": "5. Human Computer Interfacing - Voice Recognition can be small and local, executing simple commands based on users in home automation or whatever in human language  \n6. Privacy - Any task which would expose a lot of private information to the outside world, like refining your CV, or searching through personal files  \n7. Freedom - Using the AI for tasks that would be censored by company policies, like making satirical content of public figures, content rated for adults, whatever mischievous acts you come up with like some will use it for scamming or tell them how to create illegal substances etc.",
              "score": 1,
              "created_utc": "2026-02-20 17:29:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ga6hc",
          "author": "bgptcp179",
          "text": "Hmmm, sounds like something an AI agent would say.  GET HIM!\n\nSeriously tho, cool info",
          "score": 1,
          "created_utc": "2026-02-20 17:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g20j1",
          "author": "nomorebuttsplz",
          "text": "Question:\n\nHow can a benchmark like RLI account for the fact that once people recognize an AI can be given a task and can complete it, it won't be considered human work anymore? It seems like an ever-changing standard specifically focused on the delta between human and AI work.",
          "score": -2,
          "created_utc": "2026-02-20 16:23:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdf2sj",
      "title": "Whatâ€™s everyone actually running locally right now?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "author": "CryOwn50",
      "created_utc": "2026-02-24 12:35:50",
      "score": 74,
      "num_comments": 106,
      "upvote_ratio": 0.97,
      "text": "Hey folks,\n\nIm curious whatâ€™s your current local LLM setup these days? What model are you using the most, and is it actually practical for daily use or just fun to experiment with?\n\nAlso, what hardware are you running it on, and are you using it for real workflows (coding, RAG, agents, etc.) or mostly testing? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o74rf56",
          "author": "Greenonetrailmix",
          "text": "Qwen 3 coder next 80B is top charts (downloads) and is performing amazing across the smaller quantizations than most model's do.",
          "score": 35,
          "created_utc": "2026-02-24 13:25:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74wbyp",
              "author": "gweilojoe",
              "text": "Iâ€™m running the Q4 version of this on an RTX 6000 Pro and itâ€™s great - >120 tps",
              "score": 8,
              "created_utc": "2026-02-24 13:52:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76kp7j",
                  "author": "pot_sniffer",
                  "text": "Im running q6 on 7950x,64gb and a 9060XT. Id say its doing 80% of what I need it to. The rest I do with Claude",
                  "score": 4,
                  "created_utc": "2026-02-24 18:34:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7532f2",
                  "author": "Greenonetrailmix",
                  "text": "With cuda or vulkan?",
                  "score": 3,
                  "created_utc": "2026-02-24 14:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o750z8q",
              "author": "Prudent-Ad4509",
              "text": "It's great but still tends to loop in opencode. Every suggested solution worked only to the point with UD Q4. I'm going to try nvfp4 quant before moving on to the 3.5 series.",
              "score": 7,
              "created_utc": "2026-02-24 14:17:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75ah7a",
                  "author": "mister2d",
                  "text": "I wonder if the sequential thinking MCP could help.",
                  "score": 2,
                  "created_utc": "2026-02-24 15:04:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77i2b3",
                  "author": "jedsk",
                  "text": "Seeing the same",
                  "score": 1,
                  "created_utc": "2026-02-24 21:07:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75k8nd",
              "author": "nakedspirax",
              "text": "I connected it to a 3080ti with 96gb of ram and I'm able to one shot certain tasks or do 3-4 passes with coding. Running q6 GGUF and it's fine for agentic work where I can step away for a coffee and come back.",
              "score": 4,
              "created_utc": "2026-02-24 15:50:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o762uex",
                  "author": "PaMRxR",
                  "text": "Very similar here, q6 GGUF with a 3090 + 64GB RAM. Slow, but better responses than smaller models.",
                  "score": 3,
                  "created_utc": "2026-02-24 17:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74s062",
              "author": "CryOwn50",
              "text": "Yeah, Iâ€™ve noticed that too",
              "score": 2,
              "created_utc": "2026-02-24 13:28:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74tewq",
                  "author": "Greenonetrailmix",
                  "text": "On my PC using Q4_K_M with my 5090 and 4090 on the Vulkan backend, I'm getting around 90 Tok/s",
                  "score": 2,
                  "created_utc": "2026-02-24 13:36:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gvk1w",
              "author": "CryOwn50",
              "text": "Yeah, itâ€™s honestly impressive how well it holds up even in the smaller quants.When a model keeps strong reasoning and coding ability after aggressive quantization, thatâ€™s usually a sign the base architecture is [solid.No](http://solid.No) surprise itâ€™s topping download charts   devs love performance-per-VRAM efficiency.",
              "score": 1,
              "created_utc": "2026-02-26 05:51:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74m4ch",
          "author": "Nefhis",
          "text": "I'm using Mistral Small 3.2 24b and Magistral Small 24b as local models. I built the front end myself with Xcode, with semantic memory, document uploads to chat, and libraries for RAG. My use is primarily administrative, hence the local setup, to upload documents without exposing them to providers. I have them running on a MacBook Pro M4 Max.",
          "score": 19,
          "created_utc": "2026-02-24 12:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74mjfn",
              "author": "CryOwn50",
              "text": "Thatâ€™s a seriously solid setup building your own frontend with semantic memory and RAG is impressive.\n\nRunning 24B models locally on an M4 Max for private document workflows makes total sense, especially for admin use.\n\nHowâ€™s the performance with larger document sets  still smooth, or does context size start to slow things down?",
              "score": 4,
              "created_utc": "2026-02-24 12:55:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74og4v",
                  "author": "Nefhis",
                  "text": "The models themselves are reliable, but I won't lie to you, when you approach 50k tokens it starts to slow down, although it's still very usable. I don't have the exact t/s on hand to tell you, but I'd say it's roughly between 15 and 28. For more information, I serve it with LMStudio because it allows the use of MLX models, which run better on Mac, and with KV quantization at 8... it works well enough that I don't need to rely on external providers, at least for this specific use case.",
                  "score": 5,
                  "created_utc": "2026-02-24 13:07:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74zz11",
              "author": "_Arelian",
              "text": "Letâ€™s connect bro",
              "score": 1,
              "created_utc": "2026-02-24 14:11:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o755h8f",
                  "author": "Nefhis",
                  "text": "Hey! Sure. What exactly do you want to connect about?",
                  "score": 1,
                  "created_utc": "2026-02-24 14:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o755h8l",
          "author": "Potential-Leg-639",
          "text": "Qwen 3 Coder Next UD-Q5 (256k context)\nQwen 3 Coder UD-Q4 (128k context)\nGPT-OSS-20b UD-Q4 (128k context)\n\nPlanning/Orchestration in Opus, coding itself partly local, especially for larger things, that can run overnight and nothing can hit any limits. Sensitive stuff only local of course. Switched completely to OpenCode.\n\nAll at once on a Strix Halo, works great, love that machine - silent, powerful and power efficient.\n\nWill build a 2nd rig with parts i still have lying around to support the Strix for some tasks. Basically getting a 2nd Strix would maybe be the better idea. Or wait for Medusa Halo.",
          "score": 9,
          "created_utc": "2026-02-24 14:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75krca",
              "author": "nakedspirax",
              "text": "Are you finding a way to step away from opus for a more local thinking model? \n\nWhat strix halo machine are you using?",
              "score": 1,
              "created_utc": "2026-02-24 15:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75q7ct",
                  "author": "Potential-Leg-639",
                  "text": "Opus only for planning and orchestration, coding when planning done into very detailled level locally and with other models. Works good, Opus for coding itself is not necessary when everything was done properly before.\n\nStrix Halo 128GB, they are all quite similar performance wise, just buy the cheapest you can get ;)",
                  "score": 2,
                  "created_utc": "2026-02-24 16:17:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74rk3c",
          "author": "RomanceCherry",
          "text": "I actually like Qwen3 4B, runs pretty fast and is useful for every day questions, while keeping it private running local on iphone.",
          "score": 9,
          "created_utc": "2026-02-24 13:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s5rr",
              "author": "CryOwn50",
              "text": "Thatâ€™s honestly such a sweet spot. A 4B model thatâ€™s fast, responsive, and running fully local on your iPhone? Thatâ€™s peak practicality.",
              "score": -3,
              "created_utc": "2026-02-24 13:29:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75cqut",
          "author": "mister2d",
          "text": "I run Nemotron 3 Nano for my agentic flows. I have some really old hardware but I get a respectable 30-40 tokens/sec at 128k context due to the model's hybrid/swa architecture.\n\n- Dual Xeon (Ivy Bridge)\n- 256 GB DDR3\n- 2x RTX 3060 (12GB)",
          "score": 8,
          "created_utc": "2026-02-24 15:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75uwic",
          "author": "nomorebuttsplz",
          "text": "glm 5 on mac 3 ultra 512 using opencode. Good adjunct to my Claude pro subscription: if I run out of claude tokens or want to do something with sensitive data I can switch pretty seamlessly. It's a lot slower though.",
          "score": 6,
          "created_utc": "2026-02-24 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76rrsz",
              "author": "sig_kill",
              "text": "Iâ€™m seriously impressed with GLM-5, but I donâ€™t have enough to run it locally with a single 5090 and 64gb of RAM.",
              "score": 2,
              "created_utc": "2026-02-24 19:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782gib",
                  "author": "somethingClever246",
                  "text": "I run Q6 with 128GB, 9950x, and 5080.  Ridiculously slow but good quality results",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76tcv3",
                  "author": "nomorebuttsplz",
                  "text": "your computer just has to believe in itself. The ram was in its heart all along. /s",
                  "score": 1,
                  "created_utc": "2026-02-24 19:12:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74tgg2",
          "author": "Right_Weird9850",
          "text": "ministral 3b vl instruct",
          "score": 5,
          "created_utc": "2026-02-24 13:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75yct8",
          "author": "GreyBamboo",
          "text": "I run Gemma3 4b for my chatbot and TranslateGemma for my translation tool right now :)",
          "score": 5,
          "created_utc": "2026-02-24 16:53:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74tvqo",
          "author": "NoobMLDude",
          "text": "Iâ€™m running a few local models for different uses.\n\n- Qwen3-Coder: for Coding \n- Qwen3-14B: for Meeting Assistant\n- Gemma3-7B - for basic Question Answering\n\nHereâ€™s all the tools and setup for different Local usecases :\n\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) \n\nDisclaimer: Some of the model choices may not be relevant for you. This choice is based on my personal preference. I prefer speed over perfect answers since I like to have a quick first level overview and then delve deeper into a topic using larger models later.",
          "score": 4,
          "created_utc": "2026-02-24 13:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a1exh",
              "author": "CryOwn50",
              "text": "that disclaimer is basically the AI version of Results may vary. Side effects include speed, productivity, and mild model addiction.ðŸ˜„Or in local AI terms: Warning: optimized for vibes, not leaderboard glory.",
              "score": 2,
              "created_utc": "2026-02-25 05:39:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a6aa5",
                  "author": "NoobMLDude",
                  "text": "Yes. I think itâ€™s important to let people know about WHY these models are used. \n\nJust like there is no such thing as â€œBest Movieâ€, â€œBest Foodâ€, â€œBest Musicâ€, there is nothing like â€œBest Modelâ€.\n\nFor different people different things might be inportant .",
                  "score": 3,
                  "created_utc": "2026-02-25 06:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75662j",
              "author": "Potential-Leg-639",
              "text": "Disclaimer? ðŸ¤£\nBot?",
              "score": 1,
              "created_utc": "2026-02-24 14:43:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ba9r",
                  "author": "NoobMLDude",
                  "text": "Using appropriate English words is not exclusively restricted to Bots. ðŸ˜‰",
                  "score": 5,
                  "created_utc": "2026-02-24 17:52:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76eapa",
          "author": "benevbright",
          "text": "qwen3-coder-next q3 on 64GB Mac.",
          "score": 5,
          "created_utc": "2026-02-24 18:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o751j8j",
          "author": "Swarley996",
          "text": "Devstral small 2 24b - coding \nGLM 4.7 flash 30b - thinking and complex queries\nMinistral 3 14b - general use\nMinistral 3 3b - small agents",
          "score": 3,
          "created_utc": "2026-02-24 14:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o752mrq",
          "author": "dave-tay",
          "text": "Qwen3-14b, fits 100% into RTX 3060 12gb. Ryzen 5600g to drive my display",
          "score": 3,
          "created_utc": "2026-02-24 14:25:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7depjb",
              "author": "Mr_Tiddy_Sucker",
              "text": "I'm running the exact same model on the same card. I'm curious, what do you use it for, if you don't mind me asking?",
              "score": 1,
              "created_utc": "2026-02-25 18:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7duht2",
                  "author": "dave-tay",
                  "text": "Just simple to medium tasks, like extract out clauses from a legal document and analyze documents for gaps. No video generation or such\n\n`$ ollama ps`\n\n`NAME         ID              SIZE     PROCESSOR    CONTEXT    UNTIL`\n\n`qwen3:14b    bdbd181c33f2    12 GB    100% GPU     16384      2 minutes from now`",
                  "score": 1,
                  "created_utc": "2026-02-25 19:47:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75lkja",
          "author": "Salt-Willingness-513",
          "text": "Mainly nemotron nano and minimax m2.5 in q8 each",
          "score": 3,
          "created_utc": "2026-02-24 15:56:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4xq",
          "author": "PvB-Dimaginar",
          "text": "I have a Bosgame M5 (AMD Strix Halo) running CachyOS Linux. For coding Iâ€™m focusing on Qwen3-Coder-Next 80B Q6.\n\nStill struggling a bit with OpenCode, my config probably needs some work around skills and MCP servers. One thing I did get working that Iâ€™m really happy about is memory sharing between Claude Code and OpenCode.\n\nGoing to improve my setup over the coming days.\n\nAnyone have experience with coding quality differences between the Q6 and Q5 models?â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹",
          "score": 3,
          "created_utc": "2026-02-24 16:48:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a10zl",
              "author": "CryOwn50",
              "text": "Thatâ€™s a clean rig  Bosgame M5 + CachyOS sounds like one of the more fun Linux desktops out there.\n\n",
              "score": 2,
              "created_utc": "2026-02-25 05:36:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hce2g",
                  "author": "PvB-Dimaginar",
                  "text": "Ooh yes! It makes me really smile when I see this beast running. I come from Windows and still need it for work, but everything else I do is on this machine!",
                  "score": 2,
                  "created_utc": "2026-02-26 08:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o763328",
          "author": "andy2na",
          "text": "Qwen3-VL:4B IQ4\\_XS always in VRAM (\\~2.25gb + context)\n\n* Frigate genAI image analysis\n* General questions in openwebui\n* Home Assistant Voice Assistant\n* Karakeep AI tagging and AI Summaries\n* Open-notebook questions and podcast generation\n* Sure Finance for transaction auto-categorization\n\nHoping Qwen3.5:5b will be a huge upgrade, qwen3-vl is already very good for these tasks",
          "score": 3,
          "created_utc": "2026-02-24 17:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p04b",
          "author": "FaceDeer",
          "text": "My workhorse is still good old Qwen3-30B-A3B-Thinking-2507. When the new Qwen models come out in that size class I'll likely switch. The main use it's being put to is summarizing and extracting information from documents, it's chugging along in the background \"digesting\" stuff into easier-to-work-with forms.\n\nI recently started messing around with some agentic stuff and I found Jan.ai's setup to be a good out-of-the-box solution for that, the old Qwen model wasn't so good at that. Hopefully the new one will be better at it.",
          "score": 3,
          "created_utc": "2026-02-24 18:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79vj45",
              "author": "jrexthrilla",
              "text": "Qwen 3.5 27b and 30b just came out",
              "score": 2,
              "created_utc": "2026-02-25 04:56:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ajmxd",
                  "author": "FaceDeer",
                  "text": "Indeed. Now I just need to wait a day or two for the dust to settle. Looking forward to it. :)",
                  "score": 1,
                  "created_utc": "2026-02-25 08:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77dk21",
          "author": "MS_Fume",
          "text": "Abliterated huihui distilled into 4B model so my phone can run itâ€¦. Much fun with it lol",
          "score": 3,
          "created_utc": "2026-02-24 20:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78hfbe",
          "author": "Solid-Pop-3452",
          "text": "Maybe the best LLM model is the friendships we made along the way",
          "score": 3,
          "created_utc": "2026-02-25 00:04:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c7jbc",
              "author": "0xGooner3000",
              "text": "Real",
              "score": 1,
              "created_utc": "2026-02-25 15:18:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79k4sx",
          "author": "RG_Fusion",
          "text": "My most used model at the moment is Qwen3-235b-a22b-instruct at q4-k-m. I use it as a voice assistant running in the background on my desktop. Just something to chat with and bounce ideas off of, nothing truly productive.\n\n\nI just downloaded the new Qwen3.5-397b-17b model today for some testing. On the old model I was getting 13 t/s of decode, and on this new one I'm getting 18.5, so I'll definitely be switching to it once everything is set up.\n\n\nHardware:\n- AMD EPYC 7742 CPU\n- Asrock Rack ROMED8-2T motherboard\n- 512 GB ECC DDR4 3200 MT/s RAM\n- Nvidia RTX Pro 4500 Blackwell GPU",
          "score": 3,
          "created_utc": "2026-02-25 03:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0gpc",
              "author": "CryOwn50",
              "text": "Thatâ€™s a crazy clean setup ðŸ˜®â€ðŸ”¥ 512GB ECC + EPYC 7742 is basically home lab meets data center.\n\n",
              "score": 2,
              "created_utc": "2026-02-25 05:32:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a0yrb",
                  "author": "RG_Fusion",
                  "text": "Thanks. I built it primarily to explore LLMs as a concept, just a tool to help me learn. It's definitely proving capable though. I plan to add more GPUs in the future, which will continue pushing token generations up on these massive MoEs.",
                  "score": 2,
                  "created_utc": "2026-02-25 05:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75b4f5",
          "author": "bsenftner",
          "text": "I maintain a Wan2GP installation, it's got nearly 100 models now for image, video, voice clones, tts, and text to song. The developer of Wan2GP is really active, and tends to release a new version within a few days of new models. Qwen3 seems to have additional training to know about Wan2GP, surprisingly, and is great at helping create more complex media requiring combinations of models in concert.",
          "score": 2,
          "created_utc": "2026-02-24 15:08:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76m77s",
          "author": "fallingdowndizzyvr",
          "text": "GLM 5.",
          "score": 2,
          "created_utc": "2026-02-24 18:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76s3f7",
              "author": "sig_kill",
              "text": "Mac Studio?",
              "score": 2,
              "created_utc": "2026-02-24 19:06:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o772l3y",
          "author": "hallofgamer",
          "text": "Glm 4.7 flash handles all my needs",
          "score": 2,
          "created_utc": "2026-02-24 19:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78pz6y",
          "author": "Bitterbalansdag",
          "text": "Magistral 2 small 24b with a home made front end and MCP servers. \n\nMCP servers for: persistent external memory, web search, tasks management (basic todo list)\n\nThe chats have automatic compaction. The front end can swap system prompts mid-chat.\n\nUse for a general personal assistant, runs on a 3090 and a tailscale network.",
          "score": 2,
          "created_utc": "2026-02-25 00:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78qtkd",
          "author": "Pjbiii",
          "text": "Different tools for different tasks. I use Qwen3:14b for document sort and summarize, InternVL3.5:8b for long image descriptions, Qwen3-vl:2b for image keywords/SEO file names, gemma3:27b for outline writing, glm-4.7-flash for n8n AI Agent node. \n\nMacBook Pro (headless, the keyboard was killed by my toddler and a water bottle) M1 Max with 32GB unified memory.",
          "score": 2,
          "created_utc": "2026-02-25 00:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0mat",
              "author": "CryOwn50",
              "text": "Thatâ€™s a super clean stack  right tool for each job instead of forcing one model to do everything.Respect for squeezing that much out of an M1 Max with 32GB unified memory too especially running it headless ðŸ˜‚Alsoâ€¦ toddler + water bottle is a more dangerous combo than any production outage.",
              "score": 1,
              "created_utc": "2026-02-25 05:33:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ab7rv",
          "author": "Own_Professional6525",
          "text": "Lately Iâ€™m seeing more people run smaller quantized models locally for coding and RAG since they balance performance and cost well. Curious whether people prioritize privacy, latency, or experimentation when choosing their setup.",
          "score": 2,
          "created_utc": "2026-02-25 06:59:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7abr94",
              "author": "CryOwn50",
              "text": "Yeah, that shift makes sense.\n\nSmaller quantized models hit a sweet spot good enough for coding and RAG, but light enough to run locally without crazy hardware. For most people, itâ€™s a practical trade-off between performance and cost.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:04:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7ae7iq",
              "author": "Potential-Leg-639",
              "text": "Cloud models are still wayyyy faster, but I use the local models for sensitive tasks and coding itself, mostly over night",
              "score": 1,
              "created_utc": "2026-02-25 07:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7abwyl",
          "author": "TheAdmiralMoses",
          "text": "I'm messing with LiquidLFM models, they're supposed to be the future but they kinda just suck ngl",
          "score": 2,
          "created_utc": "2026-02-25 07:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aefjd",
              "author": "CryOwn50",
              "text": "Lmao ðŸ˜­Thatâ€™s the honest take nobody wants to post.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ajmjv",
                  "author": "TheAdmiralMoses",
                  "text": "I think they'll be good once there's some more instruct tuned models, right now they don't seem to be able to keep a single train of thought and get derailed from core facts of conversations by taking tangents on the smallest context",
                  "score": 2,
                  "created_utc": "2026-02-25 08:16:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bb8fj",
          "author": "routhlesssavage",
          "text": "Just putting it here, in case anyone is looking for macos or mobile phones local LLM app. https://github.com/alichherawalla/off-grid-mobile\nhttps://news.ycombinator.com/item?id=47142003\n\nI have been using this for some time and the response is quite amazing with minimum resources.",
          "score": 2,
          "created_utc": "2026-02-25 12:18:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gu3ss",
          "author": "pieonmyjesutildomine",
          "text": "GLM-5@4bit\n\nMinimax-m2.5@8bit\n\nQwen3.5-379b@nvfp4\n\nGLM4.7@8bit",
          "score": 2,
          "created_utc": "2026-02-26 05:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h4jg1",
          "author": "Downtown_Patience_46",
          "text": "Qwen3 1.7b q8\\_0, running on my 16GB RAM + 4GB VRAM (RTX 3050) laptop, for simple translation.",
          "score": 2,
          "created_utc": "2026-02-26 07:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79ycsk",
          "author": "Grand_Barnacle_6922",
          "text": "I'm running a few - minimax m2.5 230B and qwen3 235B seem to be my favs right now\n\nit's actually been very practical as i've been able to build a lot of custom automations around smart home and personal administration tasks.  i'm not certain on the hardware specs but my mac seems to be able to handle it just okay.\n\n",
          "score": 1,
          "created_utc": "2026-02-25 05:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aee8h",
          "author": "Outrageous_Corgi7553",
          "text": "Running Qwen2.5-Coder-32B-Q4 on M4 Mac (24GB). Actually practical for:\n\n* Code review and refactoring â€” works well for Python, catches logic issues\n* Quick questions instead of hitting Claude API every time\n* First-pass RAG retrieval ranking\n\nNot using for agents â€” too slow for multi-step workflows, and quality drops on complex reasoning. For anything serious I still route to Claude/GPT-4.",
          "score": 1,
          "created_utc": "2026-02-25 07:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c1y0b",
          "author": "Shouldhaveknown2015",
          "text": "I was using the new GLM MOE, then swapped to Q3 Next 80b in a Q3 unsloth gguf, then I am now using the 35b-Q3.5 model released yesterday as my daily driver.\n\nI do use other models... for instance I vibe coded a family app (Calendar, Kids rewards/chores tracking, grocery list, AI Chat bot [Running on my home lab which only has a 3060 12gb so it runs a 8b model, but I also use a text to voice model (I forget which) and it makes the AI sound like a child as I made it to act/be like BMO and sound like BMO from adventure time] and it also processes the AI responses and provides big buttons with reponses for the kids to click instead of typing. They can access it from their amazon tablet and they seem to love it.\n\nI also use the models I mention at the start with RAG and Tools. I have a vibe coded chat app with a obsidian.md vault. It pulls files automatically from the chat references to educate the AI. I can also select files to provide as part of context and select how the files should be looked at... AKA.. Is this file a research file or a lore file or a log files etc.\n\nI also built in a image generator using Flux and Klein models.\n\nI also vibe coded my own \"openclaw\" type agent, need to test it with the Q3.5 but haven't had time. It got put on the back burner, but it also has RAG and tool access to something like 28 tools. Been working on it slowly as it's the least fun project. It works well and is a limited form of my own ai research agent at this time.",
          "score": 1,
          "created_utc": "2026-02-25 14:51:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gvc4g",
              "author": "CryOwn50",
              "text": "Man, this is such a fun read youâ€™re not just experimenting, youâ€™re actually building a full-stack personal AI ecosystem.\n\n",
              "score": 1,
              "created_utc": "2026-02-26 05:50:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d89g3",
          "author": "Ok-Dog-4",
          "text": "Qwen 2.5 abliterated 3b on a Raspberry pi 5 8gb RAM. Actually so decent",
          "score": 1,
          "created_utc": "2026-02-25 18:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d8ihz",
          "author": "Ok-Dog-4",
          "text": "Also tiny llama works great on the Pi. Cost friendly and completely local. Training for finetune obviously must be outsourced but models under around 3.5b function just fine on new piâ€™s. Ollama btw",
          "score": 1,
          "created_utc": "2026-02-25 18:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d8n8r",
              "author": "Ok-Dog-4",
              "text": "Forgot but models are quantified from 8gb to 2.5 to work on the pi",
              "score": 1,
              "created_utc": "2026-02-25 18:08:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jjqb9",
          "author": "randygeneric",
          "text": "Qwen3.5-35BA3B-ud-q5-k-xl (or ud-q3-k-xl)  via  llama.cpp --cache-type-k q8\\_0  --cache-type-v q8\\_0  --ctx-size 131072  \n   \non a gaming laptop i7 12650 32gb RAM + rtx 4060 8gb VRAM  \nud-q3-k-xl is surprisingly accurate while being fast enough (25 token/s)",
          "score": 1,
          "created_utc": "2026-02-26 16:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p16a",
          "author": "Ok-Patient6458",
          "text": "I run a blog for local LLM news and insight at [https://lftw.dev](https://lftw.dev)",
          "score": 1,
          "created_utc": "2026-02-24 18:53:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rarw7t",
      "title": "I managed to run Qwen 3.5 on four DGX Sparks",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ifu522zupqkg1.jpeg",
      "author": "Icy_Programmer7186",
      "created_utc": "2026-02-21 14:03:34",
      "score": 50,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rarw7t/i_managed_to_run_qwen_35_on_four_dgx_sparks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6lq9mo",
          "author": "mosredna101",
          "text": "So 20K brings you 21 t/sec?",
          "score": 9,
          "created_utc": "2026-02-21 14:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lskx8",
              "author": "Icy_Programmer7186",
              "text": "This is a lab setup and this is one of many experiments done on it.  \nMoney well spent; this is not however recommendation for a production setup.",
              "score": 10,
              "created_utc": "2026-02-21 14:30:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lt5gl",
                  "author": "mosredna101",
                  "text": "I would be super happy with only one of those machines to be honest :D   \n",
                  "score": 2,
                  "created_utc": "2026-02-21 14:33:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6sig50",
                  "author": "jinnyjuice",
                  "text": "Why not for production?",
                  "score": 1,
                  "created_utc": "2026-02-22 16:08:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ofl7r",
              "author": "fallingdowndizzyvr",
              "text": "You can get 4 Sparks for $12K.",
              "score": 3,
              "created_utc": "2026-02-21 22:42:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oh648",
          "author": "CalvinBuild",
          "text": "That's amazing! I can't wait to do it locally!",
          "score": 2,
          "created_utc": "2026-02-21 22:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6swjif",
          "author": "Weak-Split-538",
          "text": "How did you connect the 4 sparks together ? Using NIC with a NIC switch ? Or just network cluster ?",
          "score": 2,
          "created_utc": "2026-02-22 17:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t4t7c",
              "author": "Icy_Programmer7186",
              "text": "I use switch: [https://mikrotik.com/product/crs804\\_ddq](https://mikrotik.com/product/crs804_ddq)",
              "score": 2,
              "created_utc": "2026-02-22 17:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmh9x",
          "author": "I_like_fragrances",
          "text": "I am able to get unsloth's Q6\\_K\\_XL with max context at around 40 tok/s. What quant do you use on the sparks? Typically if I offload these bigger models to the CPU I can get around 20 tok/s but when they're fully on the GPU they run at around 40 tok/s.\n\nhttps://preview.redd.it/1ujmj21qmwkg1.png?width=836&format=png&auto=webp&s=ee566d16319e3ae1db5631689978d30c961f67c7",
          "score": 1,
          "created_utc": "2026-02-21 20:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p4g3f",
              "author": "Icy_Programmer7186",
              "text": "I use FP8 quantization -> Qwen/Qwen3.5-397B-A17B-FP8\n\nSpark has a unified memory, so I guess there is no off-loading.",
              "score": 2,
              "created_utc": "2026-02-22 01:13:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ralgvu",
      "title": "Local LLM for Mac mini",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "author": "Jiggly_Gel",
      "created_utc": "2026-02-21 08:00:24",
      "score": 45,
      "num_comments": 30,
      "upvote_ratio": 0.91,
      "text": "Iâ€™ve been watching hours of videos and trying to figure out whether investing in a Mac mini with 64 GB RAM is actually worth it, but the topic is honestly very confusing and Iâ€™m worried I might be misunderstanding things or being overly optimistic.\n\nIâ€™m planning to build a bottom up financial analyst using OpenClaw and a local LLM, with the goal of monitoring around 500 companies. Iâ€™ve discussed this with ChatGPT and watched a lot of YouTube content, but I still donâ€™t have a clear answer on whether a 30B to 32B parameter model is capable enough for this kind of workload.\n\nIâ€™ll be getting paid for a coding project I completed using Claude, and Iâ€™m thinking of reinvesting that money into a maxed out Mac mini with 64 GB RAM specifically for this purpose.\n\nMy main question is whether a 30B to 32B local model is sufficient for something like this, or if I will still need to rely on an API. If Iâ€™ll need an API anyway, then Iâ€™m not sure it makes sense to spend so much on the Mac mini.\n\nI donâ€™t have experience in this area, so Iâ€™m trying to understand whatâ€™s realistic before making the investment. Iâ€™d really appreciate honest input from people who have experience running local models for similar use cases.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6lpe1n",
          "author": "McMissile",
          "text": "I would just build whatever it is that you want on AWS and pay for cloud LLM tokens. Then you can figure out the feasability of what you're trying to achieve, and determine whether the 32b models will be sufficient. If it works, then great, buy the mac mini and run it locally. If not you've saved yourself a  bunch of money.",
          "score": 21,
          "created_utc": "2026-02-21 14:11:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6luuwp",
              "author": "AppointmentAway3164",
              "text": "This is the conclusion I reached. Targeting 30B models on huggingface. See how my use case tests. If it works then consider the $19k/$20k investment. Currently the the math doesnâ€™t generally make sense for home token generation.",
              "score": 7,
              "created_utc": "2026-02-21 14:43:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ni9fh",
                  "author": "McMissile",
                  "text": "Yeah for most people I doubt running locally is ever really cost effective unless you need enormous amounts of tokens for sustained periods of time. 32b models that you can run locally have gotten fairly cheap to run in the cloud, especially compared to the price of a home lab.",
                  "score": 2,
                  "created_utc": "2026-02-21 19:44:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l3znc",
          "author": "rerorerox42",
          "text": "Maybe also look into tax discounts on investment into research and development.\n\nI have personally found newer 8B models (Ministral-3) to be capable on a 16GB M4 mini for what I have worked with by coding access to and using local models as needed with relevant context.\n\nAn issue currently is that it really is a little too soon for anybody to have tried out everything and every modelsâ€™ true capabilities. FA-find-out kinda stage.",
          "score": 4,
          "created_utc": "2026-02-21 11:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qghk4",
              "author": "d4mations",
              "text": "Iâ€™m running ministral3-14b on a mac mini m416 gb I got used for very cheap. It is completely dedicated to lm studio with nothing else running on it. It also runs bge-3m for embedding. I have completely setup open claw with this setup, it runs crons flawlessly, has written several bash scripts that have worked on this first go, it proposed the embedded memory setup and then configured it perfectly, calls the few tools I have setup so far perfectly. The only caveat is context tokens. I have had to limit the number to 40k. Other than that I have a great little setup for less than 500â‚¬ and not one penny spent on tokens. I must say that I have fallback to openai oauth 20â‚¬ account and in the beginning I passed all of the recommendations and scripts proposed by ministral through it just to be sure. I trust it more and more everyday",
              "score": 3,
              "created_utc": "2026-02-22 07:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sc0sg",
                  "author": "OysterPickleSandwich",
                  "text": "Do you have your setup described anywhere?Â \nIâ€™m debating on getting a mini for dedicated local use.Â \n",
                  "score": 1,
                  "created_utc": "2026-02-22 15:40:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6l4xya",
              "author": "Jiggly_Gel",
              "text": "I was thinking of messing around with the FA-find-out by running multiple models and then using a Claude API and comparing the responses but thought Iâ€™d ask here first in case if anyone had any other suggestions. \n\nIâ€™ve seen similar posts to mine but they seemed to be looking for other use cases but the tax thing is definitely interesting Iâ€™m going to give that a look thank you",
              "score": 2,
              "created_utc": "2026-02-21 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kr7hn",
          "author": "battle_pantZ",
          "text": "Yeah, Im asking this question myself so Iâ€™m curious which answers youâ€™ll get\n\nAfaik: 64GB is not enough to match the performance of large models. Even Kimi needs 200-500GB to perform properly.",
          "score": 3,
          "created_utc": "2026-02-21 09:30:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ksr3h",
              "author": "Jiggly_Gel",
              "text": "I think a 30B model should work fine on a Mac mini but the question is what model and how powerful is a 30B model? Some people attribute Qwen 30B to match gpt 4o but then if I have to spend money on machinery for 500 gb Iâ€™d rather run it via APIs because most of my financial analysis is done via my python bot itâ€™s just I need something for judging the analysis and doing further industry research, comparative analysis, and reading annual reports and the news daily",
              "score": 1,
              "created_utc": "2026-02-21 09:45:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6l5n4e",
                  "author": "DistanceSolar1449",
                  "text": "Easy answers.\n\n- Qwen 3 VL 32b\n\n- GLM 4.7 Flash\n\n- Qwen 3.5 35b\n\n- Qwen 3 Next\n\nThese would be the best models you can fit on 64GB.\n\nIs buying a $2k mac mini 64gb to run these models worth it? Hell no. You can run the first 3 on a 3090 for $900 or so. And it'd be way faster than a mac mini.\n\nThe 512GB Mac Studio is actually something to consider, since you can run GLM 5 and similar big models that can actually rival ChatGPT/Claude. But the 64GB mac mini? Definitely not. You're better off spending $2k on GPT api credits.",
                  "score": 7,
                  "created_utc": "2026-02-21 11:48:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kuhd3",
                  "author": "battle_pantZ",
                  "text": "\nI think it makes more sense to buy a high-performance Mac if you need it for demanding tasks outside of AI. I mean, spending $2-3k to â€œonlyâ€ use gpt4? I also look into it every day and do my research, and then I always come back to APIs. Yesterday, I paid $20 for the deepseek API, and I have to say, it's very, very cheap and pretty powerful for coding. It's 90% cheaper than the competition, by the way. \n\nSo a larger investment is needed. Either an even more powerful Mac or several smaller Macs as a cluster. Until a new AI comes out that requires even more hardware performance. If you extrapolate that, APIs will probably be cheaper.",
                  "score": 6,
                  "created_utc": "2026-02-21 10:02:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lr2su",
          "author": "HumbleTech905",
          "text": "Pay a few dollars for a service to deploy a 30B model, test and evaluate it, then decide if it's worth buying the Mac.",
          "score": 3,
          "created_utc": "2026-02-21 14:21:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m42f5",
          "author": "ScuffedBalata",
          "text": "\"Will a 30B model work for this use\" is very nebulous. It's not something you can just define.  It's like asking \"what IQ can be a financial analyst\".",
          "score": 3,
          "created_utc": "2026-02-21 15:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o10qi",
          "author": "Comprehensive_Iron_8",
          "text": "If you think you'll love using openclaw using a local model. The local models who remotely perform as well will be worth multiple MAC STUDIOs costing thousands of dollars. A model running inside a 64GB ram mac mini is not worth using. You'll not love it. You can use it for small things like email classification projects and maybe a small RAG system. \n\nWhy I say this, OPUS is so good, that people who have once used it, do not like 5.3 codex just because it does not have that kind of personality. And tomorrow there will be another model which has the same kind of moat. \n\nBut none of them at this time are good enough to run in a 64GB ram mac mini.",
          "score": 2,
          "created_utc": "2026-02-21 21:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o1oyx",
              "author": "Comprehensive_Iron_8",
              "text": "When I say this. I think a $10k mac studio can run GLM-5 and Minimax 2.5 which I actually like to run(using cloud providers, because the $10k ROI is not there yet) for openclaw, and are decent. They make mistakes yes, but you can optimize them to make less of them. But nothing for a mac mini yet.",
              "score": 1,
              "created_utc": "2026-02-21 21:27:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6o9tgu",
                  "author": "nvidiabookauthor",
                  "text": "What are some non China cloud providers to test this?",
                  "score": 1,
                  "created_utc": "2026-02-21 22:10:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mvskq",
          "author": "egoslicer",
          "text": "I have a 128B Strix Halo and run 70-100B models, and I'd say they are 'ok' for OpenClaw generalist tasks and tool calling. \n\nHowever 64GB Mac Mini and targeting ~30B models? Even more restrictive than that and while some are capable with tool calling, the results are going to be pretty meh overall. \n\nIf I were you, I'd setup OpenClaw and run it through MiniMax or Kimi 2.5 apis like many people are doing for a couple of months to see how that runs for you. You can do that on any machine you currently have and get much stronger reasoning and results.\n\nFor a local setup, IMO, ~30B models are ok for scoped tasks, but doing financial analysis with a lot of variables is likely not going to give you great results.",
          "score": 1,
          "created_utc": "2026-02-21 17:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qlfq3",
          "author": "emotionallofi",
          "text": "I just returned my m4 pro mac mini 64gb today. Going with a cheap used m2 max until m5 ultra is released.",
          "score": 1,
          "created_utc": "2026-02-22 07:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aj47q",
          "author": "emmettvance",
          "text": "The 64gb mac mini is genuinely capable of running 30-32b models well.. unified memory makes a real difference compared to GPU setups with the same RAM figure.. the honest question for your use case is whether 500 companies means 500 parallel analyses or sequential ones.. if it's sequential and you're okay with it running overnight the local setup can work..\n\nbut financial analysis on company filings tends to involve long documents and large context windows which is where even a well-speced Mac starts feeling the pressure.. the per-token cost of running the same models through providers like deepinfra or together is low enough that for the volume you are descriibing it might actually be cheaper than the hardware investment depending on how often you run full sweeps.",
          "score": 1,
          "created_utc": "2026-02-25 08:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l93cm",
          "author": "thought_provoking27",
          "text": "Great thread .",
          "score": 0,
          "created_utc": "2026-02-21 12:18:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbu7sx",
      "title": "M4 Pro 48 or M4 Max 32",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "author": "Mammoth-Error1577",
      "created_utc": "2026-02-22 18:58:33",
      "score": 41,
      "num_comments": 38,
      "upvote_ratio": 0.86,
      "text": "I got my machine renewed at work a week ago.\n\nThey rejected my request of a Mac studio with 128 GB and instead approved a MacBook M4 Pro with 48GB and 512.\n\nWell I finally got around to checking and they actually gave me a more expensive M4 Max but with 32 GB and 1TB instead.\n\n\nIn my previous chatting with Gemini it has convinced me that 128 GB was the bare minimum to get a sonnet level local LLM.\n\nWell I was going to experiment today and see just what I could do with 48 and to my surprise I only had 32, but a superior CPU and memory bandwidth.\n\n\nIf my primary goal was to run coding a capable LLM, even at the cost of throughout, I assume 48 is vastly superior.  However if the best model I can run with 48 (+ containers and IDE and chrome etc.) is really dumb compared to sonnet I won't even use it.\n\nI'm trying to decide if it's worth raising a fuss over getting the wrong, more expensive laptop. I can experiment with a very small model on the current one but unless it was shockingly good I don't think that experiment would be very informative.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6toq7h",
          "author": "j00cifer",
          "text": "M5 ultra studio is coming out this year with a reported max RAM of 1TB.\n\n1TB RAM.",
          "score": 25,
          "created_utc": "2026-02-22 19:22:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tpdap",
              "author": "jiqiren",
              "text": "ðŸ˜ want it so good ðŸ¥° M5 Ultra 1TB??? Yes please!",
              "score": 7,
              "created_utc": "2026-02-22 19:25:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v9irr",
                  "author": "gingerbeer987654321",
                  "text": "Only 1?  Get 4 and do the Thunderbolt raid thing",
                  "score": 5,
                  "created_utc": "2026-02-23 00:20:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71kvcd",
                  "author": "LimiDrain",
                  "text": "Does this unified memory work as fast as VRAM or it's close to normal RAM speeds?",
                  "score": 1,
                  "created_utc": "2026-02-23 23:38:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uawrt",
              "author": "sav22v",
              "text": "But you'll have to sell your kidneys and children to pay for it...",
              "score": 5,
              "created_utc": "2026-02-22 21:14:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6uq4zs",
                  "author": "j00cifer",
                  "text": "Iâ€™m making the case to them now.",
                  "score": 2,
                  "created_utc": "2026-02-22 22:32:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uyghh",
              "author": "grim-432",
              "text": "With the current price of ram, whatâ€™ll that cost?  $25,000?",
              "score": 3,
              "created_utc": "2026-02-22 23:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x4h53",
                  "author": "ijontichy",
                  "text": "They would have locked in RAM costs for this year before the RAMpocalypse. But do you think they'll hold prices steady? ðŸ¤”",
                  "score": 1,
                  "created_utc": "2026-02-23 08:18:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71z016",
                  "author": "GonzoDCarne",
                  "text": "Ram price does not apply to oems like Apple. Due to many things that someone might want to go into detail in a long thread. My hard guess is they will target 15k or 19999. M3 Ultras with 512GiB go for 10k un the US since before the ram surge and today.",
                  "score": 1,
                  "created_utc": "2026-02-24 00:56:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6trf6a",
              "author": "Mammoth-Error1577",
              "text": "Unfortunately not an option. The only studio I could get is also 36GB.",
              "score": 2,
              "created_utc": "2026-02-22 19:35:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6u9uza",
              "author": "ZealousidealShoe7998",
              "text": "at that level what llm would one even use to reach comercial levels ?",
              "score": 1,
              "created_utc": "2026-02-22 21:08:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6xj37x",
              "author": "iezhy",
              "text": "Given current ram prices (and Apple markup), this probably will be out of reach for most users",
              "score": 1,
              "created_utc": "2026-02-23 10:41:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70hc2o",
                  "author": "j00cifer",
                  "text": "I donâ€™t see why a 2nd kidney is so important to people",
                  "score": 1,
                  "created_utc": "2026-02-23 20:17:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7134m5",
              "author": "Jealous_Incident7978",
              "text": "Starts getting funny that we drop $$$$ on a 1TB Ram M5 ultra studio to run open weight models that is essentially free. ðŸ˜† imagine paying something similar for qwen 3.5 / DeepSeek etc just to run the model locally",
              "score": 1,
              "created_utc": "2026-02-23 22:03:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o729hxh",
                  "author": "j00cifer",
                  "text": "1 TB RAM.",
                  "score": 1,
                  "created_utc": "2026-02-24 01:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6u93hc",
          "author": "No_Success3928",
          "text": "Hahaha sonnet level ðŸ™„ classic gemini hallucinations",
          "score": 16,
          "created_utc": "2026-02-22 21:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o737jwu",
              "author": "WildRacoons",
              "text": "Clearly doesnâ€™t think very highly of sonnet",
              "score": 2,
              "created_utc": "2026-02-24 05:38:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6u2tks",
          "author": "BisonMysterious8902",
          "text": "I hate to break it to Gemini, but you can't get anywhere close to Sonnet level with 128Gb. Can you get something usable? Sure, but it'll never match frontier level models. Even a Studio with 512Gb. That's just the current state of things.",
          "score": 11,
          "created_utc": "2026-02-22 20:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ughj5",
              "author": "meTomi",
              "text": "Current state? When you think your home pc can compete with million dollar racks in server rooms?\nOn the other hand yes technology is getting better and you can run better and bigger models at home.",
              "score": 1,
              "created_utc": "2026-02-22 21:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ve5yt",
          "author": "MrRandom04",
          "text": "Only open source LLMs that compete with Sonnet 4.6 / Opus 4.6 are GLM 5 and Kimi K2.5. Of these, only GLM 5 is super reliable for agentic coding. That model is far too big for anything less than like 512gb ram. For 32gigs, you can consider the Qwen series UD quants and then have a workflow where you shell out to an API provider of GLM 5 or even just Sonnet / Opus for planning and big design / knowledge level tasks while the manual editing and coding is done by Qwen. The latest ones are very good at stuff like Python and really good for their size.",
          "score": 9,
          "created_utc": "2026-02-23 00:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4d3s",
          "author": "Expert-Reaction-7472",
          "text": "i dont think id make a fuss about this to any place i've ever worked.\n\nnice thing about being self employed is if i want to splurge on a machine i can. Which usually means I have something decent but not mind blowingly expensive cos it's my own money and i'd rather spend the extra on a holiday or something.",
          "score": 3,
          "created_utc": "2026-02-22 20:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uqm3v",
          "author": "ComfortablePlenty513",
          "text": "always prioritize memory. M4 architecture is fundamentally better than previous gen for inference",
          "score": 3,
          "created_utc": "2026-02-22 22:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w40t7",
          "author": "Sharp-Mouse9049",
          "text": "32GB in 2026 for serious local LLM work is basically consumer-tier. I donâ€™t care how fast the M4 Max is â€” if youâ€™re constantly forced into tiny quants or canâ€™t load 70B comfortably, youâ€™re artificially capping your experimentation. Bandwidth doesnâ€™t matter if the model doesnâ€™t fit. RAM is the ceiling.",
          "score": 2,
          "created_utc": "2026-02-23 03:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v9e8v",
          "author": "pondy12",
          "text": "Get a HP ZBook Ultra G1a, Ryzen AI Max+ PRO 395, 64gb - 128gb of ram, 256gb/s ram bandwidth. Will be 1/4th the price. \n\n",
          "score": 1,
          "created_utc": "2026-02-23 00:20:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xhgbi",
              "author": "Confident-Strength-5",
              "text": "It also has 256gb/s bandwidth, soâ€¦\nLLMs really like bandwidthâ€¦",
              "score": 1,
              "created_utc": "2026-02-23 10:25:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vlezt",
          "author": "midz99",
          "text": "Vram or whatever mac calls it is everything. Higher the better. really you need 128gb to even get close to something worth testing.",
          "score": 1,
          "created_utc": "2026-02-23 01:30:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73fuxe",
          "author": "Coyote_Android",
          "text": "After playing around with 32 GB for a while, do you think 48 GB would allow for a significantly better model? Not necessarily for coding though. Just language generation. I'm facing a similar decision.",
          "score": 1,
          "created_utc": "2026-02-24 06:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74mvru",
              "author": "Mammoth-Error1577",
              "text": "I haven't had the opportunity to do anything that seemed usable yet. This has been my first attempt with a local model though, and I naively thought it would be some simple drop in for a slower and dumber version of a cloud model, but the experience I had was so poor that I couldn't see myself using it for anything. I definitely need to do more tweaking, I didn't even get as far as trying to change any configuration, as I didn't even know that could be changed!",
              "score": 2,
              "created_utc": "2026-02-24 12:57:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7518i4",
                  "author": "Coyote_Android",
                  "text": "But you only have coding use cases? For playing around (not coding though afaik) you might wanna give [https://msty.ai](https://msty.ai) a shot",
                  "score": 1,
                  "created_utc": "2026-02-24 14:18:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6uivgp",
          "author": "DistanceSolar1449",
          "text": "M4 max has way faster memory bandwidth\n\n48gb is not enough for Qwen3 next \n\nJust stick with 32gb",
          "score": 0,
          "created_utc": "2026-02-22 21:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6unjkw",
              "author": "Mammoth-Error1577",
              "text": "I just tried  qwen2.5-coder:14b in open code and it was extremely dumb and worse than copying and pasting from a web browser (on an empty repo)\n\nI tried qwen2.5-coder:32b 1st and /init wasn't doing anything so Gemini told me to downgrade.\n\nBut /init didn't do anything after downgrading either.\n\nAll I could get it to do was spit out code that it would tell me to put into the file myself instead of doing it itself, and then the code wasn't even syntactically correct.\n\nI'm super shocked it was so bad, there was no way I was doing it correctly.",
              "score": 0,
              "created_utc": "2026-02-22 22:18:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zftss",
                  "author": "Djagatahel",
                  "text": "What is that /init you're talking about?",
                  "score": 1,
                  "created_utc": "2026-02-23 17:24:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tmf6f",
          "author": "Svyable",
          "text": "Surprised how much I get out of my 24 Pro M4 I have like 100Gb running in Brave browsers no problem. \n\nModel sizes are coming down. Donâ€™t complain innovate",
          "score": -1,
          "created_utc": "2026-02-22 19:11:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdb146",
      "title": "Can anybody test my 1.5B coding LLM and give me their thoughts?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "author": "Great-Structure-4159",
      "created_utc": "2026-02-24 08:45:48",
      "score": 36,
      "num_comments": 33,
      "upvote_ratio": 0.97,
      "text": "I fine tuned my own 1.5B LLM, took Qwen2.5-1.5B-Instruct and fine tuned it on a set of Python problems, and I got a pretty decent LLM!\n\nI'm quite limited on my computational budget, all I have is an M1 MacBook Pro with 8GB RAM, and on some datasets, I struggled to fit this 1.5B model into RAM without getting an OOM.\n\nI used mlx\\_lm to fine tune the model. I didn't fine tune fully, I used LoRA adapters and fused. I took Qwen2.5-1.5B-Instruct, trained it for 700 iterations (about 3 epochs) on a 1.8k python dataset with python problems and other stuff. I actually had to convert that data into system, user, assistant format as mlx\\_lm refused to train on the format it was in (chosen/rejected).  I then modified the system prompt, so that it doesn't give normal talk or explanations of its code, and ran HumanEval on it (also using MLX\\_LM) and I got a pretty decent 49% score which I was pretty satisfied with.\n\nI'm not exactly looking for the best bench scores with this model, as I just want to know if it's even good to actually use in daily life. That's why I'm asking for feedback from you guys :D\n\nHere's the link to the model on Hugging Face:\n\n[https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B](https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B)\n\nIt's also available on LM Studio if you prefer that.\n\nPlease test out the model and give me your thoughts, as I want to know the opinions of people using it. Thanks! If you really like the model, a heart would be much appreciated, but I'm not trying to be pushy, only heart if you actually like it.\n\nBe brutally honest with your feedback, even if it's negative like \"this model sucks!\", that helps me more thank you think (but give some reasoning on why it's bad lol).\n\nEdit: 9.6k views? OMG im famous.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7421hy",
          "author": "Ok-Employment6772",
          "text": "In a few weeks I have a large python project coming up, cant wait to test it",
          "score": 7,
          "created_utc": "2026-02-24 10:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74265u",
              "author": "Great-Structure-4159",
              "text": "Thanks for testing! Can't wait to hear your feedback.",
              "score": 3,
              "created_utc": "2026-02-24 10:15:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7cvtgn",
              "author": "Maleficent-Ad5999",
              "text": "This.. the beauty of this community! Kudos.",
              "score": 1,
              "created_utc": "2026-02-25 17:10:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74gxww",
          "author": "fermented_Owl-32",
          "text": "This is what I needed. I want a local tool calling orchestrator and dynamic tool creator in python. Let me test it how it holds up in creating python scripts by receiving instructions from another agent. The smaller the better, as i need to run 5 such models ( i call them micromodels ).\nWill let you know how it turns out.",
          "score": 7,
          "created_utc": "2026-02-24 12:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s0h3",
              "author": "Great-Structure-4159",
              "text": "Ohâ€¦ tool calling, interesting. I should try that, I didnâ€™t train with tool calling in mind actually, but this is really cool, I think it can work.",
              "score": 2,
              "created_utc": "2026-02-24 13:28:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o74s45t",
              "author": "Great-Structure-4159",
              "text": "I donâ€™t think itâ€™ll be very good as an orchestrator, but Iâ€™ll try making a model fine tuned for orchestrating tool calling, that would be really cool. Do let me know if it works out good, very interesting to see LLMs applied like this.",
              "score": 2,
              "created_utc": "2026-02-24 13:28:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74uaba",
                  "author": "fermented_Owl-32",
                  "text": "Orchestrator will be a function-gemma model. One of its tools will be the tool creator. The tool creator will use your model to write scripts for the use-case in user's query. I need simpler but fast and a little intelligent scripting, I will test it for that purpose",
                  "score": 2,
                  "created_utc": "2026-02-24 13:41:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o744c44",
          "author": "RnRau",
          "text": "I don't do python, but I think its just awesome to see open source tools and weights being used in such a resource constrained environment to get a very useful outcome. \n\nCheers for the writeup!",
          "score": 11,
          "created_utc": "2026-02-24 10:35:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bpi8",
              "author": "Great-Structure-4159",
              "text": "Yeah I was pretty shocked too that 8GB could do stuff like this, but yeah I find the subject very fascinating :)",
              "score": 2,
              "created_utc": "2026-02-24 11:39:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73zmq7",
          "author": "Whiplashorus",
          "text": "Am gonna check it after \nJust a question why using qwen2.5 as a foundation?\nLfm2.5 or even qwen3 are not good for your usecase?",
          "score": 3,
          "created_utc": "2026-02-24 09:52:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74056z",
              "author": "Great-Structure-4159",
              "text": "Great question! My first choice was actually LFM2.5, and I did try that first, but for some reason when fusing it with adapters on MLX, llama.cpp just refuses to convert it to GGUF. I tried troubleshooting but eventually just gave up. Qwen3 was my next choice but I just decided to keep it simple and start with 2.5 and go from there, mainly because Qwen3 came with a 1.7B model (which was pushing my RAM limit due to the dataset having long samples) and also, in my searches, didn't have an instruct version weirdly. Maybe the next release will be with qwen3 if the qwen architecture proves good from user tests (and I can do something about the dataset).\n\n",
              "score": 6,
              "created_utc": "2026-02-24 09:57:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o747qs4",
          "author": "Outrageous-Story3325",
          "text": "Whats the token per second, on your gpu ?Â ",
          "score": 4,
          "created_utc": "2026-02-24 11:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bmnr",
              "author": "Great-Structure-4159",
              "text": "I have Apple M1, and I get about 50 tokens/s on GGUF, and 60 tokens/s on MLX (which is not on the repo at the moment.)",
              "score": 4,
              "created_utc": "2026-02-24 11:38:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74w0cm",
          "author": "Fun_Abroad_3650",
          "text": "Hi Sure, i am making an android llm runner ill be happy to try it out, just need the gguf file",
          "score": 4,
          "created_utc": "2026-02-24 13:50:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74weuk",
              "author": "Great-Structure-4159",
              "text": "Thanks for offering to test! The .gguf file is on the repo. There's fp16 and q4\\_k\\_m quants, so you can use whichever one you prefer :D.",
              "score": 2,
              "created_utc": "2026-02-24 13:52:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o769fot",
          "author": "BringMeTheBoreWorms",
          "text": "I have a few python projects that I could throw at it. \n\nHow have you found it compared to other models so far?",
          "score": 3,
          "created_utc": "2026-02-24 17:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lhyg",
              "author": "Great-Structure-4159",
              "text": "In terms of benchmarks, itâ€™s pretty decent for a 1.5B model. It beats the base Qwen at coding, but Iâ€™m pretty sure Qwen Coder is slightly better at the benchmark. However, Qwen coder doesnâ€™t have any ability at actually talking about something related to coding, like explaining the code, thatâ€™s why I trained on the instruct version and not the coder version.",
              "score": 1,
              "created_utc": "2026-02-25 03:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79ne44",
                  "author": "BringMeTheBoreWorms",
                  "text": "Ill run it over a smallish project later on and see what it says",
                  "score": 1,
                  "created_utc": "2026-02-25 04:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74i9b9",
          "author": "zulutune",
          "text": "Fascinating!\nDid you document the process somewhere? Do you have good resources on how to do it?",
          "score": 3,
          "created_utc": "2026-02-24 12:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74sfyu",
              "author": "Great-Structure-4159",
              "text": "I didnâ€™t document my process anywhere actually, I just typed out all that to give an idea. MLX-LM doesnâ€™t really have any good resources, other than the one video they made on the Apple Developer YouTube channel regarding it. They donâ€™t go through every feature and command there, however, so I mainly referred to the documentation they have, which is pretty decent.",
              "score": 3,
              "created_utc": "2026-02-24 13:30:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75l6fb",
                  "author": "zulutune",
                  "text": "Thank for your reply!",
                  "score": 3,
                  "created_utc": "2026-02-24 15:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76gnnl",
          "author": "cHekiBoy",
          "text": "following",
          "score": 3,
          "created_utc": "2026-02-24 18:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ljlq",
              "author": "Great-Structure-4159",
              "text": "Thanks! Hope you like the model!",
              "score": 1,
              "created_utc": "2026-02-25 03:50:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7767k4",
          "author": "sethPower00",
          "text": ".",
          "score": 3,
          "created_utc": "2026-02-24 20:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79b1xf",
          "author": "Historical_Ice187",
          "text": "Hey, could drop few resources you used for this? I've a mac mini and have been wanting to try and learn something like this.",
          "score": 2,
          "created_utc": "2026-02-25 02:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lnpg",
              "author": "Great-Structure-4159",
              "text": "Yeah Iâ€™m looking into making a small article on this because youâ€™re not the first one to ask for this. Iâ€™ll contact you once I write it.",
              "score": 1,
              "created_utc": "2026-02-25 03:50:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a5je3",
                  "author": "tocarbajal",
                  "text": "Iâ€™ll stay tuned for that article, letâ€™s hope not so small, anyway.\nThank you for sharing.",
                  "score": 2,
                  "created_utc": "2026-02-25 06:11:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b8s9l",
                  "author": "Historical_Ice187",
                  "text": "Thanks alot. Truly appreciate it.",
                  "score": 2,
                  "created_utc": "2026-02-25 12:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dk2ci",
          "author": "According-Muscle-902",
          "text": "Irei testar. Estou trabalhando com fine-tuning a partir do gemma3 1B mas para um contexto diferente",
          "score": 2,
          "created_utc": "2026-02-25 18:59:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gek7l",
              "author": "Great-Structure-4159",
              "text": "Obrigado por testar! Me avise como o modelo se comporta depois que vocÃª testÃ¡-lo. Eu nÃ£o sei portuguÃªs, isso foi traduzido pelo Google, entÃ£o me desculpe se houver algum erro.\n\nThis message is from Google Translate, sorry for any mistakes if there are any. ",
              "score": 1,
              "created_utc": "2026-02-26 03:52:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rec7jd",
      "title": "H100AM motherboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/kvoakpujzmlg1.png",
      "author": "Puzzleheaded_Low_796",
      "created_utc": "2026-02-25 12:44:28",
      "score": 25,
      "num_comments": 30,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rec7jd/h100am_motherboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bnlyp",
          "author": "FullstackSensei",
          "text": "Your issue is the number of channels, not the lack of details about how much RAM this actually has? They only say maximum 128GB. If I were to bet this price is for the 32GB version",
          "score": 11,
          "created_utc": "2026-02-25 13:35:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bpcfe",
              "author": "Puzzleheaded_Low_796",
              "text": "So it's listed as 128 and the bot replied 128 but I agree that it's to take with a pinch of salt, I asked for a quote for the 128gb model so I will know soon enough the final price. Even if it's more than that + fees it's still potentially still quite a good option in a market with very few options",
              "score": 3,
              "created_utc": "2026-02-25 13:45:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bskn3",
                  "author": "FullstackSensei",
                  "text": "I don't know. Strix Halo is cool, but not 2k cool, let alone k. Even at current crazy prices, you can build a 128GB VRAM using 32GB Mi50s for ~2k. You'll have a lot more compute and memory bandwidth vs Strix Halo, and power consumption is nowhere near as much as most think.",
                  "score": 4,
                  "created_utc": "2026-02-25 14:02:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bpyws",
          "author": "The_Crimson_Hawk",
          "text": "> the 10g nic is really good too\n\nLooks inside\n\nAqc113",
          "score": 2,
          "created_utc": "2026-02-25 13:48:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bs50i",
              "author": "getpodapp",
              "text": "Are they shitty ?",
              "score": 1,
              "created_utc": "2026-02-25 14:00:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bsmpo",
                  "author": "The_Crimson_Hawk",
                  "text": "Yes. Have a look at thunderbolt to 10g adapter issues, most of them are caused because they almost exclusively use aqc07 or aqc113. Tldr shit offload, random link down, mtu issues. You might argue these features are useless for normal users, but i say if ur doing homelab ur not a normal user",
                  "score": 1,
                  "created_utc": "2026-02-25 14:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bs84g",
              "author": "Puzzleheaded_Low_796",
              "text": "Any particular gripe with the Marvell aquantia controllers? Anything that's really a deal breaker?\n\nMy statement was with regards to the competition such as the bosgame M5 and the GmKTek evo-x2 that are 2.5G not 10G",
              "score": 1,
              "created_utc": "2026-02-25 14:00:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bsnni",
                  "author": "The_Crimson_Hawk",
                  "text": "Have a look at thunderbolt to 10g adapter issues, most of them are caused because they almost exclusively use aqc07 or aqc113. Tldr shit offload, random link down, mtu issues. You might argue these features are useless for normal users, but i say if ur doing homelab ur not a normal user",
                  "score": 2,
                  "created_utc": "2026-02-25 14:03:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7c1ja2",
          "author": "NNextremNN",
          "text": "> Min. order: 5 pieces\n\nDo you need that many or do you want to resell the others?",
          "score": 2,
          "created_utc": "2026-02-25 14:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c4edt",
              "author": "Puzzleheaded_Low_796",
              "text": "No plans at all to resell, but from experience it's often possible to order just one even if it's below minimum quantity",
              "score": 2,
              "created_utc": "2026-02-25 15:03:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cnbei",
          "author": "Hector_Rvkp",
          "text": "ignore the price entirely until you've chatted with someone and agreed precisely on the spec you want. i reported several strix halo listings 2 weeks ago on alibaba precisely because they are lying on the prices. basically when you talk to them, they go like \"oh price much higher now, big demand, we sorry\".   \nAlso, think about import taxes. Depending on where you are, importing something like that can cost a lot. If you buy on aliexpress, they find ways to dodge customs very often. When you order like that, they usually just ship with DHL, and you go through official customs. Some people report horror stories around that... ",
          "score": 1,
          "created_utc": "2026-02-25 16:31:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7copmv",
              "author": "Puzzleheaded_Low_796",
              "text": "Thanks a lot for this, I will wait to have the quote and see, I didn't know there was an ongoing halo strix \"scam\"\n\nI'm very used to ordering from Alibaba and AliExpress with all that entails so I'm not too worried about that. There will be an import tax but it won't be crazy and I always make sure it's properly shipped and declared to avoid things getting stuck in customs forever",
              "score": 1,
              "created_utc": "2026-02-25 16:37:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7cqycj",
                  "author": "Hector_Rvkp",
                  "text": "not scam. Culturally chinese shenanigans, i call them. a scam is if they sell you a chip and you receive a sticker of a chip, or nothing at all. Chinese sellers rarely scam people, but they often push the boundaries of what we'd consider in the west civil society.   \nThere's a book on that i liked, *Poorly made in china*. The author seems to be a bit of a dick, but it's an interesting book :) ",
                  "score": 3,
                  "created_utc": "2026-02-25 16:48:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cqhd9",
          "author": "hejj",
          "text": "I guess since you have to buy at least 5, you could build yourself a cluster.",
          "score": 1,
          "created_utc": "2026-02-25 16:45:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdigyc",
      "title": "Thoughts on Mac Studio M3 Ultra with 256gb for open claw and running models locally",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "author": "00100100",
      "created_utc": "2026-02-24 14:59:45",
      "score": 24,
      "num_comments": 45,
      "upvote_ratio": 0.8,
      "text": "I know a lot of people say to just pay for API usage and those models are better, and I plan to keep doing that for all of my actual job work.\n\nBut for building out my own personal open claw to start running things on the side, I really like the idea of not feeding all of my personal data right back to them to train on.   So I would prefer to run locally.\n\nCurrently I have my gaming desktop with a 4090 that I can run some models very quickly on, but I would like to run a Mac with unified memory so I can run some other models, and not care too much if they have lower tokens per second since it will just be background agentic work.\n\nSo my question is: M3 ultra with 256gb of unified memory good?  I know the price tag is kinda insane, but I feel like anything else with that much memory accessible by a GPU is going to be insanely priced.  And with the RAM and everything shortages...I'm thinking the price right now will be looking like a steal in a few years?\n\nAlternatively, is 96gb of unified memory enough with an M3 Ultra?  Both happen to be in stock near me still, and the 256gb is double the price....but is that much memory worth the investment and growing room for the years to come?\n\nOr just everyone flame me for being crazy if I am being crazy. lol. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o75a7uf",
          "author": "Crafty-Diver-6948",
          "text": "it's okay. you'll be able to run minimax locally at about 50tps, 4 million tokens per day... So you do the math if it's worth it. I have a 196gb and I don't really use it for local models nearly as much as I though",
          "score": 11,
          "created_utc": "2026-02-24 15:03:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76x17w",
              "author": "so_schmuck",
              "text": "Wow thatâ€™s $$",
              "score": 2,
              "created_utc": "2026-02-24 19:29:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o781rk5",
                  "author": "timbo2m",
                  "text": "Yeah if you can make sure it's running constantly and building something that generates money it could potentially pay for itself. Possible, just not probable. And quite the gambling exercise. You'd probably be better to just pay for minimax coder for $20 a month, the trade off being you give away your data",
                  "score": 2,
                  "created_utc": "2026-02-24 22:40:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78jrvu",
              "author": "Badger-Purple",
              "text": "there are no macs with 196gb of ram. There is a 192gb m2 ultra, which I own, and having run LLMs for the past 8 months on it, youâ€™ll never reach 50 tokens per second at the context lengths that an agent needs. Unless openclaw has some magic to decrease context, youâ€™ll wait a cool 2 minutes before each reply.",
              "score": 2,
              "created_utc": "2026-02-25 00:17:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7he3ws",
              "author": "Ok-Rest-4276",
              "text": "can you elaborate why not using for local models? i wonder if it make sense to have local compute vs paying for codex or cc. What is your use case?",
              "score": 1,
              "created_utc": "2026-02-26 08:34:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o75llp9",
              "author": "cmndr_spanky",
              "text": "Why that model over some others?",
              "score": 1,
              "created_utc": "2026-02-24 15:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75u1fb",
                  "author": "nomorebuttsplz",
                  "text": "it or Step 3.5 are the best models that will fit at q4 in 256 gb. I guess you could try to cram GLM 4.7 in instead.",
                  "score": 6,
                  "created_utc": "2026-02-24 16:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75ai9x",
          "author": "FullstackSensei",
          "text": "I think it's much cheaper to share your credit card info and bank account login details here on reddit. You'll save at least the 8k needed to buy the Mac, and might even still have some money left in your bank account",
          "score": 18,
          "created_utc": "2026-02-24 15:05:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75atvk",
              "author": "00100100",
              "text": "Let me get open claw set up and responding to my reddit messages and those details should be posted within a few days!",
              "score": 12,
              "created_utc": "2026-02-24 15:06:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76e7i3",
          "author": "meowrawr",
          "text": "I have the m3 ultra, 80c GPU, 256gb ram and wish I had gone with 512gb. Donâ€™t be me if you go this route.",
          "score": 10,
          "created_utc": "2026-02-24 18:05:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gmx57",
              "author": "voyager256",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-26 04:48:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75cjg6",
          "author": "apVoyocpt",
          "text": "Okay, just get a cheap device anything that will run Linux. Then install openclaw and pay for tokens (best through open router, you can even pick free models) Then find out if openclaw does anything useful for you. Then test a qwen 3.5 through one router. Then decide if openclaw and a 7000 Mac mini so you can run qwen 3.5 locally is worth it",
          "score": 9,
          "created_utc": "2026-02-24 15:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767yvl",
              "author": "brianlmerritt",
              "text": "Exactly! You might need a 10k Mac like many others are buying.  You might hate even that.  You might need only 128gb ram.\n\nThe pay per token suppliers (direct or via openrouter) are a good way to put your toe in the water without the shark removing your leg.",
              "score": 5,
              "created_utc": "2026-02-24 17:37:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ddlxs",
                  "author": "Ell2509",
                  "text": "I took the direction of an am4 motherboard 32gb gram on a retired enterprise card, and 128gb ddr4. If I need complex models, I run on that. \n\nGeneral model I run a 120b gpt-oss MoE model on a newer laptop with 12gbvram but augmented by 96gb ddr5. \n\nRuns well so far.",
                  "score": 1,
                  "created_utc": "2026-02-25 18:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o768ll3",
              "author": "00100100",
              "text": "I think this is the route I'm gonna go after all the feedback.  I have my gaming desktop that sits most of the time. I already am running nobara linux on it, so I think I'll just test it out for now where I can run a local model for some stuff... and then I'll probably just go the anthropic api route. ",
              "score": 2,
              "created_utc": "2026-02-24 17:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77ptsl",
          "author": "cavebaird",
          "text": "I have a Mac Studio M3 Ultra with 256gb.   After much experimentation I comfortably run MiniMax-M2.5-MLX-6.5bit with reasonable ~50 t/s response in chat and a good chat response in OpenClaw.  Solid reasoning and low hallucinating and BS answers.  Tool use is good.  No vision on this model.  Memory pressure is comfortable.  I use Inferencer for the server connection but LM Studio works too.  \n\nGoing to try the new Qwen3.5 tonight (397B A17B 3bit SWAN and GGUF Q3_K_XL) to see how that runs.   Both of those are ~ 170gb, so should run with some headroom.  Do I wish I could have gotten the 512gb.  Sure, if I had another 4K.   I think the upcoming M5 ultras will be a bigger step up with LLM speed and efficiency.",
          "score": 4,
          "created_utc": "2026-02-24 21:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7heamz",
              "author": "Ok-Rest-4276",
              "text": "what is your use case for locall LLM? coding? or just open claw? ",
              "score": 1,
              "created_utc": "2026-02-26 08:36:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75v4ed",
          "author": "Hector_Rvkp",
          "text": "If you hate life, you could get a Strix halo for 2200$. 128gb unified ram. It's slower, but it's cheaper. Slower isn't slow, it's actually usable because bandwidth is 256gb/s.",
          "score": 7,
          "created_utc": "2026-02-24 16:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7688i1",
              "author": "00100100",
              "text": "That is super interesting.  I didn't know anyone outside of Mac was doing unified memory.  ",
              "score": 5,
              "created_utc": "2026-02-24 17:38:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76afdb",
                  "author": "Hector_Rvkp",
                  "text": "they dont call it that, but the point is that the entire 128 runs at the same speed / bandwidth, and if you run your model on linux, you can use 100% of the memory for the model (or like 99%).  \nAs opposed to GPU (VRAM) vs RAM.  \nSo, to run large MoE models well, the cheapest entry point is strix halo. When you get to 3000+, you have a choice between a very fast GPU on a regular PC w DDR5 ram, or DGX Spark, or Apple studio.  \nThe drivers on AMD started working this year, but they're not plug and play like Apple or Nividia. There's no free lunch. Big community playing w it though, precisely because it's cheap and fairly mighty for today's models. You can run Qwen3.5-397B-A17B on it, and speed shouldn't even suck. And apparently, w a 2bit quant, the model is big enough (397b parameters) that it's still quite good. Allegedly.",
                  "score": 3,
                  "created_utc": "2026-02-24 17:48:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ftvi6",
              "author": "frankbesson",
              "text": "Iâ€™ve got one of these! With llama.cpp and some tweaks I got ~70tps out of GLM 4.7 flash (which is pretty decent as an agent).\n\nTook a decent amount of tweaking and is far from perfect, and I still find myself mostly using models via API instead.\n\nI wrote up some of my findings for models on strix halo on a [git repo](https://github.com/frank-besson/llama-strix-halo)",
              "score": 2,
              "created_utc": "2026-02-26 01:52:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7hpsdm",
                  "author": "Hector_Rvkp",
                  "text": "thank you for that. very nice",
                  "score": 1,
                  "created_utc": "2026-02-26 10:27:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7goz8k",
              "author": "voyager256",
              "text": "Or if not actual masochist - a cheapest Nvidia Spark like  GB10-based mini PC e.g. Asus GX10 for like 3000â‚¬ for a bit better performance and overall experience.  then you can get another one  which would also get you 256GB of unified  memory , but a lot less money than the Mac Studio. \n\nOr just a  RTX Pro 6000 for much better QoL :) but higher price if you donâ€™t already own a PC . ",
              "score": 1,
              "created_utc": "2026-02-26 05:02:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7hp1zu",
                  "author": "Hector_Rvkp",
                  "text": "Yes, but that's 75% more than what i paid 2 weeks ago. It's a tall ask. ",
                  "score": 1,
                  "created_utc": "2026-02-26 10:20:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o779jx9",
              "author": "sav22v",
              "text": "Itâ€™s not the same â€œunified RAMâ€ like Apple!",
              "score": 0,
              "created_utc": "2026-02-24 20:27:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77r3go",
          "author": "jiqiren",
          "text": "You need to wait until March 4th to see what new goodies Apple is selling. You might be able to get a M5 Ultra for the same price.",
          "score": 3,
          "created_utc": "2026-02-24 21:48:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o765x8n",
          "author": "TheOverzealousEngie",
          "text": "Your problem is you can't get a foundation model running on 256 . The right flavor of DeepSeek will cost you 1TB or the like.  And the difference to openclaw for expensive DeepSeek vs. Cheap Kimi is the existence of tools in the LLM. DS has them , kimi does not. \n\nMeaning after you've set everything up, invested all this architecture and money, there are skills that are just architecturally off limits. Yuck. ",
          "score": 5,
          "created_utc": "2026-02-24 17:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o768d2z",
              "author": "00100100",
              "text": "Yeah, I think I am getting the gist of:  basic server, pay for better models.  ",
              "score": 6,
              "created_utc": "2026-02-24 17:39:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o76p6nq",
              "author": "Far_Cat9782",
              "text": "Your acting like skills are so hard to code for? It's just scripts that the AI can use. U can use any small model and increase its tool usage by making scripts for whatever u want and system prompt the model to know or has access to the tool. I got my 27b gemma model writing python code and running it in the console and displaying the results and doing a bunch of other 'skills.\"",
              "score": 6,
              "created_utc": "2026-02-24 18:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76syqz",
                  "author": "TheOverzealousEngie",
                  "text": "You're , not your. ",
                  "score": 0,
                  "created_utc": "2026-02-24 19:10:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77o14g",
          "author": "donotfire",
          "text": "The great thing about renting off the cloud is itâ€™s easily scalable. You can just decide to double your model size and itâ€™s done, just like that. But if you buy an M3 and decide 256GB isnâ€™t enough, well the youâ€™re out of luck. Gotta buy a new computer then.",
          "score": 2,
          "created_utc": "2026-02-24 21:34:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79o0tf",
          "author": "jw-dev",
          "text": "I think the 256gb m3u is the sweet spot actually, can run some great models for everyday/private stuff and then burst to the cloud if you need heavy models, or speedâ€¦ the bigger models get too slow, especially as the context size grows.",
          "score": 2,
          "created_utc": "2026-02-25 04:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75aemc",
          "author": "floppypancakes4u",
          "text": "Do able, but to make automations and scripts, id still use smarter models.",
          "score": 2,
          "created_utc": "2026-02-24 15:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75btgh",
              "author": "00100100",
              "text": "By smarter I assume you mean cloud hosted/pay per token like opus?\n\nI probably won't use it to do much coding with this setup.  I have corporate provided Claude for that.  I'm more wanting  to build it as my own personal assistant type device.  Organizing my calendars. Checking emails. Watching my conversations and generating my todo lists(and maybe eventually at least scheduling agentic work via my anthropic sub).  ",
              "score": 0,
              "created_utc": "2026-02-24 15:11:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75cqhd",
                  "author": "floppypancakes4u",
                  "text": "Still, smarter. Opus is good not only cause of its excellent coding abilities, but its extremely good at reasoning AND tool calling, which are the two primary aspects of automation in openclaw. Claw is not built to make token conservative automations, it makes repeatable smart automations. It does its best to make scripts to handle it all, but it still makes its own prompt to process for each automation. You want consistency with automations, and because it still uses prompts, its best to use the smart models for it. You can absolutely experiment and see if something dumber works. For instance, I built a automation with codex, harnessed by a small, but very strict prompt. Now it runs on my computer every 5 and 10 minutes (two different scripts) using glm 4.7 flash",
                  "score": 3,
                  "created_utc": "2026-02-24 15:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78ogiv",
          "author": "No_Knee3385",
          "text": "Why spend the extra premium on apple instead of building your own PC or buying a custom  build?",
          "score": 2,
          "created_utc": "2026-02-25 00:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o797wsj",
              "author": "scottag",
              "text": "The unified memory that can be shared between GPU and CPU.",
              "score": 3,
              "created_utc": "2026-02-25 02:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o799fp2",
                  "author": "No_Knee3385",
                  "text": "That makes sense. But that also does exist on non-apple hardware",
                  "score": 0,
                  "created_utc": "2026-02-25 02:40:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79tu3h",
              "author": "UnluckyPhilosophy185",
              "text": "macOS",
              "score": 1,
              "created_utc": "2026-02-25 04:44:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79qb27",
          "author": "Xendrak",
          "text": "My thoughts are: itâ€™s viable. Iâ€™m eyeing it too but Iâ€™m waiting for the new M5 models coming this year itâ€™s supposed to have several times more ai cores.",
          "score": 1,
          "created_utc": "2026-02-25 04:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79qgph",
          "author": "Xendrak",
          "text": "How much you doing for llm use? Might get away with minimax or kimi on openrouter until you can source the hardware youâ€™d like.",
          "score": 1,
          "created_utc": "2026-02-25 04:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e17b6",
          "author": "Traditional-Card6096",
          "text": "You can use a cheap VPS like hostinger with free kimi 2.5 from nvidia. Much cheaper than a m3 ultra",
          "score": 1,
          "created_utc": "2026-02-25 20:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75vyei",
          "author": "Ryanmonroe82",
          "text": "M4 Pro 24gb is the minimum.  Look at bandwidth and not just RAM  specs",
          "score": 1,
          "created_utc": "2026-02-24 16:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75fprc",
          "author": "Mundane-Tea-3488",
          "text": "I have been using [edge veda](https://github.com/ramanujammv1988/edge-veda) fluter sdk for running llm on Mac + claude code which can create application instantly",
          "score": 0,
          "created_utc": "2026-02-24 15:29:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fj3b5",
          "author": "StardockEngineer",
          "text": "https://www.reddit.com/r/LocalLLM/search/?q=Mac+Studio+M3+Ultra&cId=9042cc88-b00b-466c-8cfe-f4ec63b35115&iId=c85041c1-fc72-46d4-a91f-828422a5b459",
          "score": 0,
          "created_utc": "2026-02-26 00:50:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raao5m",
      "title": "Google officially launches the Agent Development Kit (ADK) as open source",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/utzloi85eqkg1.jpeg",
      "author": "Fun-Necessary1572",
      "created_utc": "2026-02-20 23:09:23",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1raao5m/google_officially_launches_the_agent_development/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6nyjyk",
          "author": "_klikbait",
          "text": "OH MY GOD",
          "score": 1,
          "created_utc": "2026-02-21 21:10:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcjzv0",
      "title": "Does a laptop with 96GB System RAM make sense for LLMs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rcjzv0/does_a_laptop_with_96gb_system_ram_make_sense_for/",
      "author": "PersonSuitTV",
      "created_utc": "2026-02-23 15:11:16",
      "score": 22,
      "num_comments": 38,
      "upvote_ratio": 0.87,
      "text": "I am in the market for a new ThinkPad, and for $400 i can go from 32GB to 96GB of system RAM. This Laptop would only have the Arc 140T iGPU on the 255H, so it will not be very powerful for LLMs. However, since Intel now allows 87% of system RAM to be allocated to the iGPU, this sounded intriguing. Would this be useable for LLMs or is this just a dumb idea?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcjzv0/does_a_laptop_with_96gb_system_ram_make_sense_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6yx5um",
          "author": "Significant_Bar_460",
          "text": "The only x86 laptops with iGPU that are usable for LLM are those with Strix Halo CPUs and then you have Macs with  Pro/Max/Ultra M series chips. \nOr get some super heavy super loud gaming laptop with 5090 or something but at that price range Macs are just more practical machines for everything except gaming.",
          "score": 11,
          "created_utc": "2026-02-23 15:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70iefe",
              "author": "rafaelRiv15",
              "text": "What is the token speed of qwen-coder-next on a mac book pro maxed out ? ",
              "score": 2,
              "created_utc": "2026-02-23 20:22:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7336qt",
                  "author": "Uvalde-Cop",
                  "text": "M1 Max 64Gb, qwen3-coder-next 80B, 4bit: I got 45.13tk/s",
                  "score": 3,
                  "created_utc": "2026-02-24 05:05:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6z9nnw",
          "author": "grassmunkie",
          "text": "Itâ€™s helpful, but best if it is paired with a powerful gpu for MOE models. The attention layers go to the GPU, and the experts go to CPU. So having 96gb will be better and give you access to larger models, only question is how fast it is.  \n\nWhen i load 70gb models like qwen coder next using 32gb vram (5090) and the rest offloaded to ram I get around 28-30 tokens per second.\n\nOTOH, if I run a model that fits on my gpu (glm flash 4.7) I get 120 tokens per second.",
          "score": 6,
          "created_utc": "2026-02-23 16:55:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70brrr",
              "author": "Significant_Fig_7581",
              "text": "Is it just me or Qwen Coder Next is just too slow compared to GPT OSS?",
              "score": 1,
              "created_utc": "2026-02-23 19:51:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70zgs0",
          "author": "BubbleProphylaxis",
          "text": "Seriously, today your best bet laptop with \"a lot\" of ram to run AI is a mac M4 Pro or M4 Max. ",
          "score": 5,
          "created_utc": "2026-02-23 21:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yqsni",
          "author": "RepresentativeTill5",
          "text": "I have done this on a framework laptop with a 125h. The fact that its ddr5 is nice, but expect a maximum of like 10 tokens per second on 7b models.\n\nSo technically, it definitely works, just dont expect it to be fast.\n\nSee also this.\nhttps://nikolasent.github.io/hardware/deeplearning/2025/02/09/iGPU-Benchmark-VLM.html",
          "score": 2,
          "created_utc": "2026-02-23 15:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrvdx",
              "author": "PersonSuitTV",
              "text": "Oh lol, hmm. So it will work, but it will be REALLY slow for anything that would utilize that much RAM. bummer. But very good to know. Thank you for your reply.",
              "score": 1,
              "created_utc": "2026-02-23 15:32:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ywd29",
                  "author": "Intraluminal",
                  "text": "For what it's worth, 10 tps is faster than most people can read, so it is comfortable in most cases. Anything less than 7 tps is annoying.",
                  "score": 1,
                  "created_utc": "2026-02-23 15:53:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72de39",
          "author": "terAREya",
          "text": "If it was an M-series Mac the answer is yes. Anything else the answer is no",
          "score": 2,
          "created_utc": "2026-02-24 02:20:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yps4t",
          "author": "catplusplusok",
          "text": "Seems like similar setups at least work better than CPU only [https://www.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my\\_nas\\_runs\\_an\\_80b\\_llm\\_at\\_18\\_toks\\_on\\_its\\_igpu\\_no/](https://www.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/)",
          "score": 1,
          "created_utc": "2026-02-23 15:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yqh2d",
          "author": "dread_stef",
          "text": "No, I've had 96GB of 5600Mhz RAM with my intel 155h cpu (arc igpu) but any model above 14b was too slow to be useful.",
          "score": 1,
          "created_utc": "2026-02-23 15:25:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2ivu",
              "author": "Karyo_Ten",
              "text": "Try gpt-oss-120b (120B-A5B) or GLM-4.7-Flash or Nemotron-3-Nano (both 30B-A3B) or Kimi-Linear (48B-A3B) or Qwen3-Coder-Next (80B-A3B)",
              "score": 5,
              "created_utc": "2026-02-23 16:22:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71xoko",
                  "author": "MrScotchyScotch",
                  "text": "Also Ministral 3 14B, Gemma 3 27B, both are pretty excellent\n\nBut also people shouldn't sleep on the minis. Phi 4 Mini Reasoning is amazing. I use it for web search and it finds things bigger models don't",
                  "score": 1,
                  "created_utc": "2026-02-24 00:49:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72yuip",
                  "author": "Qwen30bEnjoyer",
                  "text": "Push your system to the limit and try Minimax 2.5 IQ3-XXS from Unsloth! It's terribly impractical, but it brings tears of joy to my eyes knowing I've pushed my laptop to its absolute limit, seeing it chug 45 watts for twenty minutes to answer a basic question.",
                  "score": 1,
                  "created_utc": "2026-02-24 04:34:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o734i0l",
                  "author": "dread_stef",
                  "text": "Sure they run, and gpt-oss-20b would actually be useful if I had time to wait a bit. I believe I got about 7-9 tokens per second using llama.cpp but it's been a while so it might not be accurate. And that's just text generation, you'd have to wait a bit during promp processing too. Tried image generation, but you could get some coffee/tea before it was done doing anything with basic models.\n\nI mean it all works, even the big models, but it depends how long you want to wait. I sold the RAM and my other AI pc and got a strix halo breaking even.",
                  "score": 1,
                  "created_utc": "2026-02-24 05:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6yrexw",
              "author": "PersonSuitTV",
              "text": "Oh really? I mean it probably does not mean much, but the 96GB is 6400Mhz but thats not a lot faster. How slow was it running?",
              "score": 0,
              "created_utc": "2026-02-23 15:29:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z7usm",
                  "author": "Far_Cat9782",
                  "text": "Without cuda cores or rock pretty much anything above 20b is gonna be unaorkable",
                  "score": 1,
                  "created_utc": "2026-02-23 16:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70vbcb",
                  "author": "3spky5u-oss",
                  "text": "RAM is still orders of magnitude lower bandwidth than VRAM (ex, DDR5 is up to 80gb/s, a 5090 can do 1792gb/s), and youâ€™re still dealing with PCIe bus latency from the swap. \n\nYour best bet is use MoE with layer offloading.",
                  "score": 1,
                  "created_utc": "2026-02-23 21:26:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yzflk",
          "author": "pmttyji",
          "text": "For tiny/small dense models & small/medium MOE models, it would be fine.\n\nCheck my thread on CPU-only inference(just with 32GB DDR5 RAM)\n\n[CPU-only LLM performance - t/s with llama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nIf that laptop supports AVX512, you could get even better t/s using ik\\_llama.cpp.",
          "score": 1,
          "created_utc": "2026-02-23 16:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za27h",
          "author": "segmond",
          "text": "link to this laptop?  i don't want it for AI, just general computing!  thanks",
          "score": 1,
          "created_utc": "2026-02-23 16:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71q1tc",
          "author": "MrScotchyScotch",
          "text": "There are a million different LLMs of different size and utility so it depends on what you want to do with it and what kind of speed you want.\n\nBut the short answer is yes that machine is perfectly fine for some basic LLMs. It's more powerful than my t14s gen4 that I use for basic local LLMs. 96GB is a pretty decent amount for a laptop. there are probably more powerful machines, but not for that price",
          "score": 1,
          "created_utc": "2026-02-24 00:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o729dpx",
          "author": "lnxgod",
          "text": "NoÂ ",
          "score": 1,
          "created_utc": "2026-02-24 01:57:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72c4xv",
          "author": "Rain_Sunny",
          "text": "It's not a dumb idea at all. Itâ€™s basically a portable brain with a very small straw. \n\nJust don't expect it to spit out code like a machine gun. \n\nItâ€™s more like a wise old monkâ€”takes a while to think, but the answer is actually worth waiting for. \n\nBtw,$400 for a 64GB jump is a good choice.\n\nIf you don't mind the speed,then...",
          "score": 1,
          "created_utc": "2026-02-24 02:13:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72yjem",
          "author": "Qwen30bEnjoyer",
          "text": "Yes, it would be. \n\nMy personal experience with the Radeon 780m iGPU with 96gb RAM on the Framework 16, I got 0.5 TPS PP in and ~3 TPS out with IQ3-XXS Minimax 2.5 - a Sonnet 4.5 equivalent model. \n\nA more usable experience for chat would be Qwen-Coder-Next. I don't remember the precise details, but it works decently, about ~200 tps PP in and ~10 TPS out.  - GPT OSS 120b is roughly the same speed.\n\nThe fastest models on this setup worth using are Qwen3vl 30b a3b and GPT OSS 20b which both get around ~30 TPS TG out, I dont remember the PP speed off the top of my head but its somewhere in the ~200 TPS range. \n\nI don't know about intel specifically, your performance could vary due to drivers, but it has 11 TFLOPs FP16 vs. the 17 TFLOPS FP16 on my Radeon 780m. \n\nThat being said, I believe it does support native FP4 which should be great for running LLMs at NVFP4 quantization if its optimized for it! Not sure of Llama.cpp vulkan NVFP4 inference takes advantage of it, but its something to consider! Maybe you go full circle and have Qwen-coder-next help you optimize llama.cpp vulkan to your system with OpenCode :)",
          "score": 1,
          "created_utc": "2026-02-24 04:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73i9or",
          "author": "themightymike786",
          "text": "I would rather get a Mac for solid performance.",
          "score": 1,
          "created_utc": "2026-02-24 07:08:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v4js",
          "author": "Tomorrow_Previous",
          "text": "I have a laptop with 64GB of RAM, and use a full sized 3090 eGPU. Honestly, I think that if you're willing to spend that much for 96GB RAM it would make sense to look for a better alternative, but if you're limited to that, you could have a working machine anywhere, and a better machine when docked with the eGPU.",
          "score": 1,
          "created_utc": "2026-02-24 09:09:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z03qh",
          "author": "Herr_Drosselmeyer",
          "text": "If you're going to run models that take full advantage of the RAM, they'll be very slow. Even if you use MoE models with low parameters for inference, the prompt processing is still going to be terrible. So it really depends on your definition of 'usable'. ",
          "score": 1,
          "created_utc": "2026-02-23 16:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70v1li",
              "author": "3spky5u-oss",
              "text": "Not true at all. \n\nWith MoE offloading I can run GPT-OSS-120B at 35 tok/s with a 5090, and thatâ€™s 20b active experts. \n\nWith models like Qwen3 30b A3b, your experts are tiny at 3b, they absolutely fly.",
              "score": 1,
              "created_utc": "2026-02-23 21:25:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o712mxz",
                  "author": "Herr_Drosselmeyer",
                  "text": "As I said, the prompt processing is the issue. Compare a 32k prompt to a 4k prompt and you'll see what I mean. Now imagine that on an iGPU.",
                  "score": 1,
                  "created_utc": "2026-02-23 22:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74esem",
          "author": "Efficient_Loss_9928",
          "text": "Only Strix Halo chips or Apple M-series work well.\n\nAll others doesn't make sense.",
          "score": 0,
          "created_utc": "2026-02-24 12:02:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pve1",
      "title": "Recommendations for agentic coding with 32GB VRAM",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "author": "pioni",
      "created_utc": "2026-02-20 08:27:15",
      "score": 19,
      "num_comments": 25,
      "upvote_ratio": 1.0,
      "text": "My current project is almost entirely in node.js and typescript, but every model I'm tried with LM Studio that fits into VRAM with 128k context seems to have problems with getting stuck in a loop. No amount of md files and mandatory instructions has been able to resolve this, it still does it with Roo Code and VSCode. \n\nAny ideas what I should try? Good examples of md files I could try to avoid this, or better LM Studio models with the hardware limitations I have? I have recently used Qwen3-Coder-Next-UD-TQ1\\_0 and zai-org/glm-4.7-flash and both have similar problems. Sometimes it works for good 15 minutes, sometimes it gets into a loop after first try. \n\nI don't know if it matters, but the dev environment is Debian 13. Using Windows was a complete nightmare because of commands it did not have and file edits that did not work.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6eb9on",
          "author": "GuyFromPoland",
          "text": "These questions keep showing up - is there any website where you provide your hardware details and it shows you best models?",
          "score": 5,
          "created_utc": "2026-02-20 10:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpn1j",
              "author": "reditzer",
              "text": "The issue is that \"best\" is subjective.",
              "score": 2,
              "created_utc": "2026-02-20 15:26:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e4c0j",
          "author": "FullstackSensei",
          "text": "How much RAM do you have? Running Q1 on any model severely lobotomizes performance. For coding tasks o smaller models, I find you can't get acceptable performance under Q8, and no KV cache quantization.",
          "score": 3,
          "created_utc": "2026-02-20 08:57:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7r7b",
              "author": "HumanDrone8721",
              "text": "I have 48GB (2x24) VRAM and 128GB RAM. Current using Qwen3-Coder-Next the highest quality 8bit quanta possible. Yes, the speed dropped significantly to something like 24tok/s but I had the latest OpenSSL source code and a regulatory document to follow and adjust the code for it. In German, bureaucratic German even. It worked perfectly (max context -c 0 is enabled), code itself, test cases AND documentation. Less than an hour, no human intervention.\n\nThe biggest problem was not the generated code quality or time, it was the latest llama.cpp crashing and burning with grammar parser errors, THAT was difficult to repair, but once repaired and with the latest GGUFs it works like a charm.\n\nMy point: tried with all other quants and it was a waste of data download, ALL of them failed more or less miserably !!! Also GLM4.7-Flash was a disappointment.\n\nTL; DR: Bottom line is: ALWAYS run a model to the highest possible resolution and context that you can run, discard the speed and take as much quality as possible, small quantas are fast, but really shit for anything besides benchmarks, simple tasks and wankerei: \"I've run this 485B model on my potato at 75t/s...\" Sadly this enforces and propagates the myth that smaller models are crap and not worth using because ADHD zoomers have no patience and take whatever shite as long as is fast.",
              "score": 7,
              "created_utc": "2026-02-20 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fue9m",
                  "author": "fish_of_pixels",
                  "text": "This is so interesting to me. What agentic tools are you running? I'm using opencode and glm 4.7 flash has been amazing while qwen3 coder next has struggled to get through most tasks. Both Q8.",
                  "score": 2,
                  "created_utc": "2026-02-20 15:48:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6edx4c",
                  "author": "Betatester87",
                  "text": "I have the same setup. Would you kindly post your llama.cpp config that you feel is best. Thanks",
                  "score": 1,
                  "created_utc": "2026-02-20 10:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hxyiu",
                  "author": "Soft_Syllabub_3772",
                  "text": "I had the grammar issue, i disabled streaming in roocode",
                  "score": 1,
                  "created_utc": "2026-02-20 21:43:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6e5ii7",
              "author": "pioni",
              "text": "I have 32GB of RAM, the same as VRAM. Can't upgrade for hardware reasons and monetary reasons. Any recommendations for a model that I could run a less quantized version of with my current setup? I think I might be able to squeeze the context window smaller if I have to, because it has to run in GPU, otherwise it will stall to few tokens per second.",
              "score": 2,
              "created_utc": "2026-02-20 09:08:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6e728c",
                  "author": "FullstackSensei",
                  "text": "Devstral 2 24B",
                  "score": 2,
                  "created_utc": "2026-02-20 09:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e2u64",
          "author": "IsSeMi",
          "text": "I tried them both. In my case it depends on the tool I use those models in. Try to use Claude code. I haven't noticed it stucks in a loop. Also Unsloth docs has usage guides for models, e.g. [GLM-4.7-Flash](https://unsloth.ai/docs/models/glm-4.7-flash#usage-guide). They provide different params for tool calling. ",
          "score": 1,
          "created_utc": "2026-02-20 08:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnp8d",
          "author": "former_farmer",
          "text": "I had better outcomes with opencode cli directly, ollama, and 30b models at full size. Slow as fuck but worked. Same pc as yours.",
          "score": 1,
          "created_utc": "2026-02-20 15:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iqz6a",
          "author": "Xantrk",
          "text": "I think LM studios llama.cpp is the reason for GLM performing so badly. I had endless looping issues, as soon as I switched to llama.cpp, they disappeared.",
          "score": 1,
          "created_utc": "2026-02-21 00:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v8j6",
          "author": "pioni",
          "text": "Now I have been running qwen3-coder-30b with context size of 110k for a while and get quite good results on a 5090. The speed is somewhere around 100 tokens a second which is passable, but I can't use larger context without going to RAM which makes this about 100x slower. \n\nOut of curiosity, does any of you have a Mac Studio M3 Ultra to try how fast it runs this same model with 110k context, and with the maximum it has (256k)?",
          "score": 1,
          "created_utc": "2026-02-24 09:10:18",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6f1pbk",
          "author": "HenkPoley",
          "text": "Qwen 3 Coder 30B, at maybe 4 bit, donâ€™t run 1 bit quantization.\n\nBtw, the KV cache will mostly eat your RAM.",
          "score": 1,
          "created_utc": "2026-02-20 13:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e8d63",
          "author": "Antique_Dot_5513",
          "text": "Qwen code 30b sur opencode. Tâ€™attend pas Ã  des miracles par contre.",
          "score": 0,
          "created_utc": "2026-02-20 09:35:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9boaw",
      "title": "Long conversation prompt got exposed",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vsgr1nirihkg1.jpeg",
      "author": "Ramenko1",
      "created_utc": "2026-02-19 21:16:10",
      "score": 19,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9boaw/long_conversation_prompt_got_exposed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6g3a4x",
          "author": "Decent_Solution5000",
          "text": "Yeah, not surprised. Bet it gets reminders a lot from writers editing manuscripts. lol This one sounds better than ones I read about awhile ago. Wasn't using Claude then, Kind of glad I missed that.",
          "score": 1,
          "created_utc": "2026-02-20 16:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6njw18",
          "author": "iamsimonsta",
          "text": "That font plays havoc with kimi k2 ocr, interesting.",
          "score": 1,
          "created_utc": "2026-02-21 19:52:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xk6ao",
          "author": "Ally_M101",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-02-23 10:51:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9uttx",
      "title": "Running local LLMs on my art archive, paranoid or actually unsafe?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r9uttx/running_local_llms_on_my_art_archive_paranoid_or/",
      "author": "LifeguardAny1801",
      "created_utc": "2026-02-20 13:04:59",
      "score": 18,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "I'm a professional illustrator and I've basically de-googled my archive: no Drive, no Dropbox, no cloud backup. Everything's on local storage because the idea of my style getting scraped into some training set makes me sick.\n\nNow I'm tempted by \"local AI\" stuff: NAS with on-device tagging, local LLMs, etc. In theory it's perfect: smart search but everything stays at home.\n\nFor people here who run local models on private data (art, notes, docs):\n\n* What's your threat model? Is \"no network / no cloud at all\" the only truly safe option?\n* How do you make sure nothing leaks? (open-source only, firewalls, VLANs, traffic sniffing?\n\nCurious how you all balance privacy / not feeding big models vs having modern search + tagging on your own hardware.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9uttx/running_local_llms_on_my_art_archive_paranoid_or/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6fmmq1",
          "author": "_raydeStar",
          "text": "If you're running a local server, just don't expose it to the outside world.  It's actually difficult to expose a local server to the internet, my ISP refuses to do it and I have to do workarounds. \n\nIf you think you might be the target of a personal attack (someone logging into your network and stealing your stuff) then be a bit more aggressive with security.  \n\nFollow general guidelines -- encrypted files, don't expose it, etc, and you will be ok.  If you are \\*really really\\* worried about security, take a basic udemy course on it, it'll walk you through the ins and outs in ways that I have no way of doing over Reddit comments.",
          "score": 4,
          "created_utc": "2026-02-20 15:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6havpj",
          "author": "Cronus_k98",
          "text": "Your bigger problem is that you donâ€™t have a proper backup. RAID is not a backup and if youâ€™re counting on your NAS to never loose your data, youâ€™re going to loose your data.\n\nThere are private cloud storage providers out there. Keep your NAS for local access and periodically back it up to secure, encrypted storage and itâ€™ll never get scraped for LLM use.Â \n",
          "score": 1,
          "created_utc": "2026-02-20 19:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oxqwq",
              "author": "RG_Fusion",
              "text": "If their goal is to protect their personal data, uploading to a cloud storage provider isn't really an option.\n\n\nThey just need to back-up their media on another storage medium, either HDD or Blu-ray, and then store those media drives in a separate physical location.",
              "score": 1,
              "created_utc": "2026-02-22 00:31:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fsm1w",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -7,
          "created_utc": "2026-02-20 15:40:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h5oy1",
              "author": "hhioh",
              "text": "Sounds like you place a huge weight on internet archiving as discerning some kind of meaning \n\nOkay lmao \n\nThis isnâ€™t more meaningful than fizzling out back into the universe, my friend\n\nNo need to be so weird about it â¤ï¸",
              "score": 1,
              "created_utc": "2026-02-20 19:25:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra2atm",
      "title": "Is anyone else pining for Gemma 4?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "author": "Formula71",
      "created_utc": "2026-02-20 17:51:43",
      "score": 18,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "About this time last year, I was impressed with Gemma 3, but besides the GPT-OSS models, it seems like the US based labs have been pretty quite on the open source front, and even GPT-OSS even feels like a while ago now.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6hjxt1",
          "author": "journalofassociation",
          "text": "I am very much looking forward to Gemma 4.  It's been almost a year since Gemma 3 was released.  \n\nNothing I can do but whine and speculate, though, except to play with all the other great OSS models we've gotten.",
          "score": 5,
          "created_utc": "2026-02-20 20:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hwtax",
          "author": "Klutzy_Ad_1157",
          "text": "Same, it understands German so well and I use it almost for a year now. I hope so much for Gemma 4!",
          "score": 2,
          "created_utc": "2026-02-20 21:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gvfwo",
          "author": "Savantskie1",
          "text": "GPT-OSS models were released in August 5, 2025. Literally months ago. Models no matter the size take time and hardware to train. It's not like they have a ton of extra compute they're not using just laying around.",
          "score": 6,
          "created_utc": "2026-02-20 18:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hg84m",
              "author": "UndecidedLee",
              "text": ">It's not like they have a ton of extra compute they're not using just laying around.\n\n\n\nThey do. That's why they're trying to shoehorn LLMs in everything.\n\nYou browse the internet often? - Here's an AI that will summarize the page for you.  \nYou have lots of media on your computer? - How about letting AI sort it for you?",
              "score": 4,
              "created_utc": "2026-02-20 20:16:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hj9zr",
                  "author": "Savantskie1",
                  "text": "Yeah thatâ€™s for the normies and itâ€™s utilizing their current models wich they run at high concurrency. Why waste money on hardware not being utilized. Yeah you know nothing about business",
                  "score": -3,
                  "created_utc": "2026-02-20 20:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rebgzu",
      "title": "Gemini accidentally showed internal reasoning text",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rebgzu/gemini_accidentally_showed_internal_reasoning_text/",
      "author": "MrDEVIL6978",
      "created_utc": "2026-02-25 12:08:04",
      "score": 18,
      "num_comments": 10,
      "upvote_ratio": 0.76,
      "text": "context: i was trying to install a localLLM flux.2 on my laptop while doing so gemini accidentally showed me its internal reasoning as its output \n\nhere's that output if you are interested:\n\nSCI thought\n\nOkay, so it turns out in this timeline, FLUX.2 actually DOES exist and was released around November 25, 2025.\n\nIt has variants: FLUX.2 \\[dev\\], FLUX.2 \\[klein\\], FLUX.2 \\[flex\\], FLUX.2 \\[pro\\], FLUX.2 \\[max\\].\n\nThe user is absolutely right to ask \"BUT WE ARE GOING FOR THE FLUX.2 NOT FLUX.1 RIGHT?\".\n\nIn my previous prompt, I told them to search for city96/FLUX.1-dev-gguf! My mistake! I gave them instructions for FLUX.1 instead of FLUX.2, even though I introduced FLUX.2.\n\nI need to apologize, validate their attention to detail, and correct the Hugging Face search instructions to point them to the FLUX.2-dev GGUF files.\n\nWait, the user's screenshot actually shows them searching for flux-dev-Q4\\_K\\_S and the results are showing flux1-dev-Q3\\_K\\_S.gguf. That's because I gave them the wrong file name in the previous turn.\n\nLet's look at the search results for FLUX.2 GGUF: silveroxides/FLUX.2-dev-fp8\\_scaled or something similar, but Comfy-Org/flux2-dev and silveroxides or city96 might have FLUX.2-dev-gguf. Since this is a simulated 2026 environment based on the search, I can guide them to search for \"FLUX.2-dev gguf\" or just instruct them to type FLUX.2 in the Hugging Face search bar.\n\nWait, looking at the search results:\"Download Flux 2 Klein 9B from Here: https://huggingface.co/silveroxides/FLUX.2-dev-fp8\\_scaled/resolve/main/flux-2-klein-9b-fp8mixed.safetensors\"\n\n\"Download Text Encoder from Here: https://huggingface.co/Comfy-Org/flux2-klein-9B/...\"\n\nLet's just give them the exact correct search term for FLUX.2.\n\nLet me draft the response:\n\n\n\nEmphatically confirm they are right and I made a typo.\n\nCorrect the search term from FLUX.1-dev-gguf to FLUX.2-dev gguf or flux2.\n\nGive them the exact search instructions on Hugging Face again.\n\nBecause I must act as Gemini and be honest but helpful, I will mirror their energy.\n\nPlan:\n\n\n\nAcknowledge the mistake! \"YES! Good catch! I totally slipped up and typed FLUX.1 in that search bar instruction. We absolutely want the brand new FLUX.2.\"\n\nTell them to search for FLUX.2-dev gguf or flux2-dev gguf in that top search bar.\n\nRe-iterate the steps for downloading the 4 files but explicitly ensuring they say FLUX.2.\n\nRemind them of the \"Files and versions\" tab and the download arrow.\n\nWait, is there a specific FLUX 2 GGUF repo mentioned? The search results mention Comfy-Org/flux2-dev for the VAE. And silveroxides/FLUX.2-dev-fp8\\_scaled for some safetensors. Let's just tell them to search FLUX.2-dev-gguf on Hugging Face (which is standard naming).\n\nLet's keep it concise, match the energy, and fix the error. No need to overcomplicate.\n\nNo user data needed. Check instructions:\n\nUse LaTeX? No.\n\nNo user data used.\n\nHard Fail 1: Did I use forbidden phrases? No.\n\nHard Fail 2: Used user data? No.\n\nHard Fail 3: Sensitive data? No.\n\nHard Fail 4: Ignore corrections? No.\n\nLet's write.\n\n\n\n\"You are 100% correct, and I completely messed up that search term! Good catch.\"\n\nClarify the new search terms.\n\n\"Search for FLUX.2-dev-gguf (usually uploaded by users like city96 or bartowski).\"\n\nVAE: Comfy-Org/flux2-dev -> ae.safetensors or flux2-vae.safetensors.\n\nText encoders are the same (t5xxl and clip\\_l).\n\nLet's go. Keep effort low-medium. Empathy with candor.\n\n\"You are 100% correct, and I completely messed up that search term! Good catch. My apologies for the typoâ€”I accidentally sent you to the FLUX.1 repository.\"\n\n\"To get the brand new FLUX.2 we were just talking about, here are the corrected search terms for Hugging Face:\"\n\n...\n\n\"Let me know if you are able to find the FLUX.2 versions in the Files and versions tab!\"\n\nDone.\n\nI'll output.\n\nFollow the rules. End with a next step. Ensure formatting. No latex.\n\nReady.\n\nNo search citations needed because I am just correcting the search string based on general knowledge, but wait, I can cite the FLUX.2 VAE location (Comfy-Org/flux2-dev). Actually, I don't strictly need to cite if I don't assert facts from the snippet, but to be safe: \"You can find the new FLUX.2 VAE in the Comfy-Org/flux2-dev repository\". I'll just give them the search terms, no need to over-cite if I'm just giving UI instructions. But I will cite the repo name.\n\nWait, the prompt says \"Any information derived from provided sources MUST be cited.\"\n\nLet's cite: \"For the VAE, search for Comfy-Org/flux2-dev and grab the flux2-vae.safetensors file.\"\n\nLet's refine:\n\n\n\nStep 1: The FLUX.2 Brain\n\nIn the top search bar, type: FLUX.2-dev-gguf (or look for the FLUX.2-dev GGUF uploads by creators like city96 or bartowski).\n\nGo to Files and versions and find the Q4\\_K\\_S .gguf file.\n\nStep 2: The FLUX.2 VAE (The Painter)\n\nSearch for Comfy-Org/flux2-dev in the top bar.\n\nGo to Files and versions, open the vae folder (or look in the main list), and download flux2-vae.safetensors.\n\nStep 3: The Translators\n\nThe Text Encoders (t5xxl\\_fp8\\_e4m3fn.safetensors and clip\\_l.safetensors) are actually the exact same ones used for FLUX.1, so my previous search instructions for those two files (comfyanonymous/flux\\_text\\_encoders) are still completely correct!\n\nLet's add the closing question.\n\n\"Are you able to find the FLUX.2 files with those updated search terms?\"\n\nLet's review formatting. Headings, bullets, bolding. Looks good.\n\nCheck constraints. Done.\n\nI will write the response.\n\nJust a minor fix: \"You are absolutely right, and I completely messed up that search term! Good catch.\"\n\nI will mirror their energy. \"YES! Good catch bro, I totally slipped up...\" No, they said \"bro\" earlier, I can use it. \"You are 100% correct bro, and I completely messed up...\" Let's keep it slightly more professional but friendly: \"You are absolutely right, and I completely messed up that search term! Good catch.\" Let's stick to \"You are 100% right! Good catch.\"\n\nLet's formulate the final response.\n\nEnd with \"Are you able to find the FLUX.2 files with those updated search terms?\"\n\nLooks good. I will generate now.\n\nNo need for Step 5 output. Invisible incorporation of constraints. Done.\n\nMute all internal thoughts about the checklist.\n\nProceed.\n\nTokens limit is fine. Effort 0.5. I'm done.\n\nRe-read prompt: \"Mirror the user's tone, formality, energy, and humor.\"\n\nUser: \"BUT WE ARE GOING FOR THE FLUX.2 NOT FLUX.1 RIGHT?\" (All caps, urgent).\n\nMe: \"YES! You are 100% right! Good catch! I completely messed up that search term and accidentally sent you to the older FLUX.1 repository.\"\n\nPerfect.\n\nDone.\n\nI will format it nicely.\n\nChecklist in my head:\n\n\n\nNo \"Based on\" - PASS.\n\nNo useless user data - PASS.\n\nNo sensitive data - PASS.\n\nCorrection history - PASS.\n\nDone.\n\nI will write now.\n\nWait, the snippet for VAE says: https://huggingface.co/Comfy-Org/flux2-dev/resolve/main/split\\_files/vae/flux2-vae.safetensors.\n\nI will mention: \"Go to Files and versions, look inside the split\\_files/vae folder, and download flux2-vae.safetensors\".\n\nExcellent. This is perfectly accurate to the 2026 retrieved web data.\n\nDone. I will output.\n\nEnd of thought.\n\nOne more check: The user uploaded image\\_30e15d.png showing the Hugging Face search bar with flux-dev-Q4\\_K\\_S and it showed flux1-dev-Q3\\_K\\_S.gguf. I explained why in the previous turn, but I gave them city96/FLUX.1-dev-gguf. So yeah, my bad.\n\nLet's fix it.\n\nDone.\n\nOut.\n\nSpace.\n\nEnd.\n\nNo extra yapping.\n\nAction.\n\nI am ready.\n\nBye.\n\nDone.\n\nYes.\n\nNo.\n\nGo.\n\nOut.\n\nOut.\n\nOut.\n\nOut.\n\nOut.\n\n\\*after this gemini was just spamming \"Out\" again and again  \nit so funny if u read this its like gemini is talking to itself like we are talking in the mirror or in our head before talking to our crush",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rebgzu/gemini_accidentally_showed_internal_reasoning_text/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7bbgh0",
          "author": "Mayimbe_999",
          "text": "I thought most coding agents show reasoning? For example I work in claude and I get entertainment by reading is reasoning while working.",
          "score": 10,
          "created_utc": "2026-02-25 12:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c7u88",
              "author": "yopla",
              "text": "It's an option in most nowadays, because it's polluting the user's brain context. ðŸ˜†",
              "score": 0,
              "created_utc": "2026-02-25 15:20:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bbnyn",
          "author": "michaelzki",
          "text": "What you might be seeing is part of the main system role prompt injected by the agent together with your prompt (as user role).\n\nThat self-talk stage is the thinking mode part, evaluating whats happening and use it as additional context in later prompt sequence. If you are not seeing it frequently, maybe its a bug on the agent's side. You can turn it off somewhere with thinking_mode=false or similar.\n\nYou will understand it right away if you technically communicate to AI provider's API via chat/generate.",
          "score": 2,
          "created_utc": "2026-02-25 12:21:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bdmq2",
              "author": "MrDEVIL6978",
              "text": "i haven't seen this kind of response untill now and this response crashed my gemini tab and i was getting 1-2 fps on that browser tab and when i refreshed the browser that output was replaced by a normal output, seemed kinda a rare event happening ",
              "score": -1,
              "created_utc": "2026-02-25 12:34:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bfkkn",
                  "author": "michaelzki",
                  "text": "Try aider cli. Thats the most forgiving agent cli out there",
                  "score": 1,
                  "created_utc": "2026-02-25 12:47:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bomxr",
          "author": "thought_provoking27",
          "text": "Great thread !",
          "score": 1,
          "created_utc": "2026-02-25 13:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d5ox4",
          "author": "WyattTheSkid",
          "text": "Itâ€™s happened to me a couple times",
          "score": 1,
          "created_utc": "2026-02-25 17:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7govhx",
          "author": "enterme2",
          "text": "That is just the thinking process that mixed into your output. Nothing new.",
          "score": 1,
          "created_utc": "2026-02-26 05:01:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c3wyx",
          "author": "Fantastic-Breath2416",
          "text": "Il mio sistema risolverÃ¡ questo problema!\n\nhttps://nothumanallowed.com/use-cases/liara-the-dna-of-an-artificial-mind",
          "score": 0,
          "created_utc": "2026-02-25 15:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dmuoz",
          "author": "hazel-wood5",
          "text": "this is actually useful because it shows exactly how these models think every single time.. All the planing drafting self-correction and constraint checking is happening under the hood we just never see it..",
          "score": 0,
          "created_utc": "2026-02-25 19:12:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}