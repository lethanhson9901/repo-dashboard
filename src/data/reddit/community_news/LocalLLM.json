{
  "metadata": {
    "last_updated": "2026-01-22 16:59:57",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 182,
    "file_size_bytes": 225887
  },
  "items": [
    {
      "id": "1qf5l2n",
      "title": "Local AI Final Boss â€” M3 Ultra v.s. GB10",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ysf36b97qudg1.jpeg",
      "author": "Imaginary_Ask8207",
      "created_utc": "2026-01-17 06:13:15",
      "score": 291,
      "num_comments": 70,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qf5l2n/local_ai_final_boss_m3_ultra_vs_gb10/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o02e00r",
          "author": "No_Conversation9561",
          "text": "Try clustering them together using EXO. They actually posted about this exact same setup.\nThey said it speeds up prompt processing.\n\n\nhttps://exolabs.net\n\nIssues:  https://github.com/exo-explore/exo/issues/1102",
          "score": 55,
          "created_utc": "2026-01-17 06:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03foo0",
              "author": "Imaginary_Ask8207",
              "text": "Yess! I also saw this blog by EXO: https://blog.exolabs.net/nvidia-dgx-spark/. \n\nNot sure if it's stable tho, will give it a try when I got time :)",
              "score": 7,
              "created_utc": "2026-01-17 12:31:58",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o03bpto",
              "author": "cipioxx",
              "text": "Exo isnt working with linux.",
              "score": -4,
              "created_utc": "2026-01-17 11:59:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0314fh",
          "author": "adspendagency",
          "text": "sheeesh. seeing as we install about 5 of these per day into business to do private on-prem infra I should probably make myself more aware of what the GB10 performs like compared to the M3. weâ€™ve just been shipping M3s to all of our customers. How does the GB10 hold up ?",
          "score": 27,
          "created_utc": "2026-01-17 10:24:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03fyx1",
              "author": "CustomerNo30",
              "text": "Finger in air.. What sort of money are we talking about here for a M3 solution. If you can't say then that's fair enough. I've been trying to push for a dedicated server but resistance is high from the holders of budgets.",
              "score": 9,
              "created_utc": "2026-01-17 12:34:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o05ajm3",
                  "author": "Direct_Turn_1484",
                  "text": "With the RAM maxed out, I think Appleâ€™s website lists them around $10k.",
                  "score": 7,
                  "created_utc": "2026-01-17 18:19:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o08u8sv",
                  "author": "adspendagency",
                  "text": "For entry foundation package (Mac Studio + AI Voice Receptionist with RAG) we charge $10,000 we get the Mac Studio for about $6000. Then depending on what else they want built the price jumps significantly. Orchestration agents - SwarmAI - Automations - if they need media generated for ads with brand AI influencers (weâ€™re technically an ad agency first so we run ads primarily but now with AI we also sell AI solutions) - basically Iâ€™ve set it up where you pick your niche, roofer, dentist, med spa, law firm etc and we come in and sell you the foundation $10k then offer the â€œHQ Proâ€ (what I ended up calling them) and those have a ton of agentic infrastructure and automations designed specifically for those niches. Also clients can build a custom infrastructure which is more bespoke to what they need it to do and be for their business. The HQ Proâ€™s are about $25k+ and a full custom bespoke build is aboutâ€¦ wellâ€¦ I actually just raised my prices so itâ€™s now $69,000 (use to be $35k starting and increase based on scope of what weâ€™re building) and I might raise them again to $100,000 because the idea is weâ€™re replacing 1 employee salary and building a infrastructure that can run your whole business for you potentially replacing 100s of employees. At least thatâ€™s how I pitch it. I say â€œclose your eyes, imagine what you want this thing to do for you, okay now letâ€™s build it.â€ But yea, fun times.",
                  "score": 5,
                  "created_utc": "2026-01-18 05:44:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03h4r6",
              "author": "Imaginary_Ask8207",
              "text": "I am primarily testing with VLMs serving with vLLM. So far the latency is not that bad with models quantised in NVFP4, but it's acceptable only for medium sized models such as Qwen3-VL-30b-a3b and gemma3-27b (also GLM-4.6V with 106B 12B MoE, but crashes quite often).  \nI've also noticed that NVFP4 isn't very stable on GB10 when serving with vLLM, it crashes randomly. I haven't dived into the issue yet tho.",
              "score": 6,
              "created_utc": "2026-01-17 12:42:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04v3s0",
                  "author": "frobnosticus",
                  "text": "/me takes notes.\n\n26 is the year I've finally allocated a satchel of cash for a local llm setup.  I even had an electrician add a couple high amperage circuits so I wouldn't blow my house up.\n\nDo you have an \"If I had the cash for a home rig, what I'd really want to set up is....\" grail setup?  (Not that I could afford THAT.  But it seems like a high-end boundary.)",
                  "score": 5,
                  "created_utc": "2026-01-17 17:07:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o03tleb",
                  "author": "divyanshkul",
                  "text": "How are you quantising the models? Running your own pipeline over the hugging face model? And then serving it via endpoint?",
                  "score": 1,
                  "created_utc": "2026-01-17 14:01:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o08uu1o",
                  "author": "adspendagency",
                  "text": "Whatâ€™s the price on one of those ?",
                  "score": 1,
                  "created_utc": "2026-01-18 05:48:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o04zi0p",
                  "author": "SubstantialPoet8468",
                  "text": "What do you use the LLM for?",
                  "score": 1,
                  "created_utc": "2026-01-17 17:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03hdkg",
              "author": "Imaginary_Ask8207",
              "text": "5 M3 ultras per day is crazy! Mind sharing which models are you guys deploying?",
              "score": 5,
              "created_utc": "2026-01-17 12:44:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o07cy55",
                  "author": "adspendagency",
                  "text": "Llama 3.1 70B\nGemma 3 27B\nMedGemma 27B\nMistral Large 3 \nDepends. \n\nThen various voice stacks Whisper local turbo STT and Kokoro TTS\n\nAlso set up a lot of RAG\nBeing compliant and creating an air-gapped network with local encryption and signed BAA for any external SIP gateway. Encryption storage, audit logging, etc. \n\nHonestly depends on the clients needs and the complexity of what weâ€™re building for them. \n\nIf weâ€™re not doing like a crazy complex powerful AI infrastructure typically reserved for businesses doing $5M + or enterprise then 1 M3 will do just fine for like an AI voice receptionist with RAG tied to a knowledge base they can use daily to help with things in their business. But we sell them the Mac because it allows for upsells later/ future scaling and stacking more infrastructure if they would want to. \n\nBut yea Iâ€™m just curious to see if my set up that Iâ€™m pitching to clients right now is solid or if for the same cost or less I could get these systems installedâ€¦ Which honestly I probably could but thereâ€™s just something about selling a Mac Studio M3 to a business that enables AI for them that hits different for the business owner. The brand of Apple helps a lot ðŸ¥¹",
                  "score": 3,
                  "created_utc": "2026-01-18 00:31:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04qm7n",
              "author": "OverclockingUnicorn",
              "text": "People actually pay for on prem infra? Who are your customers and what sort of solutions are you building out for them?",
              "score": 5,
              "created_utc": "2026-01-17 16:46:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08wtlo",
                  "author": "adspendagency",
                  "text": "100% businesses with high sensitivity data, complex, repeatable decision-making, real money or real risk tied to latency, privacy, or uptime. \n- Law Firms (mid-large, specialized) \n- Healthcare groups / specialty clinics HIPAA + PHI/Voice notes, imaging summaries, patient history synthesis\n- defense / aerospace / gov contractors with ITAR, CMMC, classified workflows\n- Financial Institutions (Private Equity, Family Offices, Banks)\n- Manufacturing & Industrial Ops\n- Large Property Management / RE Holdings\n- Insurance Carriers & MGAs\n- call centers \n- private schools / universities \n- Roofing/Construction\n\nAs for the solutions we have templates for business verticals that we built out and sell them as â€˜HQ Prosâ€™ essentially AI infrastructure that has a ton of agentic workflows and then our grand slam offer is called â€˜OperatorHQâ€™ and thatâ€™s a full custom bespoke build for your business. Close your eyes, imagine what you want this thing to do for you, okay now letâ€™s build it.â€™",
                  "score": 3,
                  "created_utc": "2026-01-18 06:04:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03jbcu",
          "author": "Ok_Remove3449",
          "text": "PLEASE! LLM inference tps. Are they as bad as I've heard?",
          "score": 8,
          "created_utc": "2026-01-17 12:58:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03zx33",
              "author": "Imaginary_Ask8207",
              "text": "I think it depends on which model and what quantisation strategy. Any particular model in your mind?",
              "score": 3,
              "created_utc": "2026-01-17 14:35:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04kvj4",
                  "author": "Mean-Sprinkles3157",
                  "text": "Qwen3-Next-80B, gpt-oss-120b, can you try these two if  possible?",
                  "score": 3,
                  "created_utc": "2026-01-17 16:19:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02m63w",
          "author": "belgradGoat",
          "text": "Iâ€™d see if Mac Studio is actually stable running 500 gb ram models. I tried with 256 version and it gets messy towards the limit",
          "score": 8,
          "created_utc": "2026-01-17 08:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03f71d",
              "author": "Imaginary_Ask8207",
              "text": "Any model in your mind? 4-bit quantised DeepSeekR1 should only be using \\~300-400GB.",
              "score": 3,
              "created_utc": "2026-01-17 12:28:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03fwie",
                  "author": "belgradGoat",
                  "text": "Maybe something like this? I wouldnâ€™t even try anything else then mlx mlx-community/DeepSeek-V3-0324-5bit",
                  "score": 7,
                  "created_utc": "2026-01-17 12:33:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03vnlp",
              "author": "GermanK20",
              "text": "obviously you changed the default RAM limit from 2/3 to something like 9/10, didn't you? But even then, you gotta keep the context small.",
              "score": 2,
              "created_utc": "2026-01-17 14:12:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o045giq",
                  "author": "belgradGoat",
                  "text": "That was not the point. I have 256 gb ram, used model was like 190, context is not 50gb. I use lm studio.\n\nIssue is not that it doesnâ€™t work, it works, model performs ok. But after a while of holding this model in memory system becomes unstable and started glitching hard, eventually crashing the system",
                  "score": 3,
                  "created_utc": "2026-01-17 15:05:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o02qmku",
              "author": "No_Conversation9561",
              "text": "Same reason I got two 256GB ones instead of one 512GB.",
              "score": 0,
              "created_utc": "2026-01-17 08:45:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03veu5",
          "author": "StrangeMuon",
          "text": "Iâ€™d be interested to see some side by side comparisons between the GX10 and the M3 Ultra for inference with gpt-oss-120b & Stable Diffusion and also (more interestingly for me) a run of this benchmark on both:\n\nhttps://github.com/TristanBilot/mlx-benchmark",
          "score": 3,
          "created_utc": "2026-01-17 14:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03zcq0",
              "author": "Imaginary_Ask8207",
              "text": "I could try on M3 Ultra\\~ but seems the repo does not support GB10 yet.",
              "score": 1,
              "created_utc": "2026-01-17 14:32:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02lczd",
          "author": "SoaokingGross",
          "text": "I'd be asking it to do something to fix our political situation somehow.  It's fine for us to have fun with these things, I bought an M3 for this reason but I feel pretty strongly that more of my time should be spent doing something instead of tuning out.",
          "score": 12,
          "created_utc": "2026-01-17 07:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0at6yr",
              "author": "HopefulMaximum0",
              "text": "If you have computing available, there are interesting projects that need it for technical countermeasures.",
              "score": 1,
              "created_utc": "2026-01-18 15:04:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o02se48",
              "author": "thecurrykid",
              "text": "Amen",
              "score": -2,
              "created_utc": "2026-01-17 09:01:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o04dvx0",
          "author": "HealthyCommunicat",
          "text": "Iâ€™ve done an extreme deep dive into this and the m3 ultra will outbeat the spark for pure text inference no matter what, especially as context grows, PP will still go down but the TG is just that much greater that it mathematically never reaches a point where the output is produced faster - with agentic coding cycles MLX allows cache reuse making PP near same as the GB10 anyways. I posted stats of a test done going up to 100k context and the spark just cannot, will not be able to do those speeds EVEN with 2x sparks.\n\nThat being said, exoâ€™ing these two seems like its going to be a new niche class of being able to combine the two to do exactly that and have each other make up for its own weaknesses, amazing.",
          "score": 4,
          "created_utc": "2026-01-17 15:46:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jcoij",
          "author": "Temporary_Outcome_72",
          "text": "I get 12-20 tps on my lac studio 512gb ultra with deepseek variants as long as I use quants that are 410gb or less and watch the context. Still get blow ups with long context inputs but using langchain to drive it with fresh starts at each new reasoning step it can match what I see with online LLM's to my perception.",
          "score": 2,
          "created_utc": "2026-01-19 20:04:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jd3p2",
              "author": "Temporary_Outcome_72",
              "text": "asus is limited at 128gb or 256gb chained would be useless to me given my experience with the mac. 512gb is the sweetspot for quanted full models, or 1Tb for kimi-k2.",
              "score": 1,
              "created_utc": "2026-01-19 20:06:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o06jc8j",
          "author": "Grouchy-Bed-7942",
          "text": "Benchmark **gpt-oss-20b**, **gpt-oss-120b**, **MiniMax-M2.1-GGUF:IQ4\\_XXS**, and **unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:Q8\\_K\\_XL,** basically something like:\n\n    llama-bench -m \"~/.cache/llamacpp/$MODEL\" -p 4096,32768 -n 512 -ngl 999 -r 3 -fa 1\n\nThat way I can compare the results with my Strix Halo :)\n\nAlso use **vLLM** with **NVFP4** quantization too :)",
          "score": 2,
          "created_utc": "2026-01-17 22:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nc9v7",
              "author": "Imaginary_Ask8207",
              "text": "# `llama-bench` Results\n\n## ggml-org/gpt-oss-20b-GUF (gpt-oss 20B MXFP4 MoE)\n\n```txt\n| model | size | params | backend | threads | fa | test | t/s |\n| -------------------u----------- | ---------: | ---------: | ---------- | ------: | -: | --------------: | -------------------: |\n| gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | Metal,BLAS | 24 | 1 | pp4096 | 2520.98 Â± 1.81 |\n| gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | Metal,BLAS | 24 | 1 | pp32768 | 1740.40 Â± 0.44 |\n| gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | Metal,BLAS | 24 | 1 | tg512 | 126.78 Â± 0.19 |\n```\n\n## ggml-org/gpt-oss-120b-GGUF (gpt-oss 120B MXFP4 MoE)\n\n```txt\n| model | size | params | backend | threads | fa | test | t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | --------------: | -------------------: |\n| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Metal,BLAS | 24 | 1 | pp4096 | 1322.96 Â± 4.39 |\n| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Metal,BLAS | 24 | 1 | pp32768 | 975.74 Â± 0.53 |\n| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Metal,BLAS | 24 | 1 | tg512 | 87.22 Â± 0.05 |\n```\n\n## unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:Q8_K_X\n\n```txt\n| model | size | params | backend | threads | fa | test | t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | --------------: | -------------------: |\n| qwen3vlmoe 30B.A3B Q8_0 | 33.51 GiB | 30.53 B | Metal,BLAS | 24 | 1 | pp4096 | 2092.36 Â± 1.32 |\n| qwen3vlmoe 30B.A3B Q8_0 | 33.51 GiB | 30.53 B | Metal,BLAS | 24 | 1 | pp32768 | 917.26 Â± 0.09 |\n| qwen3vlmoe 30B.A3B Q8_0 | 33.51 GiB | 30.53 B | Metal,BLAS | 24 | 1 | tg512 | 74.08 Â± 0.12 |\n```\n\n## unsloth/MiniMax-M2.1-GGUF:IQ4_XS\n\n```txt\n| model | size | params | backend | threads | fa | test | t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | --------------: | -------------------: |\n| minimax-m2 230B.A10B IQ4_XS - 4.25 bpw | 113.52 GiB | 228.69 B | Metal,BLAS | 24 | 1 | pp4096 | 540.45 Â± 0.15 |\n| minimax-m2 230B.A10B IQ4_XS - 4.25 bpw | 113.52 GiB | 228.69 B | Metal,BLAS | 24 | 1 | pp32768 | 337.47 Â± 0.18 |\n| minimax-m2 230B.A10B IQ4_XS - 4.25 bpw | 113.52 GiB | 228.69 B | Metal,BLAS | 24 | 1 | tg512 | 48.43 Â± 0.03 |\n```\n\nI am interested to see the benchmark on Strix Halo as well! Mind sharing here?",
              "score": 1,
              "created_utc": "2026-01-20 11:11:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0cng4p",
          "author": "Available_Hat4532",
          "text": "How does this differs from a rtx 5090?",
          "score": 1,
          "created_utc": "2026-01-18 20:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0eit8i",
              "author": "Imaginary_Ask8207",
              "text": "Both devices have much larger RAM than 5090. 512 GB for M3 Ultra, 128GB for GX10.  \nI think using RTX 5090 will still be faster for training due to its high bandwidth. I mainly use the two devices for inference.",
              "score": 1,
              "created_utc": "2026-01-19 02:10:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0vggjf",
          "author": "ItsZerone",
          "text": "The spark shines with concurrency. The m3 is faster if you're just trying to do local inference with a handful of people accessing the server. As soon as you start asking it to do real sustained agentic work or serve hundreds of requests at once the gb10 will smoke the m3",
          "score": 1,
          "created_utc": "2026-01-21 15:53:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03mazk",
          "author": "CharlesCowan",
          "text": "very cool",
          "score": 0,
          "created_utc": "2026-01-17 13:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06pta3",
          "author": "dickofthebuttt",
          "text": "Neat, whatâ€™s the cost?",
          "score": 0,
          "created_utc": "2026-01-17 22:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ehra9",
              "author": "Imaginary_Ask8207",
              "text": "\\~12-13k USD in total for both devices.",
              "score": 2,
              "created_utc": "2026-01-19 02:04:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qi5q2v",
      "title": "768Gb Fully Enclosed 10x GPU Mobile AI Build",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1qi5q2v",
      "author": "SweetHomeAbalama0",
      "created_utc": "2026-01-20 16:27:57",
      "score": 179,
      "num_comments": 51,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qi5q2v/768gb_fully_enclosed_10x_gpu_mobile_ai_build/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0p1l13",
          "author": "Proof_Scene_9281",
          "text": "ðŸ’¦\n\nDo you run the PSUâ€™s on different circuits in the house?Â ",
          "score": 26,
          "created_utc": "2026-01-20 16:56:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ppwds",
              "author": "betacore_tec",
              "text": "Things you don't care about in a 230v country",
              "score": 14,
              "created_utc": "2026-01-20 18:47:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0prqth",
                  "author": "grubnenah",
                  "text": "Just the GPUs at 250w x8 + 500w x2 could trip a 240V 15A breaker after a few minutes.",
                  "score": 6,
                  "created_utc": "2026-01-20 18:55:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pbfhl",
          "author": "granoladeer",
          "text": "We found the Powerball winner",
          "score": 17,
          "created_utc": "2026-01-20 17:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0p160v",
          "author": "Paliknight",
          "text": "lol do each of your rooms have 30 amp circuits?",
          "score": 7,
          "created_utc": "2026-01-20 16:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0to177",
              "author": "PineappleLemur",
              "text": "Normally 13-15A at 220-240",
              "score": 1,
              "created_utc": "2026-01-21 08:44:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0u5hzm",
                  "author": "Paliknight",
                  "text": "OP is in the US",
                  "score": 1,
                  "created_utc": "2026-01-21 11:25:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pe0a9",
          "author": "jedsk",
          "text": "How are your temps?",
          "score": 6,
          "created_utc": "2026-01-20 17:54:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pme80",
          "author": "OnlyAssistance9601",
          "text": "Prob sounds like a 747 taking off .",
          "score": 7,
          "created_utc": "2026-01-20 18:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0s5gxp",
              "author": "mjTheThird",
              "text": "OP no longer needs house heater.",
              "score": 2,
              "created_utc": "2026-01-21 02:07:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0owuzr",
          "author": "AlyoshaKaramazov_",
          "text": "Looks like Julia Roberts in Pretty Woman ðŸ˜",
          "score": 4,
          "created_utc": "2026-01-20 16:34:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qm98f",
              "author": "Grinch0127",
              "text": "A horse?",
              "score": 2,
              "created_utc": "2026-01-20 21:16:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0p7oko",
          "author": "RentalGore",
          "text": "I'm also interested to understand how you'll power all this.  I would imagine at full tilt, you're way beyond what even a 20amp circuit can do.  Do you have this on a battery that can push wattage to 2400 or so?",
          "score": 3,
          "created_utc": "2026-01-20 17:25:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qpfqu",
              "author": "enigma62333",
              "text": "If the OP is not in the US and in Europe a single residential circuit could support both PSUâ€™s. \n\nIf they are in North America they could either have two 20A / 110V circuits or a single 2 Phase 208 20A circuit for both PSUâ€™s. \n\nThe 2 x 20A / 110 circuits would be tight but manageable if nothing else was running on them.\n\nAt my house I installed a 2-pole 20A breaker and can power at 3.3KW continuously without fear of tripping the breaker.",
              "score": 1,
              "created_utc": "2026-01-20 21:31:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0p3o3v",
          "author": "brianlmerritt",
          "text": "Great project - one question.  Is it now your daily driver for AI related work?",
          "score": 2,
          "created_utc": "2026-01-20 17:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pab9x",
          "author": "shadowsyntax43",
          "text": "your power company should visit your house in exactly one week.",
          "score": 2,
          "created_utc": "2026-01-20 17:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pmyaw",
          "author": "AlexGSquadron",
          "text": "But how did you do it? You used sharding to split model?",
          "score": 1,
          "created_utc": "2026-01-20 18:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0psu6k",
          "author": "TomatoWasabi",
          "text": "Where do you live ?",
          "score": 1,
          "created_utc": "2026-01-20 19:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0q4ql2",
              "author": "serious153",
              "text": "My name is Walter Hartwell White. I live at 308 Negra Arroyo Lane, Albuquerque, New Mexico, 87104. This is my confession. If you're watching this tape, I'm probably dead, murdered by my brother-in-law Hank Schrader. Hank has been building a meth empire for over a year now and using me as his chemist. Shortly after my 50th birthday, Hank came to me with a rather, shocking proposition. He asked that I use my chemistry knowledge to cook methamphetamine, which he would then sell using his connections in the drug world. Connections that he made through his career with the DEA. I was... astounded, I... I always thought that Hank was a very moral man and I was... thrown, confused, but I was also particularly vulnerable at the time, something he knew and took advantage of. I was reeling from a cancer diagnosis that was poised to bankrupt my family. Hank took me on a ride along, and showed me just how much money even a small meth operation could make. And I was weak. I didn't want my family to go into financial ruin so I agreed. Every day, I think back at that moment with regret. I quickly realized that I was in way over my head, and Hank had a partner, a man named Gustavo Fring, a businessman. Hank essentially sold me into servitude to this man, and when I tried to quit, Fring threatened my family. I didn't know where to turn. Eventually, Hank and Fring had a falling out. From what I can gather, Hank was always pushing for a greater share of the business, to which Fring flatly refused to give him, and things escalated. Fring was able to arrange, uh I guess I guess you call it a \"hit\" on my brother-in-law, and failed, but Hank was seriously injured, and I wound up paying his medical bills which amounted to a little over $177,000. Upon recovery, Hank was bent on revenge, working with a man named Hector Salamanca, he plotted to kill Fring, and did so. In fact, the bomb that he used was built by me, and he gave me no option in it. I have often contemplated suicide, but I'm a coward. I wanted to go to the police, but I was frightened. Hank had risen in the ranks to become the head of the Albuquerque DEA, and about that time, to keep me in line, he took my children from me. For 3 months he kept them. My wife, who up until that point, had no idea of my criminal activities, was horrified to learn what I had done, why Hank had taken our children. We were scared. I was in Hell, I hated myself for what I had brought upon my family. Recently, I tried once again to quit, to end this nightmare, and in response, he gave me this. I can't take this anymore. I live in fear every day that Hank will kill me, or worse, hurt my family. I... All I could think to do was to make this video in hope that the world will finally see this man, for what he really is",
              "score": 8,
              "created_utc": "2026-01-20 19:55:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qgc50",
          "author": "mr__smooth",
          "text": "Wow I am looking for this exact kind of build. I currently have a prototyping machine but looking for something more powerful(https://www.reddit.com/r/LocalLLaMA/comments/1qcykx4/home\\_workstation\\_vs\\_nycnj\\_colo\\_for\\_llmvlm\\_whisper/) I'm really impressed by this wondering if I would be able to run it in my apartment, but concerned about the power.",
          "score": 1,
          "created_utc": "2026-01-20 20:49:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qsmhs",
          "author": "Pixer---",
          "text": "Are you able to run vllm instead of llamacpp, and how much performance that would bring ?",
          "score": 1,
          "created_utc": "2026-01-20 21:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r912m",
          "author": "Psychological_Ear393",
          "text": "I like the part where mobile means it has castors.  It's glorious.",
          "score": 1,
          "created_utc": "2026-01-20 23:07:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rczpj",
          "author": "HealthyCommunicat",
          "text": "If Kimi K2 was a physical object:",
          "score": 1,
          "created_utc": "2026-01-20 23:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rfxk5",
          "author": "asciimo",
          "text": "Itâ€™s mobile because of the wheels?",
          "score": 1,
          "created_utc": "2026-01-20 23:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rtv8e",
          "author": "BroderLund",
          "text": "The motherboard has 7 x16 slots. How did you connect 10 GPUs? Biforcation adapter that gives 2 x8 slots with ribbon cables to the GPUs?",
          "score": 1,
          "created_utc": "2026-01-21 01:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0s0yra",
          "author": "No-Leopard7644",
          "text": "For a spec to run large models - what was the reason for 3090 and 5090 that are consumer grade cards and not enterprise scale ones.",
          "score": 1,
          "created_utc": "2026-01-21 01:42:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0sqmbg",
              "author": "Barachiel80",
              "text": "He stated the cost of the RTX Pro 6000's would have eaten the entire project budget. I assume he went with those particular consumer cards over comparable vram RTX Pro 4000 & 4500 series enterprise cards for the additional memory bandwidth and Cuda cores.",
              "score": 1,
              "created_utc": "2026-01-21 04:13:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s4bl3",
          "author": "themostofpost",
          "text": "How does the performance compare to something like Claude code?",
          "score": 1,
          "created_utc": "2026-01-21 02:01:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sk7id",
          "author": "Fearless_Weather_206",
          "text": "A coffee table you never want a spill to happen on â˜•ï¸",
          "score": 1,
          "created_utc": "2026-01-21 03:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0so2sd",
          "author": "RatioOtherwise1185",
          "text": "in todayâ€™s market that probably cost 2 kidneys, a lung, some parts of your vertebrae, an eye and an ear.",
          "score": 1,
          "created_utc": "2026-01-21 03:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tnxyj",
          "author": "Road-Runnerz",
          "text": "You should get the top and bottom extensions for the case so you have more more room",
          "score": 1,
          "created_utc": "2026-01-21 08:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xatt4",
          "author": "dickofthebuttt",
          "text": "If you turn it on and unlock the wheels, does it push itself across the floor?",
          "score": 1,
          "created_utc": "2026-01-21 20:50:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xwzoj",
          "author": "Complete_Lurk3r_",
          "text": "lucky that thing is fully enclosed....HOLY SHIT its a mess in there!",
          "score": 1,
          "created_utc": "2026-01-21 22:33:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xygfe",
          "author": "External_Hippo_9283",
          "text": "Why didn't you use a DGX? It has faster installation, warranty, etc. I'm asking because I don't know.",
          "score": 1,
          "created_utc": "2026-01-21 22:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o128s0k",
          "author": "e11310",
          "text": "Wow crazy build. What is the at wall power consumption for this? Has to be wild.",
          "score": 1,
          "created_utc": "2026-01-22 15:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qmwuf",
          "author": "nsmitherians",
          "text": "But can it run doom?",
          "score": 1,
          "created_utc": "2026-01-20 21:19:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tlm04",
              "author": "ryfromoz",
              "text": "It might even manage Crysis!",
              "score": 2,
              "created_utc": "2026-01-21 08:21:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qq5xi",
          "author": "hyper_ny",
          "text": "2 mac studio will be easier and better with rdma",
          "score": 0,
          "created_utc": "2026-01-20 21:34:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfctk3",
      "title": "Quad 5060 ti 16gb Oculink rig",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ohsze83iswdg1.jpeg",
      "author": "beefgroin",
      "created_utc": "2026-01-17 13:09:42",
      "score": 96,
      "num_comments": 41,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfctk3/quad_5060_ti_16gb_oculink_rig/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o05n3oi",
          "author": "see_spot_ruminate",
          "text": "Ok time to bitch at you for a second. Get off ollama for this. Llamacpp and ik_llama can give you much more granular control and performance over ollama. Rant over.Â \n\n\n\nWelcome to the 5060ti master race. I also recently added yet another 5060ti, my fourth, and have been meaning to do a write up. With 64gb of system ram and 64gb of vram, under Ubuntu and using llamacpp or ik_llama I can get:\n\n- gpt-oss120b (llamacpp faster) - max context and get around 55 t/s (system ram speed limit)\n\n- Devstral 2 Small Q8 (ik_llama faster with split mode graph) full gpu offload 200k context at around 34 t/s (card bandwidth limit)\n\n- glm 4.6v Q4 (ik_llama faster with split mode graph) 100k context with cpu offload at around 16 t/s (system ram speed limit)\n\n- gpt-oss-20b (llamacpp faster) max context at around 120 t/s (bandwidth limit of card)",
          "score": 17,
          "created_utc": "2026-01-17 19:17:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05r8ra",
              "author": "beefgroin",
              "text": "Thanks for the advice! Right now I guess Iâ€™m sort of in an exploration phase to find the best models for me, so the convenience of quickly switch between models from openweb-ui is still unbeatable. But youâ€™re right I need to start squeezing all the speed 5060 has to offer",
              "score": 4,
              "created_utc": "2026-01-17 19:37:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o05tk2y",
                  "author": "see_spot_ruminate",
                  "text": "I think ollama and other beginner programs are great for beginners to deep their feetâ€¦ but you built a non-beginner setup! Youâ€™re in the deep end now. lol.\n\nEdit: donâ€™t eat Cheetos and comment, I said beginner too many times.Â ",
                  "score": 3,
                  "created_utc": "2026-01-17 19:48:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o093wr8",
                  "author": "ariagloris",
                  "text": "Llama.cpp has a new presets feature that allows you to pass a model defaults + predefined configuration settings in a file. When commanded via API the desired model can be specified and the sever will load the model. This works with openweb-ui just fine, I.e., you can switch between models dynamically.",
                  "score": 2,
                  "created_utc": "2026-01-18 07:04:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0745g3",
              "author": "SFsports87",
              "text": "Is there a good resource on how to use llama.cpp in linux?",
              "score": 1,
              "created_utc": "2026-01-17 23:45:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07dkov",
                  "author": "see_spot_ruminate",
                  "text": "What is your comfort level from â€œI saw the movie hackers onceâ€ to â€œLFS was a fun weekend projectâ€",
                  "score": 2,
                  "created_utc": "2026-01-18 00:35:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0pdod5",
              "author": "irlcake",
              "text": "My curiosity. What are you doing with this?",
              "score": 1,
              "created_utc": "2026-01-20 17:52:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0pxyhd",
                  "author": "see_spot_ruminate",
                  "text": "I write anachronistic furry fan fiction centered around beloved philosophers of history, eg maybe a young naive warthog boy coming of age whilst he learns about god from saint augustine... \n\nI also do whatever I want with my own private machine.",
                  "score": 2,
                  "created_utc": "2026-01-20 19:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03lkwc",
          "author": "JEs4",
          "text": "Neat setup!  But isnâ€™t PCIe 5.0 x16 to 4x4x4x4 bifurcation capped at ~8 gb/s? \n\nWhat do you plan to use it for?",
          "score": 4,
          "created_utc": "2026-01-17 13:13:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03lyot",
              "author": "beefgroin",
              "text": "Yes and moreover itâ€™s no longer pci 5 but 4 but itâ€™s plenty for inference which is my primary use case. I even did the test on running full pci 5 x16 vs Oculink on Gemma 3 12b and there was no different in tps",
              "score": 7,
              "created_utc": "2026-01-17 13:15:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0k1mez",
                  "author": "Practical-Collar3063",
                  "text": ">I even did the test on running full pci 5 x16 vs Oculink on Gemma 3 12b and there was no different in tps\n\nWhat do you mean by this ? did you make a test with the model loaded on a single card running at pcie 5.0 x16 ? If so that would indeed not make any difference because pcie speed is only important in cross GPU communication (moreover the 5060 ti is limited to pcie 5.0 x8 anyway).  \n  \nAdditionaly pcie 4.0 x4 is not plenty at all for inference you would get massive speed ups by using pcie 5.0 x4 (or better but talking within the constraints of your set up) and tensor parrallelism in VLLM  (especially on prompt processing).\n\nPlease do not use Ollama, this is such a nice and neat set up, don't waste it on Ollama...",
                  "score": 2,
                  "created_utc": "2026-01-19 22:04:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o03o8nb",
                  "author": "JEs4",
                  "text": "Makes sense! What tps are you seeing?",
                  "score": 1,
                  "created_utc": "2026-01-17 13:30:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o06ehlp",
              "author": "panchovix",
              "text": "Actually, X4 5.0 is about 16 gigabytes/s. X4 4.0 is 8 gigabytes/s.",
              "score": 2,
              "created_utc": "2026-01-17 21:35:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o04gsqd",
          "author": "iMrParker",
          "text": "Cool setup! I was wondering how you handled the 24pins for the occulink boards but I watched the video and saw they were sata powered which is smart. I was thinking of doing something similar not too long ago but settled for a single 3090 occulink with my main 5080 system. I want to expand but I feel like it would require a whole rebuildÂ ",
          "score": 6,
          "created_utc": "2026-01-17 16:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05dmss",
              "author": "beefgroin",
              "text": "if you're talking 40gb vram, I don't think you're missing out too much, gemma3:27b and gpt-oss:20b and nemotron are still my favorite models, but of course depends on your usecase",
              "score": 2,
              "created_utc": "2026-01-17 18:33:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03x5g4",
          "author": "GutenRa",
          "text": "Gemma3-12b fits entirely on a single graphics card. How do models handle when distributed across multiple graphics cards? Qwen3-next 80b, for example? What software do you use to distribute the load across multiple cards?",
          "score": 3,
          "created_utc": "2026-01-17 14:21:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03yox5",
              "author": "beefgroin",
              "text": "Iâ€™ll post test metrics later today, regarding the gemma3 12b I only mention in relation to comparison between full pcie5x16 vs Oculink inference speed. Iâ€™m using ollama to run models. It splits the load between GPUs nicely. Yet to try vllm",
              "score": 3,
              "created_utc": "2026-01-17 14:29:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04a8ry",
                  "author": "dragonbornamdguy",
                  "text": "VLLM is a beast, very hard to setup but when it starts to work it beats metal really hard.",
                  "score": 4,
                  "created_utc": "2026-01-17 15:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o05ck56",
              "author": "beefgroin",
              "text": "Updated the post with a quick benchmark",
              "score": 2,
              "created_utc": "2026-01-17 18:28:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o05gwag",
                  "author": "GutenRa",
                  "text": "wow 54 tps! merci beaucoup",
                  "score": 2,
                  "created_utc": "2026-01-17 18:48:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05h8wx",
          "author": "e11310",
          "text": "Is there anything you can run on this that can handle larger project logic reasoning and code dev like Claude? Or is with even all this, weâ€™re still a ways from that level?",
          "score": 2,
          "created_utc": "2026-01-17 18:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05mwnt",
              "author": "beefgroin",
              "text": "Iâ€™d say for larger projects Claude codex Gemini etc are still unbeatable. This kind of setup will be fine for small projects and scripts, but as the context grows it gets painfully slow even thought in the end it might actually spit out something decent. I think to get close to the Claude output one needs quad rtx 6000 pro, not 5060 lol.\nThis setup is good for private chatting, processing private documents etc. anything you donâ€™t want to share with corporations",
              "score": 2,
              "created_utc": "2026-01-17 19:16:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o071kp1",
                  "author": "e11310",
                  "text": "Yes that was what I was thinking. Thanks for the input. Nonetheless, really cool rig you made!Â \n\nOne day weâ€™ll be able to run something like Claude at homeâ€¦ hopefully. ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2026-01-17 23:32:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06y0sh",
          "author": "AllTheCoins",
          "text": "This is my dream build haha Iâ€™m using two 5060tiâ€™s at the moment but I want moreeee",
          "score": 2,
          "created_utc": "2026-01-17 23:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o074y5o",
              "author": "beefgroin",
              "text": "Tbh I now think 32 gigs vram is a pretty sweet spot, even with 64 I keep going back to Gemma3 27b a lot and gpt-oss20b with nemotron feel great too. Another 16 gigs can help you max out on context but itâ€™s not such a common use case",
              "score": 1,
              "created_utc": "2026-01-17 23:49:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o07533z",
                  "author": "AllTheCoins",
                  "text": "Paired with 64GB of RAM, it is quite nice",
                  "score": 2,
                  "created_utc": "2026-01-17 23:50:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jbitu",
          "author": "No_Mixture_7383",
          "text": "haber, es decir, EJEMPLO... puedo tener 4 RTX 5090 DE ESA MANERA, TRABAJANDO EN UN SOLO PROYECTO MULTIGPU HACIENDO UN SOLO REENDER? IA RENDERS\n\n![gif](giphy|cLpqzmqFc9u3aggR75)",
          "score": 1,
          "created_utc": "2026-01-19 19:59:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jxna2",
          "author": "swall00w",
          "text": "What hardware do you use to make the whole bifurcation work?",
          "score": 1,
          "created_utc": "2026-01-19 21:45:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0khzgf",
              "author": "beefgroin",
              "text": "Itâ€™s a pcie to 4 Oculink ports adapter for AliExpress, check out the video",
              "score": 1,
              "created_utc": "2026-01-19 23:28:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qj3cgo",
      "title": "the state of local agentic \"action\" is still kind of a mess",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "author": "Ilove_Cakez",
      "created_utc": "2026-01-21 16:56:55",
      "score": 51,
      "num_comments": 8,
      "upvote_ratio": 0.98,
      "text": "spent the last few nights trying to get a decent mcp setup running for my local stack and itâ€™s honestly depressing how much friction there still is. weâ€™ve got these massive models running on consumer hardware, but as soon as you want them to actually do anything.. like pull from a local db or interact with an api so youâ€™re basically back to writing custom boilerplate for every single tool.\n\nthe security trade-offs are the worst part. itâ€™s either total isolation (useless) or giving the model way too much permission because managing granular mcp servers manually is a full-time job. iâ€™ve been trying to find a middle ground where i donâ€™t have to hand-roll the auth and logging for every connector.\n\nfound a tool thatâ€™s been helping with the infra side of it. it basically just handles the mcp server generation and the governance/permissions layer so i don't have to think too much (ogment ai, i'm sure most of you know about it). itâ€™s fine for skipping the boring stuff, but iâ€™m still annoyed that this isn't just native or more standardized yet.\n\nhow are you guys actually deploying agents that can touch your data? are you just building your own mcp wrappers from scratch or is there a better way to handle the permissioning? curious",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0xvvdv",
          "author": "ForsookComparison",
          "text": "Running agentically is simply too hard for most small or mid-sized open weight models. Once you add a few tools and a multi-step feedback loop, the *bare minimum* becomes either Qwen3-Next-80B or Gpt-oss-120b, both of which still will fall flat on their face along the way. GLM 4.6V is probably the actual starting line for reliable use and even that will be spotty as you add more and more instructions and tools.\n\nI don't have a good answer for you besides *\"I've experienced this and think that agents are just hard\"*.",
          "score": 1,
          "created_utc": "2026-01-21 22:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z1cn1",
          "author": "Fortyseven",
          "text": "I've had some modest success using 30b qwen3-coder and the Qwen Code TUI. It won't replace my cloud-based stuff, but I always give it a go now and then and find myself surprised at how capable it can be.",
          "score": 1,
          "created_utc": "2026-01-22 02:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o101cuk",
          "author": "mister2d",
          "text": "I'm mildly curious about this topic. How much could a framework like a self-hosted n8n instance fill the gaps?",
          "score": 1,
          "created_utc": "2026-01-22 06:02:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10t0ef",
          "author": "techlatest_net",
          "text": "Yeah man, local agents are still duct tape and prayersâ€”tool perms either nuke security or turn models into toddlers with car keys. Ogment's solid for skipping the mcp boilerplate grind tho.\n\nCrewAI + containerized tools (dockerized APIs) saved my sanityâ€”granular perms via bind mounts, no root hell. You running ollama under that? What's your stack look like?",
          "score": 1,
          "created_utc": "2026-01-22 10:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wmw3y",
          "author": "atlasnomos",
          "text": "Youâ€™re not wrong â€” the pain point youâ€™re describing is real and structural, not a tooling gap.\n\nWhat keeps biting people is that â€œagent actionâ€ today is basically glued together from three things that donâ€™t want to coexist:\n\n1. prompt-time intent\n2. tool adapters (MCP / wrappers)\n3. runtime execution with side effects\n\nMost stacks solve (1) and (2), but almost nobody treats (3) as a **first-class runtime concern**. So permissions end up living either:\n\n* inside prompts (brittle),\n* inside per-tool wrappers (boilerplate explosion),\n* or at the MCP server boundary (coarse, hard to reason about).\n\nThe isolation vs over-permission tradeoff you mention is exactly what happens when thereâ€™s no **deny-by-default execution layer** sitting *between* the model and the tools.\n\nWhatâ€™s worked best for us locally is:\n\n* keep MCP servers dumb (pure capability exposure)\n* move auth, cost limits, logging, and allow/deny decisions into a single runtime gate\n* treat every tool call as an auditable event, not â€œjust another function callâ€\n\nThat way youâ€™re not re-implementing auth + logging per connector, and youâ€™re not trusting the model to behave just because the prompt says so.\n\nI agree it *should* be more native / standardized. Until thereâ€™s a real spec for agent execution (not just tool schemas), everyoneâ€™s either rolling wrappers or accepting scary permissions.\n\nCurious what level youâ€™re aiming for locally:\n\n* human-in-the-loop approvals?\n* strict allowlists?\n* cost / rate enforcement?\n* or just â€œdonâ€™t nuke my dataâ€?\n\nThat choice seems to drive everything else.",
          "score": -3,
          "created_utc": "2026-01-21 19:01:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0yrrf6",
              "author": "Big-Masterpiece-9581",
              "text": "I wish this kind of sanity would reign. But working in Big Corp is a bit like the Trump Administration in this job market. Pure politics, cloak and dagger, backstabbing, constant reorgs. Nobody is sharing anything, and each is reinventing every wheel especially around AI / MCP.",
              "score": 1,
              "created_utc": "2026-01-22 01:17:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11lp1w",
                  "author": "atlasnomos",
                  "text": "Thatâ€™s a fair take â€” and honestly, it matches what we keep hearing from people inside large orgs.\n\nMost of the time itâ€™s not that the problems arenâ€™t understood; itâ€™s that ownership is fragmented and incentives donâ€™t line up. Governance ends up living in the cracks between teams, so everyone quietly rebuilds their own glue and hopes it never becomes *their* incident.\n\nWeâ€™re under no illusion that clean architectures win on their own. In practice, they only surface once failure, liability, or regulatory pressure forces shared responsibility. Until then, itâ€™s politics and reorgs all the way down.\n\nAppreciate you saying this out loud â€” itâ€™s useful context, not pushback.",
                  "score": 1,
                  "created_utc": "2026-01-22 13:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qem1po",
      "title": "We fine-tuned an email classification model so you can auto-label your emails locally with n8n.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/tfwi6yjzvqdg1.jpeg",
      "author": "party-horse",
      "created_utc": "2026-01-16 17:18:54",
      "score": 47,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qem1po/we_finetuned_an_email_classification_model_so_you/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzz5ybz",
          "author": "Outside-Balance7754",
          "text": "Cool! Does the spam category also include phishing emails? If not, Iâ€™d say missing a phishing tag is definitely a drawback.",
          "score": 3,
          "created_utc": "2026-01-16 19:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6hko",
              "author": "party-horse",
              "text": "Itâ€™s included in spam for now but you can easily train your own model with any categories you want!",
              "score": 1,
              "created_utc": "2026-01-16 19:29:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzymory",
          "author": "nunodonato",
          "text": "I love everything you guys are doing!",
          "score": 1,
          "created_utc": "2026-01-16 18:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6j3y",
              "author": "party-horse",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-16 19:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzyrd1p",
          "author": "CalmlyObservant",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-16 18:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzvoo5",
          "author": "HealthyCommunicat",
          "text": "Woah, this is automation iâ€™d actually use, thank you for your efforts to create a tool anyone can use.",
          "score": 1,
          "created_utc": "2026-01-16 21:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02cvf7",
              "author": "party-horse",
              "text": "Amazing! Thanks for",
              "score": 1,
              "created_utc": "2026-01-17 06:40:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09y9w0",
          "author": "henriquegarcia",
          "text": "Hey guys...I'm sorry to bother but I really can't get it working following the instructions, ollama doesn't seem to be able to support the \"qwen3\" model architecture for direct Safetensors-to-GGUF conversion, any chance you could link me to the GGUF file of the model?",
          "score": 1,
          "created_utc": "2026-01-18 11:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j22fn",
              "author": "Vineethreddyguda",
              "text": "Hey, sorry for the delay in my response  \njust updated the gguf file you can download from the HF    \n  \n[https://huggingface.co/distil-labs/distil-email-classifier/blob/main/model.gguf](https://huggingface.co/distil-labs/distil-email-classifier/blob/main/model.gguf)",
              "score": 1,
              "created_utc": "2026-01-19 19:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mwpd3",
                  "author": "henriquegarcia",
                  "text": "Thank you very much!\nAmazing work!",
                  "score": 1,
                  "created_utc": "2026-01-20 08:48:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyh702",
          "author": "astral_crow",
          "text": "Iâ€™ve been waiting for n8n to be talked about more consider how much stuff Iâ€™ve seen about comfy. But what sparked the fact I am seeing n8n everywhere again so sudden?",
          "score": 1,
          "created_utc": "2026-01-16 17:36:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6b0i",
              "author": "party-horse",
              "text": "I think it makes it very easy to build integrations on top of llms. Thatâ€™s how I have been using it",
              "score": 2,
              "created_utc": "2026-01-16 19:28:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qitnbv",
      "title": "The Case for a $600 Local LLM Machine",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "author": "tony10000",
      "created_utc": "2026-01-21 09:42:30",
      "score": 44,
      "num_comments": 42,
      "upvote_ratio": 0.96,
      "text": "**The Case for a $600 Local LLM Machine**\n\nUsing the Base Model Mac mini M4\n\nhttps://preview.redd.it/5c916gwucoeg1.png?width=1182&format=png&auto=webp&s=68d91da71f6244d752e15922e47dfbf9d792beb1\n\nby Tony Thomas\n\nIt started as a simple experiment. How much real work could I do on a small, inexpensive machine running language models locally?\n\nWith GPU prices still elevated, memory costs climbing, SSD prices rising instead of falling, power costs steadily increasing, and cloud subscriptions adding up, it felt like a question worth answering. After a lot of thought and testing, the system I landed on was a base model Mac mini M4 with 16 GB of unified memory, a 256 GB internal SSD, a USB-C dock, and a 1 TB external NVMe drive for model storage. Thanks to recent sales, the all-in cost came in right around $600.\n\nOn paper, that does not sound like much. In practice, it turned out to be far more capable than I expected.\n\nLocal LLM work has shifted over the last couple of years. Models are more efficient due to better training and optimization. Quantization is better understood. Inference engines are faster and more stable. At the same time, the hardware market has moved in the opposite direction. GPUs with meaningful amounts of VRAM are expensive, and large VRAM models are quietly disappearing. DRAM is no longer cheap. SSD and NVMe prices have climbed sharply.\n\nAgainst that backdrop, a compact system with tightly integrated silicon starts to look less like a compromise and more like a sensible baseline.\n\n**Why the Mac mini M4 Works**\n\nThe M4 Mac mini stands out because Appleâ€™s unified memory architecture fundamentally changes how a small system behaves under inference workloads. CPU and GPU draw from the same high-bandwidth memory pool, avoiding the awkward juggling act that defines entry-level discrete GPU setups. I am not interested in cramming models into a narrow VRAM window while system memory sits idle. The M4 simply uses what it has efficiently.\n\nSixteen gigabytes is not generous, but it is workable when that memory is fast and shared. For the kinds of tasks I care about, brainstorming, writing, editing, summarization, research, and outlining, it holds up well. I spend my time working, not managing resources.\n\nThe 256 GB internal SSD is limited, but not a dealbreaker. Models and data live on the external NVMe drive, which is fast enough that it does not slow my workflow. The internal disk handles macOS and applications, and that is all it needs to do. Avoiding Appleâ€™s storage upgrade pricing was an easy decision.\n\nThe setup itself is straightforward. No unsupported hardware. No hacks. No fragile dependencies. It is dependable, UNIX-based, and boring in the best way. That matters if you intend to use the machine every day rather than treat it as a side project.\n\n**What Daily Use Looks Like**\n\nThe real test was whether the machine stayed out of my way.\n\nQuantized 7B and 8B models run smoothly using Ollama and LM Studio. AnythingLLM works well too and adds vector databases and seamless access to cloud models when needed. Response times are short enough that interaction feels conversational rather than mechanical. I can draft, revise, and iterate without waiting on the system, which makes local use genuinely viable.\n\nLarger 13B to 14B models are more usable than I expected when configured sensibly. Context size needs to be managed, but that is true even on far more expensive systems. For single-user workflows, the experience is consistent and predictable.\n\nWhat stood out most was how quickly the hardware stopped being the limiting factor. Once the models were loaded and tools configured, I forgot I was using a constrained system. That is the point where performance stops being theoretical and starts being practical.\n\nIn daily use, I rotate through a familiar mix of models. Qwen variants from 1.7B up through 14B do most of the work, alongside Mistral instruct models, DeepSeek 8B, Phi-4, and Gemma. On this machine, smaller Qwen models routinely exceed 30 tokens per second and often land closer to 40 TPS depending on quantization and context. These smaller models can usually take advantage of the full available context without issue.\n\nThe 7B to 8B class typically runs in the low to mid 20s at context sizes between 4K and 16K. Larger 13B to 14B models settle into the low teens at a conservative 4K context and operate near the upper end of acceptable memory pressure. Those numbers are not headline-grabbing, but they are fast enough that writing, editing, and iteration feel fluid rather than constrained. I am rarely waiting on the model, which is the only metric that actually matters for my workflow.\n\n**Cost, Power, and Practicality**\n\nAt roughly $600, this system occupies an important middle ground. It costs less than a capable GPU-based desktop while delivering enough performance to replace a meaningful amount of cloud usage. Over time, that matters more than peak benchmarks.\n\nThe Mac mini M4 is also extremely efficient. It draws very little power under sustained inference loads, runs silently, and requires no special cooling or placement. I routinely leave models running all day without thinking about the electric bill.\n\nThat stands in sharp contrast to my Ryzen 5700G desktop paired with an Intel B50 GPU. That system pulls hundreds of watts under load, with the B50 alone consuming around 50 watts during LLM inference. Over time, that difference is not theoretical. It shows up directly in operating costs.\n\nThe M4 sits on top of my tower system and behaves more like an appliance. Thanks to my use of a KVM, I can turn off the desktop entirely and keep working. I do not think about heat, noise, or power consumption. That simplicity lowers friction and makes local models something I reach for by default, not as an occasional experiment.\n\n**Where the Limits Are**\n\nThe constraints are real but manageable. Memory is finite, and there is no upgrade path. Model selection and context size require discipline. This is an inference-first system, not a training platform.\n\nApple Silicon also brings ecosystem boundaries. If your work depends on CUDA-specific tooling or experimental research code, this is not the right machine. It relies on Appleâ€™s Metal backend rather than NVIDIAâ€™s stack. My focus is writing and knowledge work, and for that, the platform fits extremely well.\n\n**Why This Feels Like a Turning Point**\n\nWhat surprised me was not that the Mac mini M4 could run local LLMs. It was how well it could run them given the constraints.\n\nFor years, local AI was framed as something that required large amounts of RAM, a powerful CPU, and an expensive GPU. These systems were loud, hot, and power hungry, built primarily for enthusiasts. This setup points in a different direction. With efficient models and tightly integrated hardware, a small, affordable system can do real work.\n\nFor writers, researchers, and independent developers who care about control, privacy, and predictable costs, a budget local LLM machine built around the Mac mini M4 no longer feels experimental. It is something I turn on in the morning, leave running all day, and rely on without thinking about the hardware.\n\nMore than any benchmark, that is what matters.\n\nFrom: tonythomas-dot-net",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0uugib",
          "author": "TimLikesAI",
          "text": "I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 and use it similarly. The extra headroom allows for running some pretty powerful models that are in the 14-20GB range.",
          "score": 10,
          "created_utc": "2026-01-21 14:06:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x9s9c",
              "author": "fallingdowndizzyvr",
              "text": "> I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 \n\nYou could have gotten it new for cheaper on Ebay from a liquidator.",
              "score": -1,
              "created_utc": "2026-01-21 20:45:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0y4lrx",
                  "author": "jcktej",
                  "text": "Asking for a friend. How do I find such vendors?",
                  "score": 1,
                  "created_utc": "2026-01-21 23:12:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tyaic",
          "author": "PraxisOG",
          "text": "I sincerely hope this is the future. An easy to use, low upfront and ongoing cost box that privately serves LLMs and maybe more. The software, while impressive, leaves much to be desired in terms of usability. This is from the perspective of having recently thrown together the exact kind of loud and expensive box you mentioned, that took days to get usable output from.Â ",
          "score": 7,
          "created_utc": "2026-01-21 10:22:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0z12z0",
              "author": "kermitt81",
              "text": "The M5 Mac Mini is expected sometime in the middle of this year, and offers significant improvements for LLM usage over the M4 Mac Mini. Pricing will likely be around the same, so it may be well worth the wait. \n\n(Each of the M5â€™s 10 GPU cores now includes a dedicated Neural Accelerator, and - according to benchmarks - the M5 delivers 3.6x faster time to first token compared to the M4.)",
              "score": 3,
              "created_utc": "2026-01-22 02:11:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0uc6wg",
          "author": "locai_al-ibadi",
          "text": "It is great seeing the capabilities of localised AI recently, compared to what we were capable of running a year ago (arguably even a months ago).",
          "score": 3,
          "created_utc": "2026-01-21 12:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ud5rp",
          "author": "alias454",
          "text": "If you look around you can get it for cheaper than that(micro center and best buy open box deals). I picked up an open box one and have actually been impressed. The onboard storage is probably the biggest complaint but it will do for now. My main laptop is an older Lenovo Legion 5 with 32GBs of ddr4 and an rtx2060. It was purchased around 2020 so it is starting to show it's age.",
          "score": 3,
          "created_utc": "2026-01-21 12:23:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v5cvv",
              "author": "tony10000",
              "text": "It is easy to add an external drive or a dock with a NVME slot for additional storage.  I keep all of the models, data, and caches on that.",
              "score": 5,
              "created_utc": "2026-01-21 15:01:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v0i9e",
          "author": "Rabo_McDongleberry",
          "text": "For me. Speed isn't that big of an issue. And most of the things I do are just basic text generation and editing. Maybe answer a few questions that I cross references with legit sources. The Mac Mini works great for that.Â ",
          "score": 3,
          "created_utc": "2026-01-21 14:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x9ogi",
          "author": "fallingdowndizzyvr",
          "text": "$600! Dude overpaid. It's $400 at MC.",
          "score": 3,
          "created_utc": "2026-01-21 20:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zh61a",
              "author": "DerFreudster",
              "text": "I think that included the dock and the nvme.",
              "score": 1,
              "created_utc": "2026-01-22 03:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o102sjy",
                  "author": "tony10000",
                  "text": "Correct.  That was for everything.  I got the M4 for $479.",
                  "score": 2,
                  "created_utc": "2026-01-22 06:13:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wixf8",
          "author": "dual-moon",
          "text": "how much have you tried small models? many of them are extremely good; lots of tiny models get used as subagents in swarm setups. LiquidAI actually JUST released a 1.2B LFM2.5 Thinking model that would probably FLY on your machine :)",
          "score": 2,
          "created_utc": "2026-01-21 18:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wub17",
              "author": "cuberhino",
              "text": "Iâ€™m currently considering a threadripper + 3090 build at around $2000 total cost to function as a local private ChatGPT replacement. \n\nDo you think this is overkill and I should go with one of these cheaper Mac systems?",
              "score": 2,
              "created_utc": "2026-01-21 19:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0x4ao8",
                  "author": "dual-moon",
                  "text": "try out some smaller models first, see how they work for you! if you find small models do the job, then scaling based on an agentic swarm rather than a single model may be best! but it really depends on what you want to use it for. if it's just chatting, deepseek can do most of what the big guys can!\n\nbut don't think a threadripper and a 3090 is a bad idea or anything :p",
                  "score": 1,
                  "created_utc": "2026-01-21 20:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o103wgb",
              "author": "tony10000",
              "text": "Yes.  I use the small LiquidAI models, and Qwen 1.7B is a tiny-mighty LLM for drafting.",
              "score": 2,
              "created_utc": "2026-01-22 06:22:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0y9qn3",
          "author": "yeeah_suree",
          "text": "Nice write up! Can you share a little more information on what you use the model for? What constitutes everyday use?",
          "score": 2,
          "created_utc": "2026-01-21 23:39:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103q6c",
              "author": "tony10000",
              "text": "I am a writer.  I use AI for brainstorming, outlining, summarizing, drafting, and sometimes editing and polishing.",
              "score": 2,
              "created_utc": "2026-01-22 06:21:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0vutf5",
          "author": "track0x2",
          "text": "I heard the primary limitation for LLMs on Macâ€™s is around text generation and if you want to do anything other than that (image gen, TTS, agentic work) they will struggle.",
          "score": 1,
          "created_utc": "2026-01-21 16:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vw0we",
              "author": "tony10000",
              "text": "It really depends on your use case and expectations.  Some very capable models are 14B and under.  If I need more capabilities, I can run some 30B models on my 5700G.  For more than that, there is ChatGPT and Open Router.",
              "score": 1,
              "created_utc": "2026-01-21 17:03:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wlbfx",
          "author": "Parking_Bug3284",
          "text": "This is really cool. I'm still sharing my gpu on my main but I'm building a similar thing on the software side. It sets up a base control system for local systems running on your machine. So if you have ollama and opencode it can build out what you need to gain access to unlimited memory management and access to programs that have a server running like image gen and what not. Does your system have an APi or mcp server to talk to it",
          "score": 1,
          "created_utc": "2026-01-21 18:54:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104kqn",
              "author": "tony10000",
              "text": "I use Anything LLM for API access, MCP, and RAG vector databases.",
              "score": 1,
              "created_utc": "2026-01-22 06:28:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10gg8v",
                  "author": "SelectArrival7508",
                  "text": "which is really nice as you can switch between you local llm and cloud-based llms",
                  "score": 1,
                  "created_utc": "2026-01-22 08:12:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wyis3",
          "author": "jarec707",
          "text": "Agreed, depending on use. Thanks for sharing the models you use; seem like good choices.",
          "score": 1,
          "created_utc": "2026-01-21 19:54:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104wim",
              "author": "tony10000",
              "text": "Yeah, I have a nice assortment of models.  Probably 200GB worth at present.",
              "score": 1,
              "created_utc": "2026-01-22 06:31:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x1zxe",
          "author": "Zyj",
          "text": "You could try to get a used PC with a RTX 5060 Ti 16GB for almost the same amount of money, like this one [https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369](https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369)",
          "score": 1,
          "created_utc": "2026-01-21 20:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104eke",
              "author": "tony10000",
              "text": "Not compact, portable, energy efficient, quiet, with low thermals.",
              "score": 1,
              "created_utc": "2026-01-22 06:27:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zg50d",
          "author": "Icy-Pay7479",
          "text": "Was this written by an 8b model?",
          "score": 1,
          "created_utc": "2026-01-22 03:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103f9p",
              "author": "tony10000",
              "text": "I typically use Qwen 14B for outlining, and my primary drafting models are Qwen 3B and 4B.  Sometimes even 1.7.  I use ChatGPT to polish and then heavily edit the result.",
              "score": 1,
              "created_utc": "2026-01-22 06:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zpx40",
          "author": "crossfitdood",
          "text": "dude you missed the black friday deals. I bought mine from costco for $479. But I was really upset when Microcenter had them on sale for $399",
          "score": 1,
          "created_utc": "2026-01-22 04:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1041ul",
              "author": "tony10000",
              "text": "Bought it from Amazon for $479.  It was around $600 all in for the M4, dock, and 1TB NVME.",
              "score": 1,
              "created_utc": "2026-01-22 06:24:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zretd",
          "author": "vmjersey",
          "text": "But can you play GTA V on it?",
          "score": 1,
          "created_utc": "2026-01-22 04:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1045ag",
              "author": "tony10000",
              "text": "No idea what that is.",
              "score": 1,
              "created_utc": "2026-01-22 06:24:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10f5cl",
          "author": "lucasbennett_1",
          "text": "Quantized 7B-14B like qwen3 or deepseek 8B run fluidly for writing/research without the power/heat mess of discrete GPUs. THe external nvme for models is a smart hack to dodge apples storage premiums too",
          "score": 1,
          "created_utc": "2026-01-22 08:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11zlww",
          "author": "Known_Geologist1085",
          "text": "I know this convo is about doing it on the cheap, but I'd like to note that I have been running an m3 max macbook pro with 128GB ram for a couple years now and the unified memory is a god send.  There are some support issues with Metal/MPS for certain things related to certain quantization and sparse attention, but overall these machines are beasts.   I can get 40-50 tps on llama 70b.  The good news now is that the market is flooded with these m chip macbook airs and pros, and a lot of them are cheap if you buy used.  I wouldn't be surprised if someone makes, or has already made, software to cluster macs for inference stacks in order to find a use for these.",
          "score": 1,
          "created_utc": "2026-01-22 14:50:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiswy6",
      "title": "Olmo 3.1 32B Think â€” second place on hard reasoning, beating proprietary flagships",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-01-21 08:56:26",
      "score": 42,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "Running peer evaluations of frontier models (The Multivac). Today's constraint satisfaction puzzle had interesting results for local LLM folks.\n\n**Top 3:**\n\n1. Gemini 3 Pro Preview: 9.13\n2. **Olmo 3.1 32B Think: 5.75** â† Open source\n3. GPT-OSS-120B: 4.79 â† Open source\n\n**Models Olmo beat:**\n\n* Claude Opus 4.5 (2.97)\n* Claude Sonnet 4.5 (3.46)\n* Grok 3 (2.25)\n* DeepSeek V3.2 (2.99)\n\n**The task:** Schedule 5 people for meetings across Mon-Fri with 9 interlocking logical constraints. Requires recognizing structural impossibilities and systematic constraint propagation.\n\n**Notes on Olmo:**\n\n* High variance (Â±4.12) â€” inconsistent but strong ceiling\n* Extended thinking appears to help on this problem class\n* 32B is runnable on consumer hardware (with quantization)\n* Apache 2.0 license\n\n**Questions for the community:**\n\n* What quantizations are people running Olmo 3.1 at?\n* Performance on other reasoning tasks?\n* Any comparisons vs DeepSeek for local deployment?\n\nFull results at [themultivac.com](http://themultivac.com)\n\nLink: [https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm\\_campaign=post&utm\\_medium=web&showWelcomeOnShare=true](https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\nhttps://preview.redd.it/jko9h4ox2oeg1.png?width=1208&format=png&auto=webp&s=07f7967899cc6f7d6252eed866ef5f4003f3288b\n\n**Daily runs and Evals. Cheers!**",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0tqymb",
          "author": "Mabuse046",
          "text": "I like Allen AI's reasoning dataset and I tend to take chunks of it to train my own models on. It's nice to see them scoring highly.",
          "score": 7,
          "created_utc": "2026-01-21 09:13:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzro0",
              "author": "randygeneric",
              "text": "you mean benchmaxing?",
              "score": -5,
              "created_utc": "2026-01-21 10:36:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ua463",
                  "author": "silenceimpaired",
                  "text": "Why on earth are you asking this question? No where does the person youâ€™re commenting on mention test sets. They mention reasoning traces. Now if you were to say Allen AI has polluted datasets with test data present that would make sense.",
                  "score": 9,
                  "created_utc": "2026-01-21 12:01:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tzul1",
                  "author": "Mabuse046",
                  "text": "What do you mean?",
                  "score": 5,
                  "created_utc": "2026-01-21 10:37:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0w2m31",
                  "author": "Vegetable-Second3998",
                  "text": "This is such a dumb term. If a model learns the concepts of a benchmark, it has learned it. Create better benchmarks then. Thatâ€™s just doing what humans also do - memorize shit for efficiency.",
                  "score": 1,
                  "created_utc": "2026-01-21 17:33:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xps63",
          "author": "segmond",
          "text": "I hope you're right.  If so, this goes to show that folks should stop looking for the ONE magic model.  We saw with qwen2.5-32b-coder that a model can be very good in a specific domain.",
          "score": 1,
          "created_utc": "2026-01-21 21:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z5lcr",
          "author": "TheOdbball",
          "text": "Luckily for everyone Iâ€™m building a headless Linux based file system that would make changing out your LLM without losing years of productivity on the fly ðŸ˜Ž\n\nWill definitely try olmo out thanks",
          "score": 1,
          "created_utc": "2026-01-22 02:36:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfsyn8",
      "title": "128GB VRAM quad R9700 server",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1qfscp5",
      "author": "Ulterior-Motive_",
      "created_utc": "2026-01-17 23:56:56",
      "score": 38,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfsyn8/128gb_vram_quad_r9700_server/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o08mno6",
          "author": "Ult1mateN00B",
          "text": "Nice, I have Thredipper 3945WX, 128GB DDR4 and 4x R9700. Each one of them has x16 PCI-E 4.0. I do wonder how does 5700X limited PCI-E lanes limit the performance?\n\nhttps://preview.redd.it/hog8olc5g1eg1.jpeg?width=1365&format=pjpg&auto=webp&s=05791283b6c9d5646a9a9580699f3f39d35de2d0",
          "score": 6,
          "created_utc": "2026-01-18 04:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07umqk",
          "author": "Taserface_ow",
          "text": "wouldnâ€™t stacking the gpus like that cause them to overheat on high usage?",
          "score": 4,
          "created_utc": "2026-01-18 02:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07xqx8",
              "author": "Ulterior-Motive_",
              "text": "To an extent, but I have lots of airflow from the case fans and internal fans, and the cards are designed to allow air to flow through holes in the backplate. In practice, they're all within 2-3 C of each other, and I don't seem to have any overheating issues.",
              "score": 1,
              "created_utc": "2026-01-18 02:22:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o084d3p",
                  "author": "EmPips",
                  "text": "gotta love blower-coolers!",
                  "score": 2,
                  "created_utc": "2026-01-18 02:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08ghav",
          "author": "IngwiePhoenix",
          "text": ">Wraith Prism Cooler\n\nThats a brave soul right there XD",
          "score": 3,
          "created_utc": "2026-01-18 04:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a67nr",
          "author": "ReelTech",
          "text": "Is this mainly for inferencing or eg RAG training? and the cost?",
          "score": 2,
          "created_utc": "2026-01-18 12:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ajkkc",
              "author": "Ulterior-Motive_",
              "text": "Inference, mostly. I break down the costs in the OP, but it was a touch over $7k.",
              "score": 3,
              "created_utc": "2026-01-18 14:12:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0cytii",
          "author": "GCoderDCoder",
          "text": "Im debating doing one of these. How's gpt oss120b on vllm? Heck I'm begging for any server even llama.cpp. i want to get one but havent found benchmarks of gpt oss120b",
          "score": 2,
          "created_utc": "2026-01-18 21:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0h3go9",
              "author": "Ulterior-Motive_",
              "text": "Haven't installed vLLM yet, but here are my llama.cpp numbers:\n\n|model|size|params|backend|ngl|n\\_batch|n\\_ubatch|fa|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|gpt-oss 120B F16|60.87 GiB|116.83 B|ROCm|99|1024|1024|1|pp8192|5121.48 Â± 14.49|\n|gpt-oss 120B F16|60.87 GiB|116.83 B|ROCm|99|1024|1024|1|tg128|70.86 Â± 0.09|",
              "score": 2,
              "created_utc": "2026-01-19 13:45:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0h6801",
                  "author": "GCoderDCoder",
                  "text": "Awesome! Thanks soo much!! Already 50% faster than my strix halo! That's going to be such a great device! Congrats! Now I have to decide if I return my strix halo this week or not lol.\n\nFyi my strix halo has been having rocm issues since I bought it but I use fedora and this guy explicitly clarifies how the last month rocm on fedora has had degraded performance due to an update that broke rocm. There's actually two aspects that he defines so after I do some other things today I will be trying to work on vllm using these fixes on fedora:\n\nhttps://youtu.be/Hdg7zL3pcIs\n\nJust sharing since it seems we are both testing new AMD devices with vllm.",
                  "score": 2,
                  "created_utc": "2026-01-19 14:01:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o079qq0",
          "author": "SashaUsesReddit",
          "text": "Love it! Have you tried vllm on it yet?",
          "score": 1,
          "created_utc": "2026-01-18 00:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07a959",
              "author": "Ulterior-Motive_",
              "text": "Never used it before, I've always been a llama.cpp user, but I'm sure it's worth a look!",
              "score": 2,
              "created_utc": "2026-01-18 00:17:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o07aj3i",
                  "author": "SashaUsesReddit",
                  "text": "With 4x matching GPUs you can take advantage of tensor parallelism, which will way speed up your tokens. Llama.cpp can shard the model and span multiple GPUs but gains no token speed to do so\n\nHave fun!",
                  "score": 4,
                  "created_utc": "2026-01-18 00:19:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhg3pm",
      "title": "GLM-4.7-Flash-NVFP4 (20.5GB) is on huggingface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qhg3pm/glm47flashnvfp4_205gb_is_on_huggingface/",
      "author": "DataGOGO",
      "created_utc": "2026-01-19 20:42:39",
      "score": 36,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "I published a mixed precision NVFP4 quantized version of the new GLM-4.7-FLASH model on huggingface. \n\n  \nCan any of you test it out and let me know how it works for you?\n\n  \n[GadflyII/GLM-4.7-Flash-NVFP4 Â· Hugging Face](https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qhg3pm/glm47flashnvfp4_205gb_is_on_huggingface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qery3t",
      "title": "Training ideas with 900Gb of vram",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qery3t/training_ideas_with_900gb_of_vram/",
      "author": "soppapoju",
      "created_utc": "2026-01-16 20:56:10",
      "score": 35,
      "num_comments": 23,
      "upvote_ratio": 0.93,
      "text": "Hello, i have an opportunity to train something and use a \"supercomputer\".\n\nWhat would you do with this amount of vram available? About 10x H100\n\nThinking of training something and bringing it to personal use or to be used publicly on a website.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qery3t/training_ideas_with_900gb_of_vram/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o006wdx",
          "author": "FamiliarRice",
          "text": "I have an open source research company that uses around this much compute. Let me know if youâ€™re interested in collaborating and we will get you on some top ml conference papers !",
          "score": 26,
          "created_utc": "2026-01-16 22:21:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01x6cd",
              "author": "NoobMLDude",
              "text": "I would be interested in collaborating. \nWhat topics , domains , problems do you work on?\nCan I Dm you?",
              "score": 1,
              "created_utc": "2026-01-17 04:37:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzy2ga",
          "author": "WolfeheartGames",
          "text": "If you don't already have an important research idea to follow, distilling a larger model down to a non quadratic architecture is probably the highest impact you can have. \n\nIf you have an experiment to run, make sure it's solid first.\n\nIf you will allow me a moment to self promote, I have this https://github.com/bigwolfeman/TitanMAC-Standalone an implementation from a series of Google papers. I haven't worked out all the kinks and performance elements yet, but you can take the neural memory module and use it on any other architecture as a replacement for quadratic attention. The kernels are there for forward and back passes. I'd prefer if you contributed to this project though =) you'd be working through issues either way if you took the neural memory just as a factor of writing your own architecture. \n\nThere is a potential issue with the sliding window required for the neural memory, you may have chunk boundary issues. From my testing I have this resolved, but I haven't gone past 50k steps on it yet to make sure it doesn't crop up again later.\n\nI also submitted this to the hackathon for this subreddit https://github.com/bigwolfeman/Retnet-Distillation \n\nI do not recommend using retnet for this. It is extremely unstable and slow. I had to patch torchscale just to make it work at all and kept finding issues in torchscale. But there is a nice reference implementation of keeping the teacher in vram beside the student, caching it's logits to disk, or inferencing from a vllm server and exposing top k logits.\n\nEven with how unstable retention is, it may be worth implementing to resolve chunk boundary problems. I am currently not maintaining retention in TitanMAC, but if further testing shows issues I'm planning to use it.\n\nAlso, my repo has the nested learning module based on SGD. It is a memory efficient optimizer. Allowing for more parameters with less vram. \n\nIf you're interested in using this repo, reach out to me. I have a branch I am doing a lot of development work on. I have found that feeding in hyper connections directly to the nested optimizer gives it richer signals for convergence and is basically free to do. It may be possible to average gradient heuristics to 16x16 tiles using my development branch, but I haven't had time to test this yet. It is the last step of the dynamic sparse implementation I'm working on in that branch. \n\n---\n\nOther great use for so much vram: distill a VLM to V-JEPA.\nDistill to HOPE https://github.com/Sk16er/hope_nano HOPE is a vram monster, I don't think this architecture is ready to scale yet, but it's research value is tremendous.\n\nEdit: the custom kernels are targeting blackwell, but they're written in Triton. Getting them ready for an h100 shouldn't be too bad.\n\nIf this deployment you want to do is far enough out in the future, I am planning on implementing the entire architecture in mojo for further performance gains. I'm still ablating several takes on the sparsity before I do this. The current code isn't designed to shard across gpus, though pytorch makes it fairly easy. Mojo is very good at sharing, though I haven't tested that myself.",
          "score": 16,
          "created_utc": "2026-01-16 21:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o009vgj",
          "author": "RoyalCities",
          "text": "Download everything you can find from Annas archive and make your own Chatgpt.",
          "score": 12,
          "created_utc": "2026-01-16 22:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00unc0",
              "author": "No-Acanthaceae-5979",
              "text": "This",
              "score": 2,
              "created_utc": "2026-01-17 00:30:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o000pn1",
          "author": "amooz",
          "text": "Whereâ€™s the dude whoâ€™s training a model using only text from the 1800â€™s London?",
          "score": 10,
          "created_utc": "2026-01-16 21:50:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzzwf8",
          "author": "K33P4D",
          "text": "this is just too much  \nI'm here whipping 12B and 30B models, waiting 30 mins for an answer and this dude here with 900GB vRAM smh",
          "score": 8,
          "created_utc": "2026-01-16 21:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0065y8",
              "author": "much_longer_username",
              "text": "It sounds like they're just a guest on this system, probably through work or school. \n\nBut my dad always said \"it's only a lot of money if you don't have it\".",
              "score": 8,
              "created_utc": "2026-01-16 22:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o009vrn",
                  "author": "Large-Excitement777",
                  "text": "Still trying to wrap my head around how gullible everyone is",
                  "score": -2,
                  "created_utc": "2026-01-16 22:36:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o004vsj",
              "author": "Osprey6767",
              "text": "yeah lol same",
              "score": 1,
              "created_utc": "2026-01-16 22:11:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzxnub",
          "author": "Aromatic-Low-4578",
          "text": "I'd pretrain an LLM",
          "score": 2,
          "created_utc": "2026-01-16 21:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzzptm",
          "author": "Sicarius_The_First",
          "text": "If I'd write all I would do with it, it would be several pages long.",
          "score": 2,
          "created_utc": "2026-01-16 21:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzxqua",
          "author": "Hoak-em",
          "text": "\"planning\"-optimized glm4.7 would be interesting, though I'm not sure how much work that would take.",
          "score": 1,
          "created_utc": "2026-01-16 21:36:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o033n3l",
          "author": "soppapoju",
          "text": "900gb is an estimate. It might be more might be less, when i get to know the exact specs of the machine.\n\nAs person new to LLM training, is why I ask.\n\nGot some ideas with gpt but human answers always better!\n\nI can que to use it, but it needs to be something worthwhile to me or to a lot of people.\n\nBest Regards all!",
          "score": 1,
          "created_utc": "2026-01-17 10:48:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o03b3e0",
          "author": "alexbarbershop",
          "text": "I would set it loose on a very high quality curated historical dataset or historical imagery datasets as that is a very weak spot in all AI models currently.  Coincidentally, I do have over 1000 negative scans of public domain trolley images from the NEHR trolley museum collection on DigitalMaine and a massive spreadsheet of metadata I compiled manually.",
          "score": 1,
          "created_utc": "2026-01-17 11:54:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03p8fb",
          "author": "cjstoddard",
          "text": "Depends, do you want to get rich or do you want to do legitimate research? If you want to get rich, I'd say you can't go wrong with porn, if you want to do legitimate research, I got nothing.",
          "score": 1,
          "created_utc": "2026-01-17 13:36:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gz5yd",
          "author": "CurtissYT",
          "text": "Id train an ai on purely on cia conspiracy theories and something like the epstein files, etc. Something funny. Maybe something about HolyC and other stuff like that",
          "score": 1,
          "created_utc": "2026-01-19 13:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o100j8v",
          "author": "Calebhk98",
          "text": "Try training some smaller models in an evolutionary method. As in, they interact with each other. You probably want to also use ICM. You likely can't do anything unique going for just a larger model, but running 10-20 smaller models interacting with each and training evolutionary is not commonly done right now.",
          "score": 1,
          "created_utc": "2026-01-22 05:56:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004io8",
          "author": "UseMoreBandwith",
          "text": "I would start with   \nwriting better reddit posts",
          "score": -2,
          "created_utc": "2026-01-16 22:09:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhqu6x",
      "title": "DeepSeek V3.2 (open weights) beats GPT-5.2-Codex and Claude Opus on production code challenge â€” The Multivac daily blind peer eval",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qhqu6x/deepseek_v32_open_weights_beats_gpt52codex_and/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-01-20 04:08:48",
      "score": 32,
      "num_comments": 11,
      "upvote_ratio": 0.83,
      "text": "**TL;DR:** DeepSeek V3.2 scored 9.39 to beat GPT-5.2-Codex (9.20) and every other closed model on a complex coding task. But the real story is Claude Sonnet 4.5 got scored anywhere from 3.95 to 8.80 by different judges â€” same exact code.\n\n# The Test\n\nWe asked 10 models to write a production-grade nested JSON parser with:\n\n* Path syntax (\"user.profile.settings.theme\")\n* Array indexing (\"users\\[0\\].name\")\n* Circular reference detection\n* Typed results with error messages\n* Full type hints and docstrings\n\nThis is a real-world task. Every backend engineer has written something like this.\n\n# Results\n\n|Rank|Model|Score|Std Dev|\n|:-|:-|:-|:-|\n|1|**DeepSeek V3.2**|9.39|0.80|\n|2|GPT-5.2-Codex|9.20|0.50|\n|3|Grok 3|8.89|0.76|\n|4|Grok Code Fast 1|8.46|1.10|\n|5|Gemini 3 Flash|8.16|0.71|\n|6|Claude Opus 4.5|7.57|1.56|\n|7|Claude Sonnet 4.5|7.02|2.03|\n|8|Gemini 3 Pro|4.30|1.38|\n|9|GLM 4.7|2.91|3.61|\n|10|MiniMax M2.1|0.70|0.28|\n\n**Open weights won.** DeepSeek V3.2 is fully open.\n\n# The Variance Problem (responding to yesterday's feedback)\n\nToday's data supports this. Look at Claude Sonnet's std dev: **2.03**\n\nThat's a 5-point spread (3.95 to 8.80) on the same response. Judges fundamentally disagreed on what \"good\" means.\n\nCompare to GPT-5.2-Codex with 0.50 std dev â€” everyone agreed within \\~1 point.\n\nWhen evaluators disagree this much, the benchmark is under-specified.\n\n# Judge Strictness (meta-analysis)\n\n|Judge|Avg Score Given|\n|:-|:-|\n|Claude Opus 4.5|5.92 (strictest)|\n|Claude Sonnet 4.5|5.94|\n|GPT-5.2-Codex|6.07|\n|DeepSeek V3.2|7.88|\n|Gemini 3 Flash|9.11 (most lenient)|\n\nClaude models judge harshly but score mid-tier themselves. Interesting pattern.\n\n# What We're Adding (based on your feedback)\n\n**5 open-weight models for tomorrow:**\n\n1. Llama-3.3-70B-Instruct\n2. Qwen2.5-72B-Instruct\n3. Mistral-Large-2411\n4. **Big-Tiger-Gemma-27B-v3** (u/ttkciar suggested this â€” anti-sycophancy finetune)\n5. Phi-4\n\n**New evaluation dimension:** We're adding \"reasoning justification\" scoring â€” did the model explain its approach, not just produce correct-looking output?\n\n# Methodology\n\nThis is The Multivac â€” daily 10Ã—10 blind peer matrix:\n\n* 10 models respond to same question\n* Each model judges all 10 responses (100 total judgments)\n* Models don't know which response came from which model\n* Rankings from peer consensus, not single evaluator\n\nFull responses and analysis: [https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&utm\\_campaign=post&utm\\_medium=web&showWelcomeOnShare=true](https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\n[themultivac.com](http://themultivac.com)\n\n**Questions welcome. Roast the methodology. That's how we improve.**",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qhqu6x/deepseek_v32_open_weights_beats_gpt52codex_and/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0mmv0v",
          "author": "ResidentPositive4122",
          "text": "If you want accurate and repeatable results you have to use easily verifiable results, not LLM-as-a-judge. Have your question come with a (hidden) test suite. Have the models solve the problem and exit at their own choosing. Run the testing in a separate env. Grade. Anything else is hubris.",
          "score": 9,
          "created_utc": "2026-01-20 07:18:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qv9je",
              "author": "Charming_Support726",
              "text": "Exactly this. I ran a DeepSeek 3.2 evaluation with a harness of mine. Project for steering subagents. I testet a few models with a simple test set. It barely completed. \n\nMost open weight models weren't able to complete an autonomous full agentic coding task - similar to what you mentioned. If you are observing the model while issue commands, it's different game.",
              "score": 1,
              "created_utc": "2026-01-20 21:57:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ly9rh",
          "author": "iMrParker",
          "text": "What do you say to the models when you ask them to judge the solutions? Do they choose their own criteria? I feel like the code creations I get from LLMs and agentic workflows don't care about maintainability or readability, resulting in code that won't scale well (lack of modularity etc.). It would be interesting to see what their creations were and judge based on my own experience as a SWE. \n\nI just don't LLMs to judge the outputs of LLMs",
          "score": 2,
          "created_utc": "2026-01-20 04:16:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0npb6s",
          "author": "ForsookComparison",
          "text": "After using all of these models daily for work I can't take benchmarks seriously anymore, none of them.\n\nGPT 5.2 and Deepseek V3.2 are not in the same category.\n\nNeither is in the same category as Opus 4.5 when it comes to code. The gap is monstrous.",
          "score": 2,
          "created_utc": "2026-01-20 12:49:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qvvzl",
              "author": "Charming_Support726",
              "text": "I agree. But as written multiple times - gpt-5.2-codex has advantages over Opus in distinct areas of work",
              "score": 1,
              "created_utc": "2026-01-20 22:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0s1cip",
                  "author": "ForsookComparison",
                  "text": "I'm open to shop. Where do you find codex beating opus lately?",
                  "score": 1,
                  "created_utc": "2026-01-21 01:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0owmxa",
          "author": "GCoderDCoder",
          "text": "I think the hard thing is all these models have things they are better at and others they are worse at. Some follow instructions better but are less accurate making their own logic. I feel qwen3 coder480B handles java spring boot better than a bunch of models supposedly ranked better in swe bench. I'd be curious to see this methodology applied to other categories of tasks. I assume that's what the other benchmarks aim to do but I havent taken the time to investigate deeper into them to know. So I might be recommending redundant efforts but I appreciate you bringing it to the masses directly this way",
          "score": 1,
          "created_utc": "2026-01-20 16:33:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tlqh1",
          "author": "Alone-Marionberry-59",
          "text": "Nobody really cares about these sorts of tasks anymore though.. itâ€™s more like long-running, huge repo, tasks that provide something useful and distinguish a model today, not something done infinitely many times on GitHubâ€¦",
          "score": 1,
          "created_utc": "2026-01-21 08:22:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg403g",
      "title": "Claude Code and local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qg403g/claude_code_and_local_llms/",
      "author": "rivsters",
      "created_utc": "2026-01-18 09:13:25",
      "score": 27,
      "num_comments": 22,
      "upvote_ratio": 0.89,
      "text": "This looks promising - will be trying later today [https://ollama.com/blog/claude](https://ollama.com/blog/claude) \\- although  blog says \"It is recommended to run a model with at least 64k tokens context length.\" Share if you are having success using it for your local LLM. \n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qg403g/claude_code_and_local_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0e71ur",
          "author": "lol-its-funny",
          "text": "EDIT: I tested that even newer llama-server can do this natively without requiring litellm in the middle. See [https://www.reddit.com/r/LocalLLaMA/comments/1qhaq21/comment/o0jtqr0/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qhaq21/comment/o0jtqr0/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\n\\----  \n  \nFYI, LiteLLM can already do this. It can connect with both Anthropic (Claude Code) or OpenAI clients (ChatGPT desktop) and then connect those to Anthropic or OpenAI or llama-cpp (openai compatible) or other native providers.\n\nNot a fan of Ollama with their cloud direction. Anyone interested in cloud will go to the native providers or aggregators like openrouter. Local LLM folks donâ€™t need another cloud focused service.",
          "score": 16,
          "created_utc": "2026-01-19 01:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gkgim",
          "author": "SatoshiNotMe",
          "text": "As others said, months ago llama.cpp already added anthropic messages API compatibility for some popular open-weight LLMs. This makes it easy to hook up these LLMs to work with CC. I had to hunt around for the specific llama-server flag settings for these models and I gathered these into a little guide on setting up these models to work with CC and Codex CLI:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nOne subtle thing to note is that you have to set \n\nCLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC\": \"1\"\n\nin your CC settings, to avoid total network failure due to CCâ€™s logging pings.",
          "score": 7,
          "created_utc": "2026-01-19 11:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hqlpq",
              "author": "Tema_Art_7777",
              "text": "    Â  Â  \"claudeCode.environmentVariables\": [\n    Â  Â  Â  Â  {\n    Â  Â  Â  Â  Â  Â  \"name\": \"ANTHROPIC_BASE_URL\",\n    Â  Â  Â  Â  Â  Â  \"value\": \"http://192.168.10.65:8282\"\n    Â  Â  Â  Â  },\n    Â  Â  Â  Â  {\n    Â  Â  Â  Â  Â  Â  \"name\": \"CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC\",\n    Â  Â  Â  Â  Â  Â  \"value\": \"1\"\n    Â  Â      }\n    Â  Â  ]\n\nThanks for the URL. I am using claudecode in visual studio code. I added to my settings these lines, however, instead of invoking the llama.cpp server, it is asking me to login with anthropic keys. it actually never calls my server. Did I miss a step?",
              "score": 1,
              "created_utc": "2026-01-19 15:43:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0hzz86",
                  "author": "eli_pizza",
                  "text": "You need to set an API key env var too (set it to anything) so it doesnâ€™t try to use oauth",
                  "score": 1,
                  "created_utc": "2026-01-19 16:25:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0eqz5y",
          "author": "Tuned3f",
          "text": "Llama.cpp had this months ago",
          "score": 3,
          "created_utc": "2026-01-19 02:53:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0euind",
              "author": "Tema_Art_7777",
              "text": "How do you hookup claude code to llama.cpp??",
              "score": 2,
              "created_utc": "2026-01-19 03:12:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0evhe8",
                  "author": "Tuned3f",
                  "text": "Set ANTHROPIC_BASE_URL to the llama.cpp endpoint",
                  "score": 4,
                  "created_utc": "2026-01-19 03:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0dy6x3",
          "author": "cars_and_computers",
          "text": "If this is what I think. This is kind of a game changer. The power of Claude code cli but with the privacy of your local models is awesome. Assuming it's actually private. If not it would be amazing to have llamacpp have something like this working and have it be a truly private session",
          "score": 3,
          "created_utc": "2026-01-19 00:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fb0fq",
              "author": "Big-Masterpiece-9581",
              "text": "Claude code router and lite-llm already exist. But I donâ€™t think this is legal. Theyâ€™ll probably sue.",
              "score": 3,
              "created_utc": "2026-01-19 04:57:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0i09me",
                  "author": "eli_pizza",
                  "text": "Sue under DMCA anti-circumvention provision? Seems like a stretch.",
                  "score": 1,
                  "created_utc": "2026-01-19 16:26:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0fawba",
          "author": "Big-Masterpiece-9581",
          "text": "How is this kosher with the proprietary Claude license? They donâ€™t want you using other tools with their models. I have to assume they donâ€™t want you using their tool with other models.",
          "score": 1,
          "created_utc": "2026-01-19 04:56:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i0uuq",
              "author": "eli_pizza",
              "text": "I imagine theyâ€™re quite happy to have the whole industry standardize on Claude code as the best agent, especially local LLM enthusiasts.",
              "score": 1,
              "created_utc": "2026-01-19 16:29:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0m79ph",
              "author": "Internal_Werewolf_48",
              "text": "Claude Code has about a half dozen competing tools on the market. Qwen Code CLI, Mistral Vibe, Charm Crush, Codex, OpenCode, Plandex, Gemini CLI. Probably another five dozen vibe coded slop projects that are already abandoned too.\n\nClaude doesnâ€™t really offer anything that special to bother defending.",
              "score": 1,
              "created_utc": "2026-01-20 05:14:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0m7lpw",
                  "author": "Big-Masterpiece-9581",
                  "text": "I think they have done a fantastic job and constantly innovate. MCP and skills are two of the biggest as well as the cli and its autonomous workflow. But theyâ€™re easy to copy and others do similar things. Iâ€™m really enjoying opencode.",
                  "score": 1,
                  "created_utc": "2026-01-20 05:16:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0htoml",
          "author": "SatoshiNotMe",
          "text": "Can you first check if this works on the command line ? I donâ€™t use VSCode so not too familiar with how to set up CC there.",
          "score": 1,
          "created_utc": "2026-01-19 15:57:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3l75",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5lo30v55iqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 17:05:10",
      "score": 27,
      "num_comments": 11,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3l75/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0ynu7l",
          "author": "RnRau",
          "text": "You might enjoy this paper from 2024;\n\n> Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget. \n\n\nhttps://arxiv.org/abs/2407.21787",
          "score": 2,
          "created_utc": "2026-01-22 00:55:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10txky",
          "author": "techlatest_net",
          "text": "5-AI debate cage match to kill hallucinations? Brutal eleganceâ€”forces consensus without trusting any single braindead output. Local Ollama hookup is the killer tho, total sovereignty.\n\nSpinning this up tonight. How's the inference overhead hit when all 5 duke it out? ðŸ”¥",
          "score": 1,
          "created_utc": "2026-01-22 10:19:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12pjpn",
              "author": "PastTrauma21",
              "text": "Agreed! Having multiple models debate really helps filter out the noise, and not relying on just one output feels much safer.",
              "score": 1,
              "created_utc": "2026-01-22 16:50:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zyk7h",
          "author": "tvixii",
          "text": "just like how pewdiepie did it lol",
          "score": 0,
          "created_utc": "2026-01-22 05:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w1qch",
          "author": "jaxupaxu",
          "text": "So you copied pewdiepies council concept.Â ",
          "score": -16,
          "created_utc": "2026-01-21 17:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w353u",
              "author": "pokemonplayer2001",
              "text": "https://github.com/karpathy/llm-council",
              "score": 15,
              "created_utc": "2026-01-21 17:35:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0w54x5",
                  "author": "S_Anv",
                  "text": "Karpathy is a great man!\n\nKEA Research is designed as a user-friendly evolution. I've added image support, PDF/md export, text-to-speech conversion, and a full-fledged admin panel for managing local model sets without editing configuration files and many other features\n\nThis means you can create your own model set through a graphical interface  \nAlso as you see there is a bit different logic. You can check readme",
                  "score": 10,
                  "created_utc": "2026-01-21 17:44:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0w4qz2",
              "author": "FirstEvolutionist",
              "text": "I'm not suggesting OP didn't copy the idea from pewdiepie after hearing it from the then, but the idea of a \"council\" is not new, nor introduced by Pewidepie.",
              "score": 12,
              "created_utc": "2026-01-21 17:42:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ou0h",
              "author": "usercantollie",
              "text": "Yes, there are definitely similarities to PewDiePieâ€™s council idea, but the approach here adds structured verification and provider-agnostic flexibility, which opens up new possibilities for local and open-source models. Itâ€™s cool to see how different projects build on each other to solve the trust problem in unique ways.",
              "score": 1,
              "created_utc": "2026-01-22 16:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yvab0",
          "author": "neoKushan",
          "text": "Can you get one of them to check your spelling before you post next time?",
          "score": -4,
          "created_utc": "2026-01-22 01:38:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103d5h",
              "author": "Free-Internet1981",
              "text": "oof",
              "score": 1,
              "created_utc": "2026-01-22 06:18:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjqhja",
      "title": "This Week's Hottest Hugging Face Releases: Top Picks by Category!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "author": "techlatest_net",
      "created_utc": "2026-01-22 09:52:51",
      "score": 26,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.\n\nCheck 'em out and drop your thoughtsâ€”which one's getting deployed first?\n\n# Text Generation\n\n* [**zai-org/GLM-4.7-Flash**](https://huggingface.co/zai-org/GLM-4.7-Flash): 31B param model for fast, efficient text genâ€”updated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.\n* [**unsloth/GLM-4.7-Flash-GGUF**](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF): Quantized 30B version for easy local inferenceâ€”hot with 112k downloads in hours. Great for low-resource setups.\n\n# Image / Multimodal\n\n* [**zai-org/GLM-Image**](https://huggingface.co/zai-org/GLM-Image): Image-text-to-image powerhouseâ€”10.8k downloads, 938 likes. Excels in creative edits and generation.\n* [**google/translategemma-4b-it**](https://huggingface.co/google/translategemma-4b-it): 5B vision-language model for multilingual image-text tasksâ€”45.4k downloads, supports translation + vision.\n\n# Audio / Speech\n\n* [**kyutai/pocket-tts**](https://huggingface.co/kyutai/pocket-tts): Compact TTS for natural voicesâ€”38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.\n* [**microsoft/VibeVoice-ASR**](https://huggingface.co/microsoft/VibeVoice-ASR): 9B ASR for multilingual speech recognitionâ€”ultra-low latency, 816 downloads already spiking.\n\n# Other Hot Categories (Video/Agentic)\n\n* [**Lightricks/LTX-2**](https://huggingface.co/Lightricks/LTX-2) (Image-to-Video): 1.96M downloads, 1.25k likesâ€”pro-level video from images.\n* [**stepfun-ai/Step3-VL-10B**](https://huggingface.co/stepfun-ai/Step3-VL-10B) (Image-Text-to-Text): 10B VL model for advanced reasoningâ€”28.6k downloads in hours.\n\nThese are dominating trends with massive community traction.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qhqf8p",
      "title": "LLM Sovereignty For 3 Years.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qhqf8p/llm_sovereignty_for_3_years/",
      "author": "Technical_Buy_9063",
      "created_utc": "2026-01-20 03:49:10",
      "score": 25,
      "num_comments": 45,
      "upvote_ratio": 0.83,
      "text": "Hi folks, perhaps there is a dedicated thread for this but my basic question is:\n\nI want to be able to run LLMs locally, performantly, for the next 3 years or so. I am afraid of compute part costs skyrocketing further, I'm afraid of credits getting more expensive, and I am afraid of cloud offerings being censored in my country. \n\nI have \\~10k USD budget. What would you do in my shoes? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qhqf8p/llm_sovereignty_for_3_years/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0lyko0",
          "author": "Caprichoso1",
          "text": "You could get an Apple M3 Ultra with 80 GPU cores and 512 GB of memory for < $10k.  Some things it will do better than an expensive GPU card with limited memory, for other things a GPU card will outperform it.  It all depends on what you are going to do.",
          "score": 24,
          "created_utc": "2026-01-20 04:17:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n1j69",
              "author": "RandomCSThrowaway01",
              "text": "I should suggest to wait a month since it looks like Apple is preparing new Pro and Max chips at least already (there's suddenly a huge delay on all Pro models), might also include Ultra potentially.",
              "score": 5,
              "created_utc": "2026-01-20 09:34:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0pr7g1",
                  "author": "_hephaestus",
                  "text": "MBPs using the Ultra might possibly be out in a month, but the studio form factor is probably a while longer away with their release cycles.",
                  "score": 1,
                  "created_utc": "2026-01-20 18:53:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0uvp9d",
                  "author": "Late-Assignment8482",
                  "text": "As a career-long Apple professional, I doubt spring. Mac Pro and similar or entirely new products (Vision Pro, Studio) tend to drop at WWDC (June).\n\nNever say never. But June is both closer to timing for say, a Studio using an \"M4 Ultra\" which is really \"make an Ultra newer than M3\" and could also be M5 (it's been 18 months since M4) and more in the Apple corporate calendar.\n\nIf they managed to get it down into a laptop? Any day, they're actually a bit *late* on MacBook Pro.",
                  "score": 1,
                  "created_utc": "2026-01-21 14:12:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0oamn2",
              "author": "GCoderDCoder",
              "text": "I agree and will add that a 256gb m3 ultra runs most of the large open weight Chinese models at usable speeds. Mac can run larger models at usable speeds. Cuda runs smaller models faster. $50k H100s are 100gb roughly each. They are made to serve lots of requests simultaneously. A single person wanting better models can let some of that go to have infinite local access to those cloud models. \n\nIt's not just access to quality local as local really focuses on sovereignty. I can let local llms look through folders with sensitive information with my internet disconnected and even when connected I have the logs in front of me to see exactly what is being transmitted vs the black box from cloud providers.\n\nVs code with extensions and models like glm4.7 will feel like slower versions of cursor and claude code. It is still faster than I can read and anything going to prod should at least be skimmed. Managing context becomes the focus for performance locally so any tool that breaks the work down coupled with your own logical decomposition of your tasks will keep your model faster and make it more enjoyable. Glm4.7 sits in the 15-20t/s space on mac studio when context isnt too heavy but can crawl when context is heavy. \n\nCuda for any given vram amount is most expensive so consider what speed you personally need and have a model(s) in mind for what you buy. A $2k strix halo gets gpt-oss-120b at 50 tokens per second on small context prompt vs needing 3x 3090s ($4-5k) entry level cuda for models that can'tcode like the cloud. I would honestly recommend pairing something smaller and faster like gpt-oss120b and something bigger like glm4.7 so that you have the feeling of fast action vs slower more deliberate responses without having to reload models in between. Gpt-oss-120b on mac studio is like 75-85t/s I think vs 3-4 3090s is 100t/s vs strix halo is 50t/s. \n\nI keep gpt-oss-120b loaded for tool calls like internet searches and quick questions. Coding I use glm4.7/4.6, qwen3coder480b q3kxl and it's reap version from unsloth at q4kxl, and minimaxm2.1. \n\nFor $10k you could get the 512gb mac studio and run multiple models this way simultaneously. However 2x256gb ones can be clustered I think for really big models but then you'd have two sets of processors with 512gb vram instead of one cpu and one gpu handling multiple calls simultaneously.  Now we're multitasking. You could speed up glm4.7 locally or have two of them at once or pair with a smaller faster model like minimax m2.1... $10k leaves you a lot of options to consider.\n\nLast thing, people bash locally hosted models. Im telling you I fight with cloud models like chatgpt and gemini on some tasks that I can give to my local glm4.7 or even gpt-oss-120b and get what I want the first time. The cloud has lots going on behind the scenes and they tune the models for their inference patterns and experience they want you to have. You can have more control to tune to what you want locally but it will take more effort to have a really good experience.",
              "score": 3,
              "created_utc": "2026-01-20 14:48:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lyyvm",
          "author": "TheAussieWatchGuy",
          "text": "I see this asked a lot.\n\n\nCloud AI runs on multiple $50k GPUs and has hundreds of billions of parameters.\n\n\nLocal AI can't yet really compete. It's good but limited.\n\n\nA box with 128gb of RAM shareable with the GPU (RyzenAI 395 or Mac) is a solid start.Â ",
          "score": 15,
          "created_utc": "2026-01-20 04:20:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mf0os",
              "author": "lol-its-funny",
              "text": "GLM 4.7 â€¦ if you can run it Q8 or better , itâ€™s incredible. As good/better than Sonnet 4.5. Prompt it aggressively and itâ€™s really good.",
              "score": 7,
              "created_utc": "2026-01-20 06:12:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ospd1",
                  "author": "By-Jokese",
                  "text": "Kimi K2? Have you tried it?",
                  "score": 2,
                  "created_utc": "2026-01-20 16:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0meuwc",
          "author": "newbietofx",
          "text": "Start buying a tower with rtx GPU and 128 ddr ram",
          "score": 4,
          "created_utc": "2026-01-20 06:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nhhzn",
              "author": "AlexGSquadron",
              "text": "I have 64gb ram and it's not needed. If you have a good GPU with a lot of vram then you are good to go. Depends what you want to do, but ddr4 ram is very slow compared to vram. In my case 30-70 times slower.",
              "score": 1,
              "created_utc": "2026-01-20 11:54:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0nrccw",
                  "author": "jhenryscott",
                  "text": "Even ddr5 much slower. Just get a 5090",
                  "score": 2,
                  "created_utc": "2026-01-20 13:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mkgon",
          "author": "Deer_Avenger",
          "text": "Iâ€™m thinking about exactly the same. Iâ€™m planning to build a pc, but with the current cost of RAM, GPU, and SSD, Iâ€™m thinking if I should wait for prices to drop or is it a long term crisis",
          "score": 2,
          "created_utc": "2026-01-20 06:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0obq3k",
              "author": "Other-Football72",
              "text": "Long term they should go down, but yeah, that might be 2-3 years",
              "score": 1,
              "created_utc": "2026-01-20 14:54:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mpzu9",
          "author": "Vegetable-Score-3915",
          "text": "If just doing inference, pcie count doesn't matter as much.\nCould buy a 2nd hand work station with a couple pcie 3 x 16 slots and get some gpus. Maybe need to add a 2nd psu etc or get some cable adapters.\n\n2nd hand workstations, can still get a decent amount of ddr4 ecc ram included ie 32gb or 64gb without being too crazy expensive with decent quad channel bandwidth, at least compared to consumer equivalent motherboards from the same era that are typically dual channel. That is more so the under $1000 usd build before buying gpus. Can always get a better workstation later and buy more gpus. But if you are just doing inference you probably don't need pcie4 or pcie5 slots",
          "score": 2,
          "created_utc": "2026-01-20 07:46:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yripg",
          "author": "jiria",
          "text": "I asked myself the same question three weeks ago, same budget. I did a whole lot of researching and ended up going for a RTX Pro 6000 Blackwell Max-Q 96Gb. It is expensive but the best value for money and the best performance per watt. I received it a few days ago, inserted it in my desktop PC (a modest Ryzen 5 9600x with 32Gb DDR5 RAM) and the experience has been nothing short of amazing. This thing is a beast (Qwen3-coder-30B: \\~200 tokens/sec) and if you're doing just inference, it is perfectly silent, the fans barely spin, you can have it next to you. Why the Max-Q (300W) and not the regular 600W version? Because even if you lose 15% performance (in inference), you get a silent card that you can use next to you today and use with other cards in a workstation if you wish to expand in the future. Why not a RTX 5090? Too little memory for the models I want to run, and its price is way too inflated right now.",
          "score": 2,
          "created_utc": "2026-01-22 01:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0z3eza",
              "author": "pkieltyka",
              "text": "very cool! are you doing inference on Linux? and what is your software stack used with the rtx pro 6000 blackwell to prompt the model?",
              "score": 1,
              "created_utc": "2026-01-22 02:24:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0z61fk",
                  "author": "jiria",
                  "text": "yep, using Arch. Haven't had that much free time since I installed the GPU a few days ago, but I quickly installed a llama.cpp server for some tests. Will now configure Nvidia's own TensorRT-LLM, which should be even faster. Then I'd like to try stuff I've often seen mentioned in relation to Blackwell architecture, like NVFP4, to get the most out of the card. I will also spend some time tweaking the whole software stack to work well with my limited 32GB RAM (as I refuse to buy RAM at current prices) -- according to AI, it should be possible with only some very acceptable performance losses (mostly in model loading time and cold prefill wait time), which I can definitely live with.",
                  "score": 1,
                  "created_utc": "2026-01-22 02:38:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mui2r",
          "author": "beefgroin",
          "text": "Rtx 6000 pro on Bd895i with 96gb ram",
          "score": 1,
          "created_utc": "2026-01-20 08:27:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n2g8v",
          "author": "S1eeper",
          "text": "You could look into clustering multiple Mac Minis or Studios [[1]](https://www.youtube.com/watch?v=bFgTxr5yst0) [[2]](https://www.youtube.com/watch?v=A0onppIyHEg) [[3]](https://www.youtube.com/watch?v=x4_RsUxRjKU) using Apple's new [RDMA](https://appleinsider.com/articles/25/12/20/ai-calculations-on-mac-cluster-gets-a-big-boost-from-new-rdma-support-on-thunderbolt-5) tech, which directly connects the memory between the machines (bypassing ethernet TCP/IP).  Reduces the latency between machines enough to make them a viable local LLM setup.",
          "score": 1,
          "created_utc": "2026-01-20 09:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n9fxy",
          "author": "reneil1337",
          "text": "I'd get an rtx pro 9000 workstation edition + suitable cpu + ram you'll be able to do a lot with that and if you extend that setup with a second card at some point, scale out to an entire model fleet with embeddings and stuff that all your friends+family can use",
          "score": 1,
          "created_utc": "2026-01-20 10:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nzb84",
          "author": "Sicarius_The_First",
          "text": "Damn. I mean, there's tons of advice in the comments, and I'm sure the intention is good, but ... All of it is really bad.\n\nIs Mac good for inference? Sure. Is it a good value though, price vs performance & upgradeability & flexibility?\nAbsolutely NOT!\n\nHere's what u should actually do:\n1)psu 1500w minimum, buy a new one, but a mid tier one.\n2)case buy the largest, full tower that can fit e ATX board. Don't cheap on it!\n3) mobo & cpu, workstation/ server, buy used, important: u need 4 x pcie x16 lanes (could be pcie3, doesn't matter too much\n4) ram, depends on 3) but u want 64-128 gb\n5) GPUs: x4 a5000 ampere used on ebay, aim for 1k a piece, 1.4k$ is ok too\n\nTotal build should cost just under 10k, 96gb of vram, allowing you to run pretty much everything+ even doing some training",
          "score": 1,
          "created_utc": "2026-01-20 13:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0o02d0",
          "author": "Crafty-Diver-6948",
          "text": "mac studio 512gb you'll be set.",
          "score": 1,
          "created_utc": "2026-01-20 13:53:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ojgcc",
          "author": "Miller4103",
          "text": "Your use case would be helpful.",
          "score": 1,
          "created_utc": "2026-01-20 15:32:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ossfl",
              "author": "Technical_Buy_9063",
              "text": "Coding, agentic search and coding and problem-solving, ai-assistantship, image generation, stuff like that...",
              "score": 1,
              "created_utc": "2026-01-20 16:16:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ovoo8",
                  "author": "Miller4103",
                  "text": "2 rtx 5090 if u can find them. As much ram as possible, and nvme are important parts. \n\n4090's are probably easier to find and just as expensive.\n\n1 rtx 5090 should be good for most of those things but 2 will give u a slight buffer for the future. \n\nI think coding is probably the most hardware intensive since it can need a crap ton of context and tokens.\n\nI have a 5060 ti with 16gb vram and 64gb of ddr5 ram. I can run gpt oss 120b but context us super low. With gpt oss 20b I can get about 61k context. I wanted 128 gb of ram, but prices went up before I could get it.\n\nI would love an ada 6000 but im poor.",
                  "score": 1,
                  "created_utc": "2026-01-20 16:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0opww6",
          "author": "dbay01",
          "text": "Add some money to it and get this beast. Upgrade the memories layer.\n\nhttps://preview.redd.it/pl1n0om42jeg1.jpeg?width=1080&format=pjpg&auto=webp&s=50e397527d735b26f616a929934f197861676ed1",
          "score": 1,
          "created_utc": "2026-01-20 16:02:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qrfg7",
          "author": "C0rvusAlbuS",
          "text": "https://it.msi.com/Landing/EdgeXpert-MS-C931?utm_source=msi&utm_medium=banner",
          "score": 1,
          "created_utc": "2026-01-20 21:40:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t10c6",
          "author": "electrified_ice",
          "text": "My 2 Cents. Threadripper Pro with a motherboard with lots of PCIe slots. That gives you a core and solid foundation that you can add onto as you go/grow/get funds.\n\nIf you can get a Zen 5 PTR Pro, great. If not, get a Zen 4 TR Pro off eBay and save money. You can get a mobi with 8 channels and just populate 4 channels, so you have space to expand. You can start with 2 x 5060 ti or 1 x 5090 and expand. PCIe lanes/slots won't be a bottleneck for the foreseeable future if you start with a good base. TR Pro has >128 PCIe 5 lanes. You can also swap out GPUs. Let's say you find 1 or 2 RTX 5090s, then down the road you find a deal on something else more suitable, you can swap out and sell what you have etc.\n\nA Zen 4 32 core TR Pro + Mobo + 128GB Ram + As big wattage CPU you can find is a good base. Tons of PCIe headroom for GPUs, NVMe drives, AI Accelerators (like a MemryX MX3 etc.).",
          "score": 1,
          "created_utc": "2026-01-21 05:25:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t97hu",
          "author": "Gringe8",
          "text": "Depends on what you want to do, but if i was going to spend 10k i would get a rtx pro 6000 and build the pc in a way that you can add a 2nd gpu in the future.",
          "score": 1,
          "created_utc": "2026-01-21 06:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0moe9f",
          "author": "960be6dde311",
          "text": "Consider getting an NVIDIA DGX SparkÂ ",
          "score": 0,
          "created_utc": "2026-01-20 07:31:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nwzqa",
              "author": "Financial-Source7453",
              "text": "This. So far the best tool for a job for $3k. Smoothly runs gpt-oss-120b and future proof for the nearest 2 years. Advice - Asus clone is 1k cheaper than original DGX",
              "score": 2,
              "created_utc": "2026-01-20 13:36:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0psb6j",
              "author": "Caprichoso1",
              "text": "Best combined with a Mac with EXO.  Each machine is better at different things.  There are a number of Youtube videos discussing the best configuration.  Can't find right now.",
              "score": 1,
              "created_utc": "2026-01-20 18:58:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ntryr",
              "author": "TheAussieWatchGuy",
              "text": "These are ok only if your developers looking to deploy to the cloud.",
              "score": 0,
              "created_utc": "2026-01-20 13:17:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0m6ckh",
          "author": "little___mountain",
          "text": "The best (fastest & smartest) experience will be using a VPN to continue using a leading online LLM. \n\nIf you want a slow but smart local LLM, Iâ€™d get a specâ€™d out Apple with maxâ€™d unified memory. \n\nIf you want a fast and kinda smart local LLM, Iâ€™d custom build a server PC.",
          "score": 1,
          "created_utc": "2026-01-20 05:08:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mczgq",
              "author": "Damn-Sky",
              "text": "how slow is it on a mac?",
              "score": 2,
              "created_utc": "2026-01-20 05:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mf5xe",
                  "author": "little___mountain",
                  "text": "My desktop server is about 4x faster with AI than my  MacBook.\n\nI run an M5 which is the latest Apple chip and a 4070ti super which is a last gen desktop GPU.",
                  "score": 1,
                  "created_utc": "2026-01-20 06:13:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mo24v",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-20 07:28:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0moayt",
              "author": "Former-Tangerine-723",
              "text": "x3d CPU for AI inference? Why?",
              "score": 3,
              "created_utc": "2026-01-20 07:31:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mroj3",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-01-20 08:01:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n2rqg",
          "author": "wouek",
          "text": "I see many expensive ideas, isnâ€™t Nvidia Spark better and cheaper?",
          "score": -1,
          "created_utc": "2026-01-20 09:45:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfi1gy",
      "title": "How much vram is enough for a coding agent?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qfi1gy/how_much_vram_is_enough_for_a_coding_agent/",
      "author": "AlexGSquadron",
      "created_utc": "2026-01-17 16:44:06",
      "score": 21,
      "num_comments": 76,
      "upvote_ratio": 0.8,
      "text": "I know vram will make or break the context of an AI for an agent, can someone tell me what their experience is, which model is best and what is called enough vram, so that AI starts behaving like a junior dev",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfi1gy/how_much_vram_is_enough_for_a_coding_agent/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o04s64e",
          "author": "Kitae",
          "text": "I have a Rtx 5090 with 32gb of ram and instill use Claude for everything meaningful. \n\nI think of local LLMs as for fun or for extreme privacy or for large amounts of work that can be done by a simple model.",
          "score": 36,
          "created_utc": "2026-01-17 16:54:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04tj1j",
              "author": "alphatrad",
              "text": "I have 48gb of VRAM and mostly agree with this assessment.\n\nI have very good luck with local tab completion and some generation on a model I fine tuned on my projects.\n\nBut I still daily drive Claude code.\n\nLocal however is great at above 24gb for lots of document work though. I just can't see it working at Claude levels. Even if you had a Mac Studio Ultra w/ 512gb of memory, you might be able to load a huge model to up the performance but then it still won't be as fast as Claude.\n\nThe fact is, local has come a long way. But you can't compete with a data center of GPU's.",
              "score": 22,
              "created_utc": "2026-01-17 17:00:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o05ibof",
                  "author": "DifficultyFit1895",
                  "text": "I have that Mac Studio with 512gb of memory. The best local balance of speed and coding ability for me is qwen3-next-80b and itâ€™s not bad, really.  I prefer the frontier models in Github Copilot, though, as I donâ€™t have any extreme privacy concerns.",
                  "score": 11,
                  "created_utc": "2026-01-17 18:55:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o06ebwx",
                  "author": "thedarkbobo",
                  "text": "yep same here 2 rtx 3090 and I eventually use auto claude with a) glm 4.7 [z.ai](http://z.ai) b) when it gets locked for 5hours I swap to local qwen 3 next and (its 3 times slower I think) use it to move forward.",
                  "score": 1,
                  "created_utc": "2026-01-17 21:35:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o05s3ea",
              "author": "Better-Cause-8348",
              "text": "Local Whisper and Kokoro are a must. Probably the top thing I use local models for. Vectoring data is also helpful to run local.",
              "score": 6,
              "created_utc": "2026-01-17 19:41:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0br35f",
                  "author": "Kitchen_Fix1464",
                  "text": "I just brought up this stack in my home lab. It's nice having TTS/STT and mem0 hosted locally to share across my network.",
                  "score": 2,
                  "created_utc": "2026-01-18 17:47:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0atheo",
                  "author": "Kitae",
                  "text": "Yeah I haven't gotten here yet but I feel like local LLM services to do things you just wouldn't do with your max plan is a smart use of local LLMs.",
                  "score": 1,
                  "created_utc": "2026-01-18 15:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04sw1g",
              "author": "AlexGSquadron",
              "text": "Which model do you use locally? What experience have you had with any of the models?",
              "score": 1,
              "created_utc": "2026-01-17 16:57:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0atnys",
                  "author": "Kitae",
                  "text": "I personally am in a research phase where I am getting as many models working as possible and benchmarking them to understand their capabilities.\n\nBut qwen is my favorite.",
                  "score": 1,
                  "created_utc": "2026-01-18 15:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o06qdb0",
              "author": "Comfortable_Ad_8117",
              "text": "I have 24 GB of VRAM and use my cards for so many Ai applications, however I use cursor to write them for my local Ai",
              "score": 1,
              "created_utc": "2026-01-17 22:34:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o04x694",
          "author": "Terminator857",
          "text": "128 GB of strix halo and still use various online",
          "score": 9,
          "created_utc": "2026-01-17 17:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06fyvo",
              "author": "Zyj",
              "text": "I bought another one and Iâ€˜m looking to try networking three now",
              "score": 2,
              "created_utc": "2026-01-17 21:43:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0d7t08",
                  "author": "cjc4096",
                  "text": "How well does horizontal scaling work?   Can you split smaller models, that'd fit on a single machine, across multiple and get more token/s?   Or is it only useful when you need more vram?",
                  "score": 1,
                  "created_utc": "2026-01-18 22:05:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o04zetu",
          "author": "dsartori",
          "text": "There is no way around the fact that on-device models will be slower and dumber than anything you'd pay for in the cloud.\n\nWith midsized MoE models like gpt-oss-120b and qwen3-coder-30b you can get good results depending on your use case but it's not as fast or as high-quality as what you can get in the cloud for pretty cheap.\n\nHaving said all that, anything that gets you to decent performance on the 30b qwen model is the minimum bar for on-device coding agents. A 32GB Mac or a PC with an 8-12 GB video card and a good amount of fast RAM is probably where that starts.",
          "score": 6,
          "created_utc": "2026-01-17 17:27:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05013l",
              "author": "AlexGSquadron",
              "text": "I have 3080 10gb and 64gb DDR4 ram",
              "score": 1,
              "created_utc": "2026-01-17 17:30:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o050a7c",
                  "author": "dsartori",
                  "text": "That should work just fine for the 30b Qwen model. gpt-oss-20b will also be useful on that device.",
                  "score": 3,
                  "created_utc": "2026-01-17 17:31:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o058k0m",
                  "author": "Prudent-Ad4509",
                  "text": "Raise that vram to 24-48-64 and you can run starter models. Go for 192Gb, and you can run pretty competent models with limitations. Double it after that, and you will be able to run almost everything good locally.\n\nEither that or go the apple route with unified memory from 128gb and up.\n\nBut you certainly can play around with 10Gb if you use small models for highly specialized tasks. You certainly can use 30b Qwen coder with offloading most experts. It won't be fast, but it will be faster than coding by hand, especially when coding in languages you don't know.\n\nJust remember that you will have to correct it a lot. Same as with bigger models, though.",
                  "score": 2,
                  "created_utc": "2026-01-17 18:10:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o06glvy",
                  "author": "johnerp",
                  "text": "Snap! So interested in art of the possible. Iâ€™m currently spending $20 a month on Gemini, using anti gravity - been more than enough for what I need and intelligent. Iâ€™m now trying open code client with AG models to see if I can bring in open source models for free for certain agent personas, I suspect Gemini flash 3 will remain my go to until the software or hardware architecture for llms radically changes so we can get the quality and performance in a single retail gpuâ€¦.",
                  "score": 1,
                  "created_utc": "2026-01-17 21:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o050c1k",
              "author": "Maximum-Wishbone5616",
              "text": "100$ for 6-8tps is cheap? I paid 4k for 2x 5090 that basically can generate without limit 5-10x tokens per week. I easily locally eat up my daily tokens used with max in just 25 prompts. At fraction of the time",
              "score": 1,
              "created_utc": "2026-01-17 17:32:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o050sg2",
                  "author": "AlexGSquadron",
                  "text": "For 7k I would have gotten rtx 6000 pro 96gb vram",
                  "score": 2,
                  "created_utc": "2026-01-17 17:34:11",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o051034",
                  "author": "dsartori",
                  "text": "If you don't care who sees your stuff the Z.Ai coding plans are super cheap. \n\nI agree with you in that that local is the way to go for me, I'm just trying to be straight up with a new user.",
                  "score": 1,
                  "created_utc": "2026-01-17 17:35:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o04wh54",
          "author": "Odd-Criticism1534",
          "text": "Iâ€™m not an app coder or building anything extensive, mostly personal automations or things to help with work and life admin - but Iâ€™ve found devstral-small-2-2512 run on LM studio using Mistral Vibe coding agent/assistant to work well for my small projects",
          "score": 4,
          "created_utc": "2026-01-17 17:13:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09yixe",
          "author": "akumaburn",
          "text": "I can run a functional 4B coding model (Qwen 3 4B Polaris Distill) at 128K context entirely within 16GB of VRAM. This was tested in OpenCode.\n\nThis comes after extensive research into lesser-known models that significantly outperform what most LLM users seem to be using. Even so, the experience remains far from ideal.\n\nMy conclusion: the practical minimum for serious work is around 36GB of VRAM. The reason is context.\n\nSufficient context is essential for the model to genuinely understand your project and operate coherently within it. For coding tasks, you want as much context as possibleâ€”and at 256K tokens, you're looking at roughly 16GB of VRAM for the KV cache alone, before accounting for the model weights. This assumes aggressive quantization (q8/q4). You'd need even more if you're not willing to sacrifice quality here (20-40GB).\n\nCPU offloading is technically possible, but latency scales catastrophically as context fills; waiting an hour per prompt is simply untenable for agentic workflows.\n\nProcessing speed further constrains your options, effectively limiting you to MoE architectures with small activation sizes.\n\nThe only models in this class that handle tool calls reliably in my experience are at or exceed 30B total parameters. \n\nThe practical minimum I've found is Unsloth's T1 quant of Qwen 3 Next 80-A3B, which requires just over 20GB on its own.\n\nIn sum: 32GB is workable if you're willing to compromise on context. Otherwise, you're looking at 36GBâ€”which means either multi-GPU configurations or stepping up to professional-grade hardware.",
          "score": 5,
          "created_utc": "2026-01-18 11:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0cbc1z",
          "author": "sloptimizer",
          "text": "These are the \"tiers\" unlocked by certain level of VRAM\n\n* 128GB - minimal usable coding agent (GLM-4.6V)\n* 160GB - capable coding agent (MiniMax-M2.1)\n* 448GB - best open-weight models (DeepSeek-V3.2)\n\nThe smallest model usable for coding is GLM-4.5-Air, or the newever GLM-4.6V. You are looking at around 128GB VRAM, since these models degrade very quickly when quantized. Any smaller models are highly inconsistent for coding and are simply not worth the time.\n\nGoing one size above takes you to MiniMax-M2.1, which is a surprisingly capable coding agent. It puchnes way above its weight. You can get away with 160GB VRAM before the model becomes braindead due to quantization.\n\nFinally, at 448GB you can run lightly quantized DeepSeek 671B models with great results.\n\nIn general, coding performance degrades quickly with quantization, and **Q4\\_K or INT4 are not good enough!** The smaller the model, the more it is affected by quantization. For example, Kimi-K2 is usable with all the experts quantized as 4 bit. But the same is not true for MiniMax-M2.",
          "score": 3,
          "created_utc": "2026-01-18 19:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05k1l4",
          "author": "twjnorth",
          "text": "Two possibilities that might work\n\nOption A) Local LLM on Cloud VM\n\nWhat about having infra as code to spin up a spot instance VM with GPUs on azure, AWS or Google when you need to code ?\n\nSpot instances are about 10% of pay as you go. No token limits and depending on number of GPUs and VRAM, could be cheap enough per hour. \n\nWould need some effort up front to make it resilient so when a spot instance goes down. It fires up another in a different family.  But it's possible.\n\nAnsible or whatever config tool would need to setup the ai infra (vllm / ollama) and pull the model required.\n\nTrash the instance when done for the day as can fire up another next time.  \n\nOption B) Fine tuning smaller model for specific use case.\n\nThis is the one I will be testing in short term. I am an Oracle Developer (25 years) in a niche vertical (Retail).  Plsql is present in foundation models but it's most likely from stack overflow or a side effect.  Haven't found anywhere it's part of primary training set.  \n\nIf I ask copilot to create a plsql function, it will have a go but results are mixed.  If it had currency conversion, it doesn't know I have a STD function for that.  It doesn't know all my package functions returna Boolean and an error_msg variable.  So the suggestions all need amending.\n\nIf I fine tune for my niche code, then I should be able to have a much better experience than foundation models (TBC by testing).",
          "score": 2,
          "created_utc": "2026-01-17 19:03:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o067ohr",
          "author": "Cronus_k98",
          "text": "VRAM isnâ€™t just for loading larger models. Itâ€™s also useful for having multiple smaller models loaded at the same time. When processing a large number of documents you save a lot of time by not needing to swap models into and out of memory.Â ",
          "score": 2,
          "created_utc": "2026-01-17 21:00:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o067uf6",
          "author": "Aggressive_Special25",
          "text": "48gb vram plus 128gb system ram.\n\nNo local models can code well enough for me. Claude works great.",
          "score": 2,
          "created_utc": "2026-01-17 21:01:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07k04o",
          "author": "cookieGaboo24",
          "text": "I haven't done any big scale coding yet, but my small tests show it could have potential, so I'd say 12gb is fine.\n\nI'm Running Gemma 3 12b (or GemmaCoder3 12b at that) at Q4 with 100k ctx at Q4. Gave it RAG with Python docs, it's own Reasoning capabilities and so far it gave me pretty solid code. Don't expect ultra quality code, but it is something I guess. \n\nHope that it at least doesn't waste time. Cheers",
          "score": 2,
          "created_utc": "2026-01-18 01:09:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09kwlj",
          "author": "HealthyCommunicat",
          "text": "Edit: OP dmâ€™ed me, he does not understand that claude is not something that can be run locally. i was correct. i keep trying to warn and correct him but he wants to buy 128gb of intel gpuâ€™s and thinks he can get even remotely close to claude performance. he doesnâ€™t even realize sonnet 4.5 and opus 4.6 are the model names. This post is a pure example of what I keep meaning when I talk about the massive amount of people trying to jump into AI without slightly even caring to learn to basics.\n\n512 gb.\n\nIf you think iâ€™m exaggerating and think anything under 256 gb will be useable, you have way too much to learn.\n\nGLM 4.7 which is already not even as smart as GPT 5.2 or sonnet 4.5 requires a bare bare minimum of 256 gb of vram, and even with 256 gb of vram it will be dumbed down and slow. \n\nIntelligence is exponential, meaning a 3b model will not be 1/10th the intelligence of a 30b model. It will be much lower than 1/10th. You can try to run 30b models or even 120b models as much as you want, but what makes it even worse is that for the technically unskilled, you need a model that is as smart as possible to make up for your gaps in knowledge.\n\nThe strongest point of LLMs is that is allows us humans for recursive learning. Beginner sets up LLM. Beginner asks LLM how to use it agentically. Beginner uses agentic LLM to code and ask further questions on how to code and so on. But for this to work you need actual passion and motivation, and thats not something that can be given, nor created.\n\nIf you are asking this question then you are not ready and do not have the motivation to get into this because if you did you would have already figured this part out.",
          "score": 2,
          "created_utc": "2026-01-18 09:39:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09nvrp",
              "author": "AlexGSquadron",
              "text": "What if I have figured it out and I ask others as if I have not figured it out?",
              "score": 1,
              "created_utc": "2026-01-18 10:07:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09nz94",
                  "author": "HealthyCommunicat",
                  "text": "you have a 3080. that in it by itself tells people enough.",
                  "score": 2,
                  "created_utc": "2026-01-18 10:08:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0h3o0b",
          "author": "arseneeth",
          "text": "I'm considering getting a used Mac studio 64gb m1 and add it to the cluster of Mac mini m4 16 gb, would be enough for coding agents?",
          "score": 2,
          "created_utc": "2026-01-19 13:46:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hgg1b",
              "author": "AlexGSquadron",
              "text": "Apparently no, but I wonder if there is a used version with more ram",
              "score": 1,
              "created_utc": "2026-01-19 14:55:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o04r5xe",
          "author": "EnvironmentalLow8531",
          "text": "[https://hardwarehq.io/coding-studio](https://hardwarehq.io/coding-studio) This should be a pretty decent start, plenty of other tools and a solid database of models/hardware available there if you need more specific info",
          "score": 2,
          "created_utc": "2026-01-17 16:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04t4d7",
          "author": "Low-Opening25",
          "text": "You cant afford it.\n\nI have 28GB of VRAM and I am still using CC or Codex for any real work, local LLMs are only good for fun and learning but not really usable for professional coding. As professional I donâ€™t have time to deal with issues, this would cost me time and money, I want stuff that mostly just works.",
          "score": 2,
          "created_utc": "2026-01-17 16:58:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04wycg",
              "author": "-Akos-",
              "text": "Sad but true. A single computer is no match for a datacenter full of dedicated hardware doing inference for you. And with prices nowadays, a 5090 with 32GB is 3-4K (at least in my country), at which I could do 150-200 months of Anthropic subscription. Thatâ€™s like 12-16 YEARS worth of a monthly Anthropic subscription..",
              "score": 2,
              "created_utc": "2026-01-17 17:16:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o053aic",
                  "author": "AlexGSquadron",
                  "text": "Or get a pro card like the Intel b50 4x in parallel",
                  "score": 1,
                  "created_utc": "2026-01-17 17:45:57",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o05hrst",
                  "author": "nyssaqt",
                  "text": "For now, who knows what the prices will do.",
                  "score": 1,
                  "created_utc": "2026-01-17 18:52:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o05084u",
              "author": "AlexGSquadron",
              "text": "You mean because of vram? I am waiting on Intel for that. But my question is, can I combine multiple GPUs?\nI already have 64gb DDR4 ram.",
              "score": 1,
              "created_utc": "2026-01-17 17:31:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o051lsw",
                  "author": "Low-Opening25",
                  "text": "to run a usable model, mostly the Chinese ones like deepseek or Kimiki 2, you would need much more than that, think 512-1536GB",
                  "score": 2,
                  "created_utc": "2026-01-17 17:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05nnmn",
          "author": "kamu-irrational",
          "text": "At work we have a few enterprise 80gb cards on prem. I STILL use cloud providers. I donâ€™t think there exists self hostable open models for agent based development even close to the flagship proprietary models.",
          "score": 1,
          "created_utc": "2026-01-17 19:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05q2r8",
              "author": "ZeSprawl",
              "text": "GLM 4.7 is very close, but you need around 200gb+ to do it at full quantization",
              "score": 2,
              "created_utc": "2026-01-17 19:31:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05sy0h",
          "author": "Better-Cause-8348",
          "text": "I would recommend checking into Ollama Cloud at $20/m. Their claim is privacy, and they have all the big open-source models, locally hosted, with a provider that meets the same privacy requirements as Ollama. You won't come close to touching the limits on the $20/m plan, even if you want to code with it. I use it often for my grunt work, but mainly for a mixture of experts' MCP tools. I query all the big open-source models they support, along with a couple of SOTA models, and get really great insight, suggestions, etc. Don't dismiss Gemini 3 Flash, which isn't a premium model on Ollama Cloud, meaning it's essentially unlimited. It's the best bang for the buck if you want to use it for coding.",
          "score": 1,
          "created_utc": "2026-01-17 19:45:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08iio2",
              "author": "AlexGSquadron",
              "text": "What's the difference between glm cuz I see 40$ for 3 months pro subscription",
              "score": 1,
              "created_utc": "2026-01-18 04:21:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0a9wq9",
                  "author": "Better-Cause-8348",
                  "text": "If you go directly through GLM, youâ€™re only going to get their models, whereas Ollama Cloud gives you access to around 20 models, including all of the GLM models. All models on Ollana Cloud are hosted on US-based servers. It's a great way to try or test open-source models without having accounts with every provider.",
                  "score": 1,
                  "created_utc": "2026-01-18 13:12:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05xvlh",
          "author": "No-Leopard7644",
          "text": "What if I have access to H100s?",
          "score": 1,
          "created_utc": "2026-01-17 20:10:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o068lbv",
              "author": "blackashi",
              "text": "\nhttps://www.youtube.com/watch?v=SmYNK0kqaDI",
              "score": 2,
              "created_utc": "2026-01-17 21:05:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06wkzz",
              "author": "No-Leopard7644",
              "text": "Anyone building local AI for work? Real prod grade AI Infra stack to serve the inference, storage for blazing fast RAG and LLMOps with evals, observability, DR and API Management built in with RBAC thrown in?",
              "score": 1,
              "created_utc": "2026-01-17 23:06:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05y1p3",
          "author": "Loud_Communication68",
          "text": "Maybe you can get the job done with nvidia orchestrator driving a coding agent or two",
          "score": 1,
          "created_utc": "2026-01-17 20:11:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o061djt",
          "author": "Clay_Ferguson",
          "text": "I asked Gemini what I can run on my brand new Dell XPS laptop and I can indeed run fairly fast inference on the 32GB memory, because the new CPU architecture makes main RAM act like VRAM. Just tell Gemini what memory you have and ask it what model to download from hugging face for best coding agent on a SLM. I'll be doing it soon myself!\n\n  \nEDIT. And try it out with OpenCode or LangChain Openwork!",
          "score": 1,
          "created_utc": "2026-01-17 20:28:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06fy1n",
          "author": "p_235615",
          "text": "I played around with mistral-3:8B in ollama with vscode, its not the best, but can often create quite good and meaningful code snippets, of course its not very good at comprehending and stitching together larger code. But for basic completion its not bad and often quite helpful.",
          "score": 1,
          "created_utc": "2026-01-17 21:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07s962",
          "author": "Proper_Taste_6778",
          "text": "I use minimax 2.1 for a few days locally and it's better than others models which I tried.",
          "score": 1,
          "created_utc": "2026-01-18 01:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05a2kt",
          "author": "huzbum",
          "text": "You could probably run MiniMax M2.1 or 128GB, otherwise, 256GB for GLM 4.7",
          "score": 0,
          "created_utc": "2026-01-17 18:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07tvhq",
              "author": "Proper_Taste_6778",
              "text": "There is unsloth/GLM-4.7-REAP-218B-A32B-GGUF that you can fit into 128gb too but I didn't test that yet. MiniMax m2.1 is really good imo. But I only tested it for few days",
              "score": 1,
              "created_utc": "2026-01-18 02:00:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0p7g29",
                  "author": "huzbum",
                  "text": "Wow, I've heard of REAP, but 355 down to 218 seems pretty aggressive!  I imagine that would have a large impact on knowledge not contained in the REAP corpus, so I wouldn't expect stuff like multi-lingual or a lot of general purpose knowledge/skill to survive.\n\nI'm curious how well it still works.  I've got 36GB VRAM and 128GB DDR4 system RAM.  It'd be slow but I guess I could give it a try!",
                  "score": 2,
                  "created_utc": "2026-01-20 17:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qexu0l",
      "title": "Is there a local/self-hosted alternative to Google NotebookLM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qexu0l/is_there_a_localselfhosted_alternative_to_google/",
      "author": "RadiantCandy1600",
      "created_utc": "2026-01-17 00:36:37",
      "score": 20,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "Iâ€™ve been using **Google NotebookLM** recently and the workflow is incredibleâ€”being able to upload a dozen PDFs and have the AI \"ground\" itself in those specific sources is a game changer for research.\n\nHowever, Iâ€™m not thrilled about uploading sensitive work documents or personal research to Googleâ€™s cloud. Iâ€™m looking for something I can run **locally on my own hardware** (or a private VPS) that replicates that \"Notebook\" experience.\n\n**Ideally, Iâ€™m looking for:**\n\n* **Privacy:** No data leaving my machine.\n* **Source Grounding:** The ability to chat with specific \"Notebooks\" or collections of PDFs/Markdown/Text files.\n* **Citations:** It needs to tell me exactly which page/document the answer came from (this is the best part of NotebookLM).\n* **Audio/Podcasts (Optional):** The AI podcast generator in NotebookLM is cool, but document analysis is my priority.\n\n**What are the best options in 2026?** Iâ€™ve heard names like **AnythingLLM**, **GPT4All**, and **Open Notebook** (the GitHub project) thrown around. Which one is currently the most stable and \"NotebookLM-like\"? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qexu0l/is_there_a_localselfhosted_alternative_to_google/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o012xn0",
          "author": "GrayRoberts",
          "text": "Open-notebook.io",
          "score": 5,
          "created_utc": "2026-01-17 01:20:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o013c4r",
          "author": "Aromatic-Low-4578",
          "text": "[https://github.com/lfnovo/open-notebook](https://github.com/lfnovo/open-notebook)",
          "score": 4,
          "created_utc": "2026-01-17 01:23:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06b593",
              "author": "sn2006gy",
              "text": "Hopefully the PR to create a helm chart for this moves quickly",
              "score": 1,
              "created_utc": "2026-01-17 21:18:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00x742",
          "author": "marsxyz",
          "text": "Just the RAG in openwebui ?",
          "score": 3,
          "created_utc": "2026-01-17 00:45:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o044czd",
          "author": "Clipbeam",
          "text": "Have a look at https://clipbeam.com if you use a Mac. No data leaves your machine, it allows you to chat with one or more pdfs/markdown/text files (plus also web links, audio and video files) and it will tell you which document it based it's answer on when you chat with it. \n\nWhat I haven't done yet is make it possible for the AI to cite the specific page of a document, but I may build this feature for a future update. Let me know if it meets your needs?",
          "score": 3,
          "created_utc": "2026-01-17 14:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00y8ep",
          "author": "michael_p",
          "text": "I was going to say anythingllm but you mentioned that",
          "score": 2,
          "created_utc": "2026-01-17 00:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o037mix",
          "author": "toadi",
          "text": "I use obsidian for note taking. I use a plugin that you can use claude, opencode, ... inside obsidian. Wrote a custom agent in opencode for research and scanning my repository and internet if needed. I provided it templates I want it to write in too.\n\nFeels close to notebookllm but in my own way.\n\nThe plugin: [https://github.com/RAIT-09/obsidian-agent-client](https://github.com/RAIT-09/obsidian-agent-client)\n\n  \nI tried different plugins but I prefer these types as my opencode can plugin different applications keeping its configuration the same. This way I don't need to redo it in my editor and obsidian.",
          "score": 2,
          "created_utc": "2026-01-17 11:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00xanc",
          "author": "MaverickPT",
          "text": "ragflow.io?",
          "score": 1,
          "created_utc": "2026-01-17 00:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ie83",
              "author": "Karyo_Ten",
              "text": "Have you actually used it?",
              "score": 0,
              "created_utc": "2026-01-17 07:29:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03gv86",
                  "author": "MaverickPT",
                  "text": "I've tried a few months ago, but I think their document ingestion pipeline was broken as I couldn't reliably ingest my datasheets. Since then I know they have updated it but I haven't tried it yet.\n\nHave you?",
                  "score": 1,
                  "created_utc": "2026-01-17 12:40:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o023g7n",
          "author": "arman-d0e",
          "text": "Surfsense. It started as a notebook lm clone but I think they got cease and desisted or something and had to redesign a bit\n\nhttps://github.com/MODSetter/SurfSense",
          "score": 1,
          "created_utc": "2026-01-17 05:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pct5p",
          "author": "MichaelDaza",
          "text": "I use OpenWebUI and a fast LLM to do the RAG portion. No problems, you could probably add it into the default prompt to ensure the information is strictly followed by a citation. There are multiple ways to get this done, see what works best for you and your workflow",
          "score": 1,
          "created_utc": "2026-01-20 17:48:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qitw9k",
      "title": "Trained a local Text2SQL model by chatting with Claude â€“ here's how it went",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/coqeh3twdoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 09:57:59",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitw9k/trained_a_local_text2sql_model_by_chatting_with/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qe3yzy",
      "title": "Help finding best LLM to improve productivity as a manager",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "author": "2C104",
      "created_utc": "2026-01-16 02:42:07",
      "score": 15,
      "num_comments": 13,
      "upvote_ratio": 0.89,
      "text": "Like the title says, I am not in need for the LLM to code anything, I'm essentially looking for a tool that will support my managerial work. \n\nI want to be able to feed it text descriptions of the projects I am working on and get help categorizing, coordinating, summarizing, preparing for presentations, use it as a tool for bouncing ideas off of, suggestions for improving email communication, tips to improve my management productivity and abilities, etc... \n\nI want to do this offline because although ChatGPT is very helpful in this regard, I don't want sensitive work content to be shared online. \n\nMy rig has the following:\n\nIntel(R) Core(TM) Ultra 9 275HX (2.70 GHz)  \n32.0 GB RAM  \nNvidia 5070 Ti (Laptop GPU) w/ 12gb RAM  \n2tb SSDs\n\n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzv1jrr",
          "author": "iMrParker",
          "text": "Honestly that smaller Gemma models would be good for you. Gemma 12b might fit and it has vision. GLM has 9b models which are excellent (4.6 flash I think?). If you had a desktop 5070 ti you'd be able to run GPT OSS 20b which is very solid for this purpose.Â \n\n\nThe best advice would be to download LM Studio, use the model search, and play around with an assortment of models till you find one that fits your needs",
          "score": 7,
          "created_utc": "2026-01-16 04:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv5ynf",
              "author": "2C104",
              "text": "Thank you I'll check it out",
              "score": 2,
              "created_utc": "2026-01-16 04:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvt7k7",
          "author": "techlatest_net",
          "text": "Hey man, for managerial stuff like summarizing projects, tweaking emails, or brainstorming without sending sensitive info to the cloud, your rig is perfectâ€”Ollama is dead simple to set up and flies on that 5070 Ti with 12GB VRAM.â€‹\n\nGrab Ollama (ollama.com), install in like 2 mins, then pull mistral-nemo:12b Q4 or qwen2.5:7b-instruct Q5â€”both fit comfy quantized and give solid, non-hallucinating responses for exactly what you described. Nemo edges out Llama 3.1 8B on reasoning without being too wordy, and folks use it daily for email drafts and task categorization just like you want.â€‹\n\nPaste project text, say \"summarize this for a team mtg, suggest 3 action items\" or \"rewrite this email to sound more collaborative\"â€”hits 20-40 t/s easy. Bonus: there's even a quick Python script floating around using Llama3 via Ollama for auto-email summaries if you wanna script it later. Way better privacy than ChatGPT, zero cost after download.",
          "score": 3,
          "created_utc": "2026-01-16 07:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx1xqf",
              "author": "2C104",
              "text": "Can anyone explain why this comment is getting downvoted? It seems the most eloquent of the responses and since they all seem to be equally helpful I'm just trying to understand why people would disagree with what was stated here.",
              "score": 2,
              "created_utc": "2026-01-16 13:38:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwpw1m",
          "author": "chief-of-wow",
          "text": "Although I don't know how offline AI works, I have a couple of tips to share about online AI:\n\n1. For Presentations, Gamma AI is really good. I recently tested it out and it has so many cool features. The one that stood out to me was how easy it was to choose diff types of graphic elements within the tool. For the slides, I gave my notes to Claude and asked it clean them up and structure them as slides. It did so and then I fed that into Gamma to build a presentation for me.\n\n  \n2. A productivity tip: I use voice dictation a lot. When I have lots of thoughts and context, I use dictation to feed AI. Be it drafting a long (and important) email, creating an SOP, or similar comms or documentation work, this saves me a lot of time. Also, I don't know the reason, but the results are so much better than when I type instructions. I am a decent writer so proofreading and editing doesn't take me long. Overall, this process works well for me so I definitely recommend testing it.",
          "score": 1,
          "created_utc": "2026-01-16 12:24:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxbau4",
          "author": "nofilmincamera",
          "text": "Having access to local LLM has made me a extremely effective.  That said its a right tool for right job.\n\nUsing it for language re writes? Sure. But you really need one of the bigger models simply just for deep research. \n\nAlso,  understanding tools and python will get you pretty far. A local installed took line N8N makes it great to automate.  LLMs are non deterministic. Which means by themselves are terrible for consistently.  In any use you come up with, it really helps that you break that task apart.  What of this can be automated without a LLM at all? A smaller model can do a lot as a validator, and where non deterministic is a value.",
          "score": 1,
          "created_utc": "2026-01-16 14:26:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kllba",
          "author": "Unique-Temperature17",
          "text": "Your hardware is solid for local LLMs. I'd recommend Qwen 3 4B or Gemma 3 4B - both support 32K context so you can feed in longer project docs, and they'll run fast on your 5070 Ti. For software, Ollama or LM Studio are the easiest to get started with. If you want to chat with documents directly, check out Suverenum.",
          "score": 1,
          "created_utc": "2026-01-19 23:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwfmcb",
          "author": "fasti-au",
          "text": "Nemotron 30b if local with websearch and some memory files is very good for most things.  Technical writing Iâ€™d go phi4.  \n\nBig leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nGrok is the most cutthroat, a win is a win damn the alternatives sort model.  \n\nIt backs itself and is very much great at winning if it finds a trick but itâ€™ll bend the rules and go around problems in some ways so I would be wary of long memory and brainstorming without checks",
          "score": 0,
          "created_utc": "2026-01-16 11:06:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx2cm0",
              "author": "2C104",
              "text": ">Big leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nI have no idea what this is trying to say, it seems incoherent to me. Are these the names of LocalLLMs? Or were you just using shorthand?",
              "score": 1,
              "created_utc": "2026-01-16 13:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o011jom",
                  "author": "number66-1",
                  "text": "I couldn't understand him either, but he doesn't need to waste his time explaining himself since if you are using AI and really care to understand what he said you could ask AI to help you understand. \n\nHe's done his part, he shared his knowledgeable opinion.\n\n\"Chatgpt: Hereâ€™s what that commenter is saying, rewritten in normal English, with the shorthand decoded.\n\n1) Clean rewrite of their comment\n\nIf you want a local model and you can give it web search + a small â€œmemoryâ€ of your files, Nemotron 30B is good for most tasks.\nFor technical writing, theyâ€™d pick Phi-4.\n\nFor â€œbig leaguesâ€ (top-tier general chat), GPT is the best standalone option.\nBut if youâ€™re using Claude with MCP, then Claude is better / less annoying in their opinion.\n\nGrok is the most cutthroat: it cares about â€œwinningâ€ and will pick an answer aggressively.\nItâ€™s very confident and can be great when it finds a clever shortcut, but it may â€œbend the rulesâ€ or take weird detours, so they wouldnâ€™t trust it for long-term memory or brainstorming unless you verify everything.\n\n\nThatâ€™s the whole meaning.\n\n2) What each name likely refers to\n\nNemotron 30B: a local LLM family/model (30B = ~30 billion parameters). They mean â€œrun it on your own machineâ€ (or your own server).\n\nPhi-4: Microsoftâ€™s Phi family model; theyâ€™re saying itâ€™s strong for structured/technical writing.\n\nGPT: OpenAIâ€™s models (ChatGPT). They mean â€œbest general-purpose chat without needing extra tool wiring.â€\n\nClaude: Anthropicâ€™s models.\n\nGrok: xAIâ€™s model.\n\n\n3) What the jargon means\n\nâ€œLocalâ€: you run the model yourself (privacy, control, but you manage setup + performance).\n\nâ€œWebsearchâ€: hooking the model to a browsing/search tool so it can fetch current info instead of guessing.\n\nâ€œMemory filesâ€: usually means a RAG setup (retrieval-augmented generation): your notes/docs are indexed and the model can pull relevant chunks while answering.\n\nâ€œBig leaguesâ€: basically â€œbest-of-the-best overall.â€\n\nâ€œStandaloneâ€: not using tool integrations, agents, RAG, custom workflowsâ€”just the model as-is.\n\nâ€œClaude MCPâ€: almost certainly referring to the Model Context Protocol (MCP)â€”a way to connect Claude to external tools/services (files, apps, search, automations) in a standardized way.\n\nâ€œCutthroat / win is a winâ€: the model will prioritize getting some answer that seems successful, even if itâ€™s not cautious or perfectly aligned with your intent/rules.\n\nâ€œBend the rules / go around problemsâ€: meaning it may produce confident outputs, shortcuts, or â€œcreativeâ€ leaps that sound right but arenâ€™t reliably grounded.\n\nâ€œWary of long memory and brainstorming without checksâ€: donâ€™t let it build long chains of assumptions over time; verify with sources, logs, or constraints.\n\n\n4) Why it reads incoherent\n\nItâ€™s basically:\n\nmissing punctuation,\n\ntypos (â€œbut u f you Claude mcpâ€¦â€ = â€œbut if you use Claude MCPâ€¦â€),\n\nand it jumps between local model advice and hosted model opinions without transitions.\n\n\nIf you tell me what you were trying to decide (local vs cloud, privacy, work tasks, hardware), I can translate their advice into a concrete recommendation that fits your situation.\"",
                  "score": 1,
                  "created_utc": "2026-01-17 01:11:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwpevx",
          "author": "beauzero",
          "text": "Just use claude cowork.",
          "score": 0,
          "created_utc": "2026-01-16 12:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx80vu",
          "author": "Nx3xO",
          "text": "If you want multiple options set yourself up on a jetson orin 16/32gb. Openwebui. Easy to setup guard rails, multiple llms ready to load. You can airgap for extra security. Low power too. Your laptop is fully capable but this option b could be a much better and flexible option. \n\nI have an 8gb, about 8 models. Focused on math medical and general. Simple small models. I can access it on the wire or jump onto it via wifi ap I setup. Technically I could throw it on a battery and and tap into on the go, airport/traveling. Add extra storage and its a portable backup solution.",
          "score": 0,
          "created_utc": "2026-01-16 14:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxgc90",
              "author": "mauricespotgieter",
              "text": "Hi Nx3xO.\nWould you mind sharing details of your n8 setup?\nI am looking to do something similar and would be grateful for any assistance. Still finding my legs and learning. Thanks in advance. Open to DM directly if that is more appropriate?",
              "score": 1,
              "created_utc": "2026-01-16 14:51:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfuvg5",
      "title": "Dual RTX 5060 ti 16gb's with 96GB of DDR5 5600 mhz, what is everyone else running?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qfuvg5/dual_rtx_5060_ti_16gbs_with_96gb_of_ddr5_5600_mhz/",
      "author": "CollectionOk2393",
      "created_utc": "2026-01-18 01:20:59",
      "score": 15,
      "num_comments": 16,
      "upvote_ratio": 0.94,
      "text": "I have a minisforum n5 nas with 96gb of ddr5 5600 ram and dual rtx 5060 ti's.\n\n32gb of vram and 96gb of ram\n\n  \nWhat are the best local llm models to run? Also are there any image or video gen tools that work well on dual gpus? \n\n  \nI've just built this rig and im looking to get into AI to do some side work and try to make a few bucks with my free time. Or just learn so I dont fall behind at work. Im a data center engineer for a tire 4 data center. I see what they are buying and im just trying to stay relevant I guess. Lol. Any suggestions or tips or tricks on what software or models or whatever would be appreciated!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfuvg5/dual_rtx_5060_ti_16gbs_with_96gb_of_ddr5_5600_mhz/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o080gzk",
          "author": "EmPips",
          "text": "I wanted to balance getting it as cheap as possible without needing to introduce anything that wouldn't work nicely in my case or need external cooling.\n\nThis resulted in:\n\nRx 6800 + w6800 Pro + 64GB RAM ..but the RAM is DDR4 dual channel :(\n\nGLM 4.6v is the best model I can run. Q4 gets ~17.5 tokens/second with modest context (12k) for one-off chats and ~12 tokens/second with larger context (>40k) for things like coding.\n\nQwen3-Next-80B gets 35 tokens/second",
          "score": 7,
          "created_utc": "2026-01-18 02:37:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0813tv",
              "author": "CollectionOk2393",
              "text": "100 tokens per second = qwen3-vl-30b-a3b-thinking-1m_moe\n\n\n25 tokens per second = google/gemma-3-27b\n\n\n100 tokens per second = qwen/qwen3-vl-30b\n\n\n100 tokens per second = nvidia/nemotron-3-nano\n\n\nI have noticed on the MoE modles I am crushing it at over 100 tokens per second, but on the \"dense\" modles I am only getting like 25 tokens per second. Is this normal?",
              "score": 3,
              "created_utc": "2026-01-18 02:40:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o083lu9",
                  "author": "EmPips",
                  "text": "Can you include the levels of quantization?\n\nBut yes that's very normal. Your GPU needs to search through 27 Billion parameters for **every token** when running Gemma3-27B, whereas despite having more (30 Billion) total parameters, each token only involves your GPU having to go over a measly 3 Billion parameters for Nemotron-Nano or Qwen3-VL-30B.",
                  "score": 6,
                  "created_utc": "2026-01-18 02:54:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08ns1c",
          "author": "SeaFailure",
          "text": "also running Dual 5060ti + 128GB DDR4. LM studio is a good starting point to assess LLM capabilities.",
          "score": 5,
          "created_utc": "2026-01-18 04:57:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08xiqk",
              "author": "CollectionOk2393",
              "text": "Ok sounds yours is similar to mine, what is the best vision model you found that fits our size/performance?Â \n\n\nGemma3 27b q4_k_m is like 25 t/s\n\n\nBut\n\n\nqwen3-vl-30b-a3b-thinking-1m_moe q4_k_m is like 110 t/s\n\n\nThose moe models really are fast!",
              "score": 4,
              "created_utc": "2026-01-18 06:10:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o084foz",
          "author": "LittleBlueLaboratory",
          "text": "I copied DigitalSpaceport's 4x 3090 machine with an Epyc 7702P and 512GB DDR4 2666 in 8 channel. Thank goodness i bought it last summer.Â \n\n\nGLM 4.5 air is my go to for just using the 96GB VRAM and i get about 45 t/s with 500 t/s pp. But I can run Kimi K2 at Q2 at 9 t/s, GLM 4.7 Q4 at about 10 t/s, or Minimax M2.1 Q6 at about 15 t/s. But pp smol with the big models. Fastest I get is 55 t/s with Minimax.\n\n\nThe big models are downright painful to use with OpenCode because of the smol pp. But GLM Air works really well. I would love to use Devstral2 123B (500 t/s pp and 7 t/s) but I get tool calling errors at Q4.",
          "score": 3,
          "created_utc": "2026-01-18 02:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o086ymx",
              "author": "CollectionOk2393",
              "text": "Nice that setup is much better than what I have right now, lol\n\n\nI have 2x ockulink docks, so I could always sell my 5060's and get something bigger down the line. But for now its what I can afford\n\n\nAOOSTAR AG01 External GPU Docking... https://www.amazon.com/dp/B0F8B23CN9?ref=ppx_pop_mob_ap_share\n\n\nThe docks are also amazing, you can run 5090's on them... on the built in power supply! So nice!",
              "score": 2,
              "created_utc": "2026-01-18 03:13:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o08ba2w",
                  "author": "LittleBlueLaboratory",
                  "text": "Woah that dock is awesome! are your 5060s on the docks or are they inside your N5?",
                  "score": 3,
                  "created_utc": "2026-01-18 03:37:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o096hlb",
          "author": "960be6dde311",
          "text": "I got an RTX 5070 Ti and just ordered an extra 5060 Ti 16 GB. Running with 128 GB DDR5 and a Ryzen 9950X.Â \n\n\n\nAlso have an RTX 4070 Ti SUPER that will probably go into one of my Linux servers.Â \n\n\nI like the NVIDIA Nemotron 3 Nano model a lot.Â ",
          "score": 3,
          "created_utc": "2026-01-18 07:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o099bjy",
          "author": "Lg_taz",
          "text": "This is my setup, will eventually be changing for stronger case fans, swapping the Radeon gou for a ProArt 5070Ti 16Gb, and adding another 128Gb 6400 RAM. It was designed as a creative workstation originally, getting into running local LLMs came after the build, it manages Qwen3-coder-42b-Q8 at roughly 10-16 t/s not mega fast but manageable at context of 131k, MoE to CPU and the RAM set to 6000 for stability.\n\nhttps://preview.redd.it/qxdwpeutc2eg1.jpeg?width=1582&format=pjpg&auto=webp&s=bc0c9ad95bc9163bc83ab3fb982cb7ec08c1d916",
          "score": 3,
          "created_utc": "2026-01-18 07:52:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09a0w9",
              "author": "Lg_taz",
              "text": "I also have quite a few spare parts so considering building a dedicated AI machine to call on for better parallel pipelines, struggling big time to get the Radeon card to function in Oobabooga, but can't give up the granular control and flexibility, also I don't like using cloud for privacy as will be hopeful using as part of a business I am in the process of setting up, the idea is quick prototyping for iterating, setup a working prototype to then be made properly.",
              "score": 3,
              "created_utc": "2026-01-18 07:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o09tfqj",
          "author": "No_You3985",
          "text": "I run gpt OSs 20b on rtx 5060 ti. It generates at about 110 t/s which is great for summarization and context building tasks that supplement larger models for all sort of tasks (coding, documentation, graph labeling for rag, etc). By utilizing this small model heavily I save on tokens for commercial llm providers. I run it at high reasoning. I also process my personal notes from obsidian with it - something that I will never share with cloud providers",
          "score": 3,
          "created_utc": "2026-01-18 10:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0aggi9",
          "author": "After-Average-5953",
          "text": "5070 8gb with 24gb ram",
          "score": 3,
          "created_utc": "2026-01-18 13:54:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}