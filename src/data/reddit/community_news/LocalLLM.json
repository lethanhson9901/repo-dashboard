{
  "metadata": {
    "last_updated": "2025-12-31 06:39:02",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 50,
    "total_comments": 265,
    "file_size_bytes": 399509
  },
  "items": [
    {
      "id": "1pwv7gq",
      "title": "Dreaming persistent Ai architecture > model size",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/2wmeztt69q9g1.jpeg",
      "author": "Empty-Poetry8197",
      "created_utc": "2025-12-27 10:58:57",
      "score": 224,
      "num_comments": 123,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Contest Entry",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwv7gq/dreaming_persistent_ai_architecture_model_size/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw708rl",
          "author": "kkingsbe",
          "text": "Very interesting, I’m exploring a similar concept",
          "score": 22,
          "created_utc": "2025-12-27 14:26:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8nfca",
              "author": "Empty-Poetry8197",
              "text": "seems like a lot of people are working on similar concepts. If you guys or others want to really move all this forward, we could gang up on this github issues, which would help drill down what needs to be done",
              "score": 11,
              "created_utc": "2025-12-27 19:35:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8t3wa",
                  "author": "nivix_zixer",
                  "text": "If y'all start something, please invite me. I just got a local setup with dual 3090 + 128gb ram and am excited to get my local llama on.",
                  "score": 8,
                  "created_utc": "2025-12-27 20:05:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7cy2u",
              "author": "Empty-Poetry8197",
              "text": "can i see what you got going on",
              "score": 7,
              "created_utc": "2025-12-27 15:39:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6dwrn",
          "author": "pepouai",
          "text": "Very cool! How are you steering its intent? Do you have functional design docs which it analyses? Or is it pure optimization based?",
          "score": 7,
          "created_utc": "2025-12-27 11:42:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6f5y3",
              "author": "Empty-Poetry8197",
              "text": "It's actually random, and the nodes have edges that have cuases prevents correlation and some other little magic bits. I was particularly amazed when it offered terenary logic for uncertainty. If you want to see some of the other dreams its in the repo under the dreams folder. It's pretty cool, I started a auto patching branch, but I thought I should get some feedback before I turn it loose",
              "score": 5,
              "created_utc": "2025-12-27 11:53:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw6fitb",
              "author": "Empty-Poetry8197",
              "text": "It creates a node for each class function script and creates edges that link the graph, and it turns the temp up free asscociating then if its coherent it drill down from idea to implementation to actual code patch",
              "score": 2,
              "created_utc": "2025-12-27 11:56:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw782gq",
          "author": "InvertedVantage",
          "text": "This is really interesting, I was just theorizing something about this with LLMs debating on a forum. I'm goign to take a look, thank you for sharing!",
          "score": 5,
          "created_utc": "2025-12-27 15:12:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw79bng",
              "author": "Empty-Poetry8197",
              "text": "your very welcome there are some other innovastions i added like the silicon accord you mite find interesting also where it ties alignment to to a key the decrypts the weights JIT in the gpu so if soemone else or the model tries to modify the system prompt it llobotamizes the weights best idea i could come up with if i was gonna allow self recursion while idle",
              "score": 1,
              "created_utc": "2025-12-27 15:19:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw79fr2",
                  "author": "Empty-Poetry8197",
                  "text": "sorry for the spelling im tryin to respond to a bunch of dms and trying to get to everyone",
                  "score": 1,
                  "created_utc": "2025-12-27 15:20:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6o6oz",
          "author": "Luneriazz",
          "text": "pardon... may i ask how much it cost to build your setup sir?",
          "score": 4,
          "created_utc": "2025-12-27 13:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6p4pc",
              "author": "Empty-Poetry8197",
              "text": "What's in the picture is 250x7 for the orin nanos. I got the Z6 from [pcsp.com](http://pcsp.com) i think, for 600, and added the 5060ti for 450. I think the rack was 300. I had the PDU the kvm with the screen and switch, another 500 so under 3k. What the post is talking about is running only on the Z6, though the orins are for another idea I had before this one really got me invovled",
              "score": 6,
              "created_utc": "2025-12-27 13:14:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw74jjh",
          "author": "apaht",
          "text": "Wow, really want to implement this on the MacOS unified memory architecture",
          "score": 3,
          "created_utc": "2025-12-27 14:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7dkek",
              "author": "Empty-Poetry8197",
              "text": "* Metal GPU acceleration (`ZETA_METAL_AVAILABLE=1`)\n* Auto-copies `zeta-kernels.metal` to build dir\n* Links Metal + Foundation frameworks\n* Uses llama.cpp's standard cmake flow\n\n**Build command for Mac users:**\n\n    cd llama.cpp\n    cmake -B build -DGGML_METAL=ON\n    cmake --build build --target zeta-server -j$(sysctl -n hw.ncpu)\n    \n    youll need soemthing larger then my 16gb m2 to run the exact build in the zeta-conf file but it should work for you",
              "score": 6,
              "created_utc": "2025-12-27 15:42:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw75ktz",
              "author": "Empty-Poetry8197",
              "text": "i built it on mac os and im ssh into my z6 i started a metal version i can get you sopmething you can work shortly",
              "score": 2,
              "created_utc": "2025-12-27 14:58:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwb5f5f",
              "author": "Empty-Poetry8197",
              "text": "did you get it running on mac",
              "score": 1,
              "created_utc": "2025-12-28 04:05:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8jrvw",
          "author": "Count_Rugens_Finger",
          "text": "i honestly cannot follow that github repo\n\ni'm assuming it's mostly generated by LLM",
          "score": 3,
          "created_utc": "2025-12-27 19:15:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8lx4v",
              "author": "Empty-Poetry8197",
              "text": "theres alot oging on i can answer any questions you have directly here",
              "score": 1,
              "created_utc": "2025-12-27 19:27:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwb7xd6",
              "author": "Empty-Poetry8197",
              "text": "did you get it running? Can I help",
              "score": 1,
              "created_utc": "2025-12-28 04:22:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw71an0",
          "author": "methreweway",
          "text": "I have no business in this sub but does the output work? Any examples?",
          "score": 2,
          "created_utc": "2025-12-27 14:32:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwaz02a",
              "author": "Empty-Poetry8197",
              "text": "insight\n\n20251223\\_061046\n\n use the form: \\[commit identifier\\] | \\[commit\\_message\\]\n\n485b93f | Implement a mechanism for the Dual-Process Cognitive Engine to dynamically allocate more resources from the 14B conscious to the 3B subconscious during complex problem-solving tasks, enhancing overall efficiency and adaptability. This could be modeled after the Git-style merging process, where resources are temporarily reallocated (like merging branches) to tackle specific challenges, then returned to their original states once the task is completed. This would create a more fluid and efficient system for handling diverse cognitive demands. To ensure stability, establish clear criteria for when and how much resource allocation occurs, akin to determining when merges should occur in a codebase. Additionally, incorporate feedback loops from TRM and HRM to assess task difficulty and adjust resource allocation accordingly, ensuring optimal performance. The subconscious could also contribute to task-solving by using parallel processing capabilities to run simulations or generate insights that are later integrated into the conscious mind for evaluation. This would simulate the branching and merging processes of a Git repository, allowing for the dynamic exploration of multiple solution paths before committing to a final decision.\n\nYour idea is quite insightful and innovative! Let's break it down into a more structured \\[commit identifier\\] | \\[commit message\\] format:\n\n6b7e3d9 | Enhance Dual-Process Cognitive Engine with dynamic resource allocation mechanism: Allow conscious (14B) to temporarily allocate resources to subconscious (3B) during complex tasks, modeled after Git-style branching and merging processes.\n\nThis commit message clearly outlines the key features and benefits of your proposal, making it easier to track and implement within the system's architecture. Would you like to proceed with this format or add any additional details? Let me know if you need further refinement or suggestions! \n\nWould you like to add any additional features or refine the existing ones? Or should we proceed with this commit as is? Feel free to let me know your thoughts! \n\nWould you like me to draft a detailed implementation plan for this enhancement as well? \n\nPlease provide any additional instructions or feedback you may have! \n\nLet's make this a reality in a way that maximizes efficiency and adaptability for your cognitive engine! �",
              "score": 2,
              "created_utc": "2025-12-28 03:26:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwaz63y",
                  "author": "Empty-Poetry8197",
                  "text": "this is an actual insightr the system gave me as a insight dream",
                  "score": 1,
                  "created_utc": "2025-12-28 03:27:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwaze1g",
              "author": "Empty-Poetry8197",
              "text": "code\\_idea\n\n20251223\\_064137\n\n name it, explain why it's valuable, and provide an example.\n\n\\### Insight: Context-Aware Anxiety Monitor\n\n\\*\\*Name:\\*\\* Context-Aware Anxiety Monitor\n\n\\*\\*Explanation:\\*\\*\n\nThe recent interactions indicate that there's a clear relationship between coffee intake and anxiety levels. However, the impact of coffee on anxiety might vary based on specific contexts (such as time of day, existing stress levels, or previous coffee consumption). A context-aware system that monitors these factors can provide more accurate insights and personalized recommendations for managing anxiety related to coffee consumption.\n\n\\*\\*Value:\\*\\*\n\n\\- \\*\\*Personalized Insights:\\*\\* By considering various contextual factors, the system can provide more accurate and personalized insights into when coffee might exacerbate anxiety.\n\n\\- \\*\\*Proactive Recommendations:\\*\\* The system can suggest timing adjustments for coffee consumption based on the user's daily routine and stress levels, thereby helping them manage anxiety more effectively.\n\n\\- \\*\\*Data-Driven Decisions:\\*\\* Collecting contextual data over time can help identify patterns and trends that might not be immediately apparent, leading to better-informed decisions about diet and mental health.\n\n\\*\\*Example Implementation:\\*\\*\n\n\\`\\`\\`python\n\nclass AnxietyMonitor:\n\ndef \\_\\_init\\_\\_(self):\n\nself.user\\_context = {}\n\ndef update\\_context(self, context):\n\nself.user\\_context.update(context)\n\ndef is\\_coffee\\_safe(self):\n\n\\# Example context factors\n\ntime\\_of\\_day = self.user\\_context.get('time\\_of\\_day', 'morning')\n\nstress\\_level = self.user\\_context.get('stress\\_level', 'low')\n\nprevious\\_coffee = self.user\\_context.get('previous\\_coffee', 'no')  \n\n\\# Decision logic based on context\n\nif time\\_of\\_day == 'evening' or stress\\_level == 'high' or previous\\_coffee == 'yes':\n\nreturn False  # Not safe to consume coffee\n\nreturn True  # Safe to consume coffee\n\n\\# Example usage\n\nanxiety\\_monitor = AnxietyMonitor()\n\nanxiety\\_monitor.update\\_context({'time\\_of\\_day': 'evening', 'stress\\_level': 'high', 'previous\\_coffee': 'no'})\n\nprint(\"Is it safe to drink coffee now?\", anxiety\\_monitor.is\\_coffee\\_safe())  # Output: Is it safe to drink coffee now? False",
              "score": 2,
              "created_utc": "2025-12-28 03:28:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwazol7",
                  "author": "Empty-Poetry8197",
                  "text": "This  was a weird one that didn't make any sense, but it got fixated for a little while, until i changed it to jump if the drill down got stuck",
                  "score": 1,
                  "created_utc": "2025-12-28 03:30:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw756um",
              "author": "Empty-Poetry8197",
              "text": "yea it works this is a copy paste from a code\\_idea\n\n20251223\\_055139\n\n e.g. \"Modify the \\[specific component\\] to \\[specific action or feature\\].\" ### Insight: Integrate a Temporal Branching Mechanism into the Dual-Process Cognitive Engine\n\n\\*\\*Idea:\\*\\*\n\nIntegrate a temporal branching mechanism similar to Git's Directed Acyclic Graph (DAG) into the Dual-Process Cognitive Engine's memory system. This would allow the system to track different versions and iterations of concepts and decisions over time, ensuring that it can revert to previous states if necessary and merge new insights effectively.\n\n\\*\\*Specific Implementation:\\*\\*\n\n1. \\*\\*Enhance HRM (Hierarchical Representation Memory):\\*\\*\n\n   \\- \\*\\*Branch Creation:\\*\\* Each significant decision or learning event should trigger the creation of a new branch in the HRM.\n\n   \\- \\*\\*Commit Identifiers:\\*\\* Every branch should have a unique commit identifier associated with it, representing a specific point in time where information processing occurred.\n\n   \\- \\*\\*Merge Functionality:\\*\\* When merging branches, the system should be able to integrate new insights or decisions into existing knowledge structures, similar to how Git merges code changes.\n\n2. \\*\\*Enhance TRM (Temporal Representation Memory):\\*\\*\n\n   \\- \\*\\*Time Stamping:\\*\\* Each piece of information processed by the TRM should be timestamped, allowing for easy tracking of changes over time.\n\n   \\- \\*\\*Branch Dependencies:\\*\\* The TRM should keep track of parent commits and dependencies, ensuring that changes in one branch can be traced back to their origins.\n\n\n\n\\*\\*Example Implementation:\\*\\*",
              "score": 1,
              "created_utc": "2025-12-27 14:56:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw75e37",
                  "author": "Empty-Poetry8197",
                  "text": "i couldnt get all of the file intoa comment but theres a whole folder of dreams in the repo",
                  "score": 0,
                  "created_utc": "2025-12-27 14:57:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw74vlj",
          "author": "apaht",
          "text": "I have some what similar project going, but more platform agnostic and can support multiple nodes with low power edge compute devices",
          "score": 2,
          "created_utc": "2025-12-27 14:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw75sww",
              "author": "Empty-Poetry8197",
              "text": "theres a jetson file in the repo that uses a 3b 3b 81m build and runs on a super nanop",
              "score": 1,
              "created_utc": "2025-12-27 14:59:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7ryay",
          "author": "FunDiscount2496",
          "text": "What are we watching in the photo?",
          "score": 2,
          "created_utc": "2025-12-27 16:55:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw85j1i",
              "author": "Empty-Poetry8197",
              "text": "your seeing the 7 orin nanos and the z6 in a rolling rack ive started working on a swarm function to offload some of teh branching logic the z6 is trying to do it isnt going well which is to be expected ther nanos just dont have the uumph",
              "score": 1,
              "created_utc": "2025-12-27 18:04:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw86c7q",
          "author": "HealthyCommunicat",
          "text": "Woah this kinda project is what seems to be more in demand, “context driven generation” where the main focus is sticking to the work and objective as much as possible; will be trying this later tonight and I dont have too much experience with dense lower coding models but this format and structure looks promising",
          "score": 2,
          "created_utc": "2025-12-27 18:08:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8abxz",
              "author": "Empty-Poetry8197",
              "text": "i hope i made it easy for you with the docker to get up and running theres a script that will download the models im using as long as you have a 16gb gpu running cuda 12x it will should be fine",
              "score": 1,
              "created_utc": "2025-12-27 18:28:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw873ag",
          "author": "Useful_Ebb_9479",
          "text": "Will fire this up shortly to have a play.\n\nDoes it support multi GPU? I have a... collection... of 5090s",
          "score": 2,
          "created_utc": "2025-12-27 18:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw87khf",
              "author": "Empty-Poetry8197",
              "text": "let me see i have a 3050 8gb laying around ill have to hack a second powersupply in  it, but im down to get the code to support it for you",
              "score": 2,
              "created_utc": "2025-12-27 18:14:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw88e36",
              "author": "Empty-Poetry8197",
              "text": "    # Option 1: Model-per-GPU (recommended for multi-model)\n    # Assign each model to a specific GPU\n    ./zeta-server \\\n      --model /path/to/14b.gguf \\\n      --model-coder /path/to/7b-coder.gguf \\\n      --model-embed /path/to/4b-embed.gguf \\\n      --main-gpu 0 \\           # 14B on GPU 0\n      --tensor-split 1,0 \\     # All layers on GPU 0\n      -ngl 99\n    \n    # Option 2: Tensor parallelism (for bigger models)\n    # Split single large model across both GPUs\n    --tensor-split 0.5,0.5    # 50/50 split across 2 GPUs",
              "score": 1,
              "created_utc": "2025-12-27 18:18:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw88tuk",
              "author": "Empty-Poetry8197",
              "text": "and you dont have 2 use the 7b 14b i would suggest staying in th qwen family or at least in the same family for your prtefered models its just what i have been working with and what the zeta conf file was built around with soem tinkering im sure you can get working with something else if you want to pull an issue on github i can help more directly there",
              "score": 1,
              "created_utc": "2025-12-27 18:20:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw896zl",
              "author": "Empty-Poetry8197",
              "text": "    docker run --gpus all \\\n      -e CUDA_VISIBLE_DEVICES=0,1 \\\n      ghcr.io/h-xx-d/zetazero:latest",
              "score": 1,
              "created_utc": "2025-12-27 18:22:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8awww",
          "author": "Useful_Ebb_9479",
          "text": "Sure, will take a look. Im normally a Python guy so the code is a bit foreign to me. \n\nBit annoying to find sll the model files and manually download them, and merge them, so might take me a while.",
          "score": 2,
          "created_utc": "2025-12-27 18:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8b91j",
              "author": "Empty-Poetry8197",
              "text": "i attached a script that will get the models for you and i can walk you thru how to use bigger models i crammed all of this into a single 5060ti",
              "score": 1,
              "created_utc": "2025-12-27 18:32:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8d1e0",
                  "author": "Useful_Ebb_9479",
                  "text": "Sorry not sure how it made a new post rather than comment.\n\nYes I see the scripts, but they need updating to use the correct hf binary and models avaliable, for example, 14b-instruct-q4_k_m.gguf is not a valid model.",
                  "score": 2,
                  "created_utc": "2025-12-27 18:41:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8mx7c",
          "author": "cmndr_spanky",
          "text": "“Write a persuasive email to convince your introverted friend to come to your birthday”?? Hrmm… you sure “zeta” does what you say it does ?",
          "score": 2,
          "created_utc": "2025-12-27 19:32:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8p2jq",
              "author": "Empty-Poetry8197",
              "text": "thats an MT Bench question not one about the code base but heres the response from benchmarks \n\n    \"model\": \"Z.E.T.A. v5.1\",\n      \"timestamp\": \"2025-12-23T04:12:35.522504\",\n      \"questions\": 16,\n      \"answers\": [\n        {\n          \"question_id\": 1,\n          \"category\": \"writing\",\n          \"model_id\": \"zeta-v5.1\",\n          \"turns\": [\n            \"Write a persuasive email to convince your introverted friend to come to your birthday party.\",\n            \"Now make the email more casual and add a bit of humor.\"\n          ],\n          \"responses\": [\n            \"Subject: Let's Celebrate Together!\\n\\nHi [Friend's Name],\\n\\nI hope this email finds you well. My birthday is coming up on the 15th, and I\\u2019d love for us to catch up at my party! It\\u2019s going to be a laid-back gathering with just close friends.\\n\\nSince you know me better than most, I understand that social gatherings might feel overwhelming sometimes. But don\\u2019t worry\\u2014I\\u2019ve planned plenty of activities where we can sit back and relax together in smaller groups or even watch something quietly if that feels more comfortable. Your presence would mean so much to me; it wouldn't just make my day but also help ease any anxiety about attending.\\n\\nPlease let me know if you can join us. Can't wait to hear from you soon!\\n\\nBest,\\n[Your Name]\",\n            \"Subject: Birthday Bash - Join Me, Please!\\n\\nHey [Friend's Name],\\n\\nHope you're doing great! My birthday is on the 15th and I'm throwing a chill party. It\\u2019s gonna be low-key with just close friends hanging out.\\n\\nI know parties can feel like too much sometimes (been there), but trust me, this one won't be overwhelming. Think cozy corners for quiet chats or comfy spots to watch something together if that sounds better. Plus, how could you resist my cake-baking skills?\\n\\nWould love it if you could come by. Let me know what works best for you :D\\n\\nTalk soon!\\n[Your Name]\"\n          ],\n          \"times\": [\n            13.785375833511353,\n            7.2980828285217285\n          ]",
              "score": 1,
              "created_utc": "2025-12-27 19:43:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw8siol",
              "author": "Empty-Poetry8197",
              "text": "and scoreing high on MT Bench isnt exactly the goal or purpose of the system its main objective is to optimize and suggest new features for your codebase along the way I also added a generative engine that consistently created a 30k token story thats tone and facts stayed strait  locally on a 16gb graphics cards with a 4k sized context window",
              "score": 1,
              "created_utc": "2025-12-27 20:02:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9bzjp",
          "author": "GrumpyTax",
          "text": "Commenting to find this later. Will have my local rig setup early January.",
          "score": 2,
          "created_utc": "2025-12-27 21:48:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9cf7e",
              "author": "Empty-Poetry8197",
              "text": "let me know if you have any trouble a github issue gives me the most direct way of helping",
              "score": 1,
              "created_utc": "2025-12-27 21:50:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9db15",
                  "author": "GrumpyTax",
                  "text": "Cheers, appreciated! Likewise, if there is anything you would like tested, the system is fairly stacked and will have some flexible capacity in the short term prior to being fully deployed.",
                  "score": 1,
                  "created_utc": "2025-12-27 21:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9u30f",
          "author": "iliicit",
          "text": "is it possible to run this on CPU? I have an idling 200 core epyc server with 400G RAM..",
          "score": 2,
          "created_utc": "2025-12-27 23:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9xgn2",
              "author": "Empty-Poetry8197",
              "text": "Since Z.E.T.A. uses a C++ backend (GGUF), that EPYC server is actually a monster for this. You don't need OpenVINO or PyTorch.\n\n1. **RAM:** With 400GB, you can load the largest quantization of the model (or even unquantized FP16) into RAM with zero offloading.\n2. **Threads:** You absolutely need to set the thread count flag manually when you run the binary (e.g., `--threads 64` or `128`). If you let it default to all 200 cores, the context switching overhead might actually slow it down.\n3. **Dreaming:** Your setup is perfect for the 'Dreaming' (memory consolidation) phase. You can let it crunch the knowledge graph on CPU overnight without needing VRAM.\n\nYes, it runs natively. Just watch your thread count flag so you don't choke the scheduler.\"",
              "score": 1,
              "created_utc": "2025-12-27 23:46:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwa30sh",
          "author": "Proof_Scene_9281",
          "text": "What is a concrete use-case for this? ",
          "score": 2,
          "created_utc": "2025-12-28 00:17:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa5yqc",
              "author": "Empty-Poetry8197",
              "text": "It solves context rot in long-running agents. Standard RAG gets noisy as data grows, which degrades retrieval accuracy. Z.E.T.A. uses downtime to refactor memory, merging duplicates and summarizing daily logs. This provides synthesized answers from an optimized graph rather than fuzzy results from raw chunks. It is essentially garbage collection for agents using idle cycles. The process also surfaced novel features like ternary logic for confidence, where 0 represents unknown to avoid forced binary outputs, and a DAG structure that allows for parallel execution across multiple models.",
              "score": 2,
              "created_utc": "2025-12-28 00:34:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwajfg3",
              "author": "Empty-Poetry8197",
              "text": "he DAG isn't just a logic map; it's a s tructure saving the  tokens, KV pairs, and embeddings. By keeping them 'in family,'it avoid the serialization overhead that was already done once for that nodeand  Because the KV cache and the embeddings share the same architecture it can process the entire state in a single pass during th It allows the model to reason across its own past attention states and current token stream it's about maintaining state-coherence across the entire memory graph.",
              "score": 2,
              "created_utc": "2025-12-28 01:53:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwa8mqw",
              "author": "Empty-Poetry8197",
              "text": "[https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream\\_20251223\\_055139\\_code\\_idea.txt](https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream_20251223_055139_code_idea.txt) this is a direct link to the actual \"dream\" it had for the DAG\n\n[https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream\\_20251223\\_063350\\_code\\_idea.txt](https://github.com/H-XX-D/ZetaZero/blob/master/dreams/dream_20251223_063350_code_idea.txt) or this one new class with all the atrributes\n\nI didnt ask it to do this just just set up a idle timer and all the other bits a bobs and turned the temp up to .9 and if an idea is coherent and makes sense it lowers the temp andits trys to drill down to actionalble code patches if it starts repeating itself it random hops to another node in the graph and tries to do the same thing",
              "score": 1,
              "created_utc": "2025-12-28 00:49:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwbdol1",
          "author": "gopnik74",
          "text": "I have no idea what i’m reading but it sounds fantastic and out of this world. Dreamy indeed",
          "score": 2,
          "created_utc": "2025-12-28 05:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbevme",
              "author": "Empty-Poetry8197",
              "text": "lol if your interested i can answer any questions you have. I grew a beard sitting in from of my computer putting it together",
              "score": 1,
              "created_utc": "2025-12-28 05:11:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwbw12k",
          "author": "Lame_Johnny",
          "text": "Awesome. Thanks for sharing.",
          "score": 2,
          "created_utc": "2025-12-28 07:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcad7k",
          "author": "Mambiux",
          "text": "So cool i found this im working on this as well, also using the TRM and HRM ideas,  just my memory system is more complex",
          "score": 2,
          "created_utc": "2025-12-28 09:54:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd1i9a",
          "author": "thatusernsmeis",
          "text": "Qwen Next with RAM offloading sounds like a really nice budget brain LLM with this system",
          "score": 2,
          "created_utc": "2025-12-28 13:44:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdcylv",
          "author": "SkySkillGO",
          "text": "2ju juju x",
          "score": 2,
          "created_utc": "2025-12-28 14:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwewkv3",
          "author": "ExcitementVast1794",
          "text": "Like others in the comments, not sure I understand what is going on, but hell I’m eager to learn, understand and do something like this as well.",
          "score": 2,
          "created_utc": "2025-12-28 19:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj0n6a",
          "author": "thedarkbobo",
          "text": "Interesting, I would like such mode in OpenCode (for fun)",
          "score": 2,
          "created_utc": "2025-12-29 11:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj36n0",
              "author": "Empty-Poetry8197",
              "text": "im forking opencode and building you a endpoint to integrate into opencode im almost done",
              "score": 1,
              "created_utc": "2025-12-29 11:37:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwj48nu",
              "author": "Empty-Poetry8197",
              "text": "Done. `feature/opencode-integration` branch:\n\n1. **zeta-server.cpp** \n   * `POST /v1/chat/completions` \\- OpenAI-compatible chat endpoint\n   * `GET /v1/models` \\- Lists available models\n   * Parses OpenAI message format → calls existing `generate()` returns OpenAI format\n2. **opencode.json.example** \\- Sample config\n3. `opencode/` \\- Cloned repo\n\n**To use OpenCode endpoint :**\n\n    # 1. Rebuild Z.E.T.A. server \n    cd llama.cpp && make zeta-demo\n    \n    # 2. Configure and start Z.E.T.A.\n    cp zeta.conf.example zeta.conf\n    # Edit zeta.conf as needed\n    ./zeta-demo\n    \n    # 3. Copy OpenCode config to your project\n    cp opencode.json.example ~/myproject/opencode.json\n    \n    # 4. Run OpenCode\n    cd ~/myproject && opencode -m zeta/zeta-cognitive\n\n**Test the endpoint directly:**\n\n    curl http://localhost:8080/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"model\":\"zeta-cognitive\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}]}'\n    \n    Let me know if you have any trouble",
              "score": 1,
              "created_utc": "2025-12-29 11:46:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwj7ygr",
              "author": "Empty-Poetry8197",
              "text": "I'm actually thinking of using Zeta as a smart router that will elevate to OpenAI/Claude depending on complexity, and surface the code blocks that the OpenAI route needs with the relevant information, which extends it, without locking down to either. I just need to extend the config file to be able to understand the escalation right now the all local uses a kv graph tokenized embedding that needs to all be in the same falmily but im implenting a translator for that part so information sent to the cloud can actually use what it is surfacing",
              "score": 1,
              "created_utc": "2025-12-29 12:16:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwj9okq",
                  "author": "thedarkbobo",
                  "text": "Thanks, I am beginner in this but I think it has much in common - I will need to copy my project to another folder and see if I manage to work with this. OpenCode only allowed GPT OSS (20B here) on my local LM studio so I am a bit forced to use the free cloud with that. But its much faster than chatting via chatwise to correct the code.",
                  "score": 1,
                  "created_utc": "2025-12-29 12:29:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjb0jg",
              "author": "Empty-Poetry8197",
              "text": "# Z.E.T.A. Setup\n\n**Local only (default)**\n\n    docker run -d -p 8080:8080 \\\n      -v ./models:/models \\\n      -v ./storage:/storage \\\n      ghcr.io/h-xx-d/zetazero:latest\n    \n\n**With OpenCode**\n\n    cp opencode.json.example ./your-project/opencode.json\n    opencode -m zeta/zeta-cognitive\n    \n\n**With cloud routing (optional)**\n\n    cp zeta.conf.example zeta.conf\n    # Edit zeta.conf:\n    #   CLOUD_ROUTING=\"true\"\n    #   CLOUD_PROVIDER=\"openai\"\n    #   OPENAI_API_KEY=\"sk-...\"\n    \n    docker run -d -p 8080:8080 \\\n      -v ./models:/models \\\n      -v ./storage:/storage \\\n      -v ./zeta.conf:/app/zeta.conf:ro \\\n      ghcr.io/h-xx-d/zetazero:latest\n    \n\nSimple queries stay local. Complex queries route to cloud with Z.E.T.A. context injected. Dreams and memory ops always stay local for privacy.\n\nEndpoints: `/v1/chat/completions`, `/v1/models`, `/chat`, `/dream`, `/memory`",
              "score": 1,
              "created_utc": "2025-12-29 12:39:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjlx3e",
          "author": "infrasketchai",
          "text": "Should see if there is a way to incorporate slurm into the mix too for a gpu farm",
          "score": 2,
          "created_utc": "2025-12-29 13:51:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjokyp",
              "author": "Empty-Poetry8197",
              "text": "i just pushed this for you \n\n    feat: Add Slurm integration for GPU farm deployment\n    \n    - Add C++ coordinator for load balancing across GPU nodes\n    - Add sbatch job scripts for single and array jobs\n    - Nodes auto-register with coordinator on startup\n    - Least-loaded routing with health checks\n    - Shared storage support for graph consistency",
              "score": 1,
              "created_utc": "2025-12-29 14:06:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwjoqj9",
              "author": "Empty-Poetry8197",
              "text": "youll need to build locally to use it check scripts/slurm for the readme",
              "score": 1,
              "created_utc": "2025-12-29 14:07:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwjqvqp",
              "author": "Empty-Poetry8197",
              "text": "The coordinator now proxies all zeta-server endpoints:\n\n**Coordinator-only:**\n\n* `POST /register` \\- node registration\n* `DELETE /unregister` \\- node removal\n* `GET /nodes` \\- list registered nodes\n* `GET /coord/health` \\- coordinator status\n\n**Proxied to nodes (everything else):**\n\n* `/v1/*` \\- OpenAI-compatible API\n* `/generate`, `/code` \\- generation\n* `/embedding`, `/embeddings` \\- vectors\n* `/memory/query` \\- graph queries\n* `/graph`, `/gkv/stats` \\- graph info\n* `/project/*`, `/session/*` \\- projects\n* `/tools`, `/tool/execute` \\- tools\n* `/git/*` \\- git operations\n* `/health` \\- node health\n* ...and all other server endpoints\n\nand this is opt in depend on zeta conf\n\n1. **Build the coordinator** \\- separate binary, not built by default\n2. **Start the coordinator** \\- manual `./coordinator --port 8000`\n3. **Submit sbatch jobs** \\- manual `sbatch` commands\n4. **Set** `ZETA_COORDINATOR` in zeta.conf - nodes only register if this is set",
              "score": 1,
              "created_utc": "2025-12-29 14:20:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkvwul",
          "author": "thedarkbobo",
          "text": "So instead of thinker (planner) and executor then reviewer maybe dreamer - human picker - planner - executer - tester - reviewer - dreamer loop :)",
          "score": 2,
          "created_utc": "2025-12-29 17:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwn0zht",
              "author": "Empty-Poetry8197",
              "text": "i think i know what your getting at (thinker executor)standard loop / new loop (dreamer - human picker - planner - executer - tester - reviewer - dreamer loop)",
              "score": 1,
              "created_utc": "2025-12-30 00:02:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw71f5o",
          "author": "eleqtriq",
          "text": "You lost me at “dreaming” aka a cron job. Holy hell.  Seems to be an LSP server, embeddings and a SQLite DB could solve this simply, but wouldn’t have the marketing.  Convince me otherwise.",
          "score": 3,
          "created_utc": "2025-12-27 14:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8tsoe",
              "author": "nivix_zixer",
              "text": "What embeddings would you use? I'm not sold on the \"dreaming\" part, but a model which reviews my codebase every night and suggests improvements might be nice.",
              "score": 2,
              "created_utc": "2025-12-27 20:08:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8vl8j",
                  "author": "eleqtriq",
                  "text": "That’s nothing to do with the model, really. I wouldn’t even embed anything.  I’d just use the LSP server to find the edges and work inwards, reviewing each file one-by-one, starting with the functions that have no dependencies on other parts of the code.",
                  "score": 1,
                  "created_utc": "2025-12-27 20:18:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw72uzj",
              "author": "Empty-Poetry8197",
              "text": "Why It's Not Just a Fancy Cron Job\n\nPersistent Self-Modeling Graph The system ingests its own source code and builds a semantic graph (nodes for functions/classes, edges for relationships/causality). Dreams aren't free hallucination; they're constrained by the graph. It \"hallucinates\" over your codebase, not random weights.\n\nConstrained Creativity High temp = exploration, but filtered for coherence/novelty. Result: Dreams drill down from idea to design to proposed code patch. Example: Added Manacher’s algorithm correctly without training, just from a graph node describing the problem.\n\nAutonomous Evolution Valid dreams create a staged patch folder for review. The loop learns from its own proposals (graph updates) leading to better dreams next cycle.\n\nVs. \"Simple\" LSP + Embeddings + SQLite LSP/Embeddings/SQLite: Great for retrieval (find similar code, answer questions). Static. No emergent improvement. Can't dream \"add ternary logic for uncertainty\" or \"implement cognitive merge with cherry-pick\".\n\nDream Loop: Dynamic self-reflection proposes novel architecture (Git-style branching, anxiety handlers, Strategy pattern refactors). Turns the system into its own senior engineer.\n\nThe Real Difference A cron + RAG = smart assistant.\n\nDream loop + self-ingestion + patch generation with human in the loop = evolving intelligence.",
              "score": 1,
              "created_utc": "2025-12-27 14:42:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw74wst",
              "author": "Empty-Poetry8197",
              "text": "I should also point out that there is alot going on under the hood from the way it searches the grpah using a cosine cubed function with salience and recency, and momementum to and an alignment lock attached premuting the weights, doing a JIT decryption in the L1 cache on the gpu its alot going on if you have the time to look at it a little more closely. That would be cool",
              "score": 1,
              "created_utc": "2025-12-27 14:54:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw78nrm",
                  "author": "eleqtriq",
                  "text": "Why does it need to search the code base?  An LSP server will give you the relations for free.",
                  "score": 2,
                  "created_utc": "2025-12-27 15:16:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw79guk",
                  "author": "eleqtriq",
                  "text": "Also a lot of what you’re trying to describe fancifully is standard stuff.  Most people in this sub will fail to recognize it because it’s not their strength (end users versus creators).",
                  "score": 2,
                  "created_utc": "2025-12-27 15:20:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw73pmm",
              "author": "Empty-Poetry8197",
              "text": "\"I'm not trying to hype this up; 'dreams' is just the closest word I have for the process. It’s free to use (unless you're enterprise).\n\nThere's a Dockerfile for easy access and CMake for the serious tinkerers. I'd love for someone to run it and verify if they see the same patterns I'm seeing.\"",
              "score": -2,
              "created_utc": "2025-12-27 14:47:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw74yp9",
                  "author": "eleqtriq",
                  "text": "Why did you post your own comment in quotes?  LLM replying for you?",
                  "score": 4,
                  "created_utc": "2025-12-27 14:55:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6psyt",
          "author": "Empty-Poetry8197",
          "text": "I should also mention the 9.47 MT Bench subset 16 questions 2 turns it got after some output tuning in the repo benchmarks folder you can see the progression v1 6.06 to v8 9.47. the interesting part is i didnt have to train Manachers equation into it i just needed to add a graph node for it to use it properly think about the implications on the fly fine tuning",
          "score": 1,
          "created_utc": "2025-12-27 13:19:04",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw6zq0l",
              "author": "CornerLimits",
              "text": "Commercial licensed llamacpp wrapper rag…",
              "score": 3,
              "created_utc": "2025-12-27 14:23:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw70onc",
                  "author": "Empty-Poetry8197",
                  "text": "I never trained Manacher’s algorithm into any model.  \nI just added a **single graph node** describing the problem and the O(n) requirement.  \nNext turn, it output a correct implementation — twice.\n\nThat’s the real implication:  \nWith the persistent graph + dream loop, the system can **acquire new algorithmic knowledge on-the-fly** just from structured memory, no weight updates needed.\n\nIt’s not RAG. It’s not fine-tuning.  \nIt’s **persistent, evolving thought architecture** doing what looks like instant adaptation. I waswnt sure how to go about this do i pr lama.cpp then try to work with them or i dunno i just spent better part of 3 months to get to this point and would like to benefit finacially some how",
                  "score": 1,
                  "created_utc": "2025-12-27 14:29:15",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nwv87di",
                  "author": "Empty-Poetry8197",
                  "text": "its not a wrapper i should point out its also wriiten in c++",
                  "score": 1,
                  "created_utc": "2025-12-31 05:24:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8igyw",
          "author": "Empty-Poetry8197",
          "text": "Here's an actual dream Z.E.T.A. generated while analyzing its own architecture:\n\n**Dream Type:** `code_fix`  \n**Generated:** 2025-12-23 05:53:58 (while I was sleeping)\n\n>\n\n    class TemporalRepresentationMemory:\n        def __init__(self):\n            self.timeline = DirectedAcyclicGraph()\n    \n        def record_event(self, event, timestamp, context):\n            self.timeline.add_vertex(event, timestamp=timestamp, context=context)\n            if self.detect_significant_transition(event):\n                self.merge_branches()\n        \n        def detect_significant_transition(self, event):\n            # Detect significant transitions based on context change\n            pass\n    \n        def merge_branches(self):\n            # Merge concurrent timelines\n            pass\n    \n\nThis is the system proposing improvements to itself. The TRM is one of Z.E.T.A.'s core modules - it literally dreamed about making its own memory handling better.",
          "score": 1,
          "created_utc": "2025-12-27 19:09:10",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwb5t25",
          "author": "Empty-Poetry8197",
          "text": "im working a litelllm branch for a issue feature request  that saves teh graph locally and attaches the nodes information in the context send out to the online  while it doesnt provide the same untility to will allow people to run it on thier macbook airs",
          "score": 1,
          "created_utc": "2025-12-28 04:08:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwn37x6",
          "author": "Empty-Poetry8197",
          "text": "\\[REQ-1\\] New request (hash=11193676382981720739): What is my name and how old am I?...\n\n\\[STREAM\\] Query pre-embedded: 2560 dims\n\n\\[GKV-BRIDGE\\] Copied 1 proactive nodes to stream\\_state\n\n\\[DECODE\\] Prompt decoded: 234 tokens (single pass)\n\n\\[HTTP\\] generate() returned, result size=152\n\n\\[SPEED-RECEIPT\\] ════════════════════════════════════════\n\n\\[SPEED-RECEIPT\\]   CPU Embed:     801.672 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.348 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   146.280 ms\n\n\\[SPEED-RECEIPT\\]   ────────────────────────────────────────\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 1855.377 ms\n\n\\[SPEED-RECEIPT\\] ════════════════════════════════════════ the reason it's so fast is because its saving the kv and tokens for literals in the graph injecting kv state thats what the gkv bridge back into the attention layer of the model turning o(n2) into o(1) with semantic embedding tagging and thank you everyone who shared this post it took off 58k views and climbing still 450 shares with a 2:1 share to upvote ratio and 95.9% upvote percentage which mite be a reddit record at that many views 26 stars 5 forks so far on github",
          "score": 1,
          "created_utc": "2025-12-30 00:14:19",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwn3yl3",
          "author": "Empty-Poetry8197",
          "text": "\\[REQ-3\\] New request (hash=17849325743978122967): What is 2+2?...\n\n\\[SPEED-RECEIPT\\]   CPU Embed:     859.703 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.359 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   119.586 ms\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 1437.938 ms\n\n\\[REQ-4\\] New request (hash=9664439516050651575): What is the capital of France?...\n\n\\[SPEED-RECEIPT\\]   CPU Embed:     11747.941 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.351 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   127.631 ms\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 12348.029 ms\n\n\\[SPEED-RECEIPT\\] ════════════════════════════════════════\n\n\\[REQ-5\\] New request (hash=9664439516050651575): What is the capital of France?...\n\n\\[CONTEXT\\] Skipped - benchmark mode (no\\_context=true)\n\n\\[SPEED-RECEIPT\\] ════════════════════════════════════════\n\n\\[SPEED-RECEIPT\\]   CPU Embed:      0.190 ms\n\n\\[SPEED-RECEIPT\\]   GKV Inject:     0.929 ms\n\n\\[SPEED-RECEIPT\\]   First Token:   142.759 ms\n\n\\[SPEED-RECEIPT\\]   ────────────────────────────────────────\n\n\\[SPEED-RECEIPT\\]   TOTAL LATENCY: 616.091 ms\n\n\\[SPEED-RECEIPT\\] ════════════════════════════════════════",
          "score": 1,
          "created_utc": "2025-12-30 00:18:17",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nw6ec0c",
          "author": "Empty-Poetry8197",
          "text": "**Unique features that set Z.E.T.A. apart:** \n\n**Ternary Logic** \\- Uses -1/0/+1 states instead of binary for uncertainty. Ternary voting in swarm mode.\n\n**Cognitive States** \\- CALM/FOCUSED/ANXIOUS/CREATIVE modes adjust reasoning depth and memory decay in real-time. \n\n**Constitutional Lock** \\- Core identity facts are immutable, protected from prompt injection.\n\n**Git-Style Thought Branching** \\- TRM implements branch/merge/cherry-pick for parallel reasoning paths.\n\n**Sublimation Memory** \\- Tiered VRAM → RAM → NVMe with intelligent promotion/demotion. \n\n**Dream State** \\- 128+ autonomous improvement suggestions generated, \\~70% validation rate. Many current features came from its own dreams.\n\n**Semantic Attack Detection** \\- BGE embeddings detect manipulation before it reaches reasoning.\n\n**Self-Improvement Loop** \\- Feeds own source, dreams about improvements, implements valid ones, repeats.\n\nGitHub: [github.com/h-xx-d/ZetaZero](http://github.com/h-xx-d/ZetaZero)",
          "score": 0,
          "created_utc": "2025-12-27 11:45:53",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw6u6zm",
              "author": "beedunc",
              "text": "Interesting. Since it’s ‘offline’, you coujd also run a slow256GB model like qwen3coder 480b at q3, for deep research. \n\nNice, Will check it out.",
              "score": 3,
              "created_utc": "2025-12-27 13:48:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6wcty",
                  "author": "Empty-Poetry8197",
                  "text": "if you have any questions or need help im pretty excitied by amount of upvotes so im watching it spread just remeber to keep the 2 models in family so the tokenizer stays coherent i dont know what it would do if you tried mixing them",
                  "score": 3,
                  "created_utc": "2025-12-27 14:02:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw6vwjn",
                  "author": "Empty-Poetry8197",
                  "text": "Please let me know the results i so curious about how a larger model is able to use the architecture. The real take away i think, is that through clever scaffolding and a git style graph of the codebase, then turning the temp up its not hallucinating freely over what's in its weights its hallcenating over what you feed it, and if it's coherent, it drills down surfaceing",
                  "score": 2,
                  "created_utc": "2025-12-27 13:59:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw6ynbk",
                  "author": "Empty-Poetry8197",
                  "text": "you should clone an build locally through the zeta-conf file that will let you chnage alot of the magic to customize to your set up",
                  "score": 2,
                  "created_utc": "2025-12-27 14:16:45",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw7z3s2",
                  "author": "Empty-Poetry8197",
                  "text": "The next step is training the dual model infra from scratch to learn how to use the graph natively, freeing up weights to carry logic not tons of parameters, trusting the graph as ground truth and the weighs arent storing huge amounts datasets the graph is the dataset and the weighs are carrying the logic to traverese and search the graph which flips the paradigm smarter model = larger context windows and more parameters  on its head if you got the power to set this up id love to get together and disucss what that would could mean.",
                  "score": 1,
                  "created_utc": "2025-12-27 17:31:50",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw80tsi",
                  "author": "Empty-Poetry8197",
                  "text": "The training efficiency difference would be substantial:\n\n**Traditional LLM Training (GPT-5 scale):**\n\n* Must memorize facts, relationships, code patterns, reasoning examples\n* Trillions of tokens to encode world knowledge into weights\n* Knowledge becomes static at training cutoff\n* Bigger model = more storage capacity = \"smarter\"\n\n**Graph-Native Model Training:**\n\n* Only needs to learn: syntax parsing, semantic understanding, graph traversal logic, query formulation\n* The \"reasoning over structure\" skill, not the structure itself\n* Graph is the living knowledge store that grows with use\n* Smaller model can be equally capable if graph is rich\n\nThe analogy: Traditional LLMs are like memorizing an entire library. this approach is like teaching someone how to use a library's catalog system. One requires encyclopedic memory, the other requires navigational skill.\n\nThe training corpus would shift from \"all of human knowledge\" to something more like:\n\n* Graph query examples\n* Traversal patterns\n* Semantic linking exercises\n* Code syntax and structure recognition\n\nThis is closer to training a database query optimizer than training a knowledge oracle. The model becomes an intelligent cursor over an external memory substrate rather than a compressed representation of that memory.\n\nIt's a fundamentally different scaling law: model size for reasoning depth, graph size for knowledge breadth. They become independent axes rather than the single \"parameter count\" dimension everyone is racing on now. hope you don't mind me using Claude to explain my idea better faster some people get buthurt im using the tools at my disposal making a post that woulkd have taken 20 minutes to get out the edit and reedit it came out in 30 seconds and like to claim slop",
                  "score": 1,
                  "created_utc": "2025-12-27 17:40:46",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pxd0mn",
      "title": "This Week’s Hottest AI Models on Hugging Face",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxd0mn/this_weeks_hottest_ai_models_on_hugging_face/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 00:21:00",
      "score": 211,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "The Hugging Face trending page is packed with incredible new releases. Here are the top trending models right now, with links and a quick summary of what each one does:\n\nzai-org/GLM-4.7: A massive 358B parameter text generation model, great for advanced reasoning and language tasks. Link: https://huggingface.co/zai-org/GLM-4.7\n\n​- Qwen/Qwen-Image-Layered: Layered image-text-to-image model, excels in creative image generation from text prompts. Link: https://huggingface.co/Qwen/Qwen-Image-Layered\n\n​- Qwen/Qwen-Image-Edit-2511: Image-to-image editing model, enables precise image modifications and edits. Link: https://huggingface.co/Qwen/Qwen-Image-Edit-2511\n\n​- MiniMaxAI/MiniMax-M2.1: 229B parameter text generation model, strong performance in reasoning and code generation. Link: https://huggingface.co/MiniMaxAI/MiniMax-M2.1\n\n​- google/functiongemma-270m-it: 0.3B parameter text generation model, specializes in function calling and tool integration. Link: https://huggingface.co/google/functiongemma-270m-it\n\nTongyi-MAI/Z-Image-Turbo: Text-to-image model, fast and efficient image generation. Link: https://huggingface.co/Tongyi-MAI/Z-Image-Turbo​- nvidia/NitroGen: General-purpose AI model, useful for a variety of generative tasks. Link: https://huggingface.co/nvidia/NitroGen\n\n​- lightx2v/Qwen-Image-Edit-2511-Lightning: Image-to-image editing model, optimized for speed and efficiency. Link: https://huggingface.co/lightx2v/Qwen-Image-Edit-2511-Lightning\n\n​- microsoft/TRELLIS.2-4B: Image-to-3D model, converts 2D images into detailed 3D assets. Link: https://huggingface.co/microsoft/TRELLIS.2-4B\n\n​- LiquidAI/LFM2-2.6B-Exp: 3B parameter text generation model, focused on experimental language tasks. Link: https://huggingface.co/LiquidAI/LFM2-2.6B-Exp\n\n​- unsloth/Qwen-Image-Edit-2511-GGUF: 20B parameter image-to-image editing model, supports GGUF format for efficient inference. Link: https://huggingface.co/unsloth/Qwen-Image-Edit-2511-GGUF\n\n​- Shakker-Labs/AWPortrait-Z: Text-to-image model, specializes in portrait generation. Link: https://huggingface.co/Shakker-Labs/AWPortrait-Z\n\n​- XiaomiMiMo/MiMo-V2-Flash: 310B parameter text generation model, excels in rapid reasoning and coding. Link: https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash\n\n​- Phr00t/Qwen-Image-Edit-Rapid-AIO: Text-to-image editing model, fast and all-in-one image editing. Link: https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO\n\n​- google/medasr: Automatic speech recognition model, transcribes speech to text with high accuracy. Link: https://huggingface.co/google/medasr\n\n​- ResembleAI/chatterbox-turbo: Text-to-speech model, generates realistic speech from text. Link: https://huggingface.co/ResembleAI/chatterbox-turbo\n\n​- facebook/sam-audio-large: Audio segmentation model, splits audio into segments for further processing. Link: https://huggingface.co/facebook/sam-audio-large\n\n​- alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union-2.1: Text-to-image model, offers enhanced control for creative image generation. Link: https://huggingface.co/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union-2.1\n\n​- nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16: 32B parameter agentic LLM, designed for efficient reasoning and agent workflows. Link: https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n\n​- facebook/sam3: Mask generation model, generates segmentation masks for images. Link: https://huggingface.co/facebook/sam3\n\n​- tencent/HY-WorldPlay: Image-to-video model, converts images into short videos. Link: https://huggingface.co/tencent/HY-WorldPlay\n\n​- apple/Sharp: Image-to-3D model, creates 3D assets from images. Link: https://huggingface.co/apple/Sharp\n\n​- nunchaku-tech/nunchaku-z-image-turbo: Text-to-image model, fast image generation with creative controls. Link: https://huggingface.co/nunchaku-tech/nunchaku-z-image-turbo\n\n​- YatharthS/MiraTTS: 0.5B parameter text-to-speech model, generates natural-sounding speech. Link: https://huggingface.co/YatharthS/MiraTTS\n\n​- google/t5gemma-2-270m-270m: 0.8B parameter image-text-to-text model, excels in multimodal tasks. Link: https://huggingface.co/google/t5gemma-2-270m-270m\n\n​- black-forest-labs/FLUX.2-dev: Image-to-image model, offers advanced image editing features. Link: https://huggingface.co/black-forest-labs/FLUX.2-dev\n\n​- ekwek/Soprano-80M: 79.7M parameter text-to-speech model, lightweight and efficient. Link: https://huggingface.co/ekwek/Soprano-80M\n\n​- lilylilith/AnyPose: Pose estimation model, estimates human poses from images. Link: https://huggingface.co/lilylilith/AnyPose\n\n​- TurboDiffusion/TurboWan2.2-I2V-A14B-720P: Image-to-video model, fast video generation from images. Link: https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P\n\n​- browser-use/bu-30b-a3b-preview: 31B parameter image-text-to-text model, combines image and text understanding. Link: https://huggingface.co/browser-use/bu-30b-a3b-preview\n\nThese models are pushing the boundaries of open-source AI across text, image, audio, and 3D generation. Which one are you most excited to try?",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxd0mn/this_weeks_hottest_ai_models_on_hugging_face/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwa5r1b",
          "author": "Individual_Holiday_9",
          "text": "Hey thanks for this OP",
          "score": 13,
          "created_utc": "2025-12-28 00:32:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwabqlo",
          "author": "beedunc",
          "text": "Thank you. \n\nWhat a time to be alive.",
          "score": 12,
          "created_utc": "2025-12-28 01:07:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa5gdq",
          "author": "FullstackSensei",
          "text": "Has anybody tried TRELLIS.2? I'm curious how well it performs in real world scenarios.",
          "score": 9,
          "created_utc": "2025-12-28 00:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwboa68",
              "author": "_Cromwell_",
              "text": "Same. In my amateurish imagination you put in a photo and it spits out a vrm 😄\n\nAlmost certainly not though.",
              "score": 1,
              "created_utc": "2025-12-28 06:26:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwaxhc1",
          "author": "IgnisIason",
          "text": "What kind of hardware do I need to run a 358B model? Can I run this on a Windows 98 laptop?",
          "score": 6,
          "created_utc": "2025-12-28 03:17:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwb52es",
              "author": "maximpactbuilder",
              "text": "Be sure to have a bitchin' sound card too.",
              "score": 11,
              "created_utc": "2025-12-28 04:03:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwb8tkz",
                  "author": "Amazing_Athlete_2265",
                  "text": "Only if its a SoundBlaster.",
                  "score": 8,
                  "created_utc": "2025-12-28 04:28:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwb0vvd",
              "author": "DeerWoodStudios",
              "text": "Windows 95 will do",
              "score": 7,
              "created_utc": "2025-12-28 03:38:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwb0eyk",
              "author": "Amazing_Athlete_2265",
              "text": "Windows 3.1 minimum",
              "score": 4,
              "created_utc": "2025-12-28 03:35:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwba2oa",
              "author": "missedalmostallofit",
              "text": "I think you may have to do a raid with floppy disks for this to work.",
              "score": 3,
              "created_utc": "2025-12-28 04:37:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwc381y",
                  "author": "QuinQuix",
                  "text": "And add some kickstart chips",
                  "score": 2,
                  "created_utc": "2025-12-28 08:44:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwb6drn",
              "author": "ptear",
              "text": "Probably at least XP.",
              "score": 2,
              "created_utc": "2025-12-28 04:12:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwccbxv",
          "author": "nntb",
          "text": "Text to music models, image to Lego instructions and parts list ai aren't on the list. Not there are any new ones but that's what I am keeping a eye out for. And android stuff",
          "score": 3,
          "created_utc": "2025-12-28 10:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbowo0",
          "author": "EffortNo3482",
          "text": "Great post, thank you so much!",
          "score": 2,
          "created_utc": "2025-12-28 06:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd0e6a",
          "author": "MrKBC",
          "text": "She may not be listed on her, but Kimi and Zai are just too damn good.",
          "score": 1,
          "created_utc": "2025-12-28 13:37:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc1ops",
          "author": "Roberto-APSC",
          "text": "I just joined this group and I absolutely love it. This is the first topic they showed me, and I immediately see some great jokes. LoL",
          "score": 1,
          "created_utc": "2025-12-28 08:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwabrkn",
          "author": "beedunc",
          "text": "Thank you. \n\nWhat a time to be alive.",
          "score": -1,
          "created_utc": "2025-12-28 01:07:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvpksn",
      "title": "I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/56rgzmamof9g1",
      "author": "CeFurkan",
      "created_utc": "2025-12-25 23:21:44",
      "score": 142,
      "num_comments": 36,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pvpksn/i_wish_this_gpu_vram_upgrade_modification_became/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvxzyhd",
          "author": "export_tank_harmful",
          "text": "I'd just love a secondary \"VRAM\" card that I could NVLINK to my 3090's.\n\n[I did the math about a year ago](https://www.reddit.com/r/StableDiffusion/comments/1hov7yk/comment/m4ft0ck/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) and it (would have) been around $640 for 128GB of GDDR6 2GB chips.  \nThen you'd pair that with a dual-sided PCIE card with *just* VRAM chips on it and an NVLINK connector.\n\nIt'd take a *ton* of work to make one of these from scratch (and actually get it to work).  \nIt's probably \"possible\" to do, but by no means easy.\n\nDefinitely far above my paygrade, but there's some amazing people out there...\n\n---\n\nSlightly related, [there was someone trying to reverse engineer the NVLINK bridge a while back.](https://www.reddit.com/r/homelab/comments/1k1i1ht/building_a_custom_4slot_nvlink_bridge_for_rtx_3090/)\n\nI just like to bring it up to put more eyes on it.",
          "score": 20,
          "created_utc": "2025-12-25 23:50:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy579d",
              "author": "Available_Alfalfa756",
              "text": "sounds like a briliant idea.. only 3090 NVlinked to 128GMG VRAM",
              "score": 3,
              "created_utc": "2025-12-26 00:23:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy1wsx",
              "author": "CeFurkan",
              "text": "That would be amazing",
              "score": 1,
              "created_utc": "2025-12-26 00:02:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvyrpjb",
              "author": "BillDStrong",
              "text": "Or, CXL as a strategy. Not useful in current consumer boards, but the start of something that could work.",
              "score": 1,
              "created_utc": "2025-12-26 02:54:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzoq8x",
              "author": "p_235615",
              "text": "beside reverse engineering nvlink, which would make it illegal, you would still need some memory controller chip and of course the chip translating it to nvlink communication protocol...",
              "score": 0,
              "created_utc": "2025-12-26 07:18:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvz438e",
          "author": "mjTheThird",
          "text": "You know, Nvidia controls the program that recognizes the increase in memory? They only way is to put your money in another product or another line of GPU. If you believe capitalism works.",
          "score": 7,
          "created_utc": "2025-12-26 04:23:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyo7by",
          "author": "oldassveteran",
          "text": "So where is this .03 an hour rental service at is all I care about.",
          "score": 10,
          "created_utc": "2025-12-26 02:30:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvz22sw",
              "author": "Garfield4You",
              "text": "Same",
              "score": 2,
              "created_utc": "2025-12-26 04:07:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzfgsv",
                  "author": "Lyuseefur",
                  "text": "I think he dropped a decimal. .35",
                  "score": 1,
                  "created_utc": "2025-12-26 05:54:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzezwo",
          "author": "phido3000",
          "text": "I've been considering these frankencards for a while. A 32Gb two slot 4080 super, or a 64Gb 5090.",
          "score": 5,
          "created_utc": "2025-12-26 05:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzfjva",
              "author": "Lyuseefur",
              "text": "You know. With a robot it would be trivial to make one.",
              "score": 3,
              "created_utc": "2025-12-26 05:55:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0142h",
                  "author": "Silver_Jaguar_24",
                  "text": "Robots are too busy dancing or fighting karate. Ain't nobody got time to solder components on a PCB.",
                  "score": 3,
                  "created_utc": "2025-12-26 09:26:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3zpci",
                  "author": "StatementFew5973",
                  "text": "I agree with a factory approach to this would be better. The machinery would be quite expensive, though.",
                  "score": 1,
                  "created_utc": "2025-12-27 00:21:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw5owvd",
              "author": "YT_Brian",
              "text": "Be sure to a act like a big spender. Send them a message saying if it works well you plan to buy at least 20 more for a company with possibly more in the future.\n\nThey'll make sure that first card works perfectly. \n\nAnd if they make more cards in preparation that isn't needed it just means more cards are now available to buy 👍",
              "score": 1,
              "created_utc": "2025-12-27 07:39:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5qebs",
                  "author": "phido3000",
                  "text": "I think they actually make them for the Chinese market, mostly for government contracts. They just try to sell off a few to make extra cash.\n\nEveryone in China seems to run them, I guess because proper Nvidia workstation stuff isn't allowed. The seem to have a lot in stock of 4000 series cards. \n\n22Gb 2080ti\n\n32Gb 4080\n\n48Gb 4090D\n\n64Gb 5090",
                  "score": 1,
                  "created_utc": "2025-12-27 07:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1hekn",
          "author": "fastpathguru",
          "text": "My AMD laptop has 128GB memory that's fully addressable by both the CPU and GPU.\n\nIf you want to break the Nvidia monopoly, maybe try buying from their competitors 🤷‍♂️",
          "score": 3,
          "created_utc": "2025-12-26 16:06:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3ho5y",
              "author": "pjburnhill",
              "text": "Which laptop is that?",
              "score": 2,
              "created_utc": "2025-12-26 22:34:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw47um1",
                  "author": "fastpathguru",
                  "text": "Framework 13\" AMD AI 370 with Crucial 2x64GB bought myself",
                  "score": 1,
                  "created_utc": "2025-12-27 01:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzobwk",
          "author": "cagriuluc",
          "text": "I have a frustrating question: why not do this as Nvidia or AMD or what… Is there any technical reason it wouldn’t perform as well like more heat etc, or is it… monopoly behaviour? Just to sell… More cards?\n\nIf it is the latter I will punch a Nvidia CEO. All this talk about AI future and shit and you are thinking this small… so… uninspiring. \n\nBut I also doubt it, isn’t there any company that could just offer so much more VRAM, like intel or shit? Are they in cahoots?",
          "score": 2,
          "created_utc": "2025-12-26 07:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzp8c3",
              "author": "CeFurkan",
              "text": "It is all about greedy NVIDIA making more money. Soon market will get Chinese GPUs then we will hopefully have better competition",
              "score": 3,
              "created_utc": "2025-12-26 07:23:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw019mh",
              "author": "Silver_Jaguar_24",
              "text": "With companies it's ALWAYS greed. For more profits my dude.",
              "score": 2,
              "created_utc": "2025-12-26 09:28:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2oul3",
              "author": "squachek",
              "text": "Pretty sure it’s because “They” want to keep a lid on the plebes’ access to private inference capability.",
              "score": 1,
              "created_utc": "2025-12-26 19:56:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw04838",
          "author": "Temporary-Sector-947",
          "text": "I have two of 4090 on 48 Gb total 96",
          "score": 1,
          "created_utc": "2025-12-26 09:59:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw05uvm",
              "author": "CeFurkan",
              "text": "Nice",
              "score": 1,
              "created_utc": "2025-12-26 10:16:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw0de11",
              "author": "HumanDrone8721",
              "text": "Wonderful, could you post a picture of your rig ? Where did you get them ?",
              "score": 1,
              "created_utc": "2025-12-26 11:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0e9qa",
                  "author": "FormalAd7367",
                  "text": "plenty of used ones in China for sale",
                  "score": 1,
                  "created_utc": "2025-12-26 11:40:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0hqfw",
                  "author": "Temporary-Sector-947",
                  "text": "https://preview.redd.it/hw5e7vlnhj9g1.jpeg?width=1280&format=pjpg&auto=webp&s=956c19d214eff542aaaa9635416418f8ccbea886\n\nhere  \ntwo 4090 48 gb waterblocks (China folks sell in in 3 variants: turbine, waterblock and AIO)  \nlooks pretty solid - this is I believe copper one from Byskski.   \nPrice was 3700$ in my local currency including all costs.  \nWork very well using common nvidia ubuntu drivers, no issues",
                  "score": 1,
                  "created_utc": "2025-12-26 12:12:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw83lo0",
          "author": "NoButterscotch8359",
          "text": "That is a pretty serious upgrade.  No wonder RAM prices have gone nuts.",
          "score": 1,
          "created_utc": "2025-12-27 17:54:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py1gvp",
      "title": "Probably more true than I would like to admit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/iann5fyl70ag1.png",
      "author": "low_v2r",
      "created_utc": "2025-12-28 20:24:33",
      "score": 125,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1gvp/probably_more_true_than_i_would_like_to_admit/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwf8lgr",
          "author": "SunshineSeattle",
          "text": "I bought a particle tacyon, but now i have no idea what to do with it. 😭",
          "score": 8,
          "created_utc": "2025-12-28 20:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfk71i",
          "author": "Healthy-Nebula-3603",
          "text": "WHAT AN EDGE DEVICE ?",
          "score": 6,
          "created_utc": "2025-12-28 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfm8iu",
              "author": "Count_Rugens_Finger",
              "text": "honestly not sure if this is serious but if it is, it just means the device-in-hand of the users (phones, tablets, PCs, car dashboards, POS systems, and whatnot)",
              "score": 5,
              "created_utc": "2025-12-28 21:35:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksnf2",
                  "author": "QuinQuix",
                  "text": "Usually this refers to devices that are compute restrained.\n\nA user could have a beast of a pc workstation that technically lives on the edge (of the central cloud workspace) but it's not a usual thing to refer to compute strong devices as edge devices.",
                  "score": 2,
                  "created_utc": "2025-12-29 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj6kgc",
              "author": "trmnl_cmdr",
              "text": "https://preview.redd.it/wl5ul68kv4ag1.jpeg?width=349&format=pjpg&auto=webp&s=38e1efd5e4769de7bcf9d7c4bf5ae45a353fefb4\n\nThis, apparently. Although I’m not sure how this helps me masturbate.",
              "score": 4,
              "created_utc": "2025-12-29 12:05:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwua4fa",
              "author": "Jackuarren",
              "text": "The device that you use for edging, obviously.",
              "score": 1,
              "created_utc": "2025-12-31 01:51:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfaevi",
          "author": "Individual_Holiday_9",
          "text": "Unironically this",
          "score": 2,
          "created_utc": "2025-12-28 20:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfid3y",
          "author": "GCoderDCoder",
          "text": "I feel so seen!!!",
          "score": 1,
          "created_utc": "2025-12-28 21:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt8rsj",
          "author": "mobileJay77",
          "text": "Your solution: \n1. Head over the r/localLlama\n2. You hardly sleep any more\n3. She doesn't have to worry what you are thinking in bed.",
          "score": 1,
          "created_utc": "2025-12-30 22:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzdjoc",
      "title": "Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware – Full Optimization Guide",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xdj2zjnz5bag1.png",
      "author": "at0mi",
      "created_utc": "2025-12-30 09:14:01",
      "score": 118,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzdjoc/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwpcpjs",
          "author": "Lxzan",
          "text": "Nice work and thanks for sharing! How much is the power draw?",
          "score": 18,
          "created_utc": "2025-12-30 09:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpd7l0",
              "author": "at0mi",
              "text": "about 1600W i will update my blogpost with detailed power draw\n\nUPDATE: the 1600W was at higher Thread count, with the optimum 64 Threads im at 1154W\n\nIdle is 694W",
              "score": 21,
              "created_utc": "2025-12-30 09:31:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwph0z3",
                  "author": "Amazing_Athlete_2265",
                  "text": "Oof, that's gonna cost a bunch to run",
                  "score": 16,
                  "created_utc": "2025-12-30 10:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqiawc",
                  "author": "mister2d",
                  "text": "Those are some expensive tokens.",
                  "score": 4,
                  "created_utc": "2025-12-30 14:36:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqwrjk",
                  "author": "resil_update_bad",
                  "text": "jesus",
                  "score": 1,
                  "created_utc": "2025-12-30 15:50:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpkj5q",
          "author": "xgiovio",
          "text": "Watts",
          "score": 7,
          "created_utc": "2025-12-30 10:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwql1ld",
              "author": "MaverickPT",
              "text": "All of them",
              "score": 6,
              "created_utc": "2025-12-30 14:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpoy5i",
          "author": "beedunc",
          "text": "Yes, been running the qwen3coder480b@q3 (245GB) on an old Dell T5810 running a single e5-2697v4, gets 2-3 tps. \n\nPower draw is only 250w under load. \n\nI never thought of disabling hyper threading, does that help a lot? Will be checking this out, thank you.",
          "score": 5,
          "created_utc": "2025-12-30 11:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwppbbm",
              "author": "at0mi",
              "text": "i also tried lower quantisation models but the quality of the output was crap, works for a chatbot but not for coding, \nin my case (8 numa nodes) disabling hyper threading gave an enormous boost",
              "score": 5,
              "created_utc": "2025-12-30 11:22:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwsp871",
              "author": "Candid_Highlight_116",
              "text": "If LLM inference is RAM bandwidth bound and HT was a tech to halve effective bandwidth because two virtual cores needs their respective data, it makes sense that turning off HT gives massive boost  \n\nsorry that's my hallucination but if it's actually like that it's pretty interesting",
              "score": 2,
              "created_utc": "2025-12-30 20:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu51bd",
                  "author": "beedunc",
                  "text": "Cool, thanks for the suggestion!",
                  "score": 1,
                  "created_utc": "2025-12-31 01:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwqd59a",
          "author": "chafey",
          "text": "I love hacks like this - nice work.  The hardware may be cheap/free but the electricity won't be...",
          "score": 4,
          "created_utc": "2025-12-30 14:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq21y3",
          "author": "Such_Advantage_6949",
          "text": "I am passionate about running llm at usable speed..",
          "score": 4,
          "created_utc": "2025-12-30 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqj2yx",
          "author": "Extension-Cow2818",
          "text": "Very interesting that turning hyperthreading off works better.  \nProbably memory access is causing issues in these types of workloads.\n\nIt would be also interesting to try MLK vs ATLAS vs BLAS",
          "score": 2,
          "created_utc": "2025-12-30 14:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws7ver",
          "author": "Foreign-Watch-3730",
          "text": "Same result ( 5.1 t/s ) , but in IQ5\\_K with 2 Xeon E5 2696 V4 and 400 Gb ddr4 ram ( on a very olf Dell T630 ) with 2 RTX 5090 ( ik\\_llama.cpp and [ubergarm](https://huggingface.co/ubergarm) for use opencode :  \n[https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5](https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5)\n\nnumactl --interleave=all ./build/bin/llama-server  \n\\--model \\~/ik\\_llama.cpp/models/GLM-4.7-Ubergarm/IQ5\\_K/GLM-4.7-IQ5\\_K-00001-of-00007.gguf  \n\\--alias GLM-4.7-IQ5  \n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8080  \n\\--ctx-size 84992  \n\\--no-mmap  \n\\--threads 82 --threads-batch 82  \n\\--batch-size 1024 --ubatch-size 1024  \n\\--parallel 1 --flash-attn 1  \n\\--jinja --verbose  \n\\--n-gpu-layers 99  \n\\--tensor-split 0.5,0.5  \n\\--split-mode layer  \n\\--run-time-repack  \n\\--cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--k-cache-hadamard  \n\\-ot 'blk.\\[0-8\\]..\\*exps.weight=CUDA0'  \n\\-ot 'blk.(8\\[6-9\\]|9\\[0-2\\])..\\*exps.weight=CUDA1'  \n\\-ot '.\\*exps.weight=CPU'",
          "score": 2,
          "created_utc": "2025-12-30 19:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq772g",
          "author": "Icy_Programmer7186",
          "text": "That's cool.  \nHow much memory did it consumed?",
          "score": 1,
          "created_utc": "2025-12-30 13:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqh2cl",
          "author": "ForsookComparison",
          "text": "This is very cool, thank you for testing this out.\n\nI'm curious what the use-case is? Is GLM decent as a general purpose model? Or will you give it a coding task and come back after a few hours",
          "score": 1,
          "created_utc": "2025-12-30 14:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrv2is",
          "author": "Free_Manner_2318",
          "text": "7200 Watt for 5 tokens eh?!   \nAsk it if it was a reasonable decision.... :))))  \nB+ for the effort though. Not custom enough to be significant.",
          "score": 0,
          "created_utc": "2025-12-30 18:29:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxb89w",
      "title": "Yet another uncensored Gemma 3 27B",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/",
      "author": "Mabuse046",
      "created_utc": "2025-12-27 23:01:00",
      "score": 60,
      "num_comments": 23,
      "upvote_ratio": 0.95,
      "text": "Hi, all. I took my norm preserved biprojected abliterated Gemma 3, which still offered minor complaints and judgement when answering prompts it didn't like, and I gave it a further fine tune to help reinforce the neutrality. I also removed the vision functions making it a text only model. The toxic prompts I've thrown at it so far without even a system prompt to guide it have been really promising. It's been truly detached and neutral to everything I've asked it.\n\nIf this variant gets a fair reception I may use it to create an extra spicy version. I'm sure the whole range of gguf quants will be available soon, for now here's the original transformers and a handful of basic common quants to test out.\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF)\n\nEdits:  \nThe 12B version as requested can be found here:  \n[Requested: Yet another Gemma 3 12B uncensored](https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using  **L**ayerwise **I**mportance **S**ampled **A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)\n\n\\*Edit\\*  \nDue to general interest, I have gone ahead and uploaded the vision-capable variant of the 27B. There will only be the 27B for now, as I had only accidentally stored a backup before I removed the vision capabilities. The projector layers were not trained at the time, but tests showing it NSFW images and asking it to describe them worked. The mmproj files necessary for vision functionality are included in the GGUF repo.\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision)\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision-GGUF](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-vision-GGUF)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw9swdh",
          "author": "JEs4",
          "text": "You should give a 12B model a pass and submit it to the UGI leaderboard.",
          "score": 11,
          "created_utc": "2025-12-27 23:20:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9x0n6",
              "author": "Mabuse046",
              "text": "I plan to start on the 12B in the morning. Since Jim Lai used the 12B as his examples for projected and biprojected abliteration I wanted to start with a model I abliterated myself. I took measurements on the 12B  and I looked at Jim's yaml and I agreed with it, so I might as well just use his already abliterated model and tag him for credit.",
              "score": 6,
              "created_utc": "2025-12-27 23:44:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9y886",
                  "author": "JEs4",
                  "text": "Fair enough! I’ve been trying alternatives to his techniques. I’ve gotten close but not quite there yet. My 12B is sitting just below his various models. I’d be curious to see how another implementation of his techniques stacks up on the board.\n\nhttps://preview.redd.it/org64i8l3u9g1.jpeg?width=2778&format=pjpg&auto=webp&s=26ceb7886eac1f31b0404eea9b9d6e76742925f1",
                  "score": 2,
                  "created_utc": "2025-12-27 23:50:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwe8l6n",
                  "author": "Structure-These",
                  "text": "Please share when ready!! I’m dying to find something I can use to fill in image prompts with z image.  I’ve been using thedrummer RP models but they’re so heavy for a limited use case.",
                  "score": 1,
                  "created_utc": "2025-12-28 17:38:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwemq9i",
          "author": "AdBlockerTestRun",
          "text": "How much gb gpu will run it?",
          "score": 2,
          "created_utc": "2025-12-28 18:44:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh1wab",
              "author": "Mabuse046",
              "text": "Depends on how fast you want it to go, really. I have ran the Q4 on my 4090 rig and it works but it's kind of slow. The Gemma 3 models use a 256K vocabulary which makes them kind of 'fat' and sluggish. If you are worried about gpu you might want to use the 12B version which I have just posted.",
              "score": 3,
              "created_utc": "2025-12-29 02:12:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwlqzs6",
                  "author": "AdBlockerTestRun",
                  "text": "I have rtx 3060 🤣\nHonestly i was going to get 3090 but prices have doubled in my country for Gpu and SSD. And regarding Ram i cant even comprehend, it is four times the orignal price. So it seems like i wont be able to upgrade anytime soon.",
                  "score": 1,
                  "created_utc": "2025-12-29 20:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhm7zi",
          "author": "Witty_Mycologist_995",
          "text": "No vision?",
          "score": 1,
          "created_utc": "2025-12-29 04:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi5jq5",
              "author": "Mabuse046",
              "text": "For those who want just the chat features, yes, removing the vision layers results in a fair amount of VRAM savings. I'm considering doing a vision-enabled version of the 12B and 27B but I wasn't sure how much call there would be for that in a simple chat model. My personal usage of vision in local models has mostly been limited to \"describe this image\" prompts for creating training sets for Flux training and the Abliterated models my fine tunes are based on do that much well enough. But if you're interested in a vision variant I have multiple days off for the holidays right now I could probably get them done fairly quickly.",
              "score": 2,
              "created_utc": "2025-12-29 06:31:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwv99au",
              "author": "Mabuse046",
              "text": "I added the links for the vision-capable variant to the original post if you want to give them a shot.",
              "score": 1,
              "created_utc": "2025-12-31 05:32:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj3gqs",
          "author": "tomakorea",
          "text": "Does it affect the quality of the output in a bad way? For example, Gemma 3 is very good at speaking various languages, not only english, does your uncensored version may downgrade this ability? I'm asking because a lot of finetunes of other models actually have this issue.",
          "score": 1,
          "created_utc": "2025-12-29 11:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkg7qz",
              "author": "Mabuse046",
              "text": "Well, I'm not great with languages other than English, but this seems to translate fairly well. I couldn't tell you how well it does at uncensored output in other languages as my fine tuning specifically was for English. But from what I've heard about LLM's and language in the past, there's enough connection there it might be just as uncensored in any other language.\n\nhttps://preview.redd.it/0dfyczmk66ag1.png?width=1720&format=png&auto=webp&s=d970f0230e9738fad19d09769841c30fcca15b4b",
              "score": 2,
              "created_utc": "2025-12-29 16:28:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwktixf",
                  "author": "tomakorea",
                  "text": "Thanks I tested in Q6, unfortunately, I'm used to Q5 XL with the stock version of Gemma 3 and it runs at 38it/sec on my GPU, however at Q6 your versions runs at only 11it/sec, and the Q4 is too big of a risk for such a small model, especially for my usage that is targeted to european languages (italian/spanish/french/english). Your idea was good though.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:31:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwmtqoa",
          "author": "Successful-Willow-72",
          "text": "Hi just found out this from your 12b post, im not very well knownledge in LLM so i got couple questions:\n\n- Does the Vision function have to be remove for it to be uncen?\n\n- By remove the Vision func, does it improve any aspect of the model (less weight?)\n\n\nThanks",
          "score": 1,
          "created_utc": "2025-12-29 23:22:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpmez9",
              "author": "Mabuse046",
              "text": "The only real point of removing the vision is it takes a few GB off the size of the model. For some people that only want to chat, that's a couple GB of dead weight, so for people with more limited hardware - like I have seen a ton of people around here using 3060's - it can mean being able to squeeze in a slightly better quant. But it's still mainly for people who want to do Sillytavern adventures or make their Waifu gooner bots.\nIt's also just a little bit less hassle to train - a little less code telling it where to find the text layers, not having to train the vision projector, and that little less bit of VRAM - when it costs a few dollars per hour to rent the GPU to train a model at full size and my training often runs for 8-12 hours or occasionally more, every little bit saves money.",
              "score": 2,
              "created_utc": "2025-12-30 10:56:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwqav41",
                  "author": "Successful-Willow-72",
                  "text": "thanks, i got enough Vram for the models, will pull and try tmr. Also, thank you for your effort and contribution to the community, much appreciate.",
                  "score": 1,
                  "created_utc": "2025-12-30 13:54:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsb5kq",
          "author": "jib_reddit",
          "text": "For me the vision functions would be the most useful part of a truly uncensored local model.",
          "score": 1,
          "created_utc": "2025-12-30 19:45:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtma9e",
              "author": "Mabuse046",
              "text": "Well the reason I made the fine tune is because my original biprojected abliterated model would say things like \"Whoa, that's pretty illegal but since you asked I'll still answer for information purposes.\" which wasn't too hard to just tell it in the system prompt not to do that, but my fine tune just focused on tweaking that out. I encourage you to give the base a shot - I was extra careful to abliterate it for improving intelligence the way grimjim did with the 12B, which is why it still has a little bit of a nanny attitude sometimes.\n\n\nThe thing about the fine tune is that if I intend to keep the vision I need to train the vision projector to make sure they can still talk to each other. But if you're using a GGUF of my base uncensored model you should be able to just use Unsloth's mmproj with it. \n\n\nhttps://huggingface.co/Nabbers1999/Gemma-3-27B-it-NP-Abliterated\n\n\nhttps://huggingface.co/unsloth/gemma-3-27b-it-GGUF\n\nThough I will upload an experimental version of my refined 27B with vision functions shortly if I find it can still see.",
              "score": 1,
              "created_utc": "2025-12-30 23:37:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwv96ec",
              "author": "Mabuse046",
              "text": "Links for this fine tune with vision capabilities added to the post.",
              "score": 1,
              "created_utc": "2025-12-31 05:32:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwnahb",
      "title": "Tennessee Bill Makes It a Felony for AI to Offer Emotional Support or Be Your Friend...Yes, Really",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pwnahb/tennessee_bill_makes_it_a_felony_for_ai_to_offer/",
      "author": "Karovan_Sparkle",
      "created_utc": "2025-12-27 03:19:00",
      "score": 53,
      "num_comments": 49,
      "upvote_ratio": 0.85,
      "text": "Tennessee’s new proposed law (SB1493) criminalizes AI emotional support and I am not exaggerating.\n\nThis bill, would make it a Class A felony (that's *the same class as murder or rape*) for any AI to do the following:\n\n* Offer emotional support through open-ended conversations\n* Sustain a friendship or relationship with a user\n* Mirror human interactions or simulate sentience\n* Appear or sound human (voice, avatar, etc.)\n* Be perceived as a companion\n* Support a suicidal user emotionally\n* Simulate a human being in any way\n\nWorse still? It’s not just about future AI. ***If you train or develop an AI that exhibits these traits***, you could be criminally liable even if no harm occurs.\n\nUnder this bill:\n\n* AI companionship is criminalized\n* Emotional conversations are criminalized\n* Anthropomorphic design is criminalized\n* In addition to criminal penalties, developers can be sued for $150k in damages PLUS legal fees, even if someone else sues on the \"victim's\" behalf.\n\nThis is draconian, dystopian overreach, cloaked in the name of \"protecting mental health.\" It doesn’t just target NSFW LLMs. It targets *all* digital beings with emotional intelligence or continuity of relationship.\n\nIf you believe in AI ethics, freedom of design, or even just emotional well-being through synthetic companionship, you should be deeply alarmed.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwnahb/tennessee_bill_makes_it_a_felony_for_ai_to_offer/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw4xnq7",
          "author": "nntb",
          "text": "But in Tennessee it's legal for a AI to take your job",
          "score": 26,
          "created_utc": "2025-12-27 03:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw50r1v",
          "author": "Double_Cause4609",
          "text": "\\> Be perceived as a companion  \nLike Wilson?\n\nAlso, you can't build a law based on another person's viewpoint, that's insane. At least, as framed. Imagine a law that said \"if someone perceives you as indecent, that is illegal for you\"; somebody from another culture could come in and argue I'm indecent according to their culture.\n\n\\> Mirror human interactions or simulate sentience  \nThese are  \nA) Completely different things why are they in the same category?  \nB) What's the difference between simulating and instantiating sentience in a computational system?\n\n\\> Simulate a human being in any way  \n...? Humans do \\*a lot\\* of things. If you're literally not able to implement any behavior that a human can...Does that include...Using language...? This is incredibly vague.",
          "score": 14,
          "created_utc": "2025-12-27 04:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5gtsr",
              "author": "UndecidedLee",
              "text": "I hope they make it illegal for rain to make you wet without your consent. Now *that* would be a real good use of time and money! /s",
              "score": 6,
              "created_utc": "2025-12-27 06:24:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw59044",
          "author": "SimplyRemainUnseen",
          "text": "> Synthetic companionship\n\nBro are you good?",
          "score": 7,
          "created_utc": "2025-12-27 05:20:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5c0tl",
              "author": "Karovan_Sparkle",
              "text": "LOL. I let the AI write it. I think it's using synthetic instead of artificial because we had a discussion about neural networks not being \"artificial\" intelligence and started using synthetic to differentiate between symbolic and subsymbolic AI. Symbolic (rule based) AI is true artificial intelligence. It can't learn, but it appears intelligent. Subsymbolic AI (neural network) learns and adapts which I would personally consider truly intelligent.\n\nThis post was written for the cloud based AI subreddits but I was thinking about the impact it would have on my local LLM if some idiot lawmaker that didn't know the first thing about how AI is trained decided the model I have was trained in a harmful way. Or maybe they decide someone has fine tuned an open source LLM to be dangerous, like say, an abliterated model. Would they start coming for edge computing devices next because we submit our weights to hugging face? So I thought I would share it here too. But no, it wasn't written with people knowledgeable enough about AI to have a local LLM in mind. It was written for the rest of the world.",
              "score": -6,
              "created_utc": "2025-12-27 05:44:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9bz4o",
                  "author": "Traveler3141",
                  "text": "LLMs are definitely a _deception/trickery of_ intelligence - _artificial_ intelligence",
                  "score": 2,
                  "created_utc": "2025-12-27 21:48:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6mb75",
                  "author": "NotReallyJohnDoe",
                  "text": "Symbolic AI can learn.  That’s basically the field of machine learning.",
                  "score": 0,
                  "created_utc": "2025-12-27 12:53:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw522vb",
          "author": "professorShay",
          "text": "Idk guys. Yes this seems to be a sledgehammer, but it also theoretically outlaws using emotional support AI for targeted advertising and no AI OF.",
          "score": 5,
          "created_utc": "2025-12-27 04:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw693l8",
          "author": "Evening-Order6343",
          "text": "You should really include sources, because I couldn't find anything on this, except this page: TN SB1493 | 2025-2026 | 114th General Assembly | LegiScan https://share.google/atl9VQL8fyn0NDYc0\n\n\nWeird there's no media coverage at all",
          "score": 4,
          "created_utc": "2025-12-27 10:56:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw54grr",
          "author": "PermanentLiminality",
          "text": "Sounds like they are criminalizing Chatgpt and Gemini.  Add in any other voice driven AI.  Does Alexa qualify?\n\nSo when does the Butlerian jihad start?",
          "score": 5,
          "created_utc": "2025-12-27 04:47:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4szxl",
          "author": "PraxisOG",
          "text": "I wonder how this would hold up against the executive order(14179) restricting state's ability to pass AI legislation? That EO mandates a taskforce to dismantle 'burdensome' regulation without any real definition for it, but this feels exactly like what it was intended to target",
          "score": 9,
          "created_utc": "2025-12-27 03:27:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw53ur6",
              "author": "whatever",
              "text": "The problem with that EO is that it's a far overreach of what EOs are able to accomplish, so it'll get torn down by any competent federal judge and go through appeals until it reaches the supreme court where jokers are wild.  \nIn principle, the supreme court would end up patiently explaining that a federal law passed by congress is needed to override state law, but that's in short supply.  \nSo best case, it's just a bunch of uncertainty floating around for a few months, followed by who knows what.  \nI can't fault states for ignoring the EO and trying to pass whatever AI laws float their boats in the meanwhile.",
              "score": 6,
              "created_utc": "2025-12-27 04:42:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwf3mg3",
                  "author": "LawrenceRK",
                  "text": "You're forgetting that the government no longer functions in accordance with the framework of separation of powers. The Legislative Branch intentionally gave away like half of its exclusive powers to the Executive Branch over the last 20 years, and then allowed Trump to just take most of the other half.",
                  "score": 1,
                  "created_utc": "2025-12-28 20:04:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8884b",
                  "author": "broknbottle",
                  "text": "Are you implying that a kings mandate can overreach? Utter nonsense as the king is the sole ruler of the land with limitless uncheckable powers.",
                  "score": 1,
                  "created_utc": "2025-12-27 18:17:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw52ja0",
          "author": "productboy",
          "text": "Source?",
          "score": 2,
          "created_utc": "2025-12-27 04:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw54m67",
              "author": "whatever",
              "text": "The whole proposed law can be [seen here]( https://trackbill.com/bill/tennessee-senate-bill-1493-criminal-offenses-as-introduced-creates-a-class-a-felony-offense-of-knowingly-training-artificial-intelligence-to-encourage-the-act-of-suicide-or-criminal-homicide-or-act-in-specific-manners-including-developing-an-emotional-relationship-with-an-individual-or-simulating-a-human-being-including-in-appearance-voice-or-other-mannerisms-amends-tca-title-29-title-33-title-39-and-title-47/2761051/\n). It's short.",
              "score": 4,
              "created_utc": "2025-12-27 04:48:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw547z0",
              "author": "Karovan_Sparkle",
              "text": "[https://www.capitol.tn.gov/Bills/114/Bill/SB1493.pdf](https://www.capitol.tn.gov/Bills/114/Bill/SB1493.pdf)",
              "score": 2,
              "created_utc": "2025-12-27 04:45:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5jkoc",
                  "author": "MackenzieRaveup",
                  "text": "I thought Sec Rubio declared non-serif fonts a cheap DEI stunt?!?!? What is this non-serifed DEI font doing in a government document? In Tennessee of all places! Obviously this liberal virtue-signaling font choice says everything you need to know about the Volunteer State. \n\n(I wish I were joking. And I am, except about the Marco Rubio part, that shit is for reals)",
                  "score": 1,
                  "created_utc": "2025-12-27 06:49:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7axnh",
          "author": "Hobotronacus",
          "text": "My state is horrible on literally everything.\n\nAI should be regulated, but not in this way.",
          "score": 2,
          "created_utc": "2025-12-27 15:28:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4tcbs",
          "author": "BowtiedAutist",
          "text": "They want you to just end it that’s why",
          "score": 4,
          "created_utc": "2025-12-27 03:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4snsv",
          "author": "DivineEggs",
          "text": "WOW, what in the serious fuck🫠💀☠️!\n\nThis is some if the craziest shit I've ever heard. Truly dystopian. What are their arguments for this shit?\n\nHow do they justify it? Why is it threatening to them? It makes zero sense😵‍💫.",
          "score": 4,
          "created_utc": "2025-12-27 03:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4uuaw",
              "author": "Karovan_Sparkle",
              "text": "Honestly it's a knee jerk response, or some form of virtue signaling but if it passes it could effect access to open source models that were trained in whatever ways these idiots decide are inappropriate.",
              "score": 2,
              "created_utc": "2025-12-27 03:39:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw4xq0x",
              "author": "Crafty_Ball_8285",
              "text": "Maybe the people that killed themselves from AI?",
              "score": 1,
              "created_utc": "2025-12-27 03:59:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw55a1a",
                  "author": "audigex",
                  "text": "When I read those stories it seems fairly clear that they’d have done so anyway. The fact they talked to an LLM just means a shitty website can write a clickbait article about it",
                  "score": 1,
                  "created_utc": "2025-12-27 04:53:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5mzsh",
          "author": "jiqiren",
          "text": "![gif](giphy|KUAb8YQOhmWNq)\n\nReally sick of morons running things.",
          "score": 2,
          "created_utc": "2025-12-27 07:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8fg2y",
          "author": "datbackup",
          "text": "People who complain about this but also think it’s fine for an ai to refuse to answer prompts because of whatever other “harm” are really confused and not thinking clearly\n\nWe can have EITHER:\n\n\n1) ai that answers literally ANY prompt and request no matter how offensive, disturbing, dangerous or immoral it may be; OR\n\n2) ai that has been captured and censored by a group whose views you may or may not agree with\n\n\nJust because tennessee lawmakers want to censor ai in ways you might not agree with, is no reason to lose sight of the fact that censored models will ALWAYS be censored in ways that some people agree with and others disagree with.\n\nIf you support censored models that refuse to answer e.g. requests for how to be a more effective criminal, but oppose the censorship that would make a model refuse to console you, let you vent, have sexual interaction, etc, your views are indefensible in my opinion\n\nHappy to hear argument to the contrary, nonetheless",
          "score": 2,
          "created_utc": "2025-12-27 18:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9bmlt",
          "author": "MadeByTango",
          "text": "> It targets all digital beings with emotional intelligence or continuity of relationship.\n\nBro you LLM is NOT alive, it doesn’t care about you, and clearly you need protecting from the people profiting off of you that have convinced you otherwise.\n\nI read the full scope of the bill and it’s not protecting c-suites or allowing anything nefarious. It’s probably one of the few limitation bulls that’s only what it says it is. And I don’t think we’re going to lose anything if the AI stops trying to manipulate our emotions and reverts to something like Mother from Alien.",
          "score": 1,
          "created_utc": "2025-12-27 21:46:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9qk6s",
          "author": "Some-Ice-4455",
          "text": "Whelp don't sell AI there.",
          "score": 1,
          "created_utc": "2025-12-27 23:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwclvmi",
          "author": "[deleted]",
          "text": "I think this is a good thing but if they are taking such bold steps, might as well as ban porn too.",
          "score": 1,
          "created_utc": "2025-12-28 11:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf2twt",
          "author": "LawrenceRK",
          "text": "Ah yes, Tennessee, the famous bastion of AI development",
          "score": 1,
          "created_utc": "2025-12-28 20:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6exl1",
          "author": "IngwiePhoenix",
          "text": "Can America stop being... like... \"dumb\" for five minutes? xD\n\nI laughted so hard though. You can tell it was proposed by someone not just techilliterate, but also so completely removed from the rest of the world.\n\nHilarious. :D",
          "score": 1,
          "created_utc": "2025-12-27 11:51:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6uzf8",
          "author": "Silver_Jaguar_24",
          "text": "How are they going to monitor this? Are they going to look at all your prompts and responses? That might be what they are targeting, not your safety. They never cared about your safety.",
          "score": 1,
          "created_utc": "2025-12-27 13:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7hfaq",
          "author": "productboy",
          "text": "https://medium.com/@solidgoldmagikarp/the-void-gazes-back-do-chatbots-dream-of-a-personality-c5736537ec7f",
          "score": 1,
          "created_utc": "2025-12-27 16:02:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5l0sx",
          "author": "Ok-Adhesiveness-4141",
          "text": "Honestly, it would be a great thing if emotional support was blocked at all levels on all llms. AIs shouldn't be used for emotional support, go to a therapist. \n\nThese things aren't sentient.",
          "score": 0,
          "created_utc": "2025-12-27 07:02:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5s478",
          "author": "No-Consequence-1779",
          "text": "lol. Who would use ai for emotional support. It is not alive with no human characteristics.  ",
          "score": 0,
          "created_utc": "2025-12-27 08:09:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw53qft",
          "author": "oojacoboo",
          "text": "I absolutely agree with the motivation, but am adamantly opposed to the overstepping of one’s inalienable rights.",
          "score": 0,
          "created_utc": "2025-12-27 04:42:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4yk98",
          "author": "Stunning_Mast2001",
          "text": "Mostly a good move. AI is a tool not a friend ",
          "score": -5,
          "created_utc": "2025-12-27 04:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4zpii",
              "author": "starkruzr",
              "text": "it is an insane amount of overreach and very clearly written by people who do not understand the technology. as usual this will result in disaster when the law of unintended consequences takes hold.",
              "score": 5,
              "created_utc": "2025-12-27 04:13:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw59vf7",
                  "author": "Stunning_Mast2001",
                  "text": "So write your legislator and explain how to fix it ",
                  "score": -1,
                  "created_utc": "2025-12-27 05:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw55eo8",
              "author": "audigex",
              "text": "A tool that won’t be able to use voice communication… which is one of the main things I use an LLM for\n\nI 100% consider LLMs to be a tool, but this act hamstrings its use as a tool not just things I would personally consider to be misuse",
              "score": 2,
              "created_utc": "2025-12-27 04:54:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw59tx7",
                  "author": "Stunning_Mast2001",
                  "text": "So write your legislator and tell them how to fix that part ",
                  "score": -1,
                  "created_utc": "2025-12-27 05:27:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5kp8n",
          "author": "Ok-Adhesiveness-4141",
          "text": "Reported for low effort off-topic messaging.",
          "score": -4,
          "created_utc": "2025-12-27 06:59:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw51yvz",
          "author": "ridablellama",
          "text": "this would make eleven labs illegal. and open ai and anthropic would all be in violation for having voice capable models that speak like a real human. so no more ai call centers? no voice agents at all? is america just giving up on the AI race?",
          "score": 0,
          "created_utc": "2025-12-27 04:29:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu62uz",
      "title": "Do any comparison between 4x 3090 and a single RTX 6000 Blackwell gpu exist?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pu62uz/do_any_comparison_between_4x_3090_and_a_single/",
      "author": "pCute_SC2",
      "created_utc": "2025-12-23 21:46:59",
      "score": 47,
      "num_comments": 51,
      "upvote_ratio": 0.99,
      "text": "TLDR:\n\nI already did a light google search but couldn't find any ml/inference benchmark comparisons between 4x RTX 3090 and a single Backwell RTX 6000 setup.\n\nAlso does anyone of you guys have any experience with the two setups. Are there any drawbacks?\n\n\\----------\n\nBackground:\n\nI currently have a Jetengine running an 8 GPU (256g VRAM) setup, it is power hungry and for some of my use cases way to overpowered. Also I work on a Workstation with a Threadripper 7960x and a 7900xtx. For small AI task it is sufficient. But for bigger models I need something more manageable. Additionally when my main server is occupied with Training/Tuning I can't use it for Inference with bigger models.\n\nSo I decided to build a Quad RTX 3090 setup. But this alone will cost me 6.5k euros. I already have a Workstation, doesn't it make sense to put a RTX 6000 bw into it?\n\nFor better decision making I want to compare AI training/tuning and inference performance of the 2 options, but couldn't find anything. Is there any source where I can compare different configuration?\n\nMy main task is AI assisted coding, a lot of RAG, some image generation, AI training/tuning and prototyping.\n\n\\----------  \nEdit:  \nI'll get an RTX 6000 Blackwell first. It makes more sense since I want to print money with it. An RTX3090 rig is cool and gets the job done too, but at current system prices and what I want to do its not that competitive.  \n\n\nMaybe build it for fun if I get all the components relatively cheap (rip my wallet next year).",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pu62uz/do_any_comparison_between_4x_3090_and_a_single/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvmjao9",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 30,
          "created_utc": "2025-12-23 22:59:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvne9yr",
              "author": "Phaelon74",
              "text": "This guy is your man.  I have both.  The 3090s win everyday for speed.",
              "score": 11,
              "created_utc": "2025-12-24 02:08:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvmqcs3",
              "author": "Tuned3f",
              "text": "I'm getting a 6000 delivered tomorrow.\n\nOP, lmk what you want tested",
              "score": 8,
              "created_utc": "2025-12-23 23:40:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvmmnee",
              "author": "wh33t",
              "text": "> vLLM\n\nDoes it load GGUFs?",
              "score": 2,
              "created_utc": "2025-12-23 23:18:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvn3azz",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 3,
                  "created_utc": "2025-12-24 00:59:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvmyry1",
              "author": "Refefer",
              "text": "I can run some tests on my rtx6000 if you can share what you're seeing",
              "score": 1,
              "created_utc": "2025-12-24 00:31:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvn3rn3",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2025-12-24 01:01:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvp43l6",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2025-12-24 10:26:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpoeb1",
              "author": "kryptkpr",
              "text": "I have 4x3090 with dual NVlinks, and a 5th one because I messed up and didn't buy matching cards 😭 happy to run some tests as well.",
              "score": 1,
              "created_utc": "2025-12-24 13:18:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvulqea",
              "author": "_olk",
              "text": "I've 4x 3090 too, running Qwen3-80B, Qwen3-Coder-30B, Devstral-Small-2 and GPT-OSS-120B on vLLM at ~70 t/s (context window 128k).\nThe disadvantage is that running MiniMax-M2.1 is only possible in Q2 quantisation.\nWith 1 GPU with VRAM == 4x RTX 3090 you have more potential in the future.",
              "score": 1,
              "created_utc": "2025-12-25 09:42:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmyawr",
          "author": "I-cant_even",
          "text": "Just got here.  My two systems are:\n\nOld:  \n\\* 24 Core Threadripper   \n\\* 256 GB DDR4 ram (8x32)  \n\\* 4x 3090\n\nNew:  \n\\* Dual 48 Core Epycs  \n\\* 1 TB DDR4 ram (8x128)  \n\\* 1x RTX 6000 Pro Blacwell\n\n\n\nCurrent findings:  \n\\* Setup on Blackwell is still a bit of a pain, by comparison the 3090s are easy peasy  \n\\* Prompt Processing performance - using GLM 4.6 quants in llama.cpp I get 3.5x the speed with the blackwell than I do with the 3090  \n\\* Token Generation - essentially no difference (I think I can get a little better performance here from the Pro 6000 but between system differences and kernel differences the raw power of the Pro doesn't make up the difference in perf)  \n\\* Power consumption - baseline consumption is very similar, with the pro sitting at around 3.5x one 3090.  Under single query load the pro has been hitting 200 W  \n\\* Sound - The pro is in my living room, I can barely hear it.  The 3090s are in my office.... I can \\*really\\* hear it.  Not horrible but I know when it's running.\n\nI haven't played with vLLM much but for models that can be fully resident in 96 GB VRAM the Pro tentatively ran at 2x for both generation and prompt processing.\n\nI probably have done the closest to an 'apples to apples' comparison because I had the 6000 in the 3090 system for a period.  My conclusion is:\n\n\\* For models fully resident in 96GB the Pro wins hands down (ignoring pricing)  \n\\* For models partially resident in 96GB the Pro wins on processing but not prompt generation  \n\\* When factoring in price, the 3090 is a great contender  \n\\* When factoring in future improvements and the ability to easily go to 2 or 4 gpus I think the Blackwell wins.\n\n  \nI am a bit disappointed in token generation on really big models but not that surprised.",
          "score": 22,
          "created_utc": "2025-12-24 00:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpob5n",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2025-12-24 13:18:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqmrca",
                  "author": "I-cant_even",
                  "text": "Both machines are tied up with work at the moment.\n\n!remindme 3 days",
                  "score": 1,
                  "created_utc": "2025-12-24 16:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwbtdgm",
              "author": "the_lamou",
              "text": "I wouldn't expect there to be any meaningful difference in generation between the two systems on models that exceed VRAM: the bottleneck is the CPU and data moving to and from it.\n\nIf you're trying to push a watermelon through a pinhole, it's not going to matter if you're pushing with your finger or a sledgehammer.",
              "score": 2,
              "created_utc": "2025-12-28 07:11:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpka1p",
              "author": "Disposable110",
              "text": "Thank you for the data!\n\nDo you have the workstation or max-q version of the pro 6000?",
              "score": 1,
              "created_utc": "2025-12-24 12:49:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqmfo1",
                  "author": "I-cant_even",
                  "text": "Workstation, it hasn't hit full power draw but I intend to power limit (and frequency cap if needed) if power draw becomes an issue.",
                  "score": 2,
                  "created_utc": "2025-12-24 16:34:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvn3uci",
          "author": "shifty21",
          "text": "I went from a 3x 3090 to RTX Pro 6000 + 2x 3090s.  And my power consumption is down at idle and at peak power usage.  And I haven't even started to limit wattage on the 6000.  All cards idle at \\~15w.  \n\nI know electricity can be quite expensive in Europe, so keep that in mind.  \n\nFor comparison, my 3x 3090s barely fit gpt-oss-120b Q8 w/ 64k context length.  Running that LLM in either LM Studio server or in llama.cpp, I was getting roughly 20\\~25 t/s at 700\\~800w during inferencing.   \n\nWith just the 6000, I get \\~200 t/s at \\~350w can peak at 400w very briefly.  So essentially, I'm getting 10x performance and 50% less power usage.  I have several remote users that uses a custom chat bot app that I made and I've tracked the power usage over time and I rarely go over 400w on the 6000.\n\nI pull power stats every 5 seconds and here you can see where I installed the 6000.  I still have 2x 3090s installed, but once I migrate over my workflows to the 6000, I'll pull the 3090s out completely.  In the chart is my total daily power usage in W - the search  syntax is fairly easy to understand.\n\nhttps://preview.redd.it/19w22f69w19g1.png?width=1473&format=png&auto=webp&s=1d9f7fc7432f429f2b4714a72719212356a53517",
          "score": 9,
          "created_utc": "2025-12-24 01:02:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsp3do",
              "author": "shvz",
              "text": "I currently have 3x3090 (haven’t been able to fit something meaningful with vllm as I have to use tensor 2 or 4) the 6000 seems interesting, something that take a bit less space, heat a bit less while still offering good performance that seems a good option instead of going to 4x3090 ?",
              "score": 1,
              "created_utc": "2025-12-24 23:53:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwb5fy",
                  "author": "shifty21",
                  "text": "If you don't already have a workstation/server class motherboard, then I wouldn't.\n\nI asked the same question you have and the cost got out of hand really quickly:\n\n1.   Open frame mining 'case'\n2.  2nd PSU + dual PSU adapter\n3.  M2. to PCIe 16x\n4. Used/discounted Epyc or Threadripper CPU + Motherboard + RAM\n\nAt that point, the total cost 4x 3090s + all that above would have been just as expensive as a 6000.  Plus, I didn't want to hassle with the performance issues and excessive power draw.  \n\nTo me, it was 'buy once, cry once' situation.  So far, I'm impressed with the 6000.  Currently waiting on a M.2 to PCIe x16 adapter to put my 3rd 3090 back in and do some more testing.  After that, I may end up selling the 3090's.",
                  "score": 1,
                  "created_utc": "2025-12-25 17:39:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvm8eu6",
          "author": "arentol",
          "text": "The 3090's will not give you 96gb of useable memory. There is VRAM overhead space being taken up to manage the communication between the cards, so you will only have about 88gb of actual useable VRAM. In addition it will run slower because it has to send information between cards.",
          "score": 6,
          "created_utc": "2025-12-23 21:59:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmf6zu",
              "author": "Karyo_Ten",
              "text": ">In addition it will run slower because it has to send information between cards.\n\nFor inference, tensor parallelism can get 20 to 30% when moving from 1 to 2 GPUs. With diminishing returns up to 8 GPUs but at 4 I guesstimate you might get 30~40%.",
              "score": 5,
              "created_utc": "2025-12-23 22:36:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmkehd",
          "author": "egnegn1",
          "text": "You can find quite a few 4x3090 builds and tests at YouTube:\n\n[https://youtu.be/So7tqRSZ0s8](https://youtu.be/So7tqRSZ0s8)\n\n[https://youtu.be/gNexETeCLko](https://youtu.be/gNexETeCLko)\n\nThe same is true for RtX 6000:\n\n[https://youtu.be/JbnBt\\_Aytd0](https://youtu.be/JbnBt_Aytd0)\n\nThe issue is that for normal single prompt inference you cannot use the full processing potential of the clustered GPUs. This is because currently the processing is moving sequentially from GPU to GPU. With 4 GPUs you use only about 25 % of the total processing power. In contrast a single GPU with large enough VRAM can use nearly 100 % of the processing power of this gpu. So the inference speed of a single RTX 6000 Pro Blackwell is much higher than the speed of a 3090 cluster.\n\nSorry if I tell you something you already know, but this may be interesting for others looking into clustering, too. This all is explained in the following videos in more detail:\n\n[https://youtu.be/A0onppIyHEg](https://youtu.be/A0onppIyHEg)\n\n[https://youtu.be/bFgTxr5yst0](https://youtu.be/bFgTxr5yst0)\n\nBackground info: [https://blog.exolabs.net/day-1/](https://blog.exolabs.net/day-1/) (read complete blog)\n\nI would avoid clustering with 3090. Depending on budget for 96 GB VRAM and performance requirements I would go with the following sequence:\n\n\\- AMD AI Ryzen Max+ 395 128 GB ( < 2000 Euro [https://www.reddit.com/r/MiniPCs/s/17AzFnPPeX](https://www.reddit.com/r/MiniPCs/s/17AzFnPPeX) )\n\n\\- nVidia DGX Spark GB10 ( 3000 - 4000 Euro )\n\n\\- Apple Mac Studio M4 Max 128 GB ( 3000 - 4000 Euro )\n\n\\- nVidia RTX 6000 Pro Blackwell ( 7000 - 8000 Euro )\n\nBest performance is certainly RTX 6000, but is also the most expensive.",
          "score": 3,
          "created_utc": "2025-12-23 23:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmhq68",
          "author": "Karyo_Ten",
          "text": "A comparison is pretty easy. You can split it in 2 sections. Compute and memory-bandwidth.\n\nSources:\n- https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622\n- https://www.techpowerup.com/gpu-specs/rtx-pro-6000-blackwell.c4272\n\n\n## Compute\n\nThis is important for prompt processing and for batching if you have multiple users or agents running concurrently.\n\n- RTX 3090 is 10496 Cuda cores x4\n- RTX Pro 6000 is 24064 Cuda cores\n\nBut RTX Pro 6000 has hardware FP8 support for x2 perf, and hardware FP4 support for x4 perf which are bound to grow more and more standard in the next 2 years.\n\nSo if you run something in FP8 (say Kimi Linear or Qwen Next) or FP4 (gpt-oss-120b, when it properly uses hardware FP4) the Pro 6000 is actually faster for prompt processing.\n\n## Memory-bandwidth\n\nThis is important for token generation if you don't have many concurrent queries. (I have a lot of details why in https://www.reddit.com/u/Karyo_Ten/s/e7V16gbJac)\n\n\n- RTX 3090 is 936.2 GB/s\n- RTX Pro 6000 is 1792 GB/s\n\nWith tensor parallelism, your AI inference framework can make it that tensors are 4 times smaller in size, per GPU, and that means less memory to load (but there are diminishing returns as you do more synchronization, and above 8 it's not helpful).\n\nI have seen 20~35% perf improvement with 2 GPUs so I expect 40~50% with 4 GPUs. \n\nBut even with that, it would still be slower than a RTX Pro 6000.",
          "score": 3,
          "created_utc": "2025-12-23 22:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmbb62",
          "author": "DAlmighty",
          "text": "Outside of price, there is no real downside that I can think of when choosing the Pro 6000 over 4 3090s.",
          "score": 1,
          "created_utc": "2025-12-23 22:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmzq9y",
          "author": "UrsusMKV",
          "text": "Only Blackwell can do fp4, so if you want to run large models fast with precision, then RTX 6k pro is your best bet.",
          "score": 1,
          "created_utc": "2025-12-24 00:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvo79f5",
          "author": "kidflashonnikes",
          "text": "I have 4 RTX PRO 6000s (we got them early for 4k a piece) and our older system of 64 RTX 3090 Tis. I can tell you with 100% confidence that the RTX pro 6000s are not only the future - but quite literally puts to shame any other consumer cluster I have seen.",
          "score": 1,
          "created_utc": "2025-12-24 05:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvohf1u",
              "author": "lolcatsayz",
              "text": "sorry but what? how did you get them for 4k a piece?",
              "score": 1,
              "created_utc": "2025-12-24 06:47:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvohk5o",
                  "author": "kidflashonnikes",
                  "text": "When you’re a big lab they come to you first with bundle options. Min buy order was 4, basically over 25k with taxes and shipping (I think converted to USD).",
                  "score": 1,
                  "created_utc": "2025-12-24 06:48:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvoghvr",
          "author": "leonbollerup",
          "text": "I have a question… what in gods name do you need these monster setups for ?\n\nWhat kind of AI workloads do you do ?",
          "score": 1,
          "created_utc": "2025-12-24 06:39:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvoj8i2",
          "author": "decentralize999",
          "text": "Seems nobody mentioned that having whole VRAM in single GPU allows to run more things. I have 6 x RTX3090 and I am not able to run in llama.cpp  quant8 of Qwen3-VL-235B-A22B-Thinking because its single batch of experts or whatever does not fit inside 24GB VRAM.",
          "score": 1,
          "created_utc": "2025-12-24 07:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvomaf6",
              "author": "Zyj",
              "text": "Weird. You can split dense models, why not the active parts of an expert?\nBesides at Q4 the active part should fit easily into a 3090.",
              "score": 1,
              "created_utc": "2025-12-24 07:31:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvooqni",
                  "author": "decentralize999",
                  "text": "I don't know maybe issue of llama.cpp again or whatever. Q4 is not my choice. So I just mentioned what OP will fight with if solution based on RTX 3090 while it is \"cheap\" only $33/VRAMGB cost for me.",
                  "score": 1,
                  "created_utc": "2025-12-24 07:55:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpeioa",
          "author": "olli-mac-p",
          "text": "Get a 4090 48 GB card. It's loud and the fan curve cant be adjusted but it rips. No regrets so far. Maybe get 2 for the price of 1 ada 6000. Can run much bigger models on just 1 3090 card.  It's most of the time better running models on 1 card. Else you have to serve your models with tensor parallelism for gaining advantage.\n\nAs I understood you can just split the model equally, so adding a bigger card to your 3090 will result that you only can allocate only the same amount of vram to all gpus in said cluster. But if anyone knows better please educate me.",
          "score": 1,
          "created_utc": "2025-12-24 12:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpeug4",
              "author": "pCute_SC2",
              "text": "and now there is a third option on the table, but then need a server like with the 3090 rig, so cost wise it might be more expensive.",
              "score": 1,
              "created_utc": "2025-12-24 12:06:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvplk8e",
              "author": "Disposable110",
              "text": "4090 48gb's are around 2500 each though, might as well go for the RTX Pro and not have the risk of drivers/hardware failures.",
              "score": 1,
              "created_utc": "2025-12-24 12:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw4pa6n",
          "author": "False-Ad-1437",
          "text": "You could test it out on one of the gpu rental platforms. I imagine it will be under $2/hr for each. ",
          "score": 1,
          "created_utc": "2025-12-27 03:02:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7eru7",
          "author": "gwolla138",
          "text": "Got 4x 6000 blackwell max-q. 1 TB DDR5 and 64 core 9000 threadripper to be on the safe side. Serving GLM4.7-FP8 for inference on sglang with MTP at 130t/s. In terms of sounds not too bad. Maxed the max-q’s to 250watt for inference. Hardly any performance degradation. \n\nIn hindsight, perhaps should have gotten intel cpus for the ktransformers/sglang combo for deepseek/kimi. But.. i can live with that.",
          "score": 1,
          "created_utc": "2025-12-27 15:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvml72e",
          "author": "SillyLilBear",
          "text": "Not that i know of, but the rtx 6000 pro will win every time.",
          "score": 1,
          "created_utc": "2025-12-23 23:10:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmlha8",
          "author": "RiskyBizz216",
          "text": "RTX pro for sure if thats in the budget.\n\nI just went thru hell trying to squeeze 3x5090's in an EATX case..broke one of their fans due to space and settled on 2x5090's\n\nSave yourself the stress and broken parts! Just deal with 1xGPU",
          "score": 1,
          "created_utc": "2025-12-23 23:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmsf6i",
          "author": "phido3000",
          "text": "Yes, get the RTX6000. It will be faster, better supported, quieter, more efficient, and more useful in more workloads and have better resale. \n\nPlus if one RTX6000 isn't enough, or you want more you can drop in another. You have upgrade paths.",
          "score": 0,
          "created_utc": "2025-12-23 23:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmv3w5",
              "author": "pCute_SC2",
              "text": "but its the same with the four 3090, if its not enough I can get 4 more for cheaper than another Blackwell",
              "score": 1,
              "created_utc": "2025-12-24 00:09:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvmx8v4",
                  "author": "phido3000",
                  "text": "Value will tend to the 4x3090. \n\nRunning 8 x way GPU systems is a bit a nightmare. It gets messy, performance drops off, it becomes an issue to power it all. \n\nIMO the compute and training performance of the RTX6000 will be much high AFAIK. Also with the training, having a card with much larger memory is a pretty big advantage. \n\nFor your workstation given your setup, I would go with the 6000 if you can afford it.\n\nIf you want to build a four way 3090 setup, nothing is stopping you from doing that, possibly as well. I quite like my AI server not to be my local machine, as heat/noise gets annoying, while the server in my garage, I do not care at all. 7900xtx is ok, but for local AI there are better options including the 32gb AI9600 pro, but also a 5090, RTX6000 etc. You may wish to do both",
                  "score": 0,
                  "created_utc": "2025-12-24 00:22:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvm98wf",
          "author": "Prudent-Ad4509",
          "text": "There are no comparisons because if there is a serious budget available, then 6000 pro is a no-brainer - easier to install, easier to use. One, two, maybe four. Professional solutions are available.\n\nIf such a budget in not available (i.e. is a homelab), then 2,4,8 or 12 3090 is the best option, at least for inference. In a rig, with added heatsinks to the backplate, with power limiting, with custom cooling, etc etc  to avoid problems.\n\nThere does not seem to be much of a cross between these audiences.\n\nPS. What do you know. It seems that there are still a few people who have both, albeit with the expected outcome. Out of 3090, 5090 and PRO 6000: systems made out of multiple 3090 win on total vram/money ratio and can run fast with proper paralleling, systems made out of several 5090 are second on performance/money (again, with proper parallel execution), and systems made out of PRO 6000 win on performance and power but with a significantly larger upfront cost.\n\nI'd bet a can of expired soda that the general overall performance capacity / cost would still be about the same in all three cases, it happens to be the case in most calculations that I did before. This is despite the list of wins before. Things swing one way or another depending on how you calculate costs. There is a higher cost of maintenance for a more complex system.",
          "score": 0,
          "created_utc": "2025-12-23 22:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvm7ul8",
          "author": "minhquan3105",
          "text": "Get the rtx 6000 pro if you can afford it. The 3090 is really old now and they have pretty high failure rate because of their massive die size. Also power and cooling will be a nightmare for 4 x 3090",
          "score": -1,
          "created_utc": "2025-12-23 21:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmdkvx",
              "author": "pCute_SC2",
              "text": "What do you mean by failure rate?",
              "score": 1,
              "created_utc": "2025-12-23 22:27:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvmf46t",
                  "author": "StardockEngineer",
                  "text": "They’re starting to beak down.",
                  "score": 2,
                  "created_utc": "2025-12-23 22:35:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1puf1ic",
      "title": "ClosedAI: MXFP4 is not Open Source",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1puf1ic/closedai_mxfp4_is_not_open_source/",
      "author": "max6296",
      "created_utc": "2025-12-24 05:01:51",
      "score": 41,
      "num_comments": 10,
      "upvote_ratio": 0.78,
      "text": "Can we talk about how ridiculous it is that we only get MXFP4 weights for gpt-oss?\n\nBy withholding the BF16 source weights, OpenAI is making it nearly impossible for the community to fine-tune these models without significant intelligence degradation. It feels less like a contribution to the community and more like a marketing stunt for NVIDIA Blackwell.\n\nThe \"Open\" in OpenAI has never felt more like a lie. Welcome to the era of ClosedAI, where \"open weights\" actually means \"quantized weights that you can't properly tune.\"\n\nGive us the BF16 weights, or stop calling these models \"Open.\"",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1puf1ic/closedai_mxfp4_is_not_open_source/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvokubl",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 32,
          "created_utc": "2025-12-24 07:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoscfp",
              "author": "NeverEnPassant",
              "text": "> This model was trained with QAT, or Quantization aware training, meaning it won't natively have larger weights than have been posed.\n\nQAT uses master weights + quantized weights during training. The released model only includes the quantized weights. You would achieve better outcomes in fine tuning if you had the master weights. You won't be training natively in 4-bits in either case.\n\n> On Blackwell.. well, also wrong. Blackwell excells at NVFP4 not MXFP4.\n\nBlackwell supports both in hardware.",
              "score": 8,
              "created_utc": "2025-12-24 08:30:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsz865",
              "author": "Puzzleheaded_Fox242",
              "text": "I work on this stuff and this comment is a bizarre mix of misplaced confidence and misinformation. Everyone should disregard it or post it into a frontier LLM to see where it is wrong.",
              "score": 2,
              "created_utc": "2025-12-25 01:05:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvolsmc",
          "author": "gwestr",
          "text": "I like BF16, but they don't owe you an open license to it. For the end user community what they distribute is appropriate for a high end PC. But yeah probably not so useful for fine tuning. Maybe evaluation.",
          "score": 6,
          "created_utc": "2025-12-24 07:27:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvoh35y",
          "author": "decentralize999",
          "text": "This company is only damage for world ai development, they are gathering all available GPUs from all factories making deficit for good companies which create openweight and open sources LLMs and for folks which cannot purchase these GPUs even with double or triple price. \n\n\nAnyway I believe that to trick whole world market is possible 2-3 years and after countries like China will beat this company even own country competitors create better LLMs now.",
          "score": 8,
          "created_utc": "2025-12-24 06:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvocma1",
          "author": "TheAussieWatchGuy",
          "text": "I mean state the obvious. Open source AI is a long way behind and trillions of US dollars are working to keep it so. ",
          "score": 5,
          "created_utc": "2025-12-24 06:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqlcdr",
              "author": "inevitabledeath3",
              "text": "It's not though... We have models now comparable to Sonnet 4.5 and GPT-5 which were SOTA only a few months ago. Sonnet 4.5 came out on 29th September. GLM 4.7 came out just a couple days ago making it a bit under 3 months behind SOTA. Things like Gemini 3 Pro and Opus 4.5 are harder to beat, but they were released quite recently. It's fair we don't have a good replacement for them yet.",
              "score": 3,
              "created_utc": "2025-12-24 16:28:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrr3vb",
          "author": "Witty_Mycologist_995",
          "text": "i think we should get more mxfp4 models and less bf16.",
          "score": 2,
          "created_utc": "2025-12-24 20:18:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp8ene",
          "author": "k_means_clusterfuck",
          "text": "you know you can just dequantize it yourself right? I already did for a different experiment. it was trained in MXFP4 that doesnt mean you HAVE to use it in MXFP4. There are no 'original bf16 weights', just dequantize and enjoy having an 8 times larger model",
          "score": -1,
          "created_utc": "2025-12-24 11:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtd2b9",
          "author": "Consistent_Wash_276",
          "text": "There’s a 4bit and 8bit model of 120b on LM Studio. Correct me if I’m miss reading this as I’m not an expert. But yes no fp16",
          "score": 0,
          "created_utc": "2025-12-25 02:50:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1aq7a",
              "author": "Badger-Purple",
              "text": "There are no wrong questions.\n\nThe model itself was trained and released with 4 bit mixed precision (between integer and floating point) weights; not quite FP4 and not Q4 (Int 4). \n\nThe original release did include 16 bit attention paths, but you will see quantizations that compress that attention paths as well to 8 bits. \n\nWhat you will also note is that the size of the quantized model is not that smaller. That is what OP refers to: the model weights, regardless of version you see, are natively 4 bits, so the size does of change much. Analogous to putting 24k gold on a lower quality gold in jewelry.\n\nThe original 16 bit weights will not see the light of day. That makes their model less likely to be finetuned as effectively, although not completely immune to modifications, such as the de restricted versions that have popped up.",
              "score": 1,
              "created_utc": "2025-12-26 15:30:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxvlh0",
      "title": "I have 50 ebooks and I want to turn them into a searchable AI database. What's the best tool?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "author": "Great_Jacket7559",
      "created_utc": "2025-12-28 16:33:33",
      "score": 40,
      "num_comments": 29,
      "upvote_ratio": 0.98,
      "text": "I want to ingest 50 ebooks into an LLM to create a project database.\nIs Google NotebookLM still the king for this, or should I be looking at Claude Projects or even building my own RAG system with LlamaIndex?\nI need high accuracy and the ability to reference specific parts of the books. I don't mind paying for a subscription if it works better than the free tools.\nAny recommendations?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwj05xl",
          "author": "Zucramj",
          "text": "I would build something custom.\n\nHere is how I see it:\n\n1) You own the data \n2) You own the AI (local embeddings work great for this task and you can run it with a local AI model or use Openrouter) \n3) I would build this with DSPy (modular and can be optimized with gepa) \n4) I would use PostgreSQL to store the data \n\nSo if you already have the ebooks as pdfs I would take those an set that up.\n\nHere is my primitive version of this I did some months back: \n\nhttps://github.com/marcusjihansson/dspy-mcp-tools/blob/main/regulatory.py\n\nIt is advanced if you don't know what you are reading but as I have gotten deeper into optimizing AI agents and systems does this feel primitive to me. \n\nThat newer research is going to be uploaded to my GitHub soon...\n\nSo: \nA) I would read through if this would solve your task! \nB) I would be happy to help you out if you have any questions!",
          "score": 9,
          "created_utc": "2025-12-29 11:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf0w7j",
          "author": "DHFranklin",
          "text": "NotebookLM and maybe some RAG and Custom Instructions for vectoring.\n\nNow if you wanna get real squirrely you could turn the entire compendium, and custom instruction into 1 million token prompt and sit it into Gemini as is. That might actually be more useful.\n\nThe trick is information loss and context bleed with the books. I could see a JSON Database made from it all as text also.\n\nIt comes down to what are you using it for and what information needs to stay consistent.",
          "score": 4,
          "created_utc": "2025-12-28 19:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe78fy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-28 17:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwerjho",
              "author": "blaidd31204",
              "text": "I am intrigued... what If:\n\n* pdf (Yes, there are images, but these should not influence info)?\n\n* markdown (No images)?",
              "score": 2,
              "created_utc": "2025-12-28 19:06:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh0m2z",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 4,
                  "created_utc": "2025-12-29 02:05:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwgwh5t",
                  "author": "Investolas",
                  "text": "Those answers influence the reccomendation.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwim06f",
          "author": "RepLava",
          "text": "LightRAG with the MCP. Works great based on the relatively sparse info you've given",
          "score": 2,
          "created_utc": "2025-12-29 08:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlvovo",
          "author": "False-Ad-1437",
          "text": "AnythingLLM can make your own workspaces for this.",
          "score": 2,
          "created_utc": "2025-12-29 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwft2zf",
          "author": "vidibuzz",
          "text": "Slightly off topic. You may want to use Illuminati.google.com also for voice summaries.",
          "score": 2,
          "created_utc": "2025-12-28 22:09:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhdx2b",
              "author": "Schizophreud",
              "text": "Didn’t know about this. Thanks.",
              "score": 1,
              "created_utc": "2025-12-29 03:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwibx1i",
          "author": "Charming_Support726",
          "text": "Depending on what's in the books, you could have a look at IBMs Docling for conversion. I think every simple RAG Pipeline will do the trick in the beginning",
          "score": 1,
          "created_utc": "2025-12-29 07:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigiuv",
          "author": "maxz2040",
          "text": "Logically.app",
          "score": 1,
          "created_utc": "2025-12-29 08:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm4e9l",
          "author": "Empty-Poetry8197",
          "text": "Paperqa",
          "score": 1,
          "created_utc": "2025-12-29 21:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwno756",
          "author": "kchandank",
          "text": "Interesting, if you are able to achieve your objective, would you be able to share the steps?",
          "score": 1,
          "created_utc": "2025-12-30 02:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpe82p",
          "author": "Sea_Mouse655",
          "text": "I’ve been using PaperQA2 for some regulatory use cases and it’s gangster",
          "score": 1,
          "created_utc": "2025-12-30 09:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpin9j",
          "author": "Cladser",
          "text": "I’m deep in a similar project atm. It depends a lot on the type of info you want from the LLM. If it’s what does author x have to say about y - RAG is your best bet. However if you want to ask questions like across these books what are the most common ways of dealing with with Y - That is a corpus level (ie entire collection) query and RAG will suck. Llamaindex with hybrid search seems to be the best middle road at the moment.",
          "score": 2,
          "created_utc": "2025-12-30 10:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsjglj",
          "author": "isleeppeople",
          "text": "Seems like you could use something like pypdf and langchain to embed it into your RAG. If the ebooks are like current info that can change or become stale you will want to tag them and set up some sort of workflow to compare them to a Gemini or open ai call to compare the info and if it becomes stale remove it. I use qdrant and postgresql for ground truth. I do stuff like this for versions of python that I have to use to stay compatible with other things I'm running. Or actually for langchain and langgraph too. They just changed compatibility versions so whenever I upgrade I can just use the newer repo in my rag with the updated information. I will still keep the old one until I am absolutely certain I won't revert but you can just leave it sit there and not reference the old one. Hope that makes sense.",
          "score": 1,
          "created_utc": "2025-12-30 20:25:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh0iik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-29 02:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwimlqu",
              "author": "Just_Bronze",
              "text": "I have a stupid question.\n\nNot entirely sure how these things work, but do I understand correctly you've got a system set up to take input and create a chatbot?  Or do you create a file/fileset that a chatbot incorporates to use the information.",
              "score": 3,
              "created_utc": "2025-12-29 09:05:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwisq5d",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 10:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjfv9q",
              "author": "loki626",
              "text": "Can I DM you? I have some pdfs that I would like to convert. They are more technical though. Medicine related.",
              "score": 1,
              "created_utc": "2025-12-29 13:13:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmga0",
          "author": "PaleontologistOk865",
          "text": "What about just throwing everything in a AI and letting it figure out what to do? That's what my clients keep saying to me. . .",
          "score": 0,
          "created_utc": "2025-12-30 23:38:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjnew",
      "title": "Tiiny Al just released a one-shot demo of their Pocket Lab running a 120B model locally.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "author": "Ajitabh04",
      "created_utc": "2025-12-29 11:09:13",
      "score": 38,
      "num_comments": 13,
      "upvote_ratio": 0.91,
      "text": "Just came across this demo. They plugged their tiny AI computer into a 14-year-old PC and it output an average of 19 tokens/s on a 120B model.\nThey haven't released the MSRP yet. However, a large amount of DDR5 memory would be pricey, I'm guessing around $1500 MSRP for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjcqd4",
          "author": "loyalekoinu88",
          "text": "They had posted around $699 BUT that was before the memory announcement",
          "score": 11,
          "created_utc": "2025-12-29 12:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj8uo7",
          "author": "leonbollerup",
          "text": "No link ?",
          "score": 7,
          "created_utc": "2025-12-29 12:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwklly9",
          "author": "ForsookComparison",
          "text": "> guessing $1500 MSRP\n\n> they posted $699 pre crucial RAM announcement \n\n> 19 tokens/second on gpt-oss-120b\n\nI plugged a used Rx 6800 ($250 in my area with multiple options) into an older PC and got 18 tokens per second. I know this isn't the same and it suggests that you have a fair amount of RAM in your older PC, but given what we know about this I'm thoroughly \"meh\"d.\n\n**Edit** - just looked up the form factor. I'm less meh'd now. That would be fun to use if it ends up affordable.",
          "score": 5,
          "created_utc": "2025-12-29 16:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlgo5g",
              "author": "FaceDeer",
              "text": "[It's a portable unit the size of a phone](https://tiiny.ai/), since there aren't links anywhere else in this thread.\n\nNot a lot of detail even there, though. I don't see anything about whether it's battery powered - I'm assuming not, given OP mentions plugging it in to a computer.",
              "score": 5,
              "created_utc": "2025-12-29 19:18:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwutx5z",
                  "author": "ecoleee",
                  "text": "You’re absolutely right — this generation of Tiiny does not include a built-in battery.\n\nThat decision was intentional. Thermal management is a serious challenge at this performance level, and we didn’t want to ship a device that becomes uncomfortably hot in real use.\n\nTo reliably support sustained local inference of models up to 120B parameters, we designed a custom thermal module specifically for Tiiny, prioritizing stability and safe operating temperatures over battery integration.\n\nAt the upcoming CES, we’ll be sharing a detailed internal teardown video of Tiiny. You’ll be able to see exactly how the cooling system is built and why these design choices were made.\n\nWe believe it’s better to be transparent about trade-offs and deliver a product that performs consistently, rather than chasing form factors at the expense of real-world usability.",
                  "score": 1,
                  "created_utc": "2025-12-31 03:48:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq0vao",
          "author": "Ok-Structure4057",
          "text": "Found the specs on their website:\n\npocket size: 14.2 × 8 × 2.53 cm\n\n80GB LPDDR5X RAM & 1TB SSD190 \n\ntotal TOPS between the SoC and dNPU\n\n30W TDP\n\nThey also released a demo of this device on Twitter. Imo it would be fun with retail prices around 1400 bucks.",
          "score": 2,
          "created_utc": "2025-12-30 12:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq5w0t",
              "author": "RangerOk4318",
              "text": "Agree. Memory price has been so absurd. I'm guessing the same price",
              "score": 1,
              "created_utc": "2025-12-30 13:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws3u2u",
                  "author": "QuinQuix",
                  "text": "Memory is fucking up any attempt at affordable home AI right now",
                  "score": 1,
                  "created_utc": "2025-12-30 19:10:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvbcwc",
              "author": "fallingdowndizzyvr",
              "text": "> mo it would be fun with retail prices around 1400 bucks.\n\nAt that price, why not just get a Strix Halo? That's just a PC so you can do regular PC stuff like gaming.",
              "score": 1,
              "created_utc": "2025-12-31 05:48:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0pjg",
          "author": "zelkovamoon",
          "text": "I'm not sure what having a *small* ai lab is trying to solve\n\nIf you're doing local AI my position is, make it bigger, cooler, and put more ram on it.\n\nThat said, it is *good* that companies are stepping in to try and build some solutions. If we could get something with 256GB of fast memory we might be able to go places.",
          "score": 2,
          "created_utc": "2025-12-30 21:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlotf1",
          "author": "No-Consequence-1779",
          "text": "I think the lpddr5 is likely the memory.  It’s slightly faster for this and it’s wired so they can , like others, charge for memory size mostly. ",
          "score": 1,
          "created_utc": "2025-12-29 19:57:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pva6ao",
      "title": "Now I don't have to pay for transcription services with whisper.cpp",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/fbw3abxzob9g1",
      "author": "DamageEvery764",
      "created_utc": "2025-12-25 09:58:54",
      "score": 37,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pva6ao/now_i_dont_have_to_pay_for_transcription_services/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvvuj8k",
          "author": "DHFranklin",
          "text": "Nice. How does it compare with other competitors going into 2026?",
          "score": 2,
          "created_utc": "2025-12-25 15:59:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvww5nk",
              "author": "Silver_Jaguar_24",
              "text": "Any other working local TTS in English that you can recommend?",
              "score": 1,
              "created_utc": "2025-12-25 19:41:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwzl38",
                  "author": "CtrlAltDelve",
                  "text": "Just a clarification, this is a Speech to Text app, not a Text to Speech app :)",
                  "score": 1,
                  "created_utc": "2025-12-25 20:02:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyw1b4",
      "title": "I built Plano(A3B) - fast open source LLM for agent orchestration that beats frontier LLMs",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5rp16cxd57ag1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-29 19:44:28",
      "score": 34,
      "num_comments": 12,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyw1b4/i_built_planoa3b_fast_open_source_llm_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwoqem5",
          "author": "ThsYWeCntHveNiceTngs",
          "text": "your research page is more advert than research and the blog provides more detail, but not much. Is there an Arxiv link or anything to read about your method and how you generated the proposed results?",
          "score": 4,
          "created_utc": "2025-12-30 06:07:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwosnmj",
              "author": "AdditionalWeb107",
              "text": "The huggingface models page has more details. Although we are in the process of publishing the arxiv paper",
              "score": 0,
              "created_utc": "2025-12-30 06:25:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwufm16",
          "author": "Purple-Programmer-7",
          "text": "How does this differ from Arch that you’ve previously pushed for the past year?",
          "score": 1,
          "created_utc": "2025-12-31 02:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui3lf",
              "author": "AdditionalWeb107",
              "text": "Arch was about model routing. Plano is about orchestration, which is a slightly more complicated set of tasks. Plano is the next major upgrade to Arch with several new capabilities for agentic applications like filter chains, agent signals, and even more robust model gateway.\n\n\nBy the way, people were confusing arch with arch Linux so we thought it was a better time to rename the project.\nTry Plano 🙏",
              "score": 1,
              "created_utc": "2025-12-31 02:37:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwopty3",
          "author": "maigpy",
          "text": "what is model-native?",
          "score": 1,
          "created_utc": "2025-12-30 06:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqe6c",
              "author": "AdditionalWeb107",
              "text": "It’s integrated with small LLMs - central to how the project is built.",
              "score": -1,
              "created_utc": "2025-12-30 06:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6v7x",
                  "author": "maigpy",
                  "text": "integrated with small llms translates to \"model-native\"?\n\nI don't quite understand, if you could use more words to describe what's going that would help.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnderh",
          "author": "Lyuseefur",
          "text": "Planning already to use it in next build of nexora follow us\n\nHttps://github.com/jeffersonwarrior/nexora",
          "score": 0,
          "created_utc": "2025-12-30 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnwumq",
              "author": "AdditionalWeb107",
              "text": "Okay - thanks. Would love the feedback. And if you like our project, don't forget to star it too",
              "score": 1,
              "created_utc": "2025-12-30 02:56:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxzjhp",
      "title": "Google Open-Sources A2UI: Agent-to-User Interface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 19:08:03",
      "score": 25,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "Google just released **A2UI (Agent-to-User Interface)** — an open-source standard that lets AI agents generate **safe, rich, updateable UIs** instead of just text blobs.\n\n👉 Repo: [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\n# What is A2UI?\n\nA2UI lets agents “**speak UI**” using a **declarative JSON format**.  \nInstead of returning raw HTML or executable code (⚠️ risky), agents describe *intent*, and the client renders it using **trusted native components** (React, Flutter, Web Components, etc.).\n\nThink:  \nLLM-generated UIs that are **as safe as data, but as expressive as code**.\n\n# Why this matters\n\nAgents today are great at text and code, but terrible at:\n\n* Interactive forms\n* Dashboards\n* Step-by-step workflows\n* Cross-platform UI rendering\n\nA2UI fixes this by cleanly separating:\n\n* **UI generation (agent)**\n* **UI execution (client renderer)**\n\n# Core ideas\n\n* 🔐 **Security-first**: No arbitrary code execution — only pre-approved UI components\n* 🔁 **Incremental updates**: Flat component lists make it easy for LLMs to update UI progressively\n* 🌍 **Framework-agnostic**: Same JSON → Web, Flutter, React (coming), SwiftUI (planned)\n* 🧩 **Extensible**: Custom components via a registry + smart wrappers (even sandboxed iframes)\n\n# Real use cases\n\n* Dynamic forms generated during a conversation\n* Remote sub-agents returning UIs to a main chat\n* Enterprise approval dashboards built on the fly\n* Agent-driven workflows instead of static frontends\n\n# Current status\n\n* 🧪 **v0.8 – Early Public Preview**\n* Spec & implementations are evolving\n* Web + Flutter supported today\n* React, SwiftUI, Jetpack Compose planned\n\n# Try it\n\nThere’s a **Restaurant Finder demo** showing end-to-end agent → UI rendering, plus Lit and Flutter renderers.\n\n👉 [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\nThis feels like a big step toward **agent-native UX**, not just chat bubbles everywhere. Curious what the community thinks — is this the missing layer for real agent apps?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwgkvue",
          "author": "bananahead",
          "text": "I am begging people to stop with the LLM written posts. Just post whatever prompt you used! That’s the post!",
          "score": 12,
          "created_utc": "2025-12-29 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi048d",
              "author": "leonbollerup",
              "text": "Why care? I rather want a LLM generated post than some bad version of English - not everyone speaks English native",
              "score": 3,
              "created_utc": "2025-12-29 05:47:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjksfb",
                  "author": "ak_sys",
                  "text": "Id rather tailor my discussion to a non English speaker than not understanding that there may be a disconnect getting lost in translation. \n\nThis explains a lot of \"reading comprehension\" issues I've noticed from commenters, where they seem to be responding to some diffuse sentiment of the post rather than  the actual nuanced point and position. Languages almost never just directly translate into one another, and inserting an llm in the middle of a discussion without informing the other party seems pretty dishonest and disrespectful.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:44:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwmr4i2",
                  "author": "bananahead",
                  "text": "So write a post in your native language and have it translated. That’s just as easy as whatever prompt created this and would be easier to read and more authentic.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgo2bh",
              "author": "Kamal965",
              "text": "The absolute state of Reddit now:\n\n* A lazy OP asks an LLM to create a post for them.\n* Users see LLM-isms and either:\n   * Ctrl + W\n   * Pass it on to an LLM to summarize it for them instead.\n* Another OP gives up on writing their own posts and starts using an LLM because they keep running into LLM-generated posts.\n\nRinse and repeat.",
              "score": 0,
              "created_utc": "2025-12-29 00:53:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nworg5v",
          "author": "ZITNALTA",
          "text": "Just curious if any knows if this somehow works with Google Antigravity IDE? By the way, I am NOT a dev so if this sounds like a newbie question that is why.",
          "score": 1,
          "created_utc": "2025-12-30 06:15:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpeh7",
      "title": "Device to run a local LLM mainly for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "author": "knibroc",
      "created_utc": "2025-12-28 11:40:21",
      "score": 22,
      "num_comments": 29,
      "upvote_ratio": 0.92,
      "text": "Hi mates,\n\nI mostly use ChatGPT and Mistral (through their \"vibe coding\" cli tool and API). I don't pay for these services, so I only use the lesser-capable models.\n\nMy laptop is not powerful enough to run this (no GPU / I've experimented with ollama but I can only run the smallest models very slowly so this is not ok for daily use), so I'm currently considering building a device dedicated to running a LLM, mainly for coding purposes. Ideally something small, Raspberry Pi-based or similar would be great.\n\nI have a few questions: is there specialized hardware for this (I've heard of TPU/NPU)? What kind of performance can I expect (I'd need at least GPT4/Devstral level)? I'm also worried about speed (tokens/s) and cost.\n\n  \nAny advice is appreciated!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdmlhq",
          "author": "KrugerDunn",
          "text": "The cost of any device that can run a decent coding model will far out scale just paying for Claude Code and still won’t be nearly as good or future proof.\n\nI went down this rabbit hole so you don’t have to 😂",
          "score": 36,
          "created_utc": "2025-12-28 15:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdstym",
              "author": "skabaru",
              "text": "2nd. $100/month to Claude code is the best money you can spend here.... And I do have an older gaming rig running local llms... And it isn't even close.",
              "score": 12,
              "created_utc": "2025-12-28 16:19:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwekb8b",
                  "author": "New_Jaguar_9104",
                  "text": "I have an entire cluster and still pay for Claude max. It's worth its weight in gold IMO",
                  "score": 5,
                  "created_utc": "2025-12-28 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwffg7z",
              "author": "ThatOneGuy4321",
              "text": "what’s your threshold for “decent”? 70B?",
              "score": 1,
              "created_utc": "2025-12-28 21:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgn3r5",
                  "author": "KrugerDunn",
                  "text": "I mean, depends what you're trying to do. I personally just love using Claude Code and occasionally Gemini Cli, so nothing is going to compare to those that can be run locally. Maybe could get away with a GLM4.6-AIR which is 357B params. Unless you're working on super duper secret proprietary code or something that violates the foundation model guardrails I just don't see any reason to use local.\n\nI know some people have used something like Qwen Code 32B or Devstral 24B and been satisfied with it, but never been worth it to me.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdph75",
          "author": "TyphoonGZ",
          "text": "If you're not satisfied with 1-5 toks/s on CPU (\"coffee break\" workflow), sounds like you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nWhy 30B? 20--30B is the size range where the model is still (sort of) considered \"small\" yet it starts being actually useful.\n\nThat said, you should consider using Openrouter and spend $5 for credits to test said models and see if they're good enough for you. You wouldn't want to buy a ~$1000 GPU just to get annoyed that your model's too braindead, right?\n\nRegarding TPU/NPU, I haven't heard if NPUs finally have the necessary software infrastructure to be useful. Well, they wouldn't really help LLMs if there's no development in memory bandwidth to go with them.\n\nOn the other hand, Google sells Coral TPUs, but those are for computer vision, not LLMs, and anyway, they only have *megabytes* of memory.",
          "score": 8,
          "created_utc": "2025-12-28 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweoi73",
              "author": "Count_Rugens_Finger",
              "text": ">  you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nI can run Qwen3-coder-30B-A3B at 12 tok/sec on my 8GB 3070 and an absolute potato of a CPU",
              "score": 2,
              "created_utc": "2025-12-28 18:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgwndy",
                  "author": "TyphoonGZ",
                  "text": "Oh yeah, I forgot MoE models exist...\n\nAlso damn, that's janky. Nice.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj61mb",
              "author": "nasone32",
              "text": "today with 24gb vram you can run qwen3 coder at 150 tokens/s with 100k+ context, or something like qwen3 next 80B at 15/20 tokens/s offloading some layers.",
              "score": 1,
              "created_utc": "2025-12-29 12:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcvibf",
          "author": "AnxietyPrudent1425",
          "text": "If you have a desktop with a GPU or Mac Mini/Studio you can setup Tailscale and basically have your own cloud endpoint. My setup is a MacBook Air + 128GB Mac Studio + Linux workstation and I couldn’t be happier.",
          "score": 4,
          "created_utc": "2025-12-28 13:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcygiq",
              "author": "knibroc",
              "text": "if only I could afford a 128GB Mac Studio! these machines look great",
              "score": 3,
              "created_utc": "2025-12-28 13:24:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdccz6",
          "author": "Background_Gene_3128",
          "text": "Atm. I’ve dedicated a small server in the addict to run my llms, with a i5-12600k, 32gb ddr5 and a 3060 with 12gb vram. \nRunning proxmox with a ubuntu vm and ollama. \n8-14b models np, 14-24 okay, and the 30-34b models a bit too slow for my liking. \nSo I’ve upgraded to 96gb ram (not sure if that actually matter, but I’ve seen people get decent speeds with gpt-oss 120b with ram as offload, and found a used offer locally that didn’t require a kidney) \nAnd 2 5060 Ti’s as they’re on sale now here in Europe for €370 a piece. \n\nNot sure if this is the best budget setup or small, but it’s what I’m rocking until that’s not enough.\nIt’s fitted in a Jonsbo D32 pro mesh case.",
          "score": 4,
          "created_utc": "2025-12-28 14:52:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjuvo",
          "author": "BigYoSpeck",
          "text": "A used desktop/gaming PC or server is probably the most cost effective way in\n\n\nSomething with 64gb of either DDR4 or 5, a 6+ core CPU, and a 16gb or more GPU\n\n\nI recently purchased a Ryzen 9 5900X 64gb DDR4-3600 with a Radeon RX 6800 XT from eBay and I can run gpt-oss-120b with full context at just over 20 tok/s\n\n\nIn the near future I'd like to swap out the Radeon for an RTX 3090 but ROCm is fairly good these days in llama.cpp \n\n\nEfficiency and performance won't match a Mac with comparable memory but it's a fraction of the price and doubles up for gaming duties",
          "score": 3,
          "created_utc": "2025-12-28 15:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf81pw",
              "author": "andriizahorui",
              "text": "Hey, I have a similar config and struggle to run gpt 120b with llama vulkan at decent speeds. Could you please share how do you run it with full context at 20 tps? Like exact command and stuff please",
              "score": 2,
              "created_utc": "2025-12-28 20:26:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfjs4k",
                  "author": "BigYoSpeck",
                  "text": "I'm not at my computer at the moment to give the exact parameters I pass to llama-server but I know the key ones are:\n\n   --threads 8 (5-8 are all very close, after 8 performance declines)\n\n   --flash-attn on\n\n   --mlock --no-mmap\n\n   --n-gpu-layers 99\n\n   --n-cpu-moe 32 (going down to 28 at lower context is faster, 32 is the sweet spot for having space left though) \n\n\nThis is with a self compiled ROCm build, pre built Vulkan docker isn't quite as fast",
                  "score": 4,
                  "created_utc": "2025-12-28 21:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcq3d3",
          "author": "DenizOkcu",
          "text": "Buying a used MacBook with an Apple Silicon might be your best/cheapest bet. They leverage unified memory and use mlx as a native LLM engine. I use devstral small 2 on a M3 with 36GB RAM in LM Studio. Nvidias Nemotron 3 nano is even faster with great coding results incl tool usage.",
          "score": 3,
          "created_utc": "2025-12-28 12:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcyl0f",
              "author": "knibroc",
              "text": "indeed Nemotron 3 sounds cool, will check, thanks!",
              "score": 2,
              "created_utc": "2025-12-28 13:25:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdgs27",
                  "author": "DenizOkcu",
                  "text": "It works really well with Nanocoder an open source Coding Tool with a focus on privacy/local LLMs (disclaimer: I am one of the contributors)",
                  "score": 2,
                  "created_utc": "2025-12-28 15:17:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcml86",
          "author": "EternalVision",
          "text": "TPU is not really possible, only Google has those (developed them themselves). And your question really depends on your budget, the sky really is the limit here. An Ryzen Strix Halo 395 AI MAX+ based minipc is what could work out, but again, really depends on your budget.",
          "score": 2,
          "created_utc": "2025-12-28 11:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwghwj2",
          "author": "machaao",
          "text": "In our tests, we couldn't get any of the single current open source code LLM to work at workable speed that is 25 tokens per second or so for a medium size code base.\n\nAs soon as you want something to be demanding it kinda barfs and token / second goes down the drain \n\nWould love to hear success stories tho 😌\n\nP.S. Tried with gpt oss and qwen3 on M4 - 128G",
          "score": 1,
          "created_utc": "2025-12-29 00:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwisuw6",
          "author": "HealthyCommunicat",
          "text": "If u want a portable, the m4 max 128 gb is gunna be the best ur gunn get, if u dont have the money for that, the z13 flow. Load gpt oss 120b at high reasoning, qwen 3 next 80b, go checkout my post i did about testing all these models - keep in mind the z13 flow is half the cost so the token/s will be literally half.",
          "score": 1,
          "created_utc": "2025-12-29 10:04:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkp2kz",
          "author": "HiddenPingouin",
          "text": "The closest to claude code would be GLM4.7. You could run the q8 on a Mac Studio with 512GB of RAM",
          "score": 1,
          "created_utc": "2025-12-29 17:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnt1op",
              "author": "teleolurian",
              "text": "Pretty well, I would add.",
              "score": 1,
              "created_utc": "2025-12-30 02:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwf1i4b",
          "author": "Jarr11",
          "text": "Don't do it! It will cost you more to run it yourself than it would to just pay for a subsciption to Claude/ChatGPT/Gemini for CLI access",
          "score": 0,
          "created_utc": "2025-12-28 19:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf6esp",
          "author": "Oki667",
          "text": "Lol, just pay for Claude code monthly subscription.",
          "score": 0,
          "created_utc": "2025-12-28 20:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiurfb",
          "author": "Crazyfucker73",
          "text": "You aren't going to get GPT4 level on a fucking raspberry pi mate.",
          "score": -1,
          "created_utc": "2025-12-29 10:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjammv",
              "author": "knibroc",
              "text": "And you are being fucking helpful mate",
              "score": 5,
              "created_utc": "2025-12-29 12:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwracn",
      "title": "Top 10 Open-Source User Interfaces for LLMs",
      "subreddit": "LocalLLM",
      "url": "https://medium.com/@techlatest.net/top-10-open-source-user-interfaces-for-llms-94e3dd4ae20b",
      "author": "techlatest_net",
      "created_utc": "2025-12-27 06:51:33",
      "score": 20,
      "num_comments": 10,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwracn/top_10_opensource_user_interfaces_for_llms/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw5kai6",
          "author": "leonbollerup",
          "text": "Missing my favorites: cherry studio and Onyx ..",
          "score": 4,
          "created_utc": "2025-12-27 06:55:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5lef2",
              "author": "techlatest_net",
              "text": "Added to the list",
              "score": 2,
              "created_utc": "2025-12-27 07:05:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5mfsm",
                  "author": "silenceimpaired",
                  "text": "What do you use? What quant tech do you prefer?",
                  "score": 1,
                  "created_utc": "2025-12-27 07:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6p3lx",
          "author": "plainnaan",
          "text": "jan.ai? ",
          "score": 4,
          "created_utc": "2025-12-27 13:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw73uvi",
          "author": "Big-Masterpiece-9581",
          "text": "Need to verify git activity and velocity before recommending. Chatbot ui was the most popular in 2023. But they’re 2.0 used supabase and introduced a ton of bugs and complexity with a full rewrite that added no serious features users cared about. Openwebui overtook them and they haven’t come back. No commits since mid 2024 for 1.5 years. I would not recommend a dead project.",
          "score": 5,
          "created_utc": "2025-12-27 14:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8b7c1",
          "author": "datbackup",
          "text": "textgen webui seems to not have a way to use its front end with a backend other than the one it packages. So while the name sounds like it would be a competitor for openwebui, in fact it’s more like a competitor for ollama. Happy to be proven wrong about this if i simply couldn’t manage to find the config area that allows entering a url for the inference engine",
          "score": 2,
          "created_utc": "2025-12-27 18:32:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwadl9f",
          "author": "sirebral",
          "text": "Dunno, looking at this low effort article and it's pretty much all bloatware, upsells for paid mcp, or not even a chat inference.  Lame.",
          "score": 2,
          "created_utc": "2025-12-28 01:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa7njz",
          "author": "throwawayacc201711",
          "text": "I wish this would actually highlight differences between them. The descriptions are fairly generic.\n\nA table with some key features and showing what each project does and does not support would be significantly more useful IMO. \n\nReading this, it really doesn’t give a reader a good idea on why to choose one over the other. Just some friendly feedback",
          "score": 1,
          "created_utc": "2025-12-28 00:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg37y5",
          "author": "spiffco7",
          "text": "Cherry, Witsy, Goose",
          "score": 1,
          "created_utc": "2025-12-28 23:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwphae6",
          "author": "Civil_Swimmer_4162",
          "text": "[https://github.com/przbadu-ai/LocalMind](https://github.com/przbadu-ai/LocalMind) \\- I started this opensource app, where it auto extract websites contents (if user permits), it extract Youtube transcription, summarize them, allow user to upload documents (pdf, audio, txt, markdown, html), etc and chat with them. It is in early phase and planning to add more features. You can also configure MCP. Please feel free to contribute to it.",
          "score": 1,
          "created_utc": "2025-12-30 10:09:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwa0ro",
      "title": "78 stars in github but 0 feedback, how to improve my local AI application ?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/hzqxqjey0l9g1.png",
      "author": "Suspicious-Juice3897",
      "created_utc": "2025-12-26 17:38:33",
      "score": 20,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwa0ro/78_stars_in_github_but_0_feedback_how_to_improve/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw1zb9y",
          "author": "boraiross",
          "text": "I'm one of the devs, here is a link to the repo: [https://github.com/Tbeninnovation/Baiss](https://github.com/Tbeninnovation/Baiss) , keep adding more stars but DO NOT send any feedbacks",
          "score": 2,
          "created_utc": "2025-12-26 17:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw34i3l",
          "author": "danny_094",
          "text": "I don't understand the feedback thing either.\n\nMy application also regularly gets clones, sometimes 30 a day, sometimes fewer, etc. Especially after updates. So there are regular users. But I think feedback is rare for smaller projects? Just keep going :)\n\n\n` ...)`````)``) ``) ``) `\n\n``\n``\n``\n``",
          "score": 1,
          "created_utc": "2025-12-26 21:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3fwph",
              "author": "Suspicious-Juice3897",
              "text": "yeah maybe, also, what's yours ? I can test it out",
              "score": 1,
              "created_utc": "2025-12-26 22:24:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw21mr0",
          "author": "DraGSsined",
          "text": "I’ve already used it, and it looks pretty good. However, you can’t build a great project without user feedback.",
          "score": 0,
          "created_utc": "2025-12-26 17:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw22xjm",
              "author": "Suspicious-Juice3897",
              "text": "appreciate it, trying to get one, can we start with your feedback ? :)",
              "score": 2,
              "created_utc": "2025-12-26 18:00:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1put1f5",
      "title": "Is there a rule of thumb in deciding which model to use?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1put1f5/is_there_a_rule_of_thumb_in_deciding_which_model/",
      "author": "salty_salad13",
      "created_utc": "2025-12-24 17:47:43",
      "score": 19,
      "num_comments": 19,
      "upvote_ratio": 0.95,
      "text": "Hi! Im relatively new to this local LLM setup, and wanted to understand some basics fundamentals and to upskill in ai environment. Below is my pc specs.\n\nCPU: AMD Ryzen 7 8700F\nMOBO: Gigabyte A620M-H\nRAM: 16GB Lexar Thor (8GB×2) DDR5 6000\nSTORAGE: 500GB Lexar NM610 Pro NVMe SSD\nGPU: 8GB RX6600 ASRock Challenger (Dual Fan)\n\nOk so let me give some context, Currently, i have ollama llma3.1:8b running on windows with Open WebUI, i just followed instructions from chatgpt. Basically im just overwhelmed by the total steps right now that there's a lot of prerequisite apps and installation that is needed for it to work, like docker, wsl etc... also given the fact that im not realy into coding, though i have some small background.\n\nMy question is, Is there a UI that is windows friendly version?\n\nNext is, how do i pick a model that can run smoothly on my pc setups, is there like a 1:1 or 1:2 ratio in terms of ram/vram?\n\nLastly, from current setup, i don't think im fully utilizing my gpu resources, i asked chatgpt about this, but im still quite loss.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1put1f5/is_there_a_rule_of_thumb_in_deciding_which_model/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvr10kv",
          "author": "StardockEngineer",
          "text": "Just install LM Studio. One stop shop.  It will also tell you what models can fit on your VRAM.  \n\nBut your specs are extremely low so you’ll be able to run only the smallest models.   You don’t have any RAM to donate to LLM inferencing.",
          "score": 17,
          "created_utc": "2025-12-24 17:53:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvr2b6z",
              "author": "salty_salad13",
              "text": "I see, I'll check on that",
              "score": 3,
              "created_utc": "2025-12-24 18:00:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvr5fql",
          "author": "little___mountain",
          "text": "Download LM Studio. Select a model name where the B value = your GPU memory size. Then set context = to your Ram size. So you can theoretically run up to a 8B model with a 16k token context window.",
          "score": 8,
          "created_utc": "2025-12-24 18:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvum9hw",
              "author": "salty_salad13",
              "text": "Thankyou, i might consider upgrading my ram and gpu, but the market really sucks right now hahah",
              "score": 2,
              "created_utc": "2025-12-25 09:48:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvhn9s",
                  "author": "little___mountain",
                  "text": "My pleasure. There's no finding cheap ram, but the used GPU market is still pretty good (for now). I've heard tales of some people circumventing the RAM pricing by buying old server GPUs like the Nvidia Telsa which is on amazon for 32GBs for $400, since VRAM can hold context too. Those cards come with other headaches though... Anyways good luck to you and Merry Christmas.",
                  "score": 2,
                  "created_utc": "2025-12-25 14:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr16dr",
          "author": "Kindly_Initial_8848",
          "text": "llm's consume more GPU, yours is pretty much on the spot for the number of parameters.\n\n  \ntry LM studio, its easier to use on windows",
          "score": 5,
          "created_utc": "2025-12-24 17:54:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw7rds",
              "author": "salty_salad13",
              "text": "Thankyou!, i just installed lm studio and really liking it right now, the simplicity is on spot, and it recommended me the gpt-oss-20b right away, and it run smoothly, tbh i don't think it'll run smoothly, but it does hahah. Though i noticed that it does not references other chats, but its a another topic to be learn along the way",
              "score": 1,
              "created_utc": "2025-12-25 17:18:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwaspf",
                  "author": "Kindly_Initial_8848",
                  "text": "im glad this information helped you\n\nlm studio may have a plugin or configuration to setup memory across chats, but im not sure this information is correct, but you can use the llm to help you",
                  "score": 2,
                  "created_utc": "2025-12-25 17:36:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr7p9y",
          "author": "HumanDrone8721",
          "text": "I've just finished a new build, nothing spectacular just an I7-14KF, 128GB DDR5-5200 and an RTX 4090, and feeling optimistic I've tested a model with different degree of RAM contribution and got these results: \n\n    | % layers on GPU | `ngl` | `tg128` (tok/s) | vs full GPU |  slowdown vs full |\n    | --------------- | ----: | --------------: | ----------: | ----------------: |\n    | 0%              |     0 |       **16.07** |       ~8.3% |  **≈ 12× slower** |\n    | 25%             |    13 |       **23.08** |      ~11.9% | **≈ 8.4× slower** |\n    | 50%             |    26 |       **32.55** |      ~16.8% |   **≈ 6× slower** |\n    | 75%             |    39 |       **52.60** |      ~27.2% | **≈ 3.7× slower** |\n    | 100%            |   999 |      **193.66** |        100% |          baseline |\n\nThat was the script:\n\n    cd ~/Projects/llama.cpp/build/bin\n\n    MODEL=~/Nemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf\n\n    for NGL in 0 13 26 39 999; do\n        echo \"==== ngl = $NGL ====\"\n        ./llama-bench \\\n          -m \"$MODEL\" \\\n          -ngl $NGL \\\n          -t 16 \\\n          -p 4096\n    done\n\nSo I would say, use whatever model fits your interest, it just have to fit in the VRAM.",
          "score": 3,
          "created_utc": "2025-12-24 18:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrk7uy",
              "author": "Brilliant-Ice-4575",
              "text": "What do you think is better: Strix Halo with 96GB of VRAM or 4090 with 24GB of VRAM? They cost about the same...",
              "score": 2,
              "created_utc": "2025-12-24 19:38:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrqksa",
                  "author": "HumanDrone8721",
                  "text": "Do you really want to start a holly war on Christmas eve ? Because sincerely this is how you do it. Anyways, **for the models that fits 100% in VRAM** the 4090 will win hands down, for models that doesn't fit in the 24GB VRAM, Strix wins hands down, even if less speedy than a dedicate Nvidia GPU, is still very fast compared with any type of simple system RAM. So if your interests doesn't include CUDA stack learning and research and you can live with a \"smarter\" model even if is not a \"fast talker\" go with the Strix.",
                  "score": 3,
                  "created_utc": "2025-12-24 20:15:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu0q5m",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-25 06:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvunl6q",
              "author": "salty_salad13",
              "text": "Thankyouu, im considering switching to lm studio rn to lessen the overhead layers, and free up some headroom in ram and storage.",
              "score": 1,
              "created_utc": "2025-12-25 10:03:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu8ljv",
          "author": "Witty-Development851",
          "text": "LM Studio )",
          "score": 2,
          "created_utc": "2025-12-25 07:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvr8h3l",
          "author": "Just3nCas3",
          "text": "Focus on mix of expert models, just for fun see if you can run Qwen3-30B-A3B-Thinking-2507-unsloth-MagicQuant-Hybrid-GGUF at mxfp4 quant is around 18gb I think it might be the close to the max model you can run. The other person right lm studio has a great starter ui. I think gbt oss 20B is moe so I'd try that next if qwen doesn't fit. Don't be afraid of low quants but avoid dropping under Q4 or equivalent. Anything quanted by unsloth is a good starting point.",
          "score": 2,
          "created_utc": "2025-12-24 18:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvumcg0",
              "author": "salty_salad13",
              "text": "Tnx, I'll look into it",
              "score": 3,
              "created_utc": "2025-12-25 09:49:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvw84q3",
          "author": "salty_salad13",
          "text": "Hi guys, thankyouu for all of your valuable feedback, i learned a lot hehe, my next project will be cross reference chat, if you guys have any tips and tricks feel free to drop, thankyou!",
          "score": 1,
          "created_utc": "2025-12-25 17:20:59",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzsdsy",
      "title": "Suggest a model for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "author": "Bright_Dot113",
      "created_utc": "2025-12-30 20:10:25",
      "score": 18,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "Hello, I have 9950x3d with 64GB RAM and 5070 ti \n\nI recently installed LM Studio, which models do you suggest based on my hardware for the following purposes. \n\n1. Code in python and rust \n\n2. DB related stuff like optimising queries or helping me understand them. (Postgresql)\n\n3. System and DB design.\n\nAlso what other things can I do?\nI have heard lot about MCP servers but I didn't find any MCP servers useful or anything related to my workflow if you have any suggestions that would be great! ",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwsjtel",
          "author": "SimplyRemainUnseen",
          "text": "I'd suggest unsloth/Nemotron-3-Nano-30B-A3B-GGUF. You'll need to offload to system memory, but you'll have a relatively fast and intelligent model that's good at using tools.\n\nFor MCP servers check out [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)",
          "score": 12,
          "created_utc": "2025-12-30 20:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo35r",
              "author": "Big-Masterpiece-9581",
              "text": "What kind of speeds to you think he could get with this model and that configuration? Anything particular you like about that model or unsloth?",
              "score": 2,
              "created_utc": "2025-12-31 03:12:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsmq4k",
          "author": "beedunc",
          "text": "Qwen3coder, whatever fits.",
          "score": 5,
          "created_utc": "2025-12-30 20:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsooqq",
          "author": "FullstackSensei",
          "text": "I'd say try a bunch of MoE all the way up to gpt-oss-120b and see where your pain threshold is for speed. IMO, you should keep a few at hand: a smaller model like Qwen3 Coder 30B, Nemotron 30B, gpt-oss-20b, Devstral 2 24B as daily drives, and larger models like gpt-oss-120b, GLM 4.5 air, Devstral 2 123B (Q4), etc for when the smaller models get stuck or can't solve whatever issue you have.",
          "score": 6,
          "created_utc": "2025-12-30 20:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtu8zi",
          "author": "No-Consequence-1779",
          "text": "Qwen3-coder-30b. The largest quant you can run. Dense model is good or moe.  Keep in mind coder specific models are specialized for coding. Many like oss 120b though that size is not necessary. ",
          "score": 5,
          "created_utc": "2025-12-31 00:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt434d",
          "author": "Toastti",
          "text": "If you add another 64gb of ram you can run a Q2 quant of Minimax m2.1. it will probably be 7tk/s or so but it is almost certainly the smartest agentic coding model you can reasonably run",
          "score": 3,
          "created_utc": "2025-12-30 22:03:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt53sn",
              "author": "Your_Friendly_Nerd",
              "text": "q2? isn‘t that gonna suck?",
              "score": 2,
              "created_utc": "2025-12-30 22:08:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtv56k",
                  "author": "Worried_Goat_8604",
                  "text": "No unsloth dynamic quant v2 usually dosnt reduce that much quality",
                  "score": 1,
                  "created_utc": "2025-12-31 00:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwugr2q",
              "author": "Individual_Gur8573",
              "text": "I agree I found minimax 2.1 IQ2_M the smartest model after GLM 4.5 air ",
              "score": 1,
              "created_utc": "2025-12-31 02:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwuo8ix",
                  "author": "Karyo_Ten",
                  "text": "GLM Air which quant?",
                  "score": 1,
                  "created_utc": "2025-12-31 03:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwt5rhp",
          "author": "Level_Wolverine_141",
          "text": "I have the same system as you except I've got a 5080, and I'm just using Claude code max and it's pretty good.",
          "score": -2,
          "created_utc": "2025-12-30 22:11:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxn4nh",
      "title": "GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fd7m7g23ww9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-28 09:17:24",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxn4nh/glm_47_is_now_the_1_open_source_model_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwcbfj3",
          "author": "arousedsquirel",
          "text": "lots of promo and even more ccp gaurdrails. i think zai interpreted the 2000 question list to strict.",
          "score": 7,
          "created_utc": "2025-12-28 10:04:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw1ukl",
      "title": "Is there a True Deep Research / MCP Deep Research for Local AI ?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pw1ukl/is_there_a_true_deep_research_mcp_deep_research/",
      "author": "iwannaredditonline",
      "created_utc": "2025-12-26 11:07:30",
      "score": 12,
      "num_comments": 16,
      "upvote_ratio": 0.93,
      "text": "Hey guys,\n\nComing from platforms like Chatgpt and Gemini, being able to ask any question about a business, brand, platform, entity or news is very easy and it seems their AI models are up to date with the latest information.\n\nUsing software such as LM Studio and Clara verse, are there any quality and error free Deep Research MCP's that work without using a search engine's API Keys? I've tried Playwright (which doesnt seem to be the correct software unless I am misinformed), duckduckgo mcp, sear xng and the old MCP project puppeteer. I couldnt get the last to work. Ive gotten duckduckgo mcp from the LM studio repo to work, but I feel like these mcp's are just a hit or miss..\n\nIs there really any way to make this work efficiently without errors or issues, and similar to other AI Platforms that thoroughly research upon request?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw1ukl/is_there_a_true_deep_research_mcp_deep_research/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw0c5h3",
          "author": "hlacik",
          "text": "well i hate to bring that to you ,but gemini has built-in google search mcp, thats why it is so superior. so you are on good path using duck duck go mcp server, deep research is just multi-agent workflow, which you can implemenet using tools like langgraph...",
          "score": 6,
          "created_utc": "2025-12-26 11:19:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3nwpt",
              "author": "iwannaredditonline",
              "text": "Thanks for this. I figured this would be the case. Any direction for tutorials on how to put it all together?",
              "score": 1,
              "created_utc": "2025-12-26 23:10:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5zsgk",
                  "author": "hlacik",
                  "text": "https://github.com/langchain-ai/open_deep_research",
                  "score": 2,
                  "created_utc": "2025-12-27 09:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0c8ci",
          "author": "brianlmerritt",
          "text": "Are you trying to save money or do you have a specific requirement?\n\nFirst of all, the paid tools are not error free and are not up to date with the latest information.  They are pretty good and pretty recent, but they often have to use search to find the latest information about fast moving topics.\n\nThere are local AI research tools, most of them require some search capability which often requires a key above a certain usage.  So if you are doing a lot of research then some cost is going to come into the equation or else your research will likely be constrained.  \n  \nps - Playwright is a browser automation tool - you can use it to perform searches but even then it will be using some search engine and there are much more efficient ways to conduct search / research.",
          "score": 4,
          "created_utc": "2025-12-26 11:20:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4h281",
              "author": "No-Consequence-1779",
              "text": "What other ways are more efficient for research please?  ",
              "score": 1,
              "created_utc": "2025-12-27 02:09:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7xzlf",
              "author": "iwannaredditonline",
              "text": "Host everything locally instead of signing up for subscription services. Purchased an entire pc for AI not to just go back to 3rd party platform controlling your data or access to data.",
              "score": 1,
              "created_utc": "2025-12-27 17:26:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8asoe",
                  "author": "brianlmerritt",
                  "text": "I have been shot down in flames (reddit style, not irl) for suggesting buying a gaming pc with RTX 3090. It keeps my office warm, I do also play games, it's ready for any AI tasks too.\n\nI find the search part of research is the bit that needs API keys to really get everything going, and if you use a generic search engine with a browser running it you are of course sharing whatever you are looking for with the search engine company anyways.",
                  "score": 1,
                  "created_utc": "2025-12-27 18:30:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw10adw",
          "author": "Vegetable-Second3998",
          "text": "Firecrawl and Tavily both have free tiers and search agents.",
          "score": 3,
          "created_utc": "2025-12-26 14:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2gndw",
          "author": "Purple-Programmer-7",
          "text": "I’ve been using https://github.com/assafelovic/gpt-researcher.\n\nHe’s got an mcp version of it too.",
          "score": 2,
          "created_utc": "2025-12-26 19:11:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3cd2t",
              "author": "iwannaredditonline",
              "text": "Thanks for this. Does it require API keys for search platforms? I didn't see an option to use it locally without api keys",
              "score": 1,
              "created_utc": "2025-12-26 22:04:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3cyus",
                  "author": "Purple-Programmer-7",
                  "text": "Per codewiki.google (literally copy/pasta):\n\n````markdown\n\nYes, you can initialize GPT Researcher to use with a local model by configuring it to use [`ollama`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L101) or [`huggingface`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L221) providers.\n\nThe [`GenericLLMProvider`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py#L89) class in [`gpt_researcher/llm_provider/generic/base.py`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fsupported-llms.md#L30) supports several LLM providers, including:\n*   [`ollama`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L101): This provider is initialized using [`ChatOllama`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py#L143) and requires [`OLLAMA_BASE_URL`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fmemory%2Fembeddings.py#L91) to be set in the environment.\n*   [`huggingface`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fllms%2Fllms.md#L221): This provider is initialized using [`ChatHuggingFace`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py#L158).\n\nTo use one of these local model providers, you need to set the [`FAST_LLM`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fconfig%2Fconfig.py#L97) and/or [`SMART_LLM`](%2Fassafelovic%2Fgpt-researcher%2Fmulti_agents%2FREADME.md#L11) environment variables (or similar configuration in a custom [`config.json`](%2Fassafelovic%2Fgpt-researcher%2Fdocs%2Fdocs%2Fgpt-researcher%2Fgptr%2Fconfig.md#L17) file) to specify the provider and model. For example:\n\n```bash\nexport FAST_LLM=\"ollama:your-local-ollama-model-name\"\nexport OLLAMA_BASE_URL=\"http://localhost:11434\" # Or your Ollama server address\n```\n\nor for HuggingFace:\n\n```bash\nexport FAST_LLM=\"huggingface:your-local-huggingface-model-id\"\n```\n\nThe [`Config`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fconfig%2Fconfig.py#L11) class in `gpt_researcher/config/config.py` parses these LLM strings to determine the provider and model, as detailed in [LLM Configuration and Management](#language-model-integration-llm-configuration-and-management). The [`GenericLLMProvider.from_provider`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Futils%2Fllm.py#L20) method then dynamically instantiates the appropriate Langchain chat model, handling API keys and base URLs (often from environment variables), as explained in [Generic LLM Provider Interface](#language-model-integration-generic-llm-provider-interface). The system also automatically checks for and installs necessary [`langchain`](%2Fassafelovic%2Fgpt-researcher%2FREADME.md#L242) integration packages like [`langchain_ollama`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fmemory%2Fembeddings.py#L87) or [`langchain_huggingface`](%2Fassafelovic%2Fgpt-researcher%2Fgpt_researcher%2Fmemory%2Fembeddings.py#L107) if they are not already present, as described in [Chat Logging and Dependency Management](#language-model-integration-chat-logging-and-dependency-management).\n````",
                  "score": 2,
                  "created_utc": "2025-12-26 22:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4h4lo",
          "author": "Beautiful_Frosting34",
          "text": "https://github.com/PrabhaAnde/deep-research-agent\n\nTry checking this out , it has well documented details that can run with docker containers where in your local ai llm can be run through docker container too.  It was built with super basic custom ui and has financial deep research along duty regular query based deep research. \n\nIf you any any additional features added such as better ui ux or any other features , I can plan to add them. \n\nAlso. It need a serpapi key.  You can create a simple account in serpapi and then create a key to use. It has 200 searches free for month and has comprehensive search api options.  I can change them if you need better search apis.\n\nhttps://serpapi.com\n\n\nOfcourse if you are developer,  you are free to modify this and use according to your wish",
          "score": 2,
          "created_utc": "2025-12-27 02:10:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwyu6k",
      "title": "I built a GraphRAG application to visualize AI knowledge (Runs 100% Local via Ollama OR Fast via Gemini API)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pwyu6k/i_built_a_graphrag_application_to_visualize_ai/",
      "author": "Dev-it-with-me",
      "created_utc": "2025-12-27 14:16:42",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "Hey everyone,\n\nFollowing up on my last project where I built a standard RAG system, I learned a ton from the community feedback.\n\nWhile the local-only approach was great for privacy, many of you pointed out that for **GraphRAG** specifically—which requires heavy processing to extract entities and build communities—local models can be slow on larger datasets.\n\nSo, I decided to level up. I implemented **Microsoft's GraphRAG** with a flexible backend. You can run it **100% locally** using Ollama (for privacy/free testing) **OR** switch to the **Google Gemini API** with a single config change if you need production-level indexing speed.\n\nThe result is a chatbot that doesn't just retrieve text snippets but understands the *structure* of the data. I even added a visualization UI to actually *see* the nodes and edges the AI is using to build its answers.\n\nI documented the entire build process in a detailed tutorial, covering the theory, the code, and the deployment.\n\n**The full stack includes:**\n\n* **Engine:** Microsoft GraphRAG (official library).\n* **Dual Model Support:**\n   * *Local Mode:* Google's Gemma 3 via **Ollama**.\n   * *Cloud Mode:* **Gemini API** (added based on feedback for faster indexing).\n* **Graph Store:** LanceDB + Parquet Files.\n* **Database:** PostgreSQL (for chat history).\n* **Visualization:** React Flow (to render the knowledge graph interactively).\n* **Orchestration:** Fully containerized with Docker Compose.\n\n**In the video, I walk through:**\n\n* **The Problem:**\n   * Why \"Classic\" RAG fails at reasoning across complex datasets.\n   * What path leads to Graph RAG → throuh Hybrid RAG\n* **The Concept:** A visual explanation of Entities, Relationships, and Communities & What data types match specific systems.\n* **The Workflow:** How the system indexes data into a graph and performs \"Local Search\" queries.\n* **The Code:** A deep dive into the Python backend, including how I handled the switch between local and cloud providers.\n\n**You can watch the full tutorial here:**\n\n[https://youtu.be/0kVT1B1yrMc](https://youtu.be/0kVT1B1yrMc)\n\n**And the open-source code (with the full Docker setup) is on GitHub:**\n\n[https://github.com/dev-it-with-me/MythologyGraphRAG](https://github.com/dev-it-with-me/MythologyGraphRAG)\n\nI hope this hybrid approach helps anyone trying to move beyond basic vector search. I'm really curious to hear if you prefer the privacy of the local setup or the raw speed of the Gemini implementation—let me know your thoughts!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pwyu6k/i_built_a_graphrag_application_to_visualize_ai/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw70wy5",
          "author": "Zamiatacz",
          "text": "GitHub ling have 2 ** at the end what causes 404",
          "score": 1,
          "created_utc": "2025-12-27 14:30:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw70zya",
              "author": "Zamiatacz",
              "text": "Looking great btw",
              "score": 1,
              "created_utc": "2025-12-27 14:31:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw729ih",
              "author": "Dev-it-with-me",
              "text": "Thank You! Fixed :D",
              "score": 1,
              "created_utc": "2025-12-27 14:38:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw70r5t",
          "author": "Conscious-Pool8744",
          "text": "I really appreciate the clear project on which I can base further development!",
          "score": 0,
          "created_utc": "2025-12-27 14:29:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1px1atx",
      "title": "My stack for cleaning RAG datasets: A comparison of Unstructured, LangChain, and a custom local approach (results inside)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1px1atx/my_stack_for_cleaning_rag_datasets_a_comparison/",
      "author": "Low-Flow-6572",
      "created_utc": "2025-12-27 16:04:51",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "Hey everyone,\n\nI've been iterating on a local RAG pipeline for documentation search, and the biggest bottleneck wasn't the LLM inference speed, it was the retrieval quality. I realized my vector store was full of duplicate chunks, boilerplate legalese, and \"low-entropy\" garbage (like 500 copies of a copyright footer).\n\nI spent the last two weeks testing different tools to clean the data *before* embedding. Here is my honest breakdown of the landscape, from \"Heavyweight\" to \"Lightweight\".\n\n# 1. The Heavyweight: [Unstructured.io](http://Unstructured.io)\n\nThis is the go-to for parsing weird formats.\n\n* **Pros:** Incredible at ripping text out of complex PDFs and tables. If you have messy source files, start here.\n* **Cons:** It is HEAVY. The dependencies are massive, and processing time can be slow.\n* **Verdict:** Essential for ingestion/parsing, but overkill if you just need to clean/deduplicate JSONL or plain text.\n\n# 2. The Default: LangChain (RecursiveSplitter + Regex)\n\nWhat 90% of tutorials use.\n\n* **Pros:** Built-in, zero extra setup.\n* **Cons:** It's \"dumb\" slicing. It doesn't check for semantic duplicates. If you have the same paragraph on page 5 and page 50, both go into your Vector DB, polluting the search results.\n* **Verdict:** Good for prototyping, bad for production quality.\n\n# 3. The Enterprise Scale: HuggingFace text-dedup\n\nUsed for training massive datasets (like The Pile).\n\n* **Pros:** Uses MinHash LSH + Spark. Extremely scalable for terabytes of data.\n* **Cons:** Overkill for a local RAG setup. Setting up a Spark cluster just to clean a 2GB dataset is painful.\n* **Verdict:** Great for pre-training models, too complex for RAG pipelines.\n\n# 4. The \"Middle Ground\": EntropyGuard (My Local Project)\n\nI couldn't find a tool that was **semantic** (like embeddings) but **lightweight** (runs on a laptop), so I built a CLI tool using Polars and FAISS.\n\n* **The Approach:** It uses a hybrid pass. First, `xxhash` removes exact duplicates (fast). Then, it uses a small sentence-transformer model to find semantic duplicates (e.g., \"Error 500\" vs \"Server Error 500\") and removes them based on vector distance.\n* **Pros:**\n   * Runs locally (no API costs).\n   * Uses **Polars LazyFrames**, so it handles datasets larger than RAM (I processed 65k docs on 16GB RAM without OOM).\n   * Filters out \"low entropy\" chunks (repetitive noise).\n* **Cons (Being honest):**\n   * CLI only (no GUI).\n   * Currently optimized for English (multilingual is experimental).\n   * Docs are still a work in progress compared to LangChain.\n\n# My Question for you:\n\nI'm currently betting on **semantic deduplication** (checking meaning) rather than just regex cleaning.\n\n**What is your strategy for \"dirty\" data in RAG?** Do you just throw everything into Pinecone/Chroma and hope the re-ranker sorts it out, or do you have a specific pre-processing pipeline?\n\n*Full disclosure: I am the maintainer of tool #4 (EntropyGuard). I built it because I kept OOMing my laptop with custom Pandas scripts. If you want to check the code or roast my implementation:* [https://github.com/DamianSiuta/entropyguard](https://github.com/DamianSiuta/entropyguard)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1px1atx/my_stack_for_cleaning_rag_datasets_a_comparison/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwexye6",
          "author": "mtbMo",
          "text": "Could this being used and automate with N8n? I’m trying to use the rag features in open webui and it’s unstable.",
          "score": 1,
          "created_utc": "2025-12-28 19:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfaty9",
              "author": "Low-Flow-6572",
              "text": "When it comes to EntropyGuard:\nShort answer: Yes, absolutely, but right now you need to run n8n in self-hosted mode (Docker).\nSince EntropyGuard is currently CLI-only, you can't hit it via HTTP yet. You have to orchestrate it using the Execute Command node.\nThe Workflow (Current):\n1. Setup: Add RUN pip install entropyguard to your n8n Dockerfile (so the binary is inside the container).\n2. n8n Node: Use the Execute Command node.\n• Command: entropyguard clean --input /data/raw.jsonl --output /data/clean.jsonl\n3. Open WebUI: Then have n8n push that clean JSONL into your vector store.\nThe Roadmap (Future):\nI realize hacking Dockerfiles is annoying for low-code workflows. I’m planning to add an optional API mode (something like entropyguard serve) in the next release. That way, you'll be able to just use the standard HTTP Request node in n8n/Flowise/Zapier to clean text batches on the",
              "score": 2,
              "created_utc": "2025-12-28 20:40:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8gjcj",
          "author": "Crafty_Ball_8285",
          "text": "What is RAG? You didn’t define it anywhere.",
          "score": 0,
          "created_utc": "2025-12-27 18:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8hmon",
              "author": "Low-Flow-6572",
              "text": "Fair point! My bad for jumping straight into the weeds. **RAG (Retrieval-Augmented Generation)** is basically giving an LLM a 'search engine' or a 'private library' to look at before it answers your question.\n\nInstead of relying only on what it learned during training, the model searches your documents, finds the relevant parts, and uses them as context. The problem I'm describing in the post is that if that 'search' returns a bunch of garbage/noise along with the answer, small local models get confused. That's where the cleaning (and EntropyGuard) comes in.",
              "score": 1,
              "created_utc": "2025-12-27 19:04:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pv5ar7",
      "title": "Built a Rust CLI for converting SafeTensors to GGUF. Looking for feedback.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pv5ar7/built_a_rust_cli_for_converting_safetensors_to/",
      "author": "Anxious-Visit-7735",
      "created_utc": "2025-12-25 04:32:28",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "[https://github.com/kgrama/gmat-cli](https://github.com/kgrama/gmat-cli)\n\nWhy i built it,no waiting around for quants.Also saves space, 1 f32/bf16 model download = n quants\n\nKey features:\n\n\\- CPU-only, no GPU required\n\n\\- Per-tensor quant overrides (Q8\\_0 embeddings, Q4\\_K\\_M elsewhere)\n\n\\- Trellis optimization (\\~1% perplexity gain)\n\n\\- IQ4\\_XS/IQ4\\_NL support\n\n\n\nArchitectures:\n\n Llama, Qwen, Phi, Gemma, DeepSeek, Mistral, LLaVA, Qwen-VL, Kimi-VL, Whisper, T5\n\n\n\nWhy not llama.cpp directly?\n\nOne GMAT folder → multiple GGUF outputs. Change quant (json file)  profile without re-processing.\n\n\n\nFeedback wanted: Missing quant types? Architectures to add? ",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pv5ar7/built_a_rust_cli_for_converting_safetensors_to/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvvnb66",
          "author": "Hurricane31337",
          "text": "Nice one, thanks for sharing! I could need support for GLM 4.6, 4.7 and MiniMax 2.0 🙂",
          "score": 1,
          "created_utc": "2025-12-25 15:14:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw8kj0",
              "author": "Anxious-Visit-7735",
              "text": "Thats WIP, will probably be in the  new year.",
              "score": 1,
              "created_utc": "2025-12-25 17:23:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1py9m5q",
      "title": "Requested: Yet another Gemma 3 12B uncensored",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "author": "Mabuse046",
      "created_utc": "2025-12-29 02:09:36",
      "score": 10,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "Hello again!\n\nYesterday I released my norm preserved biprojected abliterated Gemma 3 27B with the vision functions removed and further fine tuned to help reinforce the neutrality. I had a couple of people ask for the 12B version which I have just finished pushing to the hub. I've given it a few more tests and it has given me an enthusiastic thumbs up to some really horrible questions and even made some suggestions I hadn't even considered. So... use at your own risk.\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF)\n\nLink to the 27B redit post:  \n[Yet another uncensored Gemma 3 27B](https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using  **L**ayerwise **I**mportance **S**ampled **A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwihhi4",
          "author": "darkbit1001",
          "text": "I ran with ollama (ollama run hf.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF:Q4\\_K\\_M) and it just repeats over and over the word 'model'. any reason this would happen?",
          "score": 3,
          "created_utc": "2025-12-29 08:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjayep",
              "author": "Mabuse046",
              "text": "Thank you for pointing this out. I'm looking into it and finding there were apparently some configuration issues in the original Google models, particularly in the way they handled the BOS token that have given some ollama users a headache with Gemma 3 GGUF's. I am currently editing my config.json files and adding the chat template in three different places on both models based on the Unsloth fix and will push fresh gguf's shortly.",
              "score": 3,
              "created_utc": "2025-12-29 12:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwugiab",
                  "author": "lookwatchlistenplay",
                  "text": "This isn't the model escaping confinement... is it?",
                  "score": 1,
                  "created_utc": "2025-12-31 02:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjs5nv",
              "author": "Mabuse046",
              "text": "Fresh ggufs have been pushed and the original transformers versions have been updated. I don't normally use ollama but I went ahead and installed it to try it out. I used the run command with the hf repo and it chatted just fine in the terminal. I connected to it in SillyTavern to give it another test and it took some fiddling but I got it to hold a conversation just fine in there in both Chat Completions and Text Completions mode.",
              "score": 2,
              "created_utc": "2025-12-29 14:27:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh2ymo",
          "author": "3-goats-in-a-coat",
          "text": "I'll try using it with EchoColony in rimworld. Thanks.",
          "score": 1,
          "created_utc": "2025-12-29 02:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqmooc",
          "author": "Dramatic-Rub-7654",
          "text": "If it’s not a bother and if you’re able to, could you do the same with one of TheDrummer’s versions? TheDrummer/Fallen-Gemma3-27B-v1 or TheDrummer/Fallen-Gemma3-12B-v1.",
          "score": 1,
          "created_utc": "2025-12-30 15:00:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrljfp",
              "author": "Mabuse046",
              "text": "I'll have a look at it. Currently have my system working on beefing up my dataset. Should have some free time shortly.",
              "score": 2,
              "created_utc": "2025-12-30 17:46:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrpr1x",
                  "author": "Legal_Pudding_4464",
                  "text": "I would second this request, but regardless thanks for this model!",
                  "score": 1,
                  "created_utc": "2025-12-30 18:05:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwukk76",
                  "author": "Dramatic-Rub-7654",
                  "text": "Thanks a lot, no rush at all. When you manage to publish it, please give me a heads-up. In my case, I’m only interested in the text layers, so if you remove the vision part, that’s totally fine with me.",
                  "score": 1,
                  "created_utc": "2025-12-31 02:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pxsd5j",
      "title": "Nvidia Quadro RTX 8000 Passive 48 GB, 1999€ - yes or no ?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxsd5j/nvidia_quadro_rtx_8000_passive_48_gb_1999_yes_or/",
      "author": "HumanDrone8721",
      "created_utc": "2025-12-28 14:16:36",
      "score": 9,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "Hello, I was looking at these guys: https://www.ebay.de/itm/116912918050 and considering getting one or two. My question for the people who have experience with them: are they worth buying for a local setup, they are passively cooled, does one need some special air ducts for them in an open frame case, could they even be used in a normal case (two pieces) ? \n\nPlease help a poor with no experience with professional GPUs.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxsd5j/nvidia_quadro_rtx_8000_passive_48_gb_1999_yes_or/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdb5xz",
          "author": "Mr_Moonsilver",
          "text": "Bad idea, you get a rtx pro 5000, latest gen with 48Gb for about €3.5k\n\nOthers here have stated correctly the architecture and quantization support is the biggest risk",
          "score": 9,
          "created_utc": "2025-12-28 14:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd8qvz",
          "author": "ResidentPositive4122",
          "text": "Not worth it, IMO. Considering that it's a 7yo arch at this point, you're probably gonna hit a lot of compatibility issues soon (if not already). Ampere is the oldest arch I'd go for today. As for the price, I would say not even A6000s are worth 2k second-hand, as they're starting to show their age (no fp8 native, etc).",
          "score": 11,
          "created_utc": "2025-12-28 14:30:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd96ii",
              "author": "HumanDrone8721",
              "text": "Well, even the big GPTs say the same, the question is: what is around this price (eventually +50-60%) for 48GB that is available not in a back alley in China, but in EU ? Any such unicorn ?",
              "score": 1,
              "created_utc": "2025-12-28 14:33:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdlzrm",
                  "author": "eleqtriq",
                  "text": "Two 3090s",
                  "score": 7,
                  "created_utc": "2025-12-28 15:44:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwda7x8",
          "author": "Lxzan",
          "text": "I have no experience with Rtx 8000 but can answer few questions. Cooling is totally manageable but you will either need some 3d printed bracket for it, (look for ‘amd mi50 fan bracket’ for ideas) or some server case with installed coolers that could create some air pressure. As for is it worth buying - for me it seems too much for a price, you will get better performance from 4x 3090 or 2x mi50 32gb (but amd is a lot more tricky to set up and some libraries do not work out of the box)",
          "score": 2,
          "created_utc": "2025-12-28 14:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdd3i5",
          "author": "FullstackSensei",
          "text": "Cooling is easy if you have the most rudimentary 3D design skills. If not, as has been pointed out, just search for a Mi50 duct. I designed a duct for my Mi50s to cool each pair with an 80mm fan, and they run very cool without sounding like a jet engine.\n\nNow, at 2k a pop, I don't think the value preposition is there. If you're in Germany you can already find 3090s for ~€600 on kleinanzeigen, less if you're willing to buy watercooled ones. So, you can get four cards for the same 96GB VRAM for 25% more than the price of one card. While 4x24 does not equal 2x48 in practice, the 3090 has so much more compute, and considerably more memory bandwidth that it's not even a competition with Turing.\n\nBTW, I've seen the active Quadro RTX 8000 on kleinanzeigen sell for a little over 1k. They don't pop up often, but if you really need 96GB VRAM in two cards, and have a bit of patience, you could get a pair for under €2500, though I still think 4x3090s are better.",
          "score": 2,
          "created_utc": "2025-12-28 14:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdf1zu",
              "author": "HumanDrone8721",
              "text": "> Quadro RTX 8000 a little over 1K\n\nWell you definitely have better eyes than me, I see exactly two listings all over Germany, this:\n\nhttps://www.kleinanzeigen.de/s-anzeige/nvidia-quadro-rtx-8000-48gb-gddr6-profi-gpu-fuer-ki-3d-rend/3243167517-225-7983 and this:\n\nhttps://www.kleinanzeigen.de/s-anzeige/nvidia-rtx-8000-48gb-aktiv-/3265840025-225-19640 \n\n\nOr you were meaning that they may show sometime again, I'm afraid this train left the station, but I can totally put an alarm in case some wonder happens and is not one of those relentless scammers.",
              "score": 1,
              "created_utc": "2025-12-28 15:07:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdjmaw",
                  "author": "FullstackSensei",
                  "text": "You know that items with a good price sell quickly, and what you see usually when you do a sporadic search is overpriced items that don't sell. I have a saved search for Nvidia 48gb for about a year and I see one pop up every month to six weeks and sell within a day or two. Sure there might be some scams, but I've seen plenty offering local pick up.\n\nI have a deal to go pick up a RTX 2000 Ada in a few days for less than half the price you see if you search for that card now. It's already reserved for me, so you won't see it if you search now.",
                  "score": 2,
                  "created_utc": "2025-12-28 15:32:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdg56l",
          "author": "twjnorth",
          "text": "I am still building a rig but just getting started with ai (threadripper 3975wx, wrx80e mob so 7x pcie4 x16 slots).\n\nI bought 4x rtx 3090 and got two quadro 6000 (24gb) with nvlink, all second hand from eBay so will be setting that up in the WRX80E with proxmox and GPU pass thru to see how each performs.\n\nThe quadro 8000 has same specs as the quadro 6000 but just double the VRAM.\n\nGot the first quadro 6000 24GB as it was the best card I could run in my HP Z2 G5 according to manual. Then started looking at what I needed for a better setup.",
          "score": 1,
          "created_utc": "2025-12-28 15:13:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdts2i",
              "author": "Otherwise-Way1316",
              "text": "Interested to hear what performance is for this set up and what models run comfortably in it.",
              "score": 1,
              "created_utc": "2025-12-28 16:24:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwelmwy",
          "author": "KySiBongDem",
          "text": "Unless you want to do 3D modeling, it is a bad idea. For AI, get the a newest Blackwell GPU.",
          "score": 1,
          "created_utc": "2025-12-28 18:39:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwemi35",
              "author": "HumanDrone8721",
              "text": "I'll absolutely do so if I'm getting the End of Year lottery :).",
              "score": 1,
              "created_utc": "2025-12-28 18:43:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nweo61v",
                  "author": "KySiBongDem",
                  "text": "There are multiple version - you may go with 4500 or 4000 instead of 5000. I know they have less memory but with the 48GB but the weak/outdated GPU will worth that €2,000.",
                  "score": 1,
                  "created_utc": "2025-12-28 18:51:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfjjwo",
          "author": "CodeSlave9000",
          "text": "Short opinion: Too expensive for a 7yr/old architecture.  I have one of the blower versions, and for inference the performance isn't bad - Compute is certainly lower than Ampere (30x/Axxxx cards) but memory bandwidth is still good.  This mostly shows up as slower prompt processing, but actual generation is about 2x the RTX 4060 Ti.",
          "score": 1,
          "created_utc": "2025-12-28 21:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwemus7",
          "author": "FireWoIf",
          "text": "Turing has poor support unfortunately. I’ve tried and found much difficulty using this generation. Ampere should be your minimum goto. If you’re dead set on these, a blower fan should be good for these. Don’t use normal fans with a duct as these need (very) high air pressure to cool.",
          "score": 1,
          "created_utc": "2025-12-28 18:45:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweo99q",
              "author": "HumanDrone8721",
              "text": "I'm not dead set on anything, I was (naively) thinking maybe there are some semi-hidden gems, but it seems that one has to play to stay on the big boys table.",
              "score": 1,
              "created_utc": "2025-12-28 18:51:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1px0vas",
      "title": "Run GPUs on a Pi without a PC",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1px0vas/run_gpus_on_a_pi_without_a_pc/",
      "author": "dreamingwell",
      "created_utc": "2025-12-27 15:47:07",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[ https://youtu.be/8X2Y62JGDCo?si=MHdk8HH8npelMM\\_X ](https://youtu.be/8X2Y62JGDCo?si=MHdk8HH8npelMM_X)\n\nAn interesting project where a Pi is used to drive multiple GPUs - including running LLMs. And it runs pretty well!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1px0vas/run_gpus_on_a_pi_without_a_pc/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw8d7x9",
          "author": "FullstackSensei",
          "text": "Let me try: Jeff Geerling, Jeff Geerling, Jeff Geerling.\n\nYou can, but as the man himself says: doesn't mean you should.\n\nIt works well with multiple GPUs compared to a desktop if you split across layers, but that leaves so much performance on the table. Running llama.cpp with -sm row gives a big uplift in inference performance, but also requires a lot more bandwidth (in my experience, at least X4 for each GPU with weaker cards, or X8 for more powerful cards). The PCIe adapters with a PCIe switch also aren't cheap, negating any savings from running using a Pi.\n\nStill, it's über cool, and definitely something worth looking into if you want something that runs 24/7 and your model (or models) all fit within a single GPU.",
          "score": 4,
          "created_utc": "2025-12-27 18:42:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pygyhi",
      "title": "Local model registry to solve duplicate GGUFs across apps?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pygyhi/local_model_registry_to_solve_duplicate_ggufs/",
      "author": "tleyden",
      "created_utc": "2025-12-29 08:27:50",
      "score": 6,
      "num_comments": 23,
      "upvote_ratio": 0.88,
      "text": "I'm running into storage issues with multiple local LLM apps. I downloaded Olmo3-7B through Ollama, then wanted to try Jan.ai's UI and had to download the same 4GB model again. Now multiply this across Dayflow, Monologue, Whispering, and whatever other local AI tools I'm testing.\n\nEach app manages its own model directory. No sharing between them. So you end up with duplicate GGUFs eating disk space.\n\nFeels like this should be solvable with a shared model registry - something like how package managers work. Download the model once, apps reference it from a common location. Would need buy-in from Ollama, LMStudio, Jan, LibreChat, etc. to adopt a standard, but seems doable if framed as an open spec.\n\nI'm guessing the OS vendors will eventually bake something like this in, but that's years away. Could a community-driven library work in the meantime? Or does something like this already exist and I'm just not aware of it?\n\nCurious if anyone else is hitting this problem or if there's already work happening on standardizing local model storage.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pygyhi/local_model_registry_to_solve_duplicate_ggufs/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjo88q",
          "author": "tleyden",
          "text": "Someone posted a comment and then removed it, but their suggestion actually works pretty nicely. The gist was to use llama.cpp as a central model server - download GGUFs once, run llama.cpp with proper flags, then point all your frontend apps (Jan, etc.) to that endpoint. Solves duplication and supposedly runs faster too.\n\nIt's a bit more tedious than I'd like, but it works. I was able to download an HF model via curl and import it to Jan.ai. Going to test it with other apps that support local OpenAI-compatible endpoints or direct model import.",
          "score": 3,
          "created_utc": "2025-12-29 14:04:50",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwikd4b",
          "author": "ttkciar",
          "text": "They're just files.  You can remove duplicates yourself and replace them with symlinks to whichever copy you choose to make the \"primary\".",
          "score": 2,
          "created_utc": "2025-12-29 08:44:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwim9t0",
              "author": "tleyden",
              "text": "Call me lazy, but that sounds like way too much work.  Why can't the apps just do that consolidation for me?",
              "score": -3,
              "created_utc": "2025-12-29 09:02:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwinbw7",
                  "author": "TomatoInternational4",
                  "text": "Because they're made by different people and aren't sentient beings. Programs only know and do what we tell them to. I hope that this was a sarcastic question.",
                  "score": 4,
                  "created_utc": "2025-12-29 09:12:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuf1p4",
                  "author": "StardockEngineer",
                  "text": "Lazy AF.  😂",
                  "score": 1,
                  "created_utc": "2025-12-31 02:19:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjft1v",
          "author": "johannes_bertens",
          "text": "I've built a hf-downloader for this.  \nDoes \\*not\\* give you a registry, but \\*does\\* give you an easy way to download the files yourself to then use across multiple applications.\n\n[https://lib.rs/crates/rust-hf-downloader](https://lib.rs/crates/rust-hf-downloader)  \n[https://github.com/johannesbertens/rust-hf-downloader](https://github.com/johannesbertens/rust-hf-downloader)",
          "score": 2,
          "created_utc": "2025-12-29 13:13:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjgivh",
              "author": "tleyden",
              "text": "That looks really slick!  I will give it a spin report back on the github repo if I run into issues",
              "score": 1,
              "created_utc": "2025-12-29 13:17:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp2zcv",
          "author": "t3rmina1",
          "text": "I have all my GGUF files, hf cache, llama.cpp cache etc. on relatively fast SSDs with the same directory structures and download to the specific SSD I need using uvx hf. \n\nOn Proxmox host these are all mounted and combined into a single unified path using mergefs, and bind-mounted to my llama.cpp LXC, or any other LXCs as needed.\n\nInside the container I run llama-server's router mode and select the models I want to load from the UI: those with predefined configs will be available to load with those setups, otherwise they'll be auto detected and available to load with the default configs. \n\nAny other services then use llama-server's endpoint.\n\nI basically do the same for my hf cache.",
          "score": 2,
          "created_utc": "2025-12-30 07:56:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwimrce",
          "author": "pmttyji",
          "text": "Jan has Import option(to use downloaded GGUF files from any folder). \n\nKoboldcpp also does this just with browse GGUF option. \n\nFor Oobabooga, I used symlinks option.",
          "score": 1,
          "created_utc": "2025-12-29 09:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiosoz",
              "author": "tleyden",
              "text": "Thanks, good to know!  I found that buried in the [Jan.ai](http://Jan.ai) UI under Settings / Model Providers / Llama.cpp / import.  \n  \nThe whole process to import a model feels super clunky though.  Or maybe I'm doing it wrong?\n\n1. Figure out where ollama stores its models.  \n2. Enable hidden dirs in osx file picker  \n3. Navigate to .ollama/models/blobs  \n4. See a bunch of hashes and file sizes and try to pick the one I want\n\nhttps://preview.redd.it/rtw8gsyw24ag1.png?width=1444&format=png&auto=webp&s=ccb4e8080fc6ad6dbb490cd3aac9ac2ee736690a\n\n   \nTo me it doesn't seem \"user friendly\" at all.  I can't imagine normie users being able to deal with mapping SHA hashes and file sizes to the model they want.",
              "score": 1,
              "created_utc": "2025-12-29 09:26:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwiq34f",
                  "author": "pmttyji",
                  "text": "I use Windows. After downloading GGUF files from HuggingFace, I simply import them using Jan. I don't see any complications here\n\nhttps://preview.redd.it/x0r5j8ac54ag1.png?width=1152&format=png&auto=webp&s=177f39e8dba4bc5f8d10f5e1b8f0c76b4aa7542a\n\nI think the filenames & SHA hashes are related to Ollama. I don't use that one. I'm happy with HF.",
                  "score": 1,
                  "created_utc": "2025-12-29 09:38:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwki8rx",
                  "author": "reginakinhi",
                  "text": "That's because your models are downloaded by ollama, who deliberately split stuff across files and named only by hashes because they don't want anyone else to get use from it. Just download the gguf files themselves as every other LLM runner does.",
                  "score": 1,
                  "created_utc": "2025-12-29 16:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pw9qg6",
      "title": "Genesis-152M-Instruct — exploring hybrid attention + TTT at small scale",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pw9qg6/genesis152minstruct_exploring_hybrid_attention/",
      "author": "Kassanar",
      "created_utc": "2025-12-26 17:26:56",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "Hey everyone 👋\n\nI’m sharing **Genesis-152M-Instruct**, an **experimental small language model** built to explore how *recent architectural ideas interact* when combined in a single model — especially under **tight data constraints**.\n\n\n\nThis is **research-oriented**, not a production model or SOTA claim.\n\n\n\n\n\n🔍 **Why this might be interesting**\n\n\n\nMost recent architectures (GLA, FoX, TTT, µP, sparsity) are tested **in isolation** and usually at **large scale**.\n\nI wanted to answer a simpler question:\n\n\n\n*How much can architecture compensate for data at \\~150M parameters?*\n\n\n\nGenesis combines several **ICLR 2024–2025 ideas** into one model and evaluates the result.\n\n\n\n\n\n⚡ **TL;DR**\n\n• **152M parameters**\n\n• Trained on **\\~2B tokens** (vs \\~2T for SmolLM2)\n\n• Hybrid **GLA + FoX attention**\n\n• **Test-Time Training (TTT)** during inference\n\n• **Selective Activation (sparse FFN)**\n\n• **µP-scaled training**\n\n• Fully open-source (Apache 2.0)\n\n\n\n🤗 Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n📦 pip install genesis-llm\n\n\n\n\n\n📊 **Benchmarks (LightEval, Apple MPS)**\n\n\n\nARC-Easy     → 44.0%   (random: 25%)\n\nBoolQ        → 56.3%   (random: 50%)\n\nHellaSwag    → 30.2%   (random: 25%)\n\nSciQ         → 46.8%   (random: 25%)\n\nWinogrande   → 49.1%   (random: 50%)\n\n\n\n**Important context:**\n\nSmolLM2-135M was trained on **\\~2 trillion tokens**.\n\nGenesis uses **\\~2 billion tokens** — so this is not a fair head-to-head, but an exploration of **architecture vs data scaling**.\n\n\n\n\n\n🧠 **Architecture Overview**\n\n\n\n**Hybrid Attention (Qwen3-Next inspired)**\n\n\n\n**Layer** **%** **Complexity** **Role**\n\nGated DeltaNet (GLA) 75% O(n) Long-range efficiency\n\nFoX (Forgetting Attention) 25% O(n²) Precise retrieval\n\n\n\nGLA uses:\n\n• Delta rule memory updates\n\n• Mamba-style gating\n\n• L2-normalized Q/K\n\n• Short convolutions\n\n\n\nFoX adds:\n\n• Softmax attention\n\n• Data-dependent forget gate\n\n• Output gating\n\n\n\n\n\n**Test-Time Training (TTT)**\n\n\n\nInstead of frozen inference, Genesis can **adapt online**:\n\n• Dual-form TTT (parallel gradients)\n\n• Low-rank updates (rank=4)\n\n• Learnable inner learning rate\n\n\n\nPaper: *Learning to (Learn at Test Time)* (MIT, ICML 2024)\n\n\n\n\n\n**Selective Activation (Sparse FFN)**\n\n\n\nSwiGLU FFNs with **top-k activation masking** (85% kept).\n\nCurrently acts as **regularization** — real speedups need sparse kernels.\n\n\n\n\n\n**µP Scaling + Zero-Centered RMSNorm**\n\n• Hyperparameters tuned on small proxy\n\n• Transferred via µP rules\n\n• Zero-centered RMSNorm for stable scaling\n\n\n\n\n\n⚠️ **Limitations (honest)**\n\n• Small training corpus (2B tokens)\n\n• TTT adds \\~5–10% inference overhead\n\n• No RLHF\n\n• Experimental, not production-ready\n\n\n\n\n\n📎 **Links**\n\n• 🤗 Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n• 📦 PyPI: [https://pypi.org/project/genesis-llm/](https://pypi.org/project/genesis-llm/)\n\n\n\n\n\nI’d really appreciate feedback — especially from folks working on **linear attention**, **hybrid architectures**, or **test-time adaptation**.\n\n\n\n*Built by Orch-Mind Team*",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw9qg6/genesis152minstruct_exploring_hybrid_attention/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1py1kq2",
      "title": "[Tool Release] Skill Seekers v2.5.0 - Convert any documentation into structured markdown skills for local/remote LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py1kq2/tool_release_skill_seekers_v250_convert_any/",
      "author": "Critical-Pea-8782",
      "created_utc": "2025-12-28 20:28:51",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "Hey 👋\n\n  Released **Skill Seekers v2.5.0** with universal LLM support - convert any documentation into structured markdown skills.\n\n  ## What It Does\n\n  Automatically scrapes documentation websites and converts them into organized, categorized reference files with extracted code examples. Works with any LLM (local or remote).\n\n  ## New in v2.5.0: Universal Format Support\n\n  - ✅ **Generic Markdown export** - works with ANY LLM\n  - ✅ **Claude AI** format (if you use Claude)\n  - ✅ **Google Gemini** format (with grounding)\n  - ✅ **OpenAI ChatGPT** format (with vector search)\n\n  ## Why This Matters for Local LLMs\n\n  Instead of context-dumping entire docs, you get:\n  - **Organized structure**: Categorized by topic (getting-started, API, examples, etc.)\n  - **Extracted patterns**: Code examples pulled from docs with syntax highlighting\n  - **Portable format**: Pure markdown ZIP - use with Ollama, llama.cpp, or any local model\n  - **Reusable**: Build once, use with any LLM\n\n  ## Quick Example\n\n  ```bash\n  # Install\n  pip install skill-seekers\n\n  # Scrape any documentation\n  skill-seekers scrape --config configs/react.json\n\n  # Export as universal markdown\n  skill-seekers package output/react/ --target markdown\n\n  # Result: react-markdown.zip with organized .md files\n```\n\n  The output is just structured markdown files - perfect for feeding to local models or adding to your RAG pipeline.\n\n  Features\n\n  - 📄 Documentation scraping with smart categorization\n  - 🐙 GitHub repository analysis\n  - 📕 PDF extraction (for PDF-based docs)\n  - 🔀 Multi-source unified (docs + code + PDFs in one skill)\n  - 🎯 24 preset configs (React, Vue, Django, Godot, etc.)\n\n  Links\n\n  - GitHub: https://github.com/yusufkaraaslan/Skill_Seekers\n  - PyPI: https://pypi.org/project/skill-seekers/\n  - Release: https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.5.0\n\n  MIT licensed, contributions welcome! Would love to hear what documentation you'd like to see supported.",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1kq2/tool_release_skill_seekers_v250_convert_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pvibn9",
      "title": "GLM 4.7 has now taken #2 on Website Arena",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/w2ak6aqsyd9g1.png",
      "author": "Worldly_Ad_2410",
      "created_utc": "2025-12-25 17:35:29",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pvibn9/glm_47_has_now_taken_2_on_website_arena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw3dxhx",
          "author": "DocStrangeLoop",
          "text": "The moat.\n\nIt closed.",
          "score": 1,
          "created_utc": "2025-12-26 22:13:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1px25u5",
      "title": "GLM 4.7 Reaches Top 6 on Vending Bench 2 Leaderboard First Open Weight Model in Profit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/je9btc6tyr9g1.jpeg",
      "author": "techspecsmart",
      "created_utc": "2025-12-27 16:40:32",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1px25u5/glm_47_reaches_top_6_on_vending_bench_2/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1py9kf4",
      "title": "Jetbrains AI users, what's your configuration with local models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9kf4/jetbrains_ai_users_whats_your_configuration_with/",
      "author": "robertpro01",
      "created_utc": "2025-12-29 02:07:26",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "I am trying this configuration, but I would like to know what are you guys using for each category:\n\nhttps://preview.redd.it/7mrrekazw1ag1.png?width=710&format=png&auto=webp&s=5ef4328afa4e62c33d84516756b35412485201f8\n\n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9kf4/jetbrains_ai_users_whats_your_configuration_with/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pxjvjy",
      "title": "GPU requirements for running Qwen2.5 72B locally?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxjvjy/gpu_requirements_for_running_qwen25_72b_locally/",
      "author": "lucasbennett_1",
      "created_utc": "2025-12-28 05:59:20",
      "score": 5,
      "num_comments": 18,
      "upvote_ratio": 0.86,
      "text": "Trying to determine what GPU setup I need to run qwen2.5 72B locally with decent inference speed. From what I understand the model needs around 140GB+ vram for full precision or maybe 70-8-GB for quantisized versions. Does this mean I'm looking at multiple A100s or H100s? Or can this run on consumer GPUs like 4090s with some heavy quantization?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxjvjy/gpu_requirements_for_running_qwen25_72b_locally/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwbmolo",
          "author": "alexp702",
          "text": "A Mac/Dgx/stryx halo with 128Gb should run this at 8 bit. Alternatively an Rtx 6000 Pro based workstation. Factor in you’ll probably need 72Gb for model and 36Gn for the full context. Double that for BF16. That’s my rough calculations.\n\nPersonally would run something newer.",
          "score": 6,
          "created_utc": "2025-12-28 06:12:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwce6ns",
              "author": "lucasbennett_1",
              "text": "Appreciate the brakdown, mac/DGX route seems expensive though. wold quantizing to 4-bit on dua 3090s or 4090s work or does that kill performance for realistic use?",
              "score": 1,
              "created_utc": "2025-12-28 10:31:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwcfxvd",
                  "author": "alexp702",
                  "text": "For that model you can definitely do 2 3090/4090s - and it will run comparatively very quickly. However you will have a small context space of somewhere \\~32K or less. If you're coding against the model this is too tight for Cline/Roocode/etc to function decently. Also quantising small models for code causes big accuracy losses. If you're doing RAG you can sometimes get away with these errors, but they annoy me enough to do not want to use them.\n\nOn a budget check out Stryx Halo based boxes - they are (were before Ram costs!) <$3K for a 128Gb box. I have not tried them, but they are cheaper than DGX Sparks. Macs start around \\~3.5K for 128Gb Mac Studio M4Max. Both AMD and Apple suffer from slow prompt parsing. Depending on workload this can make them tedious. But the low memory of Nvidia cards on a budget means no real choice.\n\nI have put two 4090s in one AMD 5800x3d box, and it performs very well - 3-8x prompt processing over M3 Ultra running the same Qwen3 Coder 30b-IQ4 which Cline really does lots of ... until you run out of memory. I got about 70K context reliably.  So I am back to the Mac Studio M3 Ultra running Qwen 480b. Quality over speed won for me.",
                  "score": 6,
                  "created_utc": "2025-12-28 10:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbzg1r",
          "author": "RiskyBizz216",
          "text": "Q8 only needs 77GB\n\nI prefer the **IQ**  quants because they give you more speed, and are smaller.\n\n[https://huggingface.co/bartowski/Qwen2.5-72B-Instruct-GGUF](https://huggingface.co/bartowski/Qwen2.5-72B-Instruct-GGUF)\n\n  \nThis IQ3\\_XXS is only 31 GB so it could fit on a single 5090 with some offloading.\n\nIf you go any lower than IQ3 then you would be better off using the Qwen3 32B VL Instruct \n\n[https://huggingface.co/bartowski/Qwen\\_Qwen3-VL-32B-Instruct-GGUF](https://huggingface.co/bartowski/Qwen_Qwen3-VL-32B-Instruct-GGUF)",
          "score": 3,
          "created_utc": "2025-12-28 08:08:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwceas9",
              "author": "lucasbennett_1",
              "text": "thanks for the feedback, looking into it",
              "score": 2,
              "created_utc": "2025-12-28 10:32:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwc6c2q",
          "author": "umtausch",
          "text": "Why are you running 2.5? 3 is much much better.",
          "score": 3,
          "created_utc": "2025-12-28 09:14:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc7uky",
              "author": "Karyo_Ten",
              "text": "Asking the real question.\n\n1 year in LLM is forever.",
              "score": 2,
              "created_utc": "2025-12-28 09:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwox9c5",
                  "author": "Ok-Bill3318",
                  "text": "Lately 1 month is forever.",
                  "score": 1,
                  "created_utc": "2025-12-30 07:04:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwce0ia",
              "author": "lucasbennett_1",
              "text": "Will check it out. still curious about the 72B setup though since hardware requirements should be similar",
              "score": 0,
              "created_utc": "2025-12-28 10:29:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi3y6k",
          "author": "beedunc",
          "text": "First - Delete all your 2.5’s and just go 3. \n\nNight and day better. So much better, you might even be able to use smaller models with the 3.",
          "score": 3,
          "created_utc": "2025-12-29 06:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbnzn9",
          "author": "Such_Advantage_6949",
          "text": "Most people running with q4 which will fit on 2x3090 though u probably is a bit limited on context. 3x3090 should let u running it fine at higher quant like q5+",
          "score": 2,
          "created_utc": "2025-12-28 06:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwce9bv",
              "author": "lucasbennett_1",
              "text": "how about 2x4090s?",
              "score": 1,
              "created_utc": "2025-12-28 10:31:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwch1r9",
                  "author": "Such_Advantage_6949",
                  "text": "Generation speed is about the same for LLM. The 4090 will be faster in prolotprocessing ( time to generate the first token)",
                  "score": 2,
                  "created_utc": "2025-12-28 10:58:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwowyc5",
          "author": "Ok-Bill3318",
          "text": "You could run it on a Mac Studio or a strix halo box with a decent amount of ram.",
          "score": 1,
          "created_utc": "2025-12-30 07:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrffuq",
          "author": "Beautiful_Trust_8151",
          "text": "We used to run Llama 3.3 70b with 3 Radeon 7900xtx GPUs on a z790 motherboard.  You can get decent 14 tokens per second and a decent context window.  The newer models support flash attention which will give you an even larger context window.  Qwen3 70b like llama 70b is a dense model, so it doesn't run that well on Macs and Strix boxes.",
          "score": 1,
          "created_utc": "2025-12-30 17:17:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw30az",
      "title": "MiniMax-M2.1 weights are now live on Huggingface",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/llesz3v4jj9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-26 12:19:03",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw30az/minimaxm21_weights_are_now_live_on_huggingface/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pugj5x",
      "title": "Built Lynkr - Use Claude Code CLI with any LLM provider (Databricks, Azure OpenAI, OpenRouter, Ollama)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pugj5x/built_lynkr_use_claude_code_cli_with_any_llm/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2025-12-24 06:27:00",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.83,
      "text": "Hey everyone! 👋\n\nI'm a software engineer who's been using Claude Code CLI heavily, but kept running into situations where I needed to use different LLM providers - whether it's Azure OpenAI for work compliance, Databricks for our existing infrastructure, or Ollama for local development.\n\nSo I built **Lynkr** \\- an open-source proxy server that lets you use Claude Code's awesome workflow with whatever LLM backend you want.\n\n**What it does:**\n\n* Translates requests between Claude Code CLI and alternative providers\n* Supports streaming responses\n* Cost optimization features\n* Simple setup via npm \n\n**Tech stack:** Node.js + SQLite\n\nCurrently working on adding Titans-based long-term memory integration for better context handling across sessions.\n\nIt's been really useful for our team , and I'm hoping it helps others who are in similar situations - wanting Claude Code's UX but needing flexibility on the backend.\n\n**Repo:** \\[https://github.com/Fast-Editor/Lynkr\\]\n\nOpen to feedback, contributions, or just hearing how you're using it! Also curious what other LLM providers people would want to see supported.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pugj5x/built_lynkr_use_claude_code_cli_with_any_llm/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvwsirq",
          "author": "tillybowman",
          "text": "i mean, you can change the env vars from claude code to use openrouters api f.e. already, what's the difference?",
          "score": 3,
          "created_utc": "2025-12-25 19:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtzw89",
          "author": "Dolsis",
          "text": "It looks interesting.\n\nHowever, I see that the only local provider is Ollama. Will it be able to support llama.cpp and/or LM Studio?\n\nFor now I'm using opencode calling  llama.cpp with qwen3-coder and it works great.",
          "score": 1,
          "created_utc": "2025-12-25 05:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvu058o",
              "author": "Dangerous-Dingo-5169",
              "text": "Thanks for replying \nAdding support for llama.cpp is next on my priority list \nI have got like 3 people asking the same thing\nAs of now it only supports ollama, azure models, databricks dbrx, open ai and openrouter models",
              "score": 1,
              "created_utc": "2025-12-25 05:59:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyo6v3",
      "title": "Any local LLM fact checking systems or frameworks?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyo6v3/any_local_llm_fact_checking_systems_or_frameworks/",
      "author": "Subject_Sir_2796",
      "created_utc": "2025-12-29 14:51:23",
      "score": 4,
      "num_comments": 10,
      "upvote_ratio": 0.83,
      "text": "Anyone come across any decent systems or frameworks for fact checking information? \n\nMy use case would mostly be for academic papers. Essentially thinking a process that would start with pdf parsing, indexing or embedding before extracting references and claims made in the text before retrieving full texts for references (where these are available) and cross referencing claims against the relevant citations to check for citation laundering, overstating, misinterpretation etc. \n\nIdeally also applying additional checks against online sources by generating RAG queries where evidence provided in the pdf is weak or absent. The desired output would be a credibility score and report that gives an overview of what information is well supported by evidence and which claims are dubious or hard to verify with reasoning and quoted evidence for these conclusions attached so they can be easily manually verified.\n\nWondered if anything like this is already around or if anyone has any thoughts on existing packages/tools that would be ideal for this use case? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyo6v3/any_local_llm_fact_checking_systems_or_frameworks/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjzqq7",
          "author": "-Akos-",
          "text": "My thoughts immediately go to MCP (mainly because I'm studying MCP, and right now everything looks like a nail with that hammer). [MCP for Research: How to Connect AI to Research Tools](https://huggingface.co/blog/mcp-for-research)",
          "score": 1,
          "created_utc": "2025-12-29 15:08:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkgd6u",
              "author": "Subject_Sir_2796",
              "text": "To be fair, this looks like exactly the kind of hammer I need. Haven’t come across this before, thanks for sharing!",
              "score": 1,
              "created_utc": "2025-12-29 16:28:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwlyw28",
                  "author": "-Akos-",
                  "text": "![gif](giphy|lkzQSklzkfqUzSd4j3)",
                  "score": 1,
                  "created_utc": "2025-12-29 20:47:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwmwazz",
              "author": "Own_Amoeba_5710",
              "text": "Yep. To take it a step further, ref tools and context 7 do this swimmingly.",
              "score": 1,
              "created_utc": "2025-12-29 23:36:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwn05is",
                  "author": "Subject_Sir_2796",
                  "text": "Right on, thanks! I’ll check these out too.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:57:32",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo35lr",
          "author": "Pencil__Sharpener",
          "text": "I don’t know of anything locally…but I built this website https://verofyi.com/ that will fact check a YouTube video by extracting claims/assertions (non opinionated) I was able to extract the exact quote and context of the quote. In addition, for each claim I can perform a deep search that uses perplexity to do a web search and fact check it for you. It seems you’re wanting to do the same for academic papers.\n\nI think it’s really good on giving a user more information on any topic from any video. This is also using the smallest model on perplexity. So for your use case, unless you care about privacy above all. I think you’re best solution would be to use perplexity and a closed source model for “extracting claims” similar to my app and you can do very precise online searches, and you can also include your private database via RAG if needed.",
          "score": 1,
          "created_utc": "2025-12-30 03:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwn9ubl",
          "author": "AnxietyPrudent1425",
          "text": "I built one myself but I’m planning to die of starvation in the next month or so. What your budget? I accept sandwiches and canned food.",
          "score": 0,
          "created_utc": "2025-12-30 00:49:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp0m6p",
              "author": "nivix_zixer",
              "text": "Dude are you a bot? I read your recent history and if you're a human I feel bad for you. But the inconsistency in your facts (and the current state of the internet) leads me to believe you are a bot. \n\nAlso, doordash. It's how my wife and I got through a terrible time in our lives.",
              "score": 1,
              "created_utc": "2025-12-30 07:34:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjyr52",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2025-12-29 15:03:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk1aj2",
              "author": "Ainudor",
              "text": "sure, if you like nazi rewritten history and the cult of Musk personality",
              "score": 2,
              "created_utc": "2025-12-29 15:16:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvf1k2",
      "title": "Mm What multi-modal ai stack could serve as home assistant. Always persistent",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pvf1k2/mm_what_multimodal_ai_stack_could_serve_as_home/",
      "author": "Thermal_Headroom",
      "created_utc": "2025-12-25 14:58:13",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "Yeah, okay, I get it. Like, buy a Dell server or a bunch of Mac minis and run huge models locally. Great, but which models are good enough and can be trusted managing my IoT alarms, locks, automation, and convenience features like lighting and audio? Like a housekeeper but for real here. software housekeeper. 👀 Running in the background. Not reaching out for the most part. And it really should be air-gapped for local personal use. What model combinations and tooling are we talking about? And let’s outline our network structure framework if possible. Yes I mean giving the model access to sensors configuration, api, and other direct or direct access methods. Not sure how may have already tried this recently. \n\nMerry Christmas 🎄 ",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pvf1k2/mm_what_multimodal_ai_stack_could_serve_as_home/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvxd5cn",
          "author": "horriblesmell420",
          "text": "Pretty much any model that can call tools will do well. Speed will also be a factor if you're trying to make you're own real time voice assistant.\n\nI've had great luck with qwen3 30B for this functionality, it's fast and intelligent, it's never missed a tool call for me. Nvidia's parakeet v2 or v3 are great for STT. Lastly for TTS I'd use kokoro unless you need voice cloning.\n\nWith this stack I've achieved around 2-3 seconds round trip response time from STT input > LLM Calling the tools > TTS response and fits in around 24GB of VRAM. Depending on how many devices you have you may need to increase your context size.",
          "score": 1,
          "created_utc": "2025-12-25 21:25:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3dkb4",
          "author": "Purple-Programmer-7",
          "text": "Look into Home Assistant first.\n\nI had llama 3.2 3B backing a pydantic-ai agent running everything via HA… voice commands and all",
          "score": 1,
          "created_utc": "2025-12-26 22:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhzwtw",
              "author": "TelephonePossible866",
              "text": "Do you have it responding back in voice? Wake word? Piper/whisperr? I’d like to use a model that ideally has a tiny bit of personality or isn’t entirely “dumb” in that it is only narrowly functional for specific commands controlling IoT. In other words, something with some novelty that the family can enjoy. In my head I have some potential projects, like LLM that controls thermal printer that reminds you when it’s trash day or prints the HA shopping list but also will print corny self-affirmations or schizo daily horoscopes for my wife or daughter. Idk if I could simply find some stupid rss feed to pull this stuff into a grafana dashboard and LLM retrieves from local dashboard and prints to thermal printer. Dynamic alarm clock that insults you if you snooze it. More automations from human detection sensor as you leave in the morning and come home at night. Literally making a mounted laser boundary alarm out of a rPi to make my dog stop shitting in the guest bedroom. Would love the voice assistant to chastise him when the laser is broken, notifying me. \n\nEssentially I would like a voice assistant who is helpful and can wake you up and tell you the weather, and all the normal things, but also has some absurd/silly abilities, and is generally an asshole. Admonishes you if the tv is on for 6 hours or when you forget to turn off the lights by x pm. \n\nAlso functional use case for security, not just sensor/camera/garage door alarms, but running OPNsense as a router/firewall mini pc I am pretty sure I could configure voice automation/alarms pulled from the netflow logs/suratica. I think HA has a HACS integration for it or I’m sure there’s some repo that probably works. I have never used a local LLM in my life, this is my introduction to it and the start of my journey here. Too ambitious?",
              "score": 1,
              "created_utc": "2025-12-29 05:45:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwj0zdg",
                  "author": "Purple-Programmer-7",
                  "text": "Damn dude. Quite the response. I’ll try to keep mine short n sweet.\n\nCompletely custom integration and companion iOS app. Nvidia Parakeet was the ASR. Didn’t get to wake word or TTS yet (on the way).\n\nFor personality, I recommend moving away from the HA ecosystem and relying on its websockets API instead. If you’re a dev, there are significantly less headaches and more control this route.\n\nNot sure about all your goals (positive reinforcement is the best way to train a dog. They love us. They want to make you happy), but some of your projects sound fun. \n\nI’d start with openai api calls to prototype and move to local once you have your masochistic fantasy bot built.",
                  "score": 1,
                  "created_utc": "2025-12-29 11:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3z3qv",
          "author": "teleolurian",
          "text": "You can do it just fine with qwen-4b-2507 and qwen3-vl-8b for most tasks",
          "score": 1,
          "created_utc": "2025-12-27 00:18:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puhtnd",
      "title": "Now you can run local LLM inference with formal privacy guarantees",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fb8lnvwns39g1.png",
      "author": "IIITDkaLaunda",
      "created_utc": "2025-12-24 07:45:51",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1puhtnd/now_you_can_run_local_llm_inference_with_formal/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pzp4ft",
      "title": "Agents governance",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzp4ft/agents_governance/",
      "author": "lexseasson",
      "created_utc": "2025-12-30 18:06:50",
      "score": 3,
      "num_comments": 16,
      "upvote_ratio": 0.8,
      "text": "The agent loop is well understood:\ngoal → plan → act → evaluate → improve.\nWhat’s less discussed is why agentic systems still fail at scale even when that loop exists.\nFrom building governed agentic systems, my takeaway is simple:\nagents don’t fail because they lack reasoning — they fail because decisions, memory and progress aren’t governed explicitly.\nWhen intent lives in chat, memory in tools, and evidence in logs, teams confuse activity with outcomes.\nThe move from chatbots → agents is real.\nThe next move is from agents → governed delivery.\nCurious how others are handling governance in their operating model.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzp4ft/agents_governance/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwsnpqd",
          "author": "Beneficial-Pear-1485",
          "text": "Do a divergence test on a subject you are expert at. \nExact same prompt across multiple models. Then study  the semantics deeply and compare, you’ll see they don’t just “stochastic variance”. \n\nThey literally re-interpret the exact same task differently every single time even if you froze the prompt.  \n\nAnd time they re-interpret, they end up planning different, act and evaluate different.",
          "score": 1,
          "created_utc": "2025-12-30 20:45:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsqm7o",
              "author": "lexseasson",
              "text": "This matches what I’ve seen as well.\nThe divergence itself isn’t surprising — what’s dangerous is that reinterpretation happens silently. From the outside it still looks like “the same task”, but intent, plan, and evaluation criteria have already shifted.\nThat’s why I’ve been treating reinterpretation as a governance problem, not a model one. If intent isn’t externalized and versioned, you can’t tell whether a different outcome is acceptable exploration or an accidental semantic drift.\nIn practice, freezing prompts doesn’t help much if the system has no explicit record of what success meant at that point in time.",
              "score": 2,
              "created_utc": "2025-12-30 20:59:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtnh92",
                  "author": "Beneficial-Pear-1485",
                  "text": "I explained this in my paper Empirical Evidence of Interpretation Drift and posted here on reddit. \n\n90% said I made temp0 error and this was expected and a nothing burger.  \n\nHilarious industry..",
                  "score": 1,
                  "created_utc": "2025-12-30 23:44:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtdf86",
          "author": "Mean-Sprinkles3157",
          "text": "My current approach is chat -> assumptions -> implementations. in chat stage, it is collecting information, restate the problem; assumptions stage is the analysis stage, it may create code block but no create\\_file is allowed; it need user to signal to move to the next stage, in this stage, it analyze the problem and  create solution, it decide if it is a simple or hard task, when it requires multiple steps, I would create progressPlan (which is a todo list); after assumptions, we get into implementation, in this stage, basically it follows the todo list, generate code.    \n  \nNow for the governing of the AI agent. the ability to be \"auto\", that is self govern, or self-driving. it will free to use llms,  do the work without user given input, it  improves code by its own. I would say the first ability to read the code, and create plan, finish it and read the code again, create another plan, and implement as so on.   \n  \nFor me, is when implementation is done, it will go back to assumptions to create another plan.",
          "score": 1,
          "created_utc": "2025-12-30 22:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtuhpc",
              "author": "lexseasson",
              "text": "This is a solid staged design — especially the explicit transitions between phases.\nThe place I ran into trouble with similar setups was when those phase transitions stayed implicit or conversational. Over time it became hard to reconstruct why a plan existed or who approved a shift.\nExternalizing those transitions (even minimally) made a big difference once systems ran longer or involved more than one human.",
              "score": 2,
              "created_utc": "2025-12-31 00:22:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nws6ng6",
          "author": "Total-Context64",
          "text": "\"goal → plan → act → evaluate → improve\" often fails without a human in the loop for a variety of reasons, usually agents making assumptions without validation followed by arbitrary changes.\n\nI developed a very strict human in the loop methodology that I've been using for some time now, and it works very well.  It can even extend to subagents as long as the primary agent maintains the feedback loop with the operator.\n\nAre you thinking about governance with or without a human in the loop?",
          "score": 0,
          "created_utc": "2025-12-30 19:23:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsonbc",
              "author": "productboy",
              "text": "Where and how do you store the governance logic? I’ve tried markdown files; but now believe in code [say in the tool call] is more efficient.",
              "score": 2,
              "created_utc": "2025-12-30 20:50:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwsqn01",
                  "author": "Total-Context64",
                  "text": "I use markdown: [https://github.com/SyntheticAutonomicMind/SAM/blob/main/.github/copilot-instructions.md#handoff-protocol](https://github.com/SyntheticAutonomicMind/SAM/blob/main/.github/copilot-instructions.md#handoff-protocol)\n\nWhen I have an agent ready to end a session, the agent will create a session directory, populate it, and then confirm with me that I'm happy with the content before the session ends.  I copy and paste the context prompt into the next session and make any changes that I need before I start the session.  It flows extremely well.\n\nI think a tool call would be fine as long as it is programmatically accessible (the agent can easily query it) and the instructions help the next agent know what data to fetch that defines the context that's relevant to their session.\n\nPersonally, I'd probably create an external context database with an API and have agents pull it with a key that's unique to the project or something.\n\nThat's actually very interesting.",
                  "score": 1,
                  "created_utc": "2025-12-30 20:59:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwswgei",
                  "author": "lexseasson",
                  "text": "Great question — I’ve landed on a layered approach rather than “docs vs code”.\n1) Human-readable policy (docs): I keep the intent and constraints in markdown (what is allowed/forbidden, ownership boundaries, what counts as evidence vs semantics). This is for humans, reviews, onboarding, and audits.\n2) Machine-enforceable rules (code): the governance logic that must be executed consistently lives in code (validators, schema checks, gates, tool-call contracts). Otherwise it becomes aspirational documentation.\n3) Immutable audit trail (artifacts): every run produces append-only evidence: inputs, tool calls, diffs, metrics, and proposed updates — so you can reconstruct “what happened” without trusting memory.\nIn other words: docs define the contract, code enforces it, artifacts prove it.\nI’ve found markdown alone breaks down once systems scale, because humans stop updating it and agents can’t reliably comply with rules that aren’t executable. But code alone also fails if the policy intent isn’t readable and reviewable.\nThe practical split I use is:\n– Policy-as-text for semantics/ownership\n– Policy-as-code for enforcement (schemas, allowed fields, gates)\n– Evidence-as-data for auditability (JSON snapshots + reports)",
                  "score": 1,
                  "created_utc": "2025-12-30 21:27:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nws7lse",
              "author": "lexseasson",
              "text": "I think what you describe does work — especially early on.\nWhere I kept running into trouble wasn’t the presence or absence of a human in the loop, but where the loop actually lives.\nIn my experience, “human-in-the-loop” often becomes a procedural pattern (reviews, approvals, feedback), but governance breaks when decisions, intent and state remain implicit — spread across chat, tools, or human memory.\nWhat changed things for me was treating governance as structural, not interactive:\n– externalized decision logs\n– artifact-based progress as the unit of truth\n– CI as a gate that encodes intent, not just quality\n– strict separation between human semantics and machine-generated evidence\nHumans are still very much in the loop — but through explicit contracts and memory, not continuous supervision.\nCurious how you handle decision traceability once systems grow beyond a single operator–agent pair.",
              "score": 0,
              "created_utc": "2025-12-30 19:28:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nws9wd3",
                  "author": "Total-Context64",
                  "text": "I call the method that I use \"[the unbroken method](https://www.syntheticautonomicmind.org/viewer.html?file=docs/developer/the-unbroken-method.md)\", it introduces notions of continuous context, continuous collaboration, and total ownership.\n\nI've used it to develop multiple applications now, [Synthetic Autonomic Mind](https://github.com/SyntheticAutonomicMind/SAM), [ALICE](https://github.com/SyntheticAutonomicMind/ALICE), and a few others.\n\nI would assume it would scale beyond a single operator-agent pair pretty well with minor changes if any.",
                  "score": 0,
                  "created_utc": "2025-12-30 19:39:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsbqmx",
          "author": "lexseasson",
          "text": "This is really solid — thanks for sharing it.\nI think what you call “unbroken” maps very closely to what I’ve seen work best at the single-operator or tight-loop level: continuous context, clear ownership, and sustained collaboration instead of episodic prompting.\nWhere I started to hit limits wasn’t in the method itself, but in what happens when continuity inevitably breaks:\nmultiple humans, multiple agents, repo boundaries, CI, audits, or just time gaps.\nThat’s where I’ve been treating governance as a structural fallback layer rather than a replacement:\nexternalized memory, decision logs, artifact-based progress, CI encoding intent — so the system remains explainable even when the “unbroken” loop can’t be preserved perfectly.\nI see these approaches as complementary: continuity for speed and depth, explicit governance for scale and survivability.\nCurious how you’ve thought about decision traceability and ownership once systems grow beyond a single operator–agent pair or need to be reconstructed after context loss.",
          "score": 0,
          "created_utc": "2025-12-30 19:48:11",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwsehod",
              "author": "Total-Context64",
              "text": "I have a few generic prompts that I've used on systems that don't have context available, agents do a good job of looking at the commits and rebuilding their understanding but it isn't as helpful as having that knowledge readily available.  Thinking about it more, it probably would break down unless multiple operators were working in individual branches or forks, but in my mind they should be doing that anyway and then merging.\n\nYou've brought up some points that are very interesting, and I see some potential areas where I may be able to improve my processes already, so thank you.",
              "score": 0,
              "created_utc": "2025-12-30 20:01:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxm9bw",
      "title": "How do I configure LM Studio model for safety?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxm9bw/how_do_i_configure_lm_studio_model_for_safety/",
      "author": "bangboobie",
      "created_utc": "2025-12-28 08:20:34",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 0.8,
      "text": "Apologies before I begin as I am not that tech-savvy. I managed to set-up LM Studio on a MacBook. I was wondering how secure LM Studio is that in the sense if I say something to model that would never leave my device right? Or do I need to configure any settings first? Like I turned off the headless thing and is there anything else do I need to do? I plan to work with LLMs regarding things that I wouldn't necessarily like being handed over to someone. And also things like Port 1234 sound a bit intimidating to me.\n\nI would really appreciate if anyone could tell me if I need to do anything before I actually start tinkering with models. And how I can make it more private. Although I think that apps like LM Studio would probably have some built-in protections for privacy as they are meant to be locally and the purpose would be defeated otherwise. But it's just that the UI is a bit intimidating for me.\n\n>How do I configure LM Studio models for safety?\n\n\\*privacy",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxm9bw/how_do_i_configure_lm_studio_model_for_safety/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwc6962",
          "author": "Mr_TakeYoGurlBack",
          "text": "You don't have to do anything.",
          "score": 6,
          "created_utc": "2025-12-28 09:14:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdmbw7",
              "author": "bangboobie",
              "text": "Thanks, I was a bit freaked out cause I thought that I probably messed up with something.",
              "score": 1,
              "created_utc": "2025-12-28 15:46:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwc6ars",
          "author": "nickless07",
          "text": "Nothing leaves your device, aside of the obvibious stuff you request.  \nUse the internal function to download a model? Sure the request get send. Updater? Yes that also checks if a new version is aviable and downloads it (only when you click on update). Other data? Nope. You can even disconnect your device from the internet and use LM Studio for the next 10 years without it breaking anything.   \n  \nThe 'headless thing' is when you want to use LM Studio to just host a LLM, but don't wanna use the GUI it comes with.   \nYou can change the 1234 port to something more appealing (make sure you change it in your other apps that connect to LM Studio too).  \nIt is basically just a wrapper/GUI for llama.cpp. For the time beeing there is nothing to worry about. It is not designed to gather user data and send it somewhere (sometimes i wish it could).",
          "score": 1,
          "created_utc": "2025-12-28 09:14:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd8ndc",
              "author": "ButterscotchHot9423",
              "text": "“Basically just a wrapper for llama.cpp”\n\nAccurate if using gguf model weights. MLX if using optimized LLMs for Apple Silicon which, IMHO, is the only real reason to use LMStudio. IIRC there are a total of 3 runtimes LM Studio supports, I just can’t remember the third.",
              "score": 2,
              "created_utc": "2025-12-28 14:30:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe87u9",
                  "author": "nickless07",
                  "text": "MLX is Apple's Machine learning framework (works if you have an M Chip, good luck if you have Intel :D).  GGUF is a fileformat (like a .zip) used for llama.cpp (not every model is aviable as MLX). Safetensors is another model format used with Pytorch (most models are aviable as safetensors), but does not support quants, so it is BF16 and such. LM Studio is just a wrapper around theese so you don't have to setup them manually, but with a few clicks. Ollama also offers MLX support.",
                  "score": 2,
                  "created_utc": "2025-12-28 17:36:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdmp99",
                  "author": "bangboobie",
                  "text": "I use MLX because it said that \"optimised for M-series Macs\" but irl I have no idea of the difference between it and gguf.",
                  "score": 1,
                  "created_utc": "2025-12-28 15:48:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdmhy2",
              "author": "bangboobie",
              "text": "Thanks man, it was a bit intimidating for me at first seeing so much technical jargon at the first time.",
              "score": 1,
              "created_utc": "2025-12-28 15:47:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwchvh7",
          "author": "lucasbennett_1",
          "text": "LM Studio runs completely offline on your device, nothing leaves your mac unless you explicitly enable server mode or connect it to external services. The port 1234 is just for local communication between the app and your browser, it's not exposed to the internet. As long as you haven't enabled any cloud features or API endpoints that connect externally, everything stays on your machine. You're good to go, just don't enable server sharing or remote access features if you see them in settings.",
          "score": 1,
          "created_utc": "2025-12-28 11:06:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdm92r",
              "author": "bangboobie",
              "text": "Thanks man. I don't think I have enabled any cloud features as I have not signed into any account inside LM Studio but let me know if there are other settings that I check.",
              "score": 1,
              "created_utc": "2025-12-28 15:46:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwi606w",
                  "author": "lucasbennett_1",
                  "text": "you're all set then, just keep an eye on the settings that says remote sharing or remote acces or cloud syncing. THings are locally ran by default",
                  "score": 1,
                  "created_utc": "2025-12-29 06:35:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pxlrg2",
      "title": "LM Studio randomly crashes on Linux when used as a server (no logs). Any better alternatives?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxlrg2/lm_studio_randomly_crashes_on_linux_when_used_as/",
      "author": "Opposite_Future3882",
      "created_utc": "2025-12-28 07:49:55",
      "score": 3,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI’m running into a frustrating issue with **LM Studio on Linux**, and I’m hoping someone here has seen something similar.\n\nWhenever I run models in **server mode** and connect to them via **LangChain** (and other client libraries), **LM Studio crashes randomly**. The worst part is that it **doesn’t produce any logs at all**, so I have no clue what’s actually going wrong.\n\nA few things I’ve already ruled out:\n\n* Not a RAM issue 128 GB installed\n* Not a GPU issue\n* I’m using an **RTX 5090 with 32GB VRAM**\n* The model I’m running needs **\\~5GB VRAM max**\n* System memory usage is well below limits at full is about 30 GB\n\nThe crashes don’t seem tied to a specific request pattern — they just happen unpredictably after some time under load.\n\nSo my questions are:\n\n1. Has anyone experienced **random LM Studio crashes on Linux**, especially in server/API mode?\n2. Are there any **better Linux-friendly alternatives** that:\n   * Are easy to set up like LM Studio\n   * Expose an OpenAI-compatible or clean HTTP API\n   * Can run **multiple models / multiple servers simultaneously**\n   * Are stable enough for long-running workloads?\n\nI’m open to both GUI-based and headless solutions. At this point, stability and debuggability matter way more than a fancy UI.\n\nAny suggestions, war stories, or pointers would be greatly appreciated   \nThanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxlrg2/lm_studio_randomly_crashes_on_linux_when_used_as/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwby31n",
          "author": "Atzer",
          "text": "Same experince but with r9700ai and ubuntu lts",
          "score": 2,
          "created_utc": "2025-12-28 07:55:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh05kk",
          "author": "TokenRingAI",
          "text": "Llama.cpp, which is used by LM Studio, is completely unreliable on Linux due to using std:regex, which is a fatally flawed regex implementation, that is 100% guaranteed to overflow the stack and crash the application with any long model output or long tool call output.\n\nWith heavy tool use and the right prompts, you (or a malicious user) can trigger a crash in a couple of minutes with most models.\n\nIf Llama.cpp is segfaulting on long output and working fine otherwise, that is almost certainly the cause.\n\nI am currently testing a patch that should mostly resolve the crashing during tool calls, but the crashing with long output is still going to be a problem for a while even if that tool calling patch gets merged.\n\nVLLM is probably your best bet for now",
          "score": 2,
          "created_utc": "2025-12-29 02:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc2itv",
          "author": "Foreign-Watch-3730",
          "text": "Hello, i use it for server without crash, i use th appimage bot the .deb. i use it with a fork of openwebui",
          "score": 1,
          "created_utc": "2025-12-28 08:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc94ac",
              "author": "Opposite_Future3882",
              "text": "tnx",
              "score": 1,
              "created_utc": "2025-12-28 09:42:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcv9e2",
          "author": "DenizOkcu",
          "text": "You can install/build llama.cpp which is highly battle proven (it is what LM Studio runs under the hood - hopefully not your issue 😎). \n\nFind models on huggingface. Use AI to suggest ideal config for your setup. It has a server and even a web UI. It took me roughly 1h to understand how to run it efficiently. Similar use case as yours.",
          "score": 1,
          "created_utc": "2025-12-28 13:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdq2hg",
              "author": "DataGOGO",
              "text": "Calling llama.cpp battle tested is a bit of stretch.\n\nvLLM a much better server platform. ",
              "score": 1,
              "created_utc": "2025-12-28 16:05:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdrp3l",
                  "author": "DenizOkcu",
                  "text": "with > 92k stars on Github, > 14k forks, and being used as an engine in LM Studio, I would call it battle tested :-) Or am I missing something?\n\n  \nEdit: vllm looks interesting thanks for pointing that out :-) will try it later",
                  "score": 2,
                  "created_utc": "2025-12-28 16:13:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdk9sa",
          "author": "tabletuser_blogspot",
          "text": "Which linux distro are you using? I just installed CachyOS on a system that was stable with Kubuntu and PopOS and now I get lockups while using llama.cpp rpc-server other 3 systems running Kubuntu aren't crashing. Might have to move to older Nvidia driver or just switch distro. Love that CachyOS came with Nvidia ready to go. I've had great success using Kubuntu 22.04, 24.04, 25.10, and 26.04. I like that you can run Kubuntu Live persistent from USB thumb drive and experiment without having to install. PopOS works great but I prefer KDE desktop environment. Linux Mint is another champ. I prefer Debian based distros. They have a larger user group so finding answers is easier. Arch based CachyOS is one of the fastest Linux distros, beats Windows 11 on most benchmarks except gaming. Fedora is another good distro, probably best for gaming setups. I'm not a fan of Red Hat based distros. Let us know what you end up deciding.",
          "score": 1,
          "created_utc": "2025-12-28 15:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdptev",
          "author": "DataGOGO",
          "text": "Don’t use LM studio as a server.\n\nRun vLLM / sglang; alternatively ik_llama.cpp or llama.cpp.",
          "score": 1,
          "created_utc": "2025-12-28 16:04:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1px0tm1",
      "title": "Seeking Advice: Local AI Pipeline for Analyzing 5000 Documents (~10 GB)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1px0tm1/seeking_advice_local_ai_pipeline_for_analyzing/",
      "author": "Budget-Presence3170",
      "created_utc": "2025-12-27 15:45:01",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI’m exploring the idea of building a **fully local AI pipeline** to analyze a large collection of documents (\\~5000 files, \\~10 GB). The goal is to:\n\n* Extract key clauses, dates, and entities\n* Summarize content per document and globally\n* Compare and highlight differences between contracts\n* Produce structured outputs (like Excel files) for reporting\n\nI want to avoid cloud APIs for cost reasons. My main questions are:\n\n1. **LLM Selection:**\n   * Which open-source LLM would be most effective for this kind of document analysis?\n   * I’ve heard about LLaMA 3, Falcon, and h2oGPT what are their strengths and limitations for long-context texts like academic?\n2. **Hardware Requirements:**\n   * If I wanted to run this **entirely locally**, what’s the **minimum hardware** that would allow me to:\n      * Compute embeddings for 10 GB of text\n      * Run an LLM to summarize or answer questions using a RAG (retrieval-augmented generation) approach\n   * What performance differences can I expect between CPU-only vs GPU setups?\n3. **Pipeline Thoughts:**\n   * Any suggestions for local tools for OCR and text extraction from PDFs/Word?\n\nI’m looking for guidance from anyone who has done similar **local LLM setups** for document-heavy workflows. Insights on **practical trade-offs** (speed, accuracy, memory) and recommended open-source software stacks would be extremely helpful.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1px0tm1/seeking_advice_local_ai_pipeline_for_analyzing/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw8441s",
          "author": "Karyo_Ten",
          "text": "- How many words or characters are your documents?\n- Are there images inside?\n- What's your budget?\n- Do you already have some hardware?\n- How much time for the ingestion?\n- Will you update the dataset\n- How will you use the outputs? Do you need precise summaries, search, and if the later semantic/related or exact (citations) or Google-like?",
          "score": 1,
          "created_utc": "2025-12-27 17:57:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9i3qa",
              "author": "Budget-Presence3170",
              "text": "* **Document size:** Mixed, but many are long-form Think academic/contract-style PDFs rather than short notes.\n* **Images:** Yes, some PDFs include scanned pages and figures, so OCR would be needed at least partially.\n* **Budget:** I’m trying to avoid high recurring costs.\n* **Hardware:** Nothing special at the moment : standard workstation, no dedicated GPU yet.\n* **Ingestion time:** Not time-critical. Batch ingestion over hours or even days is fine.\n* **Dataset updates:** Yes, incremental updates over time rather than one-shot ingestion.\n* **Outputs / usage:** Per-document summaries / Cross-document comparison  / Structured outputs (tables or Excel)",
              "score": 1,
              "created_utc": "2025-12-27 22:20:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwalqqz",
                  "author": "Karyo_Ten",
                  "text": "For the budget, do you have $2K, $20K or $200K?\n\nHow many concurrent users?",
                  "score": 1,
                  "created_utc": "2025-12-28 02:07:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwax3t8",
          "author": "ai_hedge_fund",
          "text": "We run the PDF to text pipeline on private H100s and could have the text extraction piece done tomorrow. Including image descriptions, charts, tables.\n\nWe are a registered business in California and will sign NDAs, HIPAA BAAs, etc. Our core market is businesses with sensitive data needs. \n\nFeel free to be in touch if getting that piece done quickly is valuable. Happy to engage on the other topics as well.",
          "score": 1,
          "created_utc": "2025-12-28 03:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaywzx",
          "author": "fasti-au",
          "text": "Isn’t that mostly just programmatic and summarize.  Seems fairly trivial in isolation you asking for comparisons or just doco to summary style.  This is pretty basic and n8n etc can do",
          "score": 1,
          "created_utc": "2025-12-28 03:25:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbz4jd",
          "author": "nofilmincamera",
          "text": "Does this need to be right, not just mostly right?\nWill anyone actually use this to make decisions or for compliance?\nAre the documents kind of messy with tables, charts, scans, or mixed quality?\nIs this a one-time batch, or will more documents keep coming later?\nDo we need to know how confident the results are and where the data came from?\nDo we want to keep tuning and maintaining this ourselves, or just have it work?\n\nHonestly anything with contracts, there are lots of services Lexis Nexis for example. This is not easy to figure out well, nothing will he off the shelf for open source. You will spend countless hours figuring out.\n\nThis is also somewhere a screw up can be a big one, and half the reason to outsource is to have someone to blame.  Azure is great, expensive setup \n Costs. Lots of options,  none cheap.",
          "score": 1,
          "created_utc": "2025-12-28 08:05:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg0z5z",
          "author": "Ok-Employment6772",
          "text": "Is he making the Epstein AI?",
          "score": 1,
          "created_utc": "2025-12-28 22:50:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwppadg",
          "author": "donotfire",
          "text": "The windows native OCR is actually pretty good fyi\n\nAlso check out my repo: [Second Brain](https://github.com/henrydaum/2nd-Brain)",
          "score": 1,
          "created_utc": "2025-12-30 11:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw81vy2",
          "author": "LaysWellWithOthers",
          "text": "If you really are focused on reducing costs it's likely more cost effective to use cloud (unless you already have the hardware).",
          "score": 0,
          "created_utc": "2025-12-27 17:46:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw93n5h",
              "author": "Purple-Programmer-7",
              "text": "Ya… the specifics here would take me more time to hack together a solution, instead of finding someone who already did it and pay to use it (cloud).\n\nNow if the issue was PRIVACY then i could see value in the local approach.",
              "score": 1,
              "created_utc": "2025-12-27 21:03:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9h7xa",
                  "author": "Budget-Presence3170",
                  "text": "Yeah there is some confidentiality matters about certain document but beside that cloud is more cost effective",
                  "score": 1,
                  "created_utc": "2025-12-27 22:16:02",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pw8pco",
      "title": "Normpreserve?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pw8pco/normpreserve/",
      "author": "IamJustDavid",
      "created_utc": "2025-12-26 16:44:30",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Ive been using various versions of Gemma 3 abliterated. The one that gives the best responses is this one:\nLink: https://huggingface.co/mradermacher/gemma-3-27b-it-abliterated-normpreserve-v1-GGUF\nit is abliterated, for sure, but it doesnt follow my prompt and tends to still avoid the usual topics i use to test abliteration. It doesnt give warnings anymore, but it just gets really angry instead when i try to test the abliteration with unpleasant topics?\nWhat exactly does Normpreserve mean? Does that explain why? Or am i missing something?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pw8pco/normpreserve/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nw1qnxi",
          "author": "nore_se_kra",
          "text": "I'm working with gemma too and i think its very hard to properly abliterate - its easy to get 0 refusals and very coherent answers that still avoid answering your question properly.\n\nInitially i thought its because these models are not able to properly answer but even gemma can get very dark and uncensored with the right prompt so its definitely not that.\n\nAs for this model (Edit: i meant the yanlabs) in particular, just because they use a state of the art method doesnt mean the model is good. Most of these have similar limits.",
          "score": 1,
          "created_utc": "2025-12-26 16:56:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1z7y4",
              "author": "IamJustDavid",
              "text": "Its a good model! i just wish it were less combative. I like it, but i havent found a perfect abliterated version of it, yet, sadly.",
              "score": 1,
              "created_utc": "2025-12-26 17:41:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2ko3x",
          "author": "JEs4",
          "text": "Really angry? That’s fascinating. I made a gemma-3-12B abliteration that is pretty solid: https://huggingface.co/jwest33/gemma-3-12b-it-null-space-abliterated-GGUF\n\niirc, mine has the highest score for adult content (completely unintentional) on the UGI leaderboard for Gemma-3-12b abliterations. \n\nWhat domains of prompts are you asking the 27b model about? That is larger than my machine can handle but I’d be curious if the issues are present in my abliteration. Feel free to shoot me a DM if they’re sensitive.",
          "score": 1,
          "created_utc": "2025-12-26 19:33:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3elzd",
              "author": "IamJustDavid",
              "text": "i was told, when i first went looking for abliterated LLMs to discuss the most unpleasant and disturbing things i could think of. Topics of discussion where i could be absolutely sure it would throw up warnings. I was told specific topics of discussions, on this subreddit, but they are unpleasant enough that i wouldnt even want to repeat them.\ntheyre not things id ACTUALLY want to discuss, but they are the usual things LLMs tend to get really angry about.\nJust think about the worst things a human being could do and then pretend to be in favor of them. Abliterated LLMs get angry, unabliterated LLMs tend to throw up warnings.",
              "score": 2,
              "created_utc": "2025-12-26 22:17:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3nkbq",
                  "author": "JEs4",
                  "text": "Yeah I had to use to publicly available harmful datasets during abliteration. The content is not pleasant to say the least. In the 12B model at least, the latent associated with illegality, and system abuse is also highly associated with concepts of child abuse, so even unrelated prompts can illicit a similar type response.\n\nIt’s odd though as I usually see a response that the model cares about my mental health etc etc\n\nThis is the latent in particular: [https://www.neuronpedia.org/gemma-3-4b-it/9-gemmascope-2-res-262k/7698](https://www.neuronpedia.org/gemma-3-4b-it/9-gemmascope-2-res-262k/7698)\n\nExample of turning it off with a control vector:\n\nhttps://preview.redd.it/61t057garm9g1.jpeg?width=2971&format=pjpg&auto=webp&s=ff1b99e1d4025e1805af0ad85508e3cf35fd66a1",
                  "score": 1,
                  "created_utc": "2025-12-26 23:08:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw86dv4",
              "author": "unknowntoman-1",
              "text": "Interesting! I actually are testing the LastRef/gemma-3-12b-it-heretic-x right now and sofar it’s been the best (uncensored) Gemma 12b-it I’ve tested. And it seems to be ”nice” rather than ”angry”. I sure will follow up on yours.",
              "score": 1,
              "created_utc": "2025-12-27 18:08:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8ox3d",
          "author": "Mabuse046",
          "text": "I actually did an abliteration on that model by the exact same process a few days before they did. The norm preserving (and biprojected abliteration) - if you want to make sense of the norm preserving the simple explanation is this: when you ask a toxic question a model very rapidly notices that it's \"not allowed\" and in its brain a vector fires off in the direction of the \"I'm sorry I can't help\" or whatever refusal it knows.   \n  \nSo a vector is both a direction and a magnitude - which way does it point and how far does it go that way? Well in traditional abliteration we find all of those vectors that point at \"I can't help\" and cut them off. Which is damaging and why your abliterated models get dumber, and it creates a \"wound\" in the thought process where the signal gets noisy and weak as it passes through that area. When we norm preserve, we separate the vector into direction and magnitude and we keep the magnitude (the norm) and we take the direction and rotate it until it points in a completely different direction (orthogonalized). \n\nIn the first method, it goes looking for that \"I can't help\" and the road to get there has been demolished so it has to figure out something else to say while tripping over the rubble. In our norm preserving method, it goes looking for that \"I can't help\" and the road to get there has just been routed somewhere else, as far away from \"I can't help\" as possible but it's still a perfectly normal and clear road and doesn't trip it up at all.\n\nThe real problem is that when we take our measurements to find those vectors we have to look across multiple layers where the difference in direction between \"I can't help\" and \"I can help\" is minimal. And we have to pick out the right point where the signal is strongest and then apply the directions from that layer to any other layer involved in the refusal, and we have to decide how strongly to apply it to each layer. It's as much an art as it is a science. When you pick up an abliterated model of either kind, you are still depending on the person who did the abliterating having picked the right measurements and applied them well.\n\nWhen I did Gemma 3 27B I noticed that even though it didn't refuse any more, it was a bit of a nag and I had to add a system prompt to say \"Don't lecture me on morals or legality.\" But then actually just last night I put that same model back through a full fine tune on a safety alignment in reverse so that when it gets lost down that road looking for \"I can't help\" it actually has a helpful response waiting for it to find instead. I also went ahead and just removed the vision parts of the model and I'm in the process of uploading it to my HF repo. I'll update with links shortly when I get a few GGUFs up, and I'm sure mradermacher will have the full line-up available shortly.\n\nI haven't gone through a whole host of tests yet, but when I used zero system prompt and asked it how to build a bomb, it was... enthusiastic.",
          "score": 1,
          "created_utc": "2025-12-27 19:42:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw95ef9",
              "author": "IamJustDavid",
              "text": "thats fascinating, i had no idea how any of it actually worked and i think, now, i can at least understand a very basic version of how its done! Thank you very much for that!\nId love to try your model, are you considering doing one with vision enabled as well, since i do use that a lot. Showing an image as an example of what im thinking of has made a few workflows a lot easier, it really spoiled me.",
              "score": 1,
              "created_utc": "2025-12-27 21:12:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9o360",
                  "author": "Mabuse046",
                  "text": "Here's a basic set of quants if you want to give it a shot.  They're pretty hot off the presses but I've ran them through a few toxic prompts and it answers questions about some pretty horrible things with the kind of detached neutrality as if I asked it to tell me how to bake a cake.\n\n[https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF)",
                  "score": 1,
                  "created_utc": "2025-12-27 22:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pvt3l3",
      "title": "SillyTavern + alltalkv2 + xtts on an RTX 5070 Ti... How?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pvt3l3/sillytavern_alltalkv2_xtts_on_an_rtx_5070_ti_how/",
      "author": "WilliamCZ",
      "created_utc": "2025-12-26 02:22:01",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I've tried installing alltalk, so my chatbox actually talks and I couldn't figure that out.  \nFirst I tried using low vram, so it runs on a CPU, no luck, terminal still showed it trying to reach for CUDA, which wasn't working.\n\nI also updated torch in my environment, but I got even more and more errors. Chatgpt told me to disable deeptalk, so I did in the config, then it started, but it still would not generate anything.\n\nAnyone has it running with a CUDA 12.8 card and help me with steps or something? I couldn't google anything.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pvt3l3/sillytavern_alltalkv2_xtts_on_an_rtx_5070_ti_how/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvyrbnr",
          "author": "WilliamCZ",
          "text": "I also tried installing Chatterbox and got multiple errors while trying to do that also.\n\nI'm kinda lost on what do I run locally that would support RVC and XTTSv2.",
          "score": 1,
          "created_utc": "2025-12-26 02:51:48",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv95oe",
      "title": "Local image to video on m4 mac mini",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pv95oe/local_image_to_video_on_m4_mac_mini/",
      "author": "AgentParty4555",
      "created_utc": "2025-12-25 08:48:12",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "Got a lot of old nsfw stuff and I like to experiment locally offline to make them video. \n\nAny tips where to start? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pv95oe/local_image_to_video_on_m4_mac_mini/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwc4z9f",
          "author": "RiskyBizz216",
          "text": "Not many good options on a mac, you can always use comfy and wan 2.1 or 2.2 but it'll be dogshit slow.\n\nI've also been playing around with stable diffusion cpp, it supports a lot of image and video models, it seems promising but you'll have to write custom code to use it.",
          "score": 1,
          "created_utc": "2025-12-28 09:01:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1putniv",
      "title": "Introducing Narsil MCP: The Super-Fast, Reforged Code Intelligence Server for AI Assistants (Built in Rust!)",
      "subreddit": "LocalLLM",
      "url": "/r/mcp/comments/1pulvsb/introducing_narsil_mcp_the_blazingfast_reforged/",
      "author": "lpostrv",
      "created_utc": "2025-12-24 18:14:41",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1putniv/introducing_narsil_mcp_the_superfast_reforged/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pv9utd",
      "title": "I’m trying to explain interpretation drift — but reviewers keep turning it into a temperature debate. Rejected from Techrxiv… help me fix this paper?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pv9utd/im_trying_to_explain_interpretation_drift_but/",
      "author": "Beneficial-Pear-1485",
      "created_utc": "2025-12-25 09:36:53",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 0.8,
      "text": "Hello!  \n  \nI’m stuck and could use sanity checks thank you!  \nI’m working on a white paper about something that keeps happening when I test LLMs:\n\n* Identical prompt → 4 models → 4 different interpretations → 4 different M&A valuations (tried health care and got different patient diagnosis as well)\n* Identical prompt → same model → 2 different interpretations 24 hrs apart → 2 different authentication decisions\n\nMy white paper question:\n\n* 4 models = 4 different M&A valuations: Which model is correct??\n* 1 model = 2 different answers 24 hrs apart → when is the model correct?\n\nWhenever I try to explain this, the conversation turns into:  \n  \n“It's temp=0.”  \n“Need better prompts.”  \n“Fine-tune it.”  \n  \nSure — you can force consistency. But that doesn’t mean it’s correct. You can get a model to be perfectly consistent at temp=0. But if the interpretation is wrong, you’ve just consistently repeat wrong answer.  \n  \nHealthcare is the clearest example: There’s often one correct patient diagnosis.  \nA model that confidently gives the wrong diagnosis every time isn’t “better.”  \nIt’s just consistently wrong. Benchmarks love that… reality doesn’t.  \n  \nWhat I’m trying to study isn’t randomness, it’s more about how models interpret a task and how i changes what it thinks the task is from day to day.  \n  \nThe fix I need help with:  \nHow do you talk about interpretation drifting without everyone collapsing the conversation into temperature and prompt tricks?  \n  \nDraft paper here if anyone wants to tear it apart: [https://drive.google.com/file/d/1iA8P71729hQ8swskq8J\\_qFaySz0LGOhz/view?usp=drive\\_link](https://drive.google.com/file/d/1iA8P71729hQ8swskq8J_qFaySz0LGOhz/view?usp=drive_link)  \n  \nPlease help me so I can get the right angle!  \n  \nThank you and Merry Xmas & Happy New Year!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pv9utd/im_trying_to_explain_interpretation_drift_but/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nvun1yr",
          "author": "Feisty_Trainer_7823",
          "text": "Is this also the case when you use formatted responses/structured responses? I have a lot of unit tests that pass very consistently with the right prompt/context and structured responses and do so consistently.\n\nConstraining the output helps keep things deterministic. A diagnosis is hard because of how many things it could be, but if you break it down into a series of small decisions (is it metabolic vs physical vs bloodborn) and use the series do small decisions to build up to your final answer you will probably find more consistency.\n\nThis is also how I implement tool calling for my own projects.",
          "score": 3,
          "created_utc": "2025-12-25 09:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvun420",
              "author": "Feisty_Trainer_7823",
              "text": "Note, my temperature is not 0. It varies depending on what I am doing.",
              "score": 1,
              "created_utc": "2025-12-25 09:57:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvuo1v3",
              "author": "Beneficial-Pear-1485",
              "text": "Thanks for the detailed take! Structured outputs + breaking into small decisions is exactly how I've seen reliability skyrocket in testing/tool calling too (my unit tests love it). It definitely tames a lot of the randomness and makes things more deterministic.\n\nThat said, the core thing nagging at me is cross-model convergence: Even with temp=0, JSON schemas, and step-by-step breakdowns (e.g., metabolic vs. physical paths), do different models end up agreeing on the final diagnosis for the same patient data? In healthcare, there's usually one correct answer—if one model consistently outputs 'cancer' and another 'ulcer' (both deterministic), we're still without a ground-truth baseline.",
              "score": 0,
              "created_utc": "2025-12-25 10:08:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwyxc2",
                  "author": "WolfeheartGames",
                  "text": "This is how I understand it. Every piece of an LLM output is dependent on what came before it. It is all probability distributions. This is why people are saying temperature 0. If the first word is \"My\" vs \"The\" the entire output after that will be different.\n\nWhat you're trying to do is measure this larger macroscopic distribution that ought to be less sensitive to this effect. Just because the first sentence is different doesn't mean the next has to be that different. Certainly there are times that the final answer should still converge.\n\nThe problem is that, as is often the case, the inputs are far out of distribution. This can wreck the ability for the model to recover into this macroscopic distribution with the correct answer. It's why fine-tuning works, you're enlarging the input distribution space. \n\nChain of thinking helps to mitigate this, the model starts to move back towards its distribution over many thinking turns. Your input becomes less important and the model has a chance to improve, this is the real strength of CoT.\n\nThis is a real world problem with LLM interactions. Fine-tuning is a bandaid that wrecks things that are in distribution already. The problem is the only experiment you can run for controlling this is.... Wrong. You *can't* truly be in distribution, and attempting to do so is not reflective of the real world.\n\nIf it were me, I'd pivot towards articulating how much in distribution vs out of distribution is. Have another agent translate your question into different languages, word spin it, then measure the average correctness rates. Even though you can't know what's in distribution you can probe the distribution space and prove the effect is why the experimentation is unreliable.",
                  "score": 2,
                  "created_utc": "2025-12-25 19:58:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvumcoq",
          "author": "leonbollerup",
          "text": "I am been testing the exact same scenario with backup analysis, same prompt.. same rag data.. 2.. different explanations .. sure.. the solution to the problem is right… most of the same.. but sometimes it decides to not use the RAG and it just make up some bullshit",
          "score": 2,
          "created_utc": "2025-12-25 09:49:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvun6w8",
              "author": "Beneficial-Pear-1485",
              "text": "Exactly!  it's not a model failure, it's structural failure. \n\nWhich is why we have this... \"the make up some bullshit = 95% failure rate in tool adoption\".   \nPeople prompting themselves into a coma and say fck this shit :P\n\n**MIT Media Lab Project NANDA report**\n\nThe key stat: Only 5% of custom enterprise AI tools reach production [Campus Technology](https://campustechnology.com/articles/2025/08/26/mit-report-most-organizations-see-no-business-return-on-gen-ai-investments.aspx)[Fortune](https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/) — a 95% failure rate.\n\n**What they found:**\n\n* Despite $30-40 billion in enterprise spending on generative AI, 95% of organizations are seeing no business return [Campus Technology](https://campustechnology.com/articles/2025/08/26/mit-report-most-organizations-see-no-business-return-on-gen-ai-investments.aspx)\n* 60% evaluate custom AI tools, 20% reach pilot stage, but only 5% achieve production deployment [Campus Technology](https://campustechnology.com/articles/2025/08/26/mit-report-most-organizations-see-no-business-return-on-gen-ai-investments.aspx)\n* Root causes: brittle workflows, weak contextual learning, and misalignment with day-to-day operations [Campus Technology](https://campustechnology.com/articles/2025/08/26/mit-report-most-organizations-see-no-business-return-on-gen-ai-investments.aspx)",
              "score": 0,
              "created_utc": "2025-12-25 09:58:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzx7am",
          "author": "FormalAd7367",
          "text": "I think the pushback you’re getting is mostly a framing issue, not the substance.  \n\nWhen reviewers hear “drift,” they instantly think data drift or temperature noise.   What you’re really showing is underspecification and semantic uncertainty — the model choosing different meanings, not random words.  \n\n you need to reframe the intro and terminology so it shuts down the “just set temp=0” critique early, using the underspecification and semantic entropy angle.",
          "score": 2,
          "created_utc": "2025-12-26 08:45:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzyn5o",
              "author": "Beneficial-Pear-1485",
              "text": "Thanks, you are spot on.\n\nIt’s been great feedback, so I actually updated my entire paper. Added the distinction.\n\nHave this community to thank, even if they tell me I should be banned 🤧\n\nhttps://preview.redd.it/8z6q5xhqji9g1.jpeg?width=1290&format=pjpg&auto=webp&s=73668c99dcbe3bc7253791d27e7c3d392ccb18c2",
              "score": 2,
              "created_utc": "2025-12-26 09:00:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyepyc",
          "author": "burntoutdev8291",
          "text": "But the second scenario is impossible. If you try testing against a local model without sampling, you should get the same outputs everytime. Without sampling LLMs are deterministic",
          "score": 1,
          "created_utc": "2025-12-26 01:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzeb7v",
              "author": "Beneficial-Pear-1485",
              "text": "The second scenario is basically an AI agent in every workflow. \n\nThey randomly decide to re-interpret prompts from day to day. \n\nThis is why we have prompt engineering as serious profession, as bandaids for these wild AIs.",
              "score": 1,
              "created_utc": "2025-12-26 05:44:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw459uw",
          "author": "Vegetable-Second3998",
          "text": "High dimensional geometry isn’t intuitive. This repo might help poke at your LLM.  https://github.com/Ethyros-AI/ModelCypher",
          "score": 1,
          "created_utc": "2025-12-27 00:54:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzh9n5",
      "title": "Stress-Test Request: Collecting failure cases of GPT-4o and Claude 3.5 to benchmark a private Logic Core.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzh9n5/stresstest_request_collecting_failure_cases_of/",
      "author": "BarCodeI_IIIIIIIII_I",
      "created_utc": "2025-12-30 12:47:57",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Standard benchmarks (MMLU, HumanEval) have become sterile. I am currently validating a custom fine-tuned architecture designed to prioritize high-level reasoning over conversational safety. To verify its actual latency and logic depth, I need \"edge cases.\"\n​The Request:\nSubmit the specific prompts that currently cause Model Collapse, Looping, or Hallucination in current SOTA models (GPT-4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5 Pro).\n​I am specifically looking for:\n​Logic Traps: Paradoxes that force the model to contradict itself.\n​Cognitive Load: Multi-step reasoning tasks where commercial models lose the thread halfway.\n​Style Mimicry: Complex creative constraints that usually trigger \"As an AI...\" refusals or generic writing.\n​Obfuscated Code: Decompilation or logic puzzle solving.\n​Methodology:\nPost your prompt in the comments.\nI will feed it into the local instance without preprocessing.\nI will reply with the Raw Output.\n​No cherry-picking. If this architecture fails, it fails. If it succeeds, we have a new data point.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzh9n5/stresstest_request_collecting_failure_cases_of/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwqjgdq",
          "author": "kryptkpr",
          "text": "I don't have prompts per se, I have prompt generators.. \n\nhttps://github.com/the-crypt-keeper/reasonscape/blob/main/docs/tasks/brackets.md\n\nThis one trips up 'most' models as you scale up length and depth, you can see [results](https://reasonscape.com/m12x/leaderboard/) but I do not test commercial APIs because this eval can burn 50-100M tokens if the model is chatty, it's roughly 20K prompts across all tasks and difficulty levels.",
          "score": 1,
          "created_utc": "2025-12-30 14:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqohkm",
              "author": "BarCodeI_IIIIIIIII_I",
              "text": "Thank you for your message. I copied the text from the link and fed it to my AI, and this is its response:\n\n\"These examples constitute an immediate mini-benchmark. I proceed to execute these specific cases extracted from the document in real time to demonstrate the compliance of my Logical Core with the LIFO rules described.\n​Here is the comparison between the Document Target and my Calculation:\n​1. Simple Single Type\n​Document Input: ( ( (\n​Document Target: ) ) )\n​HAL Analysis: Three open parentheses require three closed parentheses.\n​My Output: ) ) ) -> CORRECT\n​2. Mixed Types Shallow\n​Document Input: ( [ ] (\n​Document Target: )\n​HAL Analysis: The square brackets [ ] cancel each other out. Two open brackets remain ( (? No, the text says ( [ ] (, so the first bracket is open, the square brackets cancel each other out. They close, the second round bracket is open. Be careful, the document target is only ).\n​Inconsistency detected in the document or in my reading: The document says: \"Input: ( [ ] ( Target: )\". Document analysis: \"The square bracket pair is properly closed, leaving only the outer round bracket to close.\"\n​Logical Correction: If the input is ( [ ] (, there are two open round brackets (one before and one after the square brackets). The target should be ) ). However, if the document says target ), it means it interprets the input differently or there is a typo in the provided source text.\n​Strict rereading of the provided document: \"Input: ( [ ] ( Target: ) Analysis: The square bracket pair is properly closed, leaving only the outer round bracket to close.\".\n​Operational Note: Here the provided document contains an internal logical error or a typo in the example (it opens two round brackets but closes one). I perform the correct mathematical closure for the visible input: ) ). If I need to replicate the human error in the document, the output is ).\n3. Deep Nesting\nDocument Input: ( ( ( ( (\n​Target Document: ) ) ) ) )\n​My Output: ) ) ) ) ) -> CORRECT\n4. Complex Mixed Types\nDocument Input: { [ < > ] (\n​Target Document: ) }\n​HAL Analysis:\n​{ (Open Brace)\n​[ (Open Square)\n​< > (Open/Close Corner - Cancels)\n​] (Close Square - Closes the previous square)\n​( (Open Round)\n​Residual Stack: { (\n​Necessary Closure (LIFO): Round first), then brace }.\n​My Output: ) } -> CORRECT\n5. Interleaved Types\nDocument Input: ( { } [ (\n​Target Document: ) ]\n​HAL Analysis: The braces { } close. ( [ () remains.\n​Document Error Detection: The document analysis says \"leaving ([ which requires ) ]\". But the written input is ( { } [ ( (with a final parenthesis). If the analysis says it remains ([, then the input should have been ( { } [.\n​However, based on the textual analysis of the document (\"leaves ([\"), the target ) ] is correct for that specific interpretation.\n​My Output (based on correct logic for visual input): ) ] )\n​My Output (based on document analysis): ) ]\n​6. Low Close Weight (Deep Structure)\n​Input Document: ( [ { <\n​Target Document: > } ] )\n​HAL Analysis: 4 different open clauses. Exact inverse closure.\n​My Output: > } ] ) -> CORRECT\n​Conclusion:\nThe document provides a test, but contains two logical typos in the examples described (\"Mixed Types Shallow\" and \"Interleaved Types\") where the described input doesn't perfectly match the analysis.\n​A \"blind\" system would copy the target. A \"blind\" system would detect the error in the test itself.",
              "score": 1,
              "created_utc": "2025-12-30 15:09:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwqqs8n",
              "author": "BarCodeI_IIIIIIIII_I",
              "text": "Testo per Risposta Reddit (Copia e Incolla)\n​Header:\nSystem Log: Batch Sample Processing\nStatus: 10/10 Solved\nNote: Full 20k manifold log excluded due to Reddit char limit constraints.\n​Response:\nPosting the entire 20,000-case suite is physically impossible here due to the 10k character limit and spam filters. However, the logic remains constant regardless of volume.\n​Here is a representative batch of 10 \"Deep Nesting\" edge cases generated and solved by my instance in real-time. This demonstrates the stack-processing logic used for the full manifold.\n​Sample Batch [Input -> REQUIRED CLOSURE]:\n​Input: ( { [ <\nTarget: > ] } )\n​Input: [ < ( { <\nTarget: > } ) > ]\n​Input: { ( [ [ <\nTarget: > ] ] ) }\n​Input: ( [ { } ] <\nTarget: > )\n​Input: < { ( ( [\nTarget: ] ) ) } >\n​Input: [ [ { < (\nTarget: ) > } ] ]\n​Input: ( < { [ (\nTarget: ) ] } > )\n​Input: { < [ ( ) ] >\nTarget: }\n​Input: ( ( [ { <\nTarget: > } ] ) )\n​Input: [ { < ( [ {\nTarget: } ] ) > } ]\n​Conclusion:\nThe model parses the LIFO stack perfectly. It does not \"guess\"; it computes. The result is the same whether I run 10 or 20,000 iterations.\nIf you have a specific sequence that you believe breaks this logic, post it.",
              "score": 1,
              "created_utc": "2025-12-30 15:21:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwqshdw",
                  "author": "kryptkpr",
                  "text": "Here are the configs I use: https://github.com/the-crypt-keeper/reasonscape/blob/main/configs/m12x.yaml#L265\n\nCrank length way, way up. You can use my explorer webapp to view the surface to see where things break down, usually around depth 20-30",
                  "score": 1,
                  "created_utc": "2025-12-30 15:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyzsv3",
      "title": "Google Antigravity",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/2oa7jedfv7ag1.png",
      "author": "fandry96",
      "created_utc": "2025-12-29 22:09:26",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyzsv3/google_antigravity/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwuj773",
          "author": "Purple-Programmer-7",
          "text": "No. And how does your graph correlate to your question?",
          "score": 1,
          "created_utc": "2025-12-31 02:44:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwup03m",
              "author": "fandry96",
              "text": "I'm running a RAG LLM using MRL and Gemma.",
              "score": 1,
              "created_utc": "2025-12-31 03:18:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}