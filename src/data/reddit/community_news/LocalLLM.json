{
  "metadata": {
    "last_updated": "2026-01-25 08:55:25",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 148,
    "file_size_bytes": 171960
  },
  "items": [
    {
      "id": "1qi5q2v",
      "title": "768Gb Fully Enclosed 10x GPU Mobile AI Build",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1qi5q2v",
      "author": "SweetHomeAbalama0",
      "created_utc": "2026-01-20 16:27:57",
      "score": 197,
      "num_comments": 53,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qi5q2v/768gb_fully_enclosed_10x_gpu_mobile_ai_build/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0p1l13",
          "author": "Proof_Scene_9281",
          "text": "ðŸ’¦\n\nDo you run the PSUâ€™s on different circuits in the house?Â ",
          "score": 26,
          "created_utc": "2026-01-20 16:56:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ppwds",
              "author": "betacore_tec",
              "text": "Things you don't care about in a 230v country",
              "score": 12,
              "created_utc": "2026-01-20 18:47:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0prqth",
                  "author": "grubnenah",
                  "text": "Just the GPUs at 250w x8 + 500w x2 could trip a 240V 15A breaker after a few minutes.",
                  "score": 6,
                  "created_utc": "2026-01-20 18:55:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pbfhl",
          "author": "granoladeer",
          "text": "We found the Powerball winner",
          "score": 16,
          "created_utc": "2026-01-20 17:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0p160v",
          "author": "Paliknight",
          "text": "lol do each of your rooms have 30 amp circuits?",
          "score": 7,
          "created_utc": "2026-01-20 16:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0to177",
              "author": "PineappleLemur",
              "text": "Normally 13-15A at 220-240",
              "score": 1,
              "created_utc": "2026-01-21 08:44:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0u5hzm",
                  "author": "Paliknight",
                  "text": "OP is in the US",
                  "score": 1,
                  "created_utc": "2026-01-21 11:25:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pe0a9",
          "author": "jedsk",
          "text": "How are your temps?",
          "score": 6,
          "created_utc": "2026-01-20 17:54:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pme80",
          "author": "OnlyAssistance9601",
          "text": "Prob sounds like a 747 taking off .",
          "score": 7,
          "created_utc": "2026-01-20 18:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0s5gxp",
              "author": "mjTheThird",
              "text": "OP no longer needs house heater.",
              "score": 2,
              "created_utc": "2026-01-21 02:07:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0owuzr",
          "author": "AlyoshaKaramazov_",
          "text": "Looks like Julia Roberts in Pretty Woman ðŸ˜",
          "score": 5,
          "created_utc": "2026-01-20 16:34:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qm98f",
              "author": "Grinch0127",
              "text": "A horse?",
              "score": 2,
              "created_utc": "2026-01-20 21:16:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0p7oko",
          "author": "RentalGore",
          "text": "I'm also interested to understand how you'll power all this.  I would imagine at full tilt, you're way beyond what even a 20amp circuit can do.  Do you have this on a battery that can push wattage to 2400 or so?",
          "score": 3,
          "created_utc": "2026-01-20 17:25:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qpfqu",
              "author": "enigma62333",
              "text": "If the OP is not in the US and in Europe a single residential circuit could support both PSUâ€™s. \n\nIf they are in North America they could either have two 20A / 110V circuits or a single 2 Phase 208 20A circuit for both PSUâ€™s. \n\nThe 2 x 20A / 110 circuits would be tight but manageable if nothing else was running on them.\n\nAt my house I installed a 2-pole 20A breaker and can power at 3.3KW continuously without fear of tripping the breaker.",
              "score": 1,
              "created_utc": "2026-01-20 21:31:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0p3o3v",
          "author": "brianlmerritt",
          "text": "Great project - one question.  Is it now your daily driver for AI related work?",
          "score": 2,
          "created_utc": "2026-01-20 17:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pab9x",
          "author": "shadowsyntax43",
          "text": "your power company should visit your house in exactly one week.",
          "score": 2,
          "created_utc": "2026-01-20 17:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1grm6z",
          "author": "VaporyCoder7",
          "text": "Can it run doom?",
          "score": 2,
          "created_utc": "2026-01-24 18:03:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pmyaw",
          "author": "AlexGSquadron",
          "text": "But how did you do it? You used sharding to split model?",
          "score": 1,
          "created_utc": "2026-01-20 18:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0psu6k",
          "author": "TomatoWasabi",
          "text": "Where do you live ?",
          "score": 1,
          "created_utc": "2026-01-20 19:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0q4ql2",
              "author": "serious153",
              "text": "My name is Walter Hartwell White. I live at 308 Negra Arroyo Lane, Albuquerque, New Mexico, 87104. This is my confession. If you're watching this tape, I'm probably dead, murdered by my brother-in-law Hank Schrader. Hank has been building a meth empire for over a year now and using me as his chemist. Shortly after my 50th birthday, Hank came to me with a rather, shocking proposition. He asked that I use my chemistry knowledge to cook methamphetamine, which he would then sell using his connections in the drug world. Connections that he made through his career with the DEA. I was... astounded, I... I always thought that Hank was a very moral man and I was... thrown, confused, but I was also particularly vulnerable at the time, something he knew and took advantage of. I was reeling from a cancer diagnosis that was poised to bankrupt my family. Hank took me on a ride along, and showed me just how much money even a small meth operation could make. And I was weak. I didn't want my family to go into financial ruin so I agreed. Every day, I think back at that moment with regret. I quickly realized that I was in way over my head, and Hank had a partner, a man named Gustavo Fring, a businessman. Hank essentially sold me into servitude to this man, and when I tried to quit, Fring threatened my family. I didn't know where to turn. Eventually, Hank and Fring had a falling out. From what I can gather, Hank was always pushing for a greater share of the business, to which Fring flatly refused to give him, and things escalated. Fring was able to arrange, uh I guess I guess you call it a \"hit\" on my brother-in-law, and failed, but Hank was seriously injured, and I wound up paying his medical bills which amounted to a little over $177,000. Upon recovery, Hank was bent on revenge, working with a man named Hector Salamanca, he plotted to kill Fring, and did so. In fact, the bomb that he used was built by me, and he gave me no option in it. I have often contemplated suicide, but I'm a coward. I wanted to go to the police, but I was frightened. Hank had risen in the ranks to become the head of the Albuquerque DEA, and about that time, to keep me in line, he took my children from me. For 3 months he kept them. My wife, who up until that point, had no idea of my criminal activities, was horrified to learn what I had done, why Hank had taken our children. We were scared. I was in Hell, I hated myself for what I had brought upon my family. Recently, I tried once again to quit, to end this nightmare, and in response, he gave me this. I can't take this anymore. I live in fear every day that Hank will kill me, or worse, hurt my family. I... All I could think to do was to make this video in hope that the world will finally see this man, for what he really is",
              "score": 8,
              "created_utc": "2026-01-20 19:55:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qgc50",
          "author": "mr__smooth",
          "text": "Wow I am looking for this exact kind of build. I currently have a prototyping machine but looking for something more powerful(https://www.reddit.com/r/LocalLLaMA/comments/1qcykx4/home\\_workstation\\_vs\\_nycnj\\_colo\\_for\\_llmvlm\\_whisper/) I'm really impressed by this wondering if I would be able to run it in my apartment, but concerned about the power.",
          "score": 1,
          "created_utc": "2026-01-20 20:49:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qsmhs",
          "author": "Pixer---",
          "text": "Are you able to run vllm instead of llamacpp, and how much performance that would bring ?",
          "score": 1,
          "created_utc": "2026-01-20 21:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r912m",
          "author": "Psychological_Ear393",
          "text": "I like the part where mobile means it has castors.  It's glorious.",
          "score": 1,
          "created_utc": "2026-01-20 23:07:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rczpj",
          "author": "HealthyCommunicat",
          "text": "If Kimi K2 was a physical object:",
          "score": 1,
          "created_utc": "2026-01-20 23:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rfxk5",
          "author": "asciimo",
          "text": "Itâ€™s mobile because of the wheels?",
          "score": 1,
          "created_utc": "2026-01-20 23:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rtv8e",
          "author": "BroderLund",
          "text": "The motherboard has 7 x16 slots. How did you connect 10 GPUs? Biforcation adapter that gives 2 x8 slots with ribbon cables to the GPUs?",
          "score": 1,
          "created_utc": "2026-01-21 01:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0s0yra",
          "author": "No-Leopard7644",
          "text": "For a spec to run large models - what was the reason for 3090 and 5090 that are consumer grade cards and not enterprise scale ones.",
          "score": 1,
          "created_utc": "2026-01-21 01:42:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0sqmbg",
              "author": "Barachiel80",
              "text": "He stated the cost of the RTX Pro 6000's would have eaten the entire project budget. I assume he went with those particular consumer cards over comparable vram RTX Pro 4000 & 4500 series enterprise cards for the additional memory bandwidth and Cuda cores.",
              "score": 1,
              "created_utc": "2026-01-21 04:13:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s4bl3",
          "author": "themostofpost",
          "text": "How does the performance compare to something like Claude code?",
          "score": 1,
          "created_utc": "2026-01-21 02:01:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sk7id",
          "author": "Fearless_Weather_206",
          "text": "A coffee table you never want a spill to happen on â˜•ï¸",
          "score": 1,
          "created_utc": "2026-01-21 03:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0so2sd",
          "author": "RatioOtherwise1185",
          "text": "in todayâ€™s market that probably cost 2 kidneys, a lung, some parts of your vertebrae, an eye and an ear.",
          "score": 1,
          "created_utc": "2026-01-21 03:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tnxyj",
          "author": "Road-Runnerz",
          "text": "You should get the top and bottom extensions for the case so you have more more room",
          "score": 1,
          "created_utc": "2026-01-21 08:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xatt4",
          "author": "dickofthebuttt",
          "text": "If you turn it on and unlock the wheels, does it push itself across the floor?",
          "score": 1,
          "created_utc": "2026-01-21 20:50:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xwzoj",
          "author": "Complete_Lurk3r_",
          "text": "lucky that thing is fully enclosed....HOLY SHIT its a mess in there!",
          "score": 1,
          "created_utc": "2026-01-21 22:33:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xygfe",
          "author": "External_Hippo_9283",
          "text": "Why didn't you use a DGX? It has faster installation, warranty, etc. I'm asking because I don't know.",
          "score": 1,
          "created_utc": "2026-01-21 22:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o128s0k",
          "author": "e11310",
          "text": "Wow crazy build. What is the at wall power consumption for this? Has to be wild.",
          "score": 1,
          "created_utc": "2026-01-22 15:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19d7l1",
          "author": "j4ys0nj",
          "text": "you got a 240V outlet for that bad boy?",
          "score": 1,
          "created_utc": "2026-01-23 16:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qmwuf",
          "author": "nsmitherians",
          "text": "But can it run doom?",
          "score": 1,
          "created_utc": "2026-01-20 21:19:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tlm04",
              "author": "ryfromoz",
              "text": "It might even manage Crysis!",
              "score": 2,
              "created_utc": "2026-01-21 08:21:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qq5xi",
          "author": "hyper_ny",
          "text": "2 mac studio will be easier and better with rdma",
          "score": 0,
          "created_utc": "2026-01-20 21:34:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql4hwj",
      "title": "RTX Pro 6000 $7999.99",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "author": "I_like_fragrances",
      "created_utc": "2026-01-23 22:05:23",
      "score": 65,
      "num_comments": 58,
      "upvote_ratio": 0.95,
      "text": "Price of RTX Pro 6000 Max-Q edition is going for $7999.99 at Microcenter.\n\n[https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card](https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card)\n\nDoes it seem like a good time to buy?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1c5v68",
          "author": "Green-Dress-113",
          "text": "If you want to run 2 or 4 GPUs, Max-Q is the way to go for cooling. I have dual blackwell 6000 pro workstations and one heats the other with the fans blowing sideways. While the power max is 600W I'm only seeing avg 300W during inference with peaks to 350W, but not full 600W consumption So Max-Q being 300W is the sweet spot for performance and cooling.",
          "score": 11,
          "created_utc": "2026-01-24 00:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d5r3r",
              "author": "SillyLilBear",
              "text": "I run two workstation cards I have them power limited to 300W and get 96% of the performance of 600W.",
              "score": 4,
              "created_utc": "2026-01-24 03:35:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1j8410",
                  "author": "m2845",
                  "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
                  "score": 1,
                  "created_utc": "2026-01-25 01:05:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j83rm",
              "author": "m2845",
              "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
              "score": 0,
              "created_utc": "2026-01-25 01:05:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bnzd7",
          "author": "Big_River_",
          "text": "the performance difference is more like 10% and yes 96gb gddr7 vram at 300w stable is best perf per watt there is to stack up a wrx90 threadripper build with - best card for home lab by far",
          "score": 18,
          "created_utc": "2026-01-23 22:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1db5zv",
              "author": "Sufficient-Past-9722",
              "text": "Just need a good waterblock for it.",
              "score": 1,
              "created_utc": "2026-01-24 04:09:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1dla2o",
                  "author": "Paliknight",
                  "text": "For a 300w card?",
                  "score": 1,
                  "created_utc": "2026-01-24 05:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1by4af",
          "author": "morriscl81",
          "text": "You can get it cheaper than that by at least $600-700 from companies like Exxact Corp",
          "score": 5,
          "created_utc": "2026-01-23 23:26:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bzwly",
              "author": "ilarp",
              "text": "how to order from them?",
              "score": 2,
              "created_utc": "2026-01-23 23:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c0iyo",
                  "author": "morriscl81",
                  "text": "Go to their website and make a request for a quote. They will send you an invoice.  Thatâ€™s how I got mine",
                  "score": 3,
                  "created_utc": "2026-01-23 23:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c08qr",
          "author": "MierinLanfear",
          "text": "Max q is lower power for if you want to run more than 2 cards on your workstation.   If your only running 1 or 2 cards go for the full version unless you plan on adding more later.\n\nThere are education discounts too.   Prices are likely to go up so now is a good time to buy.",
          "score": 4,
          "created_utc": "2026-01-23 23:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cgwrf",
          "author": "No-Leopard7644",
          "text": "Do you monetize this investment or itâ€™s for fun?",
          "score": 3,
          "created_utc": "2026-01-24 01:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dre0b",
          "author": "gaidzak",
          "text": "Cheapest I found so far is from provantage for non education $7261 dollars. I hope this is real or I am understanding this pricing. \n\n[https://www.provantage.com/nvidia-9005g153220000001\\~7NVID0M1.htm](https://www.provantage.com/nvidia-9005g153220000001~7NVID0M1.htm)\n\nFor education purchases, any NVidia Partner can get them down to $6000;",
          "score": 3,
          "created_utc": "2026-01-24 06:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bx3g3",
          "author": "separatelyrepeatedly",
          "text": "Get workstation and power limit it",
          "score": 5,
          "created_utc": "2026-01-23 23:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cecci",
              "author": "Big_River_",
              "text": "the max-q variant is also a blower card therefore well suited by design to stacking in a box - multiple workstation variants even power limited require powered ddr5 risers or they will thermal throttle each other - so if you only get one sure get a workstation otherwise max-q for sure",
              "score": 3,
              "created_utc": "2026-01-24 00:54:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1cg56k",
          "author": "Qs9bxNKZ",
          "text": "Naw.  Full power version.\n\nThen afterburner if you want to reduce power.  \n\nThen undervolt if you want to reduce heat.  \n\nMost people are not going to be running more than two of them in a case.  And I mean more than two because bifurcation along with chipset performance unless youâ€™re on a threadripper or Xeon \n\nHave two in one box and I put out about 1200W at 70% along with 950mv.  \n\n$8099 for the black box version.  $7999 for the bulk nvidia version.  Minus $2-400 for education and bulk discounts.",
          "score": 5,
          "created_utc": "2026-01-24 01:04:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dr26a",
              "author": "gaidzak",
              "text": "Education pricing for a RTX 6000 Pro Server is 6k.. I'm about to hit the BUY button.",
              "score": 1,
              "created_utc": "2026-01-24 06:02:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1egntb",
                  "author": "t3rmina1",
                  "text": "Where are you getting 6k? I'm getting edu quotes that are a bit higher for WS",
                  "score": 1,
                  "created_utc": "2026-01-24 09:49:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bij91",
          "author": "snamuh",
          "text": "Whatâ€™s the deal with the max-a ed?",
          "score": 4,
          "created_utc": "2026-01-23 22:06:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biody",
              "author": "I_like_fragrances",
              "text": "It is 300W power versus 600W. Typically gets a 20% reduced performance but same VRAM and cores.",
              "score": 8,
              "created_utc": "2026-01-23 22:07:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1bjn31",
                  "author": "hornynnerdy69",
                  "text": "20% reduced performance isnâ€™t nothing, especially when you consider that might put its performance below a 5090 for models that can fit in 32GB VRAM. And you could get a standard 6000 Pro for under 20% more money (seeing them at like $8750 recently)",
                  "score": 4,
                  "created_utc": "2026-01-23 22:12:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1bk6hr",
                  "author": "nero519",
                  "text": "I don't get it, what's the point of it? Do they just sell a normal card with artificial limitations to make it cheaper or is there something actually missing, like slower memories or less cores",
                  "score": 3,
                  "created_utc": "2026-01-23 22:14:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bzbor",
              "author": "getfitdotus",
              "text": "Better card i run 4 of these. All air cooled and work great max 60c",
              "score": 2,
              "created_utc": "2026-01-23 23:32:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1cep80",
          "author": "queerintech",
          "text": "I just bought a 5000 to pair with my 5070ti I considered the 6000 but whew.  ðŸ˜…",
          "score": 2,
          "created_utc": "2026-01-24 00:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dr57r",
          "author": "Foreign_Presence7344",
          "text": "Could you use the workstation version and a max q in the same box?",
          "score": 2,
          "created_utc": "2026-01-24 06:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dvdor",
          "author": "AlexGSquadron",
          "text": "They were going for $7500??",
          "score": 2,
          "created_utc": "2026-01-24 06:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f8zog",
          "author": "gweilojoe",
          "text": "Get from Computer Central - They donâ€™t charge tax for purchases outside of California. I bought mine from them (regular version not Qmax) and itâ€™s worked great.",
          "score": 2,
          "created_utc": "2026-01-24 13:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbkhb",
          "author": "Phaelon74",
          "text": "You can get them for 5800 ish if you spend the time to find the vendor, do inception program, etc.",
          "score": 2,
          "created_utc": "2026-01-24 13:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cfxrs",
          "author": "Cold_Hard_Sausage",
          "text": "Is this one of those gadgets thatâ€™s going to be 10 bucks in 5 years?",
          "score": 5,
          "created_utc": "2026-01-24 01:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dbxs7",
              "author": "Sufficient-Past-9722",
              "text": "If you had bought an Ampere A6000 back in January 2021, you could still sell it for something close to the original price. Similar for the 3090.",
              "score": 8,
              "created_utc": "2026-01-24 04:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1cjk1m",
              "author": "Br4ne",
              "text": "more like 20k in 2 months if you ask me",
              "score": 5,
              "created_utc": "2026-01-24 01:24:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fa25c",
              "author": "Mysterious-String420",
              "text": "Share a link to any current 10$ gadget with butt loads of vram",
              "score": 1,
              "created_utc": "2026-01-24 13:43:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1g1tep",
                  "author": "Cold_Hard_Sausage",
                  "text": "Bro, have someone teach you the concept of sarcasm",
                  "score": 1,
                  "created_utc": "2026-01-24 16:08:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ck7rm",
          "author": "Accomplished-Grade78",
          "text": "Anyone compare dual Max-q vs DGXA Spark? \n\n192GB vs 128GB\n\nDDR7 vs LPDDR5X unified \n\nWhat does this mean for real world performance, in your experience?",
          "score": 2,
          "created_utc": "2026-01-24 01:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1d0fbd",
          "author": "TheRiddler79",
          "text": "If I'm being fair, that thing looks bad to the fucking bone, but if I what's going to spend eight grand right now, I'd probably look for server box that would run eight V100 32 GB. Like I understand the difference in the technology and I suppose it just depends on your ultimate goal, but you could run twice as large of an AI and your inference speed would still be lightning fast. But again everybody has their own motivations. For me I'm kind of like looking at where can I get the largest amount of vram for the minimal amount of money which I'm sure most people also think but at the end of the day I also will trade 2000 tokens a second on GPT OSS 120b for 500 tokens a second on Minimax 2.1",
          "score": 1,
          "created_utc": "2026-01-24 03:02:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkudvz",
      "title": "I gave my local LLM pipeline a brain - now it thinks before it speaks",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 15:50:14",
      "score": 59,
      "num_comments": 16,
      "upvote_ratio": 0.91,
      "text": "[Video from sequential retrieval](https://reddit.com/link/1qkudvz/video/9mel0vq9d4fg1/player)\n\nIn the video you can see how and that it works.\n\nJarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience.Â [u/frank\\_brsrk](/user/frank_brsrk/)Â Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nðŸ§  Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions â†’ AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions â†’ instant answer.\n\nComplex questions â†’ step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout toÂ [u/frank\\_brsrk](/user/frank_brsrk/)Â for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build ðŸ˜…\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\nsimple graphic:\n\n[Simple visualization of MCP retrieval](https://preview.redd.it/f9tm59rkd4fg1.png?width=863&format=png&auto=webp&s=6a65eda552b3b846863f75c59ee04018eb6d6c41)\n\n[Simple visualization pipeline](https://preview.redd.it/6h40kd1sd4fg1.png?width=1147&format=png&auto=webp&s=770dd510d8ebf0fe8b68f6a81bb7ab40d34fa862)\n\n@/[frank\\_brsrk](/user/frank_brsrk/) architecture of the causal intelligence module\n\n[architecture of the causal intelligence module](https://preview.redd.it/6x03fioje4fg1.jpg?width=2800&format=pjpg&auto=webp&s=7df7325899fd7dd3f8ca8743ebca4d04845868b5)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1amvip",
          "author": "GCoderDCoder",
          "text": "This is great! I love that a whole generation of technologists have spent our lives trying to be Iron Man and the few making the most progress by working together on it are trying to ruin it for the rest of us now that we can finally see the light at the other end of the tunnel. Thanks for helping the rest of us in the struggle to keep up!\n\nI'll be more focused one these types of projects when they eventually fire me because they don't realize how much we have to correct the AI still but I wish I had time to be consistent working on some joint projects like this. I'm trying to figure out how to piece together things like roo code with something like vibe kanban and MCPs in an opinionated way to reduce the manual burden while allowing more than coding using local LLMs and then here goes Anthropic with their cowork thing lol\n\nKeep up the hard work! When the fairytale of benevolent AI providers crumbles people will be looking for local LLMs.",
          "score": 10,
          "created_utc": "2026-01-23 19:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1aoe33",
              "author": "danny_094",
              "text": "That's exactly why I'm building this. Local-first, privacy-first, yours forever.",
              "score": 7,
              "created_utc": "2026-01-23 19:45:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a5160",
          "author": "No-Leopard7644",
          "text": "Congratulations, sounds interesting, thank you for sharing this information. Is it open source?",
          "score": 4,
          "created_utc": "2026-01-23 18:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a6tb4",
              "author": "danny_094",
              "text": "Yes you can Download on GitHub. It will always be free for private users. Everything for local users :)",
              "score": 6,
              "created_utc": "2026-01-23 18:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1c4d05",
          "author": "Endflux",
          "text": "If I can find the time I'll give it a try this weekend and let you know how it goes! Thanks for sharing :)",
          "score": 3,
          "created_utc": "2026-01-24 00:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c4op2",
              "author": "danny_094",
              "text": "I'm asking for it. I really need more user feedback :D",
              "score": 2,
              "created_utc": "2026-01-24 00:01:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aq2xn",
          "author": "burn-n-die",
          "text": "What's the system configuration you are using?",
          "score": 2,
          "created_utc": "2026-01-23 19:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1avqci",
              "author": "danny_094",
              "text": "Everything runs on an RTX 2060 Super.CPU and RAM aren't really being used. You can find my Ollama container file in the wiki.",
              "score": 3,
              "created_utc": "2026-01-23 20:19:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1b1v5b",
                  "author": "burn-n-die",
                  "text": "Thanks. I am not a software engineer and I don't code. Will you guide be straight forward?",
                  "score": 2,
                  "created_utc": "2026-01-23 20:48:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dvx2l",
          "author": "Hot_Rip_4912",
          "text": "Wow ,man that feels good",
          "score": 2,
          "created_utc": "2026-01-24 06:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f58ev",
          "author": "JinkerGaming",
          "text": "Amazing! Thank you good sir. I will certainly be forking this. :)",
          "score": 2,
          "created_utc": "2026-01-24 13:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ixovi",
          "author": "sweetbacon",
          "text": "This looks **very** interesting, thanks for sharing.",
          "score": 2,
          "created_utc": "2026-01-25 00:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j5isr",
          "author": "yeahlloow",
          "text": "Can someone please explain what is the difference between this and the thinking mode of normal LLMs?",
          "score": 2,
          "created_utc": "2026-01-25 00:51:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dk5rd",
          "author": "leonbollerup",
          "text": "Hey, if you have 15 min to spare â€¦ listen this pod cast, or scroll forward to the part about augmented LLM - maybe thatâ€™s something that improve your solution even more\n\n[https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1](https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1)\n\nIâ€™m far from done with ArcAI.. but I could share you the concepts.\n\nhttps://preview.redd.it/sgo3krcbd8fg1.jpeg?width=812&format=pjpg&auto=webp&s=57fbd6c153fac073071e46c4985b71fadb7fcaeb\n\nThese are the result of controlled continued tests",
          "score": 1,
          "created_utc": "2026-01-24 05:10:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qitnbv",
      "title": "The Case for a $600 Local LLM Machine",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "author": "tony10000",
      "created_utc": "2026-01-21 09:42:30",
      "score": 48,
      "num_comments": 55,
      "upvote_ratio": 0.94,
      "text": "**The Case for a $600 Local LLM Machine**\n\nUsing the Base Model Mac mini M4\n\nhttps://preview.redd.it/5c916gwucoeg1.png?width=1182&format=png&auto=webp&s=68d91da71f6244d752e15922e47dfbf9d792beb1\n\nby Tony Thomas\n\nIt started as a simple experiment. How much real work could I do on a small, inexpensive machine running language models locally?\n\nWith GPU prices still elevated, memory costs climbing, SSD prices rising instead of falling, power costs steadily increasing, and cloud subscriptions adding up, it felt like a question worth answering. After a lot of thought and testing, the system I landed on was a base model Mac mini M4 with 16 GB of unified memory, a 256 GB internal SSD, a USB-C dock, and a 1 TB external NVMe drive for model storage. Thanks to recent sales, the all-in cost came in right around $600.\n\nOn paper, that does not sound like much. In practice, it turned out to be far more capable than I expected.\n\nLocal LLM work has shifted over the last couple of years. Models are more efficient due to better training and optimization. Quantization is better understood. Inference engines are faster and more stable. At the same time, the hardware market has moved in the opposite direction. GPUs with meaningful amounts of VRAM are expensive, and large VRAM models are quietly disappearing. DRAM is no longer cheap. SSD and NVMe prices have climbed sharply.\n\nAgainst that backdrop, a compact system with tightly integrated silicon starts to look less like a compromise and more like a sensible baseline.\n\n**Why the Mac mini M4 Works**\n\nThe M4 Mac mini stands out because Appleâ€™s unified memory architecture fundamentally changes how a small system behaves under inference workloads. CPU and GPU draw from the same high-bandwidth memory pool, avoiding the awkward juggling act that defines entry-level discrete GPU setups. I am not interested in cramming models into a narrow VRAM window while system memory sits idle. The M4 simply uses what it has efficiently.\n\nSixteen gigabytes is not generous, but it is workable when that memory is fast and shared. For the kinds of tasks I care about, brainstorming, writing, editing, summarization, research, and outlining, it holds up well. I spend my time working, not managing resources.\n\nThe 256 GB internal SSD is limited, but not a dealbreaker. Models and data live on the external NVMe drive, which is fast enough that it does not slow my workflow. The internal disk handles macOS and applications, and that is all it needs to do. Avoiding Appleâ€™s storage upgrade pricing was an easy decision.\n\nThe setup itself is straightforward. No unsupported hardware. No hacks. No fragile dependencies. It is dependable, UNIX-based, and boring in the best way. That matters if you intend to use the machine every day rather than treat it as a side project.\n\n**What Daily Use Looks Like**\n\nThe real test was whether the machine stayed out of my way.\n\nQuantized 7B and 8B models run smoothly using Ollama and LM Studio. AnythingLLM works well too and adds vector databases and seamless access to cloud models when needed. Response times are short enough that interaction feels conversational rather than mechanical. I can draft, revise, and iterate without waiting on the system, which makes local use genuinely viable.\n\nLarger 13B to 14B models are more usable than I expected when configured sensibly. Context size needs to be managed, but that is true even on far more expensive systems. For single-user workflows, the experience is consistent and predictable.\n\nWhat stood out most was how quickly the hardware stopped being the limiting factor. Once the models were loaded and tools configured, I forgot I was using a constrained system. That is the point where performance stops being theoretical and starts being practical.\n\nIn daily use, I rotate through a familiar mix of models. Qwen variants from 1.7B up through 14B do most of the work, alongside Mistral instruct models, DeepSeek 8B, Phi-4, and Gemma. On this machine, smaller Qwen models routinely exceed 30 tokens per second and often land closer to 40 TPS depending on quantization and context. These smaller models can usually take advantage of the full available context without issue.\n\nThe 7B to 8B class typically runs in the low to mid 20s at context sizes between 4K and 16K. Larger 13B to 14B models settle into the low teens at a conservative 4K context and operate near the upper end of acceptable memory pressure. Those numbers are not headline-grabbing, but they are fast enough that writing, editing, and iteration feel fluid rather than constrained. I am rarely waiting on the model, which is the only metric that actually matters for my workflow.\n\n**Cost, Power, and Practicality**\n\nAt roughly $600, this system occupies an important middle ground. It costs less than a capable GPU-based desktop while delivering enough performance to replace a meaningful amount of cloud usage. Over time, that matters more than peak benchmarks.\n\nThe Mac mini M4 is also extremely efficient. It draws very little power under sustained inference loads, runs silently, and requires no special cooling or placement. I routinely leave models running all day without thinking about the electric bill.\n\nThat stands in sharp contrast to my Ryzen 5700G desktop paired with an Intel B50 GPU. That system pulls hundreds of watts under load, with the B50 alone consuming around 50 watts during LLM inference. Over time, that difference is not theoretical. It shows up directly in operating costs.\n\nThe M4 sits on top of my tower system and behaves more like an appliance. Thanks to my use of a KVM, I can turn off the desktop entirely and keep working. I do not think about heat, noise, or power consumption. That simplicity lowers friction and makes local models something I reach for by default, not as an occasional experiment.\n\n**Where the Limits Are**\n\nThe constraints are real but manageable. Memory is finite, and there is no upgrade path. Model selection and context size require discipline. This is an inference-first system, not a training platform.\n\nApple Silicon also brings ecosystem boundaries. If your work depends on CUDA-specific tooling or experimental research code, this is not the right machine. It relies on Appleâ€™s Metal backend rather than NVIDIAâ€™s stack. My focus is writing and knowledge work, and for that, the platform fits extremely well.\n\n**Why This Feels Like a Turning Point**\n\nWhat surprised me was not that the Mac mini M4 could run local LLMs. It was how well it could run them given the constraints.\n\nFor years, local AI was framed as something that required large amounts of RAM, a powerful CPU, and an expensive GPU. These systems were loud, hot, and power hungry, built primarily for enthusiasts. This setup points in a different direction. With efficient models and tightly integrated hardware, a small, affordable system can do real work.\n\nFor writers, researchers, and independent developers who care about control, privacy, and predictable costs, a budget local LLM machine built around the Mac mini M4 no longer feels experimental. It is something I turn on in the morning, leave running all day, and rely on without thinking about the hardware.\n\nMore than any benchmark, that is what matters.\n\nFrom: tonythomas-dot-net",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0uugib",
          "author": "TimLikesAI",
          "text": "I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 and use it similarly. The extra headroom allows for running some pretty powerful models that are in the 14-20GB range.",
          "score": 11,
          "created_utc": "2026-01-21 14:06:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x9s9c",
              "author": "fallingdowndizzyvr",
              "text": "> I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 \n\nYou could have gotten it new for cheaper on Ebay from a liquidator.",
              "score": 0,
              "created_utc": "2026-01-21 20:45:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0y4lrx",
                  "author": "jcktej",
                  "text": "Asking for a friend. How do I find such vendors?",
                  "score": 1,
                  "created_utc": "2026-01-21 23:12:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tyaic",
          "author": "PraxisOG",
          "text": "I sincerely hope this is the future. An easy to use, low upfront and ongoing cost box that privately serves LLMs and maybe more. The software, while impressive, leaves much to be desired in terms of usability. This is from the perspective of having recently thrown together the exact kind of loud and expensive box you mentioned, that took days to get usable output from.Â ",
          "score": 6,
          "created_utc": "2026-01-21 10:22:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0z12z0",
              "author": "kermitt81",
              "text": "The M5 Mac Mini is expected sometime in the middle of this year, and offers significant improvements for LLM usage over the M4 Mac Mini. Pricing will likely be around the same, so it may be well worth the wait. \n\n(Each of the M5â€™s 10 GPU cores now includes a dedicated Neural Accelerator, and - according to benchmarks - the M5 delivers 3.6x faster time to first token compared to the M4.)",
              "score": 3,
              "created_utc": "2026-01-22 02:11:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12v5g7",
                  "author": "tony10000",
                  "text": "It will be interesting to see how the recent memory and storage price increases will impact pricing.  I don't think we will see a sub-$500 deal on the M5 Mini anytime soon.",
                  "score": 2,
                  "created_utc": "2026-01-22 17:15:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uc6wg",
          "author": "locai_al-ibadi",
          "text": "It is great seeing the capabilities of localised AI recently, compared to what we were capable of running a year ago (arguably even a months ago).",
          "score": 3,
          "created_utc": "2026-01-21 12:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ud5rp",
          "author": "alias454",
          "text": "If you look around you can get it for cheaper than that(micro center and best buy open box deals). I picked up an open box one and have actually been impressed. The onboard storage is probably the biggest complaint but it will do for now. My main laptop is an older Lenovo Legion 5 with 32GBs of ddr4 and an rtx2060. It was purchased around 2020 so it is starting to show it's age.",
          "score": 3,
          "created_utc": "2026-01-21 12:23:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v5cvv",
              "author": "tony10000",
              "text": "It is easy to add an external drive or a dock with a NVME slot for additional storage.  I keep all of the models, data, and caches on that.",
              "score": 6,
              "created_utc": "2026-01-21 15:01:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v0i9e",
          "author": "Rabo_McDongleberry",
          "text": "For me. Speed isn't that big of an issue. And most of the things I do are just basic text generation and editing. Maybe answer a few questions that I cross references with legit sources. The Mac Mini works great for that.Â ",
          "score": 3,
          "created_utc": "2026-01-21 14:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wixf8",
          "author": "dual-moon",
          "text": "how much have you tried small models? many of them are extremely good; lots of tiny models get used as subagents in swarm setups. LiquidAI actually JUST released a 1.2B LFM2.5 Thinking model that would probably FLY on your machine :)",
          "score": 2,
          "created_utc": "2026-01-21 18:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103wgb",
              "author": "tony10000",
              "text": "Yes.  I use the small LiquidAI models, and Qwen 1.7B is a tiny-mighty LLM for drafting.",
              "score": 3,
              "created_utc": "2026-01-22 06:22:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1at66e",
                  "author": "GeroldM972",
                  "text": "Can concur about the LFM2 and LFM 2.5 models, because these are part of the set 'LM Studio - editor's pick' and for a very good reason.  These work really well with the LM Studio software.\n\nMy system is Ryzen 5 2400 with W11. 32 GB DDR4 RAM, a Crucial 2,5\" SSD (240 GB) and a AMD R580 GPU...with 16 GB of VRAM on it. So, 'simple' and 'weak-sauce' would be apt descriptions of this system. And yet, those LFM models work very well in here. Even if it is via Vulkan.\n\nedit:  \nIf you use LM Studio, it comes with MCP support, so these small models can now also look on the internet, can keep track of time and a few other things I find handy. Is very easy to set up and your local LLM becomes much more useful (if you trust information on the internet of course).",
                  "score": 1,
                  "created_utc": "2026-01-23 20:07:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0wub17",
              "author": "cuberhino",
              "text": "Iâ€™m currently considering a threadripper + 3090 build at around $2000 total cost to function as a local private ChatGPT replacement. \n\nDo you think this is overkill and I should go with one of these cheaper Mac systems?",
              "score": 2,
              "created_utc": "2026-01-21 19:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0x4ao8",
                  "author": "dual-moon",
                  "text": "try out some smaller models first, see how they work for you! if you find small models do the job, then scaling based on an agentic swarm rather than a single model may be best! but it really depends on what you want to use it for. if it's just chatting, deepseek can do most of what the big guys can!\n\nbut don't think a threadripper and a 3090 is a bad idea or anything :p",
                  "score": 2,
                  "created_utc": "2026-01-21 20:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y9qn3",
          "author": "yeeah_suree",
          "text": "Nice write up! Can you share a little more information on what you use the model for? What constitutes everyday use?",
          "score": 2,
          "created_utc": "2026-01-21 23:39:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103q6c",
              "author": "tony10000",
              "text": "I am a writer.  I use AI for brainstorming, outlining, summarizing, drafting, and sometimes editing and polishing.",
              "score": 1,
              "created_utc": "2026-01-22 06:21:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o13x82a",
                  "author": "tony10000",
                  "text": "I just posted another article today on this forum that details my workflow.",
                  "score": 0,
                  "created_utc": "2026-01-22 20:06:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0zg50d",
          "author": "Icy-Pay7479",
          "text": "Was this written by an 8b model?",
          "score": 2,
          "created_utc": "2026-01-22 03:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103f9p",
              "author": "tony10000",
              "text": "I typically use Qwen 14B for outlining, and my primary drafting models are Qwen 3B and 4B.  Sometimes even 1.7.  I use ChatGPT to polish and then heavily edit the result.",
              "score": 1,
              "created_utc": "2026-01-22 06:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zpx40",
          "author": "crossfitdood",
          "text": "dude you missed the black friday deals. I bought mine from costco for $479. But I was really upset when Microcenter had them on sale for $399",
          "score": 2,
          "created_utc": "2026-01-22 04:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1041ul",
              "author": "tony10000",
              "text": "Bought it from Amazon for $479.  It was around $600 all in for the M4, dock, and 1TB NVME.",
              "score": 1,
              "created_utc": "2026-01-22 06:24:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zretd",
          "author": "vmjersey",
          "text": "But can you play GTA V on it?",
          "score": 2,
          "created_utc": "2026-01-22 04:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1045ag",
              "author": "tony10000",
              "text": "No idea what that is.",
              "score": 2,
              "created_utc": "2026-01-22 06:24:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10f5cl",
          "author": "lucasbennett_1",
          "text": "Quantized 7B-14B like qwen3 or deepseek 8B run fluidly for writing/research without the power/heat mess of discrete GPUs. THe external nvme for models is a smart hack to dodge apples storage premiums too",
          "score": 2,
          "created_utc": "2026-01-22 08:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13xfmc",
              "author": "tony10000",
              "text": "I agree!",
              "score": 1,
              "created_utc": "2026-01-22 20:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11zlww",
          "author": "Known_Geologist1085",
          "text": "I know this convo is about doing it on the cheap, but I'd like to note that I have been running an m3 max macbook pro with 128GB ram for a couple years now and the unified memory is a god send.  There are some support issues with Metal/MPS for certain things related to certain quantization and sparse attention, but overall these machines are beasts.   I can get 40-50 tps on llama 70b.  The good news now is that the market is flooded with these m chip macbook airs and pros, and a lot of them are cheap if you buy used.  I wouldn't be surprised if someone makes, or has already made, software to cluster macs for inference stacks in order to find a use for these.",
          "score": 2,
          "created_utc": "2026-01-22 14:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12ws2s",
              "author": "tony10000",
              "text": "There is a solution for clustering Macs called Exo, but you need Thunderbolt 5 for top performance using RDMA.  Several YouTube videos demonstrate how well the clusters work for inference.",
              "score": 1,
              "created_utc": "2026-01-22 17:22:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x9ogi",
          "author": "fallingdowndizzyvr",
          "text": "$600! Dude overpaid. It's $400 at MC.",
          "score": 2,
          "created_utc": "2026-01-21 20:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zh61a",
              "author": "DerFreudster",
              "text": "I think that included the dock and the nvme.",
              "score": 2,
              "created_utc": "2026-01-22 03:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o102sjy",
                  "author": "tony10000",
                  "text": "Correct.  That was for everything.  I got the M4 for $479.",
                  "score": 3,
                  "created_utc": "2026-01-22 06:13:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0vutf5",
          "author": "track0x2",
          "text": "I heard the primary limitation for LLMs on Macâ€™s is around text generation and if you want to do anything other than that (image gen, TTS, agentic work) they will struggle.",
          "score": 1,
          "created_utc": "2026-01-21 16:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vw0we",
              "author": "tony10000",
              "text": "It really depends on your use case and expectations.  Some very capable models are 14B and under.  If I need more capabilities, I can run some 30B models on my 5700G.  For more than that, there is ChatGPT and Open Router.",
              "score": 2,
              "created_utc": "2026-01-21 17:03:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wlbfx",
          "author": "Parking_Bug3284",
          "text": "This is really cool. I'm still sharing my gpu on my main but I'm building a similar thing on the software side. It sets up a base control system for local systems running on your machine. So if you have ollama and opencode it can build out what you need to gain access to unlimited memory management and access to programs that have a server running like image gen and what not. Does your system have an APi or mcp server to talk to it",
          "score": 1,
          "created_utc": "2026-01-21 18:54:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104kqn",
              "author": "tony10000",
              "text": "I use Anything LLM for API access, MCP, and RAG vector databases.",
              "score": 2,
              "created_utc": "2026-01-22 06:28:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10gg8v",
                  "author": "SelectArrival7508",
                  "text": "which is really nice as you can switch between you local llm and cloud-based llms",
                  "score": 2,
                  "created_utc": "2026-01-22 08:12:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wyis3",
          "author": "jarec707",
          "text": "Agreed, depending on use. Thanks for sharing the models you use; seem like good choices.",
          "score": 1,
          "created_utc": "2026-01-21 19:54:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104wim",
              "author": "tony10000",
              "text": "Yeah, I have a nice assortment of models.  Probably 200GB worth at present.",
              "score": 1,
              "created_utc": "2026-01-22 06:31:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x1zxe",
          "author": "Zyj",
          "text": "You could try to get a used PC with a RTX 5060 Ti 16GB for almost the same amount of money, like this one [https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369](https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369)",
          "score": 1,
          "created_utc": "2026-01-21 20:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104eke",
              "author": "tony10000",
              "text": "Not compact, portable, energy efficient, quiet, with low thermals.",
              "score": 1,
              "created_utc": "2026-01-22 06:27:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15ou5c",
          "author": "nitinmms1",
          "text": "My Mac Mini M4 24GB can run Qwen 14b quantized at decent speed.\nImage generation models with comfyui, though feel slow.\nBut I still feel 64GB M4 will easily do as a good base local AI machine.",
          "score": 1,
          "created_utc": "2026-01-23 01:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bd8k6",
          "author": "parboman",
          "text": "Tried doing anything with mlx instead of ollama?",
          "score": 1,
          "created_utc": "2026-01-23 21:42:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bf21h",
              "author": "tony10000",
              "text": "I have a MLX version of llama.cpp on the system, and LM Studio can also run:\n\nMetal Llama.cpp\n\nLM Studio MLX\n\nI can run both native MLX builds and GGUFs using Metal.\n\nAlso: Ollama on macOS automatically utilizes Apple'sÂ Metal APIÂ for GPU acceleration on Apple Silicon (M1/M2/M3/M4 chips), requiring no additional configuration.",
              "score": 1,
              "created_utc": "2026-01-23 21:50:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjqhja",
      "title": "This Week's Hottest Hugging Face Releases: Top Picks by Category!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "author": "techlatest_net",
      "created_utc": "2026-01-22 09:52:51",
      "score": 48,
      "num_comments": 2,
      "upvote_ratio": 0.99,
      "text": "Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.\n\nCheck 'em out and drop your thoughtsâ€”which one's getting deployed first?\n\n# Text Generation\n\n* [**zai-org/GLM-4.7-Flash**](https://huggingface.co/zai-org/GLM-4.7-Flash): 31B param model for fast, efficient text genâ€”updated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.\n* [**unsloth/GLM-4.7-Flash-GGUF**](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF): Quantized 30B version for easy local inferenceâ€”hot with 112k downloads in hours. Great for low-resource setups.\n\n# Image / Multimodal\n\n* [**zai-org/GLM-Image**](https://huggingface.co/zai-org/GLM-Image): Image-text-to-image powerhouseâ€”10.8k downloads, 938 likes. Excels in creative edits and generation.\n* [**google/translategemma-4b-it**](https://huggingface.co/google/translategemma-4b-it): 5B vision-language model for multilingual image-text tasksâ€”45.4k downloads, supports translation + vision.\n\n# Audio / Speech\n\n* [**kyutai/pocket-tts**](https://huggingface.co/kyutai/pocket-tts): Compact TTS for natural voicesâ€”38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.\n* [**microsoft/VibeVoice-ASR**](https://huggingface.co/microsoft/VibeVoice-ASR): 9B ASR for multilingual speech recognitionâ€”ultra-low latency, 816 downloads already spiking.\n\n# Other Hot Categories (Video/Agentic)\n\n* [**Lightricks/LTX-2**](https://huggingface.co/Lightricks/LTX-2) (Image-to-Video): 1.96M downloads, 1.25k likesâ€”pro-level video from images.\n* [**stepfun-ai/Step3-VL-10B**](https://huggingface.co/stepfun-ai/Step3-VL-10B) (Image-Text-to-Text): 10B VL model for advanced reasoningâ€”28.6k downloads in hours.\n\nThese are dominating trends with massive community traction.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o14xz85",
          "author": "Count_Rugens_Finger",
          "text": "I have found that GLM-4.7 30B-A3B model is actually inferior to Qwen3-Coder 30B-A3B for programming.  Anyone else?",
          "score": 1,
          "created_utc": "2026-01-22 23:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dhymn",
          "author": "Blizado",
          "text": "Why is Qwen3-TTS missing? For me clearly the most exciting new TTS models, especially since they have multi-language support and not only english as still too many new TTS models have.",
          "score": 1,
          "created_utc": "2026-01-24 04:55:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3cgo",
      "title": "the state of local agentic \"action\" is still kind of a mess",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "author": "Ilove_Cakez",
      "created_utc": "2026-01-21 16:56:55",
      "score": 47,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "spent the last few nights trying to get a decent mcp setup running for my local stack and itâ€™s honestly depressing how much friction there still is. weâ€™ve got these massive models running on consumer hardware, but as soon as you want them to actually do anything.. like pull from a local db or interact with an api so youâ€™re basically back to writing custom boilerplate for every single tool.\n\nthe security trade-offs are the worst part. itâ€™s either total isolation (useless) or giving the model way too much permission because managing granular mcp servers manually is a full-time job. iâ€™ve been trying to find a middle ground where i donâ€™t have to hand-roll the auth and logging for every connector.\n\nfound a tool thatâ€™s been helping with the infra side of it. it basically just handles the mcp server generation and the governance/permissions layer so i don't have to think too much (ogment ai, i'm sure most of you know about it). itâ€™s fine for skipping the boring stuff, but iâ€™m still annoyed that this isn't just native or more standardized yet.\n\nhow are you guys actually deploying agents that can touch your data? are you just building your own mcp wrappers from scratch or is there a better way to handle the permissioning? curious",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0z1cn1",
          "author": "Fortyseven",
          "text": "I've had some modest success using 30b qwen3-coder and the Qwen Code TUI. It won't replace my cloud-based stuff, but I always give it a go now and then and find myself surprised at how capable it can be.",
          "score": 2,
          "created_utc": "2026-01-22 02:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xvvdv",
          "author": "ForsookComparison",
          "text": "Running agentically is simply too hard for most small or mid-sized open weight models. Once you add a few tools and a multi-step feedback loop, the *bare minimum* becomes either Qwen3-Next-80B or Gpt-oss-120b, both of which still will fall flat on their face along the way. GLM 4.6V is probably the actual starting line for reliable use and even that will be spotty as you add more and more instructions and tools.\n\nI don't have a good answer for you besides *\"I've experienced this and think that agents are just hard\"*.",
          "score": 1,
          "created_utc": "2026-01-21 22:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o101cuk",
          "author": "mister2d",
          "text": "I'm mildly curious about this topic. How much could a framework like a self-hosted n8n instance fill the gaps?",
          "score": 1,
          "created_utc": "2026-01-22 06:02:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10t0ef",
          "author": "techlatest_net",
          "text": "Yeah man, local agents are still duct tape and prayersâ€”tool perms either nuke security or turn models into toddlers with car keys. Ogment's solid for skipping the mcp boilerplate grind tho.\n\nCrewAI + containerized tools (dockerized APIs) saved my sanityâ€”granular perms via bind mounts, no root hell. You running ollama under that? What's your stack look like?",
          "score": 1,
          "created_utc": "2026-01-22 10:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wmw3y",
          "author": "atlasnomos",
          "text": "Youâ€™re not wrong â€” the pain point youâ€™re describing is real and structural, not a tooling gap.\n\nWhat keeps biting people is that â€œagent actionâ€ today is basically glued together from three things that donâ€™t want to coexist:\n\n1. prompt-time intent\n2. tool adapters (MCP / wrappers)\n3. runtime execution with side effects\n\nMost stacks solve (1) and (2), but almost nobody treats (3) as a **first-class runtime concern**. So permissions end up living either:\n\n* inside prompts (brittle),\n* inside per-tool wrappers (boilerplate explosion),\n* or at the MCP server boundary (coarse, hard to reason about).\n\nThe isolation vs over-permission tradeoff you mention is exactly what happens when thereâ€™s no **deny-by-default execution layer** sitting *between* the model and the tools.\n\nWhatâ€™s worked best for us locally is:\n\n* keep MCP servers dumb (pure capability exposure)\n* move auth, cost limits, logging, and allow/deny decisions into a single runtime gate\n* treat every tool call as an auditable event, not â€œjust another function callâ€\n\nThat way youâ€™re not re-implementing auth + logging per connector, and youâ€™re not trusting the model to behave just because the prompt says so.\n\nI agree it *should* be more native / standardized. Until thereâ€™s a real spec for agent execution (not just tool schemas), everyoneâ€™s either rolling wrappers or accepting scary permissions.\n\nCurious what level youâ€™re aiming for locally:\n\n* human-in-the-loop approvals?\n* strict allowlists?\n* cost / rate enforcement?\n* or just â€œdonâ€™t nuke my dataâ€?\n\nThat choice seems to drive everything else.",
          "score": -3,
          "created_utc": "2026-01-21 19:01:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0yrrf6",
              "author": "Big-Masterpiece-9581",
              "text": "I wish this kind of sanity would reign. But working in Big Corp is a bit like the Trump Administration in this job market. Pure politics, cloak and dagger, backstabbing, constant reorgs. Nobody is sharing anything, and each is reinventing every wheel especially around AI / MCP.",
              "score": 1,
              "created_utc": "2026-01-22 01:17:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11lp1w",
                  "author": "atlasnomos",
                  "text": "Thatâ€™s a fair take â€” and honestly, it matches what we keep hearing from people inside large orgs.\n\nMost of the time itâ€™s not that the problems arenâ€™t understood; itâ€™s that ownership is fragmented and incentives donâ€™t line up. Governance ends up living in the cracks between teams, so everyone quietly rebuilds their own glue and hopes it never becomes *their* incident.\n\nWeâ€™re under no illusion that clean architectures win on their own. In practice, they only surface once failure, liability, or regulatory pressure forces shared responsibility. Until then, itâ€™s politics and reorgs all the way down.\n\nAppreciate you saying this out loud â€” itâ€™s useful context, not pushback.",
                  "score": 1,
                  "created_utc": "2026-01-22 13:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qiswy6",
      "title": "Olmo 3.1 32B Think â€” second place on hard reasoning, beating proprietary flagships",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-01-21 08:56:26",
      "score": 46,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "Running peer evaluations of frontier models (The Multivac). Today's constraint satisfaction puzzle had interesting results for local LLM folks.\n\n**Top 3:**\n\n1. Gemini 3 Pro Preview: 9.13\n2. **Olmo 3.1 32B Think: 5.75** â† Open source\n3. GPT-OSS-120B: 4.79 â† Open source\n\n**Models Olmo beat:**\n\n* Claude Opus 4.5 (2.97)\n* Claude Sonnet 4.5 (3.46)\n* Grok 3 (2.25)\n* DeepSeek V3.2 (2.99)\n\n**The task:** Schedule 5 people for meetings across Mon-Fri with 9 interlocking logical constraints. Requires recognizing structural impossibilities and systematic constraint propagation.\n\n**Notes on Olmo:**\n\n* High variance (Â±4.12) â€” inconsistent but strong ceiling\n* Extended thinking appears to help on this problem class\n* 32B is runnable on consumer hardware (with quantization)\n* Apache 2.0 license\n\n**Questions for the community:**\n\n* What quantizations are people running Olmo 3.1 at?\n* Performance on other reasoning tasks?\n* Any comparisons vs DeepSeek for local deployment?\n\nFull results at [themultivac.com](http://themultivac.com)\n\nLink: [https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm\\_campaign=post&utm\\_medium=web&showWelcomeOnShare=true](https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\nhttps://preview.redd.it/jko9h4ox2oeg1.png?width=1208&format=png&auto=webp&s=07f7967899cc6f7d6252eed866ef5f4003f3288b\n\n**Daily runs and Evals. Cheers!**",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0tqymb",
          "author": "Mabuse046",
          "text": "I like Allen AI's reasoning dataset and I tend to take chunks of it to train my own models on. It's nice to see them scoring highly.",
          "score": 8,
          "created_utc": "2026-01-21 09:13:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzro0",
              "author": "randygeneric",
              "text": "you mean benchmaxing?",
              "score": -5,
              "created_utc": "2026-01-21 10:36:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ua463",
                  "author": "silenceimpaired",
                  "text": "Why on earth are you asking this question? No where does the person youâ€™re commenting on mention test sets. They mention reasoning traces. Now if you were to say Allen AI has polluted datasets with test data present that would make sense.",
                  "score": 10,
                  "created_utc": "2026-01-21 12:01:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0w2m31",
                  "author": "Vegetable-Second3998",
                  "text": "This is such a dumb term. If a model learns the concepts of a benchmark, it has learned it. Create better benchmarks then. Thatâ€™s just doing what humans also do - memorize shit for efficiency.",
                  "score": 3,
                  "created_utc": "2026-01-21 17:33:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tzul1",
                  "author": "Mabuse046",
                  "text": "What do you mean?",
                  "score": 7,
                  "created_utc": "2026-01-21 10:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xps63",
          "author": "segmond",
          "text": "I hope you're right.  If so, this goes to show that folks should stop looking for the ONE magic model.  We saw with qwen2.5-32b-coder that a model can be very good in a specific domain.",
          "score": 2,
          "created_utc": "2026-01-21 21:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z5lcr",
          "author": "TheOdbball",
          "text": "Luckily for everyone Iâ€™m building a headless Linux based file system that would make changing out your LLM without losing years of productivity on the fly ðŸ˜Ž\n\nWill definitely try olmo out thanks",
          "score": 1,
          "created_utc": "2026-01-22 02:36:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o152b5x",
          "author": "Direct_Turn_1484",
          "text": "I tried running this one in Ollama and it crashed. Yes I updated Ollama. I tried downloading it again. Still crashed. I really wanted to play with Omni-3.1. I guess Iâ€™ll just have to try with vllm.",
          "score": 1,
          "created_utc": "2026-01-22 23:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o159t1u",
              "author": "Chalutation",
              "text": "vllm is way more efficient than ollama, but you don't have the pull feature like ollama does.",
              "score": 1,
              "created_utc": "2026-01-23 00:09:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o15u91m",
              "author": "jadecamaro",
              "text": "Crashed for me in LM Studio too even lowering context down",
              "score": 1,
              "created_utc": "2026-01-23 02:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhg3pm",
      "title": "GLM-4.7-Flash-NVFP4 (20.5GB) is on huggingface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qhg3pm/glm47flashnvfp4_205gb_is_on_huggingface/",
      "author": "DataGOGO",
      "created_utc": "2026-01-19 20:42:39",
      "score": 34,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "I published a mixed precision NVFP4 quantized version of the new GLM-4.7-FLASH model on huggingface. \n\n  \nCan any of you test it out and let me know how it works for you?\n\n  \n[GadflyII/GLM-4.7-Flash-NVFP4 Â· Hugging Face](https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qhg3pm/glm47flashnvfp4_205gb_is_on_huggingface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qk9ked",
      "title": "Good local LLM for coding?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "author": "Expensive-Time-7209",
      "created_utc": "2026-01-22 22:56:33",
      "score": 31,
      "num_comments": 24,
      "upvote_ratio": 0.94,
      "text": "I'm looking for a a good local LLM for coding that can run on my rx 6750 xt which is old but I believe the 12gb will allow it to run 30b param models but I'm not 100% sure. I think GLM 4.7 flash is currently the best but posts like this [https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular\\_opinion\\_glm\\_47\\_flash\\_is\\_just\\_a/](https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular_opinion_glm_47_flash_is_just_a/) made me hesitant\n\nBefore you say just download and try, my lovely ISP gives me a strict monthly quota so I can't be downloading random LLMS just to try them out",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o163q5w",
          "author": "Javanese1999",
          "text": "[https://huggingface.co/TIGER-Lab/VisCoder2-7B](https://huggingface.co/TIGER-Lab/VisCoder2-7B) = Better version of Qwen2.5-Coder-7B-Instruct\n\n[https://huggingface.co/openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) =Very fast under 20b, even if your model size exceeds the VRAM capacity and goes into ram.\n\n[https://huggingface.co/NousResearch/NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B) = Max picks IQ4\\_XS. This is just an alternative\n\nBut of all of them, my rational choice fell on gpt-oss-20b. It's heavily censored in refusal prompts, but it's quite reliable for light coding.",
          "score": 11,
          "created_utc": "2026-01-23 02:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16qo3q",
          "author": "RnRau",
          "text": "Pick a coding MoE model and then use llama.cpp inference engine to offload some of the model to your system ram.",
          "score": 3,
          "created_utc": "2026-01-23 05:14:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o190rhs",
              "author": "BrewHog",
              "text": "Does llama.cpp have the ability to use both CPU and GPU? Or are you suggesting running one process in CPU and another in GPU?",
              "score": 1,
              "created_utc": "2026-01-23 15:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1bxmro",
                  "author": "RnRau",
                  "text": "It can use both in the same process. Do a google on 'moe offloading'.",
                  "score": 3,
                  "created_utc": "2026-01-23 23:23:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17b5hk",
              "author": "mintybadgerme",
              "text": "Or LMstudio.",
              "score": 0,
              "created_utc": "2026-01-23 07:59:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18zcxh",
          "author": "DarkXanthos",
          "text": "I run QWEN3 coder 30B on my M1 Max 64GB and it works pretty well. I think I wouldn't go larger though.",
          "score": 3,
          "created_utc": "2026-01-23 15:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o191fwg",
              "author": "BrewHog",
              "text": "How much RAM does it use? Is that quantized?",
              "score": 1,
              "created_utc": "2026-01-23 15:16:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fb7mt",
                  "author": "guigouz",
                  "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally Q3 uses around 20gb here (~14gb on gpu + 6gb on system ram) for a 50k context.\n\nI also tried Q2 but it's too dumb for actual coding, Q3 seems to be the sweet spot for smaller GPUs (Q4 is not **that** better).",
                  "score": 2,
                  "created_utc": "2026-01-24 13:50:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14y7b5",
          "author": "Used_Chipmunk1512",
          "text": "Nope, 30B quantized to q4 will be too much for your gpu, don't download it. Stick with models under 10B",
          "score": 5,
          "created_utc": "2026-01-22 23:08:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14ykru",
              "author": "Expensive-Time-7209",
              "text": "Any recommendations under 10B?",
              "score": 1,
              "created_utc": "2026-01-22 23:10:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14zjyb",
                  "author": "iMrParker",
                  "text": "GLM 4.6v flash is pretty competent for its size. It should fit quantized with an okay context sizeÂ ",
                  "score": 1,
                  "created_utc": "2026-01-22 23:15:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o17he3f",
          "author": "vivus-ignis",
          "text": "I've had the best results so far with gpt-oss:20b.",
          "score": 2,
          "created_utc": "2026-01-23 08:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15prnv",
          "author": "Available-Craft-5795",
          "text": "GPT OSS 20B if it fits. Could work just fine in RAM though.  \nIts surprisingly good",
          "score": 1,
          "created_utc": "2026-01-23 01:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1exduu",
              "author": "Virtual_Actuary8217",
              "text": "Not even support agent tool calling no thank you",
              "score": 0,
              "created_utc": "2026-01-24 12:17:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1f9apu",
                  "author": "Available-Craft-5795",
                  "text": "https://preview.redd.it/xhk81ws4wafg1.png?width=965&format=png&auto=webp&s=be4edb166f90d403c3016ec71a66aa288a66b4e2\n\nWhat?  \nYes it does.",
                  "score": 1,
                  "created_utc": "2026-01-24 13:39:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o160ny9",
          "author": "No-Leopard7644",
          "text": "Try devstral, Qwen 2.5 Coder. You need to choose a quant so that the size of the model fits the vram. Also for coding you need some vram for context. What are using for model inference?",
          "score": 1,
          "created_utc": "2026-01-23 02:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o163jhs",
          "author": "nitinmms1",
          "text": "Anything beyond 8b q4 will be difficult.",
          "score": 1,
          "created_utc": "2026-01-23 02:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17n36t",
          "author": "WishfulAgenda",
          "text": "Iâ€™ve found that higher q in smaller models is really helpful. Also donâ€™t forget your system prompt or agent instructions.",
          "score": 1,
          "created_utc": "2026-01-23 09:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ao98f",
          "author": "Few_Size_4798",
          "text": "There are reviews on YouTube from last week:\n\nThe situation is as follows: even if you don't skimp on the Strix Halo ($2000+ today), all local ones can be shoved in the ass: Claude rules, and Gemini is already pretty good.",
          "score": 1,
          "created_utc": "2026-01-23 19:44:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bxos0",
          "author": "Inevitable_Yard_6381",
          "text": "Hi totally new but tired of waiting Gemini on Android studio to answer...I have a MacBook Pro M1 pro 16 GB ram.. Any chance I could use a local LLM? \nAnd if possible how to integrate with my IDE to work like an agents and have access to my project? Could also be possible to send links to learn some new API or dependency? \nThanks in advance!!",
          "score": 1,
          "created_utc": "2026-01-23 23:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbadr",
          "author": "guigouz",
          "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally",
          "score": 1,
          "created_utc": "2026-01-24 13:50:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gqpua",
          "author": "SnooBunnies8392",
          "text": "I had Nvidia RTX 3060 12GB and I used\n\nQwen3 Coder @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nand\n\nGPT OSS 20B @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nBoth did offload a bit to system ram, but they were both useful anyway.",
          "score": 1,
          "created_utc": "2026-01-24 17:59:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhqu6x",
      "title": "DeepSeek V3.2 (open weights) beats GPT-5.2-Codex and Claude Opus on production code challenge â€” The Multivac daily blind peer eval",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qhqu6x/deepseek_v32_open_weights_beats_gpt52codex_and/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-01-20 04:08:48",
      "score": 29,
      "num_comments": 12,
      "upvote_ratio": 0.82,
      "text": "**TL;DR:** DeepSeek V3.2 scored 9.39 to beat GPT-5.2-Codex (9.20) and every other closed model on a complex coding task. But the real story is Claude Sonnet 4.5 got scored anywhere from 3.95 to 8.80 by different judges â€” same exact code.\n\n# The Test\n\nWe asked 10 models to write a production-grade nested JSON parser with:\n\n* Path syntax (\"user.profile.settings.theme\")\n* Array indexing (\"users\\[0\\].name\")\n* Circular reference detection\n* Typed results with error messages\n* Full type hints and docstrings\n\nThis is a real-world task. Every backend engineer has written something like this.\n\n# Results\n\n|Rank|Model|Score|Std Dev|\n|:-|:-|:-|:-|\n|1|**DeepSeek V3.2**|9.39|0.80|\n|2|GPT-5.2-Codex|9.20|0.50|\n|3|Grok 3|8.89|0.76|\n|4|Grok Code Fast 1|8.46|1.10|\n|5|Gemini 3 Flash|8.16|0.71|\n|6|Claude Opus 4.5|7.57|1.56|\n|7|Claude Sonnet 4.5|7.02|2.03|\n|8|Gemini 3 Pro|4.30|1.38|\n|9|GLM 4.7|2.91|3.61|\n|10|MiniMax M2.1|0.70|0.28|\n\n**Open weights won.** DeepSeek V3.2 is fully open.\n\n# The Variance Problem (responding to yesterday's feedback)\n\nToday's data supports this. Look at Claude Sonnet's std dev: **2.03**\n\nThat's a 5-point spread (3.95 to 8.80) on the same response. Judges fundamentally disagreed on what \"good\" means.\n\nCompare to GPT-5.2-Codex with 0.50 std dev â€” everyone agreed within \\~1 point.\n\nWhen evaluators disagree this much, the benchmark is under-specified.\n\n# Judge Strictness (meta-analysis)\n\n|Judge|Avg Score Given|\n|:-|:-|\n|Claude Opus 4.5|5.92 (strictest)|\n|Claude Sonnet 4.5|5.94|\n|GPT-5.2-Codex|6.07|\n|DeepSeek V3.2|7.88|\n|Gemini 3 Flash|9.11 (most lenient)|\n\nClaude models judge harshly but score mid-tier themselves. Interesting pattern.\n\n# What We're Adding (based on your feedback)\n\n**5 open-weight models for tomorrow:**\n\n1. Llama-3.3-70B-Instruct\n2. Qwen2.5-72B-Instruct\n3. Mistral-Large-2411\n4. **Big-Tiger-Gemma-27B-v3** (u/ttkciar suggested this â€” anti-sycophancy finetune)\n5. Phi-4\n\n**New evaluation dimension:** We're adding \"reasoning justification\" scoring â€” did the model explain its approach, not just produce correct-looking output?\n\n# Methodology\n\nThis is The Multivac â€” daily 10Ã—10 blind peer matrix:\n\n* 10 models respond to same question\n* Each model judges all 10 responses (100 total judgments)\n* Models don't know which response came from which model\n* Rankings from peer consensus, not single evaluator\n\nFull responses and analysis: [https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&utm\\_campaign=post&utm\\_medium=web&showWelcomeOnShare=true](https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\n[themultivac.com](http://themultivac.com)\n\n**Questions welcome. Roast the methodology. That's how we improve.**",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qhqu6x/deepseek_v32_open_weights_beats_gpt52codex_and/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0mmv0v",
          "author": "ResidentPositive4122",
          "text": "If you want accurate and repeatable results you have to use easily verifiable results, not LLM-as-a-judge. Have your question come with a (hidden) test suite. Have the models solve the problem and exit at their own choosing. Run the testing in a separate env. Grade. Anything else is hubris.",
          "score": 11,
          "created_utc": "2026-01-20 07:18:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qv9je",
              "author": "Charming_Support726",
              "text": "Exactly this. I ran a DeepSeek 3.2 evaluation with a harness of mine. Project for steering subagents. I testet a few models with a simple test set. It barely completed. \n\nMost open weight models weren't able to complete an autonomous full agentic coding task - similar to what you mentioned. If you are observing the model while issue commands, it's different game.",
              "score": 1,
              "created_utc": "2026-01-20 21:57:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c11nc",
                  "author": "True_Requirement_891",
                  "text": "This is different from my experience where deepseek v3.2 the non reasoning variant is able to finish end to end tasks.\n\n\nWhile the reasoning variant is kinda messy still.\n\nMake sure you are using the official deepseek api for eval and not openrouter.",
                  "score": 1,
                  "created_utc": "2026-01-23 23:42:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ly9rh",
          "author": "iMrParker",
          "text": "What do you say to the models when you ask them to judge the solutions? Do they choose their own criteria? I feel like the code creations I get from LLMs and agentic workflows don't care about maintainability or readability, resulting in code that won't scale well (lack of modularity etc.). It would be interesting to see what their creations were and judge based on my own experience as a SWE. \n\nI just don't LLMs to judge the outputs of LLMs",
          "score": 2,
          "created_utc": "2026-01-20 04:16:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0npb6s",
          "author": "ForsookComparison",
          "text": "After using all of these models daily for work I can't take benchmarks seriously anymore, none of them.\n\nGPT 5.2 and Deepseek V3.2 are not in the same category.\n\nNeither is in the same category as Opus 4.5 when it comes to code. The gap is monstrous.",
          "score": 2,
          "created_utc": "2026-01-20 12:49:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qvvzl",
              "author": "Charming_Support726",
              "text": "I agree. But as written multiple times - gpt-5.2-codex has advantages over Opus in distinct areas of work",
              "score": 2,
              "created_utc": "2026-01-20 22:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0s1cip",
                  "author": "ForsookComparison",
                  "text": "I'm open to shop. Where do you find codex beating opus lately?",
                  "score": 1,
                  "created_utc": "2026-01-21 01:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0owmxa",
          "author": "GCoderDCoder",
          "text": "I think the hard thing is all these models have things they are better at and others they are worse at. Some follow instructions better but are less accurate making their own logic. I feel qwen3 coder480B handles java spring boot better than a bunch of models supposedly ranked better in swe bench. I'd be curious to see this methodology applied to other categories of tasks. I assume that's what the other benchmarks aim to do but I havent taken the time to investigate deeper into them to know. So I might be recommending redundant efforts but I appreciate you bringing it to the masses directly this way",
          "score": 1,
          "created_utc": "2026-01-20 16:33:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tlqh1",
          "author": "Alone-Marionberry-59",
          "text": "Nobody really cares about these sorts of tasks anymore though.. itâ€™s more like long-running, huge repo, tasks that provide something useful and distinguish a model today, not something done infinitely many times on GitHubâ€¦",
          "score": 1,
          "created_utc": "2026-01-21 08:22:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg403g",
      "title": "Claude Code and local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qg403g/claude_code_and_local_llms/",
      "author": "rivsters",
      "created_utc": "2026-01-18 09:13:25",
      "score": 29,
      "num_comments": 24,
      "upvote_ratio": 0.91,
      "text": "This looks promising - will be trying later today [https://ollama.com/blog/claude](https://ollama.com/blog/claude) \\- although  blog says \"It is recommended to run a model with at least 64k tokens context length.\" Share if you are having success using it for your local LLM. \n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qg403g/claude_code_and_local_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0e71ur",
          "author": "lol-its-funny",
          "text": "EDIT: I tested that even newer llama-server can do this natively without requiring litellm in the middle. See [https://www.reddit.com/r/LocalLLaMA/comments/1qhaq21/comment/o0jtqr0/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qhaq21/comment/o0jtqr0/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\n\\----  \n  \nFYI, LiteLLM can already do this. It can connect with both Anthropic (Claude Code) or OpenAI clients (ChatGPT desktop) and then connect those to Anthropic or OpenAI or llama-cpp (openai compatible) or other native providers.\n\nNot a fan of Ollama with their cloud direction. Anyone interested in cloud will go to the native providers or aggregators like openrouter. Local LLM folks donâ€™t need another cloud focused service.",
          "score": 17,
          "created_utc": "2026-01-19 01:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gkgim",
          "author": "SatoshiNotMe",
          "text": "As others said, months ago llama.cpp already added anthropic messages API compatibility for some popular open-weight LLMs. This makes it easy to hook up these LLMs to work with CC. I had to hunt around for the specific llama-server flag settings for these models and I gathered these into a little guide on setting up these models to work with CC and Codex CLI:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nOne subtle thing to note is that you have to set \n\nCLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC\": \"1\"\n\nin your CC settings, to avoid total network failure due to CCâ€™s logging pings.",
          "score": 7,
          "created_utc": "2026-01-19 11:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hqlpq",
              "author": "Tema_Art_7777",
              "text": "    Â  Â  \"claudeCode.environmentVariables\": [\n    Â  Â  Â  Â  {\n    Â  Â  Â  Â  Â  Â  \"name\": \"ANTHROPIC_BASE_URL\",\n    Â  Â  Â  Â  Â  Â  \"value\": \"http://192.168.10.65:8282\"\n    Â  Â  Â  Â  },\n    Â  Â  Â  Â  {\n    Â  Â  Â  Â  Â  Â  \"name\": \"CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC\",\n    Â  Â  Â  Â  Â  Â  \"value\": \"1\"\n    Â  Â      }\n    Â  Â  ]\n\nThanks for the URL. I am using claudecode in visual studio code. I added to my settings these lines, however, instead of invoking the llama.cpp server, it is asking me to login with anthropic keys. it actually never calls my server. Did I miss a step?",
              "score": 1,
              "created_utc": "2026-01-19 15:43:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0hzz86",
                  "author": "eli_pizza",
                  "text": "You need to set an API key env var too (set it to anything) so it doesnâ€™t try to use oauth",
                  "score": 1,
                  "created_utc": "2026-01-19 16:25:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0eqz5y",
          "author": "Tuned3f",
          "text": "Llama.cpp had this months ago",
          "score": 3,
          "created_utc": "2026-01-19 02:53:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0euind",
              "author": "Tema_Art_7777",
              "text": "How do you hookup claude code to llama.cpp??",
              "score": 2,
              "created_utc": "2026-01-19 03:12:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0evhe8",
                  "author": "Tuned3f",
                  "text": "Set ANTHROPIC_BASE_URL to the llama.cpp endpoint",
                  "score": 4,
                  "created_utc": "2026-01-19 03:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0dy6x3",
          "author": "cars_and_computers",
          "text": "If this is what I think. This is kind of a game changer. The power of Claude code cli but with the privacy of your local models is awesome. Assuming it's actually private. If not it would be amazing to have llamacpp have something like this working and have it be a truly private session",
          "score": 3,
          "created_utc": "2026-01-19 00:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fb0fq",
              "author": "Big-Masterpiece-9581",
              "text": "Claude code router and lite-llm already exist. But I donâ€™t think this is legal. Theyâ€™ll probably sue.",
              "score": 3,
              "created_utc": "2026-01-19 04:57:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0i09me",
                  "author": "eli_pizza",
                  "text": "Sue under DMCA anti-circumvention provision? Seems like a stretch.",
                  "score": 1,
                  "created_utc": "2026-01-19 16:26:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0fawba",
          "author": "Big-Masterpiece-9581",
          "text": "How is this kosher with the proprietary Claude license? They donâ€™t want you using other tools with their models. I have to assume they donâ€™t want you using their tool with other models.",
          "score": 1,
          "created_utc": "2026-01-19 04:56:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i0uuq",
              "author": "eli_pizza",
              "text": "I imagine theyâ€™re quite happy to have the whole industry standardize on Claude code as the best agent, especially local LLM enthusiasts.",
              "score": 1,
              "created_utc": "2026-01-19 16:29:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0m79ph",
              "author": "Internal_Werewolf_48",
              "text": "Claude Code has about a half dozen competing tools on the market. Qwen Code CLI, Mistral Vibe, Charm Crush, Codex, OpenCode, Plandex, Gemini CLI. Probably another five dozen vibe coded slop projects that are already abandoned too.\n\nClaude doesnâ€™t really offer anything that special to bother defending.",
              "score": 1,
              "created_utc": "2026-01-20 05:14:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0m7lpw",
                  "author": "Big-Masterpiece-9581",
                  "text": "I think they have done a fantastic job and constantly innovate. MCP and skills are two of the biggest as well as the cli and its autonomous workflow. But theyâ€™re easy to copy and others do similar things. Iâ€™m really enjoying opencode.",
                  "score": 1,
                  "created_utc": "2026-01-20 05:16:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0htoml",
          "author": "SatoshiNotMe",
          "text": "Can you first check if this works on the command line ? I donâ€™t use VSCode so not too familiar with how to set up CC there.",
          "score": 1,
          "created_utc": "2026-01-19 15:57:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3l75",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5lo30v55iqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 17:05:10",
      "score": 26,
      "num_comments": 11,
      "upvote_ratio": 0.74,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3l75/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0ynu7l",
          "author": "RnRau",
          "text": "You might enjoy this paper from 2024;\n\n> Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget. \n\n\nhttps://arxiv.org/abs/2407.21787",
          "score": 3,
          "created_utc": "2026-01-22 00:55:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10txky",
          "author": "techlatest_net",
          "text": "5-AI debate cage match to kill hallucinations? Brutal eleganceâ€”forces consensus without trusting any single braindead output. Local Ollama hookup is the killer tho, total sovereignty.\n\nSpinning this up tonight. How's the inference overhead hit when all 5 duke it out? ðŸ”¥",
          "score": 1,
          "created_utc": "2026-01-22 10:19:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12pjpn",
              "author": "PastTrauma21",
              "text": "Agreed! Having multiple models debate really helps filter out the noise, and not relying on just one output feels much safer.",
              "score": 1,
              "created_utc": "2026-01-22 16:50:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zyk7h",
          "author": "tvixii",
          "text": "just like how pewdiepie did it lol",
          "score": 0,
          "created_utc": "2026-01-22 05:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w1qch",
          "author": "jaxupaxu",
          "text": "So you copied pewdiepies council concept.Â ",
          "score": -14,
          "created_utc": "2026-01-21 17:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w353u",
              "author": "pokemonplayer2001",
              "text": "https://github.com/karpathy/llm-council",
              "score": 16,
              "created_utc": "2026-01-21 17:35:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0w54x5",
                  "author": "S_Anv",
                  "text": "Karpathy is a great man!\n\nKEA Research is designed as a user-friendly evolution. I've added image support, PDF/md export, text-to-speech conversion, and a full-fledged admin panel for managing local model sets without editing configuration files and many other features\n\nThis means you can create your own model set through a graphical interface  \nAlso as you see there is a bit different logic. You can check readme",
                  "score": 10,
                  "created_utc": "2026-01-21 17:44:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0w4qz2",
              "author": "FirstEvolutionist",
              "text": "I'm not suggesting OP didn't copy the idea from pewdiepie after hearing it from the then, but the idea of a \"council\" is not new, nor introduced by Pewidepie.",
              "score": 13,
              "created_utc": "2026-01-21 17:42:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ou0h",
              "author": "usercantollie",
              "text": "Yes, there are definitely similarities to PewDiePieâ€™s council idea, but the approach here adds structured verification and provider-agnostic flexibility, which opens up new possibilities for local and open-source models. Itâ€™s cool to see how different projects build on each other to solve the trust problem in unique ways.",
              "score": 1,
              "created_utc": "2026-01-22 16:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yvab0",
          "author": "neoKushan",
          "text": "Can you get one of them to check your spelling before you post next time?",
          "score": -4,
          "created_utc": "2026-01-22 01:38:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103d5h",
              "author": "Free-Internet1981",
              "text": "oof",
              "score": 1,
              "created_utc": "2026-01-22 06:18:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhqf8p",
      "title": "LLM Sovereignty For 3 Years.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qhqf8p/llm_sovereignty_for_3_years/",
      "author": "Technical_Buy_9063",
      "created_utc": "2026-01-20 03:49:10",
      "score": 26,
      "num_comments": 45,
      "upvote_ratio": 0.85,
      "text": "Hi folks, perhaps there is a dedicated thread for this but my basic question is:\n\nI want to be able to run LLMs locally, performantly, for the next 3 years or so. I am afraid of compute part costs skyrocketing further, I'm afraid of credits getting more expensive, and I am afraid of cloud offerings being censored in my country. \n\nI have \\~10k USD budget. What would you do in my shoes? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qhqf8p/llm_sovereignty_for_3_years/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0lyko0",
          "author": "Caprichoso1",
          "text": "You could get an Apple M3 Ultra with 80 GPU cores and 512 GB of memory for < $10k.  Some things it will do better than an expensive GPU card with limited memory, for other things a GPU card will outperform it.  It all depends on what you are going to do.",
          "score": 23,
          "created_utc": "2026-01-20 04:17:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n1j69",
              "author": "RandomCSThrowaway01",
              "text": "I should suggest to wait a month since it looks like Apple is preparing new Pro and Max chips at least already (there's suddenly a huge delay on all Pro models), might also include Ultra potentially.",
              "score": 5,
              "created_utc": "2026-01-20 09:34:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0pr7g1",
                  "author": "_hephaestus",
                  "text": "MBPs using the Ultra might possibly be out in a month, but the studio form factor is probably a while longer away with their release cycles.",
                  "score": 1,
                  "created_utc": "2026-01-20 18:53:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0uvp9d",
                  "author": "Late-Assignment8482",
                  "text": "As a career-long Apple professional, I doubt spring. Mac Pro and similar or entirely new products (Vision Pro, Studio) tend to drop at WWDC (June).\n\nNever say never. But June is both closer to timing for say, a Studio using an \"M4 Ultra\" which is really \"make an Ultra newer than M3\" and could also be M5 (it's been 18 months since M4) and more in the Apple corporate calendar.\n\nIf they managed to get it down into a laptop? Any day, they're actually a bit *late* on MacBook Pro.",
                  "score": 1,
                  "created_utc": "2026-01-21 14:12:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0oamn2",
              "author": "GCoderDCoder",
              "text": "I agree and will add that a 256gb m3 ultra runs most of the large open weight Chinese models at usable speeds. Mac can run larger models at usable speeds. Cuda runs smaller models faster. $50k H100s are 100gb roughly each. They are made to serve lots of requests simultaneously. A single person wanting better models can let some of that go to have infinite local access to those cloud models. \n\nIt's not just access to quality local as local really focuses on sovereignty. I can let local llms look through folders with sensitive information with my internet disconnected and even when connected I have the logs in front of me to see exactly what is being transmitted vs the black box from cloud providers.\n\nVs code with extensions and models like glm4.7 will feel like slower versions of cursor and claude code. It is still faster than I can read and anything going to prod should at least be skimmed. Managing context becomes the focus for performance locally so any tool that breaks the work down coupled with your own logical decomposition of your tasks will keep your model faster and make it more enjoyable. Glm4.7 sits in the 15-20t/s space on mac studio when context isnt too heavy but can crawl when context is heavy. \n\nCuda for any given vram amount is most expensive so consider what speed you personally need and have a model(s) in mind for what you buy. A $2k strix halo gets gpt-oss-120b at 50 tokens per second on small context prompt vs needing 3x 3090s ($4-5k) entry level cuda for models that can'tcode like the cloud. I would honestly recommend pairing something smaller and faster like gpt-oss120b and something bigger like glm4.7 so that you have the feeling of fast action vs slower more deliberate responses without having to reload models in between. Gpt-oss-120b on mac studio is like 75-85t/s I think vs 3-4 3090s is 100t/s vs strix halo is 50t/s. \n\nI keep gpt-oss-120b loaded for tool calls like internet searches and quick questions. Coding I use glm4.7/4.6, qwen3coder480b q3kxl and it's reap version from unsloth at q4kxl, and minimaxm2.1. \n\nFor $10k you could get the 512gb mac studio and run multiple models this way simultaneously. However 2x256gb ones can be clustered I think for really big models but then you'd have two sets of processors with 512gb vram instead of one cpu and one gpu handling multiple calls simultaneously.  Now we're multitasking. You could speed up glm4.7 locally or have two of them at once or pair with a smaller faster model like minimax m2.1... $10k leaves you a lot of options to consider.\n\nLast thing, people bash locally hosted models. Im telling you I fight with cloud models like chatgpt and gemini on some tasks that I can give to my local glm4.7 or even gpt-oss-120b and get what I want the first time. The cloud has lots going on behind the scenes and they tune the models for their inference patterns and experience they want you to have. You can have more control to tune to what you want locally but it will take more effort to have a really good experience.",
              "score": 3,
              "created_utc": "2026-01-20 14:48:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lyyvm",
          "author": "TheAussieWatchGuy",
          "text": "I see this asked a lot.\n\n\nCloud AI runs on multiple $50k GPUs and has hundreds of billions of parameters.\n\n\nLocal AI can't yet really compete. It's good but limited.\n\n\nA box with 128gb of RAM shareable with the GPU (RyzenAI 395 or Mac) is a solid start.Â ",
          "score": 15,
          "created_utc": "2026-01-20 04:20:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mf0os",
              "author": "lol-its-funny",
              "text": "GLM 4.7 â€¦ if you can run it Q8 or better , itâ€™s incredible. As good/better than Sonnet 4.5. Prompt it aggressively and itâ€™s really good.",
              "score": 7,
              "created_utc": "2026-01-20 06:12:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ospd1",
                  "author": "By-Jokese",
                  "text": "Kimi K2? Have you tried it?",
                  "score": 2,
                  "created_utc": "2026-01-20 16:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0meuwc",
          "author": "newbietofx",
          "text": "Start buying a tower with rtx GPU and 128 ddr ram",
          "score": 4,
          "created_utc": "2026-01-20 06:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nhhzn",
              "author": "AlexGSquadron",
              "text": "I have 64gb ram and it's not needed. If you have a good GPU with a lot of vram then you are good to go. Depends what you want to do, but ddr4 ram is very slow compared to vram. In my case 30-70 times slower.",
              "score": 1,
              "created_utc": "2026-01-20 11:54:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0nrccw",
                  "author": "jhenryscott",
                  "text": "Even ddr5 much slower. Just get a 5090",
                  "score": 2,
                  "created_utc": "2026-01-20 13:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mkgon",
          "author": "Deer_Avenger",
          "text": "Iâ€™m thinking about exactly the same. Iâ€™m planning to build a pc, but with the current cost of RAM, GPU, and SSD, Iâ€™m thinking if I should wait for prices to drop or is it a long term crisis",
          "score": 2,
          "created_utc": "2026-01-20 06:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0obq3k",
              "author": "Other-Football72",
              "text": "Long term they should go down, but yeah, that might be 2-3 years",
              "score": 1,
              "created_utc": "2026-01-20 14:54:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mpzu9",
          "author": "Vegetable-Score-3915",
          "text": "If just doing inference, pcie count doesn't matter as much.\nCould buy a 2nd hand work station with a couple pcie 3 x 16 slots and get some gpus. Maybe need to add a 2nd psu etc or get some cable adapters.\n\n2nd hand workstations, can still get a decent amount of ddr4 ecc ram included ie 32gb or 64gb without being too crazy expensive with decent quad channel bandwidth, at least compared to consumer equivalent motherboards from the same era that are typically dual channel. That is more so the under $1000 usd build before buying gpus. Can always get a better workstation later and buy more gpus. But if you are just doing inference you probably don't need pcie4 or pcie5 slots",
          "score": 2,
          "created_utc": "2026-01-20 07:46:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yripg",
          "author": "jiria",
          "text": "I asked myself the same question three weeks ago, same budget. I did a whole lot of researching and ended up going for a RTX Pro 6000 Blackwell Max-Q 96Gb. It is expensive but the best value for money and the best performance per watt. I received it a few days ago, inserted it in my desktop PC (a modest Ryzen 5 9600x with 32Gb DDR5 RAM) and the experience has been nothing short of amazing. This thing is a beast (Qwen3-coder-30B: \\~200 tokens/sec) and if you're doing just inference, it is perfectly silent, the fans barely spin, you can have it next to you. Why the Max-Q (300W) and not the regular 600W version? Because even if you lose 15% performance (in inference), you get a silent card that you can use next to you today and use with other cards in a workstation if you wish to expand in the future. Why not a RTX 5090? Too little memory for the models I want to run, and its price is way too inflated right now.",
          "score": 2,
          "created_utc": "2026-01-22 01:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0z3eza",
              "author": "pkieltyka",
              "text": "very cool! are you doing inference on Linux? and what is your software stack used with the rtx pro 6000 blackwell to prompt the model?",
              "score": 1,
              "created_utc": "2026-01-22 02:24:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0z61fk",
                  "author": "jiria",
                  "text": "yep, using Arch. Haven't had that much free time since I installed the GPU a few days ago, but I quickly installed a llama.cpp server for some tests. Will now configure Nvidia's own TensorRT-LLM, which should be even faster. Then I'd like to try stuff I've often seen mentioned in relation to Blackwell architecture, like NVFP4, to get the most out of the card. I will also spend some time tweaking the whole software stack to work well with my limited 32GB RAM (as I refuse to buy RAM at current prices) -- according to AI, it should be possible with only some very acceptable performance losses (mostly in model loading time and cold prefill wait time), which I can definitely live with.",
                  "score": 1,
                  "created_utc": "2026-01-22 02:38:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mui2r",
          "author": "beefgroin",
          "text": "Rtx 6000 pro on Bd895i with 96gb ram",
          "score": 1,
          "created_utc": "2026-01-20 08:27:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n2g8v",
          "author": "S1eeper",
          "text": "You could look into clustering multiple Mac Minis or Studios [[1]](https://www.youtube.com/watch?v=bFgTxr5yst0) [[2]](https://www.youtube.com/watch?v=A0onppIyHEg) [[3]](https://www.youtube.com/watch?v=x4_RsUxRjKU) using Apple's new [RDMA](https://appleinsider.com/articles/25/12/20/ai-calculations-on-mac-cluster-gets-a-big-boost-from-new-rdma-support-on-thunderbolt-5) tech, which directly connects the memory between the machines (bypassing ethernet TCP/IP).  Reduces the latency between machines enough to make them a viable local LLM setup.",
          "score": 1,
          "created_utc": "2026-01-20 09:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n9fxy",
          "author": "reneil1337",
          "text": "I'd get an rtx pro 9000 workstation edition + suitable cpu + ram you'll be able to do a lot with that and if you extend that setup with a second card at some point, scale out to an entire model fleet with embeddings and stuff that all your friends+family can use",
          "score": 1,
          "created_utc": "2026-01-20 10:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nzb84",
          "author": "Sicarius_The_First",
          "text": "Damn. I mean, there's tons of advice in the comments, and I'm sure the intention is good, but ... All of it is really bad.\n\nIs Mac good for inference? Sure. Is it a good value though, price vs performance & upgradeability & flexibility?\nAbsolutely NOT!\n\nHere's what u should actually do:\n1)psu 1500w minimum, buy a new one, but a mid tier one.\n2)case buy the largest, full tower that can fit e ATX board. Don't cheap on it!\n3) mobo & cpu, workstation/ server, buy used, important: u need 4 x pcie x16 lanes (could be pcie3, doesn't matter too much\n4) ram, depends on 3) but u want 64-128 gb\n5) GPUs: x4 a5000 ampere used on ebay, aim for 1k a piece, 1.4k$ is ok too\n\nTotal build should cost just under 10k, 96gb of vram, allowing you to run pretty much everything+ even doing some training",
          "score": 1,
          "created_utc": "2026-01-20 13:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0o02d0",
          "author": "Crafty-Diver-6948",
          "text": "mac studio 512gb you'll be set.",
          "score": 1,
          "created_utc": "2026-01-20 13:53:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ojgcc",
          "author": "Miller4103",
          "text": "Your use case would be helpful.",
          "score": 1,
          "created_utc": "2026-01-20 15:32:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ossfl",
              "author": "Technical_Buy_9063",
              "text": "Coding, agentic search and coding and problem-solving, ai-assistantship, image generation, stuff like that...",
              "score": 1,
              "created_utc": "2026-01-20 16:16:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ovoo8",
                  "author": "Miller4103",
                  "text": "2 rtx 5090 if u can find them. As much ram as possible, and nvme are important parts. \n\n4090's are probably easier to find and just as expensive.\n\n1 rtx 5090 should be good for most of those things but 2 will give u a slight buffer for the future. \n\nI think coding is probably the most hardware intensive since it can need a crap ton of context and tokens.\n\nI have a 5060 ti with 16gb vram and 64gb of ddr5 ram. I can run gpt oss 120b but context us super low. With gpt oss 20b I can get about 61k context. I wanted 128 gb of ram, but prices went up before I could get it.\n\nI would love an ada 6000 but im poor.",
                  "score": 1,
                  "created_utc": "2026-01-20 16:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0opww6",
          "author": "dbay01",
          "text": "Add some money to it and get this beast. Upgrade the memories layer.\n\nhttps://preview.redd.it/pl1n0om42jeg1.jpeg?width=1080&format=pjpg&auto=webp&s=50e397527d735b26f616a929934f197861676ed1",
          "score": 1,
          "created_utc": "2026-01-20 16:02:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qrfg7",
          "author": "C0rvusAlbuS",
          "text": "https://it.msi.com/Landing/EdgeXpert-MS-C931?utm_source=msi&utm_medium=banner",
          "score": 1,
          "created_utc": "2026-01-20 21:40:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t10c6",
          "author": "electrified_ice",
          "text": "My 2 Cents. Threadripper Pro with a motherboard with lots of PCIe slots. That gives you a core and solid foundation that you can add onto as you go/grow/get funds.\n\nIf you can get a Zen 5 PTR Pro, great. If not, get a Zen 4 TR Pro off eBay and save money. You can get a mobi with 8 channels and just populate 4 channels, so you have space to expand. You can start with 2 x 5060 ti or 1 x 5090 and expand. PCIe lanes/slots won't be a bottleneck for the foreseeable future if you start with a good base. TR Pro has >128 PCIe 5 lanes. You can also swap out GPUs. Let's say you find 1 or 2 RTX 5090s, then down the road you find a deal on something else more suitable, you can swap out and sell what you have etc.\n\nA Zen 4 32 core TR Pro + Mobo + 128GB Ram + As big wattage CPU you can find is a good base. Tons of PCIe headroom for GPUs, NVMe drives, AI Accelerators (like a MemryX MX3 etc.).",
          "score": 1,
          "created_utc": "2026-01-21 05:25:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t97hu",
          "author": "Gringe8",
          "text": "Depends on what you want to do, but if i was going to spend 10k i would get a rtx pro 6000 and build the pc in a way that you can add a 2nd gpu in the future.",
          "score": 1,
          "created_utc": "2026-01-21 06:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0moe9f",
          "author": "960be6dde311",
          "text": "Consider getting an NVIDIA DGX SparkÂ ",
          "score": 1,
          "created_utc": "2026-01-20 07:31:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nwzqa",
              "author": "Financial-Source7453",
              "text": "This. So far the best tool for a job for $3k. Smoothly runs gpt-oss-120b and future proof for the nearest 2 years. Advice - Asus clone is 1k cheaper than original DGX",
              "score": 2,
              "created_utc": "2026-01-20 13:36:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0psb6j",
              "author": "Caprichoso1",
              "text": "Best combined with a Mac with EXO.  Each machine is better at different things.  There are a number of Youtube videos discussing the best configuration.  Can't find right now.",
              "score": 1,
              "created_utc": "2026-01-20 18:58:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ntryr",
              "author": "TheAussieWatchGuy",
              "text": "These are ok only if your developers looking to deploy to the cloud.",
              "score": 0,
              "created_utc": "2026-01-20 13:17:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0m6ckh",
          "author": "little___mountain",
          "text": "The best (fastest & smartest) experience will be using a VPN to continue using a leading online LLM. \n\nIf you want a slow but smart local LLM, Iâ€™d get a specâ€™d out Apple with maxâ€™d unified memory. \n\nIf you want a fast and kinda smart local LLM, Iâ€™d custom build a server PC.",
          "score": 1,
          "created_utc": "2026-01-20 05:08:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mczgq",
              "author": "Damn-Sky",
              "text": "how slow is it on a mac?",
              "score": 2,
              "created_utc": "2026-01-20 05:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mf5xe",
                  "author": "little___mountain",
                  "text": "My desktop server is about 4x faster with AI than my  MacBook.\n\nI run an M5 which is the latest Apple chip and a 4070ti super which is a last gen desktop GPU.",
                  "score": 1,
                  "created_utc": "2026-01-20 06:13:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mo24v",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-20 07:28:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0moayt",
              "author": "Former-Tangerine-723",
              "text": "x3d CPU for AI inference? Why?",
              "score": 3,
              "created_utc": "2026-01-20 07:31:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mroj3",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-01-20 08:01:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n2rqg",
          "author": "wouek",
          "text": "I see many expensive ideas, isnâ€™t Nvidia Spark better and cheaper?",
          "score": -1,
          "created_utc": "2026-01-20 09:45:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qljfuw",
      "title": "AI & ML Weekly â€” Hugging Face Highlights",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "author": "techlatest_net",
      "created_utc": "2026-01-24 10:16:03",
      "score": 19,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "Here are the most notableÂ **AI models released or updated this week on Hugging Face**, categorized for easy scanning ðŸ‘‡\n\n# Text & Reasoning Models\n\n* **GLM-4.7 (358B)**Â â€” Large-scale multilingual reasoning modelÂ [https://huggingface.co/zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)\n* **GLM-4.7-Flash (31B)**Â â€” Faster, optimized variant for text generationÂ [https://huggingface.co/zai-org/GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)\n* **Unsloth GLM-4.7-Flash GGUF (30B)**Â â€” Quantized version for local inferenceÂ [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n* **LiquidAI LFM 2.5 Thinking (1.2B)**Â â€” Lightweight reasoning-focused LLMÂ [https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)\n* **Alibaba DASD-4B-Thinking**Â â€” Compact thinking-style language modelÂ [https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking](https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking)\n\n# Agent & Workflow Models\n\n* **AgentCPM-Report (8B)**Â â€” Agent model optimized for report generationÂ [https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n* **AgentCPM-Explore (4B)**Â â€” Exploration-focused agent reasoning modelÂ [https://huggingface.co/openbmb/AgentCPM-Explore](https://huggingface.co/openbmb/AgentCPM-Explore)\n* **Sweep Next Edit (1.5B)**Â â€” Code-editing and refactoring assistantÂ [https://huggingface.co/sweepai/sweep-next-edit-1.5B](https://huggingface.co/sweepai/sweep-next-edit-1.5B)\n\n# Audio: Speech, Voice & TTS\n\n* **VibeVoice-ASR (9B)**Â â€” High-quality automatic speech recognitionÂ [https://huggingface.co/microsoft/VibeVoice-ASR](https://huggingface.co/microsoft/VibeVoice-ASR)\n* **PersonaPlex 7B**Â â€” Audio-to-audio personality-driven voice modelÂ [https://huggingface.co/nvidia/personaplex-7b-v1](https://huggingface.co/nvidia/personaplex-7b-v1)\n* **Qwen3 TTS (1.7B)**Â â€” Custom & base voice text-to-speech modelsÂ [https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base)Â [https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice)Â [https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign)\n* **Pocket-TTS**Â â€” Lightweight open TTS modelÂ [https://huggingface.co/kyutai/pocket-tts](https://huggingface.co/kyutai/pocket-tts)\n* **HeartMuLa OSS (3B)**Â â€” Text-to-audio generation modelÂ [https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B](https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B)\n\n# Vision: Image, OCR & Multimodal\n\n* **Step3-VL (10B)**Â â€” Vision-language multimodal modelÂ [https://huggingface.co/stepfun-ai/Step3-VL-10B](https://huggingface.co/stepfun-ai/Step3-VL-10B)\n* **LightOnOCR 2 (1B)**Â â€” OCR-focused vision-language modelÂ [https://huggingface.co/lightonai/LightOnOCR-2-1B](https://huggingface.co/lightonai/LightOnOCR-2-1B)\n* **TranslateGemma (4B / 12B / 27B)**Â â€” Multimodal translation modelsÂ [https://huggingface.co/google/translategemma-4b-it](https://huggingface.co/google/translategemma-4b-it)Â [https://huggingface.co/google/translategemma-12b-it](https://huggingface.co/google/translategemma-12b-it)Â [https://huggingface.co/google/translategemma-27b-it](https://huggingface.co/google/translategemma-27b-it)\n* **MedGemma 1.5 (4B)**Â â€” Medical-focused multimodal modelÂ [https://huggingface.co/google/medgemma-1.5-4b-it](https://huggingface.co/google/medgemma-1.5-4b-it)\n\n# Image Generation & Editing\n\n* **GLM-Image**Â â€” Text-to-image generation modelÂ [https://huggingface.co/zai-org/GLM-Image](https://huggingface.co/zai-org/GLM-Image)\n* **FLUX.2 Klein (4B / 9B)**Â â€” High-quality image-to-image modelsÂ [https://huggingface.co/black-forest-labs/FLUX.2-klein-4B](https://huggingface.co/black-forest-labs/FLUX.2-klein-4B)Â [https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n* **Qwen Image Edit (LoRA / AIO)**Â â€” Advanced image editing & multi-angle editsÂ [https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA](https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA)Â [https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO)\n* **Z-Image-Turbo**Â â€” Fast text-to-image generationÂ [https://huggingface.co/Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)\n\n# Video Generation\n\n* **LTX-2**Â â€” Image-to-video generation modelÂ [https://huggingface.co/Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)\n\n# Any-to-Any / Multimodal\n\n* **Chroma (6B)**Â â€” Any-to-any multimodal generationÂ [https://huggingface.co/FlashLabs/Chroma-4B](https://huggingface.co/FlashLabs/Chroma-4B)",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qjccpp",
      "title": "AMD ROCm 7.2 now released with more Radeon graphics cards supported, ROCm Optiq introduced",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/AMD-ROCm-7.2-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-21 22:24:16",
      "score": 18,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjccpp/amd_rocm_72_now_released_with_more_radeon/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": [
        {
          "id": "o10t8zr",
          "author": "techlatest_net",
          "text": "ROCm 7.2 dropping RDNA4 support + Optiq? Finally Radeon GPUs get real ML loveâ€”ComfyUI workflows about to fly. Windows builds too? AMD actually shipping for consumers.\n\nTime to dust off the 7900 XTX and pit it vs my CUDA stack",
          "score": 2,
          "created_utc": "2026-01-22 10:12:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qitw9k",
      "title": "Trained a local Text2SQL model by chatting with Claude â€“ here's how it went",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/coqeh3twdoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 09:57:59",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitw9k/trained_a_local_text2sql_model_by_chatting_with/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qgzdqo",
      "title": "opencode with superpowers. It can do everything in a container with docker and nix",
      "subreddit": "LocalLLM",
      "url": "https://grigio.org/opencode-with-superpowers-it-can-do-everything-in-a-container-with-docker-and-nix/",
      "author": "Deep_Traffic_7873",
      "created_utc": "2026-01-19 09:09:36",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qgzdqo/opencode_with_superpowers_it_can_do_everything_in/",
      "domain": "grigio.org",
      "is_self": false,
      "comments": [
        {
          "id": "o0ge5py",
          "author": "WolfeheartGames",
          "text": "This is such a cool idea.",
          "score": 2,
          "created_utc": "2026-01-19 10:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0intvs",
          "author": "c4rb0nX1",
          "text": "will try this for sure.",
          "score": 1,
          "created_utc": "2026-01-19 18:12:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qii3h2",
      "title": "Can I add a second GPU to use it's vram in addition of the vram of my main GPU to load bigger models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qii3h2/can_i_add_a_second_gpu_to_use_its_vram_in/",
      "author": "Babidibidibida",
      "created_utc": "2026-01-21 00:01:34",
      "score": 13,
      "num_comments": 35,
      "upvote_ratio": 0.93,
      "text": "I have a 5070 Ti 16gb + a 7950X + 96gb of ram. I was waiting for the 5070Ti Super 24Gb to be release to buy one, but the ram shortage situation made me buy a 16gb in a hurry.  \nObviously I can't load as big of models in vram as I was expecting (and RTX 4090 and 5090 are too expensive) but I thought that maybe it was possible  to add a second GPU like a second hand 24gb RTX3090 or a RTX 5060 16gb in order to use the vram of that second GPU in addition to the vram of the first one? (so Vram from GPU 1+2 would be seen as just one big capacity) Is it possible to do that? If yes, how? I'm using LM Studio\n\nWhat would be best between a 3090 (24Gb) and 5060 Ti (16gb)? I know there's more vram in the 3090 but maybe i\\_t's less fitted for AI than a more recent 5060 Ti?\n\nThanks",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qii3h2/can_i_add_a_second_gpu_to_use_its_vram_in/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0rlmq1",
          "author": "FullstackSensei",
          "text": "I have a few 3090s and recently got a 5060ti (just before prices went up) to play with a bit. Long story short: it can't hold a candle next to the 3090. I'm sure someone will come here to talk about how Ampere lacks fp8 or nvfp4, but if you don't absolutely need these two formats natively supported, the 3090 wins in everything else. It has 50% more VRAM and 100% more memory bandwidth. The former lets you load bigger models, and if you have more than one 3090 there's less waste in VRAM (that's mainly a software deficiency in current inference engines). The memory bandwidth means more of the compute is accessible more of the time.\n\nThe 5070ti is abice the 3090 in terms of compute, and practically has the same memory bandwidth. So, think of the 3090 as a second 5070ti with 50% more VRAM.\n\nGenerally speaking, moar VRAM is moar better. I'd even sell the 5070Ti since that is more expensive than the 3090 where I live and put the money towards getting two 3090s. 48GB  >> 32GB VRAM.",
          "score": 14,
          "created_utc": "2026-01-21 00:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rm4jg",
              "author": "Babidibidibida",
              "text": "Thanks for the detailed answer!",
              "score": 2,
              "created_utc": "2026-01-21 00:18:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0rnei1",
              "author": "Prudent-Ad4509",
              "text": "It is better to keep at least one blackwell card for the few initial model layers or for the draft model. The other 2,4, or multiples of 4 3090s can pick up the rest.",
              "score": 1,
              "created_utc": "2026-01-21 00:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0rsxcn",
                  "author": "FullstackSensei",
                  "text": "I have yet to see a draft model that is actually useful in real world scenarios. Put another way, if the draft model had a high acceptance rate, the big model would be out of a job.\n\nMultiple cards of the same model enable row splitting (-sm row) in llama.cpp which improves performance much more than a draft model, and if you have a power of two number of cards and use mainly one model, you can run vllm with tensor parallelism for even higher generation speed.",
                  "score": 3,
                  "created_utc": "2026-01-21 00:55:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0rx22x",
                  "author": "Babidibidibida",
                  "text": "My PC is also a gaming PC so i'll keep my 5070Ti for gaming anyway :D",
                  "score": 1,
                  "created_utc": "2026-01-21 01:19:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0sd2as",
              "author": "Mabuse046",
              "text": "Not to mention the 3090 still has Nvlink which is so much faster than PCIE.",
              "score": 1,
              "created_utc": "2026-01-21 02:50:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tiy56",
                  "author": "FullstackSensei",
                  "text": "For inference it doesn't make a big difference if you have 8 Gen 3 lanes for each card or more. It's been tested multiple times and the difference is always something like 3%1",
                  "score": 2,
                  "created_utc": "2026-01-21 07:56:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0t4soo",
              "author": "remghoost7",
              "text": ">I'm sure someone will come here to talk about how Ampere lacks fp8...\n\nDual 3090's here.\n\nThe only downside for a lack of FP8 are video models.  \nEverything else locally hosted AI related is *freaking awesome* with this much VRAM.",
              "score": 1,
              "created_utc": "2026-01-21 05:53:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0rl6r1",
          "author": "Prudent-Ad4509",
          "text": "For LLM inference - yeah, 3090 24gb is the king. llama.cpp works best with several gpus, lm studio seems to work too, their online help is one web search away.",
          "score": 4,
          "created_utc": "2026-01-21 00:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rm5yn",
              "author": "Babidibidibida",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-21 00:18:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rpyep",
                  "author": "TechnologyGrouchy679",
                  "text": "guess you got your answer",
                  "score": 1,
                  "created_utc": "2026-01-21 00:39:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0rvxav",
              "author": "No_Ad4069",
              "text": "I use LM Studio with two 5080s and confirm that it works",
              "score": 1,
              "created_utc": "2026-01-21 01:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0rx32r",
              "author": "FuzzeWuzze",
              "text": "Are there good(free) ways as a consumer to mesh cards from other systems together with mine?  I have several cards spread across different systems, but no system that has  a motherboard that can hold them all, nor do i want to end the other usees of those machines.  It'd be nice to just throw everything my house has at it, even if its just an additional shitty 1080ti with 11Gb ...",
              "score": 1,
              "created_utc": "2026-01-21 01:19:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0rzlhd",
                  "author": "Prudent-Ad4509",
                  "text": "There are some options, but nothing too simple. Mostly splitters x16->4x4x4x4x. As for 1080ti, they are best kept as a backup gpu just in case. Or used for gaming, the are still mostly relevant, often even for 4k60Hz with carefully configured settings.",
                  "score": 1,
                  "created_utc": "2026-01-21 01:34:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0uagto",
              "author": "FrumunduhCheese",
              "text": "Do you need the latest pci 5 or 6 or whatever it is ? I have an old hp z620 Iâ€™m looking at throwing a 3090 in it has 64 gb ddr3 and 32 cores.",
              "score": 1,
              "created_utc": "2026-01-21 12:03:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ub2yi",
                  "author": "Prudent-Ad4509",
                  "text": "The proper way to handle a few PCIe 5.0 cards for training is to buy $2000 PCIe switch for 100 pci lanes. Same for PCIe 4.0, for half the price. Then plug it into whatever, any gaming PC will do. Every other perfect option costs more. But for inference you will be good with 4.0 x8, and probably ok with x4 if you do not do tensor parallel.",
                  "score": 1,
                  "created_utc": "2026-01-21 12:08:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0s05dc",
          "author": "WishfulAgenda",
          "text": "Dual  5070ti here. Works great with 30b q6 35k context. Bifurcated 8x8 pcie gen4",
          "score": 3,
          "created_utc": "2026-01-21 01:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rmg0a",
          "author": "TheAussieWatchGuy",
          "text": "More VRAM is king. You can absolutely have multiple GPUs, 3060s have even had a resurgence, put four or eight in a single motherboard ... The 3090 is still the best value but it's aging.Â \n\n\nPurely for AI and running big models Mac or Ryzen AI 395 with 128gb+ of ddr5 system memory are the best value because you can share 112gb with the built in GPU and run giant models.\n\n\nGPUs still offer better tokens per second so it's still a trade off... But you'll never run a 200b+ parameter model on less than 96gb unified memory anyway so really upto you. Smaller models on a multiple GPUs, faster tokens per second but limited in how big the models you can run are. Unifies platform sharing CPU / GPU memory, run much bigger wmarter models but slower token output.Â ",
          "score": 2,
          "created_utc": "2026-01-21 00:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0roo5w",
          "author": "starkruzr",
          "text": "*if* you have the PCIe lanes and *if* you can find cheap enough 5060Ti 16GBs, you can make an argument that they represent a good option. a single 3090 with 24GB VRAM is upwards of $850 rn. two 5060Tis with 16GB each (32GB total) is *theoretically* about the same price as that 3090, but idk how much stock of them remains out there.",
          "score": 2,
          "created_utc": "2026-01-21 00:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rxsji",
          "author": "jacek2023",
          "text": "Yes, that's what real men do.",
          "score": 1,
          "created_utc": "2026-01-21 01:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sjpz4",
          "author": "No-Consequence-1779",
          "text": "Depending upon the speed you need, you can pick up server gpus for cheap all day.Â ",
          "score": 1,
          "created_utc": "2026-01-21 03:30:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sg6wb",
          "author": "digabledingo",
          "text": "keep in mind that apple can share video memory with its system ram , combining the two which no other platform can do... I almost very close came to switching to apple",
          "score": 0,
          "created_utc": "2026-01-21 03:09:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0t5cau",
              "author": "MrNantir",
              "text": "Isn't that what the Strix Halo / AMD Ai Max+ 395 does also?",
              "score": 2,
              "created_utc": "2026-01-21 05:58:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tltjl",
                  "author": "digabledingo",
                  "text": "I think there's a way to set them up in a way where you can link them as two sperate cards each one firing a unique seed , at least that's how I understand 2 gpus in comfyai",
                  "score": 1,
                  "created_utc": "2026-01-21 08:23:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0t36lt",
              "author": "Babidibidibida",
              "text": "I already got a powerful PC I don't see myself buying an Appl, they are so expensive, and I also game so I need a PC",
              "score": 1,
              "created_utc": "2026-01-21 05:41:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkxpuo",
      "title": "AMD Ryzen AI Software 1.7 released for improved performance on NPUs, new model support",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/AMD-Ryzen-AI-Software-1.7",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-23 17:52:03",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkxpuo/amd_ryzen_ai_software_17_released_for_improved/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qluto8",
      "title": "HashIndex: No more Vector RAG",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 18:33:54",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 0.87,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama and llama cpp . Give it a try and consider implementing it in your system â€” you might like it! Give us a star maybe hahahaha\n\n[https://github.com/JasonHonKL/HashIndex/tree/main](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1h4awo",
          "author": "jschw217",
          "text": "Why does it require httpx? Any connections to remote servers?",
          "score": 1,
          "created_utc": "2026-01-24 18:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h4gs2",
              "author": "jasonhon2013",
              "text": "Oh itâ€™s is for the purpose of fetching APIs like open router not connected to any remote server no worries !",
              "score": 2,
              "created_utc": "2026-01-24 18:57:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ifug5",
          "author": "FaceDeer",
          "text": "I didn't see any documentation there about how the \"guts\" of the system worked, so [I asked Gemini to do a Deep Research run to produce one](https://gemini.google.com/share/2e13e0d1a6fe). Some key bits:\n\n> The documentation for HashIndex identifies it as a \"vectorless\" index system. This characterization is central to its \"under the hood\" operations. Instead of calculating a mathematical hash or a vector embedding, the system invokes an LLM to generate what it terms a \"semantic hash key\".  \n\n> When a document is ingested by HashIndex, it is first split into segments or pages. For each segment, the system initiates a dual-process LLM call. The first process involves generating a highly descriptive, human-readable label that encapsulates the core theme of the content. This labelâ€”for example, ``revenue_projections_FY2024_Q3``â€”serves as the index key in the hash map. The second process generates a concise summary of the page.\n\n> This \"single-pass\" parsing allows the document to be structured for retrieval without the need for pre-computed embedding datasets. However, the cost of this precision is time. While a traditional cryptographic hash function $H(x)$ or an embedding model can process data in milliseconds, the semantic key generation in HashIndex requires significant inference time, typically 2 to 3 seconds per page.\n\n[...]\n\n> In HashIndex, the hash table is implemented in-memory, allowing for rapid access once the indexing phase is complete. The \"hash function\" in this context is the cognitive process performed by the LLM during key generation. This approach eliminates the need for complex tree rebalancing and multi-level traversal required by systems like ChatIndex or PageIndex. However, it places a higher burden on the \"agentic\" side of the retrieval process, as the agent must now navigate a flat list of keys rather than a hierarchical tree. \n\nDoes this look like an accurate summary of how it works? Might be worth calling out that the \"hash\" in this case is not a traditional hash in the way that word is usually meant, but an LLM-generated semantic \"tag\" of sorts.",
          "score": 1,
          "created_utc": "2026-01-24 22:38:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}