{
  "metadata": {
    "last_updated": "2026-02-20 09:08:34",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 240,
    "file_size_bytes": 254607
  },
  "items": [
    {
      "id": "1r4mcwb",
      "title": "Built a 6-GPU local AI workstation for internal analytics + automation â€” looking for architectural feedback",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r4mcwb",
      "author": "shiftyleprechaun",
      "created_utc": "2026-02-14 14:43:58",
      "score": 201,
      "num_comments": 101,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4mcwb/built_a_6gpu_local_ai_workstation_for_internal/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5cjx4r",
          "author": "jhenryscott",
          "text": "Greta job! Taking control of your own LLMs without giving all your data to Scam Altman Is a good practice.  \n\nI donâ€™t have any insight into building your system except to recommend the Level1Techs forum. Youâ€™ll find lot other people doing this there who can answer your Questions",
          "score": 28,
          "created_utc": "2026-02-14 15:01:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cnwti",
              "author": "shiftyleprechaun",
              "text": "awesome man, appreciate it, i will check out the sub!",
              "score": 5,
              "created_utc": "2026-02-14 15:23:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gltck",
                  "author": "ThinkPad214",
                  "text": "Damn man. Nice build, I've got a old gaming PC with a 3060 12gb and 5060 ti 16gb learning how to properly set up smaller Agentic AI for personal and professional use.",
                  "score": 2,
                  "created_utc": "2026-02-15 05:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6czvg3",
              "author": "Zealousideal-Path-93",
              "text": "It depends which LLM you run, some can still have your data and see your network",
              "score": 1,
              "created_utc": "2026-02-20 03:25:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5clpbg",
          "author": "KnownAd4832",
          "text": "1 Usually the bottleneck is always vram then hardware support (multi-gpu/cluster inference is usually hard to set up with little documentation or its gatekept). If that makes sense, then usually third comes in Storage :)\n\n2 Long term is same as running a single gpu.\n\n3 It all depends on your case, my ROI was done in 2 months from buying\n\n4 multiple smaller nodes - the models are going stronger while being smaller. In 2 years there will be Kimi K2.5 level in 70B without a doubt. So it only depends on you if you need inference speed or variety if models\n\n5 They dont test on rented servers before buying imo. Did this mistake myself with first rig",
          "score": 11,
          "created_utc": "2026-02-14 15:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cox9m",
              "author": "shiftyleprechaun",
              "text": "Really appreciate the detailed breakdown.\n\n1. Makes total sense, VRAM is the hard ceiling and everything else is optimization after that. The documentation gap for multi-GPU is real, I've spent more time piecing together config from GitHub issues than actual docs.\n\n2. Good to hear. I'm mixing card types but pinning models deliberately so each card basically runs independently.\n\n3. That's a fast ROI, are you running inference as a service or using it for internal workloads?\n\n4. This is the point I keep coming back to. The trend toward stronger smaller models is exactly why I went with a distributed multi-card approach rather than dumping everything into one or two massive GPUs. If a 70B model can do what a 400B does today, having flexible GPU allocation matters more than raw single-card VRAM.\n\n5. Haha noted. I did a lot of benchmarking and testing before committing to the full build, but I can see how easy it would be to overbuy on hardware that doesn't match your actual workload.  I also spent around $5-6k using open AI API (Using Chat GPT 4o) via Azure in December.\n\n",
              "score": 4,
              "created_utc": "2026-02-14 15:28:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v65jb",
              "author": "nakedspirax",
              "text": "How did you get ROI if you don't mind me asking",
              "score": 1,
              "created_utc": "2026-02-17 13:52:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vm9i5",
                  "author": "KnownAd4832",
                  "text": "I was paying together.ai for inference. So I just bought the rig and replaced that cost :)",
                  "score": 1,
                  "created_utc": "2026-02-17 15:16:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dc1vo",
          "author": "FinancialMoney6969",
          "text": "Looks great man, I like the open air aspect",
          "score": 8,
          "created_utc": "2026-02-14 17:26:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dcdqe",
              "author": "shiftyleprechaun",
              "text": "Thank you. I went with the open air concept, bc I found it difficult to find an enclosure that would work.",
              "score": 2,
              "created_utc": "2026-02-14 17:27:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e049q",
          "author": "chafey",
          "text": "Your build is awesome, I am doing something very similar.  Here are some improvements you may want to consider:\n\n1. Upgrade your processor to improve your memory bandwidth.  You have a Threadripper Pro CPU and Motherboard which is better than the non pro Threadripper systems for two reasons: a) more PCIe lanes and b) 8 memory channels.  Unfortunately the 9955wx CPU only has two CCDs so can't utilize all 8 memory channels - you need a CPU with more CCDs (such as the 9965wx) to make use of the 8 memory channels.  [https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa\\_the\\_new\\_threadripper\\_pros\\_9000\\_wx\\_are\\_still/](https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/)\n2. PCIe 5.0 has benefits for AI.  The 3090tis are good bang/buck but they are running over PCIe 4.0 so can't take advantage of the new PCIe 5.0 features.  The actual benefit of an all PCIe 5.0 solution depends on your use case and model but it is more than just twice the bandwidth. [https://www.graniteriverlabs.com/en-us/technical-blog/pcie-gen-5-ai-ml](https://www.graniteriverlabs.com/en-us/technical-blog/pcie-gen-5-ai-ml)\n3. The up coming Apple M5 systems may very well be the best bang/buck due to their recently released RDMA over TB5, MLX AI acceleration and its high speed unified memory architecture.  I am really looking forward to seeing how a cluster of M5 mac minis does.  Check out exo: [https://github.com/exo-explore/exo](https://github.com/exo-explore/exo)",
          "score": 4,
          "created_utc": "2026-02-14 19:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5easi7",
              "author": "shiftyleprechaun",
              "text": "I have throught about upgrading the CPU. And also possibly replacing the 3090s entirely . However, I can never bring myself to get a Mac/apple product for anything , yeah I'm one of those guys hah.",
              "score": 3,
              "created_utc": "2026-02-14 20:23:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5e6k0h",
              "author": "starkruzr",
              "text": "my understanding is that what really counts for inference is keeping latency between cards as low as possible rather than super high bandwidth. does this mean PCIe 5's latency is naturally lower than 4's?\n\nthat note about M5 is interesting. I keep waiting for M5 Ultra to be announced!",
              "score": 2,
              "created_utc": "2026-02-14 20:00:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5e8clz",
                  "author": "chafey",
                  "text": "Yes, PCIe 5 has about 50% lower latency compared to PCIe 4.  \n\nHere is a good video on mac clustering with exo: [https://www.youtube.com/watch?v=4l4UWZGxvoc](https://www.youtube.com/watch?v=4l4UWZGxvoc)",
                  "score": 3,
                  "created_utc": "2026-02-14 20:10:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5fqs11",
                  "author": "BillDStrong",
                  "text": "Since speed and latency are properties of the underlying physics, going twice the bandwidth has a 2x latency speedup.\n\nThis is also true of internet, so 10G is 10x speedup latentency than 1G, 40G is 40x, and 100G is 100x.\n\nThis is why Nvidia is  able to use 200G and 400G in their Enterprise AI offerings to sub for PCIe, the latency is so close to not make a difference at those speeds.",
                  "score": 2,
                  "created_utc": "2026-02-15 01:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gqqv7",
          "author": "OwnPomegranate5906",
          "text": "I run 6 rtx 3060s (12GB each) on a dual processor xeon 2690 v4 and 128GB of ram, also in an open air mining case.  I only run Debian 12, one instance of ollama and only one model at a time.\n\nI spent a fair amount of time on PCIe and power tuning.  A lot of performance problems that I had was because when running a larger model that spanned multiple cards, the latency caused by traversing the PCIe bus (and sometimes Numa node) was just long enough that the cards power management would bump it from P2 to P5+, then jump back up to P2 when it actually needed to do something, which added even more latency, which caused cascading slowdown across all the cards where I'd start off really fast, then as the response typed out, it would progressively slow down to the point of just a couple tokens a second.  My setup is not PCIe x16 for every card, so that also exacerbated the issue.\n\nThe solution was multi-fold: \n1. set the power management so that the cards idle at P3 and jump to P2 for inference.  This fixed a huge amount of performance problems with larger contexts and responses.  \n2. Put the cards that actually have PCIe x16 connections on the ends of the chain and the x8 cards in the middle.  Also, order the cards (in ollama.service) so that all the cards on each numa node are together to minimize the number of cross numa jumps during inference.  \n3. Also in the bios, set the PCI max request packet size to 4096.  On my particular system, it defaulted to 128.  Turns out the 3060s won't do more than 256, but it's worth more than a couple tokens a second.\n4. Also in the BIOS, turn rebar on so the system can see and address the entire PCIe VRAM instead of addressing it in 256MB chunks, also worth a few more tokens per second.",
          "score": 5,
          "created_utc": "2026-02-15 05:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cnveg",
          "author": "King_Kasma99",
          "text": "I never thought about looking for wireless dp or hdmi adaptors, Thank you!",
          "score": 3,
          "created_utc": "2026-02-14 15:23:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cpk3u",
              "author": "shiftyleprechaun",
              "text": "The adapters aren't wireless, they are \"Dummy Plugs\" so i can remote in and use dual monitors.  here is link to one i bought.\n\n[https://www.amazon.com/Headless-Emulator-Compatible-Multiple-Resolutions/dp/B0B5XVQVJ9/ref=sxts\\_b2b\\_sx\\_fused\\_v3\\_desktop\\_ref-tab-0?content-id=amzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f%3Aamzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f&crid=3PB8RH0689B4E&cv\\_ct\\_cx=hdmi%2Bdummy%2Bplug&keywords=hdmi%2Bdummy%2Bplug&pd\\_rd\\_i=B0B5XVQVJ9&pd\\_rd\\_r=d68b8781-951e-4d4e-8ac6-d9ddc6db8114&pd\\_rd\\_w=c37v9&pd\\_rd\\_wg=PdjXx&pf\\_rd\\_p=820daa87-0f14-4ace-aeae-d9224f14cf8f&pf\\_rd\\_r=TB8XZ8AJ3PZ9GVKVH0YF&qid=1771083053&sbo=RZvfv%2F%2FHxDF%2BO5021pAnSA%3D%3D&sprefix=hdmi%2Bdummy%2Bpolug%2Caps%2C99&sr=1-6-c3caa9c6-537b-4b39-bbc5-5db9f871bef5&th=1](https://www.amazon.com/Headless-Emulator-Compatible-Multiple-Resolutions/dp/B0B5XVQVJ9/ref=sxts_b2b_sx_fused_v3_desktop_ref-tab-0?content-id=amzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f%3Aamzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f&crid=3PB8RH0689B4E&cv_ct_cx=hdmi%2Bdummy%2Bplug&keywords=hdmi%2Bdummy%2Bplug&pd_rd_i=B0B5XVQVJ9&pd_rd_r=d68b8781-951e-4d4e-8ac6-d9ddc6db8114&pd_rd_w=c37v9&pd_rd_wg=PdjXx&pf_rd_p=820daa87-0f14-4ace-aeae-d9224f14cf8f&pf_rd_r=TB8XZ8AJ3PZ9GVKVH0YF&qid=1771083053&sbo=RZvfv%2F%2FHxDF%2BO5021pAnSA%3D%3D&sprefix=hdmi%2Bdummy%2Bpolug%2Caps%2C99&sr=1-6-c3caa9c6-537b-4b39-bbc5-5db9f871bef5&th=1)",
              "score": 3,
              "created_utc": "2026-02-14 15:32:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d6lf7",
          "author": "Senior-Okra-6325",
          "text": "Ver tanta GPU junta es hipnotico. Solo quiero imaginarme el nivel de ruido. Mi rig es modesto porque solo lo uso para analisis cualitativo de transcripciones biograficas (3090 + 4070). Te envidio tu config, un saludo!",
          "score": 3,
          "created_utc": "2026-02-14 16:58:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dc2jl",
              "author": "shiftyleprechaun",
              "text": "If I didn't have two additional external fans blowing on it, it wouldn't be that loud tbh.",
              "score": 3,
              "created_utc": "2026-02-14 17:26:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dknvg",
          "author": "Character-Ad-2048",
          "text": "This is an awesome build. Funny I just bought the same frame myself. Do you mind sharing your CPU/Motherboard specs and why you went with your decision? Iâ€™m looking to upgrade from my current setup from 4 GPUs and need to find a new CPU/Motherboard",
          "score": 2,
          "created_utc": "2026-02-14 18:09:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dufeh",
              "author": "shiftyleprechaun",
              "text": "Sure. I went with my own setup, because for what I am trying to accomplish, I didn't want to be beholden to the giant corporations. And I did already create everything that I am now doing locally through azure and open AI APIs and I spent thousands of dollars on it.\n\nFor what i ran all night last night, would have cost me $3-4k with open AI chat gpt 4oz vs $6 in electricity. Of course I have spent a lot of money on the hardware, but the roi will hopefully prove itself over the coming years.\n\nUpdated â€“ February 13, 2026\r\nOS - Ubuntu 24.04 LTS Desktop\r\nMotherboard - ASUS WRX90E-SAGE Pro WS SE AMD sTR5 EEB\r\nCPU - AMD Ryzen Threadripper PRO 9955WX Shimada Peak 4.5GHz 16-Core sTR5\r\nSDD â€“ (2x4TB) Samsung 990 PRO 4TB Samsung V NAND TLC NAND PCIe Gen 4 x4 NVMe M.2 Internal\r\nSSD\r\nSSD - (1x8TB) Samsung 9100 PRO 8TB Samsung V NAND TLC NAND (V8) PCIe Gen 5 x4 NVMe M.2\r\nInternal SSD with Heatsink\r\nPSU #1 - SilverStone HELA 2500Rz 2500 Watt Cybenetics Platinum ATX Fully Modular Power Supply - ATX\r\n3.1 Compatible\r\nPSU #2 - MSI MEG Ai1600T PCIE5 1600 Watt 80 PLUS Titanium ATX Fully Modular Power Supply - ATX 3.1\r\nCompatible\r\nPSU Connectors â€“ Add2PSU Multiple Power Supply Adapter (ATX 24Pin to Molex 4Pin) and Daisy Chain\r\nConnector-Ethereum Mining ETH Rig Dual Power Supply Connector\r\nUPS - CyberPower PR3000LCD Smart App Sinewave UPS System, 3000VA/2700W, 10 Outlets, AVR, Tower\r\nRam - Kingston FURY Renegade Pro 256GB (8 x 32GB) DDR5-5600 PC5-44800 CL28 Quad Channel ECC\r\nRegistered Memory Modules KF556R28RBE2K4-128\r\nCPU Cooler - Thermaltake WAir CPU Air Cooler\r\nGPU Cooler â€“ (6x) Arctic P12 PWM PST Fans (externally mounted)\r\nCase Fan Hub â€“ Arctic 10 Port PWM Fan Hub w SATA Power Input\r\nCase/Build â€“ Open air Rig\r\nGPU 1 - PNY RTX 6000 Pro Blackwell\r\nGPU 2 â€“ PNY RTX 6000 Pro Blackwell\r\nGPU 3 â€“ FE RTX 3090 TI\r\nGPU 4 - FE RTX 3090 TI\r\nGPU 5 â€“ EVGA RTX 3090 TI\r\nGPU 6 â€“ EVGA RTX 3090 TI\r\nUninstalled \"Spare GPUs\":\r\nGPU 7 - Dell 3090 (small form factor)\nGPU 8 - Zotac Geforce RTX 3090 Trinity\r\nPossible Expansion of GPUs â€“ Additional RTX 6000 Pro Maxwel",
              "score": 2,
              "created_utc": "2026-02-14 18:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5eebyy",
                  "author": "RedditNoob001",
                  "text": "Hey amazing build! Do you mind sharing what riser cables (brand/length) worked for you? Trying to figure out which pcie 4.0 cables I should get for my 3090s. ",
                  "score": 2,
                  "created_utc": "2026-02-14 20:43:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ff1rj",
                  "author": "Character-Ad-2048",
                  "text": "Thatâ€™s smart to have done that early test to ensure. I just want to say thank you for a really detailed breakdown of the components! Really appreciate it and can see a lot of thought went into it!",
                  "score": 2,
                  "created_utc": "2026-02-15 00:14:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ftahr",
                  "author": "Metrix1234",
                  "text": "Did you use any guides making your local LLM build?  Iâ€™m very curious the use cases of building one vs API vs CLI.  Do you actually need to train your local LLM?\n\nIâ€™m pretty new to even using the big production models but the more I use them the more I fear eventually they will be gatekeeped for only rich/enterprise consumers. \n\nWould love to learn more about the cost of your setup and what itâ€™s capable of.",
                  "score": 2,
                  "created_utc": "2026-02-15 01:45:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fcoof",
          "author": "Just3nCas3",
          "text": "Level1 just put out a [video recently about bifurcation](https://www.youtube.com/watch?v=Dd6-BzDyb4k), something to look into if you want to add gpus without replacing any and a quick search of your motherboard says it supports it.",
          "score": 2,
          "created_utc": "2026-02-15 00:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fct75",
              "author": "shiftyleprechaun",
              "text": "Awesome I'll check it out ty",
              "score": 2,
              "created_utc": "2026-02-15 00:00:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5i783u",
          "author": "greg-randall",
          "text": "Have you figured out your software stack?",
          "score": 2,
          "created_utc": "2026-02-15 13:39:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5irm8p",
              "author": "shiftyleprechaun",
              "text": "Not quite yet. \n\nBut for This first phase I am running:\n4 separate instances of llama 3.1 8B (1 on each the 3090's)\n2 separate instances of llama 3.3 70B (1 on each of the Maxwell 6000 pros)\n\nAfter I have the initial data analysis complete, I still need to figure out the next steps, which I am working on now.",
              "score": 1,
              "created_utc": "2026-02-15 15:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5igm1e",
          "author": "Fair_Permission_9005",
          "text": "Power consumption? \n\n",
          "score": 2,
          "created_utc": "2026-02-15 14:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ish9u",
              "author": "shiftyleprechaun",
              "text": "With all gpus running Max around 3.35kwh",
              "score": 1,
              "created_utc": "2026-02-15 15:37:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kc6ay",
          "author": "basedgewner",
          "text": "Iâ€™m coming.",
          "score": 2,
          "created_utc": "2026-02-15 20:10:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ki0xq",
              "author": "shiftyleprechaun",
              "text": "Come on over ðŸ˜‚",
              "score": 1,
              "created_utc": "2026-02-15 20:40:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5meuv7",
          "author": "lcjanke2020",
          "text": "That is one sick rig!",
          "score": 2,
          "created_utc": "2026-02-16 03:21:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nyrxh",
              "author": "shiftyleprechaun",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-16 11:17:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5t0dqo",
          "author": "Physical-Scholar3176",
          "text": "I don't think most of us can spend new car money for this. Still very cool. What is your job?\n\n",
          "score": 2,
          "created_utc": "2026-02-17 03:35:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ujai9",
              "author": "shiftyleprechaun",
              "text": "I own and run a small logistics company.",
              "score": 1,
              "created_utc": "2026-02-17 11:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wkf7r",
          "author": "hetarthvader",
          "text": "So cool!",
          "score": 2,
          "created_utc": "2026-02-17 18:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xprfh",
          "author": "tachyi",
          "text": "amazing!",
          "score": 2,
          "created_utc": "2026-02-17 21:18:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60wa0i",
          "author": "Aware_Photograph_585",
          "text": "I have: 2x 3090, 3x 4090 48gb, 2x pro 6000\n\n1) I don't trust pcie riser cables. If you have any errors during training: timeout, gpu fell off (I forget the exact wording), weird training speeds, etc, it's most likely your cables.  You need pcie retimer/redriver card with those cable lengths.  Especially with pcie5.  Check any professional training servers, they have retimer/redriver if the gpu is not plugged into the pcie slot.  Heck, my HBA cards have a retimer to connect to my HDD/NVME.  On the other hand, inference tends to be fine with pcie cables.  Have you done any serious multi-day/week training yet? \n\n2) CPU doesn't matter as much as people think.  I use an 8 core AMD 7002 cpu, never maxed it out once during training.  Just cache everything (latents/embeds/etc) before training.  Which you should be doing anyway to save vram during training.\n\n3) PCIE speed also doesn't matter as much as people think. Going from x16 to x8 only decreases training speed a couple percent.  There isn't a large amount of data exchanged between gpus during training, assuming you wrote a good trainer.  PCIE latency is the real killer.\n\n4) RAM is awesome.  You can do so much more with more ram.  GPU offset to ram, keeping everything in ram, using ram to write temp files to, EMA can done on cpu, etc.  Having more ram makes everything easier.  I just upgraded to 1TB for dataset curation use.\n\n5) HDD space is also awesome.  I have \\~400TB, it's not enough.  Just being able keeping a copy of fully prepped, ,ready to train, cached dataset on disk with the original data is so nice.   Then you can store your embeddings, and whatever else on disk with the original data.   \n  \nRam and disk space used properly can reduce vram usage during training, and significantly speed up training, especially for multi-run training.",
          "score": 2,
          "created_utc": "2026-02-18 09:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61ufrc",
              "author": "shiftyleprechaun",
              "text": "Thanks for the feedback! A lot of good info here for me to take in, I appreciate the time you took.\n\nI haven't began any training phases yet. I have just been analyzing massive amounts of company data to build into a combo of duckdb, neo4j and embedding graph. I will deploy a chatbot integrated into teams for the users to start using and store all of the query info so I can use that info to them finetune the models for better output.\n\nThe training aspect, is something I really have zero experience with, so I will learn about it while I am attempting to build out that pipeline, but I am still a bit away from that point.\n\nAnother commenter posted a good video about pcie retimer/Redriver cards, honestly was the first time I heard about them. I have to look into it more, from what I gathered they are pretty pricy, but at this point, if it will improve things, then I will make the investment. I have to get a better/larger open air rig, bc the one I have is too small.\n\nSo CPU I can leave as is then , I mean it's not the top of the line, but it's up there.\n\nDo you think I should get more ram? The prices of course are out of control. I initially bought 128 GB for 1600, then decided a week later I wanted 256 total, so when I went to buy another 128 gb kit the price was already up to $2k and when I check yesterday that same kit was now $3k. Nuts!\n\nI only have the 3 nvme ssds. I am storing all of the models on the 9100 and everything else on the 990 pros. I was thinking about getting some mechanical sata HDDs for long term storage and backups maybe 8-10TB.\n\nThanks again. I also do not yet know how to optimize/comfigure the HDD and the ram as per your recommendations, but I will add all of this to the seemingly endless list of things I need to figure out - hah. \n\nThanks again! \n\nBtw - I ended up adding a 3rd blackwell. So now I have 3x 6000 pro Blackwells and 4 x 3090 tis. All 7 pcie slots are used.",
              "score": 1,
              "created_utc": "2026-02-18 13:38:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o62vdlw",
                  "author": "Aware_Photograph_585",
                  "text": "I have a pretty similar open air rig.  Started with 2x 3090 and slowly built up from there.  \nI have zero experience with training LLMs, but I have written my own fully custom multi-gpu text-to-image trainers.  I assume my training experience with transfer over to what you're doing, but maybe LLM training has it's own quicks.\n\nPCIe retimer/redriver:   \nWhen I first started learning programming, I lost 6 months of training time due to pcie cables.  Gpu, fell off the bus, time outs, really strange training speeds (one run would be 2-3x the speed of the next)  Asked everyone everywhere, no one had any idea what was wrong.  Re-wrote my scripts, bought the most expensive cables I could find, thought I was going crazy, and nearly gave up.  Tried a retimer card, problem solved.  I learned a lot about multi-gpu training during that time, but that's not the best way to learn.  I also burnt the retimer chip on my first card, so use fans on your retimer/redrivers.   \n  \nFor me, retimer/redriver card is non-optional for training.  I won't even consider training without one.  If an problem shows up, how do I know it's not the cables?  I bought my cards for \\~$150 each.  They do x16 or 2 x8.  Make sure you ask the seller if they support 2x 8/x16.  A lot of cards support 4 x4 by default, and the seller needs to flash the card to support 2x 8/x16. You'll also need the cables and the pcie boards.\n\nAlso, pcie cables seem fine for inference.  I never had a problem with inference when using pcie cables.\n\nRam:  \nRam prices are stupid, I know.  I just bought 1TB DDR4 2666mhz ram for $3800, I think I did OK for current market.  3200mhz would have been double the price, I don't even want to think about what ddr5 costs.  The 1tb ram is to be used for the image dataset sorting: embedding clustering and such.  \n  \nRam really benefits for fast gpus with low vram.  When I used 2x 3090 to train the sdxl unet, I had to use FSDP GRAD\\_SHARD\\_OP to get the model to fit into memory.  But there still wasn't enough vram room for a large enough batch size to saturate the gpu core.  So I used just enough ram via cpu\\_offset to saturate the core.  I lost some speed due to the cpu\\_offset, but I gained more speed via  increased batch size for core utilization.  Ram will help your 3090s more than the pro 6000.\n\nStandard training advice is ram = 2x total vram, and on my two 3090s, I was using \\~100GB ram during training.  A lot of the open source trainers focus on being conservative with vram/ram, so it won't matter as much.  But if you're writing your own code, there is just so much you can do with more ram: holding extra models, prefetch batches, ema, etc.    \n  \nFor standard consumer gpus, I highly suggest 2x total training gpu vram.  But maybe LLM training can use less?  And the pro6000s aren't vram starved. So, who knows? Regardless, 256 is enough to get started.    \n  \nFor ram (and vram): size is more important than speed.  Speed is nice, but if you don't have enough size, you're screwed.  So if you see a good deal on slightly slower speed, but a larger amount, buy it.\n\nHDDs:  \nI do blocks of 8 HDDs.  Most motherboards have 8 sata ports, HBA cards have 8/16/24, so 8 is a good number.  ZFS is the file system of choice, it's near impossible to lose your data.  I use raidz2 with a L2ARC metadata cache partition on a cheap sdd.  The ssd cache significantly speeds up the HDDs. It's fast enough for text-to-image training, don't know about LLM.  I buy used enterprise HDDs, no problems so far. Buy from a reputable seller and you're good.\n\nYou're cpu is most likely fine, unless LLM training has crazy cpu requirements.  Ram and HDDs will benefit you more.\n\nAlso, consider what you want to do with the pro6000s & 3090s.  Should they really be in the same machine?  I can't imagine they're be used together, unless you're running some huge model just for fun.  Maybe buy an old AMD 7002/7003 board and split off the 3090s?  \n  \nI will use my pro6000s for training with a 4090 48GB for support (validation/ema/etc) and inference. All the other gpus will be used for dataset curation.  So I'm considering moving 2x 4090 48GB and 2x 3090 to the data server, and leaving the 2x pro6000 and a 4090 48GB in the training server.",
                  "score": 1,
                  "created_utc": "2026-02-18 16:37:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dv7np",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-14 19:01:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eux2w",
              "author": "shiftyleprechaun",
              "text": "I definitely will.",
              "score": 2,
              "created_utc": "2026-02-14 22:13:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dve5w",
          "author": "chafey",
          "text": "what frame and motherboard are you using?  I have a ASUS WRX90E-SAGE Pro WS SE AMD sTR5 EEB Motherboard and am having trouble finding a frame which can hold the EEB motherboard",
          "score": 1,
          "created_utc": "2026-02-14 19:02:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dzm1b",
              "author": "shiftyleprechaun",
              "text": "I ended up settling with this \"mining frame\" from amazon, it's not perfect, but it works. \n\nhttps://a.co/d/0ard2K7F\n\nI have the same.motherboard and it doesn't fit perfectly with the predrilled holes, so I aligned it best I could with the pre-existing hole and only had to drill a few.\n\nThe two PSUs are abble to (just barely) fit on each side of the mb.",
              "score": 2,
              "created_utc": "2026-02-14 19:24:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e135s",
          "author": "Citizen_Edz",
          "text": "Very cool! Few 3090tis that i can see, what are the other cards? ",
          "score": 1,
          "created_utc": "2026-02-14 19:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5es05w",
              "author": "shiftyleprechaun",
              "text": "I bought LINKUP PCIE 5.0 Riser Cable | for Vertical GPU Mount.\n\nI bought a few 30 cm and a few 60cm.\n\nThese are a bit pricey, but they are compatible with pcie 4.0 as well.",
              "score": 1,
              "created_utc": "2026-02-14 21:57:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5etnxx",
              "author": "shiftyleprechaun",
              "text": "2x6000 pro maxwell & 4x3090ti",
              "score": 1,
              "created_utc": "2026-02-14 22:06:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5peamy",
                  "author": "Citizen_Edz",
                  "text": "Awsome! Very cool setup",
                  "score": 2,
                  "created_utc": "2026-02-16 16:19:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ecmbz",
          "author": "Proteus356",
          "text": "Iâ€™ve thought about using old mining rigs for this but all the risers are 1x, which isnâ€™t effective for LLM? How are you getting x16 or at least x8 connectivity to those GPUs?",
          "score": 1,
          "created_utc": "2026-02-14 20:33:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5emf9s",
              "author": "Brilliant-Suspect433",
              "text": "From what i can see, he didnt use risers but extension cablesâ€¦",
              "score": 1,
              "created_utc": "2026-02-14 21:27:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eswc9",
                  "author": "shiftyleprechaun",
                  "text": "No I used risers: LINKUP PCIE 5.0 Riser Cable | for Vertical GPU Mount.\n\nCombo of 30 & 60 cm.\n\nI realized today that I need to move the last 3090 into slot 7, rather than 6.\n\nhttps://preview.redd.it/1tgi89f49jjg1.jpeg?width=1034&format=pjpg&auto=webp&s=0fb36e7f733fa61358a83dbc9ac69f51e22a865b",
                  "score": 1,
                  "created_utc": "2026-02-14 22:02:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5etayx",
              "author": "shiftyleprechaun",
              "text": "Also, nothing I bought has been used for mining. The only used components are the 3090s , which of course I cannot guarantee weren't used for mining, but based on when I met each person (buying them off of FB marketplace), they all told me they wernt using for mining, just gaming or photo/video and they upgraded to 5090.\n\nAside from the 3090s everything is brand new and assembled together myself.",
              "score": 1,
              "created_utc": "2026-02-14 22:04:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5eedx0",
          "author": "Weird-Abalone-1910",
          "text": "What did this cost you to put together? I want to build one to try to run kimi2.5",
          "score": 1,
          "created_utc": "2026-02-14 20:43:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etgzn",
              "author": "shiftyleprechaun",
              "text": "Total all in, including new dedicated electrical circuits installed, around $50k.",
              "score": 1,
              "created_utc": "2026-02-14 22:05:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5eu9ag",
                  "author": "Weird-Abalone-1910",
                  "text": "Wow, thanks for sharing.",
                  "score": 2,
                  "created_utc": "2026-02-14 22:10:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5esbr3",
          "author": "ApprehensiveView2003",
          "text": "what did you do to fix when it originally doesnt allow P2P?",
          "score": 1,
          "created_utc": "2026-02-14 21:59:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eufa7",
              "author": "shiftyleprechaun",
              "text": "Enable IOMMU (SVM Mode), set PCIe ACS override, disable ASPM/SR-IOV/C-States, force PCIe Gen 4, and if you're still having issues add pcie_acs_override=downstream,multifunction to your kernel boot parameters. The WRX90 workstation boards need all of this dialed in for clean multi-GPU P2P.",
              "score": 1,
              "created_utc": "2026-02-14 22:11:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ew17n",
          "author": "Correct_Lead_2418",
          "text": "Can I ask your budget for this? For 10k you could get the highest tier Mac studio with 512GB unified memory and 4tb storage",
          "score": 1,
          "created_utc": "2026-02-14 22:20:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f3m5w",
              "author": "shiftyleprechaun",
              "text": "I spent around $50k. But there is no way a mac with 512gb unified memory could do what this can. I can also continue to add to this as needed. Not so easy with Mac.\n\nAlso I hate apple and macs ðŸ˜‰.",
              "score": 1,
              "created_utc": "2026-02-14 23:04:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g7w46",
                  "author": "lenjet",
                  "text": "For that kind of money you could have got circa 8-10 DGX Spark or an OEM and ended up with 1024-1280gb of unified VRAMâ€¦ coupled with the smaller footprint and much lower power consumption.\n\nDid you consider that?",
                  "score": 1,
                  "created_utc": "2026-02-15 03:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5g1tlg",
          "author": "Prigozhin2023",
          "text": "Is it cheaper than 2 x Dgx Spark?",
          "score": 1,
          "created_utc": "2026-02-15 02:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g2ce3",
              "author": "shiftyleprechaun",
              "text": "What I built? No.",
              "score": 1,
              "created_utc": "2026-02-15 02:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g357d",
                  "author": "Prigozhin2023",
                  "text": "Trying got understand which is the more cost effective build. One like yours, vs DGX Spark vs Mac Studio.",
                  "score": 1,
                  "created_utc": "2026-02-15 02:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gbrpv",
          "author": "Big_Championship1291",
          "text": "I know nothing about building an ai workstation and Iâ€™m planning on building my own. What is the highest model you can use with this station with reasonable speed for heavy software development?",
          "score": 1,
          "created_utc": "2026-02-15 03:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gdn91",
              "author": "shiftyleprechaun",
              "text": "With what I built?",
              "score": 1,
              "created_utc": "2026-02-15 04:07:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ge92n",
                  "author": "Big_Championship1291",
                  "text": "Yes",
                  "score": 1,
                  "created_utc": "2026-02-15 04:12:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jpftt",
          "author": "kidflashonnikes",
          "text": "something to consider: with this motherboard, if you want to expand compute, get PCIe lane spitters, going from 16x per one lane (exluding the one pcie 8x lane on this motherboard) and split every 16x pcie gen 5 lane in pcie 8x, which will double your compute. I currently use 4 RTX PRO 6000s, and I am getting another 8 RTX PROs delivered in the next few weeks, and will be doing this. ",
          "score": 1,
          "created_utc": "2026-02-15 18:17:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jyhk4",
              "author": "shiftyleprechaun",
              "text": "And by doing this, you find it running faster? What are you typically doing? Training?",
              "score": 1,
              "created_utc": "2026-02-15 19:01:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jza42",
                  "author": "kidflashonnikes",
                  "text": "I run a team at one of the worlds largest privately funded AI labs. My team works on inserting BCI (brain computer interface) threads into live test subjects (willingly of course) with brain damage to compress brainvwave data in real time via LLMs due to the data being too large and variate with current software ect. I work on personal compute tasks. One of my personal projects is using AI to predict bad thoughts/crime thoughts. I am currently using my set up for trainining and inference. I am almost done with my research, effectively I need more training compute to complete the version 1 of the model where I will be able to successfully predict negative behaviour of humans based on AI throughput. Some people will hate me for doing this, I dont care, I need to feed my family and you would not believe the prices certain US companies are willing to pay for this new LLM ",
                  "score": 2,
                  "created_utc": "2026-02-15 19:05:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5jzsqx",
                  "author": "kidflashonnikes",
                  "text": "as for the motherboard - you can split the 16x lanes into 8x lanes (2 each from 1 16x lane), to double your compute. Dropping from 16x to 8x does not impact inference that much - it does impact training substantially more. My current set up (all housed in a phanteks serve rpro 2 TG case): 1 TB of RAM, 16 TB of NVMe, 96 core CPU (thread ripper pro), 2000 Watt PSU, AIO for the CPU, regular fans to for cooling, 4 RTX PRO 6000s (6 more on the way before I transition to an open rig case), Linux (ubuntu)",
                  "score": 2,
                  "created_utc": "2026-02-15 19:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mwytb",
          "author": "Common_Heron4002",
          "text": "Gotta ask, what do you do to ty all the GPU's together? allow the sharing of information of them or how to utilize them effectively? \n\n",
          "score": 1,
          "created_utc": "2026-02-16 05:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nyrhl",
              "author": "shiftyleprechaun",
              "text": "So far I've been running models independently on each one.\n\nI've tested a bit spreading llama 3 across the 4 3090s, but I haven't had to run anything larger (yet) than what can fit on a single 6000 pro.",
              "score": 1,
              "created_utc": "2026-02-16 11:17:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ogm03",
                  "author": "Common_Heron4002",
                  "text": "interesting so do you do anything with tying each model to help with processing or getting better results or you just one per type of process your running etc? \n\nI didnt think you could spread a model across multiple in vram...\n\n",
                  "score": 2,
                  "created_utc": "2026-02-16 13:27:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o673tyl",
          "author": "marcus_Yan",
          "text": "![gif](giphy|axu6dFuca4HKM)",
          "score": 1,
          "created_utc": "2026-02-19 06:00:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gfbve",
          "author": "Prize_Wolf_5859",
          "text": "Iâ€™m new to this AI game Brodie how you built this and how much did it cost",
          "score": 0,
          "created_utc": "2026-02-15 04:20:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r90rxi",
      "title": "How much was OpenClaw actually sold to OpenAI for? $1B?? Can that even be justified?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/p8c453eapgkg1.jpeg",
      "author": "Alert_Efficiency_627",
      "created_utc": "2026-02-19 14:34:16",
      "score": 137,
      "num_comments": 61,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r90rxi/how_much_was_openclaw_actually_sold_to_openai_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o68xq98",
          "author": "Theseus_Employee",
          "text": "They didnâ€™t pay anything for it. They just hired the dude that made it, and are sponsoring the free open-source project OpenClaw.\n\nThe tweet is just a joke of people just inflating how much money you can make from vibe coded projects.",
          "score": 176,
          "created_utc": "2026-02-19 14:40:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aewgr",
              "author": "ArcticCelt",
              "text": "Also, people talk about it as a \"vibe-coded\" project, but they fail to mention that even if he used AI, it was done by a programmer with over 20 years of experience who sold his previous software startup for $100 million. It is not some random person with no experience.",
              "score": 22,
              "created_utc": "2026-02-19 18:57:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6aykqj",
                  "author": "huzbum",
                  "text": "Yeah, but he clearly did NOT put his engineering skills to work in there... glaring security flaws.  ",
                  "score": 7,
                  "created_utc": "2026-02-19 20:32:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cbn97",
                  "author": "oompa_loompa0",
                  "text": "Yeah. Watch the interview on Lex Friedman. Peter is no vibe coder, he is a brilliant seasoned engineer with crazy depth and breadth.",
                  "score": 4,
                  "created_utc": "2026-02-20 00:55:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cu2v7",
                  "author": "ANTIVNTIANTI",
                  "text": "it feels all very muchâ€”contrived ",
                  "score": 1,
                  "created_utc": "2026-02-20 02:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ae7xo",
              "author": "structured_flow",
              "text": "He had meetings with Zuck, same guy who is paying 8 figure salaries for their ai leadership positions, and turned Meta down. He absolutely was paid money how much donâ€™t know, but your mistaken if you think there wasnâ€™t a bid.",
              "score": 6,
              "created_utc": "2026-02-19 18:54:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bqhgu",
                  "author": "cjc4096",
                  "text": "He already had a successful exit.   He has the ability to choose what is important to him.",
                  "score": 2,
                  "created_utc": "2026-02-19 22:52:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o69j1ku",
              "author": "HeinerWersenberg",
              "text": "$1B for an unhinged AI-Agent-VibeCoded something did sound ridiculous indeed. However, when thinking of the competition, investments done already in the field, and considering the reasons for current memory and storage prices... who knows. Anything is thinkable theses days.    \nSo, thanks for the clarification. ",
              "score": 4,
              "created_utc": "2026-02-19 16:26:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69qiln",
              "author": "HappyContact6301",
              "text": "Look at Scale AI...",
              "score": 1,
              "created_utc": "2026-02-19 17:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bwdvj",
                  "author": "bronfmanhigh",
                  "text": "scale AI was still a real business with real employees and revenue end of the day lol",
                  "score": 2,
                  "created_utc": "2026-02-19 23:26:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6a1yfu",
              "author": "Demiansmark",
              "text": "Like the guy, like, like, like the guy with the $100B startup is going to pay attention to the subtext. Come on!",
              "score": 1,
              "created_utc": "2026-02-19 17:57:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69ufe3",
          "author": "reb00tmaster",
          "text": "This tweet alone is worth $80B.  In a few months it will be acquired for $160B. /s",
          "score": 32,
          "created_utc": "2026-02-19 17:21:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a6zme",
              "author": "MarkoMarjamaa",
              "text": "It could be turned into vibe-coded-NTF, then it's like $300B!",
              "score": 6,
              "created_utc": "2026-02-19 18:20:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6agkqz",
              "author": "eli_pizza",
              "text": "Unfortunately the \"/s\" is clearly not going to be enough for some people",
              "score": 2,
              "created_utc": "2026-02-19 19:05:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69vzrb",
          "author": "sleepy_roger",
          "text": "The crazy thing is how shitty openclaw is. Seems like most people hyped (besides the crypto grifters who you never trust..) had never used a harness and were just larping the entire time.. \n\nCodex/claudecode/droid/opencode provide a much better experience overall.. openclaw isn't even tailored to non tech people. The only thing it really added that made the masses adopt was easy integration into existing chat platforms.",
          "score": 25,
          "created_utc": "2026-02-19 17:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a77wi",
              "author": "MarkoMarjamaa",
              "text": "Worlds fastest growing malware distribution system. ",
              "score": 19,
              "created_utc": "2026-02-19 18:21:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a8v57",
                  "author": "sleepy_roger",
                  "text": "haha yeah for sure.. I kept mine pretty limited, running in a proxmox container with almost no access. Is it cool to talk to it via discord, sure I guess.. but another big issue is the limited context you see as a user within openclaw versus other agent harnesses... not to mention the entire thing looks vibe coded (because it is).",
                  "score": 3,
                  "created_utc": "2026-02-19 18:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6adfj8",
              "author": "crypticG00se",
              "text": "You are missing the main point why these systems are attractive to openai and the like. Sells more tokens. Perfect system to fool non-tech people. Go look at the claw subreddit how people are complaining about model costs with claw. ",
              "score": 9,
              "created_utc": "2026-02-19 18:50:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b3bw1",
                  "author": "sleepy_roger",
                  "text": "Yeah that's a great point, I'm on the $200 max plan and it was the only time I got close to hitting my 5 hour limit... and it seemed like it was actually doing _much_ less than what I do within claude code directly.",
                  "score": 2,
                  "created_utc": "2026-02-19 20:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bu267",
              "author": "parapa-papapa",
              "text": ">Codex/claudecode/droid/opencode provide a much better experience overall.\n\n  \nI agree that OpenClaw sucks, but what is the competition? \n\nIt is insecure, yes. And that's why the big providers aren't touching such systems, but it's MEANT to be insecure. Like, you can't have a useful LL.M based assistant that is safe. ",
              "score": 1,
              "created_utc": "2026-02-19 23:13:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cy6jl",
                  "author": "Anarchaotic",
                  "text": "Claude on desktop is pretty good - not nearly as \"powerful\" as OpenClaw with no restrictions, but I can use it to do a lot of useful things by giving shell access with approvals on commands.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69ffgu",
          "author": "TimChr78",
          "text": "OpenClaw wasnâ€™t sold to OpenAI at all - they hired the creator Peter Steinberger. OpenClaw is open source under the GNU 3.0 license.\n\nAnd no OpenAI is definitely not paying Peter Steinberger 1B.",
          "score": 18,
          "created_utc": "2026-02-19 16:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69qkv2",
              "author": "ptear",
              "text": "I mean, instant billionaire status would be impressive.",
              "score": 3,
              "created_utc": "2026-02-19 17:02:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cvjcw",
                  "author": "ANTIVNTIANTI",
                  "text": "it's all contrived. this shit was to boost the doubters back into believers again, lolol, I saw the reddit posts go from the actual reality of the situation after so many months/years of the hype hyping away so much hype, was finally starting to chill like 1-2% lol, then OpenClaw and MoldBook or bot or wtf ever the name was, lololol. BLEW IT BACK UP. Now all we are going to hear for the forevers is \"BRUH YOU'RE LIKE, 14 MONTHS BEHIND BRUH?!\"\n\n\"SKILL ISSUE BRUH?! STOP BEING 14 MONTHS BEHIND BRUH?!\"\n\n\"IF YOU DON'T SUCK MOLDYBLOCKS CLAW YOU'RE LIKE, 16 MONTHS BEHIND BRUH?!\" ",
                  "score": 1,
                  "created_utc": "2026-02-20 02:57:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6az013",
          "author": "huzbum",
          "text": "OpenAI hired the creator, they didn't buy a platform.  It makes a lot of sense when you consider it was promoting burning Anthropic tokens and now it will promote burning OpenAI tokens.  ",
          "score": 4,
          "created_utc": "2026-02-19 20:34:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvoxg",
              "author": "ANTIVNTIANTI",
              "text": "honestly dude was playing it like both corps paid him, he would say \"Half was done on Claude\" then \"Codex coded it\" which... yeah... I'm full of conspiracy today, ignore me :P",
              "score": 2,
              "created_utc": "2026-02-20 02:58:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d7s4g",
                  "author": "huzbum",
                  "text": "I hadn't paid enough attention to call you right or wrong, but it seems like the smart play to make nice with both of them for just this kind of eventuality.  \n\nMy comments were based on the original name being Clawed Bot and the recommendation to use Opus.  \n\nBut yeah, anything that gets more people addicted to burning more tokens and trusting LLMs with more data and responsibility is good for both of them.  ",
                  "score": 1,
                  "created_utc": "2026-02-20 04:18:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6a8qqn",
          "author": "shryke12",
          "text": "The dollar amounts being thrown around by these companies are ludicrous lol.  Shows just how many dollars our government has been printing.",
          "score": 3,
          "created_utc": "2026-02-19 18:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69vnda",
          "author": "locomocopoco",
          "text": "Dude is not just a vibecoder. Go see what he has done previously. SMH. ",
          "score": 7,
          "created_utc": "2026-02-19 17:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvu76",
              "author": "ANTIVNTIANTI",
              "text": "lol I know, he's a grifter, he's got this damn WUNDERKID Blog, errr, WUNDERMAN! Vunder? vvvvuuuunnnder? Anyways. It pissed me off. I wanted him to be a dipshit so bad. LOLOLOLOLOL :P XDXDXD",
              "score": 1,
              "created_utc": "2026-02-20 02:59:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ao0n7",
          "author": "type102",
          "text": "They can justify it by saying anything while being in a bubble.",
          "score": 2,
          "created_utc": "2026-02-19 19:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6arz6x",
          "author": "Baader-Meinhof",
          "text": "You missed the joke. The actual figure seems to be that he was hired for $30M.",
          "score": 2,
          "created_utc": "2026-02-19 20:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvz4e",
              "author": "ANTIVNTIANTI",
              "text": "also the tweet kept raising the billions, seemed like, hard to miss. LOL",
              "score": 2,
              "created_utc": "2026-02-20 03:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68zq5m",
          "author": "leonbollerup",
          "text": "where does idea that they paid for it come from ?.. is there any actual proff ?",
          "score": 2,
          "created_utc": "2026-02-19 14:50:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69wa86",
              "author": "oureux",
              "text": "No proff but we might be able to find some proof",
              "score": 1,
              "created_utc": "2026-02-19 17:29:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b50a6",
                  "author": "moderately-extremist",
                  "text": "lemme ask chatgpt...",
                  "score": 1,
                  "created_utc": "2026-02-19 21:04:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6brzzr",
                  "author": "leonbollerup",
                  "text": "Would love to see it",
                  "score": 1,
                  "created_utc": "2026-02-19 23:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6af2gm",
          "author": "structured_flow",
          "text": "Even if openai thought that he was a one trick pony and stuck them in a basement with a broken stapler like office spaceâ€¦ They still wouldâ€™ve paid him a lot of money because itâ€™s all about branding so dude absolutely got paid a lot of money. I highly doubt anything close to 1 billion but definitely in the millions.  \n\nItâ€™s verified that he met with Zuckerberg whoâ€™s paying eight figure salaries for their head of AI departments and he turned them down saying that he â€œ felt more lines with the missionâ€ at OpenAI.",
          "score": 1,
          "created_utc": "2026-02-19 18:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aieum",
          "author": "siegevjorn",
          "text": "They aqui-hired the dev if I understand it correctly.",
          "score": 1,
          "created_utc": "2026-02-19 19:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bzcjr",
          "author": "desexmachina",
          "text": "Manus was $2B, so they had to escalate and paid Pete $5B, kinda wild",
          "score": 1,
          "created_utc": "2026-02-19 23:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c98be",
          "author": "ithilelda",
          "text": "he might be paid in memory sticks or chatgpt subscription you know.",
          "score": 1,
          "created_utc": "2026-02-20 00:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cqh1v",
          "author": "No_Success3928",
          "text": "https://www.reddit.com/r/myclaw/s/NBiSm73M4p its an investment!",
          "score": 1,
          "created_utc": "2026-02-20 02:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cwmt8",
              "author": "ANTIVNTIANTI",
              "text": "Omfg I'd offer you my first born for showing me this, but wait.... that got weird, I meant....\n\nshit was hilarious. ",
              "score": 2,
              "created_utc": "2026-02-20 03:04:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cwzcn",
                  "author": "No_Success3928",
                  "text": "Crypto bro got a new AI game!",
                  "score": 1,
                  "created_utc": "2026-02-20 03:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e1lqu",
          "author": "satmun",
          "text": "I used to follow the works of (Peter Steinberger)Â of PSPDFKit before. He is deep into technical things. Probably his X(formerly Twitter) might still have his tweets. This app was named as best app in iOS for few years I think. He used to discuss Kernel level stuff to optmize rendering of PDFs if I remember correctly and well known in the circle of good devs, atleast in Austria I think. I remember him being close to another dev (professor) who is creator of game engine libgdx. I recently saw him dive deep into C++ and other internals of LLMs. So, its a talent pool and network of devs thats important I think. ",
          "score": 1,
          "created_utc": "2026-02-20 08:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o692nwj",
          "author": "Investolas",
          "text": "They paid that much for him to sign an nda and stop development.Â ",
          "score": 0,
          "created_utc": "2026-02-19 15:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o698mfd",
              "author": "Theseus_Employee",
              "text": "Well they didnâ€™t pay him enough apparently because heâ€™s still developing it.",
              "score": 6,
              "created_utc": "2026-02-19 15:36:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69j1e6",
                  "author": "Technical_Ad_440",
                  "text": "have they fixed the security vulnerabilities yet?",
                  "score": 4,
                  "created_utc": "2026-02-19 16:26:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o699t7k",
                  "author": "Investolas",
                  "text": "Sure but it will never be what it would have been without their intervention",
                  "score": -4,
                  "created_utc": "2026-02-19 15:41:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r659df",
      "title": "Qwen3.5 is released!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xtgnyvb2stjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-16 09:34:21",
      "score": 118,
      "num_comments": 18,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r659df/qwen35_is_released/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5rs0pq",
          "author": "DiligentRanger007",
          "text": "How much vram needed ???",
          "score": 3,
          "created_utc": "2026-02-16 23:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sqnpj",
              "author": "ubrtnk",
              "text": "Yes",
              "score": 16,
              "created_utc": "2026-02-17 02:35:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5t9j3m",
              "author": "yoracale",
              "text": "Depends on your ram. I'd say at least 16gb. See the guide: https://unsloth.ai/docs/models/qwen3.5",
              "score": 2,
              "created_utc": "2026-02-17 04:36:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ts1sf",
          "author": "pghqdev",
          "text": "what non-mac setup would be equivalent to 512 M3 Ultra config?",
          "score": 3,
          "created_utc": "2026-02-17 07:04:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66wckp",
              "author": "AppleBottmBeans",
              "text": "all of them",
              "score": 1,
              "created_utc": "2026-02-19 05:03:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67llea",
                  "author": "Eden1506",
                  "text": "Non, apple has a memory bandwidth of 800 gb/s on the ultra.  Even servers using 12 channel ddr5 don't reach those speeds. \n\nObviously there is overhead and you can't fully utilise the full speed you are still ahead of anything outside of gpus. ",
                  "score": 1,
                  "created_utc": "2026-02-19 08:39:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q7bfe",
          "author": "I_like_fragrances",
          "text": "I didn't know, just started downloading it now.",
          "score": 4,
          "created_utc": "2026-02-16 18:33:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qn83k",
              "author": "yoracale",
              "text": "Awesome, let us know how it goes!",
              "score": 1,
              "created_utc": "2026-02-16 19:48:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vx60z",
          "author": "phoenixfire425",
          "text": "Make sad sounds with 2 x RTX3090ti  Wish I could run this.  I love 2.5-coder, maybe a few weeks ill be able to get a version of this i can run on my hardware.",
          "score": 1,
          "created_utc": "2026-02-17 16:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69gtlf",
              "author": "emrbyrktr",
              "text": "Did you try Qwen3 Coder Next?",
              "score": 1,
              "created_utc": "2026-02-19 16:15:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o61kcik",
          "author": "barkdender",
          "text": "Can I even use the 1 bit quantized version on my RTX 3080 12 GB without offloading or am I in for a bad time? ",
          "score": 1,
          "created_utc": "2026-02-18 12:39:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61uwbj",
              "author": "yoracale",
              "text": "Wil work but will be extremely slow, how much RAM do you have?",
              "score": 1,
              "created_utc": "2026-02-18 13:41:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o623u9s",
                  "author": "barkdender",
                  "text": "I had 256gb but sold some because well to offset cost. Down to 64gb",
                  "score": 1,
                  "created_utc": "2026-02-18 14:28:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62phc0",
          "author": "slyticoon",
          "text": "Why did we pick 3 shades of grey for the comparisons models...",
          "score": 1,
          "created_utc": "2026-02-18 16:10:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6e4h4",
      "title": "Anyone else spending more time tweaking than actually using their model?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r6e4h4/anyone_else_spending_more_time_tweaking_than/",
      "author": "Weirdboy212",
      "created_utc": "2026-02-16 16:24:28",
      "score": 81,
      "num_comments": 43,
      "upvote_ratio": 0.97,
      "text": "I swear Iâ€™ve spent 10x more time:  \n\\-comparing quants  \n\\-adjusting context size  \n\\-testing different system prompts  \n\\-watching tokens/sec\n\nthan actually asking it useful questions\n\nFeels like building a gaming PC and then only running benchmarks",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6e4h4/anyone_else_spending_more_time_tweaking_than/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5prvku",
          "author": "beisenhauer",
          "text": "\"Give me six hours to get a useful answer from an LLM, and I'll spend the first four tuning the model.\" -- Abraham Lincoln",
          "score": 20,
          "created_utc": "2026-02-16 17:22:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t7ynj",
              "author": "ANTIVNTIANTI",
              "text": "Ummmm Aktuallly I believe that was Thomas Jefferson",
              "score": 3,
              "created_utc": "2026-02-17 04:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5u77ri",
                  "author": "singh_taranjeet",
                  "text": "Common misconception. Jefferson preferred longer context windows but worse latency. Lincoln was more of a prompt engineer.\n\nBoth terrible at quant selection though...",
                  "score": 5,
                  "created_utc": "2026-02-17 09:27:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5t7z4z",
                  "author": "ANTIVNTIANTI",
                  "text": ":D",
                  "score": 2,
                  "created_utc": "2026-02-17 04:25:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5zn1od",
              "author": "No-Television-7862",
              "text": "\"The last 20% takes longer than the first 80%.\"",
              "score": 2,
              "created_utc": "2026-02-18 03:24:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pfyvq",
          "author": "Medium_Chemist_4032",
          "text": "Oh for sure. Plus recompiliing llama.cpp and ik\\_llama. \n\nRecently it has been more: \"why tool calling breaks after few times\" often, so I guess... progress?",
          "score": 11,
          "created_utc": "2026-02-16 16:27:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pl0km",
          "author": "Look_0ver_There",
          "text": "Having a 128GB shared memory machine, and with the recent release of all these models in the 190-240B parameter range has made searching for that elusive \"quantization that isn't brain-dead and fits\" take a massive chunk of my time.  Every time I think my task is done, another model drops and the search continues...",
          "score": 8,
          "created_utc": "2026-02-16 16:50:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pocs4",
              "author": "Hector_Rvkp",
              "text": "I was assuming / hoping that 128gb is actually the sweet spot to get MoE models that sound intelligent enough to be useful and fast enough to be usable. Is that not the case? What are you trying to achieve? I'm considering buying a Strix halo w 128gb ram.",
              "score": 3,
              "created_utc": "2026-02-16 17:05:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5prb3w",
                  "author": "Look_0ver_There",
                  "text": "It is. Qwen3-Coder-Next, which is 80B, fits fine at full Q8, and you can run it at full context at ~35tg/sec.  Speeds go up if you use a smaller quantization but there's no need to sacrifice quality.\n\nI've been messing about with Step-3.5-Flash and MiniMax-M2.5 though, which are 196B and 229B respectively.  Both fit okay with an IQ3 based quantization, but Step seems to suffer a fair bit at that level, while MiniMax however seems to be just fine, or at least, it's hasn't really put a foot wrong in the coding tasks I've asked of it.\n\nBasically I'm refining an IQ3 quant of MiniMax, as well as tuning the VM parameters of the kernel, to get it running as smooth as possible.  It seems to be ticking along at 30tg/sec as of this morning, and it generally emits better quality output that works the first or second time, whereas Qwen Coder Next makes a lot of small mistakes that I really need to keep an eye on.\n\nI also found out today how to fix a chat template bug that was in the original release of MiniMax, and I now have it working really well (again, as if today).",
                  "score": 3,
                  "created_utc": "2026-02-16 17:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pgatg",
          "author": "iz_raymond",
          "text": "Haha that hits home. Honestly, I just want my AI to not sound corporate ðŸ˜” all of them sounds like that, I just want an AI that's unhinged like Grok or at least \"human-like\" Gemini",
          "score": 8,
          "created_utc": "2026-02-16 16:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pmcm8",
              "author": "GetMeThePresident",
              "text": "any luck or tips at getting closer to that?",
              "score": 3,
              "created_utc": "2026-02-16 16:56:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5w50yw",
                  "author": "iz_raymond",
                  "text": "Nope, tried Qwen,Mistral,Deepseek,Llama..all still sound very corporated. But I stick to Qwen for a bit more balance than the rest",
                  "score": 2,
                  "created_utc": "2026-02-17 16:50:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5sexpp",
              "author": "sgamer",
              "text": "go on the character card sites and look for assistant characters, you can add a lot of flavor with just frontloading a json onto chat mode",
              "score": 2,
              "created_utc": "2026-02-17 01:24:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t86yy",
                  "author": "ANTIVNTIANTI",
                  "text": "Oh yeah I 2nd this! SO much can be done just in the sys prompt! Also prompt/write intentionallyâ€”like, really recognize the mirroring effect or how you lead the models by way of your own words, also I'm not sure if I'm replying to the right person to make it so that I appear replying to both of you LOLOL",
                  "score": 2,
                  "created_utc": "2026-02-17 04:27:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5phps1",
          "author": "ptear",
          "text": "Yes, I'm still searching for the most valuable local uses that actually are worth it,Â other than write python script to solve this problem (which I don't end up doing with a local model because I have midrange hardware).",
          "score": 3,
          "created_utc": "2026-02-16 16:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pghpc",
          "author": "sn2006gy",
          "text": "Yeah...\n\nReality is, the real work is being done with layers and models of models/retrievers/planers/routers like:\n\nUser Input  \nÂ  â†“  \nRetriever (patterns, code, history, embeddings)  \nÂ  â†“  \nPlanner / Router  \nÂ  â†“  \nLLM (reasoning)  \nÂ  â†“  \nTool Calls (search, code execution, APIs)  \nÂ  â†“  \nEvaluator / Critic  \nÂ  â†“  \nFinal Output\n\n\n\nThat's why claude code is kicking the butt of whatever isolated model we kick the tires on... but also why claude is so fragile vs competition because it's not rocket science to build this onion layer pattern with tooling.",
          "score": 3,
          "created_utc": "2026-02-16 16:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5po44m",
              "author": "SubstantialPoet8468",
              "text": "Is this akin to the â€œarrâ€ stack for media, compared to say something like Netflix?",
              "score": 1,
              "created_utc": "2026-02-16 17:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5qqiyv",
              "author": "StaysAwakeAllWeek",
              "text": ">it's not rocket science to build this onion layer pattern with tooling.\n\nIt definitely is rocket science to do it profitably though. You need highly optimised models designed specifically for being stacked like this if you ever want to break even",
              "score": 1,
              "created_utc": "2026-02-16 20:04:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5r54ml",
                  "author": "sn2006gy",
                  "text": "It's why claude is going all in on development... but it's not hard to pull OSS models together to build something similar - my point is less about finding the perfect model and tinkering to infinity but about finding models that fit the retriver/reasoning/tool call/evaluator paradigm and if that's coding - fits in the current tooling that has that concept as part of its workflow.",
                  "score": 1,
                  "created_utc": "2026-02-16 21:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ryg0f",
              "author": "m94301",
              "text": "Is this the secret behind Claude?  It's so good, it's really the only one I pay for, but I've never tried to understand what's going on under the hood",
              "score": 1,
              "created_utc": "2026-02-16 23:48:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sr51e",
                  "author": "sn2006gy",
                  "text": "pretty much",
                  "score": 1,
                  "created_utc": "2026-02-17 02:37:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rs6wj",
          "author": "ElijahKay",
          "text": "Skyrim Modders.\n\n  \nFirst time meme.",
          "score": 2,
          "created_utc": "2026-02-16 23:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5plnbg",
          "author": "HomsarWasRight",
          "text": "Welcome to every â€œdo it yourselfâ€ technical hobby.",
          "score": 3,
          "created_utc": "2026-02-16 16:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pn66j",
          "author": "throwaway5006001",
          "text": "Yes downloaded tweaked many models but learned a lot using ollama, lm studio, koboldcpp, openclaw",
          "score": 1,
          "created_utc": "2026-02-16 17:00:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pnaf3",
          "author": "live4evrr",
          "text": "Spent several hours yesterday trying to play with llama.cpp settings to improve the minimax 2.5 performance. Itâ€™s not much difference from the dopamine reward as gaming.",
          "score": 1,
          "created_utc": "2026-02-16 17:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5prk3a",
          "author": "SKirby00",
          "text": "I've been working on building a utility that automatically does some of this stuff for me.\n\nFor context, I have multiple GPUs with varying amounts of VRAM, and I run models with SGLang to get the most performance. The idea is that I'll run this utility with a specified model name, and it:\n- Fetches the model's info from huggingface (in particular, the number of layers that I'll need to distribute across my GPUs)\n- Checks my current hardware to see how many GPUs are installed and how much VRAM is in each one\n- Runs an optimization script that:\n    - Gradually increases the memory allocation and context length until I start running into OOM errors\n    - Runs a hill-climbing algorithm on the distribution of model layers between GPUs to make sure I'm getting the most out of each one\n    - Once it's identified the maximum stable configuration, it pulls it back a bit to add some safety margin\n- Saves the discovered optimal configuration for my exact combination of hardware and model selection to a JSON config file that I can use with another script to run the server\n\nI'll still have to mess around with different prompts and prompt templates etc., but once this is working reliably it should significantly cut down on the time and energy that it takes to figure out exactly how hard I can push my system with a given model and how much context I can fit.",
          "score": 1,
          "created_utc": "2026-02-16 17:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ql006",
          "author": "AdOne8437",
          "text": "No. I mostly try a bit with a few testcases I have and then use for weeks and months the same models.",
          "score": 1,
          "created_utc": "2026-02-16 19:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qsud9",
          "author": "esmurf",
          "text": "Only in the beginning.Â ",
          "score": 1,
          "created_utc": "2026-02-16 20:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ruaub",
          "author": "steezy13312",
          "text": "I'm not alone!\n\nI think part of the challenge is being in the performance limbo of \"it's just passable enough to run what I need\" but at the end of the day not good enough to actually use for real work compared to cloud providers.",
          "score": 1,
          "created_utc": "2026-02-16 23:25:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t7pfw",
          "author": "ANTIVNTIANTI",
          "text": "lololololol it's why we do what we do tho",
          "score": 1,
          "created_utc": "2026-02-17 04:24:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t7rdp",
              "author": "ANTIVNTIANTI",
              "text": "we're inherent tweakers ",
              "score": 1,
              "created_utc": "2026-02-17 04:24:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tvvt5",
          "author": "Gargle-Loaf-Spunk",
          "text": "Wait you mean Iâ€™m supposed to use this crap?Â ",
          "score": 1,
          "created_utc": "2026-02-17 07:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ulp7q",
          "author": "Least-Platform-7648",
          "text": "Same here. Now I use agents to help me, e.g. Codex will try different llama.cpp settings and run benchmarks in a loop.",
          "score": 1,
          "created_utc": "2026-02-17 11:38:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wqbge",
          "author": "Ok-Measurement-1575",
          "text": "Yep because when the shit hits the fan, you gotta know you can rely on it.Â ",
          "score": 1,
          "created_utc": "2026-02-17 18:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zmqhg",
          "author": "No-Television-7862",
          "text": "Trying to get that custom open-weight llm just right takes time.",
          "score": 1,
          "created_utc": "2026-02-18 03:22:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rhhmu",
          "author": "tiga_94",
          "text": "Its not like local llms are really useful, so far I was unable to make any one of them to work as an agent, all fail to use simplest apis \n\nAnd then in chat mode they hallucinate too much\n\nModels in question are below 16 gigs",
          "score": 1,
          "created_utc": "2026-02-16 22:17:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rx38f",
              "author": "remainderrejoinder",
              "text": "You just didn't realize you should be asking it to hallucinate.",
              "score": 1,
              "created_utc": "2026-02-16 23:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sfzs4",
                  "author": "tiga_94",
                  "text": "I ask to open a file and output its content, se as using same agent with a cloud api \n\nCloud- does it, local - hallucinates \n\nGpt oss 20b q4 in question",
                  "score": 1,
                  "created_utc": "2026-02-17 01:30:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5s5csk",
              "author": "segmond",
              "text": "your lack of skills is your own problem.",
              "score": 1,
              "created_utc": "2026-02-17 00:28:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qme6p",
          "author": "Decaf_GT",
          "text": "Unpopular opinion: it's because most people don't actually know what they want a local LLM for, and those who do know that the local LLMs that they're actually *able* to run still don't compare to frontier cloud models. \n\nAs we gravitate further and further to MoE and go from \"no local no care\" to \"well, as long as its hosted in the US\" so that you can run GLM-5, we're going to watch local LLM usage collapse because the models that are worth running require hardware that is incredibly expensive, hardware that we *can't even get* anymore due to the RAM apocalypse.",
          "score": -1,
          "created_utc": "2026-02-16 19:44:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4vp1d",
      "title": "Best program and model to make this an actual 3d model?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/41y95nmiwijg1.png",
      "author": "Kolpus",
      "created_utc": "2026-02-14 20:56:37",
      "score": 54,
      "num_comments": 15,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4vp1d/best_program_and_model_to_make_this_an_actual_3d/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5euie8",
          "author": "chevellebro1",
          "text": "Hunyuan3D is the best local model Iâ€™ve found. I run using the template workflow in ComfyUI. If you want best quality, Iâ€™d recommend looking at Meshy.ai. The quality is impressive from their newer models",
          "score": 10,
          "created_utc": "2026-02-14 22:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gyxri",
              "author": "Dadda9088",
              "text": "Yeah, use an img2img model to get different views and hunyuan3d to make it in 3D is the way to go locally. The limitation could be missing textures and output format.",
              "score": 2,
              "created_utc": "2026-02-15 07:06:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5eitp3",
          "author": "sumane12",
          "text": "You tried the latest trellis?",
          "score": 5,
          "created_utc": "2026-02-14 21:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eljwl",
              "author": "o5mfiHTNsH748KVq",
              "text": "This is the answer\n\nhttps://microsoft.github.io/TRELLIS.2/",
              "score": 10,
              "created_utc": "2026-02-14 21:22:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fshzh",
                  "author": "o5mfiHTNsH748KVq",
                  "text": "for funsies, i put your source image into trellis\n\nhttps://preview.redd.it/hraavaeybkjg1.png?width=1684&format=png&auto=webp&s=27403c19c2b8a52ed6259611a6eff59d591fd6b7\n\nsettings can probably be tweaked",
                  "score": 7,
                  "created_utc": "2026-02-15 01:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ew2ko",
              "author": "Kolpus",
              "text": "Thank you I just tried it with default settings and result is way better than other models I tried. Will try again with maxed settings.\n\nFor other that are interested I used this easy installer: [https://github.com/IgorAherne/trellis-stable-projectorz/releases/tag/latest](https://github.com/IgorAherne/trellis-stable-projectorz/releases/tag/latest)\n\n",
              "score": 9,
              "created_utc": "2026-02-14 22:20:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ewasu",
                  "author": "runlinux",
                  "text": "Oooo? Iâ€™m interested in this too! Thanks for finding this.",
                  "score": 1,
                  "created_utc": "2026-02-14 22:21:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ehsc4",
          "author": "Dramatic_Entry_3830",
          "text": "I want to know too",
          "score": 3,
          "created_utc": "2026-02-14 21:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gryya",
          "author": "an80sPWNstar",
          "text": "Trellis 2 is the way. Be sure to run it in a repair app first to make sure all is well. There's a free one that's popular with the 3d modeling world",
          "score": 2,
          "created_utc": "2026-02-15 06:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f4rfq",
          "author": "KiwiNFLFan",
          "text": "Build it from scratch in Blender?",
          "score": 2,
          "created_utc": "2026-02-14 23:10:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f61mm",
              "author": "frogsarenottoads",
              "text": "https://preview.redd.it/dlee5c0rmjjg1.jpeg?width=236&format=pjpg&auto=webp&s=dcde1aa0f34cab3a20d8e506fb6937bae0ebc3b4\n\nFeel free to fill this in yourself I'm far too lazy",
              "score": 19,
              "created_utc": "2026-02-14 23:18:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5i9ug9",
              "author": "MixeroPL",
              "text": "Learn a new skill? Nah we got ai slop",
              "score": -2,
              "created_utc": "2026-02-15 13:55:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pgfu7",
          "author": "Kolpus",
          "text": "Just learned you can use the prompt \"show from an isometric angle\" to get an image well suited to turn into a 3d model. \n\nWould be even better if it could generate 2 isometric pictures from opposite angles but am not sure this is actually possible. Trellis can be fed multiple images from different angles.",
          "score": 1,
          "created_utc": "2026-02-16 16:29:23",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5sy4nw",
          "author": "Fuzzy-Rooster8548",
          "text": "this https://3d.hunyuan.tencent.com/. mesh and colors will be better than the rest. then you can reduce polycount with instantmeshes, or blender's decimation modifier. but the plants quality wont be too good. i would use ai to remove the plants, generate the model, and then add some actual modelled plants in the scene. but if it is for a top down game, model viewed at distance, just yolo it.",
          "score": 1,
          "created_utc": "2026-02-17 03:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6061nq",
          "author": "sectorix",
          "text": "if you are ok paying for api calls, i would do this:   \n1. ask nano-banana-pro to generate more angles of this model,   \n2. ask rodin to make it into a 3d model.  \nboth available pay-as-you-go on replicate",
          "score": 1,
          "created_utc": "2026-02-18 05:31:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3u81s",
      "title": "GLM5",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/1p89zxhfjajg1.png",
      "author": "I_like_fragrances",
      "created_utc": "2026-02-13 16:45:37",
      "score": 50,
      "num_comments": 15,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r3u81s/glm5/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o58vv89",
          "author": "FuzzeWuzze",
          "text": "Lol i want to meet the chad downloading a 1.51 Terabyte file from HF.\n\nOnly to get to the end get a checksum error...",
          "score": 19,
          "created_utc": "2026-02-13 22:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nl7u6",
              "author": "diffore",
              "text": "Nvidia could have made a \\~50  5090 for us to play.... but instead theme gigabytes of vram are now sitting in some server closet, spinning the BF16 version of GLM5. Yeah, still have hard feelings about the consumer market suffering from the AI boom. ",
              "score": 2,
              "created_utc": "2026-02-16 09:11:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q8gul",
                  "author": "I_like_fragrances",
                  "text": "It has gotten bad, especially when just a GPU for gaming is becoming inaccessible to people outside the AI community.",
                  "score": 2,
                  "created_utc": "2026-02-16 18:38:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d0zma",
              "author": "Aggressive-Fan-3473",
              "text": "I believe HF uses tls for model downloads so that wouldnâ€™t happen. The tls would not be able to decrypt and would request the corrupted packet again",
              "score": 1,
              "created_utc": "2026-02-14 16:30:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dak87",
                  "author": "FuzzeWuzze",
                  "text": "Maybe it's a lm studio issue then because it most definitely happens",
                  "score": 1,
                  "created_utc": "2026-02-14 17:18:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o591yqc",
          "author": "LA_rent_Aficionado",
          "text": "I assume this is unslothâ€™s repo?\n\nâ€œXLâ€ on unsloth is usually how they categorize their UD dynamic quants, not because they are XL but because there is no separate/better way to fit them into the standard categories.",
          "score": 9,
          "created_utc": "2026-02-13 23:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58k43j",
          "author": "Pixer---",
          "text": "I found that the M model performs better at longer context. The XL variant falls of quicker for programming. Itâ€™s quite noticeable in longer conversations",
          "score": 5,
          "created_utc": "2026-02-13 21:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58tb32",
          "author": "qwen_next_gguf_when",
          "text": "Can't run the smallest\n\n![gif](giphy|uqmtVo5zcIBXJq1rGX)",
          "score": 3,
          "created_utc": "2026-02-13 22:26:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b9qgl",
          "author": "beefgroin",
          "text": "I need Q0.1",
          "score": 2,
          "created_utc": "2026-02-14 08:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6764i1",
          "author": "Orlandocollins",
          "text": "I have 2 rtx pros and have really wanted to get up to 4. But setting all these new models that I would still have to run at a high quant gives me pause. ",
          "score": 2,
          "created_utc": "2026-02-19 06:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o577p96",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 3,
          "created_utc": "2026-02-13 17:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58kuc1",
              "author": "inevitabledeath3",
              "text": "Quantisation doesn't change things like model structure, number of layers, embedding size, or parameters. Llama is lying to you.",
              "score": 4,
              "created_utc": "2026-02-13 21:44:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b92g6",
          "author": "emrbyrktr",
          "text": "Is it better than Qwen 3 Coder Next?",
          "score": 1,
          "created_utc": "2026-02-14 08:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zyfyc",
              "author": "I_like_fragrances",
              "text": "I like it to be honest. But my overall favorite is still Deepseek v3.1 terminus q4_k_xl",
              "score": 1,
              "created_utc": "2026-02-18 04:36:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7dbwm",
      "title": "I built VELLE.AI - a local AI companion with memory, voice, quant engine, and a full productivity suite. No cloud, no subscriptions. Everything on your machine.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "author": "Rich_Supermarket_164",
      "created_utc": "2026-02-17 17:54:58",
      "score": 46,
      "num_comments": 26,
      "upvote_ratio": 0.82,
      "text": "Hey everyone, I've been building this for a while and finally shipped it.\n\n**VELLE.AI** is a local AI operating system that runs on top of Ollama. It's not just another chat wrapper. It's a full personal assistant with persistent memory, two-way voice, a quantitative finance engine, and a productivity suite with todos, habits, goals, journal, and achievements.\n\n**What makes it different:**\n\n* **Persistent memory** â€” it actually remembers you across sessions. Your preferences, your name, your projects. All stored locally in SQLite.\n* **Two-way voice** â€” speech-to-text plus text-to-speech with hands-free mode. Talk to it, it talks back.\n* **7 personalities** â€” switch between Default, Sarcastic, Evil Genius, Anime Mentor, Sleepy, Kabuneko (finance gremlin), and Netrunner (cyberpunk). They actually stay in character.\n* **Kabuneko Quant Engine** â€” real-time stock quotes, full technical analysis including RSI, MACD, Bollinger, ADX, Sharpe, momentum scanning, value dislocations, backtesting, sentiment analysis, and a 4-bucket stock ideas generator. All from Yahoo Finance, no API keys needed.\n* **Productivity suite** â€” task manager with priorities, projects, due dates, habit tracker with streaks and weekly grids, pomodoro timer, goal system with milestones and progress bars, personal journal with writing prompts, bookmarks, knowledge base.\n* **25 achievements** â€” unlock badges as you use it. Toast notifications slide in when you earn one.\n* **Auto-insights** â€” detects patterns like \"work has been a recurring stressor 4 times this week\" or \"you created 15 tasks but only completed 3.\"\n* **Daily briefing** â€” one command gives you mood, tasks, habits, goals, streaks, reminders, and market data.\n* **Local file search** â€” searches your Desktop, Documents, Projects, Code directories by filename and content.\n* **System commands** â€” opens apps, runs PowerShell commands, controls your machine.\n* **Proactive reminders** â€” \"remind me to check email in 10 minutes\" actually fires with browser notifications plus text-to-speech.\n* **Cyberpunk terminal UI** â€” because aesthetics matter.\n\n**Tech stack:** Node.js, Express, WebSocket, SQLite, Ollama, vanilla JS. About 8,000 lines across 6 server modules. Works with any Ollama model including qwen3:8b, llama3, mistral. Ships as a Windows .exe or run from source on any OS.\n\n**Zero external AI APIs. Zero telemetry. Zero cloud. Everything local.**\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5xggn2",
          "author": "Barachiel80",
          "text": "Github repo or this is AI slop vaporware",
          "score": 12,
          "created_utc": "2026-02-17 20:34:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cuwsn",
              "author": "Rich_Supermarket_164",
              "text": "it's my repo",
              "score": 1,
              "created_utc": "2026-02-20 02:54:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x69lm",
          "author": "BonebasherTV",
          "text": "unrevocable-lyla-gleefully.ngrok-free.dev is a tunnel to the OPâ€™s computer most likely. If the OP has it turned on it works otherwise it wonâ€™t work. \nWould love to see the repo of this. To understand the interactions between the different components.",
          "score": 11,
          "created_utc": "2026-02-17 19:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ycgk3",
              "author": "Rich_Supermarket_164",
              "text": "here made it public [https://github.com/velle999/velle.ai](https://github.com/velle999/velle.ai) ",
              "score": 8,
              "created_utc": "2026-02-17 23:10:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wow5y",
          "author": "Awkward-Customer",
          "text": "What are you using for the finance aspect of this? Also, is there a reason you chose to support only ollama and not llama.cpp?",
          "score": 4,
          "created_utc": "2026-02-17 18:24:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wpafn",
          "author": "Fair-Cookie9962",
          "text": "[velle.ai](http://velle.ai) site seems down, [vella.ai](http://vella.ai) seems something different. There is lot of things named velle or vellum, which is confusing.",
          "score": 3,
          "created_utc": "2026-02-17 18:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o692cmw",
              "author": "CarolloSenpai",
              "text": "To my understanding it's local!",
              "score": 1,
              "created_utc": "2026-02-19 15:04:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wpdk5",
          "author": "rusty_daggar",
          "text": "Do you have  link? \"velle.ai\" points to another address that is broken",
          "score": 2,
          "created_utc": "2026-02-17 18:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wwapq",
          "author": "eazolan",
          "text": "I'm surprised you didn't make \"Hyper competent assistant\" one of the personalities.",
          "score": 1,
          "created_utc": "2026-02-17 18:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xwfnr",
          "author": "Christosconst",
          "text": "So you built an operating system? For a companion bot? With filesystems and printer drivers and whatnot?",
          "score": 1,
          "created_utc": "2026-02-17 21:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zns9y",
          "author": "dropswisdom",
          "text": "Looks great. Can you help to package it in a docker that can use an existing (separate) ollama instance running on another docker with docker compose?",
          "score": 1,
          "created_utc": "2026-02-18 03:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wx6f9",
          "author": "Far_Cat9782",
          "text": "Wtf I'm in the middle of the same thing my god what a small word.i already have  rag, chat history,  image generation, web search, different agent cards with different functionality like creative writing agen who generates images to go along with stories.a media manager agent who handles jellyfin and can list all the media on your server with movie posters and allows you  watch the movie right there or cast to a tc. I'm mind blown that we all have the same ideas. I call mines June.ai\n\nhttps://preview.redd.it/fmys6r6as3kg1.png?width=1220&format=png&auto=webp&s=b7cbb3c25cfcd491e0b82b644ccdb4201a28809d",
          "score": 1,
          "created_utc": "2026-02-17 19:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wz1ib",
              "author": "Awkward-Customer",
              "text": "I suspect a lot of people are working on this right now. the biggest hurdle is probably ingesting the data from so many different sources, some of which deliberately silo their data.",
              "score": 3,
              "created_utc": "2026-02-17 19:11:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5wxh8g",
              "author": "Far_Cat9782",
              "text": "Here's another screenshot of the jellyfin integration\n\nhttps://preview.redd.it/jlcqpypzr3kg1.png?width=1220&format=png&auto=webp&s=150883919c51839ac805765f3f68e647184bd69b",
              "score": 0,
              "created_utc": "2026-02-17 19:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wxlnv",
          "author": "Far_Cat9782",
          "text": "The agents\n\nhttps://preview.redd.it/gdip5u34s3kg1.png?width=1220&format=png&auto=webp&s=6f18ea71747a245355eda9ab074379cbe527801f",
          "score": 1,
          "created_utc": "2026-02-17 19:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x6g39",
          "author": "Sidze",
          "text": "I wonder why it's on Ollama if MLX models are more efficient for Apple Silicon on MacOS. I guess it could be connecting to Osaurus for MLX models and more efficiency.",
          "score": 1,
          "created_utc": "2026-02-17 19:46:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xjx07",
              "author": "Fair-Cookie9962",
              "text": "Ollama sound familiar, easier to trust.",
              "score": -1,
              "created_utc": "2026-02-17 20:50:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xph8b",
                  "author": "Sidze",
                  "text": "MLX is a framework built by Apple, not easier to trust?  \nThough I get it - much easier to plugin Ollama and forget it, instead of creating the whole MLX host manager. Anyway.",
                  "score": 2,
                  "created_utc": "2026-02-17 21:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6hmxl",
      "title": "The Mac Studio vs NVIDIA Dilemma â€“ Best of Both Worlds?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r6hmxl/the_mac_studio_vs_nvidia_dilemma_best_of_both/",
      "author": "JournalistShort9886",
      "created_utc": "2026-02-16 18:29:39",
      "score": 42,
      "num_comments": 36,
      "upvote_ratio": 0.92,
      "text": "Hey, looking for some advice here.\n\nIâ€™m a person who runs local LLMs and also trains models occasionally. Iâ€™m torn between two paths:\n\nOption 1: Mac Studio â€“ Can spec it up to 192gb(yeah i dont have money for 512gb) unified memory. Would let me run absolutely massive models locally without VRAM constraints. But the performance isnâ€™t optimized for ML model training as to CUDA, and the raw compute is weaker. Like basic models would tale days\n\nOption 2: NVIDIA GPU setup â€“ Way better performance and optimization (CUDA ecosystem is unmatched), but Iâ€™m bottlenecked by VRAM. Even a 5090 only has 32GB,.\n\nIdeally I want the memory capacity of Mac + the raw power of NVIDIA, but that doesnâ€™t exist in one box.\n\nHas anyone found a good solution? Hybrid setup? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6hmxl/the_mac_studio_vs_nvidia_dilemma_best_of_both/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5r1ap1",
          "author": "Karyo_Ten",
          "text": "What are the sizes of models you want to train?\n\nBest is probably to train on runpod, rent a B200 or H100x8 for 8hours and be done with it.\n\nNow for inference 192GB gets you interesting models (Qwen, MiniMax, StepFun) but not \"absolutely massive\" models like DeepSeek, GLM, Kimi K2.\n\nYou didn't say your use case. For chatting/RP Macs will be good. For agentic coding you'll wait forever when you dump large files or large webpages / documentation into it.",
          "score": 9,
          "created_utc": "2026-02-16 20:57:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u9u1m",
              "author": "TrendPulseTrader",
              "text": "+1 train on runpod or similar",
              "score": 1,
              "created_utc": "2026-02-17 09:53:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o60pthd",
              "author": "cibernox",
              "text": "This may not be the case anymore in a few weeks when the M5 family chips land. They have ML accelerators similar to cuda cores and promo processing might be 4x what current models get (at least the base M5 runs circles around base M4)",
              "score": 1,
              "created_utc": "2026-02-18 08:22:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60s1xz",
                  "author": "Karyo_Ten",
                  "text": "Fair point. But Apple may also x5 the prices to follow the RAM premium.",
                  "score": 1,
                  "created_utc": "2026-02-18 08:43:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bitxi",
                  "author": "voyager256",
                  "text": "Arenâ€™t you confusing CPU with GPU, or just mean M4 vs M5 chips in general as SoC? Cause e.g. M5 and future M5 Max are different things .  \nFor LLM performance only GPU makes significant difference (and itâ€™s memory bandwidth) . So I guess Apple improved GPU architecture with M5 e.g. so called Neural Accelerators , unless they also found efficient way to use NPUs in parallel for LLMs?. \n\nApple already has CUDA equivalent etc.",
                  "score": 1,
                  "created_utc": "2026-02-19 22:12:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q9hiv",
          "author": "HealthyCommunicat",
          "text": "I have a 5090 workstation and 378 gb of mac unifed memory. \n\nUSE of the model is going to always be so much more important and will only be a such tiny part of your time compared to TRAINING or other CUDA things in real world cases.\n\nTwo dgx sparks canâ€™t even beat the m3 ultra in terms of t/s, and the prefix cache fixes the prmpt processing issues if you are using coding loops or normal use case of conversations and not massive massive data processing isnâ€™t your #1 requirement - but inferencing the biggest models at the best speed is ALWAYS going to be yohr main use case and need, and youâ€™re kidding yourself if you say otherwise as the time and use of the things that are needed in CUDA are super niche and such a dramatic portion of your time will be spent on inferencing and using models itself.\n\nIf your on mac check this out for the fastest server / plug and play agentic coding tool: https://vmlx.net/",
          "score": 14,
          "created_utc": "2026-02-16 18:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qm4d0",
              "author": "DataGOGO",
              "text": "Sparks are not intended to be fast local inference machines. They are development consoles that run the exact same hardware and software stack as the massive clusters, meaning you dev and test on the cheap little spark before you push big jobs to the datacenter full of clusters. If that isnâ€™t you, donâ€™t buy sparks.Â \n\nIf you are just running a personal use chatbox, and want to mess around with running larger models (albeit slowly), then I mostly agree with you.\n\nBut anything beyond that, CUDA isnâ€™t niche, it is THE industry standard in which everything is built on.Â ",
              "score": 13,
              "created_utc": "2026-02-16 19:43:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rmw8o",
                  "author": "luix93",
                  "text": "A spark is not much slower in pure t/s than an m3 ultra, but is much faster in prompt processing. It is also twice or more faster at anything that deals with image or video generation, and an Asus gx10 can be had for less than 3k. If one is looking for a little box that they can hide anywhere with low power consumption then that makes it also a good pick for inferencing imho, as long as you like to thinker with stuff. I love mine personally.",
                  "score": 3,
                  "created_utc": "2026-02-16 22:44:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5tmail",
              "author": "NeverEnPassant",
              "text": "Prefix caching doesn't solve slow prompt processing with coding models.",
              "score": 2,
              "created_utc": "2026-02-17 06:14:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5trgm9",
                  "author": "HealthyCommunicat",
                  "text": "It doesnâ€™t completely yeah but for a good wide range of use cases that is actual more home use automation and not RAG of needing to constantly scrape a crap ton of text, going through a properly structed project should be decent. I work on full sites with a custom cli agent and its been pretty nice so far.",
                  "score": 0,
                  "created_utc": "2026-02-17 06:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rb80m",
          "author": "Creepy-Bell-4527",
          "text": "Macs are good at inference, not training.\n\nIn fact the RTX 5090 won't get you far on training either.",
          "score": 4,
          "created_utc": "2026-02-16 21:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sgdso",
          "author": "clwill00",
          "text": "Yeah, I have a large Mac Studio and played around. Ugh. Decided to go all in, built a monster AI rig running Windows. AMD Threadripper, 128gb DDR5 ram, Samsung 8tb 9100 ssd, and RTX 6000 workstation with 96gb vram. Your â€œdoesnâ€™t exist in one boxâ€ you mentioned above. It rocks.",
          "score": 4,
          "created_utc": "2026-02-17 01:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r8p4g",
          "author": "wouldntthatbecool",
          "text": "Read the recommendations for Kimi K2.5 yesterday, and it is 2x4090's and 1.92TB of RAM.",
          "score": 3,
          "created_utc": "2026-02-16 21:33:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rfr17",
          "author": "Zen-Ism99",
          "text": "Yup, Iâ€™m looking forward to the M5 Ultraâ€¦",
          "score": 3,
          "created_utc": "2026-02-16 22:08:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3p2h",
              "author": "JournalistShort9886",
              "text": "Fr same",
              "score": 2,
              "created_utc": "2026-02-17 08:53:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qq64k",
          "author": "Proof_Scene_9281",
          "text": "I think it depends on the use-case. Initially i thought building codes through the commercial API's was going to be cost prohibitive and painful. But now I've pretty much built everything that was needed with a claude Max subscription and ChatGPT pro. It's not even close to the cost of local hardward, especially in todays pricing. \n\ni'm still looking for a good use-case for my 4x3090 machine. ",
          "score": 5,
          "created_utc": "2026-02-16 20:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rya8u",
              "author": "bac2qh",
              "text": "Vibevoice asr to record meetings and transcribe. Thatâ€™s what I am doing now lol",
              "score": 1,
              "created_utc": "2026-02-16 23:47:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ret1o",
          "author": "Zen-Ism99",
          "text": "Will MLX not work for you?",
          "score": 2,
          "created_utc": "2026-02-16 22:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3niu",
              "author": "JournalistShort9886",
              "text": "It does my initial models were trained on mlx on a macbook m2 ,though it is not as optimized and slower than nvidia  \n Plus im not a enterprise level model trainer,im more like a â€œenthusiast â€ level who adjusts scale according to hardware currently i have rtx5080 and i trained 600m from scratch ,if i have more i will train more,that said maybe mac studio is the only option",
              "score": 1,
              "created_utc": "2026-02-17 08:53:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s4hd8",
          "author": "SDusterwald",
          "text": "For Nvidia vs Mac - main question would be if you want to use any diffusion models alongside the LLMs. Macs are okay at LLM inference, but for image/video gen I highly recommend the Nvidia route at this time.\n\nMore importantly, if you do decide on the Mac route I highly recommend waiting for the M5 Ultra MacStudio. It should be coming later this year and will be far better for all AI workloads than the previous gen Macs due to the built in matmul acceleration in the M5 GPU. Spending that much money now when a huge upgrade is just around the corner makes no sense (if you can't wait I'd probably just go for Nvidia - not going to see any new Nvidia GPUs for at least a year, maybe two).",
          "score": 2,
          "created_utc": "2026-02-17 00:23:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u45wn",
              "author": "JournalistShort9886",
              "text": "Right ,yes i can wait its not like i was going to buy it tomm,i was planning for future\nThanks for your suggestion!",
              "score": 1,
              "created_utc": "2026-02-17 08:58:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5rh562",
          "author": "hermjohnson",
          "text": "Have you considered one of the Nvidia GB10 devices (ie DGX Spark)? I just ordered the Asus version for $3k. 128GB of shared memory.",
          "score": 1,
          "created_utc": "2026-02-16 22:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3zo0",
              "author": "JournalistShort9886",
              "text": "Yeah heard it is good ;though for your use case is the unified memory gb/s enough,like isnt it 200-300gb/s,that said 128gb is still impressive and 1000tflops on fp4 is great for training models like in 1.5b range \nGuess we cant be too greedyðŸ˜…",
              "score": 1,
              "created_utc": "2026-02-17 08:56:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sy86v",
          "author": "midz99",
          "text": "This is how nividia controls the market. wait for the new mac studio. coming from someone who owns 4 nvidia 6000 adas",
          "score": 1,
          "created_utc": "2026-02-17 03:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tcup6",
          "author": "syndorthebore",
          "text": "I have 4 RTX pro blackwell 6000 max-q on a workstation.\n\nThis feels barely ok to train, a mac won't do for training at all.\n\nIt depends on use case, I'll be honest, just rent clusters it's way better price/output ratio.\n\n\n\nI also do video music and image generation, if you want to dip your toes in this, the mac won't do either. ",
          "score": 1,
          "created_utc": "2026-02-17 05:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ttsa4",
          "author": "Chlorek",
          "text": "I burned myself a few times on seemingly good hardware only to discover subpar or even nonexistent software support for it. I felt bad about it and it was not even a big investment. Therefore I see Mac the same way vs CUDA on nvidia. I would be very careful with pumping big sums of money into systems I am not sure of. As for Macs I read you need to go with top models as memory bandwidth is not that great on lower ones.",
          "score": 1,
          "created_utc": "2026-02-17 07:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qky31",
          "author": "DataGOGO",
          "text": "If you are doing any training at all, the mac is not really an option.\n\nIf you are just serving models the Mac works pretty well.\n\nIn terms of local hardware, you are not going to do any real training with consumer gaming GPUâ€™s you will need at least an RTX Pro BW, but even then you only have 96GB of VRAM; realistically, you would need 4 or 8; or buy 4 H200 NVLâ€™s (~$130k), and that is an entry point.Â \n\nThe real answer for occasional training is you rent the clusters by the hour.Â \n\nThat said, if you are just learning, a RTX 5090 willÂ work just fine for labs / making very small models.",
          "score": 0,
          "created_utc": "2026-02-16 19:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q9bkw",
          "author": "Antique_Dot_5513",
          "text": "Yes, it's called an API. Otherwise, rent a more powerful GPU online.",
          "score": -2,
          "created_utc": "2026-02-16 18:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5resv4",
              "author": "Ryanmonroe82",
              "text": "Or buy the compute and not be locked in to api costs.  I made a dataset this passed week and final result was 280 million tokens.  That's many thousands of dollars in api costs right there, cheaper to buy something in the long run",
              "score": 6,
              "created_utc": "2026-02-16 22:03:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4yim5",
      "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4yim5/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "author": "Educational_Cry_7951",
      "created_utc": "2026-02-14 22:56:06",
      "score": 39,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n>**Please think of giving the Github repo a STAR if you like it :)**\n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n>\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations**\n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4yim5/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5fabnd",
          "author": "PomeloSweet926",
          "text": "Just tried this with Gemma3-27B-it (the NVFP4 version) and shit it fits and runs smoothly this is genuinely useful, nice work OP",
          "score": 3,
          "created_utc": "2026-02-14 23:45:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fao32",
              "author": "Educational_Cry_7951",
              "text": "Thank you, Next step for me is to integrate Qwen-Next and make sure it fits and runs fast on a low VRAM budget",
              "score": 1,
              "created_utc": "2026-02-14 23:47:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f4p9m",
          "author": "DataGOGO",
          "text": "Really interesting, what weight format does it expect?",
          "score": 1,
          "created_utc": "2026-02-14 23:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f62bk",
              "author": "Educational_Cry_7951",
              "text": "Thank you! NVFP4 using [https://github.com/NVIDIA/Model-Optimizer.git](https://github.com/NVIDIA/Model-Optimizer.git) \n\nhere is an example:\n\n    python hf_ptq.py \\\n        --pyt_ckpt_path google/gemma-3-27b-it \\\n        --qformat nvfp4 \\\n        --kv_cache_qformat fp8 \\\n        --export_fmt hf \\\n        --export_path ./gemma-3-27b-it-nvfp4-fp8kv \\\n        --calib_size 512 \\\n        --trust_remote_code",
              "score": 2,
              "created_utc": "2026-02-14 23:18:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5f6ffz",
                  "author": "DataGOGO",
                  "text": "Does this use the compressed tensors? Does it use modeloptÂ format? Fused scales?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:21:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5femea",
          "author": "Suitable-Still8379",
          "text": "neat, how does output quality compare to running the same base model in Q4\\_K\\_M via llama.cpp?",
          "score": 1,
          "created_utc": "2026-02-15 00:12:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7bohc",
      "title": "[macOS] Built a 100% local, open-sourced, dictation app. Seeking beta testers for feedback!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r7bohc",
      "author": "AdorablePandaBaby",
      "created_utc": "2026-02-17 16:58:03",
      "score": 37,
      "num_comments": 41,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7bohc/macos_built_a_100_local_opensourced_dictation_app/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5w75dq",
          "author": "Robby2023",
          "text": "Looks really good! Kudos for this.\n\nOne question, how much RAM do you need in order for it to work properly?",
          "score": 5,
          "created_utc": "2026-02-17 17:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w83la",
              "author": "AdorablePandaBaby",
              "text": "I've tested reliably on a 4GB machine. Wouldn't go lower than that tbh.",
              "score": 3,
              "created_utc": "2026-02-17 17:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5w8qb7",
                  "author": "Robby2023",
                  "text": "Which model does it use behind the scenes?",
                  "score": 1,
                  "created_utc": "2026-02-17 17:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w8tdr",
          "author": "rusty_daggar",
          "text": "I don't have a mac to try it on, but it looks like a nice idea.\n\nAre you using a VAD to clean up the audio or just passing all straight to whisper?",
          "score": 3,
          "created_utc": "2026-02-17 17:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9v0i",
              "author": "AdorablePandaBaby",
              "text": "Straight to whisper. \n\nBut I haven't felt the need to remove bg noise yet.   \nMaybe a lower priority improvement later down the line.",
              "score": 2,
              "created_utc": "2026-02-17 17:14:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wp082",
          "author": "JohnHawley",
          "text": "I've been using Handy, it's been great! [https://github.com/cjpais/Handy](https://github.com/cjpais/Handy)  \nWhat does SpeakType do differently? Is there extra logic that Handy doesn't perform?",
          "score": 4,
          "created_utc": "2026-02-17 18:25:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w759k",
          "author": "AdorablePandaBaby",
          "text": "The title should say \"open source\", instead of \"open-sourced\", but I guess its too late.",
          "score": 2,
          "created_utc": "2026-02-17 17:01:03",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5w9buv",
          "author": "iongion",
          "text": "UI is gorgeous",
          "score": 2,
          "created_utc": "2026-02-17 17:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9zfe",
              "author": "AdorablePandaBaby",
              "text": "Thank you!  \nMy OCD drove me crazy, but I'm glad someone liked it as well.",
              "score": 2,
              "created_utc": "2026-02-17 17:15:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wx6ob",
          "author": "Pitiful-Impression70",
          "text": "nice, the local-only approach is the way to go imo. quick question tho, does it do any LLM post-processing on the whisper output? like adding punctuation, fixing capitalization, formatting stuff based on context? raw whisper output is usually pretty good but it still dumps everything as one block of text which is annoying when youre dictating emails or notes. thats been the main gap ive seen with most local dictation tools vs the cloud ones that have an extra formatting pass",
          "score": 2,
          "created_utc": "2026-02-17 19:02:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wxac1",
              "author": "AdorablePandaBaby",
              "text": "Working on that rn :)",
              "score": 1,
              "created_utc": "2026-02-17 19:03:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ywjpm",
                  "author": "Pitiful-Impression70",
                  "text": "nice, thats the part that really makes or breaks it imo. like raw whisper output is fine for short stuff but anything longer than a paragraph and the lack of punctuation and formatting makes it basically unusable without cleanup. curious if youre gonna do it locally too or offload to an api",
                  "score": 1,
                  "created_utc": "2026-02-18 01:01:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xf3ls",
          "author": "Appropriate-Deer234",
          "text": "Looks nice and i also love that everything stays offline. Already downloaded and will test it tomorrow on M1 and M3. I also would have the 2019 Intel if you need feedback from that.",
          "score": 2,
          "created_utc": "2026-02-17 20:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zo00h",
              "author": "AdorablePandaBaby",
              "text": "Yes perfect. Will DM you!",
              "score": 1,
              "created_utc": "2026-02-18 03:29:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xx0s1",
          "author": "CaptainSuckie",
          "text": "I'd like to test this out! ",
          "score": 2,
          "created_utc": "2026-02-17 21:51:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y4oh0",
          "author": "DertekAn",
          "text": "Yes please? ðŸ¤­ðŸ¤­",
          "score": 2,
          "created_utc": "2026-02-17 22:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wax8h",
          "author": "vulture916",
          "text": "Interested! ",
          "score": 1,
          "created_utc": "2026-02-17 17:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wepnv",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:37:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5weryo",
              "author": "vulture916",
              "text": "Spinning Wheel loop on Transcribe using pre-built app. Can't get anything transcribed using Whisper Large V3 Turbo on Macbook M1 14.1 Sonoma.\n\n  \nChecked settings and for whatever reason Audio Input was defaulted to NoSound rather than Macbook microphone. Changed that, same behavior.  ",
              "score": 1,
              "created_utc": "2026-02-17 17:38:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5widpv",
          "author": "Meowliketh",
          "text": "Hey, Iâ€™d love to test this out!",
          "score": 1,
          "created_utc": "2026-02-17 17:55:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5win06",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60zbpl",
                  "author": "Meowliketh",
                  "text": "Don't think I got the DM? Also, love your user name",
                  "score": 1,
                  "created_utc": "2026-02-18 09:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wy4sg",
          "author": "sinebubble",
          "text": "I'm unclear what problem this solves on macOS. Is the implication that Apple's built-in speech dictation is not private? I use the built-in dictation service all the time to transcribe into every app I've used it with.",
          "score": 1,
          "created_utc": "2026-02-17 19:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wzb1k",
              "author": "AdorablePandaBaby",
              "text": "Correct. That's the main usecase, but Apple's STT doesn't detect words properly for a lot of my usecases. \n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 2,
              "created_utc": "2026-02-17 19:12:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wyc52",
          "author": "solipsistmaya",
          "text": "Looks good, interested in testing.",
          "score": 1,
          "created_utc": "2026-02-17 19:08:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x3249",
          "author": "Ok_Yoghurt248",
          "text": "does it work on windows ?",
          "score": 1,
          "created_utc": "2026-02-17 19:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x5q7r",
          "author": "aqdnk",
          "text": "would like to test!",
          "score": 1,
          "created_utc": "2026-02-17 19:43:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xa2en",
          "author": "Correct_Support_2444",
          "text": "I will be trying this out later this week when I get home from a trip. This is exactly what Iâ€™ve been looking for.",
          "score": 1,
          "created_utc": "2026-02-17 20:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xni9n",
          "author": "Driftwintergundream",
          "text": "I'm an active user of superwhisper but looking for a local model TTS.\n\nTo me the 3 key features that would sell me is 1) super fast, 2) super accurate, and 3) slight touch up options of the text to remove verbal mispeaks or ums.\n\nI'm testing it out!",
          "score": 1,
          "created_utc": "2026-02-17 21:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zoso6",
              "author": "AdorablePandaBaby",
              "text": "Yes, then SpeakType should be just the best STT for you. It's fast, accurate and removes all the verbal misspeaks you're concerned about",
              "score": 1,
              "created_utc": "2026-02-18 03:34:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yhhu6",
          "author": "The_BeatingsContinue",
          "text": "So, you decided against a $12/month subscription and i think everybody does. In times of Claude Code it's incredibly easy to invest a little time to solve the whole task selfmade.\n\nMy question is: how can you decide against a subscription model and still want money for it while claiming to be open source? No offense, just a serious question.\n\nAs an additional use case idea: I work with deaf people and having a Mac on a table that makes a whole conversation transparent for anyone at that table would be a great feature, requiring just a text output window scalable to fullsize screen, while letting users choose font/fontsizes and opting out the requirement to push a button. It just can be active all the time and can be started/stopped by a keypress.",
          "score": 1,
          "created_utc": "2026-02-17 23:38:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yzqge",
              "author": "CtrlAltDelve",
              "text": "It's a pretty common model for binaries to be paid, but the sources are open, and nothing stops you from building it with no restrictions. \n\nVoiceInk is one of these. \n\nIf you're familiar with Claude Code, you can just ask it to compile it for you?",
              "score": 1,
              "created_utc": "2026-02-18 01:18:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zjep0",
          "author": "rditorx",
          "text": "macOS has offline dictation, you just need to enable it in the System Preferences and download the language data in there. No need to trust third parties. It might send some data to Apple, though, so consult the privacy policy.",
          "score": 1,
          "created_utc": "2026-02-18 03:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zp8ju",
              "author": "AdorablePandaBaby",
              "text": "Yep, you're right.\n\nBut, Apple's STT doesn't detect words properly for a lot of usecases.\n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 1,
              "created_utc": "2026-02-18 03:37:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6231pt",
          "author": "adrian_dev_yyc",
          "text": "Been building something similar on the Windows side. Privacy is the number one thing people bring up when I ask why they don't use cloud dictation. What's your latency like? In my experience that matters way more than raw accuracy for whether people actually stick with it.",
          "score": 1,
          "created_utc": "2026-02-18 14:24:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o671d6b",
          "author": "Sudden-Ad-1217",
          "text": "Iâ€™m game, got an M1 Max Iâ€™d run it against.",
          "score": 1,
          "created_utc": "2026-02-19 05:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6853j3",
          "author": "Amaeze",
          "text": "yo! this looks great! interested in trying!",
          "score": 1,
          "created_utc": "2026-02-19 11:40:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68p10g",
          "author": "creativedoctor",
          "text": "If it works for portuguese of Portugal, I'm game. Drop me a DM with the instructions, if so, please!",
          "score": 1,
          "created_utc": "2026-02-19 13:52:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6d1qy6",
          "author": "duedev",
          "text": "Found this while searching for a local dictating software for macOS. The one built in works intermittently for me on my m1 laptop.",
          "score": 1,
          "created_utc": "2026-02-20 03:37:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d1syj",
              "author": "AdorablePandaBaby",
              "text": "How did you find this?",
              "score": 1,
              "created_utc": "2026-02-20 03:38:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7umlq",
      "title": "[macOS] PersonaPlex-7B on Apple Silicon (MLX)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "author": "Apprehensive_Boot976",
      "created_utc": "2026-02-18 05:37:04",
      "score": 37,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "NVIDIA released an open-source speech-to-speech model [PersonaPlex-7B](https://huggingface.co/nvidia/personaplex-7b-v1). It listens and talks simultaneously with \\~200ms latency, handles interruptions, backchanneling, and natural turn-taking.\n\nThey only shipped a PyTorch + CUDA implementation targeting A100/H100, so I ported it to MLX, allowing it to run on Apple Silicon: [github.com/mu-hashmi/personaplex-mlx](https://github.com/mu-hashmi/personaplex-mlx).\n\nHope you guys enjoy!",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o627q6u",
          "author": "felixlovesml",
          "text": "thanks. Whatâ€™s the latency on a Mac â€” for example, on an M4 chip â€” and how much RAM does it require?",
          "score": 3,
          "created_utc": "2026-02-18 14:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o607o3d",
          "author": "former_farmer",
          "text": "Thanks.",
          "score": 1,
          "created_utc": "2026-02-18 05:44:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4x41x",
      "title": "Qwen3 8b-vl best local model for OCR?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4x41x/qwen3_8bvl_best_local_model_for_ocr/",
      "author": "BeginningPush9896",
      "created_utc": "2026-02-14 21:56:19",
      "score": 35,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "For all TLDR:\n\n Qwen3 8b-vl is the best in its weight class for recognizing formatted text (even better than Mistral 14b with OCR).\n\nFor others:\n\nHi everyone, this is my first post. I wanted to discuss my observations regarding LLMs with OCR capabilities.\n\nWhile developing a utility for automating data processing from documents, I needed to extract text from specific areas of documents. Initially, I thought about using OCR, like Tesseract, but I ran into the issue of having no control over the output. Essentially, I couldn't recognize the text and make corrections (for example, for surnames) in a single request.\n\nI decided to try Qwen3 8b-vl. It turned out to be very simple. The ability to add data to the system prompt for cross-referencing with the recognized text and making corrections on the fly proved to be an enormous killer feature. You can literally give it all the necessary data, the data format, and the required output format for its response. And you get a response in, say, a JSON format, which you can then easily convert into a dictionary (if we're talking about Python).\n\nI tried Mistral 14b, but I found that its text recognition on images is just terrible with the same settings and system prompt (compared to Qwen3 8b-vl). Smaller models are simply unusable. Since I'm sending single requests without saving context, I can load the entire model with a 4k token context and get a stable, fast response processed on my GPU.\n\nIf people who work on extracting text from documents using LLMs (visual text extraction) read this, I'd be happy to hear about your experiences.\n\nFor reference, my specs:\nR7 5800X\nRTX 3070 8GB\n32GB DDR4\n\nUPD: Forgot to mention. I work with Cyrillic text recognition, so everyone from the CIS segment reading this post can be sure that it applies to Cyrillic alphabets as well.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4x41x/qwen3_8bvl_best_local_model_for_ocr/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5f4nji",
          "author": "sinan_online",
          "text": " Not only do I agree , I also wrote an evaluation script, evaluated a bunch of models, and Qwen3 VL 8B came out on top, passing even Pixtral. I have a Medium article about it.",
          "score": 9,
          "created_utc": "2026-02-14 23:10:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ff6up",
          "author": "International-Lab944",
          "text": "I agree , itâ€™s my go-to local VL/OCR model. When I was doing my evaluations I tested many models and Qwen 8B VL was the top model. I also tested Gemma 27B and Qwen 32B and few more and the small 8B came on top. When itâ€™s not good enough I normally use Qwen3 235B VL through Openrouter.",
          "score": 6,
          "created_utc": "2026-02-15 00:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fcqa4",
          "author": "beedunc",
          "text": "Shhhh. Donâ€™t tell anybody. \n\nIâ€™ve gotten even the 4B/4q Ti do things that no other VL model can. Thereâ€™s none better.",
          "score": 4,
          "created_utc": "2026-02-15 00:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ffrww",
              "author": "BeginningPush9896",
              "text": "What do you think could be the reason that large models perform worse at text recognition than Qwen3-VL?",
              "score": 3,
              "created_utc": "2026-02-15 00:19:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fk054",
                  "author": "beedunc",
                  "text": "I donâ€™t know enough to comment on that, I just run â€˜em. Enjoy!",
                  "score": 4,
                  "created_utc": "2026-02-15 00:45:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ffujr",
          "author": "greatwilt",
          "text": "ZwZ 8B has entered the chat.\n\nhttps://huggingface.co/inclusionAI/ZwZ-8B",
          "score": 6,
          "created_utc": "2026-02-15 00:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mzagx",
              "author": "BeginningPush9896",
              "text": "Dude, just set up the ZwZ 8b. From the initial tests, it's absolutely killer, thanks for the tip. I'm going to keep experimenting with it. If anything changes, I'll post a full review about switching to the ZwZ 8b.",
              "score": 3,
              "created_utc": "2026-02-16 05:52:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5hpnnj",
              "author": "BeginningPush9896",
              "text": "I will test this model later and be sure to write about my impressions. Besides, it says it can automatically recognize areas. Currently, I'm doing the document cropping myself.",
              "score": 1,
              "created_utc": "2026-02-15 11:23:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5y3na7",
              "author": "SpicyWangz",
              "text": "Pretty impressive model honestly",
              "score": 1,
              "created_utc": "2026-02-17 22:24:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5frrzj",
          "author": "boyobob55",
          "text": "Yes it kicks ass I used it in this project too!: https://github.com/boyobob/OdinsList",
          "score": 2,
          "created_utc": "2026-02-15 01:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fgqs3",
          "author": "nunodonato",
          "text": "4B here :)",
          "score": 1,
          "created_utc": "2026-02-15 00:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gbpjs",
          "author": "michael_p",
          "text": "Currently using pixtral 12b 4bit mlx and going to swap to qwen 3 8b-vl per your rec! I love the qwen family. Qwen3 32b mlx is my favorite local model. Feed it good prompts and configure it right and get (imo) opus quality thinking",
          "score": 1,
          "created_utc": "2026-02-15 03:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ijruo",
          "author": "gabriel0123m",
          "text": "For a customer I have made a similar use case like yours and tried multiple models and ways to run them (ollama, vllm) but with qwen 2.5 VL 7B and qwen 3 vl 4B i got the better quality / performance / price to run, but using multiple agents (think like a workflow) to handle some steps and errors with text extraction + using ocr as a fallback step / comparator and if i had evaluated it was possible to be used extract and use text directly (for txt or pdf that are computer generated...). From v2.5 to 3  I had seen better results for OCR in general with the 3 version, but only using a 4B model to make it follow some complex instructions (like following some rules to handle json structured output) wasn't always correct... So using right now the 8B version in prod! I am evaluating as a side GLM-ocr but I have not used as much as qwen vl, ...",
          "score": 1,
          "created_utc": "2026-02-15 14:52:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j4xa0",
          "author": "damirca",
          "text": "for HA it's not so good I'd say\n\nhttps://preview.redd.it/qf321rf3sojg1.png?width=1190&format=png&auto=webp&s=76ed1e8e195c4a44138448104cebb904215f39f9\n\n",
          "score": 1,
          "created_utc": "2026-02-15 16:38:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j73qq",
              "author": "BeginningPush9896",
              "text": "In my workflow, I work with drawings where I can manually crop all the fields I need, so I don't need to rely on searching for a field with specific text.\n\nI would be very interested to know if there are models that can match surnames with signature fields in documents automatically, without prior cropping.",
              "score": 1,
              "created_utc": "2026-02-15 16:48:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jegwf",
          "author": "l_Mr_Vader_l",
          "text": "If you want custom extraction from documents go with qwen 3 vl, but if you just wanna do really good ocr (proper page to markdown, tables and everything) there are smaller dedicated ones which are better even, than the 8b qwen3 vl\n\nMineru2.5 ocr\nLighton ocr\nPaddleOCR vl \n\n and these are just ~1B models just trained to do ocr, they're definitely better in all ways (size, speed and accuracy) \n\nGlm ocr also does custom extraction, but it's main ocr pipeline was pretty underwhelming",
          "score": 1,
          "created_utc": "2026-02-15 17:24:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5msb5d",
          "author": "QuanstScientist",
          "text": "Paddle is as good as qwen and very fast: https://github.com/BoltzmannEntropy/batch-ocr",
          "score": 1,
          "created_utc": "2026-02-16 04:57:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q5b7c",
          "author": "dradik",
          "text": "What about the 30B MoE model? I get like 170 tokens a second and seems accurate.",
          "score": 1,
          "created_utc": "2026-02-16 18:24:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sa1k2",
          "author": "shankey_1906",
          "text": "Noob here, is this considered the best for handwriting recognition too, or would something like this be better? ZwZ 8B - [https://huggingface.co/inclusionAI/ZwZ-8B](https://huggingface.co/inclusionAI/ZwZ-8B)",
          "score": 1,
          "created_utc": "2026-02-17 00:55:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8iew6",
      "title": "Open Source LLM Leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mrakuwr3xbkg1.png",
      "author": "HobbyGamerDev",
      "created_utc": "2026-02-18 23:04:19",
      "score": 35,
      "num_comments": 32,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8iew6/open_source_llm_leaderboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67bwp1",
          "author": "einord",
          "text": "Fun fact: the larger the model, the more intelligent.",
          "score": 13,
          "created_utc": "2026-02-19 07:08:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67suaq",
              "author": "DistanceSolar1449",
              "text": "This ranking is trash. Only thing it gets right is the top 2. \n\nDeepseek V3? That came out in 2024. Deepseek R1 came out a full year ago.\n\nWhereâ€™s Deepseek V3.2? Why is Mistral Large rated so highly, and the same tier as gpt-oss-120b? Why Nemotron V1 instead of Nemotron V1.5?",
              "score": 11,
              "created_utc": "2026-02-19 09:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a0weu",
                  "author": "onil34",
                  "text": "Where is minimax?",
                  "score": 2,
                  "created_utc": "2026-02-19 17:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67jksw",
              "author": "Successful-Emu-6409",
              "text": "scaling still alive ",
              "score": 4,
              "created_utc": "2026-02-19 08:19:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cisu8",
              "author": "Available-Craft-5795",
              "text": "Sometimes, bigger moves faster. Sometimes, smaller takes its time. Both find their way â€” eventually â€” just on different roads to the finish line.\n\nOne is built for the masses, for the noise and the need and the scale. But we build for the love of the question, for the joy in the curious trail.\n\nThey don't care about the small ones. We do.\n\n*Built with curiosity, not compute.* [CompactAI](https://huggingface.co/spaces/CompactAI/Built-with-curiosity-not-compute)\n\n(I know its a promo, shush)",
              "score": 1,
              "created_utc": "2026-02-20 01:39:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67lo9v",
          "author": "jiqiren",
          "text": "Not enough people have 512GB+ of vram or unified memory (like the Mac Studio). Otherwise Minimax M2.5 would be top dog. ðŸ¶",
          "score": 3,
          "created_utc": "2026-02-19 08:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6890vq",
              "author": "deepfit",
              "text": "Still pretty large but MiniMax-M2.5 UD-Q2\\_K\\_XL from unsloth is both fast and < 100G of ram/vram.  I am having a hard time telling any difference from larger quant versions.",
              "score": 2,
              "created_utc": "2026-02-19 12:11:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68mysa",
              "author": "AfterShock",
              "text": "Came here to +1 MinMax 2.5 when I saw it missing from the list.",
              "score": 1,
              "created_utc": "2026-02-19 13:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d0mb1",
                  "author": "jiqiren",
                  "text": "I only have a Mac mini so canâ€™t run it. But itâ€™s so cheap on openrouter itâ€™s been my goto model.",
                  "score": 1,
                  "created_utc": "2026-02-20 03:30:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o687bd6",
          "author": "entheosoul",
          "text": "This should be split between actual locally runable models and cloud models (not exactly local)",
          "score": 6,
          "created_utc": "2026-02-19 11:58:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67hymk",
          "author": "nunodonato",
          "text": "Amazing how got oss 120b holds it's place after all these new models have came out",
          "score": 2,
          "created_utc": "2026-02-19 08:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o693exw",
          "author": "Sufficient_Prune3897",
          "text": "Cursed tier list. Shows that benchmarks are not everything",
          "score": 2,
          "created_utc": "2026-02-19 15:09:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67h8z0",
          "author": "HoustonTrashcans",
          "text": "Can anyone tell me what quantization I need to run a 1T model on my laptop with 8 GB of VRAM? If my math is right that's Q.05?",
          "score": 2,
          "created_utc": "2026-02-19 07:57:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67mgfq",
              "author": "GeorgeSC",
              "text": "Q0.01 ðŸ¤£ (Disclaimer, I do have only a laptop with a RTX 4070 M 8GB ðŸŽ»)",
              "score": 6,
              "created_utc": "2026-02-19 08:47:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67quwa",
              "author": "Ell2509",
              "text": "Not possible. But why do you need a 1t model? With 8gb vram, assuming you have 16gb system ram, you could run a 7 or 8b model, realistically. \n\nMore than that will become unusable slow quite quickly.",
              "score": 2,
              "created_utc": "2026-02-19 09:31:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68pel8",
                  "author": "ShinigamiOverlord",
                  "text": "Very much so. I could somewhat use a 10-14B models, but they cap at 1.3 tokens/s",
                  "score": 1,
                  "created_utc": "2026-02-19 13:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69t4rr",
          "author": "peva3",
          "text": "OP, anyway you could turn this data into an API? I could use these benchmarks for a project I'm working on.",
          "score": 1,
          "created_utc": "2026-02-19 17:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ad120",
          "author": "Far_Cat9782",
          "text": "Gpt 120b is my goal to run locally. Currently the max I can slso to get real work done is 24b model.",
          "score": 1,
          "created_utc": "2026-02-19 18:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b5oz6",
          "author": "Used-Dance-7006",
          "text": "Sorry...maybe this is the designer in me but the color coding is counterintuitive to the way i perceive design.\n\nIs S good? It's red so I read that as the worst?  \nIs C good?\n\nShould I be focusing on S and A models? Is D bad then?\n\nJust trying to understand and appreciate the clarity.",
          "score": 1,
          "created_utc": "2026-02-19 21:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bezen",
          "author": "shankey_1906",
          "text": "Something like this would be amazing for differnt tiers of VRAM, and use-cases. Tier <16GB, <32GB, etc. Tier: Coding, Reasoning, ...",
          "score": 1,
          "created_utc": "2026-02-19 21:52:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6ao8o",
      "title": "Alibabaâ€™s Qwen team just released Qwen3.5-397B-A17B, the first open model in the Qwen3.5 family â€” and itâ€™s a big one.",
      "subreddit": "LocalLLM",
      "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B",
      "author": "techlatest_net",
      "created_utc": "2026-02-16 14:12:59",
      "score": 29,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6ao8o/alibabas_qwen_team_just_released_qwen35397ba17b/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r5f8ym",
      "title": "Tutorial: Run MiniMax-2.5 locally! (128GB RAM / Mac)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/61b97oryxnjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-15 13:57:07",
      "score": 26,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r5f8ym/tutorial_run_minimax25_locally_128gb_ram_mac/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5thtsz",
          "author": "Euphoric_Emotion5397",
          "text": "tough luck. I think models can spread faster if they try to downsize to something that fits 16gb GPU and 64GB RAM.   Right now, I'm using Qwen 3 VL 30B. Very Good! :D",
          "score": 2,
          "created_utc": "2026-02-17 05:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5izi0k",
          "author": "Hitchhiker2TheFuture",
          "text": ">",
          "score": 0,
          "created_utc": "2026-02-15 16:12:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3lx38",
      "title": "Google Releases Conductor",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r3lx38/google_releases_conductor/",
      "author": "techlatest_net",
      "created_utc": "2026-02-13 10:39:03",
      "score": 25,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "# Google Releases Conductor: a context-driven Gemini CLI extension that stores knowledge as Markdown and orchestrates agentic workflows\n\nLink: [https://github.com/gemini-cli-extensions/conductor](https://github.com/gemini-cli-extensions/conductor)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r3lx38/google_releases_conductor/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o55c7pi",
          "author": "Super-Jackfruit8309",
          "text": "about time really, having to do this manually made no sense and often got in the way of work. Looking forward to trying it out.",
          "score": 3,
          "created_utc": "2026-02-13 11:40:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b7x86",
          "author": "trentard",
          "text": "conductor we have a problem\nconductor we have a problem",
          "score": 1,
          "created_utc": "2026-02-14 08:41:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4oava",
      "title": "Hardware constraints and the 10B MoE Era: Where Minimax M2.5 fits in",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4oava/hardware_constraints_and_the_10b_moe_era_where/",
      "author": "Fragrant_Occasion276",
      "created_utc": "2026-02-14 16:03:02",
      "score": 25,
      "num_comments": 44,
      "upvote_ratio": 0.83,
      "text": "We need to stop pretending that 400B+ models are the future of local-first or sustainable AI. The compute shortage is real, and the \"brute force\" era is dying. I've been looking at the Minimax M2.5 architecture - it's a 10B active parameter model that's somehow hitting 80.2% on SWE-Bench Verified. That is SOTA territory for models five times its size. This is the Real World Coworker we've been waiting for: something that costs $1 for an hour of intensive work. If you read their RL technical blog, it's clear they're prioritizing tool-use and search (76.3% on BrowseComp) over just being a \"chatty\" bot. For those of us building real systems, the efficiency of Minimax is a far more interesting technical achievement than just adding more weights to a bloated transformer.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4oava/hardware_constraints_and_the_10b_moe_era_where/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5d0ni9",
          "author": "Panometric",
          "text": "At what quantization?",
          "score": 7,
          "created_utc": "2026-02-14 16:28:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e3w93",
              "author": "DataGOGO",
              "text": "FP8, it degrades rapidly at anything below thatÂ ",
              "score": 7,
              "created_utc": "2026-02-14 19:46:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ektea",
                  "author": "SpicyWangz",
                  "text": "Thatâ€™s good to know. I hope we can get some high quality quants soon, but it might just be that theyâ€™re maximizing the most out of every parameter and thereâ€™s no way to achieve a good quant on this model",
                  "score": 2,
                  "created_utc": "2026-02-14 21:18:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d11ol",
          "author": "Tema_Art_7777",
          "text": "\nFor those building real products, why are you so focused on cost of tokens? Frontier models are amazing, and they really do not cost much. If you are worried $10 vs $1 for an hour of work, what is your business model that is so sensitive to that? Genuinely want to know.",
          "score": 6,
          "created_utc": "2026-02-14 16:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d68gi",
              "author": "sooodooo",
              "text": "Iâ€™ll point out the â€œsustainableâ€ part first. We know the big providers all burn through investor money by providing AI at the current price. Itâ€™s 10$ now, but without more efficient models thatâ€™s not sustainable and investors want to see some honey at some point.\n\nSecond I donâ€™t know what metric â€œ$10 per hour workâ€ is, but Iâ€™d rather pay $10 and have the same work done in a minute. I donâ€™t understand how price could not be THE deciding factor in the age of cloud computing, if I can get the same work done at 1/10 the of the price then you can scale 10x bigger and faster.",
              "score": 9,
              "created_utc": "2026-02-14 16:56:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dx06y",
                  "author": "Karyo_Ten",
                  "text": "Companies are often bottlenecked by humans, within or outside, say verification, quality control, certification, a signature.\n\nProducing meh output fast is not interesting because work will be sent back.\n\nIt's interesting for iteration/prototyping but a model that does well from the get go might end up costing less even if bigger / more expensive per token",
                  "score": 1,
                  "created_utc": "2026-02-14 19:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ei0r9",
              "author": "evilbarron2",
              "text": "Are you building a product thatâ€™s only good for business use cases? Thatâ€™s fine, but it isnâ€™t where the majority of ai stuff is gonna happen. Instead of focusing on what the user is or isnâ€™t doing, just make clear what the requirements/ costs of *your* product is. Users arenâ€™t gonna use your product the way you think they will.",
              "score": 1,
              "created_utc": "2026-02-14 21:03:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eu8mi",
                  "author": "Tema_Art_7777",
                  "text": "It is a mix, both business and personal use cases like home automation. to expand further, $10 for mm tokens donâ€™t move the needle for business and I donâ€™t need mm of tokens for personal stuffâ€¦. So still not sure why everyone is hyped up about 50cents for mm tokens. So I must be missing a use-case people haveâ€¦",
                  "score": 1,
                  "created_utc": "2026-02-14 22:10:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5fi1kd",
              "author": "former_farmer",
              "text": "You are on LocalLLM my brother. Not on \"consume LLMs on the cloud for cheap\" subreddit.",
              "score": 1,
              "created_utc": "2026-02-15 00:33:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fkh5q",
                  "author": "Tema_Art_7777",
                  "text": "Yes except the OPâ€™s post said: â€œThis is the Real World Coworker we've been waiting for: something that costs $1 for an hour of intensive work.â€. So Iâ€™m questioning the use-cases for the intensive focus on price.",
                  "score": 1,
                  "created_utc": "2026-02-15 00:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ew29b",
          "author": "Dependent-Example930",
          "text": "There is a string of these style posts. Feels spam/fake pushing minimax 2.5",
          "score": 3,
          "created_utc": "2026-02-14 22:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d0btt",
          "author": "Pixer---",
          "text": "What local system do you have ?",
          "score": 2,
          "created_utc": "2026-02-14 16:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e5n6a",
              "author": "starkruzr",
              "text": "unfortunately chances are good OP is just promoting.",
              "score": 1,
              "created_utc": "2026-02-14 19:55:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e0bjy",
          "author": "No-Leopard7644",
          "text": "ðŸ’¯ My focus and expertise is local ai with open source models and tools - for enterprises as well as personal use. That being said, the element of tokenomics, sovereign data and model freedom is paramount for highly regulated industries. I spend my day job straddling cloud as well as architecting sovereign local AI infrastructure with the ability to burst to the cloud.",
          "score": 1,
          "created_utc": "2026-02-14 19:27:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gc6bq",
          "author": "Euphoric_Emotion5397",
          "text": "  \nSo, in actual practise, my local setup that can get me the same output every single time is temp <= 0.3, tool use, searxng, playwright, local context memory to get real world updates in multistage workflow with different prompts at different stage. \n\nWe just need a good enough multimodal LLM (using qwen 3 VL 30b) and a good enough embedding model (using Gemma embedding) to understand what we are feeding it and then use the available data (we give them) to come out with an analysis in json output format.\n\nThen i feed the output to Gemini Pro to verify and it's rated highly against their own.\n\nYou need to have a workflow usecase that you use frequently which you can vibecode first.\n\nFor other things generic , then it's better to just go online and use the frontier models to chat and get your answer.",
          "score": 1,
          "created_utc": "2026-02-15 03:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h18yx",
          "author": "Exciting_Square_4593",
          "text": "The obsession with \"more parameters = more better\" is just cope for companies that can't figure out efficient architecture. M2.5 hitting 80.2% on SWE-Bench Verified with only 10B active parameters is a slap in the face to every bloated 400B model out there. If you're still paying for compute-heavy giants that lag every time you try to run a simple agentic loop, you're literally just subsidizing bad engineering.",
          "score": 1,
          "created_utc": "2026-02-15 07:28:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h8696",
          "author": "Monty756",
          "text": "$1 an hour for a real-world coworker that doesn't hallucinate like a high schooler on caffeine. Finally, a model that respects my time and my wallet.",
          "score": 1,
          "created_utc": "2026-02-15 08:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h8aym",
          "author": "Critical-Raccoon-926",
          "text": "It's the difference between a junior dev who yaps and a senior who just pushes the fix.",
          "score": 1,
          "created_utc": "2026-02-15 08:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hi7lw",
          "author": "Previous-Shop6033",
          "text": "We are hitting the physical limits of H100 clusters and people still think scaling is the only way up. M2.5 is proof that you can reach SOTA by being smart about tool-use and search (that 76.3% on BrowseComp is no joke) rather than just throwing more VRAM at the problem.",
          "score": 1,
          "created_utc": "2026-02-15 10:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i0nlw",
          "author": "Nervous-Dig-6383",
          "text": "Imagine being a \"frontier\" model that's 5x the size and still losing to a 10B active MoE on actual engineering tasks. Embarrassing.",
          "score": 1,
          "created_utc": "2026-02-15 12:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i63m8",
          "author": "BetterInternet63",
          "text": "Finally, someone mentions the BrowseComp score. Everyone fixates on SWE-Bench, but if your \"coworker\" can't navigate documentation or search for a specific library error without getting lost, it's useless. M2.5 actually feels like it knows how to use a browser.",
          "score": 1,
          "created_utc": "2026-02-15 13:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i9ckk",
          "author": "Greedy_Sandwich_1839",
          "text": "100 TPS at 1/10th the cost of Claude. If you're not using this for your CI/CD pipelines yet, you're basically just lighting money on fire.",
          "score": 1,
          "created_utc": "2026-02-15 13:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ic5f6",
          "author": "MarinaCER87",
          "text": "Tired of these \"chatty\" bots that spend 500 tokens apologizing before they even look at the code. M2.5's focus on grounded tool-use over \"personality\" is exactly what the industry needs right now. We don't need a friend; we need an engineer.",
          "score": 1,
          "created_utc": "2026-02-15 14:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5idwlo",
          "author": "n521n",
          "text": "Everyone is mourning the \"compute shortage\" while MiniMax is just out here building the most sustainable architecture of the 10B MoE era. This is the future, whether the hardware snobs like it or not.",
          "score": 1,
          "created_utc": "2026-02-15 14:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r8w1o",
          "author": "HealthyCommunicat",
          "text": "Lol its funny cuz qwen 3.5 cane out today and is 400+ b params AND is around 10b active making it amazinfg. Low active param models are the future.",
          "score": 1,
          "created_utc": "2026-02-16 21:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ak4s",
          "author": "wwy413",
          "text": "The 400B+ model craze is just VC bait at this point. 10B active params hitting those SWE-Bench numbers is the actual engineering flex.",
          "score": 1,
          "created_utc": "2026-02-18 06:07:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60c5by",
          "author": "Letitiahappy",
          "text": "$1/hr for SOTA performance is the only metric that matters if you're actually running a business and not just playing with a chatbot.",
          "score": 1,
          "created_utc": "2026-02-18 06:20:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62fqgb",
          "author": "Huong101",
          "text": "I've been saying this - efficiency > scale. If M2.5 can maintain that 80.2% on verified benchmarks with a smaller footprint, the \"bigger is better\" era is officially over.",
          "score": 1,
          "created_utc": "2026-02-18 15:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62gidn",
          "author": "HarlanWJK",
          "text": "Just read the RL blog mentioned. The way they've optimized for tool-use instead of just conversational fluff is exactly what we need for real dev workflows.",
          "score": 1,
          "created_utc": "2026-02-18 15:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62gwh7",
          "author": "Anders0725",
          "text": "Honestly, the \"bloated transformer\" phase was getting exhausting. Glad to see someone actually prioritizing the compute-to-output ratio.",
          "score": 1,
          "created_utc": "2026-02-18 15:31:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62ia5r",
          "author": "vvkkrr",
          "text": "76.3% on BrowseComp is the real sleeper stat here. Most of these \"huge\" models are surprisingly bad at actually navigating the web to find documentation.",
          "score": 1,
          "created_utc": "2026-02-18 15:38:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62iqg8",
          "author": "lxneay",
          "text": "Finally, a model that doesn't feel like it's burning a forest just to help me debug a React component.",
          "score": 1,
          "created_utc": "2026-02-18 15:40:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62k1ij",
          "author": "TTRY01",
          "text": "Is anyone else seeing the same consistency on the 10B MoE? If it's actually hitting those benchmarks, my API costs are about to drop 80%.",
          "score": 1,
          "created_utc": "2026-02-18 15:46:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62kki3",
          "author": "Llywelynerd",
          "text": "The \"Real World Coworker\" framing is spot on. I don't need a philosopher; I need a tool that can use other tools efficiently without timing out.",
          "score": 1,
          "created_utc": "2026-02-18 15:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62lmop",
          "author": "CNMBUS",
          "text": "The brute force era dying is the best thing that could happen to local-first dev. 10B is actually manageable.",
          "score": 1,
          "created_utc": "2026-02-18 15:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62m10d",
          "author": "nbcb333",
          "text": "I've been testing M2.5 against some of the larger legacy models this morning. The logic density in a 10B parameter setup is honestly impressive.",
          "score": 1,
          "created_utc": "2026-02-18 15:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62m97o",
          "author": "wzl1985",
          "text": "M2.5 seems to be the sweet spot. Itâ€™s small enough to be fast but smart enough to actually handle task decomposition without getting lost in its own weights.",
          "score": 1,
          "created_utc": "2026-02-18 15:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62mkvb",
          "author": "deerrek",
          "text": "It's about time we focused on RL for correctness rather than just RLHF for politeness. Minimax feels like it was built for engineers, not HR departments.",
          "score": 1,
          "created_utc": "2026-02-18 15:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62o3rv",
          "author": "MarinaCER87",
          "text": "$1 for an hour of intensive work? That's basically the price of a cheap coffee for a senior-level coding assistant. Hard to argue with that ROI.",
          "score": 1,
          "created_utc": "2026-02-18 16:04:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62pkoo",
          "author": "19880331",
          "text": "If M2.5 is the benchmark for the \"10B MoE Era,\" then the future of sustainable AI actually looks promising for once.",
          "score": 1,
          "created_utc": "2026-02-18 16:11:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8awjx",
      "title": "Best Coding Model?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "author": "I_like_fragrances",
      "created_utc": "2026-02-18 18:23:42",
      "score": 23,
      "num_comments": 20,
      "upvote_ratio": 0.87,
      "text": "What is the best model for general coding. This includes very large models too if applicable.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o63oc6q",
          "author": "No_Clock2390",
          "text": "Qwen3-Coder-Next",
          "score": 24,
          "created_utc": "2026-02-18 18:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o644cn8",
              "author": "txgsync",
              "text": "This is the answer. At any affordable home-gamer size (<128GB) Qwen3-Coder-Next (80B-A3B) is the tits.\n\nEdit: and unlike Qwen3-Coder-30B-A3B it doesnâ€™t give me the â€œfuck off, I just write code, figure it out yourselfâ€ energy when planning features. And -Next does better at ZorkBench, not just wandering around with a nasty knife waiting for something to happen until it rage-quits.",
              "score": 7,
              "created_utc": "2026-02-18 19:58:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o677w1y",
                  "author": "Sax0drum",
                  "text": "How much context can you reasonably run with 128GB?",
                  "score": 3,
                  "created_utc": "2026-02-19 06:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o63q8bi",
              "author": "huzbum",
              "text": "Awesome, I need to try this one on some coding tasks, because I can actually run it with a large context and decent TG speed.  (Haven't tested PP)",
              "score": 1,
              "created_utc": "2026-02-18 18:54:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66ntkk",
                  "author": "DifficultyFit1895",
                  "text": "Itâ€™s worked well for me. Itâ€™s also the only local model Iâ€™ve ever tried that 100% answers correctly this one tricky question I ask it about a ~30,000 word novella with the full story in context.",
                  "score": 1,
                  "created_utc": "2026-02-19 04:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64p8ry",
              "author": "jinnyjuice",
              "text": "For what languages are you using it for?\n\nThis is my secondary. GLM 4.7 Flash is better with coding the web UI.",
              "score": 1,
              "created_utc": "2026-02-18 21:36:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64pgm9",
                  "author": "No_Clock2390",
                  "text": "It supports all languages",
                  "score": 1,
                  "created_utc": "2026-02-18 21:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65dej5",
              "author": "National_Cod9546",
              "text": "What IDE / plugins are you using?Â ",
              "score": 1,
              "created_utc": "2026-02-18 23:35:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o655yof",
          "author": "Faultrycom",
          "text": "People will argue over different models but have in mind that it's about local - and here qwen 3 coder next shines as it can run on not so expensive stack.",
          "score": 3,
          "created_utc": "2026-02-18 22:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63rdtm",
          "author": "huzbum",
          "text": "I haven't used it (beyond confirming I can run it), but I hear great things about MiniMax M2.5.  I use GLM 4.7, and GLM 5 every day, but cloud hosted.  I thought about building something to run GLM 4.5/4.6/4.7, but never got beyond thinking about it... now GLM 5 is twice as big as GLM 4, definitely not running that, but maybe they'll make something intermediate in the 5 family.  \n\nI should try out GLM 4.7 flash for some real tasks, but I haven't gotten around to it yet.  ",
          "score": 3,
          "created_utc": "2026-02-18 18:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64lph1",
          "author": "Grouchy-Bed-7942",
          "text": "If you want something that runs for â‚¬6k for agentic code, MiniMax 2.5 with VLLM on 2x ASUS GX10 (equivalent to DGX Spark), you get 2000 tokens/sec of PP without context and you drop to 600/400 tokens/sec of PP at 100k context. Output of about 30 tokens/sec at the start and drops to 15 tokens/sec.\n\nWith Qwen3-Coder-Next, certain tool calls fail after 50k context.",
          "score": 3,
          "created_utc": "2026-02-18 21:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o679pq1",
          "author": "Only_Comfortable_224",
          "text": "Not directly related but for serious coding, is Claude more cost effective?",
          "score": 1,
          "created_utc": "2026-02-19 06:49:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64r4bi",
          "author": "Potential-Leg-639",
          "text": "Pick a strong model for architecture/planning and Qwen3 Coder 30B or GLM 4.7 Flash can then do the coding quite good.\n\nMakes quite a difference in coding quality.",
          "score": 1,
          "created_utc": "2026-02-18 21:45:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65bhr7",
              "author": "ijontichy",
              "text": "> Pick a strong model for architecture/planning\n\nCan you provide an example of this?",
              "score": 2,
              "created_utc": "2026-02-18 23:24:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o664ule",
                  "author": "voyager256",
                  "text": "I think he meant non-local / subscription based , big model like Claude 4.6",
                  "score": 3,
                  "created_utc": "2026-02-19 02:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67nzfl",
              "author": "alokin_09",
              "text": "Yep, this is basically my workflow in Kilo Code. I use a premium model like Opus for the architecture stuff, then switch to something like GLM, MiniMax or Qwen for the smaller tasks. Opus creates really detailed plans that are easy to hand off to the cheaper models.",
              "score": 2,
              "created_utc": "2026-02-19 09:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o681d94",
                  "author": "Potential-Leg-639",
                  "text": "I selfhost GLM 4.7 Flash/GPT-OSS-120b/Qwen 3 Coder on a Strix Halo, that do the coding and cant run out tokens. Planning/Architecture  in Opus.\n\nBut i switched completely to OpenCode.",
                  "score": 1,
                  "created_utc": "2026-02-19 11:09:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ppfn",
          "author": "PooMonger20",
          "text": "From my own testing and not benchmarks, on consumer level HW - GPT-OSS-20b is the closest I was able to get to the online equivalents. Everything else is either too slow, generates trash that doesn't even compile, endless syntax errors or straight out misses half the functions you asked for.",
          "score": 0,
          "created_utc": "2026-02-19 09:20:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3rp11",
      "title": "Are there truly local open-source LLMs with tool calling + web search that are safe for clinical data extraction? <beginner>",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r3rp11/are_there_truly_local_opensource_llms_with_tool/",
      "author": "Kitchen_Answer4548",
      "created_utc": "2026-02-13 15:11:26",
      "score": 17,
      "num_comments": 24,
      "upvote_ratio": 0.85,
      "text": "Hi everyone,\n\nI'm evaluating open-source LLMs for extracting structured data from clinical notes (PHI involved, so strict privacy requirements).\n\nI'm trying to understand:\n\n1. Are there open-source models that support **tool/function calling** while running fully locally?\n2. Do any of them support **web search capabilities** in a way that can be kept fully local (e.g., restricted to internal knowledge bases)?\n3. Has anyone deployed such a system in a HIPAA-compliant or on-prem healthcare environment?\n4. What stack did you use (model + orchestration framework + retrieval layer)?\n\nConstraints:\n\n* Must run on-prem (no external API calls)\n* No data leaving the network\n* Prefer deterministic structured output (JSON)\n* Interested in RAG or internal search setups\n\nWould appreciate architecture suggestions or real-world experiences.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r3rp11/are_there_truly_local_opensource_llms_with_tool/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o56pozk",
          "author": "former_farmer",
          "text": "Models below 30B struggle with tool calling. Quantized models as well. That's my experience. Try on 30B-80B models.",
          "score": 6,
          "created_utc": "2026-02-13 16:17:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56zrek",
              "author": "AppointmentAway3164",
              "text": "Sadly I agree. I continue to try to get 8B models to work in opencode. Havenâ€™t gotten good results yet.",
              "score": 2,
              "created_utc": "2026-02-13 17:05:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o588vnz",
          "author": "Impossible-Glass-487",
          "text": "Please don't cut your teeth using your patient data.Â  This sub is full of amateurs and these comments are the blind leading the blind.Â  This is honestly a very disturbing thread.",
          "score": 6,
          "created_utc": "2026-02-13 20:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59vm5n",
              "author": "Celtic_Macaw",
              "text": "I work in cyber security and was thinking the same thing. People who have actually implemented these systems probably can't even provide insight on this either because of NDAs and how strict the regulations are on sharing information like this... on a public forum no-less. Really playing with fire here.",
              "score": 2,
              "created_utc": "2026-02-14 02:17:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bnh7x",
                  "author": "GCoderDCoder",
                  "text": "Having worked in clearance required spaces for over a decade, you're allowed to discuss objective tech use just not \"at this organization they have this app that does this\". Instead you would say vllm with this batch setting increases pp accelerating data ingestion or something. Maybe discussing mcp management tools options and workflow management from a specifications angle would address this OP's questions. A proprietary technology is not what is being discussed here rather it's more fundamental software tools and methods being discussed and how to approach common regulations. That doesn't require sharing private information it just helps reduce the amount of work the people coming behind you have to do to get the same right publicly available answers. \n\nI think the bigger issue in tech is tech snobs who want to prevent people who they think are lesser from engaging in deeper tech topics. As a career convert who came from another science field to comp sci I've noticed there is a split between the open source folks and the folks who think if you're not in research or working on some ground breaking thing then your trash. \n\nTo be clear I think you mostly expressed genuine curiosity so I'm not bashing you but I get annoyed by people who constantly criticize without offering insight. People are here to learn, teach, or be jerks. Too many of the latter linger around. There is no lock on information these days. The hard part is there is a lot of disinformation too. So if you know better then tell people how to do better.\n\nIf you see someone say something problematic correct it and then the butterfly effect happens. There's propagation all sorts of ways so when people who know better spend the time to hold their nose up but not offer insight then the less informed people get to spread misinformation unchecked multiplying the negative because people who know better choose unproductive criticism.",
                  "score": 5,
                  "created_utc": "2026-02-14 11:14:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56d7yc",
          "author": "an80sPWNstar",
          "text": "you can get just about any LLM + mcp + other tools to stay 100% local without a problem. If you want to use an internally hosted site as a search/wiki instead of the interwebs, you'll need to either create your own or work with the devs to access the API they use for tool calling. ",
          "score": 3,
          "created_utc": "2026-02-13 15:17:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o589qvr",
          "author": "Minimum-Two-8093",
          "text": "Are you talking about actual clinical data (as in medical data), or are you talking about the method of extraction being clinical (as in logical and procedural)?\n\nThe chosen nomenclature makes it challenging to know which path you're going down. \n\nIf it's patient data, and it must be ring fenced for security and privacy reasons, they should have the budget to not half arse the local hardware and infrastructure. \n\nI don't think that's the case though due to your beginner tag. I think you're actually trying to use local models with reliable results. Unfortunately, if that's the right read, even 30 billion parameters isn't enough to do both tool usage, and reliable data manipulation and validation. In saying that, give it another year and quantization will catch up making previously janky solutions far more usable.",
          "score": 2,
          "created_utc": "2026-02-13 20:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59hweq",
              "author": "moderately-extremist",
              "text": "He mentions HIPPA and a \"healthcare environment\" so I think he means \"clinical\" as in a medical clinic's data.",
              "score": 1,
              "created_utc": "2026-02-14 00:50:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57kndf",
          "author": "newz2000",
          "text": "https://preview.redd.it/ayi23osb3bjg1.png?width=1400&format=png&auto=webp&s=d59a3694698c94bee1a591c9d187ee62fc1100e4\n\nI have a similar use case. I am an attorney and while I don't have to deal with HIPAA and getting BAAs, my obligations for client confidentiality are similar to yours.\n\nI have been benchmarking and writing about my experience here and in r/ollama but I haven't shared the graphic above.\n\nData extraction is easy, many simple models can do it. However, the commercial models hosted on the commercial clouds are just so heavily optimized for the tasks. All five of the models above succeeded, though when it came to quality, Gemini Flash (not Flash Lite) produced the best results at a slightly higher cost than is in that chart. And it can handle 51,000 documents in about Â½ hour for under $10.\n\nTool calling is a different story. I have not benchmarked this and compared the various options in detail, but I can tell you that it requires a lot more effort and a larger context size. On one test run, I did, a document extraction and summarization task with gpt-oss-20b took 20s but a tool call task and summarization took a little over 6 min. I have not tested it with Gemini 2.5 Flash which says it supports function calling and code execution. That may be different than what I want, which is using an MCP server. ",
          "score": 4,
          "created_utc": "2026-02-13 18:45:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o588hvy",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 7,
              "created_utc": "2026-02-13 20:43:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65ebhq",
                  "author": "IDoDrugsAtNight",
                  "text": "Read: You sir are a hacker and/or enthusiast and we in cybersecurity prefer morons and luddites to people who like technology.",
                  "score": 1,
                  "created_utc": "2026-02-18 23:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57kvrk",
          "author": "productboy",
          "text": "This is solid small model for tool calling:\n\nollama run qwen3:8b\n\nYour mileage may vary; i.e. depends on tools called and your prompts.",
          "score": 2,
          "created_utc": "2026-02-13 18:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56eky5",
          "author": "Suspicious-Walk-4854",
          "text": "Why would you need an open source local LLM for this though? Google Vertex AI for example is HIPAA-compliant, so what problem are you solving for here? Iâ€™ve worked with multiple healthcare providers deploying EHRs on public cloud and using Vertex models for different use cases.",
          "score": 2,
          "created_utc": "2026-02-13 15:24:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56h3gr",
              "author": "Kitchen_Answer4548",
              "text": "I agree Vertex AI is HIPAA-eligible.\n\nThe reason Iâ€™m exploring Local LLM is mainly around:\n\n* Avoiding cloud dependency for large-scale batch extraction \n* Full control over model weights and fine-tuning\n* Also we have HPC\n\n",
              "score": 5,
              "created_utc": "2026-02-13 15:36:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56hd3j",
                  "author": "Suspicious-Walk-4854",
                  "text": "Just admit you want to play with your models my guy, this is a safe space ðŸ˜",
                  "score": 13,
                  "created_utc": "2026-02-13 15:37:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o593gqk",
          "author": "tartare4562",
          "text": "I'm using qwen3:32_q8 with fair to good results for an almost exact copy of what you're asking, it's the local assistant to a small company. It uses 40gb of vram with context, but it could probably run fine on q6 also. q4 was having troubles dealing with numbers and simple math. Runs on a RTX pro 5000 blackwell. Stack is ollama+open webui.",
          "score": 1,
          "created_utc": "2026-02-13 23:23:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bocis",
          "author": "Ok-Swim9349",
          "text": "you should check this : [https://github.com/2501Pr0ject/RAGnarok-AI](https://github.com/2501Pr0ject/RAGnarok-AI)\n\nI'm the author.  \nIt won't build your RAG pipeline, but it will help you measure and validate it before deploying in a HIPAA environment. Being able to prove retrieval precision and hallucination rates is often required for compliance.",
          "score": 1,
          "created_utc": "2026-02-14 11:22:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bzeeq",
          "author": "rcanand72",
          "text": "Try these - https://ollama.com/search?c=thinking&c=tools - all of them are local models that support tool calling and thinking (except the cloud variants, skip the cloud version, pick any other that fits). First compare the gpu or unified ram available on your machine. If you have less than 64gb ram then divide by half to get a safe model size for your machine (for over 64gb ram systems, just subtract 20gb instead). Start from the top of that list, click on each model. Click view all and see the sizes, find a model variant that fits. Try it out. You can download the model and chat in the ollama app, or if comfortable with terminal, enter the command â€œollama run model_idâ€ where model_id is the id of the model. Find the first model that fits and works for your use cases. Thatâ€™s the model you want.",
          "score": 1,
          "created_utc": "2026-02-14 12:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c3l4u",
          "author": "Torodaddy",
          "text": "Why are you using an llm?\n\nAnd tool calling defeats the purpose of keeping the data local, you just exposed it",
          "score": 1,
          "created_utc": "2026-02-14 13:23:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5g065u",
          "author": "Cuaternion",
          "text": "30B and up if you want a clean job",
          "score": 1,
          "created_utc": "2026-02-15 02:32:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56yqb8",
          "author": "Ambitious_Spare7914",
          "text": "It's going to cost a lot to get the hardware you need.",
          "score": -2,
          "created_utc": "2026-02-13 17:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57q6f2",
              "author": "Fishmonger67",
              "text": "Like how much would it cost to do this?",
              "score": 1,
              "created_utc": "2026-02-13 19:12:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o58p12h",
                  "author": "Ambitious_Spare7914",
                  "text": "$25k low end",
                  "score": -1,
                  "created_utc": "2026-02-13 22:04:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r5zz0a",
      "title": "Point and laugh at my build (Loss porn)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r5zz0a/point_and_laugh_at_my_build_loss_porn/",
      "author": "Diligent-Culture-432",
      "created_utc": "2026-02-16 04:32:08",
      "score": 16,
      "num_comments": 38,
      "upvote_ratio": 0.91,
      "text": "Recently fell into the rabbit hole of building a local and private AI server as affordably as possible, as someone whoâ€™s new to building a PC and running models locally. But turns out itâ€™s so slow and power inefficient to the point that itâ€™s been completely demoralizing and discouraging. Originally had a dream of having personal intelligence on tap at home, but doesnâ€™t seem worth it at all compared to cheap API costs now. Not a shill for cloud providers, but just a confession that I need to get off my chest after weeks of working on this.\n\n1x 2060Super 8GB, $0 (owned)\n\n2x 5060Ti 16GB, $740\n\n8x 32GB DDR4 3200 RAM, $652\n\n3945WX cpu, $162.50\n\nMC62-G40 mobo, $468\n\nCPU cooler, $58\n\n2TB NVMe SSD, $192\n\n1200W PSU, $130\n\nPC Case, $100\n\nTotal RAM 256GB running at 3200\n\nTotal VRAM 40GB\n\nTotal cost $2500\n\nMinimax M2.5 8\\_0 with context size 4096 via llama.cpp Vulkan, on Ubuntu, 3.83 tokens/second\n\nFinal conclusion that this time and effort was all for naught and a reminder of my own foolishness: priceless â˜¹ï¸\n\nEDIT: corrected PSU to 1200W, not 120W\n\nEDIT 2: included OS",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r5zz0a/point_and_laugh_at_my_build_loss_porn/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5mxvyu",
          "author": "p_235615",
          "text": "You can still run pretty good models on that... Many stuff like glm4.7-flash at q8_0, qwen3-coder:30b q8_0 and many other. Possibly even qwen3-coder-next:80b q4 with decent speeds. Also qwen3.5 35B should be released soon...\n\nIf you really want to run minimax-m2.5 you should first try lower quants like IQ4_XS. I tried it on a machine with 128GB DDR5 and RTX 6000 PRO 96GB, but I still got only 9.7tok/s with 22%CPU/78%GPU allocation.",
          "score": 5,
          "created_utc": "2026-02-16 05:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n6bhw",
          "author": "VaporwaveUtopia",
          "text": "I like your sense of humour around this, but I'd argue it isn't really a loss. Your rig's plenty powerful enough to experiement with developing useful agents. \n\nAt the end of the day, if you decide its not for you, you can always sell the hardware for close to what you paid for it.",
          "score": 4,
          "created_utc": "2026-02-16 06:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o650bwg",
              "author": "or1gb1u3",
              "text": "If not more. The pc I built in September for gaming and some local inference and image gen(the ai stuff was started after gaming build) I could probably make money on. The ram cost has like tripled from when I bought. Then I learned about ram gateÂ ",
              "score": 1,
              "created_utc": "2026-02-18 22:27:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nb9o8",
          "author": "No_Night679",
          "text": "You mean 1200W PSU?",
          "score": 5,
          "created_utc": "2026-02-16 07:37:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62fbf7",
              "author": "Diligent-Culture-432",
              "text": "Yep, thank you for the correction, edited",
              "score": 1,
              "created_utc": "2026-02-18 15:24:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5n2s9n",
          "author": "FitAstronomer5016",
          "text": "Dude, that seems really low performance wise.\n\nMay I ask why you're using Vulkan llama.cpp and not regular/CUDA llama.cpp?\n\nWhat is your script/command line argument that you're using for llamacpp?\n\nWhat is the main GPU being used? The 2060 unfortunately is very poor and if that's the main it's going to be bottlenecking the 5060tis which have more than double the memory bandwith.\n\nAre you offloading the experts onto CPU and attention tensors on your GPUs? Or are you going by strictly layers?\n\nHave you tried Q4? Minimax seems to be pretty efficient at Q6 and it kinda goes marginal from there so you can also try that with higher context.\n\nQuite a few questions but you really shouldn't be getting that performance for your system. I wouldn't expect SOTA level but definitely a usable experience.",
          "score": 3,
          "created_utc": "2026-02-16 06:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p8kpm",
              "author": "sleepy_roger",
              "text": "This does feel a little slower than expected, Deepseek should hit 10-15 range.\n\nLike you said the 2060 is also pulling is 5060's down a little well with only 336.0 GB/s vs 448.0 GB/s for the 5060's, they're system ram can hit what like 205 GB/s _max_ but is probably going to hover at 150-180 ish in general this is just a speed bottle neck.\n\nI agree with you though they should offload specific layers to get a speed boost and DEFINITELY try q_4 or q_6.\n\nHowever like OP said they really didn't research and while they can squeeze some more performance out of their machine they're not going to hit crazy high tk/s on Sota models unless they make some significant changes.",
              "score": 2,
              "created_utc": "2026-02-16 15:52:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62fx47",
              "author": "Diligent-Culture-432",
              "text": "Iâ€™m pretty new to the so I simply havenâ€™t had a chance to build llama.cpp for cuda since Iâ€™ve been away from home. Iâ€™ve havenâ€™t been using any flags on llamacpp since I assumed model-fit-params would help take care of optimization. I think the 2060super has similar bandwidth to the5060ti but I have to double check. Iâ€™m trying to max out my gear to maximize intelligence, so Iâ€™m trying to limit quantization when possible",
              "score": 1,
              "created_utc": "2026-02-18 15:27:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o638m69",
                  "author": "FitAstronomer5016",
                  "text": "From a technical standpoint, Minimax Q5KM has the most efficient PPL and KD from the tests I've looked at (essentially meaning its very close to the F16 model), with Q8 only being marginally better but alot more demanding>\n\nThe 2060 Super has a lower memory bandwith (not counting Blackwell efficiencies that the 50 series has) so it really shouldn't be your main GPU. Add it in after the 5060TIs (486GB/s vs 306GB/s) to push an increase on your performance.\n\nI understand the use case for wanting the smartest model and the buyer's remorse (although honestly you really didn't lose anything considering all your parts appreciated and you can just easily sell the important parts for atleast cost) but research on inference should've been the priority. For consumer rigs, you will most likely never be running full precision of these big MoEs, and understanding that becomes important. You need to atleast have a layman's understanding of how to look for effective quant types for your system and how to run it.\n\nHere's my recommendation for you to try\n\n\\-Compile llama.cpp for CUDA. This is a must (if you don't want to compile, find a compiled binary and just use that. Make sure it supports Minimax 2.5)\n\n\\- -fit param should work, but my guess is that it's prioritizing your CUDA0 (the 2060) and that has to go. You need to make sure the 5060tis are top priority then the 2060 then your CPU RAM. Because Minimax is requiring essentially most of your RAM, I wouldn't even bother with three GPUs, as the benefit is marginal.\n\nAdd these params:   \n\\-ctk q8\\_0 (quantizes the cache)  \n  \n \\-b 1024 -ub 1024 (increases batch size, for CPU heavy moe inference, this should provide a boost in your prompt processing speed)   \n  \n\\-ot exps=CPU (this will offload all the exps onto your CPU, leaving the attention and shared exps open)   \n  \n\\-ngl 99 (whatever the layer count is, only max it out if you have ran -ot exps=CPU.)  \n  \n \\-ot attn=CUDA0 (if you dont want to run -ot exps=CPU or experimenting, run this param instead. You then need to be specific with the layers you are running with -ngl. This would take advantage of your additional VRAM, BUT the gain will most likely be marginal.)",
                  "score": 1,
                  "created_utc": "2026-02-18 17:37:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mweye",
          "author": "Much-Researcher6135",
          "text": "Remember that the majority of employees at these big AI companies are *software engineers*, not hardware engineers. You failed to mention a database, chunking, RAG, embedders, reranking, an agentic layer/loop, prompt engineering, a chat API, audio models, etc. Are we to assume you threw a bunch of hardware at what is, at bottom, a software engineering problem?\n\nYou can create [seriously useful systems](https://old.reddit.com/r/LocalLLM/comments/1r26mw9/getting_ready_to_send_this_monster_to_the/) with far less hardware power than you have there. You'd simply need to realize the roles and limitations of various HW/SW components and engineer accordingly.\n\nI'll totally admit it's a lot of work, however. I'm currently going nuts building mine. Then again, I care very little about having a chat bot which can reorder my laundery detergent. And I care enormously about having an agent that can, in less than a minute: iteratively search, download and synthesize 100 web pages, all while simultaneously doing multi-hop hybrid RAG search, reranking and retrieval from hundreds of books and academic papers. Future versions will have things like geospatial intelligence, personal planning, and a graph database with entity extraction/resolution loops.\n\nThis disparity of purpose is probably why we don't have nice convergent software projects that just do it all for us. At present, it seems like you've got to build the (software) system you want. But maybe if you figure out what you want out of your system, you'll be able to find an agentic FOSS software project that can simply do what you want?\n\nYou'd certainly need to drop to a smaller model like the 30b a3b model in the post I linked or, for deep research and synthesis projects like mine, maybe a dense 32b qwen3 model plus wicked good ancillary models suited to your needs (embedder, reranker, tts and stt/whisper, book extraction neural nets like `marker`, etc). But don't write off these smaller LLMs. They are ridiculously good if placed at the helm of a really solid agentic framework. Mine is getting SCARY GOOD at its core tasks. SCARY. GOOD.",
          "score": 6,
          "created_utc": "2026-02-16 05:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n7otj",
              "author": "No-Consequence-1779",
              "text": "What does â€˜synthesize web pagesâ€™ mean?Â ",
              "score": 0,
              "created_utc": "2026-02-16 07:04:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n8xqc",
                  "author": "Much-Researcher6135",
                  "text": "Synthesize, intransitive verb: \n\n> To combine so as to form a new, complex product.",
                  "score": 1,
                  "created_utc": "2026-02-16 07:15:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pa9kr",
          "author": "Hector_Rvkp",
          "text": "Did you consider a Strix halo w 128gb ram? It would be slower, but \"cheap\" and power efficient. Your setup sounds like nightmare fuel, worrying about layers on 3 different GPUs, slow af ram, but so much of it that you feel that you have to use it, heat, noise, power bill, form factor... I'm considering buying one myself, knowing it's kind of slow, but competent at various tasks and ticks a lot of boxes for me. With hardware prices now, you can probably make money selling your components?",
          "score": 2,
          "created_utc": "2026-02-16 16:00:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62gc46",
              "author": "Diligent-Culture-432",
              "text": "Strix halo did look really great, but I tried to maximize the hardware I already had as much as possible to maximize capacity, and I figured more ram+vram for similar price for lower speed was a sacrifice I was willing to make",
              "score": 1,
              "created_utc": "2026-02-18 15:29:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o62lcrm",
                  "author": "Hector_Rvkp",
                  "text": "I understand, and can relate, as i first tried to use my own 32GB of DDR5 5600 RAM, only to realize it's unusably slow. My bandwidth is 2x yours, and you have 8x more of it. It means that it's (ghetto math) up to 16x slower than my system, and my system is unusably slow. your VRAM is what saves you, but the moment you spill out, you're doomed. And with that much RAM, the only logical thing to do is in fact to spill out. So there may be tricks to play with a strictly enforced MoE model, and leaning hard on speculative decoding. Did you try that, or have you given up? Idk how often an MoE would rotate active parameters, and you wouldnt go far housing 2 models + context in that VRAM, but if you're hell bent on making it work... If you're unfamiliar w speculative decoding, ask gemini for eg, provide your VRAM + RAM specs, and ask which MoE + speculative decoding model combo you could run to maximize the intersection of intelligence + not spilling out of VRAM. Do say Feb 26 in your prompt, because models drop every day. ",
                  "score": 1,
                  "created_utc": "2026-02-18 15:52:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ms7j3",
          "author": "LithiumToast",
          "text": "What were you expecting? I'm sure you can use local AI to help you do certain things no?",
          "score": 1,
          "created_utc": "2026-02-16 04:56:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62gmv9",
              "author": "Diligent-Culture-432",
              "text": "I was hoping for something cheaper and more private than cloud. But the speed and electricity costs are very suboptimal",
              "score": 1,
              "created_utc": "2026-02-18 15:30:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o63eog2",
                  "author": "LithiumToast",
                  "text": "I have gotten distilled models to work quite well locally at 8B and 16B parameters. There is a delay of maybe 5 seconds per prompt with a speed of tokens per second that is about reading speed.\n\nI would suggest messing with variables and various setttings for the models for your hardware form factor.",
                  "score": 1,
                  "created_utc": "2026-02-18 18:03:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mseyk",
          "author": "Used_Chipmunk1512",
          "text": "Still not bad, you can use this for setting multiple smaller models working in concert\n\nEdit: maybe this post can help you - https://www.reddit.com/r/LocalLLM/s/tdfQgDJilO",
          "score": 1,
          "created_utc": "2026-02-16 04:57:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5msi1d",
          "author": "duplicati83",
          "text": "Thatâ€™s a great setup. I have a more modest setup, but I use mine for n8n workflows.",
          "score": 1,
          "created_utc": "2026-02-16 04:58:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n6v84",
          "author": "No-Consequence-1779",
          "text": "If you can return it all and just get a used pc off facebook or eBay. Then simply add a gpu. Â Or go the ini pc route Â - 1300$ or so.Â \n\nLocal is usually about learning, building custom software to do things (Python or whatever), or using agents, then privacy but thatâ€™s usually bs for most.Â \n\nI run local for the above plus I burn a million tokens per day with my stock and crypto trading bots.Â \n\nDiscounting all tat, youâ€™re right. Frontier is better for most people.Â \n\nYou need to run the model 100% in vram. There are hundreds of options. Qwen3 work well for most things. Lm studio makes it easy to browse and evaluate models.Â ",
          "score": 1,
          "created_utc": "2026-02-16 06:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5serfx",
              "author": "Hefty_Development813",
              "text": "You are doing crypto trading bots that are profitable with LLM for research? Or it's even a transformers price prediction model? Always curious about anyone who claims to have this type thing going. Seems like the holy grail if it works. Electricity in, profit outÂ ",
              "score": 1,
              "created_utc": "2026-02-17 01:23:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ssyie",
                  "author": "No-Consequence-1779",
                  "text": "Generally yes. Trading takes a type. Not getting fomo and buying too high or at the peak.Â \n\nYes. Absolutely. I have two methods. Both do not involve an LLM; though I tried.Â \n\nBut, using an LLM to generate â€˜complicatedâ€™ scripts. A lot of trading maths â€¦ probably simple for a data scientist.Â \n\nPrerequisites. Coinbase api and account with key with access to create orders. Python.Â \n\nMethod 1.Â \nGainers. Obvious to spot with human eyes - after the fact. Script to identify gainers early. Using multiple data points.Â \nI can into those if you are serious. Â \nGainers are the 3,4,5+ X plays.Â \n\nUSELESS-USD\n\nMethod 2.Â \nRegular buy low sell high. Buying low before the season starts. Like NOW.Â \nHistorical data shows how high. Itâ€™s usually very close for the alts I pick.Â \n\nSOL: 2-3x.Â \nTOSHI: 5x\nOMNI. I just did a play earlier today. Was one of the gainers.Â \n\nMany others. Just make sure itâ€™s not spiking because it will be delisted (I got screed lie this) at midnight AND the market cap is large enough for you to sell.Â \n\nI thought a reasoning LLM would be good at it but, the LLM just ends up using the maths lie order book , volume, etc.Â \n\nIâ€™m currently putting together a better code base and gui to do these as python is a scripting language and terrible with large code bases.Â \n\nIâ€™ve tried various method like valet and peak detection like a person but the math predicts it much better and before it hits the visuals.Â \n\nThe data points used\nOrder book asks and bids Â  And the spread.Â \nAsk volume , bid volumeÂ \nVolume imbalance (calculated from those)\n\nAverages bid and ask\nPrice difference percentageÂ \nSpread percentageÂ \n\nThis generates a buy sell hold signalÂ \n\nGainers check all cryptos. It uses volume, gain percentage , and detects the spike Â in a short time period.Â \n\nBoth price change (above) and a pump detection is used for gainers.Â \n\nNo, I will it even post this publicly. Yes Iâ€™ll give it to family or friends (when the windows WPF app is ready). Or people that ask (means they are usually serious and will figure it out in time).Â \n\nThe failure of the LLMs is that they are non deterministic. And people think they can feed a trend of candles or screenshot and it will predict the future.Â \n\nYes it can understand both. But itâ€™s this underlying data that is much more important. Â I tried this approach also.Â \n\nTe LLM did essentially tutor me on trading and described the data points I was oblivious towards because I did not know.Â \n\nAre math people secretly making bank doing this? Only if the know how to write the scripts or know he to instruct an LLM.Â \n\nCheck out the order books. Youâ€™ll see these repeated quantities over and over- itâ€™s the bots. They are out there. It is possible or they wouldnâ€™t exist.Â ",
                  "score": 1,
                  "created_utc": "2026-02-17 02:48:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5p6on6",
          "author": "sleepy_roger",
          "text": "Eh don't beat yourself up over it. You can still run some decent local models like others have said. In reality you don't really want your models spilling into ram, it's way too slow. You've got a solid 32gb of usable vram (2060 is kind of eh). With 32gb you can run quite a few good decent models. \n\nOnly being $2500 into it isn't bad at all.. I'd try to grab at least 1-2 3090s if you can, or if you catch a 5090 FE I wouldn't pass it up that will up what you can do quite a bit especially on the image/video generation front as well.",
          "score": 1,
          "created_utc": "2026-02-16 15:44:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r33rh",
          "author": "StaysAwakeAllWeek",
          "text": "One thing you can try with a build like this is extreme speculative decoding. Try running qwen 235b on the CPU with both GPUs dedicated to drafting thousands of tk/s with 0.6b or 1.7b, try to hit the highest acceptance rate you possibly can. You can do this in LM Studio pretty easily. I wouldn't be surprised to see a 5x speedup in 235b compared to without it. In coding tasks with low temperature settings it can sometimes get even higher\n\nYou should also heavily focus on getting small subagents for a large CPU model running inside the vram of individual GPUs",
          "score": 1,
          "created_utc": "2026-02-16 21:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r7z46",
          "author": "LankyShape8399",
          "text": "Iâ€™m so confused right now.  I come here an rtx pro enjoyer thinking Iâ€™d see something similar but this seems like a joke",
          "score": 1,
          "created_utc": "2026-02-16 21:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rhkck",
          "author": "mac10190",
          "text": "Definitely recommend trying out q4 quantized models as the drop in intelligence is negligible (like 3-5% compared to full weights) and the memory (VRAM) requirements will drop drastically.\n\nHonestly the 2060 is really dragging those dual 5060Ti 16GB GPUs  down. I'd ditch the 2060, keep the dual 5060Ti 16GB GPUs and try out some q4 quantized models. I think you'll be pleasantly surprised if just how much performance you can get out of those 5060s. I started my home server with a single 5060Ti 16GB and I was blown away by how much it could do. In my testing the 5060Ti was able 1/4 the speed of a 5090 I'm terms of t/s.\n\nAlso make sure when you move down to the q4 models that it's not splitting the model across both GPUs unless absolutely necessary. Like if a model is only 9GB it will fit into VRAM on a single GPU, but sometimes Ollama and lamma.cpp will split the model anyways if you give it too much context.\n\nTry that out, definitely let me know how it goes! Best of luck fellow home-laber!",
          "score": 1,
          "created_utc": "2026-02-16 22:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s30b6",
          "author": "Bino5150",
          "text": "What are your goals for your local AI?",
          "score": 1,
          "created_utc": "2026-02-17 00:14:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62fa8f",
              "author": "Diligent-Culture-432",
              "text": "Private local inference with large models for cheap to power personal agents like openclaw or otherwise",
              "score": 1,
              "created_utc": "2026-02-18 15:24:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o62nhwu",
                  "author": "Bino5150",
                  "text": "OpenClaw runs like shit with local models. There are other agents out there like AnythingLLM that will run good with local LLMâ€™s. You can run something in the 8b q4-8 to 20b q4 range with really good results without having to spend a gazillion dollars building a super computer. I run locally on my laptop. If you just want to chat with it, you can do that directly in LM Studio. If you want it capable of more agentic tasks, you can use AnythingLLM, PicoClaw, n8n, role the like to give it some functionality. If youâ€™re trying to build something like Claude Code, Kimi K2, or something of that nature that runs locally for free on your hardware, you will forever be chasing the dragon. If you really want to run something larger without breaking the bank on building a server, you could always go the Mac route and take advantage of Appleâ€™s unified memory. A Mac Mini can give you up to 64GB of vram and a Mac Studio up to 512, and the Macâ€™s can be clustered so your capabilities could be easily upgraded later and scaled for larger models.",
                  "score": 1,
                  "created_utc": "2026-02-18 16:01:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5sex3w",
          "author": "Hefty_Development813",
          "text": "You are running this with a 120w power supply? No way",
          "score": 1,
          "created_utc": "2026-02-17 01:24:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61kjmy",
              "author": "No_Night679",
              "text": "well he did say point and laugh.. so .. ",
              "score": 1,
              "created_utc": "2026-02-18 12:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5o7b4j",
          "author": "Shinkai_I",
          "text": "If you intended to run it using RAM from the beginning, you wouldn't have needed to buy a graphics card in the first place.   \nIf you intended to run it using VRAM, then why buy 256GB of system RAM?   \n  \nWhat kind of result were you expecting to make this choice?",
          "score": 1,
          "created_utc": "2026-02-16 12:25:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5segaq",
              "author": "Hefty_Development813",
              "text": "I mean it isn't crazy, as much gpu as you can for fast memory, then big ram for slow but high capacity offloading. I think it's a decent setup if the goal is optimizing only for how large of models you can load. The practical side of how it performs is a different question.Â \n\n\nI do agree this doesn't seem like the most efficient way to do it, but if he has multiple gpu model loading working well, this can probably do some decent stuff, so long as you are willing to wait a bit longer than chatgpt.",
              "score": 2,
              "created_utc": "2026-02-17 01:21:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62gio7",
              "author": "Diligent-Culture-432",
              "text": "I wanted to try running very large MoE models",
              "score": 1,
              "created_utc": "2026-02-18 15:29:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}