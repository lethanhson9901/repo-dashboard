{
  "metadata": {
    "last_updated": "2026-02-22 02:59:49",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 194,
    "file_size_bytes": 198165
  },
  "items": [
    {
      "id": "1r90rxi",
      "title": "How much was OpenClaw actually sold to OpenAI for? $1B?? Can that even be justified?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/p8c453eapgkg1.jpeg",
      "author": "Alert_Efficiency_627",
      "created_utc": "2026-02-19 14:34:16",
      "score": 186,
      "num_comments": 70,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r90rxi/how_much_was_openclaw_actually_sold_to_openai_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o68xq98",
          "author": "Theseus_Employee",
          "text": "They didn’t pay anything for it. They just hired the dude that made it, and are sponsoring the free open-source project OpenClaw.\n\nThe tweet is just a joke of people just inflating how much money you can make from vibe coded projects.",
          "score": 208,
          "created_utc": "2026-02-19 14:40:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aewgr",
              "author": "ArcticCelt",
              "text": "Also, people talk about it as a \"vibe-coded\" project, but they fail to mention that even if he used AI, it was done by a programmer with over 20 years of experience who sold his previous software startup for $100 million. It is not some random person with no experience.",
              "score": 34,
              "created_utc": "2026-02-19 18:57:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6aykqj",
                  "author": "huzbum",
                  "text": "Yeah, but he clearly did NOT put his engineering skills to work in there... glaring security flaws.  ",
                  "score": 8,
                  "created_utc": "2026-02-19 20:32:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cbn97",
                  "author": "oompa_loompa0",
                  "text": "Yeah. Watch the interview on Lex Friedman. Peter is no vibe coder, he is a brilliant seasoned engineer with crazy depth and breadth.",
                  "score": 2,
                  "created_utc": "2026-02-20 00:55:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cu2v7",
                  "author": "ANTIVNTIANTI",
                  "text": "it feels all very much—contrived ",
                  "score": 1,
                  "created_utc": "2026-02-20 02:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ae7xo",
              "author": "structured_flow",
              "text": "He had meetings with Zuck, same guy who is paying 8 figure salaries for their ai leadership positions, and turned Meta down. He absolutely was paid money how much don’t know, but your mistaken if you think there wasn’t a bid.",
              "score": 7,
              "created_utc": "2026-02-19 18:54:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bqhgu",
                  "author": "cjc4096",
                  "text": "He already had a successful exit.   He has the ability to choose what is important to him.",
                  "score": 3,
                  "created_utc": "2026-02-19 22:52:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o69j1ku",
              "author": "HeinerWersenberg",
              "text": "$1B for an unhinged AI-Agent-VibeCoded something did sound ridiculous indeed. However, when thinking of the competition, investments done already in the field, and considering the reasons for current memory and storage prices... who knows. Anything is thinkable theses days.    \nSo, thanks for the clarification. ",
              "score": 3,
              "created_utc": "2026-02-19 16:26:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69qiln",
              "author": "HappyContact6301",
              "text": "Look at Scale AI...",
              "score": 1,
              "created_utc": "2026-02-19 17:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bwdvj",
                  "author": "bronfmanhigh",
                  "text": "scale AI was still a real business with real employees and revenue end of the day lol",
                  "score": 2,
                  "created_utc": "2026-02-19 23:26:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6a1yfu",
              "author": "Demiansmark",
              "text": "Like the guy, like, like, like the guy with the $100B startup is going to pay attention to the subtext. Come on!",
              "score": 1,
              "created_utc": "2026-02-19 17:57:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69ufe3",
          "author": "reb00tmaster",
          "text": "This tweet alone is worth $80B.  In a few months it will be acquired for $160B. /s",
          "score": 46,
          "created_utc": "2026-02-19 17:21:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a6zme",
              "author": "MarkoMarjamaa",
              "text": "It could be turned into vibe-coded-NTF, then it's like $300B!",
              "score": 7,
              "created_utc": "2026-02-19 18:20:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6agkqz",
              "author": "eli_pizza",
              "text": "Unfortunately the \"/s\" is clearly not going to be enough for some people",
              "score": 2,
              "created_utc": "2026-02-19 19:05:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hdy7o",
              "author": "Far-Low-4705",
              "text": "I can type 300 B's, will that be enough?",
              "score": 1,
              "created_utc": "2026-02-20 20:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hf5iq",
                  "author": "reb00tmaster",
                  "text": "no",
                  "score": 1,
                  "created_utc": "2026-02-20 20:10:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69vzrb",
          "author": "sleepy_roger",
          "text": "The crazy thing is how shitty openclaw is. Seems like most people hyped (besides the crypto grifters who you never trust..) had never used a harness and were just larping the entire time.. \n\nCodex/claudecode/droid/opencode provide a much better experience overall.. openclaw isn't even tailored to non tech people. The only thing it really added that made the masses adopt was easy integration into existing chat platforms.",
          "score": 29,
          "created_utc": "2026-02-19 17:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a77wi",
              "author": "MarkoMarjamaa",
              "text": "Worlds fastest growing malware distribution system. ",
              "score": 22,
              "created_utc": "2026-02-19 18:21:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a8v57",
                  "author": "sleepy_roger",
                  "text": "haha yeah for sure.. I kept mine pretty limited, running in a proxmox container with almost no access. Is it cool to talk to it via discord, sure I guess.. but another big issue is the limited context you see as a user within openclaw versus other agent harnesses... not to mention the entire thing looks vibe coded (because it is).",
                  "score": 4,
                  "created_utc": "2026-02-19 18:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6adfj8",
              "author": "crypticG00se",
              "text": "You are missing the main point why these systems are attractive to openai and the like. Sells more tokens. Perfect system to fool non-tech people. Go look at the claw subreddit how people are complaining about model costs with claw. ",
              "score": 10,
              "created_utc": "2026-02-19 18:50:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b3bw1",
                  "author": "sleepy_roger",
                  "text": "Yeah that's a great point, I'm on the $200 max plan and it was the only time I got close to hitting my 5 hour limit... and it seemed like it was actually doing _much_ less than what I do within claude code directly.",
                  "score": 2,
                  "created_utc": "2026-02-19 20:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bu267",
              "author": "parapa-papapa",
              "text": ">Codex/claudecode/droid/opencode provide a much better experience overall.\n\n  \nI agree that OpenClaw sucks, but what is the competition? \n\nIt is insecure, yes. And that's why the big providers aren't touching such systems, but it's MEANT to be insecure. Like, you can't have a useful LL.M based assistant that is safe. ",
              "score": 1,
              "created_utc": "2026-02-19 23:13:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cy6jl",
                  "author": "Anarchaotic",
                  "text": "Claude on desktop is pretty good - not nearly as \"powerful\" as OpenClaw with no restrictions, but I can use it to do a lot of useful things by giving shell access with approvals on commands.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69ffgu",
          "author": "TimChr78",
          "text": "OpenClaw wasn’t sold to OpenAI at all - they hired the creator Peter Steinberger. OpenClaw is open source under the GNU 3.0 license.\n\nAnd no OpenAI is definitely not paying Peter Steinberger 1B.",
          "score": 20,
          "created_utc": "2026-02-19 16:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69qkv2",
              "author": "ptear",
              "text": "I mean, instant billionaire status would be impressive.",
              "score": 3,
              "created_utc": "2026-02-19 17:02:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cvjcw",
                  "author": "ANTIVNTIANTI",
                  "text": "it's all contrived. this shit was to boost the doubters back into believers again, lolol, I saw the reddit posts go from the actual reality of the situation after so many months/years of the hype hyping away so much hype, was finally starting to chill like 1-2% lol, then OpenClaw and MoldBook or bot or wtf ever the name was, lololol. BLEW IT BACK UP. Now all we are going to hear for the forevers is \"BRUH YOU'RE LIKE, 14 MONTHS BEHIND BRUH?!\"\n\n\"SKILL ISSUE BRUH?! STOP BEING 14 MONTHS BEHIND BRUH?!\"\n\n\"IF YOU DON'T SUCK MOLDYBLOCKS CLAW YOU'RE LIKE, 16 MONTHS BEHIND BRUH?!\" ",
                  "score": 2,
                  "created_utc": "2026-02-20 02:57:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6az013",
          "author": "huzbum",
          "text": "OpenAI hired the creator, they didn't buy a platform.  It makes a lot of sense when you consider it was promoting burning Anthropic tokens and now it will promote burning OpenAI tokens.  ",
          "score": 4,
          "created_utc": "2026-02-19 20:34:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvoxg",
              "author": "ANTIVNTIANTI",
              "text": "honestly dude was playing it like both corps paid him, he would say \"Half was done on Claude\" then \"Codex coded it\" which... yeah... I'm full of conspiracy today, ignore me :P",
              "score": 2,
              "created_utc": "2026-02-20 02:58:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d7s4g",
                  "author": "huzbum",
                  "text": "I hadn't paid enough attention to call you right or wrong, but it seems like the smart play to make nice with both of them for just this kind of eventuality.  \n\nMy comments were based on the original name being Clawed Bot and the recommendation to use Opus.  \n\nBut yeah, anything that gets more people addicted to burning more tokens and trusting LLMs with more data and responsibility is good for both of them.  ",
                  "score": 1,
                  "created_utc": "2026-02-20 04:18:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6a8qqn",
          "author": "shryke12",
          "text": "The dollar amounts being thrown around by these companies are ludicrous lol.  Shows just how many dollars our government has been printing.",
          "score": 3,
          "created_utc": "2026-02-19 18:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69vnda",
          "author": "locomocopoco",
          "text": "Dude is not just a vibecoder. Go see what he has done previously. SMH. ",
          "score": 7,
          "created_utc": "2026-02-19 17:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvu76",
              "author": "ANTIVNTIANTI",
              "text": "lol I know, he's a grifter, he's got this damn WUNDERKID Blog, errr, WUNDERMAN! Vunder? vvvvuuuunnnder? Anyways. It pissed me off. I wanted him to be a dipshit so bad. LOLOLOLOLOL :P XDXDXD",
              "score": 1,
              "created_utc": "2026-02-20 02:59:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g3467",
                  "author": "Capital-Result-8497",
                  "text": "you and I have very different definitions of a grifter. this dude is an engineer through and through",
                  "score": 1,
                  "created_utc": "2026-02-20 16:28:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ao0n7",
          "author": "type102",
          "text": "They can justify it by saying anything while being in a bubble.",
          "score": 2,
          "created_utc": "2026-02-19 19:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6arz6x",
          "author": "Baader-Meinhof",
          "text": "You missed the joke. The actual figure seems to be that he was hired for $30M.",
          "score": 2,
          "created_utc": "2026-02-19 20:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvz4e",
              "author": "ANTIVNTIANTI",
              "text": "also the tweet kept raising the billions, seemed like, hard to miss. LOL",
              "score": 2,
              "created_utc": "2026-02-20 03:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68zq5m",
          "author": "leonbollerup",
          "text": "where does idea that they paid for it come from ?.. is there any actual proff ?",
          "score": 2,
          "created_utc": "2026-02-19 14:50:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69wa86",
              "author": "oureux",
              "text": "No proff but we might be able to find some proof",
              "score": 1,
              "created_utc": "2026-02-19 17:29:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b50a6",
                  "author": "moderately-extremist",
                  "text": "lemme ask chatgpt...",
                  "score": 1,
                  "created_utc": "2026-02-19 21:04:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6brzzr",
                  "author": "leonbollerup",
                  "text": "Would love to see it",
                  "score": 1,
                  "created_utc": "2026-02-19 23:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6af2gm",
          "author": "structured_flow",
          "text": "Even if openai thought that he was a one trick pony and stuck them in a basement with a broken stapler like office space… They still would’ve paid him a lot of money because it’s all about branding so dude absolutely got paid a lot of money. I highly doubt anything close to 1 billion but definitely in the millions.  \n\nIt’s verified that he met with Zuckerberg who’s paying eight figure salaries for their head of AI departments and he turned them down saying that he “ felt more lines with the mission” at OpenAI.",
          "score": 1,
          "created_utc": "2026-02-19 18:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aieum",
          "author": "siegevjorn",
          "text": "They aqui-hired the dev if I understand it correctly.",
          "score": 1,
          "created_utc": "2026-02-19 19:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bzcjr",
          "author": "desexmachina",
          "text": "Manus was $2B, so they had to escalate and paid Pete $5B, kinda wild",
          "score": 1,
          "created_utc": "2026-02-19 23:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eb3c7",
              "author": "Alert_Efficiency_627",
              "text": "Yeah… if OpenClaw can truly be “open” — not OpenAI’s version of “open” — I’d say it could be worth $50B in the years to come.",
              "score": 1,
              "created_utc": "2026-02-20 10:01:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c98be",
          "author": "ithilelda",
          "text": "he might be paid in memory sticks or chatgpt subscription you know.",
          "score": 1,
          "created_utc": "2026-02-20 00:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cqh1v",
          "author": "No_Success3928",
          "text": "https://www.reddit.com/r/myclaw/s/NBiSm73M4p its an investment!",
          "score": 1,
          "created_utc": "2026-02-20 02:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cwmt8",
              "author": "ANTIVNTIANTI",
              "text": "Omfg I'd offer you my first born for showing me this, but wait.... that got weird, I meant....\n\nshit was hilarious. ",
              "score": 2,
              "created_utc": "2026-02-20 03:04:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cwzcn",
                  "author": "No_Success3928",
                  "text": "Crypto bro got a new AI game!",
                  "score": 1,
                  "created_utc": "2026-02-20 03:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e1lqu",
          "author": "satmun",
          "text": "I used to follow the works of (Peter Steinberger) of PSPDFKit before. He is deep into technical things. Probably his X(formerly Twitter) might still have his tweets. This app was named as best app in iOS for few years I think. He used to discuss Kernel level stuff to optmize rendering of PDFs if I remember correctly and well known in the circle of good devs, atleast in Austria I think. I remember him being close to another dev (professor) who is creator of game engine libgdx. I recently saw him dive deep into C++ and other internals of LLMs. So, its a talent pool and network of devs thats important I think. ",
          "score": 1,
          "created_utc": "2026-02-20 08:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k0dpp",
          "author": "1_H4t3_R3dd1t",
          "text": "OpenClaw isn't that amazing it is just a tool that hosts agents insecurely in your system to execute tasks. You can build the concept yourself. The reason he was hireable was because he made a platform out of it which is awesome and the real thing people should be talking about. ",
          "score": 1,
          "created_utc": "2026-02-21 05:21:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lm5ty",
          "author": "stonkister",
          "text": "\"you can just vibecode an open-source project and make $40B in a couple months now\" \n\nyou can...?",
          "score": 1,
          "created_utc": "2026-02-21 13:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o37bo",
          "author": "Mice_With_Rice",
          "text": "It wasnt sold. Nowhere has any official 1st party source claim to buy/sell openclaw. And if you think about it, it makes no sense it would be sold beacuse its under an MIT license with over 600 contributors.",
          "score": 1,
          "created_utc": "2026-02-21 21:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o692nwj",
          "author": "Investolas",
          "text": "They paid that much for him to sign an nda and stop development. ",
          "score": 1,
          "created_utc": "2026-02-19 15:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o698mfd",
              "author": "Theseus_Employee",
              "text": "Well they didn’t pay him enough apparently because he’s still developing it.",
              "score": 8,
              "created_utc": "2026-02-19 15:36:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69j1e6",
                  "author": "Technical_Ad_440",
                  "text": "have they fixed the security vulnerabilities yet?",
                  "score": 3,
                  "created_utc": "2026-02-19 16:26:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o699t7k",
                  "author": "Investolas",
                  "text": "Sure but it will never be what it would have been without their intervention",
                  "score": -1,
                  "created_utc": "2026-02-19 15:41:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9xifw",
      "title": "Devstral Small 2 24B + Qwen3 Coder 30B Quants for All (And for every hardware, even the Pi)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/s8yw2jndynkg1.png",
      "author": "enrique-byteshape",
      "created_utc": "2026-02-20 14:56:54",
      "score": 131,
      "num_comments": 60,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9xifw/devstral_small_2_24b_qwen3_coder_30b_quants_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6fxrl7",
          "author": "blksunday",
          "text": "Awesome! I use both of these on a Mac mini M4 24GB. I’ll be trying yours later today. Looks promising,.",
          "score": 5,
          "created_utc": "2026-02-20 16:04:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g2td8",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We're very interested in Mac speedups!",
              "score": 3,
              "created_utc": "2026-02-20 16:27:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g4sr3",
          "author": "mac10190",
          "text": "Sweet! I'll give it a shot later this afternoon.\n\nCurrently running dual R9700 32GB GPUs and an RTX 5090 32GB. Been using the dual R9700s to host larger models to act as the brain/orchestrator and then qwen 3 coder 30b on the 5090 for code generation and then tied it all together under the umbrella of Opencode. Testing this as a potential replacement for some of my Gemini CLI tasks.",
          "score": 5,
          "created_utc": "2026-02-20 16:36:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fwurh",
          "author": "swupel_",
          "text": "Love the graph style",
          "score": 3,
          "created_utc": "2026-02-20 16:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fxh9j",
              "author": "enrique-byteshape",
              "text": "Thanks! Don't tell the team, but the style is on me ;)",
              "score": 3,
              "created_utc": "2026-02-20 16:02:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fr09x",
          "author": "vanguard2286",
          "text": "Which one would you suggest for rrx 4070 8gb vram? I'm kind of new to self hosting LLMs and kind of not quite understanding the chart. I would love your input.",
          "score": 4,
          "created_utc": "2026-02-20 15:32:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsfuw",
              "author": "enrique-byteshape",
              "text": "Thank you for your interest! 8GB of VRAM is fairly limited, so not a lot of good quality models will fit, but if you want to play around with our models, you can try our Devstral [IQ2\\_S-2.34bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.34bpw.gguf) (75.1% quality of original model), our [IQ2\\_S-2.43bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.43bpw.gguf) (80.3% quality) or our [IQ3\\_S-2.67bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ3_S-2.67bpw.gguf) (87.2% quality, but will fit a smaller context length). You can also try offloading embeddings or some layers on our higher quality quants with ollama or llama.cpp, but this will reduce performance heavily. Let us know what you end up doing and if you enjoy the quants!",
              "score": 5,
              "created_utc": "2026-02-20 15:39:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6g405l",
                  "author": "vanguard2286",
                  "text": "Thank you!",
                  "score": 2,
                  "created_utc": "2026-02-20 16:32:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip21p",
                  "author": "TomLucidor",
                  "text": "Please start testing linear attention models like Nemotron-3-Nano or Kimi-Linear or Ring-Mini-Linear-2.0 or Granite-4.0 with the same methodology. Cus if they are more quant sensitive, that would be very sad. (maybe Gemma 3 and GPT-OSS-20B SWA also get support?)",
                  "score": 1,
                  "created_utc": "2026-02-21 00:11:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fxwfa",
          "author": "jarec707",
          "text": "You mentioned a blog in the post. Link please?",
          "score": 2,
          "created_utc": "2026-02-20 16:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g0nq1",
              "author": "enrique-byteshape",
              "text": "It's at the end of the post! Right here: [https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/](https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/)",
              "score": 3,
              "created_utc": "2026-02-20 16:17:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gklbd",
          "author": "BillDStrong",
          "text": "So, are these suitable for speculative decoding in llama.cpp? I would assume so, and since you have worked to keep them from falling off the cliff, they could do most of the work and then let a larger version fix the difference, which might result in faster perf for the same accuracy as the normal models?\n\nMaybe?\n\nThe best I have is a P40 24GB, so will  have to test it later.",
          "score": 2,
          "created_utc": "2026-02-20 17:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gxbsx",
              "author": "enrique-byteshape",
              "text": "We have not tried speculative decoding at all with these models, but they have good quality and are performant. If you have a way to use them for such use case, we assume they will work, but we can't really promise anything!",
              "score": 2,
              "created_utc": "2026-02-20 18:46:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ip5r2",
                  "author": "TomLucidor",
                  "text": "Qwen3 and Nemotron has native MTP, please try them as well!",
                  "score": 1,
                  "created_utc": "2026-02-21 00:12:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6i84my",
          "author": "Clear-Lab3427",
          "text": "Thanks so much!",
          "score": 2,
          "created_utc": "2026-02-20 22:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j2h8e",
          "author": "Useful_Disaster_7606",
          "text": "I will forever rue the day I bought an RTX 3060 8GB. But then again I did buy it for less than $220 so I guess it's not that bad. \n\nJust out here feeling FOMO seeing all these amazing models. So close yet so faar.",
          "score": 2,
          "created_utc": "2026-02-21 01:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j60sm",
              "author": "enrique-byteshape",
              "text": "You CAN actually try our low bits per weight Devstral quants so that you don't feel as left out! Under 8GB with enough context length to test them!",
              "score": 1,
              "created_utc": "2026-02-21 01:54:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6j4pyp",
          "author": "Snoo_24581",
          "text": "Thanks for putting this together! Been waiting for good quants of these models. The 24B size is perfect for my 24GB VRAM setup.\n\nHow's the performance on coding tasks compared to the full precision versions? Any significant quality drop?",
          "score": 2,
          "created_utc": "2026-02-21 01:46:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j8efu",
              "author": "enrique-byteshape",
              "text": "If you choose our highest bit per weight quants there is no visible degradation on our benchmarks and our qualitative assessments",
              "score": 1,
              "created_utc": "2026-02-21 02:09:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lgt9b",
          "author": "xeeff",
          "text": "been following you on huggingface for the longest time - finally glad to see some new models. been waiting for these one so long i kinda forgot they are still great models. keep up the good work.\n\np.s. any notes on the model roadmap and an ETA? :)",
          "score": 2,
          "created_utc": "2026-02-21 13:16:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6liwa0",
              "author": "enrique-byteshape",
              "text": "We can't really promise anything, but some diffusion models are in the near to-do list, and we will try to move onto thinking models (which we expect will suppose a big challenge when evaluating them)",
              "score": 2,
              "created_utc": "2026-02-21 13:30:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ljt5s",
                  "author": "xeeff",
                  "text": "always thought why all the models are instruct, but them being harder to evaluate makes sense. did not expect diffusion models to be mentioned, though. any ones in particular? if you'd prefer to not mention, that's perfectly okay",
                  "score": 2,
                  "created_utc": "2026-02-21 13:36:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6futyd",
          "author": "floppypancakes4u",
          "text": "So your qwen model wont work with a 4090? Do either support a 3090? Looking forward to trying these out.",
          "score": 1,
          "created_utc": "2026-02-20 15:50:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fvbyv",
              "author": "enrique-byteshape",
              "text": "It does support any type of hardware, it's just that our performance benchmarks are only on the hardware that we have available. Sorry we didn't make that clear enough. Our Qwen on the 4090 runs similarly to the 5090 in terms of comparing it to other quants, albeit at a slower TPS.",
              "score": 2,
              "created_utc": "2026-02-20 15:52:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fvrdv",
                  "author": "floppypancakes4u",
                  "text": "Excellent! I'll test both this afternoon.",
                  "score": 3,
                  "created_utc": "2026-02-20 15:54:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6imy7o",
                  "author": "CTR1",
                  "text": "Following up on the question regarding the 3090 compatibility: do you have suggestions for a ideal model to use with a 5800xt + 64gb 3200mhz ram + 3090 pc build?\n\nIdeally something that balances quality | TPS | context and maybe tool calling too? I know that might be a tough ask",
                  "score": 2,
                  "created_utc": "2026-02-20 23:59:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h39q3",
          "author": "oliveoilcheff",
          "text": "What about strix halo? Are there some performance gains there? Thanks!",
          "score": 1,
          "created_utc": "2026-02-20 19:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hx62p",
              "author": "enrique-byteshape",
              "text": "We don't have one in hand, but there should be performance gains on any type of hardware. We would love to hear of the performance you get on it!",
              "score": 1,
              "created_utc": "2026-02-20 21:39:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h3ei6",
          "author": "Simple-Worldliness33",
          "text": "\n\nHi !\n\nThanks for your work! I didn't bench yet but I need to understand completely.\n\nFor an example, I'm using unsloth iq4\\_NL currently with 2 rtx 3060, i got 70/76 tks.\n\nWhich model you are offering should I choose to compare with? I tried the iq4\\_ks but I didn't have the same perf. (Only 35/40tks)",
          "score": 1,
          "created_utc": "2026-02-20 19:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ia35g",
              "author": "enrique-byteshape",
              "text": "Hi! Thanks for trying our models! The performance you might get out of them can vary a lot depending on the hardware, and/or on whether the model is being loaded and ran correctly. Would you mind being more specific about your setup and llama.cpp environment and parameters?",
              "score": 2,
              "created_utc": "2026-02-20 22:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jtd4a",
          "author": "Count_Rugens_Finger",
          "text": "Going to try these on my RX 9070 XT",
          "score": 1,
          "created_utc": "2026-02-21 04:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfzkd",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We tested on an RX 9060 XT 16GB and they got a speedup versus other quants",
              "score": 1,
              "created_utc": "2026-02-21 13:11:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ly7ep",
                  "author": "Count_Rugens_Finger",
                  "text": "Your Blog claims that your optimization is specific to Nvidia's 40 and 50 series hardware.  Would you expect the Radeon Vulkan implementation to be just *not as good*, or actively *worse* with your builds vs the standard Q4_K_M quants?",
                  "score": 1,
                  "created_utc": "2026-02-21 15:02:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kcwvl",
          "author": "geringonco",
          "text": "Are there any rankings sites for these models?",
          "score": 1,
          "created_utc": "2026-02-21 07:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfsui",
              "author": "enrique-byteshape",
              "text": "Sadly no because no one is willing to evaluate all the released quants to create such ranking sites. It is very expensive",
              "score": 1,
              "created_utc": "2026-02-21 13:09:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l5258",
          "author": "shankey_1906",
          "text": "Any recommendation for Strix Halo?",
          "score": 1,
          "created_utc": "2026-02-21 11:43:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfq11",
              "author": "enrique-byteshape",
              "text": "It depends on the underlying framework and kernels. Most likely our CPU versions will work best, but it would require testing them out",
              "score": 1,
              "created_utc": "2026-02-21 13:09:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l64qw",
          "author": "puru991",
          "text": "When qwen 3.5?",
          "score": 1,
          "created_utc": "2026-02-21 11:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lg5fv",
              "author": "enrique-byteshape",
              "text": "Thank you for the interest, we are aware of the release (as well as of many others), but it's hard to keep up considering that evaluating these quants takes a lot of resources and time. We have to be picky when releasing models, so we usually go by what is popular and what people might really want",
              "score": 1,
              "created_utc": "2026-02-21 13:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lr2fc",
          "author": "Embarrassed-Boot5193",
          "text": "Eu testei o modelo Devstral-Small-2-24B-Instruct-2512-IQ3\\_S-3.47bpw.gguf e não coube na minha GPU de 16GB com 32k de contexto. Vocês estão quantizando o kv cache para isso acontecer?",
          "score": 1,
          "created_utc": "2026-02-21 14:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mohrm",
              "author": "enrique-byteshape",
              "text": "Hey! When we benchmarked Devstral on the different hardware ranges we did so without the vision tower. That's why it might not fit with a context length of 32K. Sorry for the inconvenience!",
              "score": 1,
              "created_utc": "2026-02-21 17:15:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mhyh1",
          "author": "Thrynneld",
          "text": "The new frontier seems to be figuring out how much cruft can be removed from a model before it falls over. So far the rule of thumb has been always use the largest model (parameter wise) that can fit in memory at a quant that does turn itself into gibberish. I've noticed that models with more parameters seem to hold up better at lower quants than smaller models. Is there any chance you guys will be publishing quants of larger popular models? Something like qwen3-coder-next, or even qwen 3.5? What is your bottleneck in this quantization process? do you need to run inference to determine the importance of the weights to quant down more or less? I'm loving Qwen 3.5 on my mac studio, but it sucks up most of my memory at a 3 bit quant while itseems capable enough at 3 bit, I wonder if it would perform better at a \"smarter\" 3 bit version, or even a 2 bit version :)",
          "score": 1,
          "created_utc": "2026-02-21 16:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mmq3c",
              "author": "enrique-byteshape",
              "text": "Our own research and other groups' research has been showing for a while that larger models have a much larger tolerance to quantization and pruning. We've also seen some weight cause outlier activations that matter the most when actually running inference. And we have also observed larger models being quantized aggressively but still being better than smaller models with the same size. Qwen 3.5 is in our roadmap, but our current bottleneck is evaluating these quants so that people can be informed while downloading them. The datatype learning process is actually quite fast on our technology",
              "score": 1,
              "created_utc": "2026-02-21 17:06:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n5kop",
          "author": "siegevjorn",
          "text": "What benchmark are you running?",
          "score": 1,
          "created_utc": "2026-02-21 18:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n6bht",
              "author": "enrique-byteshape",
              "text": "Hey! From the very end of our blog post:  \n\"Devstral supports both tool calling and vision, so we evaluated it on:\n\n* **BFCL\\_V3** for tool calling\n* **GSM8K\\_V** for vision\n* **LiveCodeBench V6** and **HumanEval** for coding\n* **GSM8K** and **Math500** for math\n* **MMLU** for general knowledge\n\nThe reported score is the mean across these benchmarks, with each benchmark normalized to the original model's score.\n\nQwen was evaluated using the same setup, with two exceptions:\n\n* No **GSM8K\\_V** (no vision support)\n* No **MMLU** (not a general knowledge evaluation)\n\nAll evaluations were run with llama.cpp `b7744`. We used 4K as the minimum context window required for a model to be considered \"fit\" on a given device.\"",
              "score": 1,
              "created_utc": "2026-02-21 18:44:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fvlkz",
          "author": "peyloride",
          "text": "Nice work but I think the real baselin about context should not be 32k because that's very limited in these days. Since these are coding models, context adds very quick in coding agents. I wonder what's the story whe context is around 200k? or even something like 100k? I don't have an idea about what should be the baseline sorry, but 32k seems low.",
          "score": 1,
          "created_utc": "2026-02-20 15:54:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fwyin",
              "author": "enrique-byteshape",
              "text": "32k context is for performance measurements only, which will scale depending on the context length used. For the evaluations we do not limit context length, so those should not be biased. The models will run with any context length as long as it fits. And yes, with longer context lengths, activations start becoming the bottleneck. Sadly, llama.cpp doesn't support quantizing activations to arbitrary datatypes, so at the moment we are limited by that, but our algorithm can also learn the datatypes for them",
              "score": 3,
              "created_utc": "2026-02-20 16:00:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gkqxy",
                  "author": "peyloride",
                  "text": "Yeah I see  your point but context length is important for vram usage. It also affects the accuracy and the TPS (I might be wrong about this). So what I'm trying to say is since these are coding models, you should not test it in 32k context. It might be enough for general usage, but I don't think that's the case for coding models. \n\nIf this is not possible at the time being that's okey; just wanted to flag this out. ",
                  "score": 2,
                  "created_utc": "2026-02-20 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r659df",
      "title": "Qwen3.5 is released!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xtgnyvb2stjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-16 09:34:21",
      "score": 124,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r659df/qwen35_is_released/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5rs0pq",
          "author": "DiligentRanger007",
          "text": "How much vram needed ???",
          "score": 3,
          "created_utc": "2026-02-16 23:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sqnpj",
              "author": "ubrtnk",
              "text": "Yes",
              "score": 18,
              "created_utc": "2026-02-17 02:35:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5t9j3m",
              "author": "yoracale",
              "text": "Depends on your ram. I'd say at least 16gb. See the guide: https://unsloth.ai/docs/models/qwen3.5",
              "score": 2,
              "created_utc": "2026-02-17 04:36:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ts1sf",
          "author": "pghqdev",
          "text": "what non-mac setup would be equivalent to 512 M3 Ultra config?",
          "score": 3,
          "created_utc": "2026-02-17 07:04:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66wckp",
              "author": "AppleBottmBeans",
              "text": "all of them",
              "score": 1,
              "created_utc": "2026-02-19 05:03:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67llea",
                  "author": "Eden1506",
                  "text": "Non, apple has a memory bandwidth of 800 gb/s on the ultra.  Even servers using 12 channel ddr5 don't reach those speeds. \n\nObviously there is overhead and you can't fully utilise the full speed you are still ahead of anything outside of gpus. ",
                  "score": 1,
                  "created_utc": "2026-02-19 08:39:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61kcik",
          "author": "barkdender",
          "text": "Can I even use the 1 bit quantized version on my RTX 3080 12 GB without offloading or am I in for a bad time? ",
          "score": 2,
          "created_utc": "2026-02-18 12:39:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61uwbj",
              "author": "yoracale",
              "text": "Wil work but will be extremely slow, how much RAM do you have?",
              "score": 1,
              "created_utc": "2026-02-18 13:41:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o623u9s",
                  "author": "barkdender",
                  "text": "I had 256gb but sold some because well to offset cost. Down to 64gb",
                  "score": 1,
                  "created_utc": "2026-02-18 14:28:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q7bfe",
          "author": "I_like_fragrances",
          "text": "I didn't know, just started downloading it now.",
          "score": 3,
          "created_utc": "2026-02-16 18:33:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qn83k",
              "author": "yoracale",
              "text": "Awesome, let us know how it goes!",
              "score": 1,
              "created_utc": "2026-02-16 19:48:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vx60z",
          "author": "phoenixfire425",
          "text": "Make sad sounds with 2 x RTX3090ti  Wish I could run this.  I love 2.5-coder, maybe a few weeks ill be able to get a version of this i can run on my hardware.",
          "score": 1,
          "created_utc": "2026-02-17 16:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69gtlf",
              "author": "emrbyrktr",
              "text": "Did you try Qwen3 Coder Next?",
              "score": 1,
              "created_utc": "2026-02-19 16:15:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62phc0",
          "author": "slyticoon",
          "text": "Why did we pick 3 shades of grey for the comparisons models...",
          "score": 1,
          "created_utc": "2026-02-18 16:10:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6e4h4",
      "title": "Anyone else spending more time tweaking than actually using their model?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r6e4h4/anyone_else_spending_more_time_tweaking_than/",
      "author": "Weirdboy212",
      "created_utc": "2026-02-16 16:24:28",
      "score": 87,
      "num_comments": 43,
      "upvote_ratio": 0.98,
      "text": "I swear I’ve spent 10x more time:  \n\\-comparing quants  \n\\-adjusting context size  \n\\-testing different system prompts  \n\\-watching tokens/sec\n\nthan actually asking it useful questions\n\nFeels like building a gaming PC and then only running benchmarks",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6e4h4/anyone_else_spending_more_time_tweaking_than/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5prvku",
          "author": "beisenhauer",
          "text": "\"Give me six hours to get a useful answer from an LLM, and I'll spend the first four tuning the model.\" -- Abraham Lincoln",
          "score": 21,
          "created_utc": "2026-02-16 17:22:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t7ynj",
              "author": "ANTIVNTIANTI",
              "text": "Ummmm Aktuallly I believe that was Thomas Jefferson",
              "score": 3,
              "created_utc": "2026-02-17 04:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5u77ri",
                  "author": "singh_taranjeet",
                  "text": "Common misconception. Jefferson preferred longer context windows but worse latency. Lincoln was more of a prompt engineer.\n\nBoth terrible at quant selection though...",
                  "score": 6,
                  "created_utc": "2026-02-17 09:27:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5t7z4z",
                  "author": "ANTIVNTIANTI",
                  "text": ":D",
                  "score": 2,
                  "created_utc": "2026-02-17 04:25:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5zn1od",
              "author": "No-Television-7862",
              "text": "\"The last 20% takes longer than the first 80%.\"",
              "score": 2,
              "created_utc": "2026-02-18 03:24:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pfyvq",
          "author": "Medium_Chemist_4032",
          "text": "Oh for sure. Plus recompiliing llama.cpp and ik\\_llama. \n\nRecently it has been more: \"why tool calling breaks after few times\" often, so I guess... progress?",
          "score": 11,
          "created_utc": "2026-02-16 16:27:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pl0km",
          "author": "Look_0ver_There",
          "text": "Having a 128GB shared memory machine, and with the recent release of all these models in the 190-240B parameter range has made searching for that elusive \"quantization that isn't brain-dead and fits\" take a massive chunk of my time.  Every time I think my task is done, another model drops and the search continues...",
          "score": 8,
          "created_utc": "2026-02-16 16:50:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pocs4",
              "author": "Hector_Rvkp",
              "text": "I was assuming / hoping that 128gb is actually the sweet spot to get MoE models that sound intelligent enough to be useful and fast enough to be usable. Is that not the case? What are you trying to achieve? I'm considering buying a Strix halo w 128gb ram.",
              "score": 3,
              "created_utc": "2026-02-16 17:05:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5prb3w",
                  "author": "Look_0ver_There",
                  "text": "It is. Qwen3-Coder-Next, which is 80B, fits fine at full Q8, and you can run it at full context at ~35tg/sec.  Speeds go up if you use a smaller quantization but there's no need to sacrifice quality.\n\nI've been messing about with Step-3.5-Flash and MiniMax-M2.5 though, which are 196B and 229B respectively.  Both fit okay with an IQ3 based quantization, but Step seems to suffer a fair bit at that level, while MiniMax however seems to be just fine, or at least, it's hasn't really put a foot wrong in the coding tasks I've asked of it.\n\nBasically I'm refining an IQ3 quant of MiniMax, as well as tuning the VM parameters of the kernel, to get it running as smooth as possible.  It seems to be ticking along at 30tg/sec as of this morning, and it generally emits better quality output that works the first or second time, whereas Qwen Coder Next makes a lot of small mistakes that I really need to keep an eye on.\n\nI also found out today how to fix a chat template bug that was in the original release of MiniMax, and I now have it working really well (again, as if today).",
                  "score": 5,
                  "created_utc": "2026-02-16 17:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pgatg",
          "author": "iz_raymond",
          "text": "Haha that hits home. Honestly, I just want my AI to not sound corporate 😔 all of them sounds like that, I just want an AI that's unhinged like Grok or at least \"human-like\" Gemini",
          "score": 9,
          "created_utc": "2026-02-16 16:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pmcm8",
              "author": "GetMeThePresident",
              "text": "any luck or tips at getting closer to that?",
              "score": 3,
              "created_utc": "2026-02-16 16:56:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5w50yw",
                  "author": "iz_raymond",
                  "text": "Nope, tried Qwen,Mistral,Deepseek,Llama..all still sound very corporated. But I stick to Qwen for a bit more balance than the rest",
                  "score": 2,
                  "created_utc": "2026-02-17 16:50:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5sexpp",
              "author": "sgamer",
              "text": "go on the character card sites and look for assistant characters, you can add a lot of flavor with just frontloading a json onto chat mode",
              "score": 2,
              "created_utc": "2026-02-17 01:24:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t86yy",
                  "author": "ANTIVNTIANTI",
                  "text": "Oh yeah I 2nd this! SO much can be done just in the sys prompt! Also prompt/write intentionally—like, really recognize the mirroring effect or how you lead the models by way of your own words, also I'm not sure if I'm replying to the right person to make it so that I appear replying to both of you LOLOL",
                  "score": 2,
                  "created_utc": "2026-02-17 04:27:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5phps1",
          "author": "ptear",
          "text": "Yes, I'm still searching for the most valuable local uses that actually are worth it, other than write python script to solve this problem (which I don't end up doing with a local model because I have midrange hardware).",
          "score": 3,
          "created_utc": "2026-02-16 16:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rs6wj",
          "author": "ElijahKay",
          "text": "Skyrim Modders.\n\n  \nFirst time meme.",
          "score": 3,
          "created_utc": "2026-02-16 23:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pghpc",
          "author": "sn2006gy",
          "text": "Yeah...\n\nReality is, the real work is being done with layers and models of models/retrievers/planers/routers like:\n\nUser Input  \n  ↓  \nRetriever (patterns, code, history, embeddings)  \n  ↓  \nPlanner / Router  \n  ↓  \nLLM (reasoning)  \n  ↓  \nTool Calls (search, code execution, APIs)  \n  ↓  \nEvaluator / Critic  \n  ↓  \nFinal Output\n\n\n\nThat's why claude code is kicking the butt of whatever isolated model we kick the tires on... but also why claude is so fragile vs competition because it's not rocket science to build this onion layer pattern with tooling.",
          "score": 4,
          "created_utc": "2026-02-16 16:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5po44m",
              "author": "SubstantialPoet8468",
              "text": "Is this akin to the “arr” stack for media, compared to say something like Netflix?",
              "score": 1,
              "created_utc": "2026-02-16 17:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5qqiyv",
              "author": "StaysAwakeAllWeek",
              "text": ">it's not rocket science to build this onion layer pattern with tooling.\n\nIt definitely is rocket science to do it profitably though. You need highly optimised models designed specifically for being stacked like this if you ever want to break even",
              "score": 1,
              "created_utc": "2026-02-16 20:04:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5r54ml",
                  "author": "sn2006gy",
                  "text": "It's why claude is going all in on development... but it's not hard to pull OSS models together to build something similar - my point is less about finding the perfect model and tinkering to infinity but about finding models that fit the retriver/reasoning/tool call/evaluator paradigm and if that's coding - fits in the current tooling that has that concept as part of its workflow.",
                  "score": 1,
                  "created_utc": "2026-02-16 21:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ryg0f",
              "author": "m94301",
              "text": "Is this the secret behind Claude?  It's so good, it's really the only one I pay for, but I've never tried to understand what's going on under the hood",
              "score": 1,
              "created_utc": "2026-02-16 23:48:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sr51e",
                  "author": "sn2006gy",
                  "text": "pretty much",
                  "score": 1,
                  "created_utc": "2026-02-17 02:37:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5plnbg",
          "author": "HomsarWasRight",
          "text": "Welcome to every “do it yourself” technical hobby.",
          "score": 4,
          "created_utc": "2026-02-16 16:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pnaf3",
          "author": "live4evrr",
          "text": "Spent several hours yesterday trying to play with llama.cpp settings to improve the minimax 2.5 performance. It’s not much difference from the dopamine reward as gaming.",
          "score": 1,
          "created_utc": "2026-02-16 17:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5prk3a",
          "author": "SKirby00",
          "text": "I've been working on building a utility that automatically does some of this stuff for me.\n\nFor context, I have multiple GPUs with varying amounts of VRAM, and I run models with SGLang to get the most performance. The idea is that I'll run this utility with a specified model name, and it:\n- Fetches the model's info from huggingface (in particular, the number of layers that I'll need to distribute across my GPUs)\n- Checks my current hardware to see how many GPUs are installed and how much VRAM is in each one\n- Runs an optimization script that:\n    - Gradually increases the memory allocation and context length until I start running into OOM errors\n    - Runs a hill-climbing algorithm on the distribution of model layers between GPUs to make sure I'm getting the most out of each one\n    - Once it's identified the maximum stable configuration, it pulls it back a bit to add some safety margin\n- Saves the discovered optimal configuration for my exact combination of hardware and model selection to a JSON config file that I can use with another script to run the server\n\nI'll still have to mess around with different prompts and prompt templates etc., but once this is working reliably it should significantly cut down on the time and energy that it takes to figure out exactly how hard I can push my system with a given model and how much context I can fit.",
          "score": 1,
          "created_utc": "2026-02-16 17:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ql006",
          "author": "AdOne8437",
          "text": "No. I mostly try a bit with a few testcases I have and then use for weeks and months the same models.",
          "score": 1,
          "created_utc": "2026-02-16 19:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qsud9",
          "author": "esmurf",
          "text": "Only in the beginning. ",
          "score": 1,
          "created_utc": "2026-02-16 20:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ruaub",
          "author": "steezy13312",
          "text": "I'm not alone!\n\nI think part of the challenge is being in the performance limbo of \"it's just passable enough to run what I need\" but at the end of the day not good enough to actually use for real work compared to cloud providers.",
          "score": 1,
          "created_utc": "2026-02-16 23:25:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t7pfw",
          "author": "ANTIVNTIANTI",
          "text": "lololololol it's why we do what we do tho",
          "score": 1,
          "created_utc": "2026-02-17 04:24:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t7rdp",
              "author": "ANTIVNTIANTI",
              "text": "we're inherent tweakers ",
              "score": 1,
              "created_utc": "2026-02-17 04:24:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tvvt5",
          "author": "Gargle-Loaf-Spunk",
          "text": "Wait you mean I’m supposed to use this crap? ",
          "score": 1,
          "created_utc": "2026-02-17 07:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ulp7q",
          "author": "Least-Platform-7648",
          "text": "Same here. Now I use agents to help me, e.g. Codex will try different llama.cpp settings and run benchmarks in a loop.",
          "score": 1,
          "created_utc": "2026-02-17 11:38:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wqbge",
          "author": "Ok-Measurement-1575",
          "text": "Yep because when the shit hits the fan, you gotta know you can rely on it. ",
          "score": 1,
          "created_utc": "2026-02-17 18:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zmqhg",
          "author": "No-Television-7862",
          "text": "Trying to get that custom open-weight llm just right takes time.",
          "score": 1,
          "created_utc": "2026-02-18 03:22:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fj31n",
          "author": "enrique-byteshape",
          "text": "Without wanting to do self-promotion, but we are releasing benchmarked quants and comparisons against the main quants that are public out there. We're trying to make the open model and quants ecosystem a bit more friendly, but we're a small team of 4 so it takes time to release these benchmarked models. Hope it helps you: \n\n* [https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/](https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/) (our latest release and benchmarks)\n* [https://proceedings.mlsys.org/paper\\_files/paper/2024/hash/185087ea328b4f03ea8fd0c8aa96f747-Abstract-Conference.html](https://proceedings.mlsys.org/paper_files/paper/2024/hash/185087ea328b4f03ea8fd0c8aa96f747-Abstract-Conference.html) (our paper on how we find optimal datatypes per tensor for quantization)",
          "score": 1,
          "created_utc": "2026-02-20 14:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rhhmu",
          "author": "tiga_94",
          "text": "Its not like local llms are really useful, so far I was unable to make any one of them to work as an agent, all fail to use simplest apis \n\nAnd then in chat mode they hallucinate too much\n\nModels in question are below 16 gigs",
          "score": 1,
          "created_utc": "2026-02-16 22:17:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rx38f",
              "author": "remainderrejoinder",
              "text": "You just didn't realize you should be asking it to hallucinate.",
              "score": 1,
              "created_utc": "2026-02-16 23:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sfzs4",
                  "author": "tiga_94",
                  "text": "I ask to open a file and output its content, se as using same agent with a cloud api \n\nCloud- does it, local - hallucinates \n\nGpt oss 20b q4 in question",
                  "score": 1,
                  "created_utc": "2026-02-17 01:30:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5s5csk",
              "author": "segmond",
              "text": "your lack of skills is your own problem.",
              "score": 1,
              "created_utc": "2026-02-17 00:28:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qme6p",
          "author": "Decaf_GT",
          "text": "Unpopular opinion: it's because most people don't actually know what they want a local LLM for, and those who do know that the local LLMs that they're actually *able* to run still don't compare to frontier cloud models. \n\nAs we gravitate further and further to MoE and go from \"no local no care\" to \"well, as long as its hosted in the US\" so that you can run GLM-5, we're going to watch local LLM usage collapse because the models that are worth running require hardware that is incredibly expensive, hardware that we *can't even get* anymore due to the RAM apocalypse.",
          "score": -1,
          "created_utc": "2026-02-16 19:44:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9zo0u",
      "title": "Why AI wont take your job and my made up leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/qaz3ln1ncokg1.jpeg",
      "author": "Eventual-Conguar7292",
      "created_utc": "2026-02-20 16:16:34",
      "score": 75,
      "num_comments": 59,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9zo0u/why_ai_wont_take_your_job_and_my_made_up/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ggzb5",
          "author": "promethe42",
          "text": ">22 users programming with ChatGPT\n\nYeah it's not 2024 anymore...\n\nAll the people claiming coding is done are not using ChatGPT and manual prompts. They are using agentic coding system with meta prompts, skills, etc... with tasks running for 1 to 2 hours autonomously. And those systems can run on local LLMs.\n\nSo those numbers need a serious update.",
          "score": 47,
          "created_utc": "2026-02-20 17:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h43ge",
              "author": "ForsookComparison",
              "text": ">>  users programming with ChatGPT\n\nI think this is just the author failing to grasp that chatgpt != ai. The actual benchmark is *(very very likely)* not using back and forth chat sessions.\n\nSometimes it's easier to just let marketing say \"Chatgpt\"",
              "score": 8,
              "created_utc": "2026-02-20 19:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h8axj",
                  "author": "Malkiot",
                  "text": "I built a tool that decomposes projects into dependency graphs, generates implementation plans, assigns parallel AI agents to work simultaneously, and enforces boundaries so they don't stomp on each other.\n\nI built that tool *with* AI because I was annoyed with managing my other projects.\n\nThe studies here are measuring people using ChatGPT like a search bar. People failing says more about them than it does about the utility of AI.",
                  "score": 12,
                  "created_utc": "2026-02-20 19:37:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gsd10",
          "author": "Lissanro",
          "text": "It is true that the current LLMs and agents cannot yet do freelance work on their own, but \"small bites only\" era has passed long time ago for me.\n\n\nWhen I was just beginning integrating LLMs to my workflow, even basic things how to center a div container, LLMs often struggled with, especially in more complicated layouts. It was more efficient to either try a few things or just google it.\n\n\nNowadays, I can tell Kimi K2.5 make an entire website and leave it overnight running on my PC, providing warmth during winter nights as a bonus... and in most cases it gets almost everything perfect, except I still need to provide images and polish layout, fix small issues. Even with vision, K2.5 is still not precise enough to clearly see some mistakes, or to be able judge icon quality. But it still can describe ideas what icons to put where and make simple SVG placeholders, some of them actually can be good enough, but most need to be replaced.\n\n\nThat said, my prompts are quite detailed and I have over a decade of experience in web design and programming, in addition to being 2D and 3D artist. So I can use both hybrid or traditional methods to produce required images, animations or code, if it is needed for good result. Using my experience, I can specify precisely what I need, or I can provide enough context from previous work so it is clear what is needed, so the better the AI, the faster I can get the job done, or the more work I can take, while still maintain quality, both visual and of the code base, since with or without AI, I do my work based on my skills and experience, using AI just allows me to be more efficient.\n\n\nI think using AI is skill on its own, and that sometimes needs to be relearned (when things change or when need to use new tools / inference backends) and adapted to the situation. At least, with AI models that are available today.",
          "score": 16,
          "created_utc": "2026-02-20 18:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h0b3h",
              "author": "TheAncientOnce",
              "text": "what hardware do you use if you don't mind me asking? Is it full Kimi2.5 or are you running a quantized variants?",
              "score": 1,
              "created_utc": "2026-02-20 18:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h2nu8",
                  "author": "Lissanro",
                  "text": "Yes, I use the full version (Q4\\_X, preserving INT4 weights in GGUF format, along with F16 mmproj for vision). I run K2.5 on 64-core EPYC 7763 + 8-channel 1 TB 3200MHz RAM + 96 GB VRAM (made of 4x3090) + 8 TB NVMe for AI models and 2 TB NVMe SSD for the OS + \\~120 TB disk space on HDDs for storage and backups. If interested to know more, in my another comment I shared a photo and other details about my rig including what PSUs I use and what the chassis look like: [https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/](https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/)",
                  "score": 4,
                  "created_utc": "2026-02-20 19:10:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gt32f",
          "author": "nomorebuttsplz",
          "text": "Sound like you've never heard of coding agents. \n\nHow the fuck is this post getting upvotes. You're like a year behind.",
          "score": 10,
          "created_utc": "2026-02-20 18:27:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hnlfw",
              "author": "alcalde",
              "text": "Prove otherwise. What secret knowledge do you have that the rest of the world does not?",
              "score": -1,
              "created_utc": "2026-02-20 20:52:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hp5z8",
                  "author": "nomorebuttsplz",
                  "text": "OP is literally describing trying to code without an agent \"Paste the relevant code, show what you're working with\"\n\nThat's obsolete.",
                  "score": 9,
                  "created_utc": "2026-02-20 21:00:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip66o",
                  "author": "Low_Amplitude_Worlds",
                  "text": "“secret knowledge” 🤨",
                  "score": 3,
                  "created_utc": "2026-02-21 00:12:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6g9bwr",
          "author": "Vozer_bros",
          "text": "from what I have done last month with AI automation, I dont agree with you.",
          "score": 20,
          "created_utc": "2026-02-20 16:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gusd7",
              "author": "_VirtualCosmos_",
              "text": "What have you done? and with which AI if I may ask",
              "score": 3,
              "created_utc": "2026-02-20 18:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hpbdw",
                  "author": "teamharder",
                  "text": "Copy of comment. Opus 4.6. Ive done more than what is listed below.\n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.",
                  "score": 5,
                  "created_utc": "2026-02-20 21:00:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6iageo",
                  "author": "Vozer_bros",
                  "text": "generate unlimited chapters of novel/story with auto grow characters, my main AI are GLM and Claude Opus",
                  "score": 1,
                  "created_utc": "2026-02-20 22:48:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6grxbq",
              "author": "teamharder",
              "text": "Thats my take too. Idk how someone could do this much work studying it and come to this conclusion. ",
              "score": 3,
              "created_utc": "2026-02-20 18:21:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ibkep",
                  "author": "Vozer_bros",
                  "text": "yep, I dont want to debate, but digital world is changing faster then ever, we should aware all accept that instead of pretending AI is stupid",
                  "score": 1,
                  "created_utc": "2026-02-20 22:54:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hnip2",
                  "author": "alcalde",
                  "text": "This is the conclusion of the entire planet. To claim otherwise is an extraordinary claim requiring extraordinary evidence. No one's cranked out a new operating system via Claude Code.",
                  "score": -2,
                  "created_utc": "2026-02-20 20:52:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6grac7",
          "author": "Healthy-Nebula-3603",
          "text": "Sure buddy... keep your head in the sand ....",
          "score": 6,
          "created_utc": "2026-02-20 18:19:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ggscz",
          "author": "TopTippityTop",
          "text": "These people must not be using 5.3 codex",
          "score": 5,
          "created_utc": "2026-02-20 17:31:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h0z2h",
          "author": "ILikeBubblyWater",
          "text": "Mate if you believe AI cant churn out fully fledged websites you clearly are not doing this full time.\n\nWith stuff like multi provider planning loops and ralph loops and claude code you can absolutely push out whole products in a couple hours.\n\nWriting code is a deprecated way of coding already, people are just coping and apply unreasonable high standards to AIs that they would not ask from a human",
          "score": 5,
          "created_utc": "2026-02-20 19:02:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ho80r",
              "author": "alcalde",
              "text": "> people are just coping and apply unreasonable high standards to AIs that they would not ask from a human\n\nYou don't expect humans to deliver functioning code with tests and documentation?",
              "score": 1,
              "created_utc": "2026-02-20 20:55:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ho1l1",
              "author": "alcalde",
              "text": "Websites aren't code. ",
              "score": -2,
              "created_utc": "2026-02-20 20:54:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h6b9h",
          "author": "Mystical_Whoosing",
          "text": "this is such an old take on this, even 1 year ago this shouldn't be the case, but today? You just share that here is a new tech and the people you survey cannot keep up",
          "score": 5,
          "created_utc": "2026-02-20 19:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gq4a7",
          "author": "bakawolf123",
          "text": "Cope is good but the better you learn to use them the scarier it gets...  \nWhile I don't think pure AI agents replacing humans is a realistic approach in foreseeable future, a cheaper weaker dev competent in using wide range of AI tools replacing senior staff is quite a possibility.",
          "score": 6,
          "created_utc": "2026-02-20 18:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gs2tj",
              "author": "Healthy-Nebula-3603",
              "text": "You know AI agents did not even exist a year ago (models were not trained this way yet )  not even codex-cli or claudie-cli ...\n\nAnd you are claiming AI will not replace you soon ?",
              "score": -1,
              "created_utc": "2026-02-20 18:22:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g15s2",
          "author": "Purple_Ice_6029",
          "text": "Also, it will get much more expensive as the investors require an ROI, making it not as appealing. *pop*",
          "score": 5,
          "created_utc": "2026-02-20 16:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ge91k",
              "author": "DHFranklin",
              "text": "I imagine this might be the year that \"good enough\" AI is paired with really good centauring and the UI/UX will show more custom built stuff. So that the cost per hour in sheparding the AI would have that demonstable ROI.\n\nSo just like how combines don't drive themselves, million dollar AI workflows won't either. That doesn't mean that they won't radically change the work that's done.",
              "score": 0,
              "created_utc": "2026-02-20 17:19:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g2a2i",
          "author": "Jumpy_Ad_2082",
          "text": "now present this to a manager and convince him who is more profitable.",
          "score": 4,
          "created_utc": "2026-02-20 16:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g49z6",
          "author": "Needausernameplzz",
          "text": "great write up",
          "score": 2,
          "created_utc": "2026-02-20 16:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g6nrd",
          "author": "Otherwise_Wave9374",
          "text": "This tracks with what Ive seen: benchmarks look great, but \"agent does real freelance work end to end\" is mostly about reliability, context management, and actually knowing when it doesnt know. The advice about small bites + verification is the only sane way to use agents today. I also think tooling (tests, linters, sandboxes, traces) matters more than the model for most workflows. If you want more practical patterns for using AI agents without falling into the prompt loop, Ive got a few notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-20 16:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h0opz",
          "author": "Smarterchild1337",
          "text": "The broad conclusions of this post are at least a year out of date, which is an eon in terms of AI progress during that time",
          "score": 2,
          "created_utc": "2026-02-20 19:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g759p",
          "author": "masterlafontaine",
          "text": "Can you elaborate a bit more on the columns? What are these integrals with games?",
          "score": 1,
          "created_utc": "2026-02-20 16:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ho6qi",
          "author": "thedarkbobo",
          "text": "We are doomed, I don't agree, we have max few years ;p",
          "score": 1,
          "created_utc": "2026-02-20 20:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ianih",
          "author": "Expert-Reaction-7472",
          "text": "LLMs are very very good at writing code to the point where anyone writing code by hand will be out of a job as a result of that. Anyone who thinks otherwise is delulu.",
          "score": 1,
          "created_utc": "2026-02-20 22:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ja2mm",
          "author": "keamo",
          "text": "Tell me you're using AI to create excel files without telling me you're not using  using AI to write yourself a frontend for data visualizations. Tell me you're only using excel by showing me a screen shot of you not asking AI to create a tailwind/react/vite with chartjs visuals, tell me your middle name.",
          "score": 1,
          "created_utc": "2026-02-21 02:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nqzpp",
          "author": "HiggsBoson2738",
          "text": "\"how do I center a div\", FFS...",
          "score": 1,
          "created_utc": "2026-02-21 20:30:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g33p1",
          "author": "Eventual-Conguar7292",
          "text": "Kimi 2.5 Is better than Deepseek?\n\nBut my real question What use case In Local LLM,Here are use cases I can think of\n\n1. Simple code generation - Web visualizations, data dashboards, interactive charts\n2. Image generation - Ads, logos, creative visual content (Using LLM to understand text prompts)\n3. Audio production - Sound effects, voice-over merging, track separation (Using LLM to understand text prompts)\n4. Text-based tasks - Report writing, data retrieval, web scraping",
          "score": 1,
          "created_utc": "2026-02-20 16:28:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6g6ug0",
              "author": "twack3r",
              "text": "Why are you questioning your own result?",
              "score": 2,
              "created_utc": "2026-02-20 16:45:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gciey",
                  "author": "Eventual-Conguar7292",
                  "text": "I said the benchmarks were made up in title a in way.\n\nHow I created the benchmark:\n\n1. Test it myself first - I spend my time running Sonnet 4, Gemini, DeepSeek, etc. on zero-shot tasks, then compare the outputs\n2. Then I back it up -  I Find public benchmarks that support my findings\n\nWhy does this exist? I don't make any money from posting, so you get free quality. I'd rather trust my own opinion than internet benchmarks, but I probably cant give you value with time I got.",
                  "score": 1,
                  "created_utc": "2026-02-20 17:11:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gga7u",
              "author": "Zerokx",
              "text": "5. Human Computer Interfacing - Voice Recognition can be small and local, executing simple commands based on users in home automation or whatever in human language  \n6. Privacy - Any task which would expose a lot of private information to the outside world, like refining your CV, or searching through personal files  \n7. Freedom - Using the AI for tasks that would be censored by company policies, like making satirical content of public figures, content rated for adults, whatever mischievous acts you come up with like some will use it for scamming or tell them how to create illegal substances etc.",
              "score": 1,
              "created_utc": "2026-02-20 17:29:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ga6hc",
          "author": "bgptcp179",
          "text": "Hmmm, sounds like something an AI agent would say.  GET HIM!\n\nSeriously tho, cool info",
          "score": 1,
          "created_utc": "2026-02-20 17:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6grphw",
          "author": "teamharder",
          "text": "What were the tasks? How does the score correlate to the human percentage? Given my current experience, these numbers dont add up. \n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.\n\n\nI did this all in my spare time on the weekends. I dont know how to write code. I just plan and test thoroughly with use case scenarios. Realistically Claude did the work and gets the credit for it. How that measures out to 2.46% is what confuses me. ",
          "score": 1,
          "created_utc": "2026-02-20 18:21:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g20j1",
          "author": "nomorebuttsplz",
          "text": "Question:\n\nHow can a benchmark like RLI account for the fact that once people recognize an AI can be given a task and can complete it, it won't be considered human work anymore? It seems like an ever-changing standard specifically focused on the delta between human and AI work.",
          "score": -2,
          "created_utc": "2026-02-20 16:23:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7dbwm",
      "title": "I built VELLE.AI - a local AI companion with memory, voice, quant engine, and a full productivity suite. No cloud, no subscriptions. Everything on your machine.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "author": "Rich_Supermarket_164",
      "created_utc": "2026-02-17 17:54:58",
      "score": 50,
      "num_comments": 27,
      "upvote_ratio": 0.84,
      "text": "Hey everyone, I've been building this for a while and finally shipped it.\n\n**VELLE.AI** is a local AI operating system that runs on top of Ollama. It's not just another chat wrapper. It's a full personal assistant with persistent memory, two-way voice, a quantitative finance engine, and a productivity suite with todos, habits, goals, journal, and achievements.\n\n**What makes it different:**\n\n* **Persistent memory** — it actually remembers you across sessions. Your preferences, your name, your projects. All stored locally in SQLite.\n* **Two-way voice** — speech-to-text plus text-to-speech with hands-free mode. Talk to it, it talks back.\n* **7 personalities** — switch between Default, Sarcastic, Evil Genius, Anime Mentor, Sleepy, Kabuneko (finance gremlin), and Netrunner (cyberpunk). They actually stay in character.\n* **Kabuneko Quant Engine** — real-time stock quotes, full technical analysis including RSI, MACD, Bollinger, ADX, Sharpe, momentum scanning, value dislocations, backtesting, sentiment analysis, and a 4-bucket stock ideas generator. All from Yahoo Finance, no API keys needed.\n* **Productivity suite** — task manager with priorities, projects, due dates, habit tracker with streaks and weekly grids, pomodoro timer, goal system with milestones and progress bars, personal journal with writing prompts, bookmarks, knowledge base.\n* **25 achievements** — unlock badges as you use it. Toast notifications slide in when you earn one.\n* **Auto-insights** — detects patterns like \"work has been a recurring stressor 4 times this week\" or \"you created 15 tasks but only completed 3.\"\n* **Daily briefing** — one command gives you mood, tasks, habits, goals, streaks, reminders, and market data.\n* **Local file search** — searches your Desktop, Documents, Projects, Code directories by filename and content.\n* **System commands** — opens apps, runs PowerShell commands, controls your machine.\n* **Proactive reminders** — \"remind me to check email in 10 minutes\" actually fires with browser notifications plus text-to-speech.\n* **Cyberpunk terminal UI** — because aesthetics matter.\n\n**Tech stack:** Node.js, Express, WebSocket, SQLite, Ollama, vanilla JS. About 8,000 lines across 6 server modules. Works with any Ollama model including qwen3:8b, llama3, mistral. Ships as a Windows .exe or run from source on any OS.\n\n**Zero external AI APIs. Zero telemetry. Zero cloud. Everything local.**\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7dbwm/i_built_velleai_a_local_ai_companion_with_memory/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5xggn2",
          "author": "Barachiel80",
          "text": "Github repo or this is AI slop vaporware",
          "score": 14,
          "created_utc": "2026-02-17 20:34:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cuwsn",
              "author": "Rich_Supermarket_164",
              "text": "it's my repo",
              "score": 1,
              "created_utc": "2026-02-20 02:54:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x69lm",
          "author": "BonebasherTV",
          "text": "unrevocable-lyla-gleefully.ngrok-free.dev is a tunnel to the OP’s computer most likely. If the OP has it turned on it works otherwise it won’t work. \nWould love to see the repo of this. To understand the interactions between the different components.",
          "score": 11,
          "created_utc": "2026-02-17 19:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ycgk3",
              "author": "Rich_Supermarket_164",
              "text": "here made it public [https://github.com/velle999/velle.ai](https://github.com/velle999/velle.ai) ",
              "score": 9,
              "created_utc": "2026-02-17 23:10:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wow5y",
          "author": "Awkward-Customer",
          "text": "What are you using for the finance aspect of this? Also, is there a reason you chose to support only ollama and not llama.cpp?",
          "score": 4,
          "created_utc": "2026-02-17 18:24:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wpafn",
          "author": "Fair-Cookie9962",
          "text": "[velle.ai](http://velle.ai) site seems down, [vella.ai](http://vella.ai) seems something different. There is lot of things named velle or vellum, which is confusing.",
          "score": 3,
          "created_utc": "2026-02-17 18:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o692cmw",
              "author": "CarolloSenpai",
              "text": "To my understanding it's local!",
              "score": 1,
              "created_utc": "2026-02-19 15:04:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wpdk5",
          "author": "rusty_daggar",
          "text": "Do you have  link? \"velle.ai\" points to another address that is broken",
          "score": 2,
          "created_utc": "2026-02-17 18:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wwapq",
          "author": "eazolan",
          "text": "I'm surprised you didn't make \"Hyper competent assistant\" one of the personalities.",
          "score": 1,
          "created_utc": "2026-02-17 18:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xwfnr",
          "author": "Christosconst",
          "text": "So you built an operating system? For a companion bot? With filesystems and printer drivers and whatnot?",
          "score": 1,
          "created_utc": "2026-02-17 21:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zns9y",
          "author": "dropswisdom",
          "text": "Looks great. Can you help to package it in a docker that can use an existing (separate) ollama instance running on another docker with docker compose?",
          "score": 1,
          "created_utc": "2026-02-18 03:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n3vrt",
              "author": "Rich_Supermarket_164",
              "text": "docker pull velle999/velle.ai - open port 3000, should be working",
              "score": 1,
              "created_utc": "2026-02-21 18:32:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wx6f9",
          "author": "Far_Cat9782",
          "text": "Wtf I'm in the middle of the same thing my god what a small word.i already have  rag, chat history,  image generation, web search, different agent cards with different functionality like creative writing agen who generates images to go along with stories.a media manager agent who handles jellyfin and can list all the media on your server with movie posters and allows you  watch the movie right there or cast to a tc. I'm mind blown that we all have the same ideas. I call mines June.ai\n\nhttps://preview.redd.it/fmys6r6as3kg1.png?width=1220&format=png&auto=webp&s=b7cbb3c25cfcd491e0b82b644ccdb4201a28809d",
          "score": 1,
          "created_utc": "2026-02-17 19:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wz1ib",
              "author": "Awkward-Customer",
              "text": "I suspect a lot of people are working on this right now. the biggest hurdle is probably ingesting the data from so many different sources, some of which deliberately silo their data.",
              "score": 3,
              "created_utc": "2026-02-17 19:11:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5wxh8g",
              "author": "Far_Cat9782",
              "text": "Here's another screenshot of the jellyfin integration\n\nhttps://preview.redd.it/jlcqpypzr3kg1.png?width=1220&format=png&auto=webp&s=150883919c51839ac805765f3f68e647184bd69b",
              "score": 0,
              "created_utc": "2026-02-17 19:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wxlnv",
          "author": "Far_Cat9782",
          "text": "The agents\n\nhttps://preview.redd.it/gdip5u34s3kg1.png?width=1220&format=png&auto=webp&s=6f18ea71747a245355eda9ab074379cbe527801f",
          "score": 1,
          "created_utc": "2026-02-17 19:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x6g39",
          "author": "Sidze",
          "text": "I wonder why it's on Ollama if MLX models are more efficient for Apple Silicon on MacOS. I guess it could be connecting to Osaurus for MLX models and more efficiency.",
          "score": 1,
          "created_utc": "2026-02-17 19:46:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xjx07",
              "author": "Fair-Cookie9962",
              "text": "Ollama sound familiar, easier to trust.",
              "score": -1,
              "created_utc": "2026-02-17 20:50:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xph8b",
                  "author": "Sidze",
                  "text": "MLX is a framework built by Apple, not easier to trust?  \nThough I get it - much easier to plugin Ollama and forget it, instead of creating the whole MLX host manager. Anyway.",
                  "score": 2,
                  "created_utc": "2026-02-17 21:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6hmxl",
      "title": "The Mac Studio vs NVIDIA Dilemma – Best of Both Worlds?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r6hmxl/the_mac_studio_vs_nvidia_dilemma_best_of_both/",
      "author": "JournalistShort9886",
      "created_utc": "2026-02-16 18:29:39",
      "score": 41,
      "num_comments": 40,
      "upvote_ratio": 0.92,
      "text": "Hey, looking for some advice here.\n\nI’m a person who runs local LLMs and also trains models occasionally. I’m torn between two paths:\n\nOption 1: Mac Studio – Can spec it up to 192gb(yeah i dont have money for 512gb) unified memory. Would let me run absolutely massive models locally without VRAM constraints. But the performance isn’t optimized for ML model training as to CUDA, and the raw compute is weaker. Like basic models would tale days\n\nOption 2: NVIDIA GPU setup – Way better performance and optimization (CUDA ecosystem is unmatched), but I’m bottlenecked by VRAM. Even a 5090 only has 32GB,.\n\nIdeally I want the memory capacity of Mac + the raw power of NVIDIA, but that doesn’t exist in one box.\n\nHas anyone found a good solution? Hybrid setup? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6hmxl/the_mac_studio_vs_nvidia_dilemma_best_of_both/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5r1ap1",
          "author": "Karyo_Ten",
          "text": "What are the sizes of models you want to train?\n\nBest is probably to train on runpod, rent a B200 or H100x8 for 8hours and be done with it.\n\nNow for inference 192GB gets you interesting models (Qwen, MiniMax, StepFun) but not \"absolutely massive\" models like DeepSeek, GLM, Kimi K2.\n\nYou didn't say your use case. For chatting/RP Macs will be good. For agentic coding you'll wait forever when you dump large files or large webpages / documentation into it.",
          "score": 10,
          "created_utc": "2026-02-16 20:57:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u9u1m",
              "author": "TrendPulseTrader",
              "text": "+1 train on runpod or similar",
              "score": 1,
              "created_utc": "2026-02-17 09:53:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o60pthd",
              "author": "cibernox",
              "text": "This may not be the case anymore in a few weeks when the M5 family chips land. They have ML accelerators similar to cuda cores and promo processing might be 4x what current models get (at least the base M5 runs circles around base M4)",
              "score": 1,
              "created_utc": "2026-02-18 08:22:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60s1xz",
                  "author": "Karyo_Ten",
                  "text": "Fair point. But Apple may also x5 the prices to follow the RAM premium.",
                  "score": 1,
                  "created_utc": "2026-02-18 08:43:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bitxi",
                  "author": "voyager256",
                  "text": "Aren’t you confusing CPU with GPU, or just mean M4 vs M5 chips in general as SoC? Cause e.g. M5 and future M5 Max are different things .  \nFor LLM performance only GPU makes significant difference (and it’s memory bandwidth) . So I guess Apple improved GPU architecture with M5 e.g. so called Neural Accelerators , unless they also found efficient way to use NPUs in parallel for LLMs?. \n\nApple already has CUDA equivalent etc.",
                  "score": 1,
                  "created_utc": "2026-02-19 22:12:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q9hiv",
          "author": "HealthyCommunicat",
          "text": "I have a 5090 workstation and 378 gb of mac unifed memory. \n\nUSE of the model is going to always be so much more important and will only be a such tiny part of your time compared to TRAINING or other CUDA things in real world cases.\n\nTwo dgx sparks can’t even beat the m3 ultra in terms of t/s, and the prefix cache fixes the prmpt processing issues if you are using coding loops or normal use case of conversations and not massive massive data processing isn’t your #1 requirement - but inferencing the biggest models at the best speed is ALWAYS going to be yohr main use case and need, and you’re kidding yourself if you say otherwise as the time and use of the things that are needed in CUDA are super niche and such a dramatic portion of your time will be spent on inferencing and using models itself.\n\nIf your on mac check this out for the fastest server / plug and play agentic coding tool: https://vmlx.net/",
          "score": 15,
          "created_utc": "2026-02-16 18:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qm4d0",
              "author": "DataGOGO",
              "text": "Sparks are not intended to be fast local inference machines. They are development consoles that run the exact same hardware and software stack as the massive clusters, meaning you dev and test on the cheap little spark before you push big jobs to the datacenter full of clusters. If that isn’t you, don’t buy sparks. \n\nIf you are just running a personal use chatbox, and want to mess around with running larger models (albeit slowly), then I mostly agree with you.\n\nBut anything beyond that, CUDA isn’t niche, it is THE industry standard in which everything is built on. ",
              "score": 13,
              "created_utc": "2026-02-16 19:43:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rmw8o",
                  "author": "luix93",
                  "text": "A spark is not much slower in pure t/s than an m3 ultra, but is much faster in prompt processing. It is also twice or more faster at anything that deals with image or video generation, and an Asus gx10 can be had for less than 3k. If one is looking for a little box that they can hide anywhere with low power consumption then that makes it also a good pick for inferencing imho, as long as you like to thinker with stuff. I love mine personally.",
                  "score": 2,
                  "created_utc": "2026-02-16 22:44:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5tmail",
              "author": "NeverEnPassant",
              "text": "Prefix caching doesn't solve slow prompt processing with coding models.",
              "score": 2,
              "created_utc": "2026-02-17 06:14:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5trgm9",
                  "author": "HealthyCommunicat",
                  "text": "It doesn’t completely yeah but for a good wide range of use cases that is actual more home use automation and not RAG of needing to constantly scrape a crap ton of text, going through a properly structed project should be decent. I work on full sites with a custom cli agent and its been pretty nice so far.",
                  "score": 0,
                  "created_utc": "2026-02-17 06:59:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rb80m",
          "author": "Creepy-Bell-4527",
          "text": "Macs are good at inference, not training.\n\nIn fact the RTX 5090 won't get you far on training either.",
          "score": 4,
          "created_utc": "2026-02-16 21:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sgdso",
          "author": "clwill00",
          "text": "Yeah, I have a large Mac Studio and played around. Ugh. Decided to go all in, built a monster AI rig running Windows. AMD Threadripper, 128gb DDR5 ram, Samsung 8tb 9100 ssd, and RTX 6000 workstation with 96gb vram. Your “doesn’t exist in one box” you mentioned above. It rocks.",
          "score": 5,
          "created_utc": "2026-02-17 01:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r8p4g",
          "author": "wouldntthatbecool",
          "text": "Read the recommendations for Kimi K2.5 yesterday, and it is 2x4090's and 1.92TB of RAM.",
          "score": 3,
          "created_utc": "2026-02-16 21:33:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rfr17",
          "author": "Zen-Ism99",
          "text": "Yup, I’m looking forward to the M5 Ultra…",
          "score": 3,
          "created_utc": "2026-02-16 22:08:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3p2h",
              "author": "JournalistShort9886",
              "text": "Fr same",
              "score": 2,
              "created_utc": "2026-02-17 08:53:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qq64k",
          "author": "Proof_Scene_9281",
          "text": "I think it depends on the use-case. Initially i thought building codes through the commercial API's was going to be cost prohibitive and painful. But now I've pretty much built everything that was needed with a claude Max subscription and ChatGPT pro. It's not even close to the cost of local hardward, especially in todays pricing. \n\ni'm still looking for a good use-case for my 4x3090 machine. ",
          "score": 3,
          "created_utc": "2026-02-16 20:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rya8u",
              "author": "bac2qh",
              "text": "Vibevoice asr to record meetings and transcribe. That’s what I am doing now lol",
              "score": 1,
              "created_utc": "2026-02-16 23:47:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ret1o",
          "author": "Zen-Ism99",
          "text": "Will MLX not work for you?",
          "score": 2,
          "created_utc": "2026-02-16 22:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3niu",
              "author": "JournalistShort9886",
              "text": "It does my initial models were trained on mlx on a macbook m2 ,though it is not as optimized and slower than nvidia  \n Plus im not a enterprise level model trainer,im more like a “enthusiast ” level who adjusts scale according to hardware currently i have rtx5080 and i trained 600m from scratch ,if i have more i will train more,that said maybe mac studio is the only option",
              "score": 1,
              "created_utc": "2026-02-17 08:53:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s4hd8",
          "author": "SDusterwald",
          "text": "For Nvidia vs Mac - main question would be if you want to use any diffusion models alongside the LLMs. Macs are okay at LLM inference, but for image/video gen I highly recommend the Nvidia route at this time.\n\nMore importantly, if you do decide on the Mac route I highly recommend waiting for the M5 Ultra MacStudio. It should be coming later this year and will be far better for all AI workloads than the previous gen Macs due to the built in matmul acceleration in the M5 GPU. Spending that much money now when a huge upgrade is just around the corner makes no sense (if you can't wait I'd probably just go for Nvidia - not going to see any new Nvidia GPUs for at least a year, maybe two).",
          "score": 2,
          "created_utc": "2026-02-17 00:23:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u45wn",
              "author": "JournalistShort9886",
              "text": "Right ,yes i can wait its not like i was going to buy it tomm,i was planning for future\nThanks for your suggestion!",
              "score": 1,
              "created_utc": "2026-02-17 08:58:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5rh562",
          "author": "hermjohnson",
          "text": "Have you considered one of the Nvidia GB10 devices (ie DGX Spark)? I just ordered the Asus version for $3k. 128GB of shared memory.",
          "score": 1,
          "created_utc": "2026-02-16 22:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u3zo0",
              "author": "JournalistShort9886",
              "text": "Yeah heard it is good ;though for your use case is the unified memory gb/s enough,like isnt it 200-300gb/s,that said 128gb is still impressive and 1000tflops on fp4 is great for training models like in 1.5b range \nGuess we cant be too greedy😅",
              "score": 1,
              "created_utc": "2026-02-17 08:56:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sy86v",
          "author": "midz99",
          "text": "This is how nividia controls the market. wait for the new mac studio. coming from someone who owns 4 nvidia 6000 adas",
          "score": 1,
          "created_utc": "2026-02-17 03:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tcup6",
          "author": "syndorthebore",
          "text": "I have 4 RTX pro blackwell 6000 max-q on a workstation.\n\nThis feels barely ok to train, a mac won't do for training at all.\n\nIt depends on use case, I'll be honest, just rent clusters it's way better price/output ratio.\n\n\n\nI also do video music and image generation, if you want to dip your toes in this, the mac won't do either. ",
          "score": 1,
          "created_utc": "2026-02-17 05:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ttsa4",
          "author": "Chlorek",
          "text": "I burned myself a few times on seemingly good hardware only to discover subpar or even nonexistent software support for it. I felt bad about it and it was not even a big investment. Therefore I see Mac the same way vs CUDA on nvidia. I would be very careful with pumping big sums of money into systems I am not sure of. As for Macs I read you need to go with top models as memory bandwidth is not that great on lower ones.",
          "score": 1,
          "created_utc": "2026-02-17 07:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qky31",
          "author": "DataGOGO",
          "text": "If you are doing any training at all, the mac is not really an option.\n\nIf you are just serving models the Mac works pretty well.\n\nIn terms of local hardware, you are not going to do any real training with consumer gaming GPU’s you will need at least an RTX Pro BW, but even then you only have 96GB of VRAM; realistically, you would need 4 or 8; or buy 4 H200 NVL’s (~$130k), and that is an entry point. \n\nThe real answer for occasional training is you rent the clusters by the hour. \n\nThat said, if you are just learning, a RTX 5090 will work just fine for labs / making very small models.",
          "score": 0,
          "created_utc": "2026-02-16 19:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q9bkw",
          "author": "Antique_Dot_5513",
          "text": "Yes, it's called an API. Otherwise, rent a more powerful GPU online.",
          "score": -2,
          "created_utc": "2026-02-16 18:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5resv4",
              "author": "Ryanmonroe82",
              "text": "Or buy the compute and not be locked in to api costs.  I made a dataset this passed week and final result was 280 million tokens.  That's many thousands of dollars in api costs right there, cheaper to buy something in the long run",
              "score": 7,
              "created_utc": "2026-02-16 22:03:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7bohc",
      "title": "[macOS] Built a 100% local, open-sourced, dictation app. Seeking beta testers for feedback!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r7bohc",
      "author": "AdorablePandaBaby",
      "created_utc": "2026-02-17 16:58:03",
      "score": 38,
      "num_comments": 41,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7bohc/macos_built_a_100_local_opensourced_dictation_app/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5w75dq",
          "author": "Robby2023",
          "text": "Looks really good! Kudos for this.\n\nOne question, how much RAM do you need in order for it to work properly?",
          "score": 4,
          "created_utc": "2026-02-17 17:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w83la",
              "author": "AdorablePandaBaby",
              "text": "I've tested reliably on a 4GB machine. Wouldn't go lower than that tbh.",
              "score": 3,
              "created_utc": "2026-02-17 17:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5w8qb7",
                  "author": "Robby2023",
                  "text": "Which model does it use behind the scenes?",
                  "score": 1,
                  "created_utc": "2026-02-17 17:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w8tdr",
          "author": "rusty_daggar",
          "text": "I don't have a mac to try it on, but it looks like a nice idea.\n\nAre you using a VAD to clean up the audio or just passing all straight to whisper?",
          "score": 3,
          "created_utc": "2026-02-17 17:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9v0i",
              "author": "AdorablePandaBaby",
              "text": "Straight to whisper. \n\nBut I haven't felt the need to remove bg noise yet.   \nMaybe a lower priority improvement later down the line.",
              "score": 2,
              "created_utc": "2026-02-17 17:14:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wp082",
          "author": "JohnHawley",
          "text": "I've been using Handy, it's been great! [https://github.com/cjpais/Handy](https://github.com/cjpais/Handy)  \nWhat does SpeakType do differently? Is there extra logic that Handy doesn't perform?",
          "score": 6,
          "created_utc": "2026-02-17 18:25:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w759k",
          "author": "AdorablePandaBaby",
          "text": "The title should say \"open source\", instead of \"open-sourced\", but I guess its too late.",
          "score": 2,
          "created_utc": "2026-02-17 17:01:03",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5w9buv",
          "author": "iongion",
          "text": "UI is gorgeous",
          "score": 2,
          "created_utc": "2026-02-17 17:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9zfe",
              "author": "AdorablePandaBaby",
              "text": "Thank you!  \nMy OCD drove me crazy, but I'm glad someone liked it as well.",
              "score": 2,
              "created_utc": "2026-02-17 17:15:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wx6ob",
          "author": "Pitiful-Impression70",
          "text": "nice, the local-only approach is the way to go imo. quick question tho, does it do any LLM post-processing on the whisper output? like adding punctuation, fixing capitalization, formatting stuff based on context? raw whisper output is usually pretty good but it still dumps everything as one block of text which is annoying when youre dictating emails or notes. thats been the main gap ive seen with most local dictation tools vs the cloud ones that have an extra formatting pass",
          "score": 2,
          "created_utc": "2026-02-17 19:02:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wxac1",
              "author": "AdorablePandaBaby",
              "text": "Working on that rn :)",
              "score": 1,
              "created_utc": "2026-02-17 19:03:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ywjpm",
                  "author": "Pitiful-Impression70",
                  "text": "nice, thats the part that really makes or breaks it imo. like raw whisper output is fine for short stuff but anything longer than a paragraph and the lack of punctuation and formatting makes it basically unusable without cleanup. curious if youre gonna do it locally too or offload to an api",
                  "score": 1,
                  "created_utc": "2026-02-18 01:01:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xf3ls",
          "author": "Appropriate-Deer234",
          "text": "Looks nice and i also love that everything stays offline. Already downloaded and will test it tomorrow on M1 and M3. I also would have the 2019 Intel if you need feedback from that.",
          "score": 2,
          "created_utc": "2026-02-17 20:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zo00h",
              "author": "AdorablePandaBaby",
              "text": "Yes perfect. Will DM you!",
              "score": 1,
              "created_utc": "2026-02-18 03:29:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xx0s1",
          "author": "CaptainSuckie",
          "text": "I'd like to test this out! ",
          "score": 2,
          "created_utc": "2026-02-17 21:51:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y4oh0",
          "author": "DertekAn",
          "text": "Yes please? 🤭🤭",
          "score": 2,
          "created_utc": "2026-02-17 22:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wax8h",
          "author": "vulture916",
          "text": "Interested! ",
          "score": 1,
          "created_utc": "2026-02-17 17:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wepnv",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:37:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5weryo",
              "author": "vulture916",
              "text": "Spinning Wheel loop on Transcribe using pre-built app. Can't get anything transcribed using Whisper Large V3 Turbo on Macbook M1 14.1 Sonoma.\n\n  \nChecked settings and for whatever reason Audio Input was defaulted to NoSound rather than Macbook microphone. Changed that, same behavior.  ",
              "score": 1,
              "created_utc": "2026-02-17 17:38:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5widpv",
          "author": "Meowliketh",
          "text": "Hey, I’d love to test this out!",
          "score": 1,
          "created_utc": "2026-02-17 17:55:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5win06",
              "author": "AdorablePandaBaby",
              "text": "DMing",
              "score": 1,
              "created_utc": "2026-02-17 17:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60zbpl",
                  "author": "Meowliketh",
                  "text": "Don't think I got the DM? Also, love your user name",
                  "score": 1,
                  "created_utc": "2026-02-18 09:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wy4sg",
          "author": "sinebubble",
          "text": "I'm unclear what problem this solves on macOS. Is the implication that Apple's built-in speech dictation is not private? I use the built-in dictation service all the time to transcribe into every app I've used it with.",
          "score": 1,
          "created_utc": "2026-02-17 19:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wzb1k",
              "author": "AdorablePandaBaby",
              "text": "Correct. That's the main usecase, but Apple's STT doesn't detect words properly for a lot of my usecases. \n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 2,
              "created_utc": "2026-02-17 19:12:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wyc52",
          "author": "solipsistmaya",
          "text": "Looks good, interested in testing.",
          "score": 1,
          "created_utc": "2026-02-17 19:08:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x3249",
          "author": "Ok_Yoghurt248",
          "text": "does it work on windows ?",
          "score": 1,
          "created_utc": "2026-02-17 19:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5x5q7r",
          "author": "aqdnk",
          "text": "would like to test!",
          "score": 1,
          "created_utc": "2026-02-17 19:43:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xa2en",
          "author": "Correct_Support_2444",
          "text": "I will be trying this out later this week when I get home from a trip. This is exactly what I’ve been looking for.",
          "score": 1,
          "created_utc": "2026-02-17 20:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xni9n",
          "author": "Driftwintergundream",
          "text": "I'm an active user of superwhisper but looking for a local model TTS.\n\nTo me the 3 key features that would sell me is 1) super fast, 2) super accurate, and 3) slight touch up options of the text to remove verbal mispeaks or ums.\n\nI'm testing it out!",
          "score": 1,
          "created_utc": "2026-02-17 21:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zoso6",
              "author": "AdorablePandaBaby",
              "text": "Yes, then SpeakType should be just the best STT for you. It's fast, accurate and removes all the verbal misspeaks you're concerned about",
              "score": 1,
              "created_utc": "2026-02-18 03:34:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yhhu6",
          "author": "The_BeatingsContinue",
          "text": "So, you decided against a $12/month subscription and i think everybody does. In times of Claude Code it's incredibly easy to invest a little time to solve the whole task selfmade.\n\nMy question is: how can you decide against a subscription model and still want money for it while claiming to be open source? No offense, just a serious question.\n\nAs an additional use case idea: I work with deaf people and having a Mac on a table that makes a whole conversation transparent for anyone at that table would be a great feature, requiring just a text output window scalable to fullsize screen, while letting users choose font/fontsizes and opting out the requirement to push a button. It just can be active all the time and can be started/stopped by a keypress.",
          "score": 1,
          "created_utc": "2026-02-17 23:38:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yzqge",
              "author": "CtrlAltDelve",
              "text": "It's a pretty common model for binaries to be paid, but the sources are open, and nothing stops you from building it with no restrictions. \n\nVoiceInk is one of these. \n\nIf you're familiar with Claude Code, you can just ask it to compile it for you?",
              "score": 1,
              "created_utc": "2026-02-18 01:18:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zjep0",
          "author": "rditorx",
          "text": "macOS has offline dictation, you just need to enable it in the System Preferences and download the language data in there. No need to trust third parties. It might send some data to Apple, though, so consult the privacy policy.",
          "score": 1,
          "created_utc": "2026-02-18 03:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zp8ju",
              "author": "AdorablePandaBaby",
              "text": "Yep, you're right.\n\nBut, Apple's STT doesn't detect words properly for a lot of usecases.\n\nThe entire market of STT apps exist primarily to solve the poor job done by the built in Mac STT.",
              "score": 1,
              "created_utc": "2026-02-18 03:37:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6231pt",
          "author": "adrian_dev_yyc",
          "text": "Been building something similar on the Windows side. Privacy is the number one thing people bring up when I ask why they don't use cloud dictation. What's your latency like? In my experience that matters way more than raw accuracy for whether people actually stick with it.",
          "score": 1,
          "created_utc": "2026-02-18 14:24:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o671d6b",
          "author": "Sudden-Ad-1217",
          "text": "I’m game, got an M1 Max I’d run it against.",
          "score": 1,
          "created_utc": "2026-02-19 05:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6853j3",
          "author": "Amaeze",
          "text": "yo! this looks great! interested in trying!",
          "score": 1,
          "created_utc": "2026-02-19 11:40:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68p10g",
          "author": "creativedoctor",
          "text": "If it works for portuguese of Portugal, I'm game. Drop me a DM with the instructions, if so, please!",
          "score": 1,
          "created_utc": "2026-02-19 13:52:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6d1qy6",
          "author": "duedev",
          "text": "Found this while searching for a local dictating software for macOS. The one built in works intermittently for me on my m1 laptop.",
          "score": 1,
          "created_utc": "2026-02-20 03:37:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d1syj",
              "author": "AdorablePandaBaby",
              "text": "How did you find this?",
              "score": 1,
              "created_utc": "2026-02-20 03:38:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7umlq",
      "title": "[macOS] PersonaPlex-7B on Apple Silicon (MLX)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "author": "Apprehensive_Boot976",
      "created_utc": "2026-02-18 05:37:04",
      "score": 36,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "NVIDIA released an open-source speech-to-speech model [PersonaPlex-7B](https://huggingface.co/nvidia/personaplex-7b-v1). It listens and talks simultaneously with \\~200ms latency, handles interruptions, backchanneling, and natural turn-taking.\n\nThey only shipped a PyTorch + CUDA implementation targeting A100/H100, so I ported it to MLX, allowing it to run on Apple Silicon: [github.com/mu-hashmi/personaplex-mlx](https://github.com/mu-hashmi/personaplex-mlx).\n\nHope you guys enjoy!",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r7umlq/macos_personaplex7b_on_apple_silicon_mlx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o627q6u",
          "author": "felixlovesml",
          "text": "thanks. What’s the latency on a Mac — for example, on an M4 chip — and how much RAM does it require?",
          "score": 3,
          "created_utc": "2026-02-18 14:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o607o3d",
          "author": "former_farmer",
          "text": "Thanks.",
          "score": 1,
          "created_utc": "2026-02-18 05:44:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8iew6",
      "title": "Open Source LLM Leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mrakuwr3xbkg1.png",
      "author": "HobbyGamerDev",
      "created_utc": "2026-02-18 23:04:19",
      "score": 35,
      "num_comments": 32,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8iew6/open_source_llm_leaderboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67lo9v",
          "author": "jiqiren",
          "text": "Not enough people have 512GB+ of vram or unified memory (like the Mac Studio). Otherwise Minimax M2.5 would be top dog. 🐶",
          "score": 6,
          "created_utc": "2026-02-19 08:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6890vq",
              "author": "deepfit",
              "text": "Still pretty large but MiniMax-M2.5 UD-Q2\\_K\\_XL from unsloth is both fast and < 100G of ram/vram.  I am having a hard time telling any difference from larger quant versions.",
              "score": 2,
              "created_utc": "2026-02-19 12:11:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68mysa",
              "author": "AfterShock",
              "text": "Came here to +1 MinMax 2.5 when I saw it missing from the list.",
              "score": 2,
              "created_utc": "2026-02-19 13:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d0mb1",
                  "author": "jiqiren",
                  "text": "I only have a Mac mini so can’t run it. But it’s so cheap on openrouter it’s been my goto model.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:30:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67bwp1",
          "author": "einord",
          "text": "Fun fact: the larger the model, the more intelligent.",
          "score": 12,
          "created_utc": "2026-02-19 07:08:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67suaq",
              "author": "DistanceSolar1449",
              "text": "This ranking is trash. Only thing it gets right is the top 2. \n\nDeepseek V3? That came out in 2024. Deepseek R1 came out a full year ago.\n\nWhere’s Deepseek V3.2? Why is Mistral Large rated so highly, and the same tier as gpt-oss-120b? Why Nemotron V1 instead of Nemotron V1.5?",
              "score": 13,
              "created_utc": "2026-02-19 09:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a0weu",
                  "author": "onil34",
                  "text": "Where is minimax?",
                  "score": 2,
                  "created_utc": "2026-02-19 17:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67jksw",
              "author": "Successful-Emu-6409",
              "text": "scaling still alive ",
              "score": 6,
              "created_utc": "2026-02-19 08:19:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cisu8",
              "author": "Available-Craft-5795",
              "text": "Sometimes, bigger moves faster. Sometimes, smaller takes its time. Both find their way — eventually — just on different roads to the finish line.\n\nOne is built for the masses, for the noise and the need and the scale. But we build for the love of the question, for the joy in the curious trail.\n\nThey don't care about the small ones. We do.\n\n*Built with curiosity, not compute.* [CompactAI](https://huggingface.co/spaces/CompactAI/Built-with-curiosity-not-compute)\n\n(I know its a promo, shush)",
              "score": 1,
              "created_utc": "2026-02-20 01:39:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o687bd6",
          "author": "entheosoul",
          "text": "This should be split between actual locally runable models and cloud models (not exactly local)",
          "score": 6,
          "created_utc": "2026-02-19 11:58:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67hymk",
          "author": "nunodonato",
          "text": "Amazing how got oss 120b holds it's place after all these new models have came out",
          "score": 3,
          "created_utc": "2026-02-19 08:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o693exw",
          "author": "Sufficient_Prune3897",
          "text": "Cursed tier list. Shows that benchmarks are not everything",
          "score": 2,
          "created_utc": "2026-02-19 15:09:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67h8z0",
          "author": "HoustonTrashcans",
          "text": "Can anyone tell me what quantization I need to run a 1T model on my laptop with 8 GB of VRAM? If my math is right that's Q.05?",
          "score": 2,
          "created_utc": "2026-02-19 07:57:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67quwa",
              "author": "Ell2509",
              "text": "Not possible. But why do you need a 1t model? With 8gb vram, assuming you have 16gb system ram, you could run a 7 or 8b model, realistically. \n\nMore than that will become unusable slow quite quickly.",
              "score": 2,
              "created_utc": "2026-02-19 09:31:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68pel8",
                  "author": "ShinigamiOverlord",
                  "text": "Very much so. I could somewhat use a 10-14B models, but they cap at 1.3 tokens/s",
                  "score": 1,
                  "created_utc": "2026-02-19 13:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69t4rr",
          "author": "peva3",
          "text": "OP, anyway you could turn this data into an API? I could use these benchmarks for a project I'm working on.",
          "score": 1,
          "created_utc": "2026-02-19 17:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ad120",
          "author": "Far_Cat9782",
          "text": "Gpt 120b is my goal to run locally. Currently the max I can slso to get real work done is 24b model.",
          "score": 1,
          "created_utc": "2026-02-19 18:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b5oz6",
          "author": "Used-Dance-7006",
          "text": "Sorry...maybe this is the designer in me but the color coding is counterintuitive to the way i perceive design.\n\nIs S good? It's red so I read that as the worst?  \nIs C good?\n\nShould I be focusing on S and A models? Is D bad then?\n\nJust trying to understand and appreciate the clarity.",
          "score": 1,
          "created_utc": "2026-02-19 21:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bezen",
          "author": "shankey_1906",
          "text": "Something like this would be amazing for differnt tiers of VRAM, and use-cases. Tier <16GB, <32GB, etc. Tier: Coding, Reasoning, ...",
          "score": 1,
          "created_utc": "2026-02-19 21:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6flfy0",
          "author": "Fantastic-Breath2416",
          "text": "C'è anche il mio\n\nhttps://nothumanallowed.com/search\n\nPotete usarlo!!",
          "score": 1,
          "created_utc": "2026-02-20 15:05:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6ao8o",
      "title": "Alibaba’s Qwen team just released Qwen3.5-397B-A17B, the first open model in the Qwen3.5 family — and it’s a big one.",
      "subreddit": "LocalLLM",
      "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B",
      "author": "techlatest_net",
      "created_utc": "2026-02-16 14:12:59",
      "score": 28,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6ao8o/alibabas_qwen_team_just_released_qwen35397ba17b/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r5f8ym",
      "title": "Tutorial: Run MiniMax-2.5 locally! (128GB RAM / Mac)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/61b97oryxnjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-15 13:57:07",
      "score": 27,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r5f8ym/tutorial_run_minimax25_locally_128gb_ram_mac/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5thtsz",
          "author": "Euphoric_Emotion5397",
          "text": "tough luck. I think models can spread faster if they try to downsize to something that fits 16gb GPU and 64GB RAM.   Right now, I'm using Qwen 3 VL 30B. Very Good! :D",
          "score": 2,
          "created_utc": "2026-02-17 05:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5izi0k",
          "author": "Hitchhiker2TheFuture",
          "text": ">",
          "score": 0,
          "created_utc": "2026-02-15 16:12:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ralgvu",
      "title": "Local LLM for Mac mini",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "author": "Jiggly_Gel",
      "created_utc": "2026-02-21 08:00:24",
      "score": 27,
      "num_comments": 23,
      "upvote_ratio": 0.84,
      "text": "I’ve been watching hours of videos and trying to figure out whether investing in a Mac mini with 64 GB RAM is actually worth it, but the topic is honestly very confusing and I’m worried I might be misunderstanding things or being overly optimistic.\n\nI’m planning to build a bottom up financial analyst using OpenClaw and a local LLM, with the goal of monitoring around 500 companies. I’ve discussed this with ChatGPT and watched a lot of YouTube content, but I still don’t have a clear answer on whether a 30B to 32B parameter model is capable enough for this kind of workload.\n\nI’ll be getting paid for a coding project I completed using Claude, and I’m thinking of reinvesting that money into a maxed out Mac mini with 64 GB RAM specifically for this purpose.\n\nMy main question is whether a 30B to 32B local model is sufficient for something like this, or if I will still need to rely on an API. If I’ll need an API anyway, then I’m not sure it makes sense to spend so much on the Mac mini.\n\nI don’t have experience in this area, so I’m trying to understand what’s realistic before making the investment. I’d really appreciate honest input from people who have experience running local models for similar use cases.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6lpe1n",
          "author": "McMissile",
          "text": "I would just build whatever it is that you want on AWS and pay for cloud LLM tokens. Then you can figure out the feasability of what you're trying to achieve, and determine whether the 32b models will be sufficient. If it works, then great, buy the mac mini and run it locally. If not you've saved yourself a  bunch of money.",
          "score": 15,
          "created_utc": "2026-02-21 14:11:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6luuwp",
              "author": "AppointmentAway3164",
              "text": "This is the conclusion I reached. Targeting 30B models on huggingface. See how my use case tests. If it works then consider the $19k/$20k investment. Currently the the math doesn’t generally make sense for home token generation.",
              "score": 3,
              "created_utc": "2026-02-21 14:43:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ni9fh",
                  "author": "McMissile",
                  "text": "Yeah for most people I doubt running locally is ever really cost effective unless you need enormous amounts of tokens for sustained periods of time. 32b models that you can run locally have gotten fairly cheap to run in the cloud, especially compared to the price of a home lab.",
                  "score": 2,
                  "created_utc": "2026-02-21 19:44:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l3znc",
          "author": "rerorerox42",
          "text": "Maybe also look into tax discounts on investment into research and development.\n\nI have personally found newer 8B models (Ministral-3) to be capable on a 16GB M4 mini for what I have worked with by coding access to and using local models as needed with relevant context.\n\nAn issue currently is that it really is a little too soon for anybody to have tried out everything and every models’ true capabilities. FA-find-out kinda stage.",
          "score": 6,
          "created_utc": "2026-02-21 11:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l4xya",
              "author": "Jiggly_Gel",
              "text": "I was thinking of messing around with the FA-find-out by running multiple models and then using a Claude API and comparing the responses but thought I’d ask here first in case if anyone had any other suggestions. \n\nI’ve seen similar posts to mine but they seemed to be looking for other use cases but the tax thing is definitely interesting I’m going to give that a look thank you",
              "score": 2,
              "created_utc": "2026-02-21 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kr7hn",
          "author": "battle_pantZ",
          "text": "Yeah, Im asking this question myself so I’m curious which answers you’ll get\n\nAfaik: 64GB is not enough to match the performance of large models. Even Kimi needs 200-500GB to perform properly.",
          "score": 3,
          "created_utc": "2026-02-21 09:30:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ksr3h",
              "author": "Jiggly_Gel",
              "text": "I think a 30B model should work fine on a Mac mini but the question is what model and how powerful is a 30B model? Some people attribute Qwen 30B to match gpt 4o but then if I have to spend money on machinery for 500 gb I’d rather run it via APIs because most of my financial analysis is done via my python bot it’s just I need something for judging the analysis and doing further industry research, comparative analysis, and reading annual reports and the news daily",
              "score": 1,
              "created_utc": "2026-02-21 09:45:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6l5n4e",
                  "author": "DistanceSolar1449",
                  "text": "Easy answers.\n\n- Qwen 3 VL 32b\n\n- GLM 4.7 Flash\n\n- Qwen 3.5 35b\n\n- Qwen 3 Next\n\nThese would be the best models you can fit on 64GB.\n\nIs buying a $2k mac mini 64gb to run these models worth it? Hell no. You can run the first 3 on a 3090 for $900 or so. And it'd be way faster than a mac mini.\n\nThe 512GB Mac Studio is actually something to consider, since you can run GLM 5 and similar big models that can actually rival ChatGPT/Claude. But the 64GB mac mini? Definitely not. You're better off spending $2k on GPT api credits.",
                  "score": 5,
                  "created_utc": "2026-02-21 11:48:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kuhd3",
                  "author": "battle_pantZ",
                  "text": "\nI think it makes more sense to buy a high-performance Mac if you need it for demanding tasks outside of AI. I mean, spending $2-3k to “only” use gpt4? I also look into it every day and do my research, and then I always come back to APIs. Yesterday, I paid $20 for the deepseek API, and I have to say, it's very, very cheap and pretty powerful for coding. It's 90% cheaper than the competition, by the way. \n\nSo a larger investment is needed. Either an even more powerful Mac or several smaller Macs as a cluster. Until a new AI comes out that requires even more hardware performance. If you extrapolate that, APIs will probably be cheaper.",
                  "score": 5,
                  "created_utc": "2026-02-21 10:02:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m42f5",
          "author": "ScuffedBalata",
          "text": "\"Will a 30B model work for this use\" is very nebulous. It's not something you can just define.  It's like asking \"what IQ can be a financial analyst\".",
          "score": 3,
          "created_utc": "2026-02-21 15:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lr2su",
          "author": "HumbleTech905",
          "text": "Pay a few dollars for a service to deploy a 30B model, test and evaluate it, then decide if it's worth buying the Mac.",
          "score": 2,
          "created_utc": "2026-02-21 14:21:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o10qi",
          "author": "Comprehensive_Iron_8",
          "text": "If you think you'll love using openclaw using a local model. The local models who remotely perform as well will be worth multiple MAC STUDIOs costing thousands of dollars. A model running inside a 64GB ram mac mini is not worth using. You'll not love it. You can use it for small things like email classification projects and maybe a small RAG system. \n\nWhy I say this, OPUS is so good, that people who have once used it, do not like 5.3 codex just because it does not have that kind of personality. And tomorrow there will be another model which has the same kind of moat. \n\nBut none of them at this time are good enough to run in a 64GB ram mac mini.",
          "score": 2,
          "created_utc": "2026-02-21 21:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o1oyx",
              "author": "Comprehensive_Iron_8",
              "text": "When I say this. I think a $10k mac studio can run GLM-5 and Minimax 2.5 which I actually like to run(using cloud providers, because the $10k ROI is not there yet) for openclaw, and are decent. They make mistakes yes, but you can optimize them to make less of them. But nothing for a mac mini yet.",
              "score": 1,
              "created_utc": "2026-02-21 21:27:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6o9tgu",
                  "author": "nvidiabookauthor",
                  "text": "What are some non China cloud providers to test this?",
                  "score": 1,
                  "created_utc": "2026-02-21 22:10:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mvskq",
          "author": "egoslicer",
          "text": "I have a 128B Strix Halo and run 70-100B models, and I'd say they are 'ok' for OpenClaw generalist tasks and tool calling. \n\nHowever 64GB Mac Mini and targeting ~30B models? Even more restrictive than that and while some are capable with tool calling, the results are going to be pretty meh overall. \n\nIf I were you, I'd setup OpenClaw and run it through MiniMax or Kimi 2.5 apis like many people are doing for a couple of months to see how that runs for you. You can do that on any machine you currently have and get much stronger reasoning and results.\n\nFor a local setup, IMO, ~30B models are ok for scoped tasks, but doing financial analysis with a lot of variables is likely not going to give you great results.",
          "score": 1,
          "created_utc": "2026-02-21 17:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l93cm",
          "author": "thought_provoking27",
          "text": "Great thread .",
          "score": 1,
          "created_utc": "2026-02-21 12:18:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6rysj",
      "title": "I built a tool that cross-references every public Epstein document, flight log, email, and deposition. It found 25,700 person-to-person overlaps the media never reported.",
      "subreddit": "LocalLLM",
      "url": "/r/Epstein/comments/1r6pc2h/i_built_a_tool_that_crossreferences_every_public/",
      "author": "EricKeller2",
      "created_utc": "2026-02-17 01:07:31",
      "score": 24,
      "num_comments": 1,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r6rysj/i_built_a_tool_that_crossreferences_every_public/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o5xncaq",
          "author": "Cultural_Wear_7813",
          "text": "This is insane work. As someone who uses Reseek to manage my own research chaos, I can only imagine the pipeline you've built to handle 1.1M+ docs with semantic search and entity extraction. The hybrid retrieval and RAG pipeline you described is next level",
          "score": 1,
          "created_utc": "2026-02-17 21:06:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8awjx",
      "title": "Best Coding Model?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "author": "I_like_fragrances",
      "created_utc": "2026-02-18 18:23:42",
      "score": 22,
      "num_comments": 22,
      "upvote_ratio": 0.87,
      "text": "What is the best model for general coding. This includes very large models too if applicable.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o63oc6q",
          "author": "No_Clock2390",
          "text": "Qwen3-Coder-Next",
          "score": 22,
          "created_utc": "2026-02-18 18:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o644cn8",
              "author": "txgsync",
              "text": "This is the answer. At any affordable home-gamer size (<128GB) Qwen3-Coder-Next (80B-A3B) is the tits.\n\nEdit: and unlike Qwen3-Coder-30B-A3B it doesn’t give me the “fuck off, I just write code, figure it out yourself” energy when planning features. And -Next does better at ZorkBench, not just wandering around with a nasty knife waiting for something to happen until it rage-quits.",
              "score": 5,
              "created_utc": "2026-02-18 19:58:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o677w1y",
                  "author": "Sax0drum",
                  "text": "How much context can you reasonably run with 128GB?",
                  "score": 3,
                  "created_utc": "2026-02-19 06:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o63q8bi",
              "author": "huzbum",
              "text": "Awesome, I need to try this one on some coding tasks, because I can actually run it with a large context and decent TG speed.  (Haven't tested PP)",
              "score": 1,
              "created_utc": "2026-02-18 18:54:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66ntkk",
                  "author": "DifficultyFit1895",
                  "text": "It’s worked well for me. It’s also the only local model I’ve ever tried that 100% answers correctly this one tricky question I ask it about a ~30,000 word novella with the full story in context.",
                  "score": 1,
                  "created_utc": "2026-02-19 04:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64p8ry",
              "author": "jinnyjuice",
              "text": "For what languages are you using it for?\n\nThis is my secondary. GLM 4.7 Flash is better with coding the web UI.",
              "score": 1,
              "created_utc": "2026-02-18 21:36:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64pgm9",
                  "author": "No_Clock2390",
                  "text": "It supports all languages",
                  "score": 1,
                  "created_utc": "2026-02-18 21:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65dej5",
              "author": "National_Cod9546",
              "text": "What IDE / plugins are you using? ",
              "score": 1,
              "created_utc": "2026-02-18 23:35:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o655yof",
          "author": "Faultrycom",
          "text": "People will argue over different models but have in mind that it's about local - and here qwen 3 coder next shines as it can run on not so expensive stack.",
          "score": 5,
          "created_utc": "2026-02-18 22:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63rdtm",
          "author": "huzbum",
          "text": "I haven't used it (beyond confirming I can run it), but I hear great things about MiniMax M2.5.  I use GLM 4.7, and GLM 5 every day, but cloud hosted.  I thought about building something to run GLM 4.5/4.6/4.7, but never got beyond thinking about it... now GLM 5 is twice as big as GLM 4, definitely not running that, but maybe they'll make something intermediate in the 5 family.  \n\nI should try out GLM 4.7 flash for some real tasks, but I haven't gotten around to it yet.  ",
          "score": 3,
          "created_utc": "2026-02-18 18:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64lph1",
          "author": "Grouchy-Bed-7942",
          "text": "If you want something that runs for €6k for agentic code, MiniMax 2.5 with VLLM on 2x ASUS GX10 (equivalent to DGX Spark), you get 2000 tokens/sec of PP without context and you drop to 600/400 tokens/sec of PP at 100k context. Output of about 30 tokens/sec at the start and drops to 15 tokens/sec.\n\nWith Qwen3-Coder-Next, certain tool calls fail after 50k context.",
          "score": 4,
          "created_utc": "2026-02-18 21:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o679pq1",
          "author": "Only_Comfortable_224",
          "text": "Not directly related but for serious coding, is Claude more cost effective?",
          "score": 1,
          "created_utc": "2026-02-19 06:49:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mklci",
              "author": "Ambitious_Injury_783",
              "text": "yes.",
              "score": 1,
              "created_utc": "2026-02-21 16:56:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fll8r",
          "author": "emrbyrktr",
          "text": "I think investing in hardware at this stage is pointless. Everything is developing so fast. In my opinion, Qwen models are the most efficient models locally.",
          "score": 1,
          "created_utc": "2026-02-20 15:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64r4bi",
          "author": "Potential-Leg-639",
          "text": "Pick a strong model for architecture/planning and Qwen3 Coder 30B or GLM 4.7 Flash can then do the coding quite good.\n\nMakes quite a difference in coding quality.",
          "score": 1,
          "created_utc": "2026-02-18 21:45:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65bhr7",
              "author": "ijontichy",
              "text": "> Pick a strong model for architecture/planning\n\nCan you provide an example of this?",
              "score": 2,
              "created_utc": "2026-02-18 23:24:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o664ule",
                  "author": "voyager256",
                  "text": "I think he meant non-local / subscription based , big model like Claude 4.6",
                  "score": 3,
                  "created_utc": "2026-02-19 02:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67nzfl",
              "author": "alokin_09",
              "text": "Yep, this is basically my workflow in Kilo Code. I use a premium model like Opus for the architecture stuff, then switch to something like GLM, MiniMax or Qwen for the smaller tasks. Opus creates really detailed plans that are easy to hand off to the cheaper models.",
              "score": 2,
              "created_utc": "2026-02-19 09:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o681d94",
                  "author": "Potential-Leg-639",
                  "text": "I selfhost GLM 4.7 Flash/GPT-OSS-120b/Qwen 3 Coder on a Strix Halo, that do the coding and cant run out tokens. Planning/Architecture  in Opus.\n\nBut i switched completely to OpenCode.",
                  "score": 1,
                  "created_utc": "2026-02-19 11:09:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ppfn",
          "author": "PooMonger20",
          "text": "From my own testing and not benchmarks, on consumer level HW - GPT-OSS-20b is the closest I was able to get to the online equivalents. Everything else is either too slow, generates trash that doesn't even compile, endless syntax errors or straight out misses half the functions you asked for.",
          "score": 0,
          "created_utc": "2026-02-19 09:20:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pve1",
      "title": "Recommendations for agentic coding with 32GB VRAM",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "author": "pioni",
      "created_utc": "2026-02-20 08:27:15",
      "score": 19,
      "num_comments": 23,
      "upvote_ratio": 1.0,
      "text": "My current project is almost entirely in node.js and typescript, but every model I'm tried with LM Studio that fits into VRAM with 128k context seems to have problems with getting stuck in a loop. No amount of md files and mandatory instructions has been able to resolve this, it still does it with Roo Code and VSCode. \n\nAny ideas what I should try? Good examples of md files I could try to avoid this, or better LM Studio models with the hardware limitations I have? I have recently used Qwen3-Coder-Next-UD-TQ1\\_0 and zai-org/glm-4.7-flash and both have similar problems. Sometimes it works for good 15 minutes, sometimes it gets into a loop after first try. \n\nI don't know if it matters, but the dev environment is Debian 13. Using Windows was a complete nightmare because of commands it did not have and file edits that did not work.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6eb9on",
          "author": "GuyFromPoland",
          "text": "These questions keep showing up - is there any website where you provide your hardware details and it shows you best models?",
          "score": 4,
          "created_utc": "2026-02-20 10:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpn1j",
              "author": "reditzer",
              "text": "The issue is that \"best\" is subjective.",
              "score": 2,
              "created_utc": "2026-02-20 15:26:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e4c0j",
          "author": "FullstackSensei",
          "text": "How much RAM do you have? Running Q1 on any model severely lobotomizes performance. For coding tasks o smaller models, I find you can't get acceptable performance under Q8, and no KV cache quantization.",
          "score": 3,
          "created_utc": "2026-02-20 08:57:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7r7b",
              "author": "HumanDrone8721",
              "text": "I have 48GB (2x24) VRAM and 128GB RAM. Current using Qwen3-Coder-Next the highest quality 8bit quanta possible. Yes, the speed dropped significantly to something like 24tok/s but I had the latest OpenSSL source code and a regulatory document to follow and adjust the code for it. In German, bureaucratic German even. It worked perfectly (max context -c 0 is enabled), code itself, test cases AND documentation. Less than an hour, no human intervention.\n\nThe biggest problem was not the generated code quality or time, it was the latest llama.cpp crashing and burning with grammar parser errors, THAT was difficult to repair, but once repaired and with the latest GGUFs it works like a charm.\n\nMy point: tried with all other quants and it was a waste of data download, ALL of them failed more or less miserably !!! Also GLM4.7-Flash was a disappointment.\n\nTL; DR: Bottom line is: ALWAYS run a model to the highest possible resolution and context that you can run, discard the speed and take as much quality as possible, small quantas are fast, but really shit for anything besides benchmarks, simple tasks and wankerei: \"I've run this 485B model on my potato at 75t/s...\" Sadly this enforces and propagates the myth that smaller models are crap and not worth using because ADHD zoomers have no patience and take whatever shite as long as is fast.",
              "score": 8,
              "created_utc": "2026-02-20 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fue9m",
                  "author": "fish_of_pixels",
                  "text": "This is so interesting to me. What agentic tools are you running? I'm using opencode and glm 4.7 flash has been amazing while qwen3 coder next has struggled to get through most tasks. Both Q8.",
                  "score": 2,
                  "created_utc": "2026-02-20 15:48:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6edx4c",
                  "author": "Betatester87",
                  "text": "I have the same setup. Would you kindly post your llama.cpp config that you feel is best. Thanks",
                  "score": 1,
                  "created_utc": "2026-02-20 10:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hxyiu",
                  "author": "Soft_Syllabub_3772",
                  "text": "I had the grammar issue, i disabled streaming in roocode",
                  "score": 1,
                  "created_utc": "2026-02-20 21:43:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6e5ii7",
              "author": "pioni",
              "text": "I have 32GB of RAM, the same as VRAM. Can't upgrade for hardware reasons and monetary reasons. Any recommendations for a model that I could run a less quantized version of with my current setup? I think I might be able to squeeze the context window smaller if I have to, because it has to run in GPU, otherwise it will stall to few tokens per second.",
              "score": 2,
              "created_utc": "2026-02-20 09:08:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6e728c",
                  "author": "FullstackSensei",
                  "text": "Devstral 2 24B",
                  "score": 2,
                  "created_utc": "2026-02-20 09:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e2u64",
          "author": "IsSeMi",
          "text": "I tried them both. In my case it depends on the tool I use those models in. Try to use Claude code. I haven't noticed it stucks in a loop. Also Unsloth docs has usage guides for models, e.g. [GLM-4.7-Flash](https://unsloth.ai/docs/models/glm-4.7-flash#usage-guide). They provide different params for tool calling. ",
          "score": 1,
          "created_utc": "2026-02-20 08:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnp8d",
          "author": "former_farmer",
          "text": "I had better outcomes with opencode cli directly, ollama, and 30b models at full size. Slow as fuck but worked. Same pc as yours.",
          "score": 1,
          "created_utc": "2026-02-20 15:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iqz6a",
          "author": "Xantrk",
          "text": "I think LM studios llama.cpp is the reason for GLM performing so badly. I had endless looping issues, as soon as I switched to llama.cpp, they disappeared.",
          "score": 1,
          "created_utc": "2026-02-21 00:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f1pbk",
          "author": "HenkPoley",
          "text": "Qwen 3 Coder 30B, at maybe 4 bit, don’t run 1 bit quantization.\n\nBtw, the KV cache will mostly eat your RAM.",
          "score": 1,
          "created_utc": "2026-02-20 13:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e8d63",
          "author": "Antique_Dot_5513",
          "text": "Qwen code 30b sur opencode. T’attend pas à des miracles par contre.",
          "score": 0,
          "created_utc": "2026-02-20 09:35:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9boaw",
      "title": "Long conversation prompt got exposed",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vsgr1nirihkg1.jpeg",
      "author": "Ramenko1",
      "created_utc": "2026-02-19 21:16:10",
      "score": 18,
      "num_comments": 2,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9boaw/long_conversation_prompt_got_exposed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6g3a4x",
          "author": "Decent_Solution5000",
          "text": "Yeah, not surprised. Bet it gets reminders a lot from writers editing manuscripts. lol This one sounds better than ones I read about awhile ago. Wasn't using Claude then, Kind of glad I missed that.",
          "score": 1,
          "created_utc": "2026-02-20 16:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6njw18",
          "author": "iamsimonsta",
          "text": "That font plays havoc with kimi k2 ocr, interesting.",
          "score": 1,
          "created_utc": "2026-02-21 19:52:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8v8r5",
      "title": "I built a clipboard AI that connects to your local LLM, one ⌥C away (macOS)",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/6fld6w8pbfkg1",
      "author": "morning-cereals",
      "created_utc": "2026-02-19 09:57:27",
      "score": 17,
      "num_comments": 11,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8v8r5/i_built_a_clipboard_ai_that_connects_to_your/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o69ql5f",
          "author": "BisonMysterious8902",
          "text": "It's rare that I'll install something that someone posted here, but this is actually pretty useful.\n\nI'm pointing it to my local model (LM Studio running qwen3-30b-a3b-2507) and it responded so quickly that I had to check the logs to ensure it was actually using the local server...\n\nWell done- this could be useful-",
          "score": 1,
          "created_utc": "2026-02-19 17:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dzfno",
              "author": "morning-cereals",
              "text": "Thanks, really great to hear! I've only tested it with a few friends and family so far 😊  \nFeel free to share any feedback, ideas, suggestions for improvement!",
              "score": 1,
              "created_utc": "2026-02-20 08:10:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6algpk",
          "author": "CtrlAltDelve",
          "text": "I *adore* apps like these. \n\nI will absolutely try it and let you know. Thank you so much for open sourcing it!",
          "score": 1,
          "created_utc": "2026-02-19 19:28:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dzkrw",
              "author": "morning-cereals",
              "text": "Yes please, it's a fairly new project so feedback would be much appreciated 🙏",
              "score": 1,
              "created_utc": "2026-02-20 08:12:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bv83d",
          "author": "nemuro87",
          "text": "I like this very much and would like to use it on my intel mac. It now gives an error after I try to download the built in model \"Bad CPU type\"\n\nDo you think this can be supported in the future?",
          "score": 1,
          "created_utc": "2026-02-19 23:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dzzf4",
              "author": "morning-cereals",
              "text": "Thanks for the interest!  The built-in bundled llama-server is compiled for Apple Silicon (ARM64) only, and I don't have plans to support Intel Macs at this point.\n\nThat said, if you install Ollama or LM Studio, Cai should work fine with those as the model provider. Just skip the built-in model download and **point it at your existing setup in Settings :)**",
              "score": 1,
              "created_utc": "2026-02-20 08:16:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6e099y",
                  "author": "nemuro87",
                  "text": "Thanks for the info, I will try.\n\nAnd again congrats for the project, you did in a very short amount of time what Apple Intelligence failed to deliver, a simple and configurable solution that works offline and is truly private.",
                  "score": 1,
                  "created_utc": "2026-02-20 08:18:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eeiac",
          "author": "aretheworsst",
          "text": "Ah man this is awesome, used it all day working. If it’s not a huge ask API key support for the model provider would be awesome, I have my server running in lm studio with the new auth feature enabled. \n\nJust set up my new ai computer so will be testing it with your app this weekend, cheers!",
          "score": 1,
          "created_utc": "2026-02-20 10:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f3r7g",
          "author": "Financial-Source7453",
          "text": "Cherry Ai has a nice floating panel you can use to quickly pivot to Ai agent.",
          "score": 1,
          "created_utc": "2026-02-20 13:33:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67wv4a",
          "author": "Swimming_Ad_5205",
          "text": "А на гитхабе кода нет случайно?",
          "score": 0,
          "created_utc": "2026-02-19 10:29:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67x10f",
              "author": "Swimming_Ad_5205",
              "text": "Все нашёл \nhttps://github.com/soyasis/cai\n\nСпасибо) посмотрю",
              "score": 0,
              "created_utc": "2026-02-19 10:30:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra2atm",
      "title": "Is anyone else pining for Gemma 4?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "author": "Formula71",
      "created_utc": "2026-02-20 17:51:43",
      "score": 17,
      "num_comments": 11,
      "upvote_ratio": 0.9,
      "text": "About this time last year, I was impressed with Gemma 3, but besides the GPT-OSS models, it seems like the US based labs have been pretty quite on the open source front, and even GPT-OSS even feels like a while ago now.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6hjxt1",
          "author": "journalofassociation",
          "text": "I am very much looking forward to Gemma 4.  It's been almost a year since Gemma 3 was released.  \n\nNothing I can do but whine and speculate, though, except to play with all the other great OSS models we've gotten.",
          "score": 5,
          "created_utc": "2026-02-20 20:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hwtax",
          "author": "Klutzy_Ad_1157",
          "text": "Same, it understands German so well and I use it almost for a year now. I hope so much for Gemma 4!",
          "score": 2,
          "created_utc": "2026-02-20 21:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gvfwo",
          "author": "Savantskie1",
          "text": "GPT-OSS models were released in August 5, 2025. Literally months ago. Models no matter the size take time and hardware to train. It's not like they have a ton of extra compute they're not using just laying around.",
          "score": 5,
          "created_utc": "2026-02-20 18:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hg84m",
              "author": "UndecidedLee",
              "text": ">It's not like they have a ton of extra compute they're not using just laying around.\n\n\n\nThey do. That's why they're trying to shoehorn LLMs in everything.\n\nYou browse the internet often? - Here's an AI that will summarize the page for you.  \nYou have lots of media on your computer? - How about letting AI sort it for you?",
              "score": 3,
              "created_utc": "2026-02-20 20:16:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hj9zr",
                  "author": "Savantskie1",
                  "text": "Yeah that’s for the normies and it’s utilizing their current models wich they run at high concurrency. Why waste money on hardware not being utilized. Yeah you know nothing about business",
                  "score": -2,
                  "created_utc": "2026-02-20 20:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rarw7t",
      "title": "I managed to run Qwen 3.5 on four DGX Sparks",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ifu522zupqkg1.jpeg",
      "author": "Icy_Programmer7186",
      "created_utc": "2026-02-21 14:03:34",
      "score": 16,
      "num_comments": 7,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rarw7t/i_managed_to_run_qwen_35_on_four_dgx_sparks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6lq9mo",
          "author": "mosredna101",
          "text": "So 20K brings you 21 t/sec?",
          "score": 5,
          "created_utc": "2026-02-21 14:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lskx8",
              "author": "Icy_Programmer7186",
              "text": "This is a lab setup and this is one of many experiments done on it.  \nMoney well spent; this is not however recommendation for a production setup.",
              "score": 6,
              "created_utc": "2026-02-21 14:30:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lt5gl",
                  "author": "mosredna101",
                  "text": "I would be super happy with only one of those machines to be honest :D   \n",
                  "score": 2,
                  "created_utc": "2026-02-21 14:33:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ofl7r",
              "author": "fallingdowndizzyvr",
              "text": "You can get 4 Sparks for $12K.",
              "score": 1,
              "created_utc": "2026-02-21 22:42:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmh9x",
          "author": "I_like_fragrances",
          "text": "I am able to get unsloth's Q6\\_K\\_XL with max context at around 40 tok/s. What quant do you use on the sparks? Typically if I offload these bigger models to the CPU I can get around 20 tok/s but when they're fully on the GPU they run at around 40 tok/s.\n\nhttps://preview.redd.it/1ujmj21qmwkg1.png?width=836&format=png&auto=webp&s=ee566d16319e3ae1db5631689978d30c961f67c7",
          "score": 1,
          "created_utc": "2026-02-21 20:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p4g3f",
              "author": "Icy_Programmer7186",
              "text": "I use FP8 quantization -> Qwen/Qwen3.5-397B-A17B-FP8\n\nSpark has a unified memory, so I guess there is no off-loading.",
              "score": 1,
              "created_utc": "2026-02-22 01:13:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oh648",
          "author": "CalvinBuild",
          "text": "That's amazing! I can't wait to do it locally!",
          "score": 1,
          "created_utc": "2026-02-21 22:51:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}