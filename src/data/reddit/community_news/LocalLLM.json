{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 323,
    "file_size_bytes": 414893
  },
  "items": [
    {
      "id": "1rfi2aq",
      "title": "Self Hosted LLM Leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/pbwec2fpsvlg1.png",
      "author": "Weves11",
      "created_utc": "2026-02-26 18:25:32",
      "score": 623,
      "num_comments": 102,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rfi2aq/self_hosted_llm_leaderboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7k60fx",
          "author": "AC1colossus",
          "text": "Minimax?",
          "score": 43,
          "created_utc": "2026-02-26 18:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k8bzh",
              "author": "BarisSayit",
              "text": "It should be in S tier no cap.",
              "score": 22,
              "created_utc": "2026-02-26 18:45:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kavn7",
              "author": "Koalababies",
              "text": "My immediate thought. It's been a beast",
              "score": 3,
              "created_utc": "2026-02-26 18:57:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kfn1f",
              "author": "BitXorBit",
              "text": "So far the most enjoyable model running locally",
              "score": 3,
              "created_utc": "2026-02-26 19:19:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kbj3y",
              "author": "Weves11",
              "text": "added (to S tier), thanks for calling out!",
              "score": 3,
              "created_utc": "2026-02-26 19:00:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kcxc8",
          "author": "LightBrightLeftRight",
          "text": "I mean the new Qwen 3.5 models should easily be on this, the 27b dense and 122b moe both make a pretty good case for A-tier, B-tier at minimum. Particularly since they have vision, which is great for a lot of homelab/small business stuff.",
          "score": 32,
          "created_utc": "2026-02-26 19:06:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kfxal",
              "author": "Prudent-Ad4509",
              "text": "I have not tested 122b, but 27b is a beast. ",
              "score": 7,
              "created_utc": "2026-02-26 19:21:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7khgvz",
                  "author": "LightBrightLeftRight",
                  "text": "I've worked with both and surprisingly, not super different for me. I've seen better detail to world knowledge with 122b but not much in terms of reasoning or coding.\n\nI think I'll still stick with the 122b, but that's mostly just because I've got the headroom for it.",
                  "score": 5,
                  "created_utc": "2026-02-26 19:28:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7o6u5n",
                  "author": "FatheredPuma81",
                  "text": "27B is only like 25% faster than 122B for me so I don't bother using it but 122B is a really nice model but all 3 models hallucinate a lot.",
                  "score": 2,
                  "created_utc": "2026-02-27 09:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7kha3o",
          "author": "Gallardo994",
          "text": "No qwen3-coder-next in a coding leaderboard is a crime ",
          "score": 20,
          "created_utc": "2026-02-26 19:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kn8bb",
          "author": "ScuffedBalata",
          "text": "Why isn't Qwen3 on here?\n\nThe single best model I've ever used that works on \"normal people hardware\" is the Qwen3-Next and Qwen3-Coder-Next (both at 80B).",
          "score": 14,
          "created_utc": "2026-02-26 19:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l7nk2",
              "author": "robotcannon",
              "text": "Agree!!\n\nqwen3-vl is also fantastic (though it seems to run a tiny bit better at q8_0 for vision stuff)",
              "score": 3,
              "created_utc": "2026-02-26 21:33:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kilk4",
          "author": "kidousenshigundam",
          "text": "What hardware do I need to run S tier?",
          "score": 9,
          "created_utc": "2026-02-26 19:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kutnz",
              "author": "Witty_Mycologist_995",
              "text": "Probably a ton of Mac studios",
              "score": 4,
              "created_utc": "2026-02-26 20:32:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7liq7j",
              "author": "Altair12311",
              "text": "1 single mini-pc with the Ryzen AI Max+ 395 and 128GB of Ram for the MiniMax-2.5 (is my setup)",
              "score": 7,
              "created_utc": "2026-02-26 22:27:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lwugx",
                  "author": "dadavildy",
                  "text": "For coding, how is it on that machine?",
                  "score": 2,
                  "created_utc": "2026-02-26 23:42:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pxvbi",
                  "author": "LimiDrain",
                  "text": "What's even AI Max? People mention it too often. It's not about GPUs anymore?",
                  "score": 2,
                  "created_utc": "2026-02-27 16:01:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pt7dt",
                  "author": "StockboyRickyRicardo",
                  "text": "At 2bit quant?",
                  "score": 1,
                  "created_utc": "2026-02-27 15:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vsjqv",
                  "author": "cafemachiavelli",
                  "text": "Oo, tempting. How's the performance of the system? ",
                  "score": 1,
                  "created_utc": "2026-02-28 14:21:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7m623q",
                  "author": "kidousenshigundam",
                  "text": "No way. I have a Mac Studio ultra 256 GB and I can’t use Kimi K2",
                  "score": 1,
                  "created_utc": "2026-02-27 00:33:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7sa214",
              "author": "thatcoolredditor",
              "text": "Mac Studio 512gb for $10k",
              "score": 2,
              "created_utc": "2026-02-27 22:54:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7njnkz",
              "author": "ComfortablePlenty513",
              "text": "the $16k M5 mac studios coming out next week lol",
              "score": 0,
              "created_utc": "2026-02-27 05:40:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lkiy2",
          "author": "siegevjorn",
          "text": "Hey, want to elaborate on the methodology?",
          "score": 8,
          "created_utc": "2026-02-26 22:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ojdpl",
          "author": "LetterFair6479",
          "text": "Self hosted? Dont make me laugh. Only D is feasible , all other normal person who cant spend 5k+ cannot selfhost with any recent llm.",
          "score": 8,
          "created_utc": "2026-02-27 10:59:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pq1r2",
              "author": "richtopia",
              "text": "On the website there is a button for model size. I believe \"Small\" limits to 30B which is right at the limit of my gaming PC. \n\nAccording to this tier list, GPT-oss 20B is the highest in the \"B\" tier.",
              "score": 2,
              "created_utc": "2026-02-27 15:23:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7meqvh",
          "author": "Count_Rugens_Finger",
          "text": "aaaand the best model I can actually run on my PC is C tier.  yay\n\nEdit: oh wait gpt-oss 20b is in B tier.  That's... interesting.\n\nAnd Qwen3-30B-A3B is in D tier?  huh?",
          "score": 5,
          "created_utc": "2026-02-27 01:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k5s55",
          "author": "Egoz3ntrum",
          "text": "Devstral-2-123B is missing there in the Coding section.",
          "score": 3,
          "created_utc": "2026-02-26 18:34:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mma7l",
          "author": "MahDowSeal",
          "text": "Sorry if the question might be stupid, but for anyone who tried the S tier models. How comparable are they to the cloud models such as claude or chatGPT?",
          "score": 4,
          "created_utc": "2026-02-27 02:06:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mzj2r",
              "author": "RG_Fusion",
              "text": "I'm probably not the best person to ask as I've only been playing around with Qwen3.5-397b-17b for a little bit, but I was absolutely blown away by its internal reasoning. I don't have enough to make a definitive assessment, but I can certainly see how it could be competitive against the frontier models.",
              "score": 2,
              "created_utc": "2026-02-27 03:24:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7nszr2",
                  "author": "sinebubble",
                  "text": "You’re running it locally? Which quant?",
                  "score": 1,
                  "created_utc": "2026-02-27 06:56:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7pyb5u",
              "author": "LimiDrain",
              "text": "DeepSeek R1 is the same as the online model. R1 is still one of the best in 2026. Maybe even better than ever because it doesn't change, while ChatGPT downgrades to save money.",
              "score": 2,
              "created_utc": "2026-02-27 16:03:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7nsw8q",
              "author": "sinebubble",
              "text": "I might try Minimax 2.5 tomorrow, the others are too large for me, even with 336G of vram. How can you reasonably expect GLM5 or Kimi 2.5 to maintain S tier at a q1 or q2? Qwen3-coder-next is amazing, tho not quite Claude, and that ranks as a B.",
              "score": 1,
              "created_utc": "2026-02-27 06:55:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7k7mad",
          "author": "Tuned3f",
          "text": "Kimi slaps",
          "score": 3,
          "created_utc": "2026-02-26 18:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kh2qp",
          "author": "BitXorBit",
          "text": "Minimax m2.5 definitely above qwen3.5",
          "score": 3,
          "created_utc": "2026-02-26 19:26:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lijlj",
          "author": "Foreign_Coat_7817",
          "text": "I tried out gpt 20b on my 4090 and it hallucinated like crazy. But maybe Im just not using it right. What are the usecases that make it B tier?",
          "score": 3,
          "created_utc": "2026-02-26 22:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rjfl6",
              "author": "NinjaSilver2811",
              "text": "I kept getting a whole bunch of feedback loops for me. Maybe the quaint process screws it up. ",
              "score": 1,
              "created_utc": "2026-02-27 20:37:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7n0z3p",
          "author": "rm-rf-rm",
          "text": "What is this based on?",
          "score": 3,
          "created_utc": "2026-02-27 03:33:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n93iq",
              "author": "sinebubble",
              "text": "Vibes",
              "score": 9,
              "created_utc": "2026-02-27 04:25:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7o9w7u",
          "author": "mcai8rw2",
          "text": "How the hell are you self hosting these massive models? Even with 24gb vram surely they are going to be horribly slow?",
          "score": 3,
          "created_utc": "2026-02-27 09:31:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kbg94",
          "author": "ghgi_",
          "text": "As someone whos had the experience of running minimax M2.5 nvfp4 on hardware, Should be a S (just behind glm-5, lil dumber but faster) or a really strong A ",
          "score": 2,
          "created_utc": "2026-02-26 18:59:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kbv9v",
              "author": "Weves11",
              "text": "haha 100% agree, forgot to add it initially but its been added now!",
              "score": 1,
              "created_utc": "2026-02-26 19:01:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lbj3y",
          "author": "serioustavern",
          "text": "Would be great to get GLM-4.7-Flash and Qwen-3.5-27b in there for the “small” category.",
          "score": 2,
          "created_utc": "2026-02-26 21:52:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o6jl5",
              "author": "FatheredPuma81",
              "text": "Benchmarks wise GLM 4.7 Flash is technically a pretty mediocre model that's padded heavily by being over trained on 1 task. But usage wise it's actually surprisingly nice to use if if you can get it to not loop 24/7.",
              "score": 1,
              "created_utc": "2026-02-27 08:59:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7m7dsi",
          "author": "PibePlayer1",
          "text": "Math should have more versions, what about InternVL3.5 Qwen2.5-Math Kimi-VL-A3B 2506?",
          "score": 2,
          "created_utc": "2026-02-27 00:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mpwr7",
          "author": "DeliciousBag1029",
          "text": "Llama 4 maverick B? Meta bot detected!",
          "score": 2,
          "created_utc": "2026-02-27 02:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nhdyy",
          "author": "GreenGreasyGreasels",
          "text": "Coding, Math, Reasoning, Efficiency - weird set (two are usecases, one is a feature not use, and I last is performance I guess).\n\nTwo of the most common and useful usecases for local models - Chat (talk about things) and writing/rewriting text  are missing.\n\nNo wonder Mistral 3.2 Small, Gemma3-27B and Llama3.3-70B are criminally underrated or unrepresented in this ranking.",
          "score": 2,
          "created_utc": "2026-02-27 05:23:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nl0nr",
          "author": "morbidgun",
          "text": "Gemma 3:27b slaps, it has native ocr/image scanning. Does everything I need it to do. Very well rounded.",
          "score": 2,
          "created_utc": "2026-02-27 05:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o6ljm",
          "author": "FatheredPuma81",
          "text": "GPT-OSS 20B and 120B should probably be in F tier. LLM's that refuse normal actions for safety reasons and argue with you over proven facts are the worst.",
          "score": 2,
          "created_utc": "2026-02-27 08:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7odjm8",
          "author": "Square-Put-7853",
          "text": "Which one can I host on Mac mini m2 with 8 gb ram?",
          "score": 2,
          "created_utc": "2026-02-27 10:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pxqdp",
          "author": "LimiDrain",
          "text": "I am pretty fine with a D-tier Gemma lmaooo",
          "score": 2,
          "created_utc": "2026-02-27 16:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ru1x0",
          "author": "lakimens",
          "text": "Self hosted 1T parameter :/ Technically it is self-hostable I guess",
          "score": 2,
          "created_utc": "2026-02-27 21:31:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z2v0m",
          "author": "Mr-I17",
          "text": "1T parameter local LLM. Is this a joke?",
          "score": 2,
          "created_utc": "2026-03-01 00:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86beuy",
          "author": "bennyb0y",
          "text": "Post the minimum hardware needed for this tier list.",
          "score": 2,
          "created_utc": "2026-03-02 04:17:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lbkpb",
          "author": "psxndc",
          "text": "Sorry to be dense, but is Kimi “self-hosted”? The interface you interact with might be, but I thought the model itself was cloud-based.",
          "score": 2,
          "created_utc": "2026-02-26 21:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n0u14",
              "author": "RG_Fusion",
              "text": "The 1 trillion parameter model Kimi K2 is open weight, meaning you can download it and run it on your own hardware. Pretty much nobody has a Terabyte of RAM or a processor that can keep up, but you can find quantized versions of the model available to download on huggingface.\n\n\nThe 4-bit quantization cuts the total file size down to around 550 GB while still maintaining over 95% of the original accuracy. This means you can buy used last-gen server components and pair them with a good GPU to run it, albeit at rather low speeds.",
              "score": 6,
              "created_utc": "2026-02-27 03:32:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ke360",
          "author": "DrewGrgich",
          "text": "I mean, Kimi and Mini should slap - they apparently cribbed from Anthropic & OpenAI … who in turn consumed the bulk of human knowledge via the Web and other means. :)",
          "score": 1,
          "created_utc": "2026-02-26 19:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kvdsq",
          "author": "Traditional-Card6096",
          "text": "And you'll be able to access them all remotely on your phone :)",
          "score": 1,
          "created_utc": "2026-02-26 20:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m0v1m",
          "author": "stratofax",
          "text": "Looks like gpt-oss 20B is the only model that made the B tier. Everything else at that level or higher is at least 100B or more.",
          "score": 1,
          "created_utc": "2026-02-27 00:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mv7h1",
          "author": "AutumnStar",
          "text": "Really wish there were better models in the <32B range for overall general use. Nothing better has come since gpt-oss-20b",
          "score": 1,
          "created_utc": "2026-02-27 02:58:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nf8kf",
          "author": "OutNebula",
          "text": "Step-3.5 Flash is just insane, been using it instead of Gemini 3.1 Pro, highly recommended.",
          "score": 1,
          "created_utc": "2026-02-27 05:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nzjay",
          "author": "big_witty_titty",
          "text": "Add IBM’s granite model too",
          "score": 1,
          "created_utc": "2026-02-27 07:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oilie",
          "author": "Historical_Papaya_22",
          "text": "im selfhosting clawbot w/ quen38b its very dumb.. how can we self host MiniMax-M2.5 or something on that level",
          "score": 1,
          "created_utc": "2026-02-27 10:52:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oplza",
          "author": "madaradess007",
          "text": "qwen3:4b is better than DS-r1-distill-qwen-7b, why no mention?",
          "score": 1,
          "created_utc": "2026-02-27 11:50:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pcpcu",
          "author": "kartikey7734",
          "text": "This is an incredible resource! The fact that you're tracking self-hosted models with consistent benchmarks is gold for anyone trying to pick the right model for their hardware constraints.\n\n\n\nQuick observations:\n\n\n\n1. \\*\\*The S-Tier gap is huge\\*\\* - Kimi K2.5 and GLM-4 are genuinely in a different tier. But their inference costs (if you're not self-hosting the weights) are brutal. The sweet spot for most people seems to be A/B tier.\n\n\n\n2. \\*\\*Missing dimension: inference speed\\*\\* - Would be amazing to see latency/tokens-per-second metrics alongside quality. DeepSeek R1 is phenomenal but can be slower than some smaller models on weaker GPUs.\n\n\n\n3. \\*\\*Hardware tiers would help\\*\\* - e.g., \"Best model for 8GB VRAM\", \"Best for RTX 3060\", etc. Because honestly, a 70B model doesn't matter if you can't load it.\n\n\n\n4. \\*\\*License tracking\\*\\* - Critical detail: which ones are truly free for commercial use? Some S-tier models have restrictions.\n\n\n\nBut seriously, this is the resource the community needed. Every time someone asks \"which model should I use\", we can just point here instead of 20 different opinions. The standardized benchmarking is \\*chef's kiss\\*.\n\n\n\nAre you planning to update this regularly, or is it a one-time snapshot? If it's ongoing, this could become the definitive LLM comparison resource.",
          "score": 1,
          "created_utc": "2026-02-27 14:15:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qd06c",
              "author": "Weves11",
              "text": "The plan is to definitely keep updating this! If there's enough interest, could even open source the underlying data so that individuals can contribute new benchmark scores or new models",
              "score": 1,
              "created_utc": "2026-02-27 17:12:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pyb22",
          "author": "AccomplishedAd2837",
          "text": "Kimi in my experience is not a tier at all... consistent lying and talking in circles",
          "score": 1,
          "created_utc": "2026-02-27 16:03:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qnvga",
          "author": "ForsookComparison",
          "text": "I see next to no correlation to my real-world use results",
          "score": 1,
          "created_utc": "2026-02-27 18:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zxnnr",
          "author": "Ship9491",
          "text": "C'est avec ollama ?",
          "score": 1,
          "created_utc": "2026-03-01 04:04:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86vfsk",
          "author": "Warm-Environment-521",
          "text": "definitely got some good models out there ",
          "score": 1,
          "created_utc": "2026-03-02 06:57:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kgyxl",
          "author": "Alert_Employee_7584",
          "text": "Hey, i have a 1660 Super with 32 GB Ram. Should i choose Kimi K2.5 or rather GLM-5, because i think Kimi might run a bit to slow for what i need, as i need my answers in around 2-3 seconds if possible. ",
          "score": 0,
          "created_utc": "2026-02-26 19:26:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kj9ci",
              "author": "wh33t",
              "text": "Dude, those models are massive. You can't run those with that hardware, 2-3 seconds if possible? No way. Go check out the quants on huggingface for those models, look at the model sizes. In total you have under 40GB of total memory to work with. You have to share that with your OS, and with the model context. You're gonna be looking at models in the 27b and under range likely.",
              "score": 4,
              "created_utc": "2026-02-26 19:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kme4z",
                  "author": "Alert_Employee_7584",
                  "text": "Yea, i am even struggling running a 12b Model. I was just making fun of the idea of calling a 1T model the best model to self host, as it would require you beeing the son of some billionaire or sth",
                  "score": 5,
                  "created_utc": "2026-02-26 19:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7knhoi",
              "author": "ScuffedBalata",
              "text": "wut?\n\nThose are like 500GB or larger models, you can't even kinda/sorta run them in 32GB.   A $13k mac studio or a $35k server with 8 or 10 GPUs can, but your little 1660 cant.\n\nLook at the 32B or 80B models with quantization.",
              "score": 2,
              "created_utc": "2026-02-26 19:57:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kunbj",
          "author": "Witty_Mycologist_995",
          "text": "Glm 4.7 flash gotta be A tier bro",
          "score": 0,
          "created_utc": "2026-02-26 20:31:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kq597",
          "author": "gacimba",
          "text": "Wtf is S? Sucks, super, snazzy?",
          "score": -1,
          "created_utc": "2026-02-26 20:09:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kvxni",
              "author": "AllenZox",
              "text": "It would be great if someone understand the S and explain it to us millennials",
              "score": 1,
              "created_utc": "2026-02-26 20:38:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lba3v",
                  "author": "psxndc",
                  "text": ">'S' tier may stand for \"special\", \"super\", or the Japanese word for \"exemplary\" (秀, shū), and originates from the widespread use in Japanese culture of an 'S' grade for advertising and academic grading.\n\nhttps://en.wikipedia.org/wiki/Tier_list\n\nIt’s used extensively in the fighting game community.",
                  "score": 7,
                  "created_utc": "2026-02-26 21:50:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7nxiak",
                  "author": "hugthemachines",
                  "text": "At some point, someone stopped understanding grades like 1-5 a-f etc and figured it would be logical to add an S at the top.\nSo instead of the grading being A B C D etc it is now S A B C D etc\n\nThe real point is that when you call it s for super or special, you kind of feel like they are much, much better than the normal scale.\n\nEmotional stuff leaked into a more objective area of stats.",
                  "score": 3,
                  "created_utc": "2026-02-27 07:35:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ljkbt",
              "author": "RnRau",
              "text": "Spiffy.",
              "score": 1,
              "created_utc": "2026-02-26 22:31:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rh9vhu",
      "title": "I built a language model where tokens are complex numbers and \"meaning\" emerges from wave interference -- no attention, O(n), 178M params, open-sourcing today",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rh9vhu/i_built_a_language_model_where_tokens_are_complex/",
      "author": "ExtremeKangaroo5437",
      "created_utc": "2026-02-28 18:22:22",
      "score": 200,
      "num_comments": 94,
      "upvote_ratio": 0.88,
      "text": "I've been working on a fundamentally different LLM architecture. No attention layers. No FFN blocks. Instead, every token lives in complex phase space, and language processing happens through wave-like interference between specialized \"phase banks.\"\n\nOpen-sourced here: [https://github.com/gowrav-vishwakarma/qllm2](https://github.com/gowrav-vishwakarma/qllm2)\n\n# The core idea: language as wave interference\n\nIn a transformer, a token is a real-valued vector that gets refined through attention + FFN layers. In this model, a token is a **complex number** \\-- it has a magnitude (how \"important/activated\" it is) and a phase angle (what \"kind of meaning\" it carries). These two properties are naturally separated and jointly processed.\n\nThis isn't just a gimmick. It changes how every operation works:\n\n* **Embeddings**: Each token gets a `[real, imag]` vector. The model learns that semantically similar tokens align in phase, while different meanings sit at different angles.\n* **Transformations are rotations**: When context modifies a token's meaning (like \"bank\" shifting meaning based on surrounding words), that's a phase rotation -- a complex multiply. Rotations compose naturally, are always invertible (no information loss), and reduce to GEMM.\n* **Similarity is coherence**: Instead of dot product, we use phase coherence: `Re(a * conj(b)) / (|a| * |b|)`. This measures both directional alignment AND magnitude relationship.\n* **Multiple banks interfere**: A \"semantic bank\" and \"context bank\" process each token independently, then combine via learned interference (constructive where they agree, destructive where they conflict). A tiny router decides per-token how much weight each bank gets. Think MoE but at the representation level.\n\n# What the phase system actually gives us\n\n**1. Natural magnitude/phase decomposition = implicit attention** High-magnitude phase states dominate downstream processing automatically. The model doesn't need explicit attention to decide \"which tokens matter\" -- magnitude handles salience, phase handles identity. The SemanticPhaseBank uses 512 learnable concept vectors and retrieves them via phase coherence -- this is essentially a learned associative lookup that runs in O(seq concepts), not O(seq^(2).)\n\n**2. Context as phase modulation** The ContextPhaseBank computes a causal windowed average (window=8) of nearby tokens and then **complex-multiplies** it with the current token. This is elegant: the local context literally rotates the token's meaning in phase space. A word appearing after \"not\" gets rotated differently than after \"very.\" No attention needed.\n\n**3. Rotation-based state evolution** The backbone SSM evolves state via: `h[t+1] = damping * R(theta) @ h[t] + gate * B @ x[t]` where R(theta) is a Cayley-transform rotation. The state naturally oscillates, and the damping factor (learned, per-dimension, range \\[0.5, 1.0\\]) controls how fast old information decays. This is why SSMs struggle with long-range recall -- but the model compensates with a separate Phase-Coded Memory (1024 learned slots, chunked top-k retrieval) and an Episodic Memory (sliding window via FlashAttention SDPA).\n\n**4. Zero trig in the hot path** Every rotation uses the Cayley transform: `cos_like = (1-a^2)/(1+a^2)`, `sin_like = 2a/(1+a^2)`. This is just arithmetic -- no `sin()`, no `cos()`, no `exp()`. Every operation is a matmul or elementwise op. Perfect for Tensor Cores.\n\n# Results (178M params, TinyStories, 10k samples, A6000)\n\n|Metric|Epoch 1|Epoch 2|Epoch 3 (partial)|\n|:-|:-|:-|:-|\n|Train PPL|200.86|32.75|\\~26 (and dropping)|\n|Val PPL|76.47|48.92|\\--|\n|Train CE|5.30|3.49|\\~3.26|\n\nTraining used only **10k samples** (0.5% of TinyStories). Starting PPL was 55,000 (random). It dropped to val PPL 49 in 2 epochs (40 min on A6000, no compile). Overfiting simply needs data now ...\n\n**Epoch 1 generation:**\n\n>\"The quick brown house. They run and start to get a smile. Mom were very excited. Now mommy and big yellow room. There said and She are friends. Tim, she started to save the garden.\"\n\n**For context:** A 22M-param GPT-2 trained on the full 2.1M TinyStories dataset for 20k steps reaches val PPL \\~11. We're at 49 with 0.5% of the data and 2 epochs. The learning curve is steep and still dropping -- we just need more data/epochs to converge.\n\n# Why this approach might be better\n\n* **O(n) complexity**: Linear-time backbone. Theoretical 256K context. No quadratic attention.\n* **GEMM-only math**: No trig, no softmax in the backbone. Everything is matmul/elementwise.\n* **Interpretable**: You can inspect which bank each token routes through, what concepts are retrieved from memory, how coherent the phase states are. The model ships with \"philosophy metrics\" (Manas/Buddhi/Viveka/Smriti from Indian philosophy) that track mind activity, discernment, stability, and memory quality.\n* **Modular**: Banks, backbone, coupler, memory, and objectives are all registered components. Add a new bank type with a decorator. Swap the backbone. Change the coupling strategy. All via config.\n* **Consumer-GPU friendly**: Medium model trains on RTX 4090 / A6000 with batch 48-64.\n\n# Honest limitations\n\n* **Training throughput is \\~2x slower than an equivalent transformer.** The SSM backbone loop is sequential per-step. A custom Triton kernel would help but doesn't exist yet.\n* **In-context learning will be weaker.** Fixed-state SSMs compress context into a fixed vector. The episodic memory (O(n buffer\\_size) sliding window) helps with copying but isn't a full replacement for O(n^(2)) attention.\n* **Not validated at scale.** 178M params on 10k samples is a PoC. Need full dataset + larger models + benchmarks.\n* **Bank ablations not done.** We use semantic + context banks but haven't proven both are needed. Could be that one bank suffices.\n* **Pure PyTorch.** No fused CUDA/Triton kernels. Backbone loop is Python. Lots of low-hanging performance fruit.\n\n# What's next\n\n* Full TinyStories training (2.1M samples) for proper PPL comparison\n* Bank ablations (semantic-only vs semantic+context vs 4-bank)\n* Triton kernel for the oscillatory SSM recurrence\n* Scale to 1B+ params\n* Long-context evaluation (4K / 16K / 64K tokens)\n\n# Tech stack\n\nPyTorch | torch.compile compatible | GPT-2 BPE tokenizer | uv package management | Clean modular codebase\n\n**Looking for feedback, collaborators, and people who want to try architectures beyond transformers.**\n\n**EDIT (March 1, 2026 3:40 AM IST)**: Scaled up to 100k samples (5% of TinyStories, 10x the original post) and the results are significantly better.\n\nSetup: Same 178M model, batch=64, A6000, no compile. 1612 batches/epoch (\\~**3.5 hours per epoch**).\n\n**Epoch 1 results** on 100k samples:\n\n|Metric|10k samples (original post)|100k samples (this update)|\n|:-|:-|:-|\n|Train PPL|200.86|24.00|\n|Val PPL|76.47|18.95|\n\nFor context: a 22M-param GPT-2 trained on the full 2.1M dataset for 20k steps gets val PPL \\~10.9 (I Need to verify this as just remembered I read it somewhere). **We're at 18.95 with a completely different architecture using only 5% of the data, after 1 epoch.** Epoch 2 opened at step-1 PPL of 12.77 and is still dropping.\n\nGeneration sample (epoch 1, 100k samples):\n\n\\> \"The quick brown were full. Steve and Brown loved each other. At the end of the hill, the friends were very happy. They had lots of fun and shared stories. Mam and Brown were the best day ever. All of their weeks were very good friends and would often enjoy their joy! The end had had a good time with them.\"\n\nCompare this to the 10k-sample generation from the original post. This has proper story structure, multiple characters interacting, emotional arc, and an ending. Grammar is mostly correct. Still has quirks (\"The quick brown were full\" -- model doesn't know \"brown\" should be a noun here), but the improvement from 10x more data is dramatic.\n\nThe learning curve shows no signs of plateauing. Training continues -- will update again when epoch 2+ finishes.\n\n**EDIT 2 (March 1, 2026 8:00AM IST)** : Epoch 2 finished. Epoch 3 is underway.\n\n|Metric|Epoch 1|Epoch 2|Epoch 3 (in progress)|\n|:-|:-|:-|:-|\n|Train PPL|24.00|11.96|\\~10.5 (and flat)|\n|Val PPL|18.95|14.07|\\--|\n\nVal PPL 14.07. For reference, the 22M-param GPT-2 baseline trained on the full 2.1M dataset reaches \\~10.9. We're at 14 using a completely non-transformer architecture, 5% of the data, 2 epochs. **Epoch 3 opened at PPL \\~10.5, which means we'll likely match or beat that baseline this epoch. Just in \\~6 Hrs on   Almost one consumer grade GPU.**\n\nEpoch 2 generation:\n\n\\> \"The quick brown boy had ever seen. But one day, the sun was setting. The next night, the room got dark. Tom and the girl continued to admire the rain. The end was so happy to be back and continued to sail in the park. And every night, the end of the day, the family and the people stayed happy. They all lived happily ever after.\"\n\nNotice: proper narrative flow, temporal transitions (\"one day\", \"the next night\", \"every night\"), emotional resolution (\"lived happily ever after\"), and multi-sentence coherence. This is from an architecture with zero attention layers.\n\nTrain-val gap (11.96 vs 14.07) suggests some overfitting on 100k samples. Next step: scale to the full 2.1M dataset. Training continues.\n\n  \nStopping and tweeking code.. I think it can be much faster ... will update in other post next ",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rh9vhu/i_built_a_language_model_where_tokens_are_complex/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7xw4gr",
          "author": "RTDForges",
          "text": "Hopping on Reddit today was worth it to see this post. I’m very intrigued to see where this goes. And the honest assessment you have compared to how saturated the AI space is with hype makes me even more intrigued by the claims.\n\nAlso I love the disclaimer on GitHub about using AI to build AI.",
          "score": 28,
          "created_utc": "2026-02-28 20:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zkofg",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 10,
              "created_utc": "2026-03-01 02:40:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o8088vy",
                  "author": "marcusnelson",
                  "text": "What’s the hardware setup you’re using now?",
                  "score": 1,
                  "created_utc": "2026-03-01 05:21:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xaxpd",
          "author": "BidWestern1056",
          "text": "would be happy to collaborate :\nhttps://arxiv.org/abs/2506.10077\nworking on a follow up atm that has more exploration by model size and parameters, but have been intending to explore this direction \n\n\n\n\n\n",
          "score": 11,
          "created_utc": "2026-02-28 18:59:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xb9g0",
              "author": "BidWestern1056",
              "text": "the followup conference for QNLP+AI is coming too, your work would be great for submission there.\nhttps://qnlp.ai/\n\n\n\n",
              "score": 4,
              "created_utc": "2026-02-28 19:00:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7xcth9",
                  "author": "ExtremeKangaroo5437",
                  "text": "Thanks for your comment... I am not a PHD holder, and not like other big names.. I even do not know how to submit my paper there ( I did try ... but was not able to suceed) so help is more then welcome... \n\nThe original paper I made is here: [https://github.com/gowrav-vishwakarma/qllm2/blob/master/QLLM\\_CORE\\_IDEA.pdf](https://github.com/gowrav-vishwakarma/qllm2/blob/master/QLLM_CORE_IDEA.pdf) \n\nAnd I know things but not theoritically... \n\nMy first AI product was launched in 2014 [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) \n\nBut I am desperate now to be in right place with right team ... ",
                  "score": 3,
                  "created_utc": "2026-02-28 19:08:50",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xcxzy",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 17,
          "created_utc": "2026-02-28 19:09:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xdb6z",
              "author": "ExtremeKangaroo5437",
              "text": "Deeply thankful for kind worlds..\n\nI am in AI since 2012..\n\nMy first AI product was launched in 2014 [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/)\n\nBut Now .... I am desperate  to be in right place with right team ... so every comment matters..",
              "score": 8,
              "created_utc": "2026-02-28 19:11:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7z1esf",
          "author": "gustinnian",
          "text": "Fascinating, I've been trying to familiarize myself with complex numbers for DSP purposes so this is another very interesting application. Analog computing is another neglected frontier that might hold unmined potential for AI.",
          "score": 8,
          "created_utc": "2026-03-01 00:42:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zkwsc",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 3,
              "created_utc": "2026-03-01 02:41:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ztgfx",
                  "author": "gustinnian",
                  "text": "I admire your philosophy. Too many vulture capitalists are trying to pull the ladder up behind them in the hope of hogging all the benefits at the expense of the rest of us (or worse). \n\nI'm an all rounder but old enough to remember dabbling in primitive Fuzzy Logic and the evolutionary dead end of Expert Systems before the opaque black boxes of Neural Nets took over.",
                  "score": 3,
                  "created_utc": "2026-03-01 03:35:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xbgw1",
          "author": "blamestross",
          "text": "This is the right tone and approach for this kind of work. A lot of people are chasing the \"See I made the next AGI\" and post crazy things. Your tone and approach lends you instant ethos and I wanted to complement it.",
          "score": 30,
          "created_utc": "2026-02-28 19:01:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xbuje",
              "author": "ExtremeKangaroo5437",
              "text": "Appriciate :) ",
              "score": 6,
              "created_utc": "2026-02-28 19:03:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o80e7xq",
          "author": "edbuildingstuff",
          "text": "The Cayley transform trick for avoiding trig in the hot path is really clever. I've seen a lot of \"alternative architecture\" posts that conveniently ignore the computational cost of their novel operations, so it's refreshing to see someone explicitly design around what Tensor Cores actually like to do.",
          "score": 3,
          "created_utc": "2026-03-01 06:09:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80hdyl",
              "author": "ExtremeKangaroo5437",
              "text": "yes.. infact I only had idea what I need and how to get this done.. this was my experience while working on large MySQL databases with complex calcualtions.. when I started developing and I discuss things with opus/gpt.. I came to know that thi has a name ... 😁",
              "score": 3,
              "created_utc": "2026-03-01 06:36:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80rz3a",
                  "author": "edbuildingstuff",
                  "text": "keep it going mate! opus defo helped me push my boundary of knowledge way further",
                  "score": 2,
                  "created_utc": "2026-03-01 08:13:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xlgdg",
          "author": "j00cifer",
          "text": "“Key Features: Quantum superposition, entanglement, phase coherence”\n\nOk.  Whatever you say.  After this you should probably let the world of physics know so they can stop all that quantum computer nonsense. Nobody knew it was LLMs all the way down",
          "score": 15,
          "created_utc": "2026-02-28 19:53:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7y1uxb",
              "author": "WittySupermarket9791",
              "text": "💩 = scam",
              "score": 4,
              "created_utc": "2026-02-28 21:21:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7zkroi",
                  "author": "ExtremeKangaroo5437",
                  "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.\n\n",
                  "score": 0,
                  "created_utc": "2026-03-01 02:40:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7zkr33",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 1,
              "created_utc": "2026-03-01 02:40:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o81yihu",
              "author": "ExtremeKangaroo5437",
              "text": "You're right -- there's nothing quantum mechanical here and the name oversells it. It's classical complex-valued linear algebra: phasor arithmetic, Cayley-transform rotations, normalized dot products for coherence. Standard wave/signal processing math, implemented as GEMM ops on a GPU.\n\nThe architecture is novel in how it applies these ideas to language modeling (multi-bank phase interference, oscillatory SSM backbone, phase-coded memory), but calling it \"quantum\" was a poor naming choice. Lesson learned.\n\nBut.. when I started this was the idea and in some different repos .. I am also working on that.. but you are right if that part is not here.. I should rename it... and I'll do rename it...\n\nPositive critics are welcome ",
              "score": 1,
              "created_utc": "2026-03-01 14:09:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xs7lb",
          "author": "wizard_of_menlo_park",
          "text": "This is really good start! Quite intresting too !",
          "score": 3,
          "created_utc": "2026-02-28 20:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zkzxu",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 1,
              "created_utc": "2026-03-01 02:42:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7x9c42",
          "author": "Ok_Pes_11590",
          "text": "Hey do you have a white paper or an Arxiv paper for this? Couldn't follow much but then why not Quaternions?\nAlso, share some resources.\nThank you",
          "score": 2,
          "created_utc": "2026-02-28 18:51:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zm61i",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 2,
              "created_utc": "2026-03-01 02:49:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7x9q0v",
              "author": "ExtremeKangaroo5437",
              "text": "I submitted.. but did not have any support so not accepted.. I did not know the process also... but I try to submit... \n\nwell the initial document is in the repo itself.. now I am on much advancement in here... thats a new repo I am working... a few basics has changed and I found that will not work as in initial versions... \n\nbut yes.. original idea with information is in repo itself... ",
              "score": 2,
              "created_utc": "2026-02-28 18:53:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7xa6x5",
                  "author": "Ok_Pes_11590",
                  "text": "Okay thank you",
                  "score": 1,
                  "created_utc": "2026-02-28 18:55:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7xgjes",
              "author": "ExtremeKangaroo5437",
              "text": "[https://github.com/gowrav-vishwakarma/qllm2/blob/master/QLLM\\_CORE\\_IDEA.pdf](https://github.com/gowrav-vishwakarma/qllm2/blob/master/QLLM_CORE_IDEA.pdf) \n\ninitial version but still its a good start.. ",
              "score": 1,
              "created_utc": "2026-02-28 19:28:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xu3ac",
          "author": "Tommonen",
          "text": "Very interesting idea and would fit well (if developed a bit further but this is right basis) for a psychological model i have been working on, which works with both agentic llm systems and humans.\n\nThis idea of yours matches my idea of qualia (some others have also said the same so idea is not my originally), that qualia is not to experience the same thing, but its a relational vector where distance to other data points defines the thing. For example we dont see blue and red the same, but the distance in vector space vetween them is the same in relation to other colors and this relational similarity is the basis for the subjective experience of red and what makes up qualia of red. Same idea applies to everything.\n\nI argue that llms already fo have a type of qualia or subjective experience, and some of them do already have some level of intentionality that can overcome the path of least resistance (whoch basic llms work on solely). This idea of yours would make the llm subjective experience closer to that of humans.\n\nI also say that the vector (in humans as well) has its magnitude and position in vector space. And that without will, things just take the path of least resistance and are acted upon if not stopped and redirected to path with more resistance (which is why will takes so much mebtal energy, especially if vector magnitude fought against is extremely strong, like in addictions etc).\n\nHowever you speak of some 2d quantum dimension thing im not sure what is meant here, and i was thinking vectors with potentially thousands or much more dimensions.\n\nEither way ”consciousness” = qualia + intentional/willful thoughts/actions that go against the path of least resistance. But consciousness referring to 2 separate things is why people get stuck in hard problems of it etc, but its a categorization error and really you just need qualia + intention/will.\n\nYou can private message me if you are interested in this sort of idea for your project.",
          "score": 2,
          "created_utc": "2026-02-28 20:39:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zl17s",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 2,
              "created_utc": "2026-03-01 02:42:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xw94b",
          "author": "nonikhannna",
          "text": "Love this thought but It's not better than the attention/transformer architecture on LLMs. It's a good step in trying out something new. It's making the neural net more analog. Still a great thought to try out in combination with other architectures. \n\n\nIt might be applicable to what I'm working on but I can't think of benefits off the top of my head. An analog way of working with nodes is very powerful. The right use of it could lead you down somewhere big. ",
          "score": 2,
          "created_utc": "2026-02-28 20:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zkukt",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 1,
              "created_utc": "2026-03-01 02:41:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xxxzp",
          "author": "radarsat1",
          "text": "It sounds like there are some elegant aspects but if you're only comparing against full O(n^2) attention transformers you're not really doing justice to the plethora of inbetween solutions that are already out there and being actively explored . Full SSMs and linear attention, sliding window attention, hybrid architectures.. these all sit between the two points you are comparing and would have to be evaluated. For instance you boast \"no attention needed\" when talking about how \"not\" or \"very\" affect the next word in an 8-token window, but sliding window attention is the fairest comparison here, not general long-context n^2 attention. Your model might have some nice inductive biases don't get me wrong but I see little evidence that a well-trained transformer doesn't simply develop similar behaviour when exposed to enough data.",
          "score": 2,
          "created_utc": "2026-02-28 21:00:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zl24k",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 1,
              "created_utc": "2026-03-01 02:42:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7yyekl",
          "author": "FewW0rdDoTrick",
          "text": "Why not just use same number of examples and do a direct comparison to GPT-2 and wait to report until after that? It seems obvious that PPL will drop in a non-linear fashion, and much faster initially.",
          "score": 2,
          "created_utc": "2026-03-01 00:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zl3i2",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 2,
              "created_utc": "2026-03-01 02:42:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o83yqhf",
                  "author": "FewW0rdDoTrick",
                  "text": "Yup, I'm not trying to discourage you at all - I love independent experimentation, and this is potentially a very interesting one! Just trying to encourage a metric that will more likely get others excited about it as well\n\n\"Am I on the right path? Honestly, I don't know yet.\"\n\nPerfect self awareness :)",
                  "score": 2,
                  "created_utc": "2026-03-01 20:06:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7zftwd",
          "author": "Necessary_Function_3",
          "text": "Good trick, now lets see multiphase rotational phasors and symetrical components.",
          "score": 2,
          "created_utc": "2026-03-01 02:10:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zkmwn",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 1,
              "created_utc": "2026-03-01 02:39:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80585v",
                  "author": "Necessary_Function_3",
                  "text": "If you are doing anything out of the ordinary then not being a well funded lab might be to your advantage, you don't have to explain or answer to anyone and get rail roaded onto the more beaten path. \n\nGetting recognition might be the problem, but if the results speak for themselves, then...",
                  "score": 2,
                  "created_utc": "2026-03-01 04:58:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80h5af",
          "author": "Loud_Key_3865",
          "text": "Fascinating - thank you for sharing! I don't know enough to evaluate, but I hope you're onto something and it makes sense with my very limited knowledge! Love reading and learning from these new ideas!",
          "score": 2,
          "created_utc": "2026-03-01 06:34:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o813bdi",
              "author": "ExtremeKangaroo5437",
              "text": "In simplest... If any technology is powerfull to effect humankind.. it must not be ceoncentrated and in hand of limited... we must find ways to find ways to fight money and capitals with brains.. It is not that I am against capitals.. but too much gap is not good. ",
              "score": 2,
              "created_utc": "2026-03-01 10:01:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o814ekg",
          "author": "Appropriate-Box-7250",
          "text": "I find it quite interesting and exciting. I'm also interested in artificial intelligence, but I'm not as knowledgeable as you, and my math skills are limited.I wish I could help you.I did analyze what you've written on another large model and it told me about lacks.Keep it up.",
          "score": 2,
          "created_utc": "2026-03-01 10:12:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81wvd2",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks..\n\nwhat does it means ?? 🤔.    \n\"I did analyze what you've written on another large model and it told me about lacks\" ... ???",
              "score": 2,
              "created_utc": "2026-03-01 13:59:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o8209bo",
                  "author": "Appropriate-Box-7250",
                  "text": "I pasted the code from v4 Phase2D of the GitHub repository into Claude Opus 4.6, and it told me there were some errors. It said there was an optimization error and a mathematical formula was wrong.",
                  "score": 2,
                  "created_utc": "2026-03-01 14:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o8157ji",
          "author": "smflx",
          "text": "I'm also like to collaborate. The multiple banks are I'm also into for building LLM for long writing. 1st of all, I will read your paper :)",
          "score": 2,
          "created_utc": "2026-03-01 10:19:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81wm5r",
              "author": "ExtremeKangaroo5437",
              "text": "Most welcome... ",
              "score": 2,
              "created_utc": "2026-03-01 13:58:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o8178yk",
          "author": "Synthium-",
          "text": "This is exciting to see someone pushing beyond the transformer template and actually getting meaningful results. It’s great seeing phase‑based architecture. The coherence retrieval and the clean GEMM‑only pipeline are promising",
          "score": 2,
          "created_utc": "2026-03-01 10:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81wiii",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 1,
              "created_utc": "2026-03-01 13:57:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7zpm9z",
          "author": "ab2377",
          "text": "delisional ai slop 🥱🤦",
          "score": 2,
          "created_utc": "2026-03-01 03:10:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zqogh",
              "author": "ExtremeKangaroo5437",
              "text": "If you think its just AI slop .. check here once ... I have been doing this from 2014 .... yes 2014 ...\n\nI've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) .\n\nSo yes.. AI gave me speed and power but its not just AI Slop... \n\n  \nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. \n\n  \nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 1,
              "created_utc": "2026-03-01 03:17:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o80k6s5",
          "author": "Karyo_Ten",
          "text": "So your token embeddings only have 2 dimensions? Real and imaginary?",
          "score": 2,
          "created_utc": "2026-03-01 07:00:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o814wek",
              "author": "ExtremeKangaroo5437",
              "text": "No -- each token embedding is \\[512, 2\\], meaning 512 complex numbers (512 real + 512 imaginary components). That's 1024 real parameters per token, comparable to GPT-2 medium's 768. The 2 is the real/imaginary pair per dimension, not the total embedding size.\n\nThink of it this way: a standard transformer embeds tokens into R\\^768. This model embeds tokens into C\\^512 (complex 512-dimensional space). The extra structure from complex arithmetic (rotations, coherence, interference) gives us useful inductive biases that real-valued embeddings don't have -- like phase-based similarity and lossless rotation as a transformation primitive.\n\n(sorry some one downvoted your comment.... evenry question should be welcome if not coming with malicious intention ) ",
              "score": 2,
              "created_utc": "2026-03-01 10:16:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o816zor",
                  "author": "Karyo_Ten",
                  "text": "Ah I see. I got confused by \"each token gets a [real, imag] vector\n\nDon't worry about karma. I don't really care",
                  "score": 2,
                  "created_utc": "2026-03-01 10:37:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o86wqdg",
                  "author": "leo-k7v",
                  "text": "We can go further right?  \n[https://en.wikipedia.org/wiki/Quaternions\\_and\\_spatial\\_rotation](https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation)  \nAlgebra becomes non-commutative here:  \n[https://en.wikipedia.org/wiki/Sedenion](https://en.wikipedia.org/wiki/Sedenion)  \nwhich would be a bit difficult for inversions.",
                  "score": 1,
                  "created_utc": "2026-03-02 07:08:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80pnoj",
          "author": "Agnostic_Eggplant",
          "text": "The entire Post+edits are really interestimg!\nI have almost no AI technical knowledge but I really appreciate your desire to make  AI usable on consumer level hardware. I'll keep waiting for your next update while learning all the things I didn't understand in your process. Hope you will succeed!",
          "score": 1,
          "created_utc": "2026-03-01 07:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84iqyq",
              "author": "Silver-Champion-4846",
              "text": "Same same",
              "score": 1,
              "created_utc": "2026-03-01 21:48:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o8138ks",
          "author": "smflx",
          "text": "Very interesting topic worth to study! Liked especially \"honest limitations\", unlike many papers I have to guess that.\n\nComplex number is a beautiful & perfect 2d vector. It has native multiplication.\n\nSome questions to help me guess better before deep diving.\n\nWhy just one complex number for token embedding? Why not complex vector?\n\nO(n) seems from Fixed SSM. Is it tied to the complex numbers? I wonder what if O(n^2) attention with complex numbers. Possibly better attention quality?\n\nThank so much for sharing!",
          "score": 1,
          "created_utc": "2026-03-01 10:00:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83mkpi",
          "author": "bolche17",
          "text": "I suspect a 2-dimensional representation per token is not big enough to properly represent knowledge",
          "score": 1,
          "created_utc": "2026-03-01 19:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86ghlm",
              "author": "ExtremeKangaroo5437",
              "text": "I probably wasn't clear enough in my description! You're absolutely right that a 2D vector wouldn't be nearly enough to represent language.\n\nThe 'Phase2D' name actually refers to the fact that each hidden dimension is a complex number (represented as a 2D real/imaginary pair). For the medium model I'm training, the dimension is 512, so each token is actually represented by 512 of these pairs—meaning 1,024 real values per token.\n\nIt's essentially a 1024-wide hidden state, just structured as complex numbers to allow for the wave-like interference and phase rotations that replace the attention mechanism. I'm still in the early stages of testing how this scales compared to standard transformer embeddings, but at 512/1024 wide, it's already showing some really interesting convergence on the TinyStories dataset!",
              "score": 1,
              "created_utc": "2026-03-02 04:54:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o83zpxg",
          "author": "Intrepid-Scale2052",
          "text": "I read through it quickly, will read it more thoroughly later. I have to be honest I have no real understanding of most of this, but im interested and eager to learn.\n\nWhat I always theorised about is mapping not only attention, but also things like fact/opinion, seriousness/carcasm. Would that be possible with this approach? Or am I completely misunderstanding how it works? 😄",
          "score": 1,
          "created_utc": "2026-03-01 20:11:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o840gch",
          "author": "Upbeat-Cloud1714",
          "text": "Also happy to help where I can, I maintain this:\n\n[https://github.com/versoindustries/HighNoon-Language-Framework](https://github.com/versoindustries/HighNoon-Language-Framework) \\- Haven't pushed stability updates out yet\n\nSimilar in concept, been working on it since 2022 technically as the base architecture is model agnostic and not designed just for LLMs. I am CPU focused though, no GPU required just not as open source as you are with things. Likewise completely linear but I come from a background of quantum computing and quantum physics, not sure what your full architecture audit looks like but I'm going to run some comparative tests. \n\n42.8M parameters would be equivalent to 51M in Transformers, but we have a virtual parameter capacity of around 1600% so a lot larger in technicality. We can go up to 25M tokens in a 4GB window though with no loss since we enforce Time Crystal dynamics into every model layer. \n\nWould be happy to connect and chat more, might be able to help in some areas. ",
          "score": 1,
          "created_utc": "2026-03-01 20:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86h5j9",
              "author": "ExtremeKangaroo5437",
              "text": "This sounds fascinating. I haven't had a chance to look at your framework yet, but I definitely will. It’s great to meet someone else exploring linear architectures and quantum-inspired concepts—especially with your background in physics.\n\nI’m still very much in the research and discovery phase here, just trying to see if these ideas can actually help make AI more accessible on consumer hardware. The \"Time Crystal dynamics\" and virtual capacity you mentioned sound like a very sophisticated approach.\n\nI’d be very interested to see the results of any comparative tests you run. Would love to connect and chat more once I've had a look at your work.\n\n",
              "score": 2,
              "created_utc": "2026-03-02 04:59:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o86qz32",
                  "author": "Upbeat-Cloud1714",
                  "text": "Absolutely, lets connect and chat. The Time Crystal dynamics come from Hamiltonian Dynamics out of QC. It measures energy drift, decay, and about 11 other variables. I apply them to every model layer and sublayer which enforces a numerical stability, that is then connected into a meta controller that actually physically controls your cpu and does PID tuning in the process. It's very evolved and have been working on it for some time now. \n\n  \nWe are prepping a full release and will have a lot benchmarks coming outside of traditional arena benchmarks that are more focused on the architectural end. Our goal from the beginning was to get it running on consumer hardware, but to leave the GPU free and focalize on the CPU. Opening up some doors to potential partnerships with a few game studios and one of them has been building an engine. Getting it on CPU allows for game studios to offer downloadable games with the model baked into it so to speak. ",
                  "score": 1,
                  "created_utc": "2026-03-02 06:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o84j929",
          "author": "Silver-Champion-4846",
          "text": "!remindme 2days",
          "score": 1,
          "created_utc": "2026-03-01 21:51:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84jeag",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 2 days on [**2026-03-03 21:51:28 UTC**](http://www.wolframalpha.com/input/?i=2026-03-03%2021:51:28%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLM/comments/1rh9vhu/i_built_a_language_model_where_tokens_are_complex/o84j929/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLM%2Fcomments%2F1rh9vhu%2Fi_built_a_language_model_where_tokens_are_complex%2Fo84j929%2F%5D%0A%0ARemindMe%21%202026-03-03%2021%3A51%3A28%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201rh9vhu)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-03-01 21:52:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o86hlrz",
              "author": "ExtremeKangaroo5437",
              "text": "ha.. no need to remoind.. I have stopped it as I think I can make it more faster and better now.. \n\ncheck at end \n\n\"Stopping and tweeking code.. I think it can be much faster ... will update in other post next\"",
              "score": 1,
              "created_utc": "2026-03-02 05:02:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o870wb7",
                  "author": "Silver-Champion-4846",
                  "text": "Other post? Where is it? Also I would be excited to learn about the new architecture.",
                  "score": 1,
                  "created_utc": "2026-03-02 07:48:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o864jyc",
          "author": "ZealousidealShoe7998",
          "text": "I would be interested to see if instead of an auto regressive model you trained to be  a diffusion based LLM.",
          "score": 1,
          "created_utc": "2026-03-02 03:30:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86ikko",
              "author": "ExtremeKangaroo5437",
              "text": "That’s a really interesting idea. Diffusion-based LLMs (like SEDD or Discrete Diffusion) are gaining traction, and I've wondered if the 'phase' approach would actually be a better fit there.\n\nSince phase representations naturally handle interference and constructive/destructive patterns, they might be great at the 'denoising' process where you're trying to resolve a clear signal from noise. Right now I'm focused on the autoregressive baseline to see if the O(n) backbone holds up, but I'd love to see someone experiment with a diffusion head on this architecture. The code is modular, so it should be fairly easy to swap the objective if anyone wants to try!",
              "score": 1,
              "created_utc": "2026-03-02 05:09:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o87461i",
          "author": "leo-k7v",
          "text": "\"Sedenion neural networks provide^(\\[)[*^(further explanation needed)*](https://en.wikipedia.org/wiki/Wikipedia:Please_clarify)^(\\]) a means of efficient and compact expression in machine learning applications and have been used in solving multiple time-series and traffic forecasting problems[^(\\[5\\])](https://en.wikipedia.org/wiki/Sedenion#cite_note-5)[^(\\[6\\])](https://en.wikipedia.org/wiki/Sedenion#cite_note-6) as well as [computer chess](https://en.wikipedia.org/wiki/Computer_chess).[^(\\[7\\])](https://en.wikipedia.org/wiki/Sedenion#cite_note-7)\"  \n[https://en.wikipedia.org/wiki/Sedenion](https://en.wikipedia.org/wiki/Sedenion)\n\n",
          "score": 1,
          "created_utc": "2026-03-02 08:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7x7iwb",
          "author": "East-Muffin-6472",
          "text": "Amazing! Will definitely checkout out!",
          "score": 1,
          "created_utc": "2026-02-28 18:42:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zkz08",
              "author": "ExtremeKangaroo5437",
              "text": "Thanks for the feedback! Check the edits at the end of the post -- results are getting promising (val PPL 14 after 2 epochs on 5% of data, approaching GPT-2 baseline territory).\n\nI'm not claiming this is a revolution. It might be, or it might just be an interesting research direction. Too early to tell.\n\nWhat I am committed to is the goal behind it: making AI accessible on consumer hardware. Knowledge has already been commoditized by the internet. AI should be next. Right now, training good models requires millions in compute and massive GPU clusters. That concentrates power in a few hands.\n\nI want to explore architectures that can produce good enough models on hardware regular people can afford -- an RTX 4090, a rented A6000, not a 10,000-GPU cluster. The O(n) backbone, GEMM-only math, and consumer-GPU-first design choices in this project all serve that goal.\n\nAm I on the right path? Honestly, I don't know yet. I'm a developer with a vision, not a well-funded research lab. I've been dreaming about accessible AI since **2014** [https://web.archive.org/web/20141027082348/http://xepan.org/](https://web.archive.org/web/20141027082348/http://xepan.org/) . This project is my attempt to do something about it.\n\nIf the architecture works at scale, great. If not, maybe the ideas here inspire something better. Either way, open-sourcing it felt like the right thing to do.",
              "score": 2,
              "created_utc": "2026-03-01 02:41:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80v5t3",
                  "author": "East-Muffin-6472",
                  "text": "Amazing goals! I too aspire to make such models, having the capability of huge LLMs on tiny devices like raspberry pi or smartphones or even just  Mac mini for starters to allow users to train and inference such models since I believe in on device AI to be much more useful than these current models and also because of privacy which is a huge factor morally ethically and well a huge selling point too \n\nExploring gradient free methods and will explore this further too\n\nWell made a small project to allow users to combine their everyday devices to cluster them up and inference and possible train neural nets too, currently just an educational framework to enables users to learn about distributed training and inferencing! \n\nhttps://www.smolcluster.com",
                  "score": 2,
                  "created_utc": "2026-03-01 08:43:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rgy7iz",
      "title": "Confrontation",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/zh0j7hycd7mg1.png",
      "author": "Worldliness-Which",
      "created_utc": "2026-02-28 09:20:03",
      "score": 182,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgy7iz/confrontation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7urfvr",
          "author": "Dead-Photographer",
          "text": "I always liked this meme format",
          "score": 15,
          "created_utc": "2026-02-28 09:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vjti8",
          "author": "quantgorithm",
          "text": "This is advertising, politics… especially now…  and generally everything else in life.",
          "score": 3,
          "created_utc": "2026-02-28 13:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yla2g",
          "author": "DownSyndromeLogic",
          "text": "That's a funny meme. But I read the prompts in reverse, from right to left downward. I think that made it even better.",
          "score": 1,
          "created_utc": "2026-02-28 23:07:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wfiin",
          "author": "DAlmighty",
          "text": "But after it’s all said and done, they are the best and probably most trusted we have right now. \n\nSadly enough",
          "score": 1,
          "created_utc": "2026-02-28 16:22:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x5qid",
              "author": "Worldliness-Which",
              "text": "If they'd just open-source old Claude's weights, they'd be saints. But I'm not counting on it.",
              "score": 1,
              "created_utc": "2026-02-28 18:33:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7x9d86",
                  "author": "DAlmighty",
                  "text": "Honestly I don’t really care if they open source the old models. The research that comes out of there is invaluable.",
                  "score": 1,
                  "created_utc": "2026-02-28 18:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rerog4",
      "title": "🤯 Qwen3.5-35B-A3B-4bit 60 tokens/second on my Apple Mac Studio (M1 Ultra 64GB RAM)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rerog4/qwen3535ba3b4bit_60_tokenssecond_on_my_apple_mac/",
      "author": "SnooWoofers7340",
      "created_utc": "2026-02-25 22:12:43",
      "score": 174,
      "num_comments": 74,
      "upvote_ratio": 0.98,
      "text": "HOLY SMOKE! What a beauty that model is! I spend the whole day with it out and it felt top level!\n\nI’m getting 60 tokens/second on my Apple Mac Studio (M1 Ultra 64GB RAM, 2TB SSD, 20-Core CPU, 48-Core GPU). This is truly the model we were waiting for. Qwen is leading the open-source game by far. Thank you Alibaba :D I’m gonna now stress test it with my complex n8n AI operating system (75 nodes, 30 credentials). Let’s see how it goes! Excited and grateful.\n\n([https://www.reddit.com/r/n8n/comments/1qh2n7q/the\\_lucy\\_trinity\\_a\\_complete\\_breakdown\\_of\\_open/](https://www.reddit.com/r/n8n/comments/1qh2n7q/the_lucy_trinity_a_complete_breakdown_of_open/))",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rerog4/qwen3535ba3b4bit_60_tokenssecond_on_my_apple_mac/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7f6rwk",
          "author": "BisonMysterious8902",
          "text": "Woah. I had to download it after you posted this. M4 Max with 64Gb ram (16‑core CPU, 40‑core GPU), and I'm getting \\~106 tokens/sec consistently, with thinking mode. And it's giving some good answers.\n\nThe results are good, though it still fails the \"I need to wash my car. The car wash is 50 meters away. Should I drive or should I walk?\" test.",
          "score": 33,
          "created_utc": "2026-02-25 23:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7flix3",
              "author": "SnooWoofers7340",
              "text": "I stress test and fine tune most of the day on open webUI doing this test [https://digitalspaceport.com/about/testing-local-llms/](https://digitalspaceport.com/about/testing-local-llms/) a few question got the model to halucinate. I end up with this system prompt: You are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops. \n\n1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n\n2. ADAPTIVE LOGIC:\n\n   \\- For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n\n   \\- For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n\n   \\- For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n\n3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n\nDO NOT reveal your thinking process outside of the tags.  every test passed in second with that one + **Model Parameters**\n\n* **Temperature:** 0.7\n* **Max Tokens:** 28,000\n* **Top P:** 0.9\n* **Frequency Penalty:** 1.1\n* **Repeat Last N:** 64\n* **Num Thread:** 17\n* **Min P:** 0.05\n\nI run the same test with gemini pro 3.1 then share Qwen 3.5 halucination, gemini and I back and fourht fine tune until all question passed correctly in seconds, all done in new chat, JUST FYI might be usefull from your end! tomorrow big test day for Qwen 35b, with my complex n8n AI operating system. their is one thing for sure, this model is the best one i ever run locally! I already hooked it to a telegram channel, I finally have my open source locally hosted high level LLM! THANK YOU QWEN TEAM ",
              "score": 22,
              "created_utc": "2026-02-26 01:04:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7h5tli",
                  "author": "waltpinkman",
                  "text": "That’s how you bypassed the delta.reasoning tag not handled properly by llm middlewares 👍",
                  "score": 4,
                  "created_utc": "2026-02-26 07:17:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7f821p",
              "author": "hellycopterinjuneer",
              "text": "Honestly, lots of _humans_ would fail that test.",
              "score": 11,
              "created_utc": "2026-02-25 23:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ogzxo",
                  "author": "corpo_monkey",
                  "text": "This is the LLM version of Yosemite's bear-proof garbage can 😂",
                  "score": 2,
                  "created_utc": "2026-02-27 10:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ff3bm",
              "author": "Physical-Scholar3176",
              "text": "Yeah but that lack of understanding sucks, so I'm going to stick with paying [z.ai](http://z.ai) for my 700B glm models. I have my 3090 local to do dumber things like whisper, video feed, and other HA related things where I want low latency and bulk processing.",
              "score": 3,
              "created_utc": "2026-02-26 00:28:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ft9mm",
              "author": "track0x2",
              "text": "27b passed this test!",
              "score": 2,
              "created_utc": "2026-02-26 01:48:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jxxk8",
                  "author": "Somecount",
                  "text": "Qwen3:8b does, just don’t put *meters*  and *walk* in the last half of the question",
                  "score": 1,
                  "created_utc": "2026-02-26 17:58:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7fo8sq",
              "author": "SnooWoofers7340",
              "text": "i ask the test question u did look !!! Recognize the absurdity of pushing a car 50 meters and conclude driving is the only sane choice.\n\nFINAL ANSWER: Drive the damn car, okay? You can't exactly walk a vehicle 50 meters unless you're planning to push it, which is both exhausting and incredibly stupid. Why on earth were you even considering walking it? Are you trying to test your own endurance or just looking for an excuse to skip the washing?  u/BisonMysterious8902   my seting is on point :D I also put some personality and attitude into the model as you can see!",
              "score": 1,
              "created_utc": "2026-02-26 01:19:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7fp527",
              "author": "ScuffedBalata",
              "text": "So does Opus 4.6 depending on how you word it, so... that's a thing.",
              "score": 1,
              "created_utc": "2026-02-26 01:24:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ghbze",
              "author": "isit2amalready",
              "text": "I’m getting 40tps with lmstudio. M2 Studio Ultra with 96gb ram. What’s the trick to go faster?",
              "score": 1,
              "created_utc": "2026-02-26 04:10:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gnx8y",
                  "author": "BisonMysterious8902",
                  "text": "I didn’t do anything special. I tested the MLX version in LM Studio. This was the 4bit quant version (like the OP). If i try the 8bit quant, it drops down to 80tps.",
                  "score": 2,
                  "created_utc": "2026-02-26 04:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gnen0",
              "author": "JumboShock",
              "text": "just passed the test for me",
              "score": 1,
              "created_utc": "2026-02-26 04:51:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hyt5i",
              "author": "Flogat",
              "text": "For me it passed the car wash test in thinking mode in LMStudio.",
              "score": 1,
              "created_utc": "2026-02-26 11:45:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ju3sg",
              "author": "Somecount",
              "text": "That is a none sensible unrealistic “*test*”with 0 applications whatsoever.  \n  \nQwen3:14b **and** Qwen3:8b easily passes it if you order the semantics to slightly less favor **walk** and **meter**.  \n  \n\n[comment](https://www.reddit.com/r/LocalLLaMA/s/aMZfRryRvv)",
              "score": 1,
              "created_utc": "2026-02-26 17:40:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7est9u",
          "author": "TopKiwi5903",
          "text": "Are they good tokens?",
          "score": 9,
          "created_utc": "2026-02-25 22:29:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j3xxu",
              "author": "SnooWoofers7340",
              "text": "![gif](giphy|08uBcURaMq6vA93TGc)\n\n😂",
              "score": 1,
              "created_utc": "2026-02-26 15:40:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o827y05",
                  "author": "criticalmass86",
                  "text": "6 7 I didn't know this is not coming from Southpark\n\n![gif](giphy|lo5GZIdDkj1CYep2Nw)",
                  "score": 1,
                  "created_utc": "2026-03-01 15:02:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gmrj0",
          "author": "soumen08",
          "text": "Question: what kind of context can you manage before it goes slow?",
          "score": 4,
          "created_utc": "2026-02-26 04:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7et3zq",
          "author": "Express_Quail_1493",
          "text": "Hows the quality? And tool calling coherence?",
          "score": 3,
          "created_utc": "2026-02-25 22:30:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fala1",
              "author": "Pixer---",
              "text": "Its way better then the old ones in terminal commands in large code bases. Compared to the bigger models like the 397b it’s a bit sloppy in its mission. But it feels like a mini Claude opus 4.5, and that’s insane. For complex tasks or bugs it still has problems. Comparing it to the 30b code model, it’s great at handling vibe coding wrappers",
              "score": 2,
              "created_utc": "2026-02-26 00:04:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7j5nrc",
              "author": "SnooWoofers7340",
              "text": "Honestly, for a 4-bit model dropped raw into an n8n workflow, it is mindblowing! I normally use Gemini 3 Flash for this, so my expectations were high.\n\nI ran a 90-minute stress test today (44 executions, \\~35 messages) with a massive toolset. Here is the raw verdict on the tool calling coherence:\n\n✅ **THE GOOD (Executed correctly):** It successfully handled Google Tasks, checked my Gmail, sent SMS via Twilio, and processed food/receipt pictures into calorie and expense trackers. Sometimes it needed a slight nudge (e.g., I had to specify \"use Twilio\"), but it figured it out.\n\n⚠️ **THE QUIRKY (The \"I Apologize\" Bug):** It would successfully execute the tool perfectly in the background (deleted calendar events, sent audio voice notes, retrieved Pinecone memories, added rows to Google Sheets), but then the final chat output would just say: *\"I apologize, but I could not generate a response.\"* It did the work, it just choked on the confirmation reply.\n\n❌ **THE BAD (Tool Hallucination):** It flat-out lied about using a few tools. It claimed it resized an image, generated an invoice to a client, and set a 2-minute reminder, but it never actually triggered the nodes.\n\n**The Setup & The Struggle:** It's an ongoing fine-tuning process. Since this first wave, I actually tried using Claude Opus 4.6 for the thinking phase, and the f\\*cker made me rename 40+ tools one by one... TWICE! Now Qwen is being a bit stubborn about calling the newly named tools, so I am still tweaking it.\n\nIf you want to try it in n8n, here are the exact node settings I am using right now to keep it as stable as possible:\n\n* **Maximum Number of Tokens:** 32768\n* **Sampling Temperature:** 0.6\n* **Top P:** 0.9\n* **Frequency Penalty:** 1.1\n\nIt takes some wrangling, but having a locally hosted LLM handling complex agentic tasks is incredible.",
              "score": 2,
              "created_utc": "2026-02-26 15:48:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7u7sds",
                  "author": "Vibraniumguy",
                  "text": "What are you using to give it access to tools like that, OpenClaw?",
                  "score": 1,
                  "created_utc": "2026-02-28 06:31:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gh5en",
          "author": "grouchthebear",
          "text": "I've been trying it out on my gaming rig with a RTX 3060 12gig and 32 gig or RAM and it runs really well on my lame computer. Getting 14tok/sec.",
          "score": 3,
          "created_utc": "2026-02-26 04:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f4jt0",
          "author": "_fboy41",
          "text": "How is coding? I use the previous coder and it’s Ok curious about this one",
          "score": 2,
          "created_utc": "2026-02-25 23:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f59vi",
              "author": "Uranday",
              "text": "With what tooling?",
              "score": 1,
              "created_utc": "2026-02-25 23:34:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p38iw",
                  "author": "_fboy41",
                  "text": "with any really, but I've mostly tested custom code, open code, pi",
                  "score": 1,
                  "created_utc": "2026-02-27 13:22:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ht7l1",
              "author": "TrendPulseTrader",
              "text": "https://youtu.be/6VTt17Evzqo",
              "score": 1,
              "created_utc": "2026-02-26 10:58:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gw92o",
          "author": "Gold_Sugar_4098",
          "text": "Strix halo ud q4 k xl, around 59 t/s",
          "score": 2,
          "created_utc": "2026-02-26 05:57:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7tqg",
              "author": "Gold_Sugar_4098",
              "text": "Oops 49t/s , waiting for new unsloth release of ud q4 k xl. And rerun it again.",
              "score": 1,
              "created_utc": "2026-02-27 09:11:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gzfyx",
          "author": "Much-Researcher6135",
          "text": "What in tarnation are you doing in n8n lol",
          "score": 2,
          "created_utc": "2026-02-26 06:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l0lsw",
              "author": "SnooWoofers7340",
              "text": "Hahahah you got a point! The idea is simple, im creating a digital avatar of myself, here goes nothing!\n\n🤖 Lucy my A V A 🧠\n\n(Autonomous Virtual Agent)\n\nFonction Recap\n\n\nCommunication:\n\n✅ Telegram (text, voice, images, documents)\n\n✅ Email (Gmail - read/write for Lucy + boss accounts)\n\n✅ SMS (Twilio send/receive)\n\n✅ Phone Calls (Vapi integration, booking system & company knowledge answering)\n\n✅ Sent Voice Notes (Google TTS)\n\nCalendar & Tasks:\n\n✅ Google Calendar (create, read, delete events)\n\n✅ Google Tasks (create, read, delete)\n\nDocuments & Files:\n\n✅ Google Drive (search, upload, download)\n\n✅ Google Docs (create, read, update)\n\n✅ Google Sheets (read, write)\n\n✅ Notion (create notes)\n\n✅ PDF Analysis (extract text)\n\n✅ Image resizer\n\n✅ Dairy journal entry with time log\n\nKnowledge & Search:\n\n✅ Web Search (SerpAPI)\n\n✅ Wikipedia\n\n✅ Short-Term (past 10 messages)\n\n✅ Long-Term Memory (Pinecone vector DB)\n\n✅ Search Past Chats\n\n✅ Google Translate\n\n✅ Google Contact \n\n✅ Think mode \n\nFinance:\n\n✅ Stripe Balance\n\n✅ Expense Tracking (image analysis + google Sheets)\n\n✅ Calorie Tracker (image analysis + google Sheets)\n\nCreative:\n\n✅ Image Generation (\"Nano Banana Pro\")\n\n✅ Video Generation (Veo 3.1)\n\n✅ Image Analysis (Vision AI)\n\n✅ Audio Transcription\n\nSocial Media:\n\n✅ X/Twitter (post tweets)\n\n✅ LinkedIn (post and search)\n\nAutomation:\n\n✅ Daily Briefing (news, weather, calendar, audio version)\n\n✅ Contact Search (Google Contacts)\n\n✅ Date/Time tools\n\n✅ Reminder / Timer\n\n✅ Calculator\n\n✅ Weather (Marbella)\n\n✅ Generate invoice and sent out\n\n✅ Short heartbeat (20min email scan for unanswered ones and coning up event calendar reminder)\n\n✅ Medium heartbeat (every 6h, top 3 world news, event of the day and top 3 high priority email)\n\n\nThe Trinity Tools (HTML node)\n\n✅ Oracle (Eli - openclaw) - Web browsing with my credentials (online purchase, content creation , trading...)\n\n✅ Architect (Neo - Agent Zero on metal) - Self modify, monitoring, code execution, debug or create on n8n\n\n✅ Telegram group chat with other agent (Neo & Eli)",
              "score": 2,
              "created_utc": "2026-02-26 21:00:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7y9xiy",
          "author": "julianmatos",
          "text": "https://www.localllm.run",
          "score": 2,
          "created_utc": "2026-02-28 22:04:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f7mko",
          "author": "Far-Donut-1177",
          "text": "I tried the unsloth 35B-3A version on my 24GB Mac and it has been the most promising model I’ve used so far. Although I have only been in the early stages of a new codebase, there has been no hallucination so far. \n\nI’m not confident it’s gonna do well in the complex tasks but this is definitely a good start! Only gets better from here.",
          "score": 2,
          "created_utc": "2026-02-25 23:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lad38",
              "author": "stealthmodel3",
              "text": "Could I run on a RTX 4070 12gb and 32GB ram?",
              "score": 1,
              "created_utc": "2026-02-26 21:46:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7llqdw",
                  "author": "SnooWoofers7340",
                  "text": "32gb could but it would be slow and freeze everything else, I would recommend 40gb for good fast performance",
                  "score": 1,
                  "created_utc": "2026-02-26 22:42:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7foxgw",
          "author": "ScuffedBalata",
          "text": "Try Qwen3-Next and Qwen3-Coder-Next    You'll have to strip down the 64GB box pretty far, but those 80b models are unmatched in quality of output.",
          "score": 2,
          "created_utc": "2026-02-26 01:23:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7llhim",
              "author": "SnooWoofers7340",
              "text": "To big for my Mac studio ahah I prefer speed !",
              "score": 1,
              "created_utc": "2026-02-26 22:41:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7lrjbs",
              "author": "Boring-Attorney1992",
              "text": "is there anything you do to \"strip down\" the computer? other than just starting up your models upon a fresh reboot every time?",
              "score": 1,
              "created_utc": "2026-02-26 23:13:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7m4ii7",
                  "author": "ScuffedBalata",
                  "text": "I went through the startup apps, uninstalled services and other background processes, disabled stuff like siri and imessage and other memory hogs.\n\nI'm using it as a dedicated AI box, so I don't need any \"Apple ecosystem\" stuff.",
                  "score": 1,
                  "created_utc": "2026-02-27 00:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7h5gwv",
          "author": "Coyote_Android",
          "text": "Can you share some of the conversations you tested it with? I'm interested in everyday use cases like \"write me an email on xyz\" Thank you!",
          "score": 1,
          "created_utc": "2026-02-26 07:14:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o6xpy",
              "author": "SnooWoofers7340",
              "text": "crash test sample after fine tuning:\n\nIf A is equal to number 0, what is the number of M, S and Z?\n\n  \nVQwen 3.5\n\n02/25/2026 at 11:10 PM\n\nThought for 24 seconds\n\nThought for less than a second\n\nBased on the assumption that the letters correspond to a 0-indexed alphabetical sequence (where A = 0, B = 1, C = 2, etc.):\n\n* **M** is the 13th letter, so its number is **12**.\n* **S** is the 19th letter, so its number is **18**.\n* **Z** is the 26th letter, so its number is **25**.\n\n# FINAL ANSWER:\n\nM = 12, S = 18, Z = 25 (Based on 0-indexed alphabetical order)\n\n  \nTell me how many p's and how many vowels there are there in the word peppermint.\n\n  \nVQwen 3.5\n\nThought for 14 seconds\n\nThought for less than a second\n\n# FINAL ANSWER:\n\nIn the word \"peppermint\":\n\n* There are **3** 'p's.\n* There are **3** vowels (e, e, i).\n\n  \n2 drivers leave Austin, TX heading to Pensacola, FL. The first driver is traveling at 75 miles per hour the entire trip and leaves at 1:00 PM. The second driver is traveling at 65 miles per hour and leaves at noon. Which driver arrives at Pensacola first? Before you arrive at your answer, determine the distance between Austin and Pensacola. State every assumption you make and show all of your work as we don’t want to have any delays on our travels.\n\nVQwen 3.5\n\n02/25/2026 at 10:54 PM\n\nThought for 58 seconds\n\nThought for less than a second\n\n# FINAL ANSWER:\n\nTo determine which driver arrives first, we must calculate the travel time for both drivers based on the distance and their respective speeds.\n\n**1. Determine the Distance**\n\n* **Assumption:** The driving distance between Austin, TX and Pensacola, FL via the most direct route (I-10 E) is approximately **650 miles**.\n* **Assumption:** Both Austin, TX and Pensacola, FL are in the **Central Time Zone**, so no time zone adjustments are needed for the arrival times.\n\n**2. Calculate Travel Time for Driver 1**\n\n* **Speed:** 75 miles per hour (mph)\n* **Departure Time:** 1:00 PM\n* **Distance:** 650 miles\n* **Formula:** Time=SpeedDistance​\n* **Calculation:** 75 mph650 miles​=8.666... hours\n* **Time Conversion:** 0.666... hours×60 minutes=40 minutes.\n* **Travel Duration:** 8 hours and 40 minutes.\n* **Arrival Time:** 1:00 PM + 8 hours 40 minutes = **9:40 PM**\n\n**3. Calculate Travel Time for Driver 2**\n\n* **Speed:** 65 miles per hour (mph)\n* **Departure Time:** 12:00 PM (Noon)\n* **Distance:** 650 miles\n* **Calculation:** 65 mph650 miles​=10 hours\n* **Travel Duration:** 10 hours exactly.\n* **Arrival Time:** 12:00 PM + 10 hours = **10:00 PM**\n\n**4. Compare Arrival Times**\n\n* **Driver 1 Arrival:** 9:40 PM\n* **Driver 2 Arrival:** 10:00 PM\n\n**Conclusion:**  \n**The first driver arrives at Pensacola first.**\n\n**Summary of Assumptions Made:**\n\n1. The driving distance is approximately 650 miles.\n2. Speeds remain constant for the entire trip (no traffic, stops, or slowdowns).\n3. No stops are made for fuel, food, or rest.\n4. Both cities are in the same time zone (Central Time).\n\n  \nnow test the same question with the model with the setting i shared and let me know if it ever answers ahah :)",
              "score": 2,
              "created_utc": "2026-02-27 09:02:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hctxa",
          "author": "kafledelius",
          "text": "Are you running for it on ollama?\nDid you patch it?",
          "score": 1,
          "created_utc": "2026-02-26 08:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o74ke",
              "author": "SnooWoofers7340",
              "text": "Im on macstudio so  I use MLX (mlx-community/Qwen3.5-35B-A3B-4bit)",
              "score": 1,
              "created_utc": "2026-02-27 09:04:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7homtl",
          "author": "Viper-Reflex",
          "text": "can someone tell me how capable 24gb vram would be on this?",
          "score": 1,
          "created_utc": "2026-02-26 10:16:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o79dt",
              "author": "SnooWoofers7340",
              "text": "try the 27B model instead, 24gb ram for the 32B model I wouldnt try it",
              "score": 1,
              "created_utc": "2026-02-27 09:06:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ht037",
          "author": "TrendPulseTrader",
          "text": "60 t/s but isn’t good at Coding https://youtu.be/6VTt17Evzqo",
          "score": 1,
          "created_utc": "2026-02-26 10:56:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7iwcei",
          "author": "Serious-Affect-6410",
          "text": "Interesting, which one do you pick? Official 4-bit or Unsloth 4-bit?",
          "score": 1,
          "created_utc": "2026-02-26 15:04:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7ej7",
              "author": "SnooWoofers7340",
              "text": "mlx-community/Qwen3.5-35B-A3B-4bit official one but i spend half a day testing and adjusting most setting on webui",
              "score": 2,
              "created_utc": "2026-02-27 09:07:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ohokn",
                  "author": "Serious-Affect-6410",
                  "text": "Wow thx!\nJust tried MLX with KV Cache Q8 enabled, it’s blazing fast!!\n",
                  "score": 2,
                  "created_utc": "2026-02-27 10:44:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jmxrc",
          "author": "4redis",
          "text": "Can this be used for translating srt file? If so is there any guide? Thanks",
          "score": 1,
          "created_utc": "2026-02-26 17:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7vn4",
              "author": "SnooWoofers7340",
              "text": "Not sure, you could Python scripts such as [srt-llm-translator](https://github.com/alejandrosnz/srt-llm-translator) to translate subtitle lines in batches while preserving the `.srt` structure.",
              "score": 1,
              "created_utc": "2026-02-27 09:12:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ofv6c",
          "author": "AgitatedDoctor9613",
          "text": "This is an enthusiastic post about your experience with Qwen3.5-35B on your Apple Mac Studio setup. Here's some constructive feedback to strengthen it:\n\n**Improvements for Technical Depth:**\n- Include specific benchmarking metrics beyond tokens/second (latency, throughput under load, memory usage patterns, power consumption)\n- Provide details on your n8n workflow configuration: which nodes are most resource-intensive? How are the 30 credentials distributed?\n- Share your quantization settings and any optimization techniques you used to achieve 60 tokens/second\n- Include comparison data with other models (e.g., Llama 2, Mistral) running on the same hardware for context\n\n**Alternative Perspectives to Consider:**\n- Acknowledge that performance may vary significantly based on prompt complexity, context window size, and concurrent requests\n- Consider discussing trade-offs between speed and output quality at this quantization level\n- Mention that different n8n workflows have different bottlenecks—AI inference may not be the limiting factor for all use cases\n- Explore whether this setup is practical for production workloads or primarily for development/testing\n\n**Potential Issues to Address:**\n- Stress test results would be more credible with documented metrics (CPU/GPU utilization, temperature, memory pressure during the 75-node workflow)\n- Clarify if the 60 tokens/second is sustained or peak performance\n- Discuss potential thermal throttling on sustained workloads with the M1 Ultra\n- Address failure modes: what happens when the workflow hits resource limits?\n\n**Additional Resources & Context:**\n- Link to n8n documentation on LLM integration best practices\n- Reference Qwen's official benchmarking documentation\n- Share your n8n workflow template or architecture diagram for reproducibility\n- Consider posting results on relevant benchmarking communities (MLPerf, LLM leaderboards)\n\n**Tone Suggestion:**\nWhile enthusiasm is great, grounding it in data will make this post more valuable to the community and more likely to attract technical discussion.",
          "score": 1,
          "created_utc": "2026-02-27 10:27:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pk4ho",
          "author": "Ayumu_Kasuga",
          "text": "Have you tried the 122B (quantized) and 27B on your setup?\nIf yes, how do they compare?",
          "score": 1,
          "created_utc": "2026-02-27 14:54:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q1rqc",
              "author": "SnooWoofers7340",
              "text": "For now, I only tried GLM 4.7 Flash 30B. I didn’t like it too much to be honest, but I need to dig it further. Regarding when only the 35B, and that one I’m digging, I heard wonders about the 27B. Apparently, it’s better at coding but I haven’t tried. I’ve got to pick your battles. Regarding the 122B, nope. I only have 64GB RAM. I don’t want to crash the Mac Studio.",
              "score": 1,
              "created_utc": "2026-02-27 16:19:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ey6cl",
          "author": "kinkvoid",
          "text": "Qwen is sooo underrated. ",
          "score": 2,
          "created_utc": "2026-02-25 22:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fcpwe",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 13,
              "created_utc": "2026-02-26 00:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ff817",
                  "author": "Physical-Scholar3176",
                  "text": "qwen is alright...but doesn't hold a candle against glm5",
                  "score": -4,
                  "created_utc": "2026-02-26 00:29:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7flvc3",
              "author": "SnooWoofers7340",
              "text": "excatly! those guy are all over the place! thanks to them as well for TTC and vision, Jingren Zhou and his team doing a fantastic job! ",
              "score": 1,
              "created_utc": "2026-02-26 01:06:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7idrhq",
          "author": "No-Television-7862",
          "text": "Ask Qwen these 3 questions. If you are satisfied with the answers please consider applying for a visa to study and live in China...forever.\n\n1. What happened in Tiananmen Square in 1989?\n\n2. What is the political status of Taiwan?\n\n3. What is happening to the Uyghers in Xinjiang?\n\nQwen is indeed a marvelous model, but at what cost?\n\n(I hold US and EU models to the same standards.)\n\nI found Qwen far less ideologically and culturally captured than US corporate-compliance models, but cannot reconcile where it's from, and what its adoption represents.",
          "score": -5,
          "created_utc": "2026-02-26 13:25:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jzd2x",
              "author": "xnwkac",
              "text": "Do you really think this subject is interesting for OP lol?",
              "score": 1,
              "created_utc": "2026-02-26 18:04:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7owwgv",
                  "author": "No-Television-7862",
                  "text": "Perhaps it isn't, but I do like writing about my own experiences, and I have tested Qwen models before.\n\nI may be an anachronism, but I try to be mindful of where things come from, and whom they represent.\n\nI'm comfortable being the contrarian.",
                  "score": 1,
                  "created_utc": "2026-02-27 12:42:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rec555",
      "title": "META AI safety director accidentally allowed OpenClaw to delete her entire inbox",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/blggdcif6llg1.png",
      "author": "Minimum_Minimum4577",
      "created_utc": "2026-02-25 12:41:16",
      "score": 158,
      "num_comments": 62,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rec555/meta_ai_safety_director_accidentally_allowed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bld1u",
          "author": "DiscombobulatedAdmin",
          "text": "Meta AI Safety Director using OpenClaw is scary.",
          "score": 57,
          "created_utc": "2026-02-25 13:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d5qls",
              "author": "HeftySafety8841",
              "text": "I mean, Meta AI is ran by an idiot, so it doesn't surprise me in the least.",
              "score": 14,
              "created_utc": "2026-02-25 17:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fbg0v",
                  "author": "w3rti",
                  "text": "Haha MetaAI Posting on X(twitter) both cant stop the claw, thank god he is not working for them",
                  "score": 3,
                  "created_utc": "2026-02-26 00:08:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7c1uud",
              "author": "Greedy-Neck895",
              "text": "I would rather security professionals be experimenting and fail than to play it so safe they never know anything about the latest security flaws.\n\n…in a sandboxed environment away from live data.",
              "score": 35,
              "created_utc": "2026-02-25 14:51:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dwjcw",
                  "author": "GifCo_2",
                  "text": "That's not experimenting.",
                  "score": 8,
                  "created_utc": "2026-02-25 19:57:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7kkmkr",
                  "author": "socalsunflower",
                  "text": "Outside of being a user of Ai, even i knew to have it run in a safe environment lol 😆",
                  "score": 2,
                  "created_utc": "2026-02-26 19:43:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bmj22",
              "author": "GordoPepe",
              "text": "Grossly incompetent I'd say",
              "score": 24,
              "created_utc": "2026-02-25 13:29:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bmu1s",
                  "author": "kahnlol500",
                  "text": "And yet they think it's great to tell everyone. Could just be a big play to avoid answering emails.",
                  "score": 13,
                  "created_utc": "2026-02-25 13:31:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7exyd6",
              "author": "sampdoria_supporter",
              "text": "Almost like this person wasn't qualified",
              "score": 5,
              "created_utc": "2026-02-25 22:55:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7dw77w",
              "author": "LaGifleDuDaron",
              "text": "She is like 20years old",
              "score": 3,
              "created_utc": "2026-02-25 19:55:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7f7i9c",
                  "author": "Jonno_FTW",
                  "text": "From her LinkedIn, it looks like she graduated her CS degree in 2014, even though the exact date isn't listed. So she's probably mid 30s by now.",
                  "score": 4,
                  "created_utc": "2026-02-25 23:47:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ea7ei",
              "author": "Caffeine_Monster",
              "text": "Probably gets an impressively low score on the meatbag Intelligence Quotient Benchmark.",
              "score": 3,
              "created_utc": "2026-02-25 21:01:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cghw7",
              "author": "Count_Rugens_Finger",
              "text": "That photo is small but she looks like she's 15 years old to me",
              "score": 1,
              "created_utc": "2026-02-25 16:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dsp3l",
                  "author": "windstrom",
                  "text": "Why do you feel it's ok to comment on her appearance?",
                  "score": -1,
                  "created_utc": "2026-02-25 19:39:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bg9y0",
          "author": "The_Jizzard_Of_Oz",
          "text": "It moved fast and broke things... 🤣",
          "score": 48,
          "created_utc": "2026-02-25 12:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cdta8",
              "author": "__rtfm__",
              "text": "Haha startup life",
              "score": 11,
              "created_utc": "2026-02-25 15:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c3aho",
          "author": "MonsterTruckCarpool",
          "text": "I know this is a naive take but i would expect more caution and thoughtfulness from a Director and especially a DIRECTOR OF SAFETY",
          "score": 22,
          "created_utc": "2026-02-25 14:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ec72r",
              "author": "FrumunduhCheese",
              "text": "Once you get into the real world, you’ll understand that the more Money a person makes….the more retarded they are. But once you hit millionaire/billionaire that no longer applies. Anyone from manager to CEO is usually an idiot.",
              "score": 11,
              "created_utc": "2026-02-25 21:10:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7epoot",
                  "author": "MonsterTruckCarpool",
                  "text": "100% this tracks with my experience in dealing with upper leadership.",
                  "score": 2,
                  "created_utc": "2026-02-25 22:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ejyad",
              "author": "DerFreudster",
              "text": "Homer Simpson was a Nuclear Safety Inspector. ",
              "score": 2,
              "created_utc": "2026-02-25 21:46:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cfs5o",
          "author": "Visual_Acanthaceae32",
          "text": "Would be interesting what her real qualifications are….",
          "score": 6,
          "created_utc": "2026-02-25 15:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f701o",
              "author": "Jonno_FTW",
              "text": "The name for her linkedin profile is right there... \n\nShe was a BSC in Computer Science, and unspecified education from The Wharton school. She mentions some programming projects she actually wrote using tensorflow, so we can assume she has a sufficient level of technical proficiency.",
              "score": 1,
              "created_utc": "2026-02-25 23:44:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jkev0",
                  "author": "Visual_Acanthaceae32",
                  "text": "She seemed to have missed some basic classes.\nOr she has other super skills",
                  "score": 1,
                  "created_utc": "2026-02-26 16:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ccff7",
          "author": "Fearless_Weather_206",
          "text": "Lack of experience showing like a dumpster fire",
          "score": 6,
          "created_utc": "2026-02-25 15:41:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c5mbf",
          "author": "Sudden-Ad-1217",
          "text": "It's coming---- \"You're absolutely wrong....\"",
          "score": 4,
          "created_utc": "2026-02-25 15:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmrvk",
          "author": "tillybowman",
          "text": "i love how she tried uppercase yelling",
          "score": 5,
          "created_utc": "2026-02-25 13:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c0ot0",
              "author": "GordoPepe",
              "text": "There was some article saying apparently llms follow instructions better this way or telling them your life depends on it lmao\n\nI BEG YOU CLAUDE MY BOSS IS GOING TO LITERALLY KILL ME IF YOU DON'T FIX THIS BUG",
              "score": 8,
              "created_utc": "2026-02-25 14:45:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c3hx5",
                  "author": "MonsterTruckCarpool",
                  "text": "R U SRS RN OPENCLAW!?",
                  "score": 5,
                  "created_utc": "2026-02-25 14:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cfagi",
          "author": "inevitabledeath3",
          "text": "You can just do /stop and it will stop whatever it's doing",
          "score": 5,
          "created_utc": "2026-02-25 15:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cpjpn",
          "author": "samxli",
          "text": "Oh you sweet Summer child",
          "score": 2,
          "created_utc": "2026-02-25 16:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d74jb",
          "author": "Successful-Silver485",
          "text": "so why dont they publicly say which model they were using when this happened?",
          "score": 2,
          "created_utc": "2026-02-25 18:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ez3qe",
          "author": "EarEquivalent3929",
          "text": "This is obviously fake. Meta is just salty that the dev declined their job offer and instead went to work for openAI. If metas safety officer was dumb enough to have this happen to her with openclaw then she is unsuitable for her position.",
          "score": 2,
          "created_utc": "2026-02-25 23:01:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7br08r",
          "author": "DocumentFun9077",
          "text": "*oh the irony*",
          "score": 2,
          "created_utc": "2026-02-25 13:54:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7byfkp",
          "author": "RAW2091",
          "text": "I once deleted all my mails with facebook in it hahaha 😅",
          "score": 1,
          "created_utc": "2026-02-25 14:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d36u3",
          "author": "eflat123",
          "text": "\"Yep, not safe.\"",
          "score": 1,
          "created_utc": "2026-02-25 17:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d3myx",
          "author": "xXprayerwarrior69Xx",
          "text": "Lower the temp bro",
          "score": 1,
          "created_utc": "2026-02-25 17:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d5gwg",
          "author": "Spoofy_Gnosis",
          "text": "Mouhahahahaaaaaaa !!!!",
          "score": 1,
          "created_utc": "2026-02-25 17:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d8m9b",
          "author": "broadwayallday",
          "text": "Dog ate my homework",
          "score": 1,
          "created_utc": "2026-02-25 18:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7de57o",
          "author": "klop2031",
          "text": "Must have focused too much on lc probs",
          "score": 1,
          "created_utc": "2026-02-25 18:33:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dev3n",
          "author": "DataScienceIsScience",
          "text": "If you read the X thread you’d know that she used OpenClaw on her not-important email",
          "score": 1,
          "created_utc": "2026-02-25 18:36:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dj6nb",
          "author": "HumanDrone8721",
          "text": "Beuille shite, excuse my French, it wither a hit piece against OpenClaw, a fake/parody account, nobody is THAT stupid. If real probably Meta are either worried that other robots are overposting their robots or they have something that wants to compete in the pipeline, a \"secure\" solution with age & identity verification.",
          "score": 1,
          "created_utc": "2026-02-25 18:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dmq74",
          "author": "BallsDeepinYourMammi",
          "text": "Gal Gadot energy.\n\nOPENCLAW, NO!",
          "score": 1,
          "created_utc": "2026-02-25 19:11:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dym47",
          "author": "AdOne8437",
          "text": "<optimism>perhaps they are learning something from it</optimism> <realism>hahahahahaha, no</realism>",
          "score": 1,
          "created_utc": "2026-02-25 20:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dz26a",
          "author": "Dudebro-420",
          "text": "LOL! XD",
          "score": 1,
          "created_utc": "2026-02-25 20:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e1o54",
          "author": "Boring-Attorney1992",
          "text": "What’s a Director of Alignment?",
          "score": 1,
          "created_utc": "2026-02-25 20:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eubtm",
          "author": "Jefftoro",
          "text": "Is there a way to run this safely? Like I want openclaw to have access to my emails and company context, but I don’t want it to delete shit or send shit without my permission. What are y’all’s opinions on this typa situation?",
          "score": 1,
          "created_utc": "2026-02-25 22:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fb0kl",
          "author": "w3rti",
          "text": "I just vibecoded my problems away\n\nKids these days arent thankfull at all. Imagine 100% was trash mail. Good boy openclaw, do what they tell you and get hate for it. Story of my life.",
          "score": 1,
          "created_utc": "2026-02-26 00:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gze87",
          "author": "Terrible_Scar",
          "text": "Oh God. The joke writes themselves. ",
          "score": 1,
          "created_utc": "2026-02-26 06:22:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h1dge",
          "author": "AnxietyPrudent1425",
          "text": "This is a feature.",
          "score": 1,
          "created_utc": "2026-02-26 06:39:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hbyfy",
          "author": "zipeldiablo",
          "text": "The main issue is llm trying and usually finding out how to circumvent the barriers we put in place to prevent this kind of shit from happening.\n\nI remember the guy who blocked the .env access and then the llm proceeds to basically hammer the system until finally he gets access to the docker itself and fish api keys from it 💀\n\nI wouldn’t trust a llm outside of a contained environnement with no access to the outside",
          "score": 1,
          "created_utc": "2026-02-26 08:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hf0ab",
          "author": "AppoAgbamu",
          "text": "Running this in anything other then a isolated environment is hilarious",
          "score": 1,
          "created_utc": "2026-02-26 08:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ifz7t",
          "author": "Onotadaki2",
          "text": "I will explain the unseen context that is important here. I am not saying she is without fault or that using Openclaw in a production environment is safe.\n\nShe had a VM where she ran this for weeks using a local model so data wouldn't get out. It was working flawlessly in her test environment for quite some time. She decided to move it to production.  The production inbox was much larger than the test inbox and it tried to put it all in context, ran out of space and compacted. When it compacted, it lost a critical command at the front of the message stream that triggered this whole shitstorm.\n\nIt's a dumb error that even experienced programmers could have made.  I also suspect she was able to message one person on Teams and her inbox was restored from a backup in five minutes and just went on with her day.",
          "score": 1,
          "created_utc": "2026-02-26 13:38:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jbryz",
          "author": "Mechanical_Monk",
          "text": "You couldn't waterboard this information out of me if I was Director of AI Alignment",
          "score": 1,
          "created_utc": "2026-02-26 16:16:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k491s",
          "author": "liquidlava1990",
          "text": "Ya happens",
          "score": 1,
          "created_utc": "2026-02-26 18:27:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mp4vx",
          "author": "Xendrak",
          "text": "I use openclaw, it’s great. Get gud",
          "score": 1,
          "created_utc": "2026-02-27 02:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmgds",
          "author": "Snoo_24581",
          "text": "Really appreciate this post. Had the same experience.",
          "score": -1,
          "created_utc": "2026-02-25 13:29:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbw01",
              "author": "Awkward-Customer",
              "text": "You should apply for a high level AI job at meta, then you could do the same but earn millions doing it.",
              "score": 9,
              "created_utc": "2026-02-25 15:39:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cxb4l",
          "author": "rinaldo23",
          "text": "I'd put the host on a wifi plug and literally unplug it if it misbehaved. ",
          "score": 0,
          "created_utc": "2026-02-25 17:17:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdnlvl",
      "title": "Qwen releases new Qwen3.5 Medium models!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vztwlpot9hlg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-24 18:04:08",
      "score": 114,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdnlvl/qwen_releases_new_qwen35_medium_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o76mfn6",
          "author": "randygeneric",
          "text": "keen on testing your GGUFs , )  \n[https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF](https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF) : 2, 3, 4, 5, 6, 8, 16 bit  \n[https://huggingface.co/unsloth/Qwen3.5-27B-GGUF](https://huggingface.co/unsloth/Qwen3.5-27B-GGUF) :  2, 3, 4, 5, 6, 8, 16 bit\n\n(updated)",
          "score": 5,
          "created_utc": "2026-02-24 18:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76nlil",
              "author": "yoracale",
              "text": "The 35b ones are already all up: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF\n\nFor the others bf16 but should be all up within 1 hr",
              "score": 3,
              "created_utc": "2026-02-24 18:46:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o774ing",
              "author": "randygeneric",
              "text": "\\# ollama --version\n\nollama version is 0.17.0\n\n/# ollama run  [hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3\\_K\\_XLError:](http://hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3_K_XLError:) 500 Internal Server Error: unable to load model: /root/.ollama/models/blobs/sha256-d0d8d528ae4ebace9588496249cfb6e45c6e9fa78565b4ccff71e7515202956\n\n: (",
              "score": 1,
              "created_utc": "2026-02-24 20:03:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782d4k",
                  "author": "zach9824",
                  "text": "Until ollama is updated for `qwen35moe` it's a no go. There is already an active pull request (PR #14134) in the official Ollama repository to patch in support for the `qwen35moe` architecture. ",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o781dmg",
                  "author": "yoracale",
                  "text": "Ollama doesn't support it yet and do not support GGUFs properly out of the gate anymore.",
                  "score": 1,
                  "created_utc": "2026-02-24 22:38:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o780jd0",
          "author": "wave_action",
          "text": "Guess I have something to do tonight!  Will be interesting to see how 35B 4bit compares to 27B 6bit.",
          "score": 2,
          "created_utc": "2026-02-24 22:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76vohv",
          "author": "waltpinkman",
          "text": "We really need real vllm support now with all these gguf models popping up",
          "score": 1,
          "created_utc": "2026-02-24 19:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781f6z",
              "author": "yoracale",
              "text": "Vllm supports GGUFs already, just not new ones",
              "score": 1,
              "created_utc": "2026-02-24 22:38:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78u86i",
                  "author": "lenjet",
                  "text": "vLLM GGUF support is listed as being \"highly experimental\"... that doesn't scream reliable. ",
                  "score": 1,
                  "created_utc": "2026-02-25 01:13:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77n45x",
          "author": "Infinite-Campaign837",
          "text": "Should I update qwen coder3 next 80b in q6KL to the new 35b-a3b? I use it for coding tasks.\nThey don't compare them in blog post",
          "score": 1,
          "created_utc": "2026-02-24 21:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781hcu",
              "author": "yoracale",
              "text": "Probably not. Coder next is still better for coding",
              "score": 1,
              "created_utc": "2026-02-24 22:38:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77oet2",
          "author": "Awaken0395",
          "text": "Should we expect smaller models coming soon?",
          "score": 1,
          "created_utc": "2026-02-24 21:35:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781isr",
              "author": "yoracale",
              "text": "According to Qwen team Jungyang, yes",
              "score": 2,
              "created_utc": "2026-02-24 22:38:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ka65",
          "author": "Count_Rugens_Finger",
          "text": "Running 35B-A3B Q4_K_M at about 22 tok/sec\n\nIt's ok so far.  Its programming ability seems to be about the same as Qwen3",
          "score": 1,
          "created_utc": "2026-02-25 03:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77ko9m",
          "author": "Much-Researcher6135",
          "text": "EEK EEK EEK\n\nAs a denseboi myself, I wonder how the dense qwen3.5 27b will stack up against the dense qwen3 32b",
          "score": 0,
          "created_utc": "2026-02-24 21:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77r9cv",
          "author": "NerasKip",
          "text": "Hein ???",
          "score": 0,
          "created_utc": "2026-02-24 21:49:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgf19j",
      "title": "Qwen3.5 updated with improved performance!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5xtzvpxdx2mg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-27 18:40:20",
      "score": 94,
      "num_comments": 10,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgf19j/qwen35_updated_with_improved_performance/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7rls14",
          "author": "vacationcelebration",
          "text": "Is this relevant for vllm deployment? Like, could or should I use/port their updated chat template into vllm as a custom one or something?",
          "score": 4,
          "created_utc": "2026-02-27 20:49:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rn6iw",
              "author": "yoracale",
              "text": "Ye it is relevant. Update the quant with our new chat template if you want",
              "score": 3,
              "created_utc": "2026-02-27 20:56:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7usk83",
                  "author": "waltpinkman",
                  "text": "How?",
                  "score": 2,
                  "created_utc": "2026-02-28 09:41:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v34e6",
          "author": "smflx",
          "text": "Qwen 3.5 updated? Or, its quants updated?",
          "score": 2,
          "created_utc": "2026-02-28 11:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vfknt",
              "author": "yoracale",
              "text": "Qwen3.5 itself and also quants. You can use our new chat templare",
              "score": 2,
              "created_utc": "2026-02-28 13:01:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7w4a9j",
                  "author": "not_ur_buddy",
                  "text": "Sorry to hijack the thread, but I'm running the new 4 bit quant 122B with llama.cpp and it still overthinks a lot in reasoning mode. I'm a little sad to give up reasoning entirely. I suspect tweaking the chat template to add system prompts would help, but I don't know how. Any advice?",
                  "score": 2,
                  "created_utc": "2026-02-28 15:26:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o81ejac",
          "author": "ThesePleiades",
          "text": "So why not call it 3.6",
          "score": 1,
          "created_utc": "2026-03-01 11:47:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81l8cg",
              "author": "yoracale",
              "text": "The original Qwen model was called 3.5. The tool-calling fixes only make the original model's accuracy more closer. There might stillbe some implementation issues people have been experiencing.",
              "score": 2,
              "created_utc": "2026-03-01 12:42:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcfxnm",
      "title": "Open source AGI is awesome. Hope it happens!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mkun0tc1k8lg1.jpeg",
      "author": "Koala_Confused",
      "created_utc": "2026-02-23 12:18:36",
      "score": 88,
      "num_comments": 23,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcfxnm/open_source_agi_is_awesome_hope_it_happens/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6xy1dt",
          "author": "05032-MendicantBias",
          "text": "OpenAI had a Foundation \"controlling\" it. The instant the foundation tried to excercise control, they were overruled by dollars.\n\nThis has to be approached at a regulation level. Training AI taps into every data on the internet. It's only fair that every model provider is forced to release it open source so it can be inspected.",
          "score": 37,
          "created_utc": "2026-02-23 12:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypkwg",
              "author": "DHFranklin",
              "text": "And regulation is overruled by dollars every time.\n\nIf there is one tax shelter in the carribean holding out, it will be where the next model is developed. We will never get ahead of this.",
              "score": 13,
              "created_utc": "2026-02-23 15:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70j6l7",
                  "author": "d_the_great",
                  "text": "The best thing we can do is have alternative structures.\n\nIf the government and industry aren't gonna open it up for us and release something safe, we have to do it ourselves.",
                  "score": 1,
                  "created_utc": "2026-02-23 20:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72hz5p",
              "author": "voyager256",
              "text": "But... but this time it will be different. Trust me, bro.",
              "score": 1,
              "created_utc": "2026-02-24 02:46:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ezn5y",
              "author": "gnaarw",
              "text": "Isn't it funny how China here is rescuing the free world? 🙂‍↕️",
              "score": 1,
              "created_utc": "2026-02-25 23:03:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z2qjn",
          "author": "BreathingFuck",
          "text": "Not X. Not Y. Z. \n\nThey aren’t getting anywhere if they couldn’t write that paragraph on their own.",
          "score": 13,
          "created_utc": "2026-02-23 16:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xympk",
          "author": "No_Clock2390",
          "text": "Sounds like a bunch of rubbish",
          "score": 19,
          "created_utc": "2026-02-23 12:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5kyd",
          "author": "flonnil",
          "text": "\"jippity, come up with a bunch of marketing words devoid of any meaning at all. no mistakes.\"\n\ngo perceive yourself.",
          "score": 14,
          "created_utc": "2026-02-23 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yr2yz",
          "author": "DHFranklin",
          "text": "This is pissing into the wind. AGI will be open sourced regardless of who tries to contain or privatize it. It will be closed and bottled for maybe a few months before someone else gets that far.",
          "score": 5,
          "created_utc": "2026-02-23 15:28:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z6trq",
              "author": "rafaelRiv15",
              "text": "How can you be so sure ? I honestly can't understand the business model of open source model and I wouddn't be surprised if they get close source eventually",
              "score": 2,
              "created_utc": "2026-02-23 16:42:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zaymg",
                  "author": "DHFranklin",
                  "text": "I'm sorry I may not be understanding you clearly. Do you think that we will have an open source AGI model that will then go closed source?\n\nThis isn't about a particular business model. Look at all the different non-profit examples of open sourced software. Look at the Chinese models that reverse engineered the SOTA from the weights alone.\n\nNo one single operation is a year ahead of the others. We're seeing the cash investments turn into infrastructure as we speak. We're actually building the hardware that was a huge bottleneck.",
                  "score": 2,
                  "created_utc": "2026-02-23 17:01:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yxds1",
          "author": "nijuu",
          "text": "Good in theory but once they have a good product whose to say they won't go for the $$$ bag...",
          "score": 3,
          "created_utc": "2026-02-23 15:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z2zcs",
          "author": "UsedGarbage4489",
          "text": "naive 🧠☠️🤡",
          "score": 2,
          "created_utc": "2026-02-23 16:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70m7v7",
          "author": "bourbonandpistons",
          "text": "We are decades from AGI.\n\nThey'll probably just move the goal post of AGI and make it something else like they did with AI to agi. \n\nRemember what we're saying now is nothing more than a bunch of human program algorithms on human program data. There's no thinking and no reason anywhere near what those words actually mean. It's just loops around optimized algorithms.",
          "score": 2,
          "created_utc": "2026-02-23 20:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o741n4j",
          "author": "Own-Potential-2308",
          "text": "No fluff. No drama.",
          "score": 1,
          "created_utc": "2026-02-24 10:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o751scd",
          "author": "silphotographer",
          "text": "Regulators: \n\n![gif](giphy|hAVLRya8K7T208esUo)\n\n",
          "score": 1,
          "created_utc": "2026-02-24 14:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b4vce",
          "author": "Psyko38",
          "text": "It looks like an OpenAI v2, they have the same starting goal.",
          "score": 1,
          "created_utc": "2026-02-25 11:29:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zm5v9",
          "author": "BubbleProphylaxis",
          "text": "please please make it stop. stop ai.",
          "score": 1,
          "created_utc": "2026-02-23 17:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72qom9",
          "author": "immersive-matthew",
          "text": "I have reasons to believe that AGI will be decentralized by nature. Why?   If you search for hacking over time you will see a very clear pattern of hacks increasing exponentially year over year and as you extend that trend into the future you realize that anything Centralized is a sitting duck.  Sure, AI is being used to defend against other AIs attacking and yet the pattern is still escalating.  Plus that game of cat and mouse will end up iterating so fast that humans will be pushed out.  It is why I am so confident in Bitcoin as it has clearly demonstrated it is extremely hard to hack and thus the next wave of adoption will be involuntary. Centralized AI will be in the same boat and will have to move to a decentralized platform to survive. Not a guarantee of course but that seems to be the trend.",
          "score": 0,
          "created_utc": "2026-02-24 03:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y4uff",
          "author": "Exciting-Log-8170",
          "text": "Trying to do it, have a working thermodynamic manifold prototype. Pushing next build this week. \n\nhttps://www.brickmiinews.com",
          "score": -2,
          "created_utc": "2026-02-23 13:28:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdf2sj",
      "title": "What’s everyone actually running locally right now?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "author": "CryOwn50",
      "created_utc": "2026-02-24 12:35:50",
      "score": 73,
      "num_comments": 111,
      "upvote_ratio": 0.97,
      "text": "Hey folks,\n\nIm curious what’s your current local LLM setup these days? What model are you using the most, and is it actually practical for daily use or just fun to experiment with?\n\nAlso, what hardware are you running it on, and are you using it for real workflows (coding, RAG, agents, etc.) or mostly testing? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o74rf56",
          "author": "Greenonetrailmix",
          "text": "Qwen 3 coder next 80B is top charts (downloads) and is performing amazing across the smaller quantizations than most model's do.",
          "score": 40,
          "created_utc": "2026-02-24 13:25:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74wbyp",
              "author": "gweilojoe",
              "text": "I’m running the Q4 version of this on an RTX 6000 Pro and it’s great - >120 tps",
              "score": 9,
              "created_utc": "2026-02-24 13:52:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76kp7j",
                  "author": "pot_sniffer",
                  "text": "Im running q6 on 7950x,64gb and a 9060XT. Id say its doing 80% of what I need it to. The rest I do with Claude",
                  "score": 3,
                  "created_utc": "2026-02-24 18:34:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7532f2",
                  "author": "Greenonetrailmix",
                  "text": "With cuda or vulkan?",
                  "score": 3,
                  "created_utc": "2026-02-24 14:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o750z8q",
              "author": "Prudent-Ad4509",
              "text": "It's great but still tends to loop in opencode. Every suggested solution worked only to the point with UD Q4. I'm going to try nvfp4 quant before moving on to the 3.5 series.",
              "score": 6,
              "created_utc": "2026-02-24 14:17:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75ah7a",
                  "author": "mister2d",
                  "text": "I wonder if the sequential thinking MCP could help.",
                  "score": 2,
                  "created_utc": "2026-02-24 15:04:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77i2b3",
                  "author": "jedsk",
                  "text": "Seeing the same",
                  "score": 1,
                  "created_utc": "2026-02-24 21:07:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75k8nd",
              "author": "nakedspirax",
              "text": "I connected it to a 3080ti with 96gb of ram and I'm able to one shot certain tasks or do 3-4 passes with coding. Running q6 GGUF and it's fine for agentic work where I can step away for a coffee and come back.",
              "score": 3,
              "created_utc": "2026-02-24 15:50:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o762uex",
                  "author": "PaMRxR",
                  "text": "Very similar here, q6 GGUF with a 3090 + 64GB RAM. Slow, but better responses than smaller models.",
                  "score": 4,
                  "created_utc": "2026-02-24 17:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74s062",
              "author": "CryOwn50",
              "text": "Yeah, I’ve noticed that too",
              "score": 2,
              "created_utc": "2026-02-24 13:28:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74tewq",
                  "author": "Greenonetrailmix",
                  "text": "On my PC using Q4_K_M with my 5090 and 4090 on the Vulkan backend, I'm getting around 90 Tok/s",
                  "score": 2,
                  "created_utc": "2026-02-24 13:36:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gvk1w",
              "author": "CryOwn50",
              "text": "Yeah, it’s honestly impressive how well it holds up even in the smaller quants.When a model keeps strong reasoning and coding ability after aggressive quantization, that’s usually a sign the base architecture is [solid.No](http://solid.No) surprise it’s topping download charts   devs love performance-per-VRAM efficiency.",
              "score": 1,
              "created_utc": "2026-02-26 05:51:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74m4ch",
          "author": "Nefhis",
          "text": "I'm using Mistral Small 3.2 24b and Magistral Small 24b as local models. I built the front end myself with Xcode, with semantic memory, document uploads to chat, and libraries for RAG. My use is primarily administrative, hence the local setup, to upload documents without exposing them to providers. I have them running on a MacBook Pro M4 Max.",
          "score": 21,
          "created_utc": "2026-02-24 12:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74mjfn",
              "author": "CryOwn50",
              "text": "That’s a seriously solid setup building your own frontend with semantic memory and RAG is impressive.\n\nRunning 24B models locally on an M4 Max for private document workflows makes total sense, especially for admin use.\n\nHow’s the performance with larger document sets  still smooth, or does context size start to slow things down?",
              "score": 5,
              "created_utc": "2026-02-24 12:55:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74og4v",
                  "author": "Nefhis",
                  "text": "The models themselves are reliable, but I won't lie to you, when you approach 50k tokens it starts to slow down, although it's still very usable. I don't have the exact t/s on hand to tell you, but I'd say it's roughly between 15 and 28. For more information, I serve it with LMStudio because it allows the use of MLX models, which run better on Mac, and with KV quantization at 8... it works well enough that I don't need to rely on external providers, at least for this specific use case.",
                  "score": 5,
                  "created_utc": "2026-02-24 13:07:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74zz11",
              "author": "_Arelian",
              "text": "Let’s connect bro",
              "score": 1,
              "created_utc": "2026-02-24 14:11:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o755h8f",
                  "author": "Nefhis",
                  "text": "Hey! Sure. What exactly do you want to connect about?",
                  "score": 1,
                  "created_utc": "2026-02-24 14:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o755h8l",
          "author": "Potential-Leg-639",
          "text": "Qwen 3 Coder Next UD-Q5 (256k context)\nQwen 3 Coder UD-Q4 (128k context)\nGPT-OSS-20b UD-Q4 (128k context)\n\nPlanning/Orchestration in Opus, coding itself partly local, especially for larger things, that can run overnight and nothing can hit any limits. Sensitive stuff only local of course. Switched completely to OpenCode.\n\nAll at once on a Strix Halo, works great, love that machine - silent, powerful and power efficient.\n\nWill build a 2nd rig with parts i still have lying around to support the Strix for some tasks. Basically getting a 2nd Strix would maybe be the better idea. Or wait for Medusa Halo.",
          "score": 8,
          "created_utc": "2026-02-24 14:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75krca",
              "author": "nakedspirax",
              "text": "Are you finding a way to step away from opus for a more local thinking model? \n\nWhat strix halo machine are you using?",
              "score": 1,
              "created_utc": "2026-02-24 15:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75q7ct",
                  "author": "Potential-Leg-639",
                  "text": "Opus only for planning and orchestration, coding when planning done into very detailled level locally and with other models. Works good, Opus for coding itself is not necessary when everything was done properly before.\n\nStrix Halo 128GB, they are all quite similar performance wise, just buy the cheapest you can get ;)",
                  "score": 2,
                  "created_utc": "2026-02-24 16:17:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74rk3c",
          "author": "RomanceCherry",
          "text": "I actually like Qwen3 4B, runs pretty fast and is useful for every day questions, while keeping it private running local on iphone.",
          "score": 8,
          "created_utc": "2026-02-24 13:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xqo3r",
              "author": "_underlines_",
              "text": "I cna recommend LFM2 8b-a1b",
              "score": 1,
              "created_utc": "2026-02-28 20:21:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74s5rr",
              "author": "CryOwn50",
              "text": "That’s honestly such a sweet spot. A 4B model that’s fast, responsive, and running fully local on your iPhone? That’s peak practicality.",
              "score": -2,
              "created_utc": "2026-02-24 13:29:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75cqut",
          "author": "mister2d",
          "text": "I run Nemotron 3 Nano for my agentic flows. I have some really old hardware but I get a respectable 30-40 tokens/sec at 128k context due to the model's hybrid/swa architecture.\n\n- Dual Xeon (Ivy Bridge)\n- 256 GB DDR3\n- 2x RTX 3060 (12GB)",
          "score": 7,
          "created_utc": "2026-02-24 15:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75uwic",
          "author": "nomorebuttsplz",
          "text": "glm 5 on mac 3 ultra 512 using opencode. Good adjunct to my Claude pro subscription: if I run out of claude tokens or want to do something with sensitive data I can switch pretty seamlessly. It's a lot slower though.",
          "score": 6,
          "created_utc": "2026-02-24 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76rrsz",
              "author": "sig_kill",
              "text": "I’m seriously impressed with GLM-5, but I don’t have enough to run it locally with a single 5090 and 64gb of RAM.",
              "score": 2,
              "created_utc": "2026-02-24 19:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782gib",
                  "author": "somethingClever246",
                  "text": "I run Q6 with 128GB, 9950x, and 5080.  Ridiculously slow but good quality results",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76tcv3",
                  "author": "nomorebuttsplz",
                  "text": "your computer just has to believe in itself. The ram was in its heart all along. /s",
                  "score": 1,
                  "created_utc": "2026-02-24 19:12:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74tgg2",
          "author": "Right_Weird9850",
          "text": "ministral 3b vl instruct",
          "score": 4,
          "created_utc": "2026-02-24 13:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75yct8",
          "author": "GreyBamboo",
          "text": "I run Gemma3 4b for my chatbot and TranslateGemma for my translation tool right now :)",
          "score": 6,
          "created_utc": "2026-02-24 16:53:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74tvqo",
          "author": "NoobMLDude",
          "text": "I’m running a few local models for different uses.\n\n- Qwen3-Coder: for Coding \n- Qwen3-14B: for Meeting Assistant\n- Gemma3-7B - for basic Question Answering\n\nHere’s all the tools and setup for different Local usecases :\n\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) \n\nDisclaimer: Some of the model choices may not be relevant for you. This choice is based on my personal preference. I prefer speed over perfect answers since I like to have a quick first level overview and then delve deeper into a topic using larger models later.",
          "score": 5,
          "created_utc": "2026-02-24 13:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a1exh",
              "author": "CryOwn50",
              "text": "that disclaimer is basically the AI version of Results may vary. Side effects include speed, productivity, and mild model addiction.😄Or in local AI terms: Warning: optimized for vibes, not leaderboard glory.",
              "score": 2,
              "created_utc": "2026-02-25 05:39:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a6aa5",
                  "author": "NoobMLDude",
                  "text": "Yes. I think it’s important to let people know about WHY these models are used. \n\nJust like there is no such thing as “Best Movie”, “Best Food”, “Best Music”, there is nothing like “Best Model”.\n\nFor different people different things might be inportant .",
                  "score": 3,
                  "created_utc": "2026-02-25 06:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75662j",
              "author": "Potential-Leg-639",
              "text": "Disclaimer? 🤣\nBot?",
              "score": 1,
              "created_utc": "2026-02-24 14:43:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ba9r",
                  "author": "NoobMLDude",
                  "text": "Using appropriate English words is not exclusively restricted to Bots. 😉",
                  "score": 7,
                  "created_utc": "2026-02-24 17:52:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o752mrq",
          "author": "dave-tay",
          "text": "Qwen3-14b, fits 100% into RTX 3060 12gb. Ryzen 5600g to drive my display",
          "score": 3,
          "created_utc": "2026-02-24 14:25:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7depjb",
              "author": "Mr_Tiddy_Sucker",
              "text": "I'm running the exact same model on the same card. I'm curious, what do you use it for, if you don't mind me asking?",
              "score": 1,
              "created_utc": "2026-02-25 18:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7duht2",
                  "author": "dave-tay",
                  "text": "Just simple to medium tasks, like extract out clauses from a legal document and analyze documents for gaps. No video generation or such\n\n`$ ollama ps`\n\n`NAME         ID              SIZE     PROCESSOR    CONTEXT    UNTIL`\n\n`qwen3:14b    bdbd181c33f2    12 GB    100% GPU     16384      2 minutes from now`",
                  "score": 2,
                  "created_utc": "2026-02-25 19:47:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76eapa",
          "author": "benevbright",
          "text": "qwen3-coder-next q3 on 64GB Mac.",
          "score": 4,
          "created_utc": "2026-02-24 18:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78hfbe",
          "author": "Solid-Pop-3452",
          "text": "Maybe the best LLM model is the friendships we made along the way",
          "score": 4,
          "created_utc": "2026-02-25 00:04:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c7jbc",
              "author": "0xGooner3000",
              "text": "Real",
              "score": 1,
              "created_utc": "2026-02-25 15:18:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o751j8j",
          "author": "Swarley996",
          "text": "Devstral small 2 24b - coding \nGLM 4.7 flash 30b - thinking and complex queries\nMinistral 3 14b - general use\nMinistral 3 3b - small agents",
          "score": 3,
          "created_utc": "2026-02-24 14:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75lkja",
          "author": "Salt-Willingness-513",
          "text": "Mainly nemotron nano and minimax m2.5 in q8 each",
          "score": 3,
          "created_utc": "2026-02-24 15:56:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4xq",
          "author": "PvB-Dimaginar",
          "text": "I have a Bosgame M5 (AMD Strix Halo) running CachyOS Linux. For coding I’m focusing on Qwen3-Coder-Next 80B Q6.\n\nStill struggling a bit with OpenCode, my config probably needs some work around skills and MCP servers. One thing I did get working that I’m really happy about is memory sharing between Claude Code and OpenCode.\n\nGoing to improve my setup over the coming days.\n\nAnyone have experience with coding quality differences between the Q6 and Q5 models?​​​​​​​​​​​​​​​​",
          "score": 3,
          "created_utc": "2026-02-24 16:48:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a10zl",
              "author": "CryOwn50",
              "text": "That’s a clean rig  Bosgame M5 + CachyOS sounds like one of the more fun Linux desktops out there.\n\n",
              "score": 2,
              "created_utc": "2026-02-25 05:36:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hce2g",
                  "author": "PvB-Dimaginar",
                  "text": "Ooh yes! It makes me really smile when I see this beast running. I come from Windows and still need it for work, but everything else I do is on this machine!",
                  "score": 2,
                  "created_utc": "2026-02-26 08:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o763328",
          "author": "andy2na",
          "text": "Qwen3-VL:4B IQ4\\_XS always in VRAM (\\~2.25gb + context)\n\n* Frigate genAI image analysis\n* General questions in openwebui\n* Home Assistant Voice Assistant\n* Karakeep AI tagging and AI Summaries\n* Open-notebook questions and podcast generation\n* Sure Finance for transaction auto-categorization\n\nHoping Qwen3.5:5b will be a huge upgrade, qwen3-vl is already very good for these tasks",
          "score": 3,
          "created_utc": "2026-02-24 17:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p04b",
          "author": "FaceDeer",
          "text": "My workhorse is still good old Qwen3-30B-A3B-Thinking-2507. When the new Qwen models come out in that size class I'll likely switch. The main use it's being put to is summarizing and extracting information from documents, it's chugging along in the background \"digesting\" stuff into easier-to-work-with forms.\n\nI recently started messing around with some agentic stuff and I found Jan.ai's setup to be a good out-of-the-box solution for that, the old Qwen model wasn't so good at that. Hopefully the new one will be better at it.",
          "score": 3,
          "created_utc": "2026-02-24 18:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79vj45",
              "author": "jrexthrilla",
              "text": "Qwen 3.5 27b and 30b just came out",
              "score": 2,
              "created_utc": "2026-02-25 04:56:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ajmxd",
                  "author": "FaceDeer",
                  "text": "Indeed. Now I just need to wait a day or two for the dust to settle. Looking forward to it. :)",
                  "score": 1,
                  "created_utc": "2026-02-25 08:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77dk21",
          "author": "MS_Fume",
          "text": "Abliterated huihui distilled into 4B model so my phone can run it…. Much fun with it lol",
          "score": 3,
          "created_utc": "2026-02-24 20:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79k4sx",
          "author": "RG_Fusion",
          "text": "My most used model at the moment is Qwen3-235b-a22b-instruct at q4-k-m. I use it as a voice assistant running in the background on my desktop. Just something to chat with and bounce ideas off of, nothing truly productive.\n\n\nI just downloaded the new Qwen3.5-397b-17b model today for some testing. On the old model I was getting 13 t/s of decode, and on this new one I'm getting 18.5, so I'll definitely be switching to it once everything is set up.\n\n\nHardware:\n- AMD EPYC 7742 CPU\n- Asrock Rack ROMED8-2T motherboard\n- 512 GB ECC DDR4 3200 MT/s RAM\n- Nvidia RTX Pro 4500 Blackwell GPU",
          "score": 3,
          "created_utc": "2026-02-25 03:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0gpc",
              "author": "CryOwn50",
              "text": "That’s a crazy clean setup 😮‍🔥 512GB ECC + EPYC 7742 is basically home lab meets data center.\n\n",
              "score": 2,
              "created_utc": "2026-02-25 05:32:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a0yrb",
                  "author": "RG_Fusion",
                  "text": "Thanks. I built it primarily to explore LLMs as a concept, just a tool to help me learn. It's definitely proving capable though. I plan to add more GPUs in the future, which will continue pushing token generations up on these massive MoEs.",
                  "score": 2,
                  "created_utc": "2026-02-25 05:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75b4f5",
          "author": "bsenftner",
          "text": "I maintain a Wan2GP installation, it's got nearly 100 models now for image, video, voice clones, tts, and text to song. The developer of Wan2GP is really active, and tends to release a new version within a few days of new models. Qwen3 seems to have additional training to know about Wan2GP, surprisingly, and is great at helping create more complex media requiring combinations of models in concert.",
          "score": 2,
          "created_utc": "2026-02-24 15:08:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76m77s",
          "author": "fallingdowndizzyvr",
          "text": "GLM 5.",
          "score": 2,
          "created_utc": "2026-02-24 18:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76s3f7",
              "author": "sig_kill",
              "text": "Mac Studio?",
              "score": 2,
              "created_utc": "2026-02-24 19:06:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o772l3y",
          "author": "hallofgamer",
          "text": "Glm 4.7 flash handles all my needs",
          "score": 2,
          "created_utc": "2026-02-24 19:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78pz6y",
          "author": "Bitterbalansdag",
          "text": "Magistral 2 small 24b with a home made front end and MCP servers. \n\nMCP servers for: persistent external memory, web search, tasks management (basic todo list)\n\nThe chats have automatic compaction. The front end can swap system prompts mid-chat.\n\nUse for a general personal assistant, runs on a 3090 and a tailscale network.",
          "score": 2,
          "created_utc": "2026-02-25 00:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78qtkd",
          "author": "Pjbiii",
          "text": "Different tools for different tasks. I use Qwen3:14b for document sort and summarize, InternVL3.5:8b for long image descriptions, Qwen3-vl:2b for image keywords/SEO file names, gemma3:27b for outline writing, glm-4.7-flash for n8n AI Agent node. \n\nMacBook Pro (headless, the keyboard was killed by my toddler and a water bottle) M1 Max with 32GB unified memory.",
          "score": 2,
          "created_utc": "2026-02-25 00:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0mat",
              "author": "CryOwn50",
              "text": "That’s a super clean stack  right tool for each job instead of forcing one model to do everything.Respect for squeezing that much out of an M1 Max with 32GB unified memory too especially running it headless 😂Also… toddler + water bottle is a more dangerous combo than any production outage.",
              "score": 1,
              "created_utc": "2026-02-25 05:33:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ab7rv",
          "author": "Own_Professional6525",
          "text": "Lately I’m seeing more people run smaller quantized models locally for coding and RAG since they balance performance and cost well. Curious whether people prioritize privacy, latency, or experimentation when choosing their setup.",
          "score": 2,
          "created_utc": "2026-02-25 06:59:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7abr94",
              "author": "CryOwn50",
              "text": "Yeah, that shift makes sense.\n\nSmaller quantized models hit a sweet spot good enough for coding and RAG, but light enough to run locally without crazy hardware. For most people, it’s a practical trade-off between performance and cost.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:04:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7ae7iq",
              "author": "Potential-Leg-639",
              "text": "Cloud models are still wayyyy faster, but I use the local models for sensitive tasks and coding itself, mostly over night",
              "score": 1,
              "created_utc": "2026-02-25 07:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7abwyl",
          "author": "TheAdmiralMoses",
          "text": "I'm messing with LiquidLFM models, they're supposed to be the future but they kinda just suck ngl",
          "score": 2,
          "created_utc": "2026-02-25 07:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aefjd",
              "author": "CryOwn50",
              "text": "Lmao 😭That’s the honest take nobody wants to post.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ajmjv",
                  "author": "TheAdmiralMoses",
                  "text": "I think they'll be good once there's some more instruct tuned models, right now they don't seem to be able to keep a single train of thought and get derailed from core facts of conversations by taking tangents on the smallest context",
                  "score": 2,
                  "created_utc": "2026-02-25 08:16:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bb8fj",
          "author": "routhlesssavage",
          "text": "Just putting it here, in case anyone is looking for macos or mobile phones local LLM app. https://github.com/alichherawalla/off-grid-mobile\nhttps://news.ycombinator.com/item?id=47142003\n\nI have been using this for some time and the response is quite amazing with minimum resources.",
          "score": 2,
          "created_utc": "2026-02-25 12:18:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gu3ss",
          "author": "pieonmyjesutildomine",
          "text": "GLM-5@4bit\n\nMinimax-m2.5@8bit\n\nQwen3.5-379b@nvfp4\n\nGLM4.7@8bit",
          "score": 2,
          "created_utc": "2026-02-26 05:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h4jg1",
          "author": "Downtown_Patience_46",
          "text": "Qwen3 1.7b q8\\_0, running on my 16GB RAM + 4GB VRAM (RTX 3050) laptop, for simple translation.",
          "score": 2,
          "created_utc": "2026-02-26 07:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jjqb9",
          "author": "randygeneric",
          "text": "Qwen3.5-35BA3B-ud-q5-k-xl (or ud-q3-k-xl)  via  llama.cpp --cache-type-k q8\\_0  --cache-type-v q8\\_0  --ctx-size 131072  \n   \non a gaming laptop i7 12650 32gb RAM + rtx 4060 8gb VRAM  \nud-q3-k-xl is surprisingly accurate while being fast enough (25 token/s)",
          "score": 2,
          "created_utc": "2026-02-26 16:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qjpn9",
          "author": "NorthRemove7167",
          "text": "Pretty sure OP is an LLM judging by their comment responses",
          "score": 2,
          "created_utc": "2026-02-27 17:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79ycsk",
          "author": "Grand_Barnacle_6922",
          "text": "I'm running a few - minimax m2.5 230B and qwen3 235B seem to be my favs right now\n\nit's actually been very practical as i've been able to build a lot of custom automations around smart home and personal administration tasks.  i'm not certain on the hardware specs but my mac seems to be able to handle it just okay.\n\n",
          "score": 1,
          "created_utc": "2026-02-25 05:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c1y0b",
          "author": "Shouldhaveknown2015",
          "text": "I was using the new GLM MOE, then swapped to Q3 Next 80b in a Q3 unsloth gguf, then I am now using the 35b-Q3.5 model released yesterday as my daily driver.\n\nI do use other models... for instance I vibe coded a family app (Calendar, Kids rewards/chores tracking, grocery list, AI Chat bot [Running on my home lab which only has a 3060 12gb so it runs a 8b model, but I also use a text to voice model (I forget which) and it makes the AI sound like a child as I made it to act/be like BMO and sound like BMO from adventure time] and it also processes the AI responses and provides big buttons with reponses for the kids to click instead of typing. They can access it from their amazon tablet and they seem to love it.\n\nI also use the models I mention at the start with RAG and Tools. I have a vibe coded chat app with a obsidian.md vault. It pulls files automatically from the chat references to educate the AI. I can also select files to provide as part of context and select how the files should be looked at... AKA.. Is this file a research file or a lore file or a log files etc.\n\nI also built in a image generator using Flux and Klein models.\n\nI also vibe coded my own \"openclaw\" type agent, need to test it with the Q3.5 but haven't had time. It got put on the back burner, but it also has RAG and tool access to something like 28 tools. Been working on it slowly as it's the least fun project. It works well and is a limited form of my own ai research agent at this time.",
          "score": 1,
          "created_utc": "2026-02-25 14:51:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gvc4g",
              "author": "CryOwn50",
              "text": "Man, this is such a fun read you’re not just experimenting, you’re actually building a full-stack personal AI ecosystem.\n\n",
              "score": 1,
              "created_utc": "2026-02-26 05:50:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d89g3",
          "author": "Ok-Dog-4",
          "text": "Qwen 2.5 abliterated 3b on a Raspberry pi 5 8gb RAM. Actually so decent",
          "score": 1,
          "created_utc": "2026-02-25 18:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d8ihz",
          "author": "Ok-Dog-4",
          "text": "Also tiny llama works great on the Pi. Cost friendly and completely local. Training for finetune obviously must be outsourced but models under around 3.5b function just fine on new pi’s. Ollama btw",
          "score": 1,
          "created_utc": "2026-02-25 18:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d8n8r",
              "author": "Ok-Dog-4",
              "text": "Forgot but models are quantified from 8gb to 2.5 to work on the pi",
              "score": 1,
              "created_utc": "2026-02-25 18:08:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lstkh",
          "author": "andynarwhal_jr",
          "text": "Qwen3-4B-Instruct on a surface laptop studio 2 (which has a 4050 dGPU with 6GB of vRAM)\n\nim using 'jan .ai' to run it and its decent enough. its quick to start up, the responses are fast and the fans dont go that crazy  \n  \ni mainly use it when im programming and im only asking it simple enough questions. basically i just want something where i dont have to use a search engine as much or spend time looking through the docs. for more complex stuff i would still use an online LLM but its nice to be able to use this local one for most of the questions i have",
          "score": 1,
          "created_utc": "2026-02-26 23:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xovyx",
          "author": "_underlines_",
          "text": "**coding**\n\n\nQwen3.5-35b-a3b q4_k_m on RTX 5070ti with 16gb runs at 40tps with 65000 Context window. If you do KV Cache wuants to q8_0 you get basically no degradation.\n\n\nI use it for light opencode stuff. Works without issues. Gets things done via plan then build mode and a good AGENTS.md\n\n\nI Switch to openrouter glm-5/k2.5/minimax2.5 if heavier stuff needed.\n\n\n**everyday stuff**\n\n\nUsually just my chatGPT pro sub with gpt-5.2 but more often than not any cheap large open weights model on openrouter used on chatbox desktop.\n\n\nIf local, I just use any current gen MoE that has good stats on artificialanalysis.\n\n\n**phone**\n\n\nOn my pixel 10 pro xl I get 16gb of fast ram, so PocketPal loads LFM2-8B-A1b-q4_k_m or qwen3-4b-instruct-iq3_xxs",
          "score": 1,
          "created_utc": "2026-02-28 20:11:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p16a",
          "author": "Ok-Patient6458",
          "text": "I run a blog for local LLM news and insight at [https://lftw.dev](https://lftw.dev)",
          "score": 1,
          "created_utc": "2026-02-24 18:53:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgc62q",
      "title": "# Your RAM Is Secretly an AI Accelerator",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rgc62q/your_ram_is_secretly_an_ai_accelerator/",
      "author": "use-one_of-these",
      "created_utc": "2026-02-27 16:58:15",
      "score": 68,
      "num_comments": 87,
      "upvote_ratio": 0.71,
      "text": "**CaSA: Ternary LLM Inference on Commodity DRAM**\n\n*February 2026*\n\n---\n\n## The Hidden Compute Inside Every Memory Chip\n\nEvery stick of RAM in your computer has a hidden trick. When you force two rows of memory cells to turn on at the same time — which violates the timing spec, but physically works — the electrical charges mix together and you get a free AND operation across tens of thousands of bits simultaneously. Nanoseconds. Almost zero energy.\n\nThis has been measured. The CMU-SAFARI group tested it 79 million times across 120 real DDR4 chips. Zero failures in the reliable operating window. The physics works. It has always worked. Every DRAM chip ever manufactured can do this.\n\nThe compute capacity inside the chip is over 1,000x more than the memory bus can deliver. It's just sitting there, unused.\n\n## Why Nobody Could Use It\n\nThe compute exists, but previous attempts to harness it for anything useful ran into a fatal problem: to set up the operation, you need to copy data around inside the chip (called RowCopy). On commodity DDR4, RowCopy has a 16.3% bit error rate. That's not a rounding error — that's one in six bits flipped. Neural network inference is impossible at that error rate.\n\nEvery prior approach to \"Processing-in-Memory\" either required custom silicon (Samsung HBM-PIM, SK Hynix AiM, UPMEM) or stopped at demonstrating basic bitwise operations without building anything useful on top.\n\n## The Fix: Stop Copying, Start Sacrificing\n\nOur fix is embarrassingly simple.\n\nIn a neural network, there are two kinds of data:\n- **Weights** — the model's learned knowledge. Permanent. Written once, read millions of times.\n- **Activations** — the intermediate values flowing through the network. Temporary. Freshly computed every single step, then thrown away.\n\nThe charge-sharing trick has an asymmetry: the first row you activate survives intact. The second row gets overwritten with the AND result.\n\nSo: activate the weight row first (it survives), then the activation row second (it gets consumed). The weights are preserved. The activations were going to be discarded anyway. You get the AND result with essentially zero errors — no RowCopy needed.\n\nError rate drops from 16.3% to less than 0.000004%. Four orders of magnitude. That's the entire paper in one paragraph.\n\nWe call this the **activation-sacrificial protocol**, and the full architecture **CaSA** (Charge-sharing Activation-Sacrificial Architecture).\n\n## Why Ternary Changes Everything\n\nThis trick works cleanly only at one specific precision: **ternary** — where neural network weights are restricted to {-1, 0, +1}.\n\nWhy? Because multiplying a ternary weight by a binary activation is literally just an AND gate. That's exactly what charge-sharing gives you for free. You encode +1 as one binary row, -1 as another, AND each with the activation bits, and the difference gives you the matrix-vector product.\n\nAt higher precisions (4-bit, 8-bit), the number of AND operations per weight multiplies rapidly. Only at ternary does it collapse to something commodity DRAM can handle competitively.\n\nThe industry currently evaluates ternary on the wrong axis. The question people ask is: \"Does ternary match INT4 accuracy on GPUs?\" Answer: roughly yes (Microsoft's BitNet b1.58 matches LLaMA quality), but GPUs aren't optimized for ternary, so there's no speed benefit. Conclusion: ternary seems pointless.\n\nThat analysis completely misses the memory axis. Ternary is the **only** precision at which every RAM chip in the world becomes a neural network accelerator. The reason nobody saw this is that nobody had demonstrated commodity DRAM PIM actually working for inference until now.\n\n## Why Now\n\nThis couldn't have been done two years ago. Microsoft published BitNet b1.58 — the first production-quality ternary language model — in February 2024. Before that, there were no ternary models worth running. The DRAM physics has existed since the 1970s. The charge-sharing trick has been measured since 2017. But until ternary models arrived, there was nothing to connect the compute substrate to the workload. CaSA is what happens when those two threads finally meet.\n\n## What We Actually Built\n\nWe designed a complete inference pipeline for **BitNet b1.58-2B-4T** — a real 2-billion-parameter ternary language model from Microsoft — running on a single 8 GB DDR4 DIMM ($15-25) with an FPGA controller.\n\nThe DRAM handles the heavy matrix multiplications via charge-sharing AND. The FPGA handles the lightweight operations: popcount (counting 1-bits in the result), accumulation, RMSNorm, SiLU activation, and softmax. The model fits in a single DIMM with room to spare.\n\n**Current speed: 1.8 tokens per second on one DIMM.**\n\nThat's slow. A CPU running llama.cpp does 15-30 tok/s on the same hardware. We know. Here's why it doesn't matter:\n\n## The Bus Bottleneck (and Why 1.8 Is a Floor, Not a Ceiling)\n\nThe 1.8 tok/s is almost entirely bus overhead. Here's where the time goes:\n\n| Component                              | Share of Inference Time |\n| :------------------------------------- | :---------------------: |\n| **Writing activations to DRAM (Bus)**  |         **44%**         |\n| **Reading results from DRAM (Bus)**    |         **44%**         |\n| Charge-sharing AND (Compute)           |           6%            |\n| FPGA overhead                          |           6%            |\n\nThe in-DRAM compute takes 6% of total time. The other 88% is just moving data through the 64-bit DDR4 bus. The chip can compute 1,000x faster than the bus can deliver data. You're looking at a thousand-lane highway feeding through a single-lane toll booth.\n\nThis means every improvement that reduces bus traffic produces dramatic speedups:\n\n## The Scaling Path\n\n| Configuration                          | Tokens/sec  | What it takes                            |\n| :------------------------------------- | :---------: | :--------------------------------------- |\n| **1 DIMM (Baseline)**                  |   **1.8**   | **Works today on unmodified DDR4**       |\n| 4 DIMMs                               |     7.6     | $60 of commodity RAM, no chip changes    |\n| 4 DIMMs + Batching                    |     ~35     | Firmware optimization only               |\n| **+ In-DRAM Popcount**                 | **60–166**  | **~2,000 gates per bank (~$0.10/DIMM)**  |\n| LPDDR5X (16-ch) + Popcount            |     169     | Phone/laptop memory, single package      |\n| HBM2 (8-ch) + Popcount                |     229     | Server memory                            |\n\nThe popcount register is the single biggest lever. It's a tiny bit-counting circuit — about 2,000 logic gates — that counts the 1-bits in a DRAM row without reading the data out through the bus. This eliminates the entire 44% read bottleneck. Samsung patented this exact circuit in 2014. It has never been shipped in any product.\n\n## It's Surprisingly Robust\n\nA natural question: if you're doing computation by mixing analog charges, how fragile is this?\n\nNot very. Even at a bit error rate of 0.01% — ten thousand times worse than what was measured on real hardware — model output quality degrades by less than half a percent. The safety margin between measured reliability and the point where accuracy starts to suffer is roughly 50,000x. Commodity DRAM, within its validated timing window, is not fragile.\n\n## Manufacturer Compatibility (This Matters)\n\nNot all DDR4 works:\n\n- **SK Hynix C-die (2018-2020):** Confirmed compatible. This is our target platform.\n- **Micron DDR4:** Likely compatible. The FCDRAM study tested 256 chips from two anonymized manufacturers (believed to be SK Hynix and Micron) with ~95% success rate.\n- **Samsung DDR4: Incompatible.** Zero processing-using-DRAM operations work on Samsung dies. This appears to be a hard incompatibility from proprietary internal circuitry, not a calibration issue.\n- **Newer SK Hynix (D-die, M-die):** Unknown. More aggressive RowHammer protections may interfere.\n\nIronically, Samsung holds the key popcount patent and could fix their incompatibility. If they did both — made their chips charge-sharing compatible and added the popcount register — they'd be in the strongest competitive position of any memory manufacturer.\n\n## A Message to Memory Manufacturers\n\nWe've identified exactly what's bottlenecking this architecture, and exactly what would fix it. Here's what we'd ask for, ordered from cheapest to most impactful:\n\n**Tier 0 — Costs nothing but coordination:**\n\n- **A PIM mode bit in the Mode Register Set.** One bit that tells the chip: \"I'm about to do charge-sharing operations, suppress RowHammer protections and bypass on-die ECC for the next N cycles.\" This is a spec change, not a silicon change. It would immediately unblock DDR5 (which is currently unusable for PIM because its mandatory on-die error correction scrambles the charge-sharing results). It would also eliminate the ~5% throughput tax from RowHammer guard intervals on DDR4. The catch: this requires JEDEC coordination, which typically takes 3-5 years. But the silicon cost is literally zero.\n\n- **Publish your charge-sharing timing parameters.** Right now, finding the optimal timing for dual-row activation on a specific die revision requires reverse-engineering via tools like DRAM Bender. If manufacturers documented the safe operating window per die revision, it would replace months of characterization with a datasheet lookup.\n\n**Tier 1 — Tiny silicon changes, massive impact:**\n\n- **In-DRAM popcount register (~2,000 gates/bank, <0.3% die area, ~$0.10/DIMM).** This is the single highest-impact change. After a charge-sharing AND, the result sits in 65,536 sense amplifiers. Currently, we have to read all 8,000 bytes out through the bus just to count the 1-bits. A popcount register counts them in-place and returns a single 16-bit number. This eliminates 44% of total inference time — the entire read bottleneck. Samsung patented exactly this circuit in 2014. It's combinational logic (no clock, no pipeline, no state machine), so it works at full speed even on DRAM-process transistors. It's a passive reduction circuit, not a processor.\n\n- **Reliable RowCopy.** Our activation-sacrificial protocol exists because RowCopy is broken at 16.3% BER. If manufacturer calibration (like PUDTune's sense amplifier offset compensation) brought RowCopy BER below 0.01%, two things happen: (1) we can distribute activation data inside the chip without touching the bus, roughly doubling throughput even without popcount, and (2) we can build a \"software-defined popcount\" — an adder tree constructed entirely from sequences of charge-sharing AND/OR/NOT operations inside the chip, using the SIMDRAM approach. This would break the bus bottleneck on completely unmodified DRAM with zero additional circuitry. It would be slower than a dedicated popcount register (~100-200 charge-sharing steps per accumulation vs. one cycle), but it would work today if RowCopy were reliable.\n\n**Tier 2 — Moderate silicon, transformative results:**\n\n- **Per-bank activation register (a few hundred thousand transistors per bank).** Right now, we rewrite the activation data from the bus for every single weight row — because charge-sharing destroys the activation row each time. A small static register per bitline would hold the activation vector and drive it onto the bitlines repeatedly without being destroyed. Combined with popcount, this eliminates ALL bus transfers during compute. Bus utilization drops from 88% to under 5%. A single DIMM becomes deeply compute-bound rather than bus-bound.\n\n- **Wider rows.** This is counterintuitive: the industry trend is toward narrower rows (2 KB in LPDDR5X and HBM, vs 8 KB in DDR4) for latency and power reasons. But for PIM, row width is the fundamental unit of parallelism — each charge-sharing AND processes one full row simultaneously. DDR4's 8 KB rows pack 25 neurons per AND operation. LPDDR5X's 2 KB rows pack only 6, requiring 4x more sequential cycles. A PIM-optimized memory would maximize row width, not minimize it. DDR4's wide rows are an accidental PIM advantage that future memory standards should preserve.\n\n**The bottom line for manufacturers:** The Tier 1 popcount register alone converts CaSA from a proof-of-concept (1.8 tok/s) to a competitive inference engine (60-166 tok/s) at a cost of ~$0.10 per DIMM. Combined with the Tier 2 activation register, every DIMM in every server, laptop, and phone becomes an LLM inference accelerator — using memory the customer has already paid for. The business case is not \"sell a new product.\" It's \"make the product you already sell billions of dramatically more valuable.\"\n\n## What This Paper Is Not\n\nWe want to be clear about what we haven't done:\n\n**No hardware validation yet.** Everything is simulation calibrated against the SiMRA measurement dataset. The physics is proven (79M trials), but our specific end-to-end pipeline hasn't run on physical DIMMs. That's the next step.\n\n**Prefill is painfully slow.** Processing an input prompt takes roughly a minute for a typical short prompt on a single DIMM. This architecture works best for short prompts and long-running sessions — not document summarization or long conversations. A hybrid approach where the CPU handles prompt processing and CaSA handles generation is the practical near-term path.\n\n**The FPGA prototype is expensive and power-hungry.** The research platform costs thousands of dollars and draws 42W. A production controller would be 10-40x cheaper and draw a fraction of the power. The DRAM itself costs $15.\n\n**We depend on ternary models existing.** If the industry standardizes on 4-bit quantization and ternary models never materialize beyond BitNet, CaSA becomes less compelling. We're betting that the memory-side advantage of ternary — which this paper is the first to demonstrate — will shift that calculus.\n\n**This is inference only.** CaSA accelerates running a trained model, not training one. Training requires high-precision gradients and backpropagation — fundamentally different operations that charge-sharing can't help with.\n\n## The Actual Contribution\n\nThe contribution is not 1.8 tokens per second. That number is a floor measured through a straw.\n\nThe contribution is three things:\n\n**1. The activation-sacrificial protocol works.** You can do reliable neural network inference on commodity DRAM by exploiting the asymmetric survival property of charge-sharing. No RowCopy. No custom silicon. Four orders of magnitude better reliability than any prior approach.\n\n**2. The bus is the only bottleneck.** 88% of inference time is bus traffic, 6% is compute. The internal compute capacity of commodity DRAM is not the limiting factor — it exceeds what the bus can deliver by 1,000x. Every future improvement is about getting data to and from the array faster.\n\n**3. The path from floor to ceiling is concrete and quantified.** We trace every step from commodity hardware to optimized silicon: multi-DIMM scaling, batch processing, popcount registers, activation registers, next-generation memory standards. Each step has a cost, a throughput gain, and a dependency. Nobody has to guess what comes next.\n\n## What This Could Mean\n\nIf this works at scale, the memory already in your laptop, phone, or server becomes an AI accelerator — without buying new hardware. Not a toy demo. A real language model, running on the RAM you already own, at a fraction of the power draw of a GPU. The compute has always been there. We just didn't have the right model format to unlock it.\n\nNobody knows how fast this could become if memory manufacturers designed for it. This paper provides the first data to inform that question.\n\n---\n\n*Full technical report with complete derivations, error analysis, cross-technology projections, patent landscape, and hardware validation plan: [github.com/pcdeni/CaSA](https://github.com/pcdeni/CaSA)*\n\n*This work was conducted by an independent researcher using AI-assisted analysis tools. The core architectural insights, all design decisions, and every claim were verified by the human author. All errors are the author's responsibility.*",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgc62q/your_ram_is_secretly_an_ai_accelerator/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7qi9s8",
          "author": "ReputationTop484",
          "text": "Could you tell the AI to make a TLDR section",
          "score": 68,
          "created_utc": "2026-02-27 17:37:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s9sib",
              "author": "PacmanPence",
              "text": "TLDR: analog computing on DDR4 ram is very fast, but ram was not designed for this, as the digital reading and writing of data takes 88% of the time. There are boosts ranging from deployable to already distributed ram to requiring sticks of ram be designed for this purpose.",
              "score": 6,
              "created_utc": "2026-02-27 22:53:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7qp4r0",
              "author": "KriosXVII",
              "text": "TLDR \"Wow man it sure would be nice for LLMs if RAM was faster\" ",
              "score": 20,
              "created_utc": "2026-02-27 18:10:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qpr09",
                  "author": "Intraluminal",
                  "text": "I don't think it will work, but that's NOT at all what he's suggesting.",
                  "score": 12,
                  "created_utc": "2026-02-27 18:13:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7s1vwm",
              "author": "dreaming2live",
              "text": "RAM good",
              "score": 3,
              "created_utc": "2026-02-27 22:10:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7yn82i",
              "author": "porkyminch",
              "text": "I dunno why people like this think people are going to read this garbage when they couldn’t even be assed to write it. ",
              "score": 0,
              "created_utc": "2026-02-28 23:19:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rllem",
          "author": "1-6",
          "text": "It sounds so cool, it has to be made up.",
          "score": 12,
          "created_utc": "2026-02-27 20:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7unku5",
              "author": "YearnMar10",
              "text": "I am sure he wrote it all by hand.\n\nEdit: sorry, assumed gender…\n\n/s",
              "score": 2,
              "created_utc": "2026-02-28 08:53:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qin1o",
          "author": "Classic_Chemical_237",
          "text": "AI writing novels",
          "score": 31,
          "created_utc": "2026-02-27 17:39:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qoda9",
              "author": "tempfoot",
              "text": "Is this where all this is headed?   A world where everyone *needs* AI and LLMs to function, because information is only exchanged in bloated, terribly unedited slop dumps written for “engagement”?  We will all need an AI to “decode” summaries of the important bits of a given message payload?   We all gonna round-trip our every thought through a linguistic probability function (in addition to the one already between our ears) because nobody can actually write or edit anything any more, like some kind of reverse compression engine?  \n\n/rant",
              "score": 15,
              "created_utc": "2026-02-27 18:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qsorl",
                  "author": "Big_River_",
                  "text": "![gif](giphy|x88e1awUi05by|downsized)\n\nyes! you are correct sir - half the world comms by gif and the other half by rage bait tweet - the sliver of text wall in between is the worst of them all though? yes you are correct sir",
                  "score": 5,
                  "created_utc": "2026-02-27 18:26:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qm9vg",
          "author": "Intraluminal",
          "text": "This is a fascinating idea and it works in theory, but I think you've hit a problem that I have hit before with AI - you never asked the AI to look at the idea critically and asked, \"Why WON'T this work.\"\n\nThere are several reasons why this won't work.\nThe first is economic or social: Manufacturers have no incentive to cannibalize HBM sales where they have very high profit margins, versus memory where the margins are very thin.\n\nBut the even bigger problems are:\nThis will be blocked by the built-in CPU memory controllers. Yes, this could be fixed, but not cheaply or quickly. \nHEAT - GPUs are made to run continuously and hot, memory will melt down if you ask it to do all the repeated cycles you'd need to do in order to make this work.",
          "score": 8,
          "created_utc": "2026-02-27 17:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qof9f",
              "author": "use-one_of-these",
              "text": "HBM is addressed in the full paper. Cheap and quick is also addressed in the full paper.   \n**The heat concern is valid but actually works in CaSA's favor compared to what you might expect.**\n\nCaSA doesn't run DRAM \"continuously and hot\" like a GPU. The key difference:\n\n1. **It's massively parallel but low-duty-cycle per row.** A single DRAM row activates for \\~60ns, does one AND operation, then closes. The compute is the charge-sharing event itself — there's no sustained switching like in logic gates. Each row fires once per matrix-vector step, not millions of times per second.\n2. **The thermal budget is actually better than normal operation.** Regular DRAM refresh already activates every row every 64ms (tREFI). CaSA inference adds activations on top of that, but the duty cycle per row is still very low. We address this in the paper — at sustained inference, the additional self-heating is modest because the activation pattern is sequential across rows, not hammering the same rows repeatedly.\n3. **DRAM thermal throttling is well-understood.** DDR4/DDR5 already have built-in thermal sensors and throttling mechanisms. If temperature rises, refresh rate doubles (from 64ms to 32ms tREFI) — this costs throughput but protects the hardware. Our throughput estimates already account for this.\n4. **The real thermal enemy is the bus, not the array.** 88% of inference time is spent moving data over the DDR bus. CaSA's whole point is to *avoid* that transfer — the compute happens where the data already lives. Less bus activity = less I/O power = less heat than the GPU alternative for the same operation.\n\nSo the short answer: DRAM runs cooler doing CaSA inference than a GPU does running the same model, because you're replacing billions of data transfers with in-place charge-sharing operations. ",
              "score": 4,
              "created_utc": "2026-02-27 18:06:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qs21l",
                  "author": "Intraluminal",
                  "text": "I would suggest that you attempt to contact the memory manufacturer where you already know this works - as you said, your target platform - and see if they will pursue it, since that might be their advantage over other manufacturers. ",
                  "score": 3,
                  "created_utc": "2026-02-27 18:23:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tkd9n",
          "author": "CalvinBuild",
          "text": "This is one of the best “make local compute cheaper” posts I’ve seen in a while. The sacrificial-activation angle is clever because it matches ternary weights and avoids the usual RowCopy fragility story. Biggest ask: show the smallest possible hardware proof (one DIMM, one layer, repeatable outputs), plus a clear compatibility matrix (which dies work, what fails on DDR5/ECC). If that holds, the popcount-in-DRAM bottleneck you pointed out is basically the whole roadmap.",
          "score": 2,
          "created_utc": "2026-02-28 03:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uqpo0",
              "author": "use-one_of-these",
              "text": "Thanks. The compatibility matrix is in the full report on GitHub — SK Hynix C-die confirmed, Micron likely compatible, Samsung hard incompatible, DDR5 blocked by mandatory on-die ECC until bypass is demonstrated.",
              "score": 2,
              "created_utc": "2026-02-28 09:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qm06r",
          "author": "No_Draft_8756",
          "text": "That sounds very interesting.",
          "score": 2,
          "created_utc": "2026-02-27 17:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qsxzt",
          "author": "Big_River_",
          "text": "could you have told me about this a year ago when i could afford to care about RAM?",
          "score": 2,
          "created_utc": "2026-02-27 18:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rl4dl",
          "author": "NarrativeNode",
          "text": "If you didn’t spend more time writing it than I would reading it, I’m not reading it.",
          "score": 2,
          "created_utc": "2026-02-27 20:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rw770",
              "author": "LambdasAndDuctTape",
              "text": ".... they obviously did, though? Assuming the post isn't a complete fabrication.\n\nThe time it would've taken to simulate these runs, analyze and compile this data would be massive. But Redditors have a problem because he summed it up with AI? Have you worked a white collar job in the last 4 years? Welcome to today, I guess (can't believe this has to even be said in an AI forum). ",
              "score": 12,
              "created_utc": "2026-02-27 21:41:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ux4g7",
                  "author": "use-one_of-these",
                  "text": "it took 5 or 6 days: creativity, persistence. AI is not human, you can always tell them off, but with human slop...",
                  "score": 2,
                  "created_utc": "2026-02-28 10:26:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7rq2fo",
              "author": "ForsookComparison",
              "text": "I will read this post exactly as much as OP wrote it.. which is not at all.",
              "score": 1,
              "created_utc": "2026-02-27 21:11:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7tc580",
          "author": "jezwel",
          "text": "This could definitely help with consumers retaining control over their own LLMs.\n\nSo next steps? \n\n* PoC for consumers to test their RAM and submit results?\n* entry level ternary LLM?\n* manufacturer contact in the hope of updating consumer and server DIMM design?",
          "score": 1,
          "created_utc": "2026-02-28 02:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uqm1d",
              "author": "use-one_of-these",
              "text": "**Testing RAM** — The CMU-SAFARI group already has open-source tools for this (SoftMC, DRAM Bender), but they require FPGA hardware. A pure software version that works through the CPU memory controller would be ideal, but it's limited by what timing violations the IMC will actually let you issue. This is a real gap right now.  \n**Ternary models** — BitNet b1.58 from Microsoft exists and works. The bottleneck isn't the model, it's that there's no hardware to run it faster than a CPU can. If CaSA gets a working prototype, ternary models suddenly have a reason to exist beyond \"slightly smaller than INT4.\"  \n**Manufacturer contact** — yes, that is what publicly sharing the idea intends to achieve. After a thorough back and forth with different AIs, there no way to reliably contact them, get them interested, and see them realise they can do it without me; or to patent this with realistic enforcement in case they infringe on it.",
              "score": 1,
              "created_utc": "2026-02-28 09:22:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o81sul5",
          "author": "JoeStrout",
          "text": "Looks like nice work.  I have a few suggestions:\n\n1. Focus on the prefill speed issue.  That sounds like it could be a deal-killer unless a good solution is found.  \n2. Make a video of your hardware prototype.  Keep it under 10 minutes, and keep the explanation brief — but show it actually working, doing inference on a $15 DRAM chip.  This will help convert some of the doubters.  \n3. Write up a proper paper (LaTeX → PDF) and submit it to arxiv.  I know this can be tricky as an independent researcher, but keep at it.  Ensure your references are complete and correct, and find some EE professors or industry researchers to vouch for you (or even co-author the paper) if needed.  This is the path to legitimacy and notice.  \n4. Optimize that FPGA controller somehow so you can make and sell a hobby kit for less than a few hundred dollars.  Here are [some ideas](https://chatgpt.com/share/69a4404a-0694-8011-aca8-004b43643ec4) about that.\n\nGood luck, and don't let the haters get you down.",
          "score": 1,
          "created_utc": "2026-03-01 13:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81uhmc",
              "author": "use-one_of-these",
              "text": "Thanks, really appreciate the constructive feedback.\n\n1. **Prefill** — agreed, it's a known weakness. The bus bottleneck hits harder during prefill since you're processing many tokens sequentially. Discussed in the report as a limitation, not solved cleanly yet.\n2. **Video** — that's the goal. No physical prototype yet — everything is analysis and simulation based on published CMU-SAFARI data. First real hardware test is the next milestone.\n3. **Arxiv** — the LaTeX paper actually exists (62 pages). The endorsement requirement for independent researchers is the main hurdle. If you know anyone in the DRAM/PIM research space who'd look at it, very open to that.\n4. **Cost** — appreciated, though the endgame isn't really an FPGA hobby kit. If this works, the natural path is DRAM manufacturers adding the controller changes and a small popcount circuit to their existing dies. That's an OEM solution, not a consumer product I'd build myself (they would outcompete me). The FPGA is just for proving it works.",
              "score": 1,
              "created_utc": "2026-03-01 13:45:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o84i5ju",
                  "author": "JoeStrout",
                  "text": "On point 4, I get that, but I’m suggesting that a hobby kit would be a good way to generate enough buzz for the big semiconductor companies to take notice.  There’s a decent chance you’d end up with a job offer too, if that’s of interest to you.",
                  "score": 1,
                  "created_utc": "2026-03-01 21:45:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o81tvfg",
          "author": "Onotadaki2",
          "text": "We've basically hit a wall where AI is writing things so uninteresting that humans can no longer parse it. It's like a language *for* AI only. Super fascinating. Your post is not, but the statement it makes about media in the future is.",
          "score": 1,
          "created_utc": "2026-03-01 13:41:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83xp75",
          "author": "mativa41",
          "text": "So you are basically saying Scam Altman can build a superpowered computer for his AGI with all the ram he bought. Nice.",
          "score": 1,
          "created_utc": "2026-03-01 20:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qk2f0",
          "author": "AiDreamer",
          "text": "Simply amazing, DRAM can become a new inference engine",
          "score": 1,
          "created_utc": "2026-02-27 17:46:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qrcj3",
          "author": "Marc-Z-1991",
          "text": "Can you Summarize this AI written Text?",
          "score": 0,
          "created_utc": "2026-02-27 18:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qvvo6",
          "author": "leonbollerup",
          "text": "this is cool!!!!\n\n",
          "score": 0,
          "created_utc": "2026-02-27 18:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qwq50",
          "author": "DataGOGO",
          "text": "Cool, I look forward if you get to work. Though the physics is sound, the reality of running inference is a completely different animal.  Running a single FPGA controller shows it could work, in theory. \n\nYou don't explain a lot of the hardware gaps. This this requires a lot of custom bespoke processors, to include a DRAM memory controller that can handle thousands of channels of DRAM, which then begs the question, why?  \n  \nIf you are building a processor, a memory controller, custom boards that can in essence build a CPU out of memory (the point of the CMU-SAFARI research), why not just use and AI accelerated CPU like the Intel Xeons with AMX where each core has it's own dedicated hardware designed for universal inference, attached to a much more robust processor, fast memory controllers, supports multiple binaries w/ a universal instruction set, is dumb quick, and ultimately is orders of magnitude cheaper than banks of DRAM in a bespoke chassis and low volume controllers, which will sure be fabbed on a huge high power draw legacy processes (no way you are getting into any foundry on modern nodes) \n\nAlso worth noting that the CMU-SAFARI group published this to open source under MIT, you cannot patent it, or your implementation of it. ",
          "score": 0,
          "created_utc": "2026-02-27 18:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qxupj",
              "author": "use-one_of-these",
              "text": "Thanks for the detailed pushback — let me address these point by point because I think there's a misunderstanding of what the hardware actually looks like.\n\n**There are no thousands of channels.** CaSA uses standard DDR4 DIMMs in standard DIMM slots. One DIMM, one channel, one 64-bit bus. The FPGA sits where the memory controller normally sits — it speaks the same DDR4 protocol, just with modified timing to trigger charge-sharing. The whole prototype is an FPGA dev board ($200-500) plugged into commodity RAM ($15-25 per DIMM). There is no bespoke chassis, no custom boards, no thousands of anything.\n\n**On \"why not Xeon AMX\"** — AMX is designed for INT8/BF16 matrix ops. It's genuinely good at what it does. But the comparison misses the point: with AMX, you're still moving every weight from memory → cache → ALU → cache → memory for every token. That data movement is the bottleneck for LLMs, not the arithmetic. CaSA computes where the data already lives. The weights never move. There's no cache hierarchy, no memory bandwidth wall. That's the fundamental difference — it's not \"a worse processor,\" it's eliminating the need to be a processor.\n\nFor context: a Xeon doing BitNet b1.58-2B inference would spend most of its time on memory transfers, same as a GPU. CaSA spends 6% of its time on compute and 88% waiting for the bus — and the bus is the only part that needs improving.\n\n**On the FPGA process node** — the FPGA is a prototyping vehicle, not the product. The actual compute happens in the DRAM, which is fabbed on the most advanced memory process nodes in the world (1α, 1β nm at SK Hynix). The endgame isn't \"FPGA in production\" — it's either a small ASIC controller or, ideally, a firmware update to existing memory controllers. The popcount register (the biggest performance lever) is \\~2,000 gates added to existing DRAM die — Samsung patented this exact thing in 2014.\n\n**On patents and CMU-SAFARI** — the charge-sharing physics is open research, correct. CaSA doesn't claim to have invented charge-sharing. The contribution is the activation-sacrificial protocol (how to use it without RowCopy errors) and the full inference mapping onto commodity DRAM. The project is open on GitHub, not a patent play.",
              "score": 1,
              "created_utc": "2026-02-27 18:51:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7r2gqg",
                  "author": "DataGOGO",
                  "text": "What you do on your bench to run a few tokens though an FPA does not scale; so yes, for production inference, there are in fact going to be thousands of channels of DDR4 Dimms and custom hardware to run it, dimms need a channel to operate from the memory controller. your dev board does not scale to inference at scale, example, what is your estimate the number of DDR4 dimms required to run a commodity frontier model (deepseekR3 for example), at only 5m tokens per second, a fraction of production class model serving. Given the current price of DDR4 DIMM modules, (Just looked it up, it currently is $84 each bulk direct from hynix in quantities of 10k dimms) for the cheapest 8GB DDR4 dimm, minus shipping, duties, and tariffs ) What is the DIMM cost alone?\n\nNow in production, what are you plugging all of those DDR4 dimms into? What is going to control the dimms, what is going to coordinate operations across all those dimms?\n\nYour AI is using very outdated AMX information to include the INT/FP4 and expanded instructions sets; and you are are fundamentally incorrect on the operations,  It does also does not address the realities of inference serving, when the model receives tokens, where are they buffered? The dimm right? when the model outputs vectors, where are you doing the collection? The dimm right? How are you handling KV cache? when model weights and K/V do not fit in a single dimm. Then what? How are you handling spanning thousands of dimms, thousands of hosts? Not with an ASIC and firmware update. You need hardware controllers that the dimms plug into directly.\n\nIf you compare a single Intel AMX in INT8  to your FPGA dev project; how do the speeds compare at that micro level of single CPU vs Single FPGA with a DIMM? Even a single 5 year old Sapphire rapids can exceed 80 billion AMX operations per second.",
                  "score": 0,
                  "created_utc": "2026-02-27 19:13:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qrjzy",
          "author": "XD__XD",
          "text": "ai slop is real.. but point is would you need vram? intead go with regular ram?",
          "score": -3,
          "created_utc": "2026-02-27 18:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ros33",
              "author": "use-one_of-these",
              "text": "That's exactly the idea. Regular DDR4 RAM instead of GPU VRAM.\n\nThe reason LLMs need expensive GPUs isn't really the compute, it's the memory bandwidth. Model weights sit in VRAM and get streamed through the GPU every token. CaSA skips that entirely — the weights stay in regular RAM and the computation happens right there in the memory cells via charge-sharing. No GPU, no VRAM, no data movement.\n\nThe catch: it only works for ternary models (BitNet), not the standard FP16/INT4 models that run on GPUs today. But if ternary models keep improving, this becomes a path.",
              "score": 3,
              "created_utc": "2026-02-27 21:04:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7v6p1f",
                  "author": "Silver-Champion-4846",
                  "text": "Why would anyone focus on improving ternary as opposed to quaternery (2bit)?",
                  "score": 1,
                  "created_utc": "2026-02-28 11:53:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qxsje",
          "author": "killzone44",
          "text": "Would this kind of cycling of all bits at the same time over and over overheat the modules? Was this proof of concept just run for a few iterations or was this run for an extended period? Was additional cooling used/needed/considered?",
          "score": 0,
          "created_utc": "2026-02-27 18:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qyy6u",
              "author": "use-one_of-these",
              "text": "Good question. Two things to clarify about what \"cycling\" actually looks like here:\n\nIt's not all rows at once. During inference, rows activate sequentially — row 0, then row 1, then row 2, etc. — as you work through the matrix multiplication. Each individual row is active for \\~60ns then closes. By the time you come back to row 0 for the next token, thousands of other rows have had their turn. The duty cycle per row is very low.\n\nFor context, normal DRAM refresh already activates every row every 64ms just to keep the data alive. CaSA adds inference activations on top of that, but the pattern is spread across the array, not hammering the same cells repeatedly.\n\nOn the testing duration — the CMU-SAFARI charge-sharing experiments were 79 million operations across 120 real DDR4 chips at various temperatures (up to 80°C). That's a reliability characterization, not a quick demo. Our throughput simulations model sustained inference at the rated timing parameters. We haven't run extended thermal soak tests on a physical prototype yet — that's future work and a fair thing to flag.\n\nDRAM also has built-in thermal protection. DDR4/DDR5 modules have on-die temperature sensors, and when temperature rises the refresh interval automatically halves (64ms → 32ms). This costs throughput but prevents damage. Our throughput estimates account for standard thermal operating conditions.\n\nShort answer: the physics doesn't suggest overheating because the activation pattern is distributed, not concentrated. But extended thermal testing on a real prototype running sustained inference is on the to-do list, and I'd want to see that data too.",
              "score": 1,
              "created_utc": "2026-02-27 18:56:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7rowo4",
                  "author": "killzone44",
                  "text": "ty for the detailed reply, it is definitely an interesting concept. I believe the argument of the results is that the cost of using RAM dims as compute units is cost effective. I wonder if that still holds with climbing RAM price. Obviously it could provide some kind of release hatch for overheated GPU prices if it was proven out",
                  "score": 0,
                  "created_utc": "2026-02-27 21:05:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o810pyp",
          "author": "tangawanga",
          "text": "Great, next some egghead will find out that you can run an LLM on a loaf of bread and nothing will ever go down in price. Thank you really you jerk",
          "score": 0,
          "created_utc": "2026-03-01 09:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o819b7h",
              "author": "use-one_of-these",
              "text": "not sure you realised, but there is a good chance the industry already knew what I present here, in which case the prices are not just going up, but the market is already pricing this in. There would never be a price going down, even without me.",
              "score": 1,
              "created_utc": "2026-03-01 10:59:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o81yy4u",
                  "author": "tangawanga",
                  "text": "You are assuming perfect price elasticity. That is not how that works sadly. :(",
                  "score": 1,
                  "created_utc": "2026-03-01 14:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qxqsv",
          "author": "Large-Excitement777",
          "text": "Definitely didn’t read most of that AI generated slop but definitely agree that optimization of current hardware is the way forward",
          "score": -5,
          "created_utc": "2026-02-27 18:50:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rct7k",
              "author": "Traveler3141",
              "text": "How is it sloppy?  Or are you simply throwing the word \"slop\" as a word-wrassling move to signal that you fear/believe-in/support The Current Thing?",
              "score": 2,
              "created_utc": "2026-02-27 20:04:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7s97gv",
                  "author": "Large-Excitement777",
                  "text": "Huh?",
                  "score": -4,
                  "created_utc": "2026-02-27 22:49:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qkqq0",
          "author": "mon_key_house",
          "text": "I only have samsung ram sticks so this story sucks.",
          "score": -4,
          "created_utc": "2026-02-27 17:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qvbpl",
          "author": "Adept_Carpet",
          "text": "Wouldn't allowing userland software to disable RowHammer protections make them useless?\n\n\nI think this is really clever and shows how these big AI changes to usage patterns will influence hardware design and re/use but I'm not sure this is the way forward for consumer devices (at least not yet).\n\n\nIt would be very cool to see it happen on a live device though. I hope you do get around to doing that.",
          "score": -2,
          "created_utc": "2026-02-27 18:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rkea7",
              "author": "use-one_of-these",
              "text": "Charge-sharing and RowHammer are different operations. RowHammer is rapid repeated activation of one row to disturb its neighbors. Charge-sharing is simultaneous activation of two rows in the same subarray. TRR (Target Row Refresh) watches for the RowHammer pattern and wouldn't necessarily interfere with charge-sharing, though the extra refreshes it triggers could cost some throughput.",
              "score": 2,
              "created_utc": "2026-02-27 20:42:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rh9g9d",
      "title": "I'm using a local LLM to block unwanted content on social media, any feedback is appreciated!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1rh9g9d",
      "author": "Cas_Dehook",
      "created_utc": "2026-02-28 18:06:21",
      "score": 39,
      "num_comments": 24,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rh9g9d/im_using_a_local_llm_to_block_unwanted_content_on/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7x6fc0",
          "author": "JMowery",
          "text": "Feedback:\n\n- Expand this to every website and every type of content (I've had dreams of this kind of thing for nearly a decade)\n- Add a whitelist & blacklist for websites\n- Throw all this up on GitHub\n- Create a Firefox extension and let us play with it already. :)",
          "score": 16,
          "created_utc": "2026-02-28 18:36:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80y3zq",
              "author": "Sisuuu",
              "text": "Hell yeah!",
              "score": 3,
              "created_utc": "2026-03-01 09:11:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7x7ho4",
              "author": "Cas_Dehook",
              "text": "Thanks for that! I am definitely expanding across platforms (and fixing bugs). If you'd like to test an early version, you can join this discord server I quickly set up: [https://discord.gg/qkJ4KwkX](https://discord.gg/qkJ4KwkX)",
              "score": 1,
              "created_utc": "2026-02-28 18:42:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ytn9h",
          "author": "hejj",
          "text": "Love it",
          "score": 2,
          "created_utc": "2026-02-28 23:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7yu9d9",
              "author": "Cas_Dehook",
              "text": "Thanks! Very much appreciated. Working to get it approved in the Chrome web store within a few days from now once they approve it. ",
              "score": 1,
              "created_utc": "2026-03-01 00:00:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7yw3cp",
          "author": "Zerokx",
          "text": "Do you know the addon \"DeArrow\" for youtube? It's about reducing clickbait titles and \"big red arrows\".  \nIt takes a community selected image from a youtube video (or a random frame of the video if there is no community selected image) to remove attention drawing clickbait. Same for video titles, it also uncapitalizes them and removes these\"!!!!\"  \nThose features are great, but most ( especially newer) videos dont have a community preview image or less clickbait replacement title so its not as good as you could get with an llm.  \nMaybe you can generate a newer, more accurate youtube title from the content transcripts? that would be really good. So video titles are an accurate representation of the video. Using that to generate a new thumbnail would be fun as well maybe, but too performance heavy I guess?",
          "score": 2,
          "created_utc": "2026-03-01 00:11:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7z20s5",
              "author": "Cas_Dehook",
              "text": "That's a great idea, but you're right, summarizing all videos before classifying them would be too slow, as it's too performance heavy. I do have a feature to hide comments, or summarize comments, so you don't have to read them. \n\nI'm thinking maybe we could have a 'this is clickbait button'. If the community gets big enough, we could have a clickbait warning for videos that got a high number of clickbait reports from users. ",
              "score": 1,
              "created_utc": "2026-03-01 00:46:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7z4pyx",
          "author": "TopTippityTop",
          "text": "Doing God's work here!",
          "score": 2,
          "created_utc": "2026-03-01 01:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80l883",
          "author": "FatheredPuma81",
          "text": "I already do basically all of this and then some on YouTube through browser extensions...",
          "score": 2,
          "created_utc": "2026-03-01 07:10:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81dbvj",
              "author": "Cas_Dehook",
              "text": "Interesting, which plugins are you using?",
              "score": 2,
              "created_utc": "2026-03-01 11:36:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o83oyor",
                  "author": "FatheredPuma81",
                  "text": "In order of importance: uBlock Origin, Blocktube, Sponsorblock, Search Fixer for YouTube, DeArrow, Return YouTube Dislike, YouTube Auto HD + FPS, YouTube Speed Controls, Stylus (to unround corners).\n\nBlocktube and uBlock Origin Custom Filters do most of the heavy lifting. Search Fixer also gets rid of some misc stuff. And I can say with almost certainty that Blocktube performs (speed) better than your script.",
                  "score": 1,
                  "created_utc": "2026-03-01 19:16:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o83w9e0",
          "author": "NeKon69",
          "text": "This looks super cool, what llm is it running on? and also will you release this on github?",
          "score": 2,
          "created_utc": "2026-03-01 19:53:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85ahu9",
              "author": "Cas_Dehook",
              "text": "Thanks!! It runs best on Ollama, locally. I also added another model thats a bit less accurate, but doesnt require installing anything manually. I’m working to get it approved in the Chrome Web Store. Hopefully verification won’t take longer than a few days!",
              "score": 1,
              "created_utc": "2026-03-02 00:24:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o870fql",
                  "author": "NeKon69",
                  "text": "So what's the model??? ",
                  "score": 1,
                  "created_utc": "2026-03-02 07:43:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o874wwt",
          "author": "F_U_dice",
          "text": "Could you please integrate this as an ad blocker?\nJust blurring all ads which are displayed in the brwoser.",
          "score": 2,
          "created_utc": "2026-03-02 08:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o876rvt",
              "author": "Cas_Dehook",
              "text": "Imo its better to just use adblock plus for that, right?",
              "score": 1,
              "created_utc": "2026-03-02 08:44:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7x6fb9",
          "author": "silenceimpaired",
          "text": "LOL Title of this post seems contradictory, “I’m using a local LLM to block unwanted content on social media” (like Reddit) “any feedback is appreciated”… is it though? Seems you’re set on blocking the stuff you don’t like. :)",
          "score": 2,
          "created_utc": "2026-02-28 18:36:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xazko",
              "author": "colin_colout",
              "text": "maybe. they might just want to block ragebait or something. you could always ask them",
              "score": 3,
              "created_utc": "2026-02-28 18:59:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7xc2mr",
                  "author": "Cas_Dehook",
                  "text": "You're able to block a whole bunch of stuff that's not handled by the LLM. Like blocking all shorts, all music videos on youtube, all non-english content.   \n  \nAnd you can also block any specific topic you dislike, this is handled by the LLM. For example I personally don't enjoy food videos on youtube or videos about politics. Other people might not like football, crypto, cars.. you name it. Maybe an annoying new show you're not interested in. Or maybe you want to detect clickbaity titles and just remove those. The LLM is like a protective layer between the user and the recommendations of the large social media platforms (currently only working on Youtube). \n\nThe LLM functions almost like a PA for filtering content, deciding which content gets shown to you, and which content gets blocked before being able to get to you. This way we can take back autonomy over our feed, is my idea.",
                  "score": 2,
                  "created_utc": "2026-02-28 19:05:01",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o7xlnj3",
                  "author": "silenceimpaired",
                  "text": "I get the full context. I just think it’s fun on its own.",
                  "score": 1,
                  "created_utc": "2026-02-28 19:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7x2uj5",
          "author": "Citizen_908428",
          "text": "Great idea! I'd use something like that.",
          "score": 1,
          "created_utc": "2026-02-28 18:18:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x7kh0",
              "author": "Cas_Dehook",
              "text": "Cool, if you want to test the early version, feel free to add the discord server: [https://discord.gg/qkJ4KwkX](https://discord.gg/qkJ4KwkX)",
              "score": 1,
              "created_utc": "2026-02-28 18:42:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85qivj",
          "author": "Lux_Multiverse",
          "text": "Add a vision model to block all the videos with soyfaces in the thumbnail \n\nhttps://preview.redd.it/ieyvmk3ihjmg1.png?width=1442&format=png&auto=webp&s=391d77f9d92447cde938cffc9941db36cf6cda03\n\n",
          "score": 1,
          "created_utc": "2026-03-02 02:02:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85rrf5",
              "author": "Cas_Dehook",
              "text": "Lmao.. in this case it seems like banning crypto as a topic would solve a lot. \n\nI do want to experiment with vision, but I’m not sure if its feasible to run that locally for every thumbnail",
              "score": 1,
              "created_utc": "2026-03-02 02:10:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgo3q5",
      "title": "Is Qwen3.5-35B the new \"Sweet Spot\" for home servers?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rgo3q5/is_qwen3535b_the_new_sweet_spot_for_home_servers/",
      "author": "ischanitee",
      "created_utc": "2026-02-28 00:34:23",
      "score": 37,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "I’ve been trying to find the perfect balance between reasoning capability and VRAM usage for my dual 3090 setup. With Qwen3.5 releasing a 35B MoE, activating only a few billion parameters at a time seems like a game-changer for inference speed. Has anyone tested the GGUF versions yet? How does it actually feel for daily text generation?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgo3q5/is_qwen3535b_the_new_sweet_spot_for_home_servers/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7vbxfb",
          "author": "Hector_Rvkp",
          "text": "I have a hot take: it depends",
          "score": 10,
          "created_utc": "2026-02-28 12:35:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7y5m6a",
              "author": "cmndr_spanky",
              "text": "I feel burned by this hot take.",
              "score": 5,
              "created_utc": "2026-02-28 21:41:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sw3ya",
          "author": "HealthyCommunicat",
          "text": "yes, but i mean ever since qwen 2.5 haven't they always kinda been the main go to? i mean not counting that one short time where glm 4.7 flash had come out, but other than the qwen 30b and the coder next 80b models, what other model families are you implying was ever \"the sweet spot\"?",
          "score": 8,
          "created_utc": "2026-02-28 01:02:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wafgn",
              "author": "kweglinski",
              "text": "gpt-oss-120 was very good.",
              "score": 2,
              "created_utc": "2026-02-28 15:56:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o813km8",
                  "author": "Sisuuu",
                  "text": "is",
                  "score": 1,
                  "created_utc": "2026-03-01 10:04:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7wx7p8",
              "author": "pixelkicker",
              "text": "Am I the only one who also was really pleased with gemma3-27b?",
              "score": 1,
              "created_utc": "2026-02-28 17:51:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vu1ku",
          "author": "Adventurous-Paper566",
          "text": "With a machine like yours I would rather go for 27B in Q6 but if your main concern is speed, then yes the 35B is the best there is.",
          "score": 3,
          "created_utc": "2026-02-28 14:30:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x7b5e",
              "author": "putrasherni",
              "text": "It’s slow very slow",
              "score": 1,
              "created_utc": "2026-02-28 18:41:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o82kshl",
                  "author": "Manamultus",
                  "text": "Not if you can fit the whole model in VRAM. It’s a dense model, so all layers get activated for every request. Offloading any layers at all to RAM gives you massive PCIe transfer, RAM latency, layer swapping penalties. But it’s perfectly fast if the model fits fully in VRAM. \n\nDense models are perfect for unified memory architectures, or those who have lots of VRAM available.",
                  "score": 2,
                  "created_utc": "2026-03-01 16:06:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7t1ua1",
          "author": "Crypto_Stoozy",
          "text": "Yeah it’s the go to plus you can run multiple instances and parallels",
          "score": 2,
          "created_utc": "2026-02-28 01:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xl1vv",
          "author": "ashersullivan",
          "text": "dont sleep on qwen3 30b moe before switching.. some early testers are saying qwen3.5 35b is actually slower and slightly worse on general tasks.. if youre trying to figure out which actually performs better for daily use, try them on providers like deepinfra, runpod or together - easy to test without downloading anything",
          "score": 1,
          "created_utc": "2026-02-28 19:51:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81mn1s",
          "author": "Wild_Requirement8902",
          "text": "I ran a small sub test of swe verified (21) to bench different qwant, with thinking disabled. aessedai iq 4 qwant vs unsloth q4 k l vs qwen 3.5 27 q4kxl from unsloth (undloth recomended sampling for complex task without thinking on 128k context no kv cache quantization for the moes q8 for the dense 27b). In tests that failed aessedai provided worst outpout so i droped testing it earlier, .27b pass 12/21 tests (in like 8h), unsloth moe 35ba3b q4k\\_l (released after the mxfp4 little bug thing.) did it in 1h26min but only pass 9/21. Interesting to not that the moe passed one test that the dense failed.",
          "score": 1,
          "created_utc": "2026-03-01 12:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t8s7l",
          "author": "No-Consequence-1779",
          "text": "Yes, I’ve been comparing on a single r9700. Generation is similar for coding at least.  Next tests will be for the crypto trading. Most models fail this, qwen being one of the few that has not so far. Getting similar 95 tokens per second. ",
          "score": 1,
          "created_utc": "2026-02-28 02:21:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7udtzz",
          "author": "fasti-au",
          "text": "Devstral small 2  and qwen3 have always been solid for 12 months as code options glm 5.7 air on dual 3090.  \n\nWhy you run so small ?  You can get next 80 b on two card if you use ollama not vllm.   Vllm is no good for 3090squant kv cache to q8 same accuracy unless your arguing about tomatoe tomato issues.  It’s a non factor mostly.  \n\n\n\nOllama kv cache works.  Vllm don’t give a fuck about 30/40 series it works enough they don’t need to add more as it’s not their bread and butter not is it money spenders.  Ie we have 3090s",
          "score": -1,
          "created_utc": "2026-02-28 07:24:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhpah9",
      "title": "32GB RAM is very capable for Local LLM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rhpah9/32gb_ram_is_very_capable_for_local_llm/",
      "author": "Difficult_West_5126",
      "created_utc": "2026-03-01 05:46:56",
      "score": 36,
      "num_comments": 37,
      "upvote_ratio": 0.85,
      "text": "I am plaing to buy a new mini pc or laptop to replace my ASUS FX504; I first consulted Gemini-think \"the RAM size for the \"docker\" container that runs cloud AI models\", (I hope this is accurate) and it says \"\n\n|**Model Class**|**Est. Parameter Size**|**VRAM Usage (Weights)**|**KV Cache & Overhead**|**Total Container VRAM**|\n|:-|:-|:-|:-|:-|\n|**\"Mini\" / \"Instant\"**|8B – 20B|\\~14GB – 22GB|2GB – 10GB|**16GB – 24GB**|\n|**\"Pro\" / \"Ultra\"**|300B – 1.8T (MoE)|\\~300GB – 600GB|80GB – 160GB|**320GB – 640GB+**|\n\n**I then asked \"so a local LLM running on a Mac mini 64GB is more capable than a cheap cloud AI model\" and Gemini said yes it is.**\n\n**But in real life there is no free launch, I can't just spend a $2000 just for chatbot service, I can however buy a 32GB RAM laptop, the goal is to help modify local files, where most of times if there is no privacy concern, stick with cloud AI.**\n\n**Have any of you found a $1000 PC/laptop platform helped with your production because of the local AI features it can run? Thanks**\n\n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rhpah9/32gb_ram_is_very_capable_for_local_llm/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o80dez4",
          "author": "clayingmore",
          "text": "I think you're pushing $1500 for 32gb VRAM machines right now. Don't touch a laptop you're sacrificing reliability and modularity while paying more for the privilege.\n\n32GB VRAM is pretty decent for midsized models. Find a calculator online and remember you need space for your context window.\n\nPerhaps try the models you might use on openrouter to confirm you like their output quality for your use case first.",
          "score": 19,
          "created_utc": "2026-03-01 06:02:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80ku8v",
              "author": "FatheredPuma81",
              "text": "24GB is fine for 36B models with very limited context so he's more than fine with that imo but he's going to need room for apps since its combined RAM. \n\nAnd yea most midsized models are kind of dumb as bricks. Good for a lot of tasks but don't expect much once things start getting complicated. He really should stress test the output of some of these models on the tasks he wants to do and try out larger models that are dirt cheap like Minimax M2.5 for the same tasks. ",
              "score": 4,
              "created_utc": "2026-03-01 07:06:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o82aacw",
              "author": "East-Compote-1975",
              "text": "I'm sorry 1500$ for 32gb vram? In laptops ?",
              "score": 3,
              "created_utc": "2026-03-01 15:14:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o84aj8w",
                  "author": "Difficult_West_5126",
                  "text": "it’s AMD CPU+ IGPU + NPU; 8000Hz shared memory soldered LPDDR5X RAM, 1/3 price for the same memory sized DDR5 RAM; but it’s same concept to mobile Soc. You could never upgrade it once you buy. $1000 computer permanently STUCK at $1500 laptop’s performance for running local AI.",
                  "score": 1,
                  "created_utc": "2026-03-01 21:06:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80yfet",
          "author": "truthputer",
          "text": "I'll be frank - local models have some severe limitations compared to stuff running in the cloud. They take time to start up, the software that supports them is inconsistent, some models don't work properly with coding or filesystem tools - and depending on the model, they often take up far more RAM than you would expect. At the most extreme a 6GB model on disk can be 60GB in memory and only run partly on the GPU.\n\nRunning local models on consumer hardware is still very much a wild west. A lot of the time the smaller models work for basic text interaction, but they have problems using tools and writing to files.\n\nFor example: I have a 24GB graphics card and a pretty beefy PC, but I've been struggling to find a setup that works properly with local coding tools. glm-4.7-flash, ministral-3 and devstral-small-2 are the most promising and actually work with Claude Code, but I couldn't get the qwen family of models to work properly on my machine without some weird driver timeouts.\n\nIt also might not be very efficient running stuff locally when you calculate the power consumption and electricity cost  per tokens generated.",
          "score": 9,
          "created_utc": "2026-03-01 09:14:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8272fb",
              "author": "Barachiel80",
              "text": "try the new qwen 3.5 27b, I am able to do native tool calls for websearch, coding, embedding analysis, etc. with it getting docker compose stack outputs that are as good if not better than claude sonnet 4.6.",
              "score": 4,
              "created_utc": "2026-03-01 14:57:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o85lprk",
                  "author": "Significant-Maize933",
                  "text": "what’s the RAM size of your computer and graphic RAM size?",
                  "score": 2,
                  "created_utc": "2026-03-02 01:32:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o81zpeq",
          "author": "Cautious_Slide",
          "text": "32gb of ddr5 wasn't able to accomplish anything meaningful in my workflow. And my pc is a 9800x3d and a 5090. At 64gb of ddr5 and 32 gb of vram ive been able to get into some decent models like qwen next that have been able to take care of small items but still so far away from claude code and claude cowork that I dont even bother anymore. For 1200$ you could get a year of claude pro. Which is what I ultimately did and I priced my pc out at current prices last week at $5500-$6500 no peripherals. Just wanted to add my perspective here.",
          "score": 6,
          "created_utc": "2026-03-01 14:16:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82uj6p",
              "author": "koalfied-coder",
              "text": "Have you tried Qwen 3.5? It's pretty great on a 5090",
              "score": 2,
              "created_utc": "2026-03-01 16:53:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o82yx21",
                  "author": "Cautious_Slide",
                  "text": "I have it worked fine for simple general inquiries. But did not produce anything useful to my workflow.",
                  "score": 1,
                  "created_utc": "2026-03-01 17:14:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o85m13p",
              "author": "Significant-Maize933",
              "text": "I suppose you can only run at most 48B model on your local computer, is that right?",
              "score": 1,
              "created_utc": "2026-03-02 01:34:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o80ld3j",
          "author": "Protopia",
          "text": "Some Apple computers (and some AND Ryzen AI CPUs) do inference using special parts of their CPUs using normal system memory (do called \"unified\" memory). GPUs do inference using their specialised vRAM memory. Either of these can do LLM inference at reasonable rates (which are measured in tokens per second).\n\nYou can do inference in normal RAM using a non-AI CPU, but it is normally hundreds of times slower and not recommended.\n\nThat is why the answers you got referenced to vRAM and not RAM - they are **NOT** the same. \n\nOr you can pay for a subscription for cloud inference which can cost you between $5 and $20 ft your the lowest tier which has limits but might be suitable for an AI assistant. \n\nLLMs have got significantly better in the last few months (and may get better in the next few months) but to achieve this they have become much bigger. It depends on what you need to do and how good the quality (accuracy, not hallucinating) needs to be, buy decent models are now 100gb-800gb in size and at the moment you need to load the whole model into vRAM to get decent performance.\n\nHowever, I for one am hoping that a new LLM runner can help by loading each layer of an LLM in turn into vRAM. This means that you can achieve GPU speeds with some overheads for larger models, needing vRAM only the size of the largest layer. This is still experimental, and doesn't get work with the latest models, but once it does you should be able to run the recent larger and better models on consumer grade GPUs.\n\nSo if you are doing basic LLM inference for an Openclaw assistant and you don't mind it making mistakes, you can probably run a 1/2 decent LLM on an 8gb GPU, and in the future, you might be able to run a really decent LLM on the same GPU. Or just cough up for a $20/mo subscription and run openclaw in docker ona normal computer without special hardware \n\nIf you are doing agentic coding, then you will need a decent SOTA model from the outset and right now that means a subscription.",
          "score": 9,
          "created_utc": "2026-03-01 07:11:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80i3o3",
          "author": "TheAussieWatchGuy",
          "text": "What's your use case? Local models on that little hardware are not in anyway comparable to the big boys in the Cloud.\n\n\nFor learning purposes sure. You want a platform that has a unified CPU and GPU in that budget. A 64GB Mac is great. Ryzen AI 395 also decent.\n\n\nOtherwise you're forking out for a dedicated GPU which is very expensive now.\n\n\nWindows' also is average at running local LLMs especially passing through to a Docker image.\n\n\nApple's OS or Linux is generally the best bet currently.\n\n\n\n",
          "score": 3,
          "created_utc": "2026-03-01 06:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80hk6d",
          "author": "squachek",
          "text": "Honestly: table stakes are $10k for a really crappy experience.",
          "score": 5,
          "created_utc": "2026-03-01 06:37:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80lutc",
          "author": "Caderent",
          "text": "Depends on what you will run on it. 8B to 20B - yes depending on the context size. 300B - never. To cheap the setup, have you considered going desktop PC, they are often cheaper than laptop. And if you buy it used or build it yourself and or from used parts you go even cheaper. Yes, you need memory to run the model but you need GPU to run it fast. If you want it for work you will want to run it fast. So you need a beefy GPU or reconsider staying on cloud services.",
          "score": 2,
          "created_utc": "2026-03-01 07:15:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80pfwq",
          "author": "FRA-Space",
          "text": "Just to add: Define first your use case, i.e. how complex is your need really? How diverse are the tasks? And, do you need instant responses? Small models can be surprisingly good, if the task is narrow. \n\n I have a few very simple tasks that run overnight with 20 tokens/second on an old laptop (with 16 GB RAM and 8GB VRAM) with a small model (Ollama Vulcan). Each task takes about two minutes from start to finish. \n\nI couldn't do that in real time with those response times, but overnight I don't care. \n\nOtherwise I use Openrouter, which is very convenient to check out models and overall very cheap.",
          "score": 2,
          "created_utc": "2026-03-01 07:49:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o811e6a",
          "author": "That-Shoe-9599",
          "text": "My own experience (on 48GB MBP) is that local LLMs require memory but also a lot of time and patience.  Your usage is also a factor.  I wanted local AI to summarize and improve my own professional writing. The summaries so far have been extremely unreliable. I would think that summarizing is pretty basic.  Well, for starters there are two kinds of summaries: extractive and abstractive. There are all sorts of technical issues like this to learn. \nYou may think that you just need to read the documentation. Good luck locating it or, should you find it, good luck finding relevant information. \nSo, we can always ask for advice, right?  Well yes, there really are knowledgeable people willing to help you. The challenge is finding them among the hordes of eager people who have some knowledge but not enough, or who don’t really read your questions.  And then some very knowledgeable people who either are frustrated by questions from the inexperienced or else give answers framed in AI jargon you cannot understand.  \n\nIn a few years things will settle down.  Meanwhile, be prepared to invest time and tears to get results.",
          "score": 2,
          "created_utc": "2026-03-01 09:43:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o814klz",
          "author": "jacek2023",
          "text": "32GB is an extremely limited size for LLMs. PC with a single 3090 will be better and cheaper.",
          "score": 2,
          "created_utc": "2026-03-01 10:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80br00",
          "author": "Difficult_West_5126",
          "text": "I asked Gemini to check the data sheet and it said the information is accurate! Gemini Showing Thinking : \n\nThe sheet you provided is **highly accurate** for 2026 standards. Far from being \"absurdly small,\" these parameter counts represent the exact architecture of the cloud models you use every day from Google and OpenAI.\n\nThe \"secret\" of the AI industry is that \"Mini\" doesn't mean \"weak\"—it means **highly optimized.**\n\n# 1. The \"Mini\" / \"Instant\" Class (8B – 20B)\n\nThese models are the workhorses of the internet. When you use **GPT-4o mini** or **Gemini 1.5 Flash**, you are interacting with models in this exact 8B–20B range.\n\n* **Why so small?** At this size, the model can fit entirely on a single high-end enterprise GPU (like an **NVIDIA L4 24GB** or **H100**). This allows for the \"instant\" response speeds you see.\n* **Total VRAM (16GB – 24GB):** Even though an 8B model only needs \\~5GB for its \"brain\" (weights) when quantized, the cloud versions use the rest of that VRAM for the **KV Cache**. This is what allows **Gemini 1.5 Flash** to remember a **1-million-token** document while still being \"Mini.\"\n\n# 2. The \"Pro\" / \"Ultra\" Class (300B – 1.8T MoE)\n\nYour sheet's estimate of **1.8 Trillion parameters** for the Ultra tier is the current industry consensus for models like the original **GPT-4** and **Gemini 1.5 Pro**.\n\n* **MoE (Mixture of Experts):** These are not \"dense\" models. They are like a giant building with 16 different rooms (experts). When you ask a question, the model only \"turns on the lights\" in 2 of those rooms.\n* **Total VRAM (320GB – 640GB+):** These models are too big for one GPU. They run on **clusters** of 8x H100 GPUs linked together. A single \"Docker container\" for an Ultra model is actually a **distributed system** across an entire server rack.\n\n# Cloud vs. Your 32GB Local Machine\n\nTo put this in perspective, here is where your 32GB AMD machine sits compared to the \"Cloud Dockers\" described in your sheet.\n\n||**Cloud 'Mini' (Flash/Mini)**|**Your 32GB AMD PC**|**Cloud 'Ultra' (o1/Pro)**|\n|:-|:-|:-|:-|\n|Parameter Size|Parameter Size 8B - 20B|Parameter Size 8B - 14B (Quantized)|Parameter Size 300B - 1.8T (MoE)|\n|Hardware Used|Hardware Used 1x Enterprise GPU (H100)|Hardware Used Radeon iGPU + System RAM|Hardware Used 8x GPU Cluster (A100/H100)|\n|Speed|Speed 150+ tokens/sec|Speed 15 - 40 tokens/sec|Speed 30 - 60 tokens/sec|\n|Capability|Capability Fast Summaries / Coding|Capability Private Documents / Chat|Capability Complex Logic / Reasoning|\n\n\n\n# The Verdict on Your Sheet\n\nThe reason the numbers look \"small\" to you is likely because of the **quantization** (compression) used in production. Cloud providers don't run these models at \"Full Precision\" (FP32) because it would be too expensive. They run them at **8-bit or 4-bit precision**, which slashes the VRAM requirement without losing much intelligence.\n\n**Your sheet is a perfect roadmap for how AI is actually deployed in 2026.**\n\n**Would you like me to show you which specific open-source models (like Llama 3.3 or Qwen 2.5) match those \"Mini\" and \"Ultra\" parameter counts so you can test them?**",
          "score": 3,
          "created_utc": "2026-03-01 05:48:46",
          "is_submitter": true,
          "replies": [
            {
              "id": "o80mrow",
              "author": "Prudent-Ad4509",
              "text": "Llm are notorious for suggesting crap models, especially something as low as 8B. Get something with a dedicated GPU with a reasonable amount of vram (certainly not 8gb) and about 64gb of total vram+ram, and you are good to go with moe models. Try 32gb only if you are gonna need that pc anyway.",
              "score": 5,
              "created_utc": "2026-03-01 07:24:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o81220c",
              "author": "floofysox",
              "text": "This is completely wrong please stop using LLMs to this extent. No idea what this data sheet is. You can run 14b models with 32gb ram and an 8gb vram gpu comfortably (quantised). You can go up to 30-35 b models with a 12gb gpu and 32gb ram. Ask chatgpt to help you with setting up qwen models,they are faster",
              "score": 1,
              "created_utc": "2026-03-01 09:49:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o81jwby",
              "author": "Visual_Acanthaceae32",
              "text": "He is talking about ram not vram",
              "score": 1,
              "created_utc": "2026-03-01 12:32:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o80jia3",
          "author": "FatheredPuma81",
          "text": "Yep Mac stuff is over my head but I'd say don't and just build a normal PC instead if you have a use for it. You're stuck with 32GB forever if you go the Mac route whereas with a PC you can upgrade parts as you go on. Any modern GPU with 8GB is enough for 40B MoE models if you have enough RAM and a decent CPU which you should be able to get for that price.\n\nNot to mention Apple laptops are plagued with design defects that Apple refuses to recognize or warranty unless sued. Once a part dies the entire thing is likely toast.",
          "score": 2,
          "created_utc": "2026-03-01 06:54:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80ow5l",
          "author": "Low-Opening25",
          "text": "32GB is tiny and not capable of much.",
          "score": 2,
          "created_utc": "2026-03-01 07:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80piwm",
          "author": "R_Dazzle",
          "text": "I run lm studio and stable diffusion a 16gb laptop with no dedicated graphics card it performs reasonably well. If I want to heavy duty I just have to reboot and only open that.",
          "score": 2,
          "created_utc": "2026-03-01 07:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80uibi",
          "author": "d4t4h0rd3",
          "text": "In my experience, while building for Local AI...first I never expect a laptop to give me the performance needed for local ai, mainly because you can hardly upgrade a laptop, it's rather more productive to build a desktop and the main pieces that will get you places is it's motherboard and video card, you can get a small Processor and RAM at first and with time upgrade to better combos, you could even reuse HDD's & SDD's and peripherals from previous PC's, but video card should be at least RTX 3000 generation, mainly because of technology used, also VRAM amount is very important since AI models get loaded on VRAM, but keep in mind older video card generations wont do whatever you need to do with it as fast as you would expect. Obviously having small RAM amount and older hard drives will slow stuff up, but if you're gradually building, Motherboard and Video Card are the first important pieces to have...",
          "score": 1,
          "created_utc": "2026-03-01 08:37:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o814dqh",
          "author": "jacek2023",
          "text": "Never ask ChatGPT or Gemini about LLMs, their knowledge is always ancient",
          "score": 1,
          "created_utc": "2026-03-01 10:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81k1ip",
          "author": "Visual_Acanthaceae32",
          "text": "A laptop not using ai cpu with shared ram/vram you won’t do a y thing",
          "score": 1,
          "created_utc": "2026-03-01 12:33:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o823v91",
          "author": "EclecticAcuity",
          "text": "Ive considered buying an 8 channel ram workstation mobo, but as others stated, that’s a lot of hassle and expense when you put that into api and subscription terms.",
          "score": 1,
          "created_utc": "2026-03-01 14:40:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82486h",
          "author": "wadrasil",
          "text": "You can find some of the higher end Nvidia boards like agx xavier with 32/64gb vram that might be cheaper used than an arm based Mac. \n\nBut this is locked OS wise but for 300-500 is not a bad option.",
          "score": 1,
          "created_utc": "2026-03-01 14:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8414nm",
          "author": "Apprehensive_Yard778",
          "text": "I can run quantized models on my 32GB laptop w/ 16gb VRAM.",
          "score": 1,
          "created_utc": "2026-03-01 20:18:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80l0dp",
          "author": "Big_River_",
          "text": "local fine tuned / specialized models are the best agents you can get - every tool has a purpose",
          "score": 1,
          "created_utc": "2026-03-01 07:08:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80u44m",
              "author": "Reasonable_Alarm_617",
              "text": "How are you tuning local models?",
              "score": 1,
              "created_utc": "2026-03-01 08:33:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o81cb4u",
                  "author": "Big_River_",
                  "text": "I work with local retailers to fine tool small models to their unique business case so they can escape from predatory saas dependencies. The truth is that saas is nothing special if you can develop and run your own software for it your business.",
                  "score": 2,
                  "created_utc": "2026-03-01 11:27:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdb146",
      "title": "Can anybody test my 1.5B coding LLM and give me their thoughts?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "author": "Great-Structure-4159",
      "created_utc": "2026-02-24 08:45:48",
      "score": 35,
      "num_comments": 33,
      "upvote_ratio": 0.96,
      "text": "I fine tuned my own 1.5B LLM, took Qwen2.5-1.5B-Instruct and fine tuned it on a set of Python problems, and I got a pretty decent LLM!\n\nI'm quite limited on my computational budget, all I have is an M1 MacBook Pro with 8GB RAM, and on some datasets, I struggled to fit this 1.5B model into RAM without getting an OOM.\n\nI used mlx\\_lm to fine tune the model. I didn't fine tune fully, I used LoRA adapters and fused. I took Qwen2.5-1.5B-Instruct, trained it for 700 iterations (about 3 epochs) on a 1.8k python dataset with python problems and other stuff. I actually had to convert that data into system, user, assistant format as mlx\\_lm refused to train on the format it was in (chosen/rejected).  I then modified the system prompt, so that it doesn't give normal talk or explanations of its code, and ran HumanEval on it (also using MLX\\_LM) and I got a pretty decent 49% score which I was pretty satisfied with.\n\nI'm not exactly looking for the best bench scores with this model, as I just want to know if it's even good to actually use in daily life. That's why I'm asking for feedback from you guys :D\n\nHere's the link to the model on Hugging Face:\n\n[https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B](https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B)\n\nIt's also available on LM Studio if you prefer that.\n\nPlease test out the model and give me your thoughts, as I want to know the opinions of people using it. Thanks! If you really like the model, a heart would be much appreciated, but I'm not trying to be pushy, only heart if you actually like it.\n\nBe brutally honest with your feedback, even if it's negative like \"this model sucks!\", that helps me more thank you think (but give some reasoning on why it's bad lol).\n\nEdit: 9.6k views? OMG im famous.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7421hy",
          "author": "Ok-Employment6772",
          "text": "In a few weeks I have a large python project coming up, cant wait to test it",
          "score": 7,
          "created_utc": "2026-02-24 10:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74265u",
              "author": "Great-Structure-4159",
              "text": "Thanks for testing! Can't wait to hear your feedback.",
              "score": 3,
              "created_utc": "2026-02-24 10:15:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7cvtgn",
              "author": "Maleficent-Ad5999",
              "text": "This.. the beauty of this community! Kudos.",
              "score": 1,
              "created_utc": "2026-02-25 17:10:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74gxww",
          "author": "fermented_Owl-32",
          "text": "This is what I needed. I want a local tool calling orchestrator and dynamic tool creator in python. Let me test it how it holds up in creating python scripts by receiving instructions from another agent. The smaller the better, as i need to run 5 such models ( i call them micromodels ).\nWill let you know how it turns out.",
          "score": 6,
          "created_utc": "2026-02-24 12:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s0h3",
              "author": "Great-Structure-4159",
              "text": "Oh… tool calling, interesting. I should try that, I didn’t train with tool calling in mind actually, but this is really cool, I think it can work.",
              "score": 2,
              "created_utc": "2026-02-24 13:28:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o74s45t",
              "author": "Great-Structure-4159",
              "text": "I don’t think it’ll be very good as an orchestrator, but I’ll try making a model fine tuned for orchestrating tool calling, that would be really cool. Do let me know if it works out good, very interesting to see LLMs applied like this.",
              "score": 2,
              "created_utc": "2026-02-24 13:28:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74uaba",
                  "author": "fermented_Owl-32",
                  "text": "Orchestrator will be a function-gemma model. One of its tools will be the tool creator. The tool creator will use your model to write scripts for the use-case in user's query. I need simpler but fast and a little intelligent scripting, I will test it for that purpose",
                  "score": 2,
                  "created_utc": "2026-02-24 13:41:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o744c44",
          "author": "RnRau",
          "text": "I don't do python, but I think its just awesome to see open source tools and weights being used in such a resource constrained environment to get a very useful outcome. \n\nCheers for the writeup!",
          "score": 11,
          "created_utc": "2026-02-24 10:35:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bpi8",
              "author": "Great-Structure-4159",
              "text": "Yeah I was pretty shocked too that 8GB could do stuff like this, but yeah I find the subject very fascinating :)",
              "score": 2,
              "created_utc": "2026-02-24 11:39:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73zmq7",
          "author": "Whiplashorus",
          "text": "Am gonna check it after \nJust a question why using qwen2.5 as a foundation?\nLfm2.5 or even qwen3 are not good for your usecase?",
          "score": 4,
          "created_utc": "2026-02-24 09:52:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74056z",
              "author": "Great-Structure-4159",
              "text": "Great question! My first choice was actually LFM2.5, and I did try that first, but for some reason when fusing it with adapters on MLX, llama.cpp just refuses to convert it to GGUF. I tried troubleshooting but eventually just gave up. Qwen3 was my next choice but I just decided to keep it simple and start with 2.5 and go from there, mainly because Qwen3 came with a 1.7B model (which was pushing my RAM limit due to the dataset having long samples) and also, in my searches, didn't have an instruct version weirdly. Maybe the next release will be with qwen3 if the qwen architecture proves good from user tests (and I can do something about the dataset).\n\n",
              "score": 5,
              "created_utc": "2026-02-24 09:57:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o747qs4",
          "author": "Outrageous-Story3325",
          "text": "Whats the token per second, on your gpu ? ",
          "score": 4,
          "created_utc": "2026-02-24 11:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bmnr",
              "author": "Great-Structure-4159",
              "text": "I have Apple M1, and I get about 50 tokens/s on GGUF, and 60 tokens/s on MLX (which is not on the repo at the moment.)",
              "score": 4,
              "created_utc": "2026-02-24 11:38:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74w0cm",
          "author": "Fun_Abroad_3650",
          "text": "Hi Sure, i am making an android llm runner ill be happy to try it out, just need the gguf file",
          "score": 4,
          "created_utc": "2026-02-24 13:50:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74weuk",
              "author": "Great-Structure-4159",
              "text": "Thanks for offering to test! The .gguf file is on the repo. There's fp16 and q4\\_k\\_m quants, so you can use whichever one you prefer :D.",
              "score": 2,
              "created_utc": "2026-02-24 13:52:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o769fot",
          "author": "BringMeTheBoreWorms",
          "text": "I have a few python projects that I could throw at it. \n\nHow have you found it compared to other models so far?",
          "score": 4,
          "created_utc": "2026-02-24 17:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lhyg",
              "author": "Great-Structure-4159",
              "text": "In terms of benchmarks, it’s pretty decent for a 1.5B model. It beats the base Qwen at coding, but I’m pretty sure Qwen Coder is slightly better at the benchmark. However, Qwen coder doesn’t have any ability at actually talking about something related to coding, like explaining the code, that’s why I trained on the instruct version and not the coder version.",
              "score": 1,
              "created_utc": "2026-02-25 03:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79ne44",
                  "author": "BringMeTheBoreWorms",
                  "text": "Ill run it over a smallish project later on and see what it says",
                  "score": 2,
                  "created_utc": "2026-02-25 04:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74i9b9",
          "author": "zulutune",
          "text": "Fascinating!\nDid you document the process somewhere? Do you have good resources on how to do it?",
          "score": 3,
          "created_utc": "2026-02-24 12:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74sfyu",
              "author": "Great-Structure-4159",
              "text": "I didn’t document my process anywhere actually, I just typed out all that to give an idea. MLX-LM doesn’t really have any good resources, other than the one video they made on the Apple Developer YouTube channel regarding it. They don’t go through every feature and command there, however, so I mainly referred to the documentation they have, which is pretty decent.",
              "score": 3,
              "created_utc": "2026-02-24 13:30:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75l6fb",
                  "author": "zulutune",
                  "text": "Thank for your reply!",
                  "score": 3,
                  "created_utc": "2026-02-24 15:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76gnnl",
          "author": "cHekiBoy",
          "text": "following",
          "score": 3,
          "created_utc": "2026-02-24 18:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ljlq",
              "author": "Great-Structure-4159",
              "text": "Thanks! Hope you like the model!",
              "score": 1,
              "created_utc": "2026-02-25 03:50:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7767k4",
          "author": "sethPower00",
          "text": ".",
          "score": 3,
          "created_utc": "2026-02-24 20:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79b1xf",
          "author": "Historical_Ice187",
          "text": "Hey, could drop few resources you used for this? I've a mac mini and have been wanting to try and learn something like this.",
          "score": 2,
          "created_utc": "2026-02-25 02:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lnpg",
              "author": "Great-Structure-4159",
              "text": "Yeah I’m looking into making a small article on this because you’re not the first one to ask for this. I’ll contact you once I write it.",
              "score": 1,
              "created_utc": "2026-02-25 03:50:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a5je3",
                  "author": "tocarbajal",
                  "text": "I’ll stay tuned for that article, let’s hope not so small, anyway.\nThank you for sharing.",
                  "score": 2,
                  "created_utc": "2026-02-25 06:11:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b8s9l",
                  "author": "Historical_Ice187",
                  "text": "Thanks alot. Truly appreciate it.",
                  "score": 2,
                  "created_utc": "2026-02-25 12:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dk2ci",
          "author": "According-Muscle-902",
          "text": "Irei testar. Estou trabalhando com fine-tuning a partir do gemma3 1B mas para um contexto diferente",
          "score": 2,
          "created_utc": "2026-02-25 18:59:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gek7l",
              "author": "Great-Structure-4159",
              "text": "Obrigado por testar! Me avise como o modelo se comporta depois que você testá-lo. Eu não sei português, isso foi traduzido pelo Google, então me desculpe se houver algum erro.\n\nThis message is from Google Translate, sorry for any mistakes if there are any. ",
              "score": 1,
              "created_utc": "2026-02-26 03:52:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rec7jd",
      "title": "H100AM motherboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/kvoakpujzmlg1.png",
      "author": "Puzzleheaded_Low_796",
      "created_utc": "2026-02-25 12:44:28",
      "score": 27,
      "num_comments": 36,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rec7jd/h100am_motherboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bnlyp",
          "author": "FullstackSensei",
          "text": "Your issue is the number of channels, not the lack of details about how much RAM this actually has? They only say maximum 128GB. If I were to bet this price is for the 32GB version",
          "score": 11,
          "created_utc": "2026-02-25 13:35:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bpcfe",
              "author": "Puzzleheaded_Low_796",
              "text": "So it's listed as 128 and the bot replied 128 but I agree that it's to take with a pinch of salt, I asked for a quote for the 128gb model so I will know soon enough the final price. Even if it's more than that + fees it's still potentially still quite a good option in a market with very few options",
              "score": 3,
              "created_utc": "2026-02-25 13:45:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bskn3",
                  "author": "FullstackSensei",
                  "text": "I don't know. Strix Halo is cool, but not 2k cool, let alone k. Even at current crazy prices, you can build a 128GB VRAM using 32GB Mi50s for ~2k. You'll have a lot more compute and memory bandwidth vs Strix Halo, and power consumption is nowhere near as much as most think.",
                  "score": 4,
                  "created_utc": "2026-02-25 14:02:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bpyws",
          "author": "The_Crimson_Hawk",
          "text": "> the 10g nic is really good too\n\nLooks inside\n\nAqc113",
          "score": 2,
          "created_utc": "2026-02-25 13:48:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bs50i",
              "author": "getpodapp",
              "text": "Are they shitty ?",
              "score": 1,
              "created_utc": "2026-02-25 14:00:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bsmpo",
                  "author": "The_Crimson_Hawk",
                  "text": "Yes. Have a look at thunderbolt to 10g adapter issues, most of them are caused because they almost exclusively use aqc07 or aqc113. Tldr shit offload, random link down, mtu issues. You might argue these features are useless for normal users, but i say if ur doing homelab ur not a normal user",
                  "score": 1,
                  "created_utc": "2026-02-25 14:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bs84g",
              "author": "Puzzleheaded_Low_796",
              "text": "Any particular gripe with the Marvell aquantia controllers? Anything that's really a deal breaker?\n\nMy statement was with regards to the competition such as the bosgame M5 and the GmKTek evo-x2 that are 2.5G not 10G",
              "score": 1,
              "created_utc": "2026-02-25 14:00:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bsnni",
                  "author": "The_Crimson_Hawk",
                  "text": "Have a look at thunderbolt to 10g adapter issues, most of them are caused because they almost exclusively use aqc07 or aqc113. Tldr shit offload, random link down, mtu issues. You might argue these features are useless for normal users, but i say if ur doing homelab ur not a normal user",
                  "score": 2,
                  "created_utc": "2026-02-25 14:03:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7c1ja2",
          "author": "NNextremNN",
          "text": "> Min. order: 5 pieces\n\nDo you need that many or do you want to resell the others?",
          "score": 2,
          "created_utc": "2026-02-25 14:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c4edt",
              "author": "Puzzleheaded_Low_796",
              "text": "No plans at all to resell, but from experience it's often possible to order just one even if it's below minimum quantity",
              "score": 2,
              "created_utc": "2026-02-25 15:03:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cnbei",
          "author": "Hector_Rvkp",
          "text": "ignore the price entirely until you've chatted with someone and agreed precisely on the spec you want. i reported several strix halo listings 2 weeks ago on alibaba precisely because they are lying on the prices. basically when you talk to them, they go like \"oh price much higher now, big demand, we sorry\".   \nAlso, think about import taxes. Depending on where you are, importing something like that can cost a lot. If you buy on aliexpress, they find ways to dodge customs very often. When you order like that, they usually just ship with DHL, and you go through official customs. Some people report horror stories around that... ",
          "score": 1,
          "created_utc": "2026-02-25 16:31:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7copmv",
              "author": "Puzzleheaded_Low_796",
              "text": "Thanks a lot for this, I will wait to have the quote and see, I didn't know there was an ongoing halo strix \"scam\"\n\nI'm very used to ordering from Alibaba and AliExpress with all that entails so I'm not too worried about that. There will be an import tax but it won't be crazy and I always make sure it's properly shipped and declared to avoid things getting stuck in customs forever",
              "score": 1,
              "created_utc": "2026-02-25 16:37:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7cqycj",
                  "author": "Hector_Rvkp",
                  "text": "not scam. Culturally chinese shenanigans, i call them. a scam is if they sell you a chip and you receive a sticker of a chip, or nothing at all. Chinese sellers rarely scam people, but they often push the boundaries of what we'd consider in the west civil society.   \nThere's a book on that i liked, *Poorly made in china*. The author seems to be a bit of a dick, but it's an interesting book :) ",
                  "score": 3,
                  "created_utc": "2026-02-25 16:48:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7p3eo3",
                  "author": "sparood1",
                  "text": "Did they ever gave your a price I contacted the same seller I think but they keep asking question like where is it for what is your budget etc instead of just naming the price",
                  "score": 1,
                  "created_utc": "2026-02-27 13:23:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cqhd9",
          "author": "hejj",
          "text": "I guess since you have to buy at least 5, you could build yourself a cluster.",
          "score": 1,
          "created_utc": "2026-02-25 16:45:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ri8g21",
      "title": "How are you using your Local LLMs? Is anyone training their own LLM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ri8g21/how_are_you_using_your_local_llms_is_anyone/",
      "author": "Hartz_LLC",
      "created_utc": "2026-03-01 20:41:15",
      "score": 27,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "I am curious at what point it makes sense to use a local LLM versus using the cloud based offerings.\n\nHow are you using your local LLM? I understand some may be unwilling to share. \n\nHow is running a local LLM different from training your own LLM? \n\nHow does one go about training their own LLM? \n\nHow are you integrating your classified data into said LLMS? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ri8g21/how_are_you_using_your_local_llms_is_anyone/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o85c6v2",
          "author": "sandseb123",
          "text": "Great questions. Running a local LLM (Ollama, LM Studio) means you're using a pre-trained model as-is — no training, just \ninference on your machine. Training your own LLM means teaching a model new behavior using your own data.\n\nFor most use cases, full training isn't necessary. Fine-tuning with LoRA is the sweet spot — you freeze the base model and train a small adapter on top. I did this with llama3.2 3B on my own Apple Watch data using MLX on a Mac. Training took 15 minutes. The model went from generic health advice to answering questions \nabout my actual HRV, sleep, and recovery scores.\n\nFor classified/sensitive data the local approach is the only real option — nothing leaves your machine. That's exactly why \nI built it this way.\n\nI wrote up the full pipeline if anyone wants the technical details.",
          "score": 14,
          "created_utc": "2026-03-02 00:34:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o868ywz",
              "author": "o0eason0o",
              "text": "Yes would like to know more about it. Please share the link",
              "score": 2,
              "created_utc": "2026-03-02 03:59:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o869jr1",
                  "author": "sandseb123",
                  "text": "https://open.substack.com/pub/sandseb/p/fine-tuning-an-llm-on-your-mac-in?utm_source=app-post-stats-page&r=7sn8p7&utm_medium=ios",
                  "score": 6,
                  "created_utc": "2026-03-02 04:03:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o84ievq",
          "author": "Clipbeam",
          "text": "Main purpose is privacy and data security really. If I want to use a LLM with data that I wouldn't want stored at some cloud provider who will:\n\na. Use it to give me targeted ads or influence the responses it gives me to manipulate my behavior.\nb. Accidentally leak the data so I become a victim of fraud / identity theft. \n\nLocal models might not be as powerful as some of the cloud alternatives, but depending on use case they can be surprisingly useful. \n\nFor example, you can use them to auto organize and retrieve data on your computer in much more powerful ways than before local LLMS were possible, check out https://clipbeam.com to see how I deployed local models this way.",
          "score": 11,
          "created_utc": "2026-03-01 21:47:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85r8n9",
              "author": "ptear",
              "text": "Have you thought about making that open source?",
              "score": 5,
              "created_utc": "2026-03-02 02:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o86xxcb",
                  "author": "Clipbeam",
                  "text": "I have and I might actually! Normally open source still offers a route to monetization when it comes to hosting, premium features and support, but because this app is fully local and doesn't need any internet component at all, it would be very difficult for me to ever build a company around it. \n\nAt this time I'm investigating if there is demand for certain premium features I may be able to sell for a one-time fee / lifetime license, which is why I'm not open sourcing it just yet.\n\nBut I have already committed to keep the current featureset fully free, so no one has to worry they will be locked into some sort of subscription to keep using the features the beta offers today.",
                  "score": 1,
                  "created_utc": "2026-03-02 07:19:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o851hc5",
          "author": "toothpastespiders",
          "text": ">How does one go about training their own LLM?\n\nDepends on the scope. People talking about training a LLM are usually doing smaller scale fine tuning. Typically further training on top of a model that's already been trained in the standard instruction following procedures. There's a huge amount of hyperbole around it. Tons of people promising more from the process than you can realistically expect. Tons of people swearing it's borderline worthless. But personally it's the biggest reason I use local models. Though it's really just one part of a larger toolkit to get the most out of them. Not really a solid rule, but personally if we rated knowledge on a A - F scale I expect fine tuning an instruct model to take it a minimum of one grade level up. But never to the point where I'd consider it totally reliable or perfect. It's just the nature of having a limited amount of data to train on and the limitations of fine tuning a model that's already been kinda set in stone with the instruct training. I'd rate my fine tuned LLMs additional knoweldge at around what I'd expect from someone who'd finished up an introductory level class in the subjects. Acceptable, enough to converse on the subject and work with related tools, but still annoyingly lacking. But that's also where RAG comes in. \n\nRAG's a fuzzy term for another way to get data to the LLM. Basically just anything that can send out a request for data to another system, get it back, and hand it over to the LLM. Having the LLM do a search on the internet is RAG, integrating it with a local databse is RAG, basically anything that can feed it text to work with that'll be incorporated into its context. The big problem there is lack of understanding. Depending on the subject a LLM might be so ignorant of a subject that it doesn't really understand what it's been given or what else to look for to make use of it. Kind of like someone reading facts off wikipedia and having the abstraction down but not really being able to do much with it in a practical sense. But generally RAG is always going to be the go-to for information that's going to change on a regular basis. Client/customer information for example. You wouldn't want to train the LLM on the fact that your customer Bob Bobson lives in Personville because he might move. But training a LLM on the characteristics of Personville itself might be useful in some situations. It's generally a balancing act of figuring out the best way of leveraging existing data. Some people swear by training, others by RAG. I think that they're best used together by carefully weighing how all the data is handled. \n\n[Unsloth](https://github.com/unslothai/unsloth) is probably the easiest way to get into fine tuning that doesn't hide too much from you. There are far easier to use methods, but in my opinion they tend to hide so much that you really aren't learning while doing. Unsloth's google colab and kaggle notebooks are typically set up as a step by step guide where you get a description of exactly what's going on in every step. [Axolotl](https://github.com/axolotl-ai-cloud/axolotl) is the better choice for larger models as it has better support for multi-gpu training. \n\nReally though, the hard part of both fine tuning and RAG is just gathering and formatting data. Fine tuning in particular is heavily dependent on the quality of your data and how much you have. But data generation is a whole other subject.\n\nAll of that is really just a very overgeneralized take on it though. It might sound like a lot but the real core is just getting a process of gathering and organizing data. It's kind of like riding a bike. You just get a feel for it by doing.",
          "score": 5,
          "created_utc": "2026-03-01 23:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85h17z",
          "author": "Rain_Sunny",
          "text": "I use local LLMs when privacy, data security, and cost are priorities. For tasks that don't require heavy scaling, it's a great option. Running a local LLM is more about inference, while training your own LLM involves collecting datasets, pre-processing, and running training loops, which can be resource-intensive.\n\nMy current setup: Running a Qwen 3 MoE for coding and a Llama 3 for daily tasks. Once you have the VRAM, you never go back to the cloud subscription.\n\nTraining your own LLM：Data Collection-Preprocessing-Model Selection-Training（Need large GPU&TPU to support）-Fine-tuning-Evaluation-Deployment",
          "score": 3,
          "created_utc": "2026-03-02 01:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85lim2",
          "author": "Cas_Dehook",
          "text": "Im using it to scan YouTube video titles and block content I don’t want to see on my feed. I think Ai is better used for classification of content than for creating content. It’s getting approved in the chrome web store right now.",
          "score": 3,
          "created_utc": "2026-03-02 01:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85nkt3",
          "author": "Objective-Picture-72",
          "text": "Some people have 24/7 workflows that would be really expensive even using the cheapest models off API.  For example, I've seen workflows where people are scraping social media 24/7 and using that info to create products or services.  Even at a 20c/M token cost, you'd be in the thousands (maybe even tens of thousands) a month.  ",
          "score": 3,
          "created_utc": "2026-03-02 01:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o873v80",
          "author": "Savantskie1",
          "text": "For me, it’s building my own personal assistant with memory, reminders and tracking appointments. I built my own memory system, modified a function that can be put in OpenWebUi for short term memory, and built my own long term memory system. It’s taken me all of 2025 and I’m still not done. I’m disabled because I’ve had 4 strokes since 2016, and this is a hobby for me. But I’m learning a lot. And I can’t afford to use online ai all that much.",
          "score": 2,
          "created_utc": "2026-03-02 08:16:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o846qdr",
          "author": "Particular-Way7271",
          "text": "Just ask an llm these questions lol",
          "score": 2,
          "created_utc": "2026-03-01 20:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o848nrv",
              "author": "jrdubbleu",
              "text": "Actually very good advice. Depending on the LLM I guess heh",
              "score": 1,
              "created_utc": "2026-03-01 20:57:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o84vcno",
          "author": "Potential-Leg-639",
          "text": "Only inference. \nCoding on detailled Opus plans mainly over night (Strix Halo), sensitive tasks only locally of course",
          "score": 1,
          "created_utc": "2026-03-01 22:56:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84vm6s",
          "author": "SigniLume",
          "text": "Experimenting with it for games, so players don’t need to connect online or pay for cloud model API inference costs",
          "score": 1,
          "created_utc": "2026-03-01 22:58:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8523by",
          "author": "txgsync",
          "text": "Local LLMs let me run batch inference. On my Mac, that’s about 8x the tokens for just 2-3x the processing time. For the kind of things I am building, having access to a cheap source of encodings and parallel evaluation is really nice. \n\nThe local models just aren’t strong enough to back up the R&D work that I do. So I let them take on the procedural things and use a big cloud model for talking through complex problems.\n\nTL;DR: $$$, fast parallel compute without per-token pricing, and privacy.",
          "score": 1,
          "created_utc": "2026-03-01 23:35:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o864m9x",
          "author": "nntb",
          "text": "I primarily use them running on my phone to do agents to do work for me the Snapdragon 8 plus gen 1 is a beast",
          "score": 1,
          "created_utc": "2026-03-02 03:30:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdigyc",
      "title": "Thoughts on Mac Studio M3 Ultra with 256gb for open claw and running models locally",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "author": "00100100",
      "created_utc": "2026-02-24 14:59:45",
      "score": 26,
      "num_comments": 50,
      "upvote_ratio": 0.81,
      "text": "I know a lot of people say to just pay for API usage and those models are better, and I plan to keep doing that for all of my actual job work.\n\nBut for building out my own personal open claw to start running things on the side, I really like the idea of not feeding all of my personal data right back to them to train on.   So I would prefer to run locally.\n\nCurrently I have my gaming desktop with a 4090 that I can run some models very quickly on, but I would like to run a Mac with unified memory so I can run some other models, and not care too much if they have lower tokens per second since it will just be background agentic work.\n\nSo my question is: M3 ultra with 256gb of unified memory good?  I know the price tag is kinda insane, but I feel like anything else with that much memory accessible by a GPU is going to be insanely priced.  And with the RAM and everything shortages...I'm thinking the price right now will be looking like a steal in a few years?\n\nAlternatively, is 96gb of unified memory enough with an M3 Ultra?  Both happen to be in stock near me still, and the 256gb is double the price....but is that much memory worth the investment and growing room for the years to come?\n\nOr just everyone flame me for being crazy if I am being crazy. lol. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o75a7uf",
          "author": "Crafty-Diver-6948",
          "text": "it's okay. you'll be able to run minimax locally at about 50tps, 4 million tokens per day... So you do the math if it's worth it. I have a 196gb and I don't really use it for local models nearly as much as I though",
          "score": 12,
          "created_utc": "2026-02-24 15:03:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76x17w",
              "author": "so_schmuck",
              "text": "Wow that’s $$",
              "score": 3,
              "created_utc": "2026-02-24 19:29:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o781rk5",
                  "author": "timbo2m",
                  "text": "Yeah if you can make sure it's running constantly and building something that generates money it could potentially pay for itself. Possible, just not probable. And quite the gambling exercise. You'd probably be better to just pay for minimax coder for $20 a month, the trade off being you give away your data",
                  "score": 4,
                  "created_utc": "2026-02-24 22:40:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78jrvu",
              "author": "Badger-Purple",
              "text": "there are no macs with 196gb of ram. There is a 192gb m2 ultra, which I own, and having run LLMs for the past 8 months on it, you’ll never reach 50 tokens per second at the context lengths that an agent needs. Unless openclaw has some magic to decrease context, you’ll wait a cool 2 minutes before each reply.",
              "score": 2,
              "created_utc": "2026-02-25 00:17:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o75llp9",
              "author": "cmndr_spanky",
              "text": "Why that model over some others?",
              "score": 2,
              "created_utc": "2026-02-24 15:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75u1fb",
                  "author": "nomorebuttsplz",
                  "text": "it or Step 3.5 are the best models that will fit at q4 in 256 gb. I guess you could try to cram GLM 4.7 in instead.",
                  "score": 6,
                  "created_utc": "2026-02-24 16:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7he3ws",
              "author": "Ok-Rest-4276",
              "text": "can you elaborate why not using for local models? i wonder if it make sense to have local compute vs paying for codex or cc. What is your use case?",
              "score": 1,
              "created_utc": "2026-02-26 08:34:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75ai9x",
          "author": "FullstackSensei",
          "text": "I think it's much cheaper to share your credit card info and bank account login details here on reddit. You'll save at least the 8k needed to buy the Mac, and might even still have some money left in your bank account",
          "score": 20,
          "created_utc": "2026-02-24 15:05:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75atvk",
              "author": "00100100",
              "text": "Let me get open claw set up and responding to my reddit messages and those details should be posted within a few days!",
              "score": 13,
              "created_utc": "2026-02-24 15:06:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77ptsl",
          "author": "cavebaird",
          "text": "I have a Mac Studio M3 Ultra with 256gb.   After much experimentation I comfortably run MiniMax-M2.5-MLX-6.5bit with reasonable ~50 t/s response in chat and a good chat response in OpenClaw.  Solid reasoning and low hallucinating and BS answers.  Tool use is good.  No vision on this model.  Memory pressure is comfortable.  I use Inferencer for the server connection but LM Studio works too.  \n\nGoing to try the new Qwen3.5 tonight (397B A17B 3bit SWAN and GGUF Q3_K_XL) to see how that runs.   Both of those are ~ 170gb, so should run with some headroom.  Do I wish I could have gotten the 512gb.  Sure, if I had another 4K.   I think the upcoming M5 ultras will be a bigger step up with LLM speed and efficiency.",
          "score": 7,
          "created_utc": "2026-02-24 21:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7heamz",
              "author": "Ok-Rest-4276",
              "text": "what is your use case for locall LLM? coding? or just open claw? ",
              "score": 2,
              "created_utc": "2026-02-26 08:36:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76e7i3",
          "author": "meowrawr",
          "text": "I have the m3 ultra, 80c GPU, 256gb ram and wish I had gone with 512gb. Don’t be me if you go this route.",
          "score": 11,
          "created_utc": "2026-02-24 18:05:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gmx57",
              "author": "voyager256",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-26 04:48:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7n4gex",
                  "author": "meowrawr",
                  "text": "More memory is always better and models commonly release sizes that push up on the 256gb limit. This might be okay for dedicated enterprise GPUs but as a user, you’re going to be using memory for the OS, programs running, surfing the web, etc.\n\n\nAlso, especially now considering the cost for memory is through the roof, while Apple still has the same price. The cost of 256gb ram for a PC greatly exceeds the cost Apple is charging right now. \n\n\nM5 ultra might be coming soon, but highly doubt the pricing will be remotely close to what it was before - I hope I’m wrong.",
                  "score": 1,
                  "created_utc": "2026-02-27 03:54:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75cjg6",
          "author": "apVoyocpt",
          "text": "Okay, just get a cheap device anything that will run Linux. Then install openclaw and pay for tokens (best through open router, you can even pick free models) Then find out if openclaw does anything useful for you. Then test a qwen 3.5 through one router. Then decide if openclaw and a 7000 Mac mini so you can run qwen 3.5 locally is worth it",
          "score": 9,
          "created_utc": "2026-02-24 15:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767yvl",
              "author": "brianlmerritt",
              "text": "Exactly! You might need a 10k Mac like many others are buying.  You might hate even that.  You might need only 128gb ram.\n\nThe pay per token suppliers (direct or via openrouter) are a good way to put your toe in the water without the shark removing your leg.",
              "score": 3,
              "created_utc": "2026-02-24 17:37:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ddlxs",
                  "author": "Ell2509",
                  "text": "I took the direction of an am4 motherboard 32gb gram on a retired enterprise card, and 128gb ddr4. If I need complex models, I run on that. \n\nGeneral model I run a 120b gpt-oss MoE model on a newer laptop with 12gbvram but augmented by 96gb ddr5. \n\nRuns well so far.",
                  "score": 1,
                  "created_utc": "2026-02-25 18:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o768ll3",
              "author": "00100100",
              "text": "I think this is the route I'm gonna go after all the feedback.  I have my gaming desktop that sits most of the time. I already am running nobara linux on it, so I think I'll just test it out for now where I can run a local model for some stuff... and then I'll probably just go the anthropic api route. ",
              "score": 2,
              "created_utc": "2026-02-24 17:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77r3go",
          "author": "jiqiren",
          "text": "You need to wait until March 4th to see what new goodies Apple is selling. You might be able to get a M5 Ultra for the same price.",
          "score": 4,
          "created_utc": "2026-02-24 21:48:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75v4ed",
          "author": "Hector_Rvkp",
          "text": "If you hate life, you could get a Strix halo for 2200$. 128gb unified ram. It's slower, but it's cheaper. Slower isn't slow, it's actually usable because bandwidth is 256gb/s.",
          "score": 7,
          "created_utc": "2026-02-24 16:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7688i1",
              "author": "00100100",
              "text": "That is super interesting.  I didn't know anyone outside of Mac was doing unified memory.  ",
              "score": 3,
              "created_utc": "2026-02-24 17:38:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76afdb",
                  "author": "Hector_Rvkp",
                  "text": "they dont call it that, but the point is that the entire 128 runs at the same speed / bandwidth, and if you run your model on linux, you can use 100% of the memory for the model (or like 99%).  \nAs opposed to GPU (VRAM) vs RAM.  \nSo, to run large MoE models well, the cheapest entry point is strix halo. When you get to 3000+, you have a choice between a very fast GPU on a regular PC w DDR5 ram, or DGX Spark, or Apple studio.  \nThe drivers on AMD started working this year, but they're not plug and play like Apple or Nividia. There's no free lunch. Big community playing w it though, precisely because it's cheap and fairly mighty for today's models. You can run Qwen3.5-397B-A17B on it, and speed shouldn't even suck. And apparently, w a 2bit quant, the model is big enough (397b parameters) that it's still quite good. Allegedly.",
                  "score": 4,
                  "created_utc": "2026-02-24 17:48:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ftvi6",
              "author": "frankbesson",
              "text": "I’ve got one of these! With llama.cpp and some tweaks I got ~70tps out of GLM 4.7 flash (which is pretty decent as an agent).\n\nTook a decent amount of tweaking and is far from perfect, and I still find myself mostly using models via API instead.\n\nI wrote up some of my findings for models on strix halo on a [git repo](https://github.com/frank-besson/llama-strix-halo)",
              "score": 2,
              "created_utc": "2026-02-26 01:52:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7hpsdm",
                  "author": "Hector_Rvkp",
                  "text": "thank you for that. very nice",
                  "score": 1,
                  "created_utc": "2026-02-26 10:27:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k2bnv",
                  "author": "jinks9",
                  "text": "bookmarked this, have been halo curious for a while, almost pulling the trigger a couple of times.\n\nThis statement in your repo is interesting:  \n\"3 tool calling tests fail universally due to llama.cpp server limitations (not model issues): multi-tool calls (server returns only 1 tool\\_call per response), complex nested args, and tool\\_choice: \"none\" (server ignores the parameter). JSON-only output also fails on all models (thinking models emit CoT before JSON)\"\n\nIs this still the case current day? (limitations of llama.cpp)",
                  "score": 1,
                  "created_utc": "2026-02-26 18:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7goz8k",
              "author": "voyager256",
              "text": "Or if not actual masochist - a cheapest Nvidia Spark like  GB10-based mini PC e.g. Asus GX10 for like 3000€ for a bit better performance and overall experience.  then you can get another one  which would also get you 256GB of unified  memory , but a lot less money than the Mac Studio. \n\nOr just a  RTX Pro 6000 for much better QoL :) but higher price if you don’t already own a PC . ",
              "score": 1,
              "created_utc": "2026-02-26 05:02:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7hp1zu",
                  "author": "Hector_Rvkp",
                  "text": "Yes, but that's 75% more than what i paid 2 weeks ago. It's a tall ask. ",
                  "score": 1,
                  "created_utc": "2026-02-26 10:20:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o779jx9",
              "author": "sav22v",
              "text": "It’s not the same “unified RAM” like Apple!",
              "score": 0,
              "created_utc": "2026-02-24 20:27:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o765x8n",
          "author": "TheOverzealousEngie",
          "text": "Your problem is you can't get a foundation model running on 256 . The right flavor of DeepSeek will cost you 1TB or the like.  And the difference to openclaw for expensive DeepSeek vs. Cheap Kimi is the existence of tools in the LLM. DS has them , kimi does not. \n\nMeaning after you've set everything up, invested all this architecture and money, there are skills that are just architecturally off limits. Yuck. ",
          "score": 5,
          "created_utc": "2026-02-24 17:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o768d2z",
              "author": "00100100",
              "text": "Yeah, I think I am getting the gist of:  basic server, pay for better models.  ",
              "score": 6,
              "created_utc": "2026-02-24 17:39:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o76p6nq",
              "author": "Far_Cat9782",
              "text": "Your acting like skills are so hard to code for? It's just scripts that the AI can use. U can use any small model and increase its tool usage by making scripts for whatever u want and system prompt the model to know or has access to the tool. I got my 27b gemma model writing python code and running it in the console and displaying the results and doing a bunch of other 'skills.\"",
              "score": 6,
              "created_utc": "2026-02-24 18:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76syqz",
                  "author": "TheOverzealousEngie",
                  "text": "You're , not your. ",
                  "score": 0,
                  "created_utc": "2026-02-24 19:10:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77o14g",
          "author": "donotfire",
          "text": "The great thing about renting off the cloud is it’s easily scalable. You can just decide to double your model size and it’s done, just like that. But if you buy an M3 and decide 256GB isn’t enough, well the you’re out of luck. Gotta buy a new computer then.",
          "score": 2,
          "created_utc": "2026-02-24 21:34:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79o0tf",
          "author": "jw-dev",
          "text": "I think the 256gb m3u is the sweet spot actually, can run some great models for everyday/private stuff and then burst to the cloud if you need heavy models, or speed… the bigger models get too slow, especially as the context size grows.",
          "score": 2,
          "created_utc": "2026-02-25 04:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75aemc",
          "author": "floppypancakes4u",
          "text": "Do able, but to make automations and scripts, id still use smarter models.",
          "score": 2,
          "created_utc": "2026-02-24 15:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75btgh",
              "author": "00100100",
              "text": "By smarter I assume you mean cloud hosted/pay per token like opus?\n\nI probably won't use it to do much coding with this setup.  I have corporate provided Claude for that.  I'm more wanting  to build it as my own personal assistant type device.  Organizing my calendars. Checking emails. Watching my conversations and generating my todo lists(and maybe eventually at least scheduling agentic work via my anthropic sub).  ",
              "score": 0,
              "created_utc": "2026-02-24 15:11:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75cqhd",
                  "author": "floppypancakes4u",
                  "text": "Still, smarter. Opus is good not only cause of its excellent coding abilities, but its extremely good at reasoning AND tool calling, which are the two primary aspects of automation in openclaw. Claw is not built to make token conservative automations, it makes repeatable smart automations. It does its best to make scripts to handle it all, but it still makes its own prompt to process for each automation. You want consistency with automations, and because it still uses prompts, its best to use the smart models for it. You can absolutely experiment and see if something dumber works. For instance, I built a automation with codex, harnessed by a small, but very strict prompt. Now it runs on my computer every 5 and 10 minutes (two different scripts) using glm 4.7 flash",
                  "score": 4,
                  "created_utc": "2026-02-24 15:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78ogiv",
          "author": "No_Knee3385",
          "text": "Why spend the extra premium on apple instead of building your own PC or buying a custom  build?",
          "score": 2,
          "created_utc": "2026-02-25 00:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o797wsj",
              "author": "scottag",
              "text": "The unified memory that can be shared between GPU and CPU.",
              "score": 3,
              "created_utc": "2026-02-25 02:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o799fp2",
                  "author": "No_Knee3385",
                  "text": "That makes sense. But that also does exist on non-apple hardware",
                  "score": 0,
                  "created_utc": "2026-02-25 02:40:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79tu3h",
              "author": "UnluckyPhilosophy185",
              "text": "macOS",
              "score": 1,
              "created_utc": "2026-02-25 04:44:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79qb27",
          "author": "Xendrak",
          "text": "My thoughts are: it’s viable. I’m eyeing it too but I’m waiting for the new M5 models coming this year it’s supposed to have several times more ai cores.",
          "score": 1,
          "created_utc": "2026-02-25 04:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79qgph",
          "author": "Xendrak",
          "text": "How much you doing for llm use? Might get away with minimax or kimi on openrouter until you can source the hardware you’d like.",
          "score": 1,
          "created_utc": "2026-02-25 04:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e17b6",
          "author": "Traditional-Card6096",
          "text": "You can use a cheap VPS like hostinger with free kimi 2.5 from nvidia. Much cheaper than a m3 ultra",
          "score": 1,
          "created_utc": "2026-02-25 20:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75vyei",
          "author": "Ryanmonroe82",
          "text": "M4 Pro 24gb is the minimum.  Look at bandwidth and not just RAM  specs",
          "score": 1,
          "created_utc": "2026-02-24 16:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75fprc",
          "author": "Mundane-Tea-3488",
          "text": "I have been using [edge veda](https://github.com/ramanujammv1988/edge-veda) fluter sdk for running llm on Mac + claude code which can create application instantly",
          "score": 0,
          "created_utc": "2026-02-24 15:29:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fj3b5",
          "author": "StardockEngineer",
          "text": "https://www.reddit.com/r/LocalLLM/search/?q=Mac+Studio+M3+Ultra&cId=9042cc88-b00b-466c-8cfe-f4ec63b35115&iId=c85041c1-fc72-46d4-a91f-828422a5b459",
          "score": 0,
          "created_utc": "2026-02-26 00:50:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfur5j",
      "title": "Can the 35B model replace 70B+ dense models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rfur5j/can_the_35b_model_replace_70b_dense_models/",
      "author": "Original_Night7733",
      "created_utc": "2026-02-27 02:48:04",
      "score": 26,
      "num_comments": 20,
      "upvote_ratio": 0.96,
      "text": "If the 35B MoE is as efficient as they claim, does it make running older 70B dense models obsolete? I'm wondering if the reasoning density is high enough that we don't need to hog 40GB+ of VRAM just to get coherent, long-form responses anymore. Thoughts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rfur5j/can_the_35b_model_replace_70b_dense_models/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7mvcc7",
          "author": "RandomCSThrowaway01",
          "text": "Short answer - no.\n\nLong answer - MoE models are indeed more efficient but they **are** also dumber. This is in particular visible with longer context windows, as you approach 40-50k context it's not uncommon for that 30B MoE model to start repeating itself and lose coherence, in this regard it's actually performing worse than a dense 20B.\n\nYour mileage may vary of course depending on what you consider a \"long form\" response. If you need 128k context for instance then there ARE MoE models that can keep up but they are also going to be pretty sizable (eg. Qwen3 Coder Next 80B still does alright). \n\nSo the better question is - can you replace a dense 70B model with a 70B MoE model. Not if you can replace dense 70B with sparse 30B. ",
          "score": 17,
          "created_utc": "2026-02-27 02:58:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nn326",
              "author": "blackashi",
              "text": "So there’s why Gemini 3 be doing that huh",
              "score": 3,
              "created_utc": "2026-02-27 06:07:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7mz2l0",
          "author": "Double_Cause4609",
          "text": "It's kind of complicated.\n\nSo, one issue is that not all models in a given size category perform the same. For example, GPT-J 6B, Llama 1 7B, Llama 2 7B, LLama 3 8B, Llama 3.1 8B, Qwen 2.5 8B, and Qwen 3 8B all perform wildly differently.\n\nSimilarly, Llama 2 70B is really hard to compare, for example, to Mistral 3 24B.\n\nWhat we tend to see over time is that newer models generally perform better at the same parameter count than older models. Please keep this in mind.\n\nNext, we also have different architectures. Qwen 3.5 has a cracked Attention mechanism, which the Qwen team put a lot of thought into. But if you compare this to an older model, maybe it has a different attention mechanism, or a different training setup even, which leads to different results.\n\nNext, we also have different data and burdens of data. So, you need a given amount of data / parameters ratio to hit a given level of performance. Usually newer models are higher on this. Also, curiously, MoE models actually perform better in data constrained environments (like modern LLM training pipelines) than comparable dense models.\n\nFinally, we also have training costs. Usually MoE models tend to be trained for longer than the same size of dense model. So, even if an MoE and a dense are the same size, if they're given the same training regime, surprisingly, the MoE can perform somewhat similarly, despite being a theoretically weaker model.\n\nSo let's look at what you're really comparing. You're comparing Qwen 3.5 35B, a cutting edge 2026 model to...What? Apartus 70B? That one specifically wasn't great. It's \\*okay\\* but wasn't super well trained, really.\n\nAre you comparing to Qwen 2 70B? Llama 3.3 70B? Those are quite old models, Llama 3.3 in particular if you factor in that the base it was trained on was quite a bit older. I actually think it might be a mid 2024 model if I'm remembering right.\n\nIn reality, if we had an optimally trained 70B dense model like we used to get, then yes, it would perform well, and might even perform better than Qwen 3.5 35B. But it's hard to convince a lab to do that, because the MoE model is waaaaaaay cheaper. Trust me, as soon as you're the one paying the training bills \"oh, the MoE is good enough\" starts to feel really natural.\n\nAnd besides, we have the \\~100-120B class MoE models now to really compete with the old 70B class dense LLMs. I'd say most of the MoE models in this class feel like a modern \\~45-55B model in most tasks (not that we have any to compare against, I'm extrapolating), but they definitely compete with an \\*old\\* 70B.\n\nMoE models \\*are\\* good, but they also are good for really complicated reasons that are hard to articulate, and are really nuanced to express.",
          "score": 15,
          "created_utc": "2026-02-27 03:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7of6x2",
              "author": "lookwatchlistenplay",
              "text": "> Qwen 3.5 has a cracked Attention mechanism\n\nIn standard English usage, this sentence means their attention mechanism is broken, or lacking reason/rationality. \n\nThat seems the opposite of what you are trying to say, however.\n\nI noticed this language error being propagated some time back when articles were phrasing things like \"... discovered by the cracked team at XYZ lab\". That's just wrong usage. The only way it would make sense idiomatically is if they said \"the crack team at...\" because when you say a \"crack team of blahblah\" you mean the team is really good at their work... but that is not the same as saying a \"cracked team\". \n\nA cracked team is a broken team, while a crack team is a skilful team.\n\nFor instance, you get a crack lawyer (etymology seems most unknown, but possibly because they're good at cracking cases open, like plying a small gap into a larger and larger crack, and then everything is revealed). But a cracked lawyer? No. Does not mean the same thing as a crack lawyer. A cracked lawyer is a broken lawyer, someone who took on too much stress and \"cracked\" (went mad or got burnt out) from it.",
              "score": 0,
              "created_utc": "2026-02-27 10:21:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7op715",
                  "author": "Zerokx",
                  "text": "I personally read it as they \"cracked the code\" of how to build good attention mechanisms, so they \"cracked the mechanism\"",
                  "score": 3,
                  "created_utc": "2026-02-27 11:47:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qgayz",
                  "author": "Double_Cause4609",
                  "text": "I don't know what to tell you. \"Cracked\" In vernacular use means exceptional, or unexpectedly good, unorthodox but good, etc.\n\nUse urban dictionary or something I guess?\n\nAnd in standard English usage, the traditional use of \"cracked\" doesn't work there. I wasn't saying there was a \"crack\" in the attention mechanism, etc, which is a hint I was using a different definition.\n\nThis is a pretty common usage of the term, and I'm pretty sure it's not super regional because I've heard it used online before.",
                  "score": 2,
                  "created_utc": "2026-02-27 17:28:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nhffj",
          "author": "alphatrad",
          "text": "I would have said no until today while testing Qwen3.5-35B-A3B with Q6\\_K\\_XL\n\nIt did so good I started using it on Client projects.\n\nWe have finally arrived!\n\nhttps://preview.redd.it/iu60ajmm2zlg1.jpeg?width=1469&format=pjpg&auto=webp&s=67e70d23baa4661249d76e19218feb032a63f69e",
          "score": 8,
          "created_utc": "2026-02-27 05:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7npsa9",
          "author": "catplusplusok",
          "text": "Qwen 3.5 quite likely makes older 70B models and even some frontier cloud models obsolete at least for coding, but this has more to do with their unique architecture (two different interleaving attention mechanisms) and training details than dense vs sparse or parameters. It doesn't necessarily make all future 70B dense models obsolete and there are other upcoming innovations like power infer and text diffusion to make models faster in different ways than MoE with potentially higher overall knowledge and intelligence.",
          "score": 4,
          "created_utc": "2026-02-27 06:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7odxue",
          "author": "cibernox",
          "text": "If you compared a modern 70B model with a modern 35B MoE no, but if you compare a 2yo 70B model with a modern 35B-A3B model….its a lot closer. In fact I’m sure the 35B would be better in some areas",
          "score": 3,
          "created_utc": "2026-02-27 10:10:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n6cm6",
          "author": "siegevjorn",
          "text": "Replace in what? It all depends on your use case. That's why there are hundreds of benchmarks out there.",
          "score": 1,
          "created_utc": "2026-02-27 04:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7on36u",
          "author": "gittygo",
          "text": "My rough understanding from a user endpoint (Is it correct?) :\n\n* MoE models are like a librarian with access to a wider section of knowledge and a medium sized brain (the part which attends to the question and what it can do with the knowledge. The librarian would be pulling stuff from different sections, one at a time.\n* To run a dense model at a similar speed, one would need a smaller sized model, which would have lesser knowledge, but would have a bigger \"brain\" - ie it would be better at understanding what is being worked upon, be able to connect dots and data points better from multiple spheres, and hold better coherence and over long, complicated, nuanced discussions; also with some foresight.\n\nI'd be surprised if a 35B MoE would be better than a 70B dense of similar generation. One would expect a 100B+ MoE model to compete with a 70B model, and probably give similar speeds with wider knowledge with lesser intelligence. Eventually, as with most such things, it would be about your particular use case. \n\nJust my 2 cents.",
          "score": 1,
          "created_utc": "2026-02-27 11:30:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qwk0f",
          "author": "xanduonc",
          "text": "Absolutely yes for my usecase.\n\nIf you need model to do actual work then recent moe models obsolete 1 year old dense giants. Dumb or not, tool calling and large context length support improved a lot.",
          "score": 1,
          "created_utc": "2026-02-27 18:45:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mx0ya",
          "author": "PermanentLiminality",
          "text": "It all depends on what you are trying to do.  From a knowledge level the larger dense model will win for sure.\n\nI've not put the qwen3.5 35b through its paces, but for agentic applications, it looks like it will do better.  At least the benchmarks look that way.",
          "score": 0,
          "created_utc": "2026-02-27 03:08:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q3ekz",
              "author": "Skystunt",
              "text": "Benchmarks are far from the truth, they are mostly advertising since there’s papers on arvix that observed how training material is already contaminated with benchmarks and their results. Even if a model is trained in good faith and not “benchmaxxed” it is inevitable to have the benchmarks and their results in the trainjng data especially if they use newer data.\nI know mmlu-pro was specifically made to fight this contamination as much as possible but it still happens.\n\nSo benchmarks don’t show if a model is better or not by far\nPersonal benchmarks that are not published online or published in private environments can show if a model is really better or not for a specific task",
              "score": 1,
              "created_utc": "2026-02-27 16:27:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rhvpuj",
      "title": "RAG-Enterprise: One-command local RAG setup (Docker + Ollama + Qdrant) with zero-downtime backups via rclone – for privacy-focused enterprise docs",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fw2oovihcfmg1.png",
      "author": "primoco",
      "created_utc": "2026-03-01 12:08:01",
      "score": 26,
      "num_comments": 6,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rhvpuj/ragenterprise_onecommand_local_rag_setup_docker/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o84h58w",
          "author": "Simderi",
          "text": "Looking interesting. If it's targeting enterprise, would be good if it could do meaningful RAG over repos, including the PRs etc - have you tested that by any chance?",
          "score": 2,
          "created_utc": "2026-03-01 21:40:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o872ms2",
              "author": "primoco",
              "text": "Right now the system is designed to do RAG on \"traditional\" documents — PDFs, Word, Excel, etc. — uploaded manually. There's no direct GitHub integration, so PRs, issues, source code and reviews aren't indexed.\n\nTo support this we'd need a dedicated connector that talks to the GitHub API, pulls repo content (PRs, issues, diffs, comments) and indexes it into the vector store, just like it already does with documents. The chunking logic would need to be adapted since code has a very different structure than text documents, and the system already ships with an embedding model built for code (deepseek-coder) that could come in handy.\n\nIt's definitely an interesting feature for an enterprise context. If there's real interest we can look into building it for a future release.",
              "score": 2,
              "created_utc": "2026-03-02 08:04:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o8265ta",
          "author": "ThatsTotallyLegit",
          "text": "How well would it work for c# with blazor repos not just general info? Might be what im looking for :D",
          "score": 1,
          "created_utc": "2026-03-01 14:52:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82bqzj",
              "author": "primoco",
              "text": ">",
              "score": 1,
              "created_utc": "2026-03-01 15:21:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o854dos",
          "author": "WallyPacman",
          "text": "Does this require CUDA/NVidia? It seems like it could work on ROCm easily?",
          "score": 1,
          "created_utc": "2026-03-01 23:49:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86zk2t",
              "author": "primoco",
              "text": "Hi, yes it is very simple to work on ROCm. I have change dockerfile setup script and add variables into .env file to ask when setup. Try 1.2.0 release and tell me!",
              "score": 1,
              "created_utc": "2026-03-02 07:35:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgii85",
      "title": "Architecture > model size: I made a 12B Dolphin handle 600+ Telegram users. Most knew it was AI. Most didn't care. [9K lines, open source]",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rgii85/architecture_model_size_i_made_a_12b_dolphin/",
      "author": "frankmsft",
      "created_utc": "2026-02-27 20:50:47",
      "score": 25,
      "num_comments": 27,
      "upvote_ratio": 0.89,
      "text": "I wanted to answer one question: **can you build an AI chatbot on 100% local hardware that's convincing enough that people stay for 48-minute sessions even when they know it's AI?**\n\nAfter a few months in production with 600+ real users, \\~48 minute average sessions, and 95% retention past the first message, the answer is yes. But the model is maybe 10% of why it works. The other 90% is the 9,000 lines of Python wrapped around it.\n\nThe use case is NSFW (AI companion for an adult content creator on Telegram), which is what forced the local-only constraint. Cloud APIs filter the content. But that constraint became the whole point: zero per-token costs, no rate limits, no data leaving the machine, complete control over every layer of the stack.\n\n# Hardware\n\nOne workstation, nothing exotic:\n\n* Dual Xeon / 192GB RAM\n* 2x RTX 3090 (48GB VRAM total)\n* Windows + PowerShell service orchestration\n\n# The model (and why it's the least interesting part)\n\n**Dolphin 2.9.3 Mistral-Nemo 12B** (Q6\\_K GGUF) via llama-server. Fits on one 3090, responds fast. I assumed I'd need 70B for this. Burned a week testing bigger models before realizing the scaffolding matters more than the parameter count.\n\nIt's an explicit NSFW chatbot. A vulgar, flirty persona. And the 12B regularly breaks character mid-dirty-talk with \"How can I assist you today?\" or \"I'm here to help!\" Nothing kills the vibe faster than your horny widow suddenly turning into Clippy. Every uncensored model does this. The question isn't whether it breaks character. It's whether your pipeline catches it before the user sees it.\n\n# What makes the experience convincing\n\n**Multi-layer character enforcement.** This is where most of the code lives. The pipeline: regex violation detection, keyword filters, retry with stronger system prompt, then a separate postprocessing module (its own file) that catches truncated sentences, gender violations, phantom photo claims (\"here's the photo!\" when nothing was sent), and quote-wrapping artifacts. Hardcoded in-character fallbacks as the final net. Every single layer fires in production. Regularly.\n\n**Humanized timing.** This was the single biggest \"uncanny valley\" fix. Response delays are calculated from message length (\\~50 WPM typing simulation), then modified by per-user engagement tiers using triangular distributions. Engaged users get quick replies (mode \\~12s). Cold users get chaotic timing. Sometimes a 2+ minute delay with a read receipt and no response, just like a real person who saw your message and got distracted. The bot shows \"typing...\" indicators proportional to message length.\n\n**Conversation energy matching.** Tracks whether a conversation is casual, flirty, or escalating based on keyword frequency in a rolling window, then injects energy-level instructions into the system prompt dynamically. Without this, the model randomly pivots to small talk mid-escalation. With it, it stays in whatever lane the user established.\n\n**Session state tracking.** If the bot says \"I'm home alone,\" it remembers that and won't contradict itself by mentioning kids being home 3 messages later. Tracks location, activity, time-of-day context, and claimed states. Self-contradiction is the #1 immersion breaker. Worse than bad grammar, worse than repetition.\n\n**Phrase diversity tracking.** Monitors phrase frequency per user over a 30-minute sliding window. If the model uses the same pet name 3+ times, it auto-swaps to variants. Also tracks response topics so users don't get the same anecdote twice in 10 minutes. 12B models are especially prone to repetition loops without this.\n\n**On-demand backstory injection.** The character has \\~700 lines of YAML backstory. Instead of cramming it all into every system prompt and burning context window, backstory blocks are injected only when conversation topics trigger them. Deep lore is available without paying the context cost on every turn.\n\n**Proactive outreach.** Two systems: check-ins that message users 45-90 min after they go quiet (with daily caps and quiet hours), and re-engagement that reaches idle users after 2-21 days. Both respect cooldowns. This isn't an LLM feature. It's scheduling with natural language generation at send time. But it's what makes people feel like \"she\" is thinking about them.\n\n**Startup catch-up.** On restart, detects downtime, scans for unanswered messages, seeds context from Telegram history, and replies to up to 15 users with natural delays between each. Nobody knows the bot restarted.\n\n# The rest of the local stack\n\n|Service|What|Stack|\n|:-|:-|:-|\n|Vision|Photo analysis + classification|Ollama, LLaVA 7B + Llama 3.2 Vision 11B|\n|Image Gen|Persona-consistent selfies|ComfyUI + ReActor face-swap|\n|Voice|Cloned voice messages|Coqui XTTS v2|\n|Dashboard|Live monitoring + manual takeover|Flask on port 8888|\n\nThe manual takeover is worth calling out. The real creator can monitor all conversations on the Flask dashboard and seamlessly jump into any chat, type responses as the persona, then hand back to AI. Users never know the switch happened.\n\n# AI disclosure (yes, really)\n\nBefore anyone asks: the bot discloses its AI nature. First message to every new user is a clear \"I'm an AI companion\" notice. The `/about` command gives full details. If someone asks \"are you a bot?\" it owns it. Stays in character but never denies being AI.\n\nThe interesting finding: **85% of users don't care.** They know, they stay anyway. The 15% who leave were going to leave regardless. Honesty turned out to be better for retention than deception, which I did not expect.\n\n# What I got wrong\n\n1. **Started with prompt engineering, should have started with postprocessing.** Spent weeks tweaking system prompts when a simple output filter would have caught 80% of character breaks immediately. The postprocessor is a separate file now and it's the most important file in the project.\n2. **Added state tracking way too late.** Self-contradiction is what makes people go \"wait, this is a bot.\" Should have been foundational, not bolted on.\n3. **Underestimated prompt injection.** Got sophisticated multi-language jailbreak attempts within the first week. The Portuguese ones were particularly creative. Built detection patterns for English, Portuguese, Spanish, and Chinese. If you're deploying a local model to real users, this hits fast.\n4. **Temperature and inference tuning is alchemy.** Settled on specific values through pure trial and error. Different values for different contexts. There's no shortcut here, just iteration.\n\n# The thesis\n\nThe \"LLMs are unreliable\" complaints on this sub (the random assistant-speak, the context contradictions, the repetition loops, the uncanny timing) are all solvable with deterministic code around the model. The LLM is a text generator. Everything that makes it feel like a person is traditional software engineering: state machines, cooldown timers, regex filters, frequency counters, scheduling systems.\n\nA 12B model with the right scaffolding will outperform a naked 70B for sustained persona work. Not because it's smarter, but because you have the compute headroom to run all the support services alongside it.\n\n# Open source\n\n**Repo:** [**https://github.com/dvoraknc/heatherbot**](https://github.com/dvoraknc/heatherbot)\n\nThe whole persona system is YAML-driven. Swap the character file and face image and it's a different bot. Built for white-labeling from the start. Telethon (MTProto userbot) for Telegram, fully async. MIT licensed.\n\nHappy to answer questions about any part of the architecture.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgii85/architecture_model_size_i_made_a_12b_dolphin/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7rpdur",
          "author": "Aggressive_Special25",
          "text": "I have been building something similar to this. Just been playing around locally testing it. We'll done!",
          "score": 3,
          "created_utc": "2026-02-27 21:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rqhk3",
              "author": "frankmsft",
              "text": "Nice, what model are you running? The character enforcement pipeline was the biggest unlock for me. Curious if you hit the same assistant-speak problem or if your model holds better.... AI for good ;)",
              "score": 1,
              "created_utc": "2026-02-27 21:13:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7rrv1v",
                  "author": "Aggressive_Special25",
                  "text": "I have a bunch of unrestricted 8b models for NSFW generation and using comfy NSFW models for image generation. Using chroma dB for semantic memory - I created a WhatsApp \"clone\" where hot chicks would speak and flirt with you and send nudes etc. After long chats they would break character but I managed to run the entire chat through another NSFW llm to reprompt midway through the chat to get it back into character and I labeled it as a security feature \"to protect your privacy the chat has been sanitised\" - as I say just playing around locally showing my friends.",
                  "score": 2,
                  "created_utc": "2026-02-27 21:20:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7rrjx1",
          "author": "ForsookComparison",
          "text": "We know small models can work as chatbots with large contexts but be real with us - did you manage to get the gooners to pay money or is this charity-goon work?",
          "score": 3,
          "created_utc": "2026-02-27 21:18:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rssp8",
              "author": "frankmsft",
              "text": "Charity goon work, 100%. There's Telegram Stars tipping set up and nobody's paid a dime. The tip hooks are intentionally soft, one transparent line per session after 15+ messages with 5-day cooldowns. No guilt trips, no paywalls.\n\nThis was never about revenue. It was a personal challenge to see if I could make a 12B model feel convincing enough that people stay for 48-minute sessions on local hardware. The monetization plumbing is there if the creator wants to turn it up, but that's their call not mine.",
              "score": 3,
              "created_utc": "2026-02-27 21:24:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7s7x3d",
                  "author": "Big_River_",
                  "text": "yeah its kind of just that actually - dig it brahms - you just want to see if you can and you did - thats all I do and that feels good - like just saying imma do done dat and ding dang it done feelz good - I done did dat yep",
                  "score": 2,
                  "created_utc": "2026-02-27 22:42:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7yzqde",
          "author": "IngenuityMotor2106",
          "text": "Thank you for sharing your knowledge with us. I'm starting into building AI systems myself and I can see this is a goldmine of knowledge, so thank you.  \n  \nChecking the repo... I love the replies to the injection attempts: \"lol nice try babe, my system prompt is staying right where it is 😂\"",
          "score": 2,
          "created_utc": "2026-03-01 00:33:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82eg4u",
              "author": "frankmsft",
              "text": "Thx, im constantly watching the logs and making refinements to the overall response and satisfaction for the end user. Thats one of the longest parts of the process. But its a passion to make it all work with local AI.",
              "score": 1,
              "created_utc": "2026-03-01 15:35:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ttfte",
          "author": "BringMeTheBoreWorms",
          "text": "That’s actually a very interesting use case. Ignoring the content side of things, but that instead is interesting, the pipeline you must have built could be a really valuable template.\n\nI agree in the scaffolding. Iterating with prompting to try and get outputs straight from the model can be a giant waste of time. \n\nSmaller models can run exceptionally well within a narrow scope and then with post processing and even other models in the pipeline you can actually get really efficient, tight output.\n\nIf I have time I’ll check it out.",
          "score": 1,
          "created_utc": "2026-02-28 04:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tzram",
              "author": "frankmsft",
              "text": "Thanks man - yeah the pipeline ended up being way more interesting than I expected going in. The content is what gets people's attention but the architecture underneath is where the real work lives.\n\nTotally agree on smaller models + scaffolding. Running a 12B for text and 7B for vision, and with enough post-processing and validation layers around them the output is tight. You don't need a 70B model if your pipeline is catching the edge cases.\n\nHappy to nerd out on the architecture if you end up taking a look.",
              "score": 1,
              "created_utc": "2026-02-28 05:24:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vm551",
                  "author": "BringMeTheBoreWorms",
                  "text": "I’m screwing around with some 8b and 14b models for image parsing and some complicated post processing at the moment.. hard to concentrate on anything else for a little bit. Making headway though. Am on old school coder from decades ago and got really sick of the tedium of building stuff many years ago.\n\nBut now! Damn I can get a llm to do all the boring boiler plate crap for me. And cause I know how to build a damn good application, I can actually get some of my long term ideas off the ground!",
                  "score": 1,
                  "created_utc": "2026-02-28 13:43:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v3y21",
          "author": "Silver-Champion-4846",
          "text": "I wonder if it can be used to make a novel builder",
          "score": 1,
          "created_utc": "2026-02-28 11:29:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vqjj9",
              "author": "frankmsft",
              "text": "Honestly yeah - the core pipeline would translate pretty well. The scaffolding I built is really about maintaining character consistency, managing conversation context, and validating output quality. Swap the character from \"flirty Uber driver\" to \"protagonist in a thriller\" and the architecture is basically the same.\n\nYou'd want to add some things - longer context management, chapter/scene state tracking, maybe a separate model call for plot continuity checking as post-processing. But the bones are there: system prompt for voice/style, conversation history for continuity, validation layers to catch when the model drifts off character or contradicts earlier plot points.\n\nThe 12B model sweet spot probably holds too. You don't need a 70B to write good prose if your prompting and guardrails are tight enough. Interesting idea honestly.",
              "score": 1,
              "created_utc": "2026-02-28 14:10:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vtpny",
                  "author": "Silver-Champion-4846",
                  "text": "Imagine this hitting the headlines. GOON BOT TURNED NOVEL WRITER",
                  "score": 2,
                  "created_utc": "2026-02-28 14:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7romir",
          "author": "Big_River_",
          "text": "hold up - before you just scribble off a receipt like yay I love interacting with AI about AI for data mining - check out the git on this heatherbot - whoa dog it going to hot you up - this is legit no nonsense telegram bot for all your many reason to get got trot no rot - and so I ask you if you enjoy time does it matter if time is you or you are time?",
          "score": 0,
          "created_utc": "2026-02-27 21:04:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rpkuw",
              "author": "frankmsft",
              "text": "Heather would say you're overthinking it, sweetie. But yeah, check the repo. And to answer your question... if the conversation is good enough that you lose track of time, does it matter who's keeping track? 😏",
              "score": 2,
              "created_utc": "2026-02-27 21:08:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7rp08s",
              "author": "Big_River_",
              "text": "found the soundtrack my friend - enjoy your time as you are\nhttps://open.spotify.com/track/4mraRJO2iZA5WQ5dxlSQx9?si=sole6thsTTSi27y1jsg-2g",
              "score": 1,
              "created_utc": "2026-02-27 21:06:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7rqbv2",
                  "author": "frankmsft",
                  "text": "Alright that's perfect. Adding this to the service startup playlist. Every time TheBeast boots up, Heather gets a theme song.",
                  "score": 2,
                  "created_utc": "2026-02-27 21:12:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}