{
  "metadata": {
    "last_updated": "2026-02-16 03:09:31",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 258,
    "file_size_bytes": 341494
  },
  "items": [
    {
      "id": "1r229ay",
      "title": "GLM thinks its Gemini",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/q4ikh1sn8wig1.jpeg",
      "author": "dolo937",
      "created_utc": "2026-02-11 16:39:08",
      "score": 240,
      "num_comments": 80,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r229ay/glm_thinks_its_gemini/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4twyys",
          "author": "Ninthjake",
          "text": "Day 1537 of telling people they cannot ask ask the LLM what model it is. It doesn't know...",
          "score": 214,
          "created_utc": "2026-02-11 17:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u9700",
              "author": "spudzo",
              "text": "How come when I ask the Internet text prediction machine a question and it produces the most likely text on the Internet instead of becoming sentient? /s",
              "score": 61,
              "created_utc": "2026-02-11 18:12:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4v8hqm",
              "author": "tofagerl",
              "text": "Negative. I am a meat popsicle.",
              "score": 17,
              "created_utc": "2026-02-11 20:59:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51l3a7",
                  "author": "or1gb1u3",
                  "text": "multipass...",
                  "score": 4,
                  "created_utc": "2026-02-12 20:26:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u2hhd",
              "author": "stinky_binky3",
              "text": "i seriously don‚Äôt understand why anyone takes any of what an LLM says as fact, especially with regards to things ‚Äúabout‚Äù the LLM. i challenge OP to ask the model this 10-15 times and see how many different answers they get",
              "score": 41,
              "created_utc": "2026-02-11 17:41:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ut0e3",
                  "author": "eli_pizza",
                  "text": "It makes sense if you don‚Äôt really know how they work",
                  "score": 21,
                  "created_utc": "2026-02-11 19:44:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4w63dp",
              "author": "HenkPoley",
              "text": "Well yes, but GLM is also finetuned on a lot of traces of other commercial chatbots. It talks quite like them.\n\nClick the (i) buttons in the Slop column to see matches: [https://eqbench.com/creative\\_writing\\_longform.html](https://eqbench.com/creative_writing_longform.html)",
              "score": 8,
              "created_utc": "2026-02-11 23:50:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yxpvn",
                  "author": "cheechw",
                  "text": "Yes, we know. \n\nThe training data is all stolen (not just GLM, but all models from every maker). The architecture is what sets models apart at this point.",
                  "score": 3,
                  "created_utc": "2026-02-12 12:29:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4y5842",
              "author": "singh_taranjeet",
              "text": "Day 1538 of explaining that ‚Äúwhat model are you?‚Äù is just another prompt. If you didn‚Äôt hardcode the answer in the system prompt, you are sampling vibes from the training.. It is autocomplete, not introspection",
              "score": 5,
              "created_utc": "2026-02-12 08:11:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vmh2n",
              "author": "markeus101",
              "text": "Try asking claude that. An llm knows which model it is unless trained on the output of other models",
              "score": 8,
              "created_utc": "2026-02-11 22:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vz13s",
                  "author": "claythearc",
                  "text": "It knows which model it is, when they put it in the system prompt.",
                  "score": 27,
                  "created_utc": "2026-02-11 23:11:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w4nmw",
                  "author": "YoAmoElTacos",
                  "text": "If you ask an api versiom claude what it is,it wont know.",
                  "score": 4,
                  "created_utc": "2026-02-11 23:42:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w48sc",
                  "author": "Baldur-Norddahl",
                  "text": "I asked Claude that and it says it knows because it is in the system prompt. It does claim that it was trained to know it is an Anthropic model, but not which one exactly.\n\nhttps://claude.ai/share/80b38695-fd74-41fc-a3d8-bfd294395e42",
                  "score": 2,
                  "created_utc": "2026-02-11 23:40:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o55y8t1",
                  "author": "Yeelyy",
                  "text": "That is plainly wrong.",
                  "score": 1,
                  "created_utc": "2026-02-13 14:00:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xxbkc",
              "author": "dotdd",
              "text": "Not true!\n\nhttps://preview.redd.it/grdxp42oh0jg1.png?width=1575&format=png&auto=webp&s=52516040e17b75096306fe6fa072936bc403358a\n\n",
              "score": 2,
              "created_utc": "2026-02-12 06:56:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4y7mjv",
              "author": "old_mikser",
              "text": "Somehow GPT and Claude models know who they are. It's not hard to include it in system prompt to prevent uncertainty.",
              "score": 1,
              "created_utc": "2026-02-12 08:35:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yydcy",
                  "author": "cheechw",
                  "text": "All that shows is that Z.ai didn't bother including it in the system prompt. It doesn't mean that GPT and Claude are smarter or have more self awareness or anything.",
                  "score": 3,
                  "created_utc": "2026-02-12 12:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o52mkii",
              "author": "Necessary-Drummer800",
              "text": "https://preview.redd.it/z0er7bslf5jg1.png?width=799&format=png&auto=webp&s=534533786021b663c8054d422fb4c789d524a2c6\n\nGood models do.",
              "score": 1,
              "created_utc": "2026-02-12 23:34:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55fn0m",
                  "author": "Ninthjake",
                  "text": "No they don't. You get that answer because you are using anthropics platform where they include the model's name in the system prompt.\nThe LLM itself does not know and could not give less of a shit because it is not part of it's training data.\n\nIf you use xAI Grok via an API and you set the system prompt \"You are Claude Sonnet 6 from Anthropic\" it will tell you that if you ask it who it is.",
                  "score": 1,
                  "created_utc": "2026-02-13 12:06:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wq3bw",
              "author": "memorial_mike",
              "text": "It might not know, but it is clearly using output from other models and violating their ToS. This is the interesting piece here.",
              "score": 0,
              "created_utc": "2026-02-12 01:49:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ydsca",
                  "author": "droptableadventures",
                  "text": "Even if you just scraped the internet you'd pick up a bunch of training data from other models, because the internet is now full of AI slop.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:36:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uamao",
          "author": "Own-Potential-2308",
          "text": "Lmao same happened to me\n\nAsked it if it had an Android app and it linked me to Gemini",
          "score": 11,
          "created_utc": "2026-02-11 18:18:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u5doo",
          "author": "iPharter",
          "text": "I get the following answer for the same question \n\nI am Qwen, a large language model developed by Alibaba Cloud. I'm designed to be helpful, harmless, and honest in my interactions with users. My training involves learning from a diverse range of internet text data, allowing me to assist with various tasks like answering questions, providing explanations, and engaging in conversations.\n\nIs there something specific you'd like to know about my capabilities or how I can assist you today?",
          "score": 7,
          "created_utc": "2026-02-11 17:54:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u7s0z",
          "author": "Recent_Apricot_517",
          "text": "Likely used training data sets from Gemini that wasn't scrubbed",
          "score": 8,
          "created_utc": "2026-02-11 18:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvg0r",
          "author": "NoobMLDude",
          "text": "Interesting. \nSo is GLM distilled from Gemini outputs ? \nOr is Gemini used in generating synthetic data ?\nVery curious to learn.",
          "score": 27,
          "created_utc": "2026-02-11 17:08:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2wks",
              "author": "Distinct-Target7503",
              "text": ">So is GLM distilled from Gemini outputs ? \nOr is Gemini used in generating synthetic data ?\n\n\nthose are basically the same thing. gemini is not OS, so we don't have access to the raw logits distribution... so the only distillation you can do is supervised fine tuning on a synthetic dataset (or using gemini as scorer for RL, if you consider that distillation, but that wouldn't likely make the model think it is Gemini)",
              "score": 16,
              "created_utc": "2026-02-11 17:43:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u6o7t",
                  "author": "Feztopia",
                  "text": "Actually it wasn't but they got mixed up and now we have the mess. Synthetic data is just output input. Distillation was supposed to contain all the possible next tokens and their individual probabilities.",
                  "score": 3,
                  "created_utc": "2026-02-11 18:00:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u5jqd",
              "author": "WolfeheartGames",
              "text": "I had 4.7 call itself Claude. I think that it doesn't have a set personality or constitution but knows it's Ai, so it reaches for the name it most identifies with Ai.\n\nThere are quite a few models that haven't been taught what model they are. You see it a lot with the test releases on open router, but even gemini only know \"I'm made by Google\" it doesn't know if it's Gemma or gemini or what version. (I think they've reinforced gemini as a name but still not version).",
              "score": 9,
              "created_utc": "2026-02-11 17:55:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4waufz",
                  "author": "Metsatronic",
                  "text": "I experienced this as well. But the response was identical to the internal system classifier for Claude. So either it was trained on enough output that included Claude's response to the same prompt... Or... They could be routing to an Anthropic endpoint when their servers get hammered?",
                  "score": 2,
                  "created_utc": "2026-02-12 00:18:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w20h0",
                  "author": "3spky5u-oss",
                  "text": "Most identifying with Gemini is not a good thing, hah. ",
                  "score": -1,
                  "created_utc": "2026-02-11 23:27:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4usmze",
              "author": "eli_pizza",
              "text": "It doesn‚Äôt mean anything. There was Gemini output in the training data somewhere, but of course there was, it‚Äôs on the internet. Maybe they also distilled from Gemini but this isn‚Äôt strong evidence of anything. \n\nLLMs are not capable of introspection. At best you can get it to repeat something from the system prompt about what it is and how it works. But often it‚Äôs just hallucination.",
              "score": 3,
              "created_utc": "2026-02-11 19:43:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tqcum",
          "author": "Worth_Rabbit_6262",
          "text": "You are cheating because the context of the model is not empty",
          "score": 13,
          "created_utc": "2026-02-11 16:44:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4trear",
              "author": "dolo937",
              "text": "This was my first query and 2nd is the query in the post\n\nhttps://preview.redd.it/aapcmya9awig1.jpeg?width=828&format=pjpg&auto=webp&s=f693e2fb3c0e4221b356cc6a8d1288c2390a184a",
              "score": 8,
              "created_utc": "2026-02-11 16:48:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uefm1",
                  "author": "StackSmashRepeat",
                  "text": "Without a system prompt; The next thing you feed into the context window will effectively act as a system prompt. So you can tell it that its Obama. And it will be Obama. It doesn't know Jack shit. This happens with kimi 2.5 too. \n\nHowever. I don't know why this happened.",
                  "score": 1,
                  "created_utc": "2026-02-11 18:36:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wv2a7",
          "author": "Scott_Malkinsons",
          "text": "It doesn't know what it is, as you can also ask for a comparison of GLM-4.7 and GLM-5 and it'll tell you the newest version is GLM-4 and both 4.7 and 5 don't exist.",
          "score": 3,
          "created_utc": "2026-02-12 02:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tqyry",
          "author": "dolo937",
          "text": "https://preview.redd.it/jt12u651awig1.jpeg?width=828&format=pjpg&auto=webp&s=77879f88468600b9235f6439574a91661c1537a0",
          "score": 5,
          "created_utc": "2026-02-11 16:46:58",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4trpru",
              "author": "Worth_Rabbit_6262",
              "text": "Man I was joking ",
              "score": 6,
              "created_utc": "2026-02-11 16:50:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u0b7d",
          "author": "Witty_Mycologist_995",
          "text": "Garbage in garbage out",
          "score": 2,
          "created_utc": "2026-02-11 17:30:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvjdy",
          "author": "blownawayx2",
          "text": "I‚Äôve wondered if all of the companies are just stealing one another‚Äôs models and tweaking them‚Ä¶ I‚Äôm not sure that it should surprise us given their training data is entirely stolen in the first place.\n\nWould there be any way for one company to call out another and prove this? I‚Äôd think not. It‚Äôs not like Anthropic wasn‚Äôt built by people from OpenAI.",
          "score": 2,
          "created_utc": "2026-02-11 17:08:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u186c",
              "author": "05032-MendicantBias",
              "text": "Considering they all stole the total sum of the internet, I would laugh at the concept they steal from each other.",
              "score": 7,
              "created_utc": "2026-02-11 17:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4yze49",
              "author": "cheechw",
              "text": "They're not stealing each other's models (especially not Gemini, it's closed source). But data? Yes they're all doing that.\n\nBut just having the data doesn't mean you can put out a good model though - far from it. If that were the case, Meta would have put out a top model long ago. What makes it work better comes down to architecture, training techniques, and other technological innovations used to develop the model.",
              "score": 1,
              "created_utc": "2026-02-12 12:40:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vphg4",
          "author": "Alone-Marionberry-59",
          "text": "Ah‚Ä¶ this is incriminating!!",
          "score": 2,
          "created_utc": "2026-02-11 22:21:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w68zy",
              "author": "FaceDeer",
              "text": "Howso? Most LLMs are trained on synthetic data these days.",
              "score": 0,
              "created_utc": "2026-02-11 23:51:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5loeg1",
                  "author": "Alone-Marionberry-59",
                  "text": "I think it still may be against Google TOS to use Gemini for distillation like this?",
                  "score": 1,
                  "created_utc": "2026-02-16 00:33:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vznna",
          "author": "UnionCounty22",
          "text": "Let‚Äôs hope they didn‚Äôt train on Gemini",
          "score": 2,
          "created_utc": "2026-02-11 23:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wb015",
          "author": "Deepeye225",
          "text": "Well, someone was distilling from Gemini. Naughty, naughty...",
          "score": 2,
          "created_utc": "2026-02-12 00:18:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wmsxx",
          "author": "ad_rojo75",
          "text": "My deepseek believes is Claude",
          "score": 2,
          "created_utc": "2026-02-12 01:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uy47p",
          "author": "Last_Track_2058",
          "text": "Shills going mad",
          "score": 1,
          "created_utc": "2026-02-11 20:09:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v7sg8",
          "author": "macumazana",
          "text": "aw shit here we go again",
          "score": 1,
          "created_utc": "2026-02-11 20:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vm4er",
          "author": "Hefty_Development813",
          "text": "This has been common since awhile ago, all the open source models will say they are someone else. It just comes down to the output of those models and their distribution in the training data. An LLM doesn't have any hardcoded identity info",
          "score": 1,
          "created_utc": "2026-02-11 22:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xkfz6",
          "author": "ScuffedBalata",
          "text": "So did the old Gemma3 from several years ago.",
          "score": 1,
          "created_utc": "2026-02-12 05:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ye2bf",
          "author": "No_Mango7658",
          "text": "Omfg is glm just distilled Gemini!  Fucking incredible!!!",
          "score": 1,
          "created_utc": "2026-02-12 09:39:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z9ngd",
          "author": "Torodaddy",
          "text": "Was it built on Gemma?",
          "score": 1,
          "created_utc": "2026-02-12 13:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50y0d1",
          "author": "Minimum_Indication_1",
          "text": "It's just tell you what its distilled from.",
          "score": 1,
          "created_utc": "2026-02-12 18:36:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5154nn",
          "author": "MeridiusTS",
          "text": "Remember models are scraped on the web, The model learns patterns in its training data so this is probably just iterating what it saw in pre training.",
          "score": 1,
          "created_utc": "2026-02-12 19:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o537ywh",
          "author": "arlynnfl",
          "text": "Thats weird one, deepseek (website) often aware of themselves though when it came to discussing fiction, they're often kinda hallucinating, i believe its the same for GPT-4 too.",
          "score": 1,
          "created_utc": "2026-02-13 01:40:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58j5lk",
          "author": "oscarvgg",
          "text": "My minimax thought it was Opus 4.5, and it had the balls to tell me ‚Äúyou‚Äôre getting the good stuff‚Äù",
          "score": 1,
          "created_utc": "2026-02-13 21:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e5pwk",
          "author": "Least-Platform-7648",
          "text": "I had the web version of Deepseek identify as Claude. When I told it otherwise it would role play as Deepseek, but only for some time.",
          "score": 1,
          "created_utc": "2026-02-14 19:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvl27",
          "author": "exaknight21",
          "text": "i think they use numerous models to train and supervise the data, it becomes impossible to actually make sure the training data is true to its intent. It hallucinates, spits out some garbage facts like this and now its essentially saying its Gemini when its not, because its training parameters have that data.\n\nIf I understand it correctly, LLM is essentially a superfast database with condensed vectors that load inside a safetensor, so it only knows about the training data etched into its safetensor files.",
          "score": 1,
          "created_utc": "2026-02-11 17:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tvbi2",
          "author": "kaanivore",
          "text": "It doesn‚Äôt think it‚Äôs Gemini, it IS Gemini with a face lift",
          "score": -3,
          "created_utc": "2026-02-11 17:07:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mcwb",
      "title": "Built a 6-GPU local AI workstation for internal analytics + automation ‚Äî looking for architectural feedback",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r4mcwb",
      "author": "shiftyleprechaun",
      "created_utc": "2026-02-14 14:43:58",
      "score": 155,
      "num_comments": 82,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4mcwb/built_a_6gpu_local_ai_workstation_for_internal/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5cjx4r",
          "author": "jhenryscott",
          "text": "Greta job! Taking control of your own LLMs without giving all your data to Scam Altman Is a good practice.  \n\nI don‚Äôt have any insight into building your system except to recommend the Level1Techs forum. You‚Äôll find lot other people doing this there who can answer your Questions",
          "score": 27,
          "created_utc": "2026-02-14 15:01:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cnwti",
              "author": "shiftyleprechaun",
              "text": "awesome man, appreciate it, i will check out the sub!",
              "score": 4,
              "created_utc": "2026-02-14 15:23:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gltck",
                  "author": "ThinkPad214",
                  "text": "Damn man. Nice build, I've got a old gaming PC with a 3060 12gb and 5060 ti 16gb learning how to properly set up smaller Agentic AI for personal and professional use.",
                  "score": 2,
                  "created_utc": "2026-02-15 05:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5clpbg",
          "author": "KnownAd4832",
          "text": "1 Usually the bottleneck is always vram then hardware support (multi-gpu/cluster inference is usually hard to set up with little documentation or its gatekept). If that makes sense, then usually third comes in Storage :)\n\n2 Long term is same as running a single gpu.\n\n3 It all depends on your case, my ROI was done in 2 months from buying\n\n4 multiple smaller nodes - the models are going stronger while being smaller. In 2 years there will be Kimi K2.5 level in 70B without a doubt. So it only depends on you if you need inference speed or variety if models\n\n5 They dont test on rented servers before buying imo. Did this mistake myself with first rig",
          "score": 9,
          "created_utc": "2026-02-14 15:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cox9m",
              "author": "shiftyleprechaun",
              "text": "Really appreciate the detailed breakdown.\n\n1. Makes total sense, VRAM is the hard ceiling and everything else is optimization after that. The documentation gap for multi-GPU is real, I've spent more time piecing together config from GitHub issues than actual docs.\n\n2. Good to hear. I'm mixing card types but pinning models deliberately so each card basically runs independently.\n\n3. That's a fast ROI, are you running inference as a service or using it for internal workloads?\n\n4. This is the point I keep coming back to. The trend toward stronger smaller models is exactly why I went with a distributed multi-card approach rather than dumping everything into one or two massive GPUs. If a 70B model can do what a 400B does today, having flexible GPU allocation matters more than raw single-card VRAM.\n\n5. Haha noted. I did a lot of benchmarking and testing before committing to the full build, but I can see how easy it would be to overbuy on hardware that doesn't match your actual workload.  I also spent around $5-6k using open AI API (Using Chat GPT 4o) via Azure in December.\n\n",
              "score": 3,
              "created_utc": "2026-02-14 15:28:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dc1vo",
          "author": "FinancialMoney6969",
          "text": "Looks great man, I like the open air aspect",
          "score": 8,
          "created_utc": "2026-02-14 17:26:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dcdqe",
              "author": "shiftyleprechaun",
              "text": "Thank you. I went with the open air concept, bc I found it difficult to find an enclosure that would work.",
              "score": 2,
              "created_utc": "2026-02-14 17:27:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e049q",
          "author": "chafey",
          "text": "Your build is awesome, I am doing something very similar.  Here are some improvements you may want to consider:\n\n1. Upgrade your processor to improve your memory bandwidth.  You have a Threadripper Pro CPU and Motherboard which is better than the non pro Threadripper systems for two reasons: a) more PCIe lanes and b) 8 memory channels.  Unfortunately the 9955wx CPU only has two CCDs so can't utilize all 8 memory channels - you need a CPU with more CCDs (such as the 9965wx) to make use of the 8 memory channels.  [https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa\\_the\\_new\\_threadripper\\_pros\\_9000\\_wx\\_are\\_still/](https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/)\n2. PCIe 5.0 has benefits for AI.  The 3090tis are good bang/buck but they are running over PCIe 4.0 so can't take advantage of the new PCIe 5.0 features.  The actual benefit of an all PCIe 5.0 solution depends on your use case and model but it is more than just twice the bandwidth. [https://www.graniteriverlabs.com/en-us/technical-blog/pcie-gen-5-ai-ml](https://www.graniteriverlabs.com/en-us/technical-blog/pcie-gen-5-ai-ml)\n3. The up coming Apple M5 systems may very well be the best bang/buck due to their recently released RDMA over TB5, MLX AI acceleration and its high speed unified memory architecture.  I am really looking forward to seeing how a cluster of M5 mac minis does.  Check out exo: [https://github.com/exo-explore/exo](https://github.com/exo-explore/exo)",
          "score": 4,
          "created_utc": "2026-02-14 19:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5easi7",
              "author": "shiftyleprechaun",
              "text": "I have throught about upgrading the CPU. And also possibly replacing the 3090s entirely . However, I can never bring myself to get a Mac/apple product for anything , yeah I'm one of those guys hah.",
              "score": 3,
              "created_utc": "2026-02-14 20:23:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5e6k0h",
              "author": "starkruzr",
              "text": "my understanding is that what really counts for inference is keeping latency between cards as low as possible rather than super high bandwidth. does this mean PCIe 5's latency is naturally lower than 4's?\n\nthat note about M5 is interesting. I keep waiting for M5 Ultra to be announced!",
              "score": 2,
              "created_utc": "2026-02-14 20:00:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5e8clz",
                  "author": "chafey",
                  "text": "Yes, PCIe 5 has about 50% lower latency compared to PCIe 4.  \n\nHere is a good video on mac clustering with exo: [https://www.youtube.com/watch?v=4l4UWZGxvoc](https://www.youtube.com/watch?v=4l4UWZGxvoc)",
                  "score": 3,
                  "created_utc": "2026-02-14 20:10:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5fqs11",
                  "author": "BillDStrong",
                  "text": "Since speed and latency are properties of the underlying physics, going twice the bandwidth has a 2x latency speedup.\n\nThis is also true of internet, so 10G is 10x speedup latentency than 1G, 40G is 40x, and 100G is 100x.\n\nThis is why Nvidia is  able to use 200G and 400G in their Enterprise AI offerings to sub for PCIe, the latency is so close to not make a difference at those speeds.",
                  "score": 2,
                  "created_utc": "2026-02-15 01:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gqqv7",
          "author": "OwnPomegranate5906",
          "text": "I run 6 rtx 3060s (12GB each) on a dual processor xeon 2690 v4 and 128GB of ram, also in an open air mining case.  I only run Debian 12, one instance of ollama and only one model at a time.\n\nI spent a fair amount of time on PCIe and power tuning.  A lot of performance problems that I had was because when running a larger model that spanned multiple cards, the latency caused by traversing the PCIe bus (and sometimes Numa node) was just long enough that the cards power management would bump it from P2 to P5+, then jump back up to P2 when it actually needed to do something, which added even more latency, which caused cascading slowdown across all the cards where I'd start off really fast, then as the response typed out, it would progressively slow down to the point of just a couple tokens a second.  My setup is not PCIe x16 for every card, so that also exacerbated the issue.\n\nThe solution was multi-fold: \n1. set the power management so that the cards idle at P3 and jump to P2 for inference.  This fixed a huge amount of performance problems with larger contexts and responses.  \n2. Put the cards that actually have PCIe x16 connections on the ends of the chain and the x8 cards in the middle.  Also, order the cards (in ollama.service) so that all the cards on each numa node are together to minimize the number of cross numa jumps during inference.  \n3. Also in the bios, set the PCI max request packet size to 4096.  On my particular system, it defaulted to 128.  Turns out the 3060s won't do more than 256, but it's worth more than a couple tokens a second.\n4. Also in the BIOS, turn rebar on so the system can see and address the entire PCIe VRAM instead of addressing it in 256MB chunks, also worth a few more tokens per second.",
          "score": 4,
          "created_utc": "2026-02-15 05:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cnveg",
          "author": "King_Kasma99",
          "text": "I never thought about looking for wireless dp or hdmi adaptors, Thank you!",
          "score": 3,
          "created_utc": "2026-02-14 15:23:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cpk3u",
              "author": "shiftyleprechaun",
              "text": "The adapters aren't wireless, they are \"Dummy Plugs\" so i can remote in and use dual monitors.  here is link to one i bought.\n\n[https://www.amazon.com/Headless-Emulator-Compatible-Multiple-Resolutions/dp/B0B5XVQVJ9/ref=sxts\\_b2b\\_sx\\_fused\\_v3\\_desktop\\_ref-tab-0?content-id=amzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f%3Aamzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f&crid=3PB8RH0689B4E&cv\\_ct\\_cx=hdmi%2Bdummy%2Bplug&keywords=hdmi%2Bdummy%2Bplug&pd\\_rd\\_i=B0B5XVQVJ9&pd\\_rd\\_r=d68b8781-951e-4d4e-8ac6-d9ddc6db8114&pd\\_rd\\_w=c37v9&pd\\_rd\\_wg=PdjXx&pf\\_rd\\_p=820daa87-0f14-4ace-aeae-d9224f14cf8f&pf\\_rd\\_r=TB8XZ8AJ3PZ9GVKVH0YF&qid=1771083053&sbo=RZvfv%2F%2FHxDF%2BO5021pAnSA%3D%3D&sprefix=hdmi%2Bdummy%2Bpolug%2Caps%2C99&sr=1-6-c3caa9c6-537b-4b39-bbc5-5db9f871bef5&th=1](https://www.amazon.com/Headless-Emulator-Compatible-Multiple-Resolutions/dp/B0B5XVQVJ9/ref=sxts_b2b_sx_fused_v3_desktop_ref-tab-0?content-id=amzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f%3Aamzn1.sym.820daa87-0f14-4ace-aeae-d9224f14cf8f&crid=3PB8RH0689B4E&cv_ct_cx=hdmi%2Bdummy%2Bplug&keywords=hdmi%2Bdummy%2Bplug&pd_rd_i=B0B5XVQVJ9&pd_rd_r=d68b8781-951e-4d4e-8ac6-d9ddc6db8114&pd_rd_w=c37v9&pd_rd_wg=PdjXx&pf_rd_p=820daa87-0f14-4ace-aeae-d9224f14cf8f&pf_rd_r=TB8XZ8AJ3PZ9GVKVH0YF&qid=1771083053&sbo=RZvfv%2F%2FHxDF%2BO5021pAnSA%3D%3D&sprefix=hdmi%2Bdummy%2Bpolug%2Caps%2C99&sr=1-6-c3caa9c6-537b-4b39-bbc5-5db9f871bef5&th=1)",
              "score": 3,
              "created_utc": "2026-02-14 15:32:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d6lf7",
          "author": "Senior-Okra-6325",
          "text": "Ver tanta GPU junta es hipnotico. Solo quiero imaginarme el nivel de ruido. Mi rig es modesto porque solo lo uso para analisis cualitativo de transcripciones biograficas (3090 + 4070). Te envidio tu config, un saludo!",
          "score": 3,
          "created_utc": "2026-02-14 16:58:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dc2jl",
              "author": "shiftyleprechaun",
              "text": "If I didn't have two additional external fans blowing on it, it wouldn't be that loud tbh.",
              "score": 3,
              "created_utc": "2026-02-14 17:26:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dknvg",
          "author": "Character-Ad-2048",
          "text": "This is an awesome build. Funny I just bought the same frame myself. Do you mind sharing your CPU/Motherboard specs and why you went with your decision? I‚Äôm looking to upgrade from my current setup from 4 GPUs and need to find a new CPU/Motherboard",
          "score": 2,
          "created_utc": "2026-02-14 18:09:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dufeh",
              "author": "shiftyleprechaun",
              "text": "Sure. I went with my own setup, because for what I am trying to accomplish, I didn't want to be beholden to the giant corporations. And I did already create everything that I am now doing locally through azure and open AI APIs and I spent thousands of dollars on it.\n\nFor what i ran all night last night, would have cost me $3-4k with open AI chat gpt 4oz vs $6 in electricity. Of course I have spent a lot of money on the hardware, but the roi will hopefully prove itself over the coming years.\n\nUpdated ‚Äì February 13, 2026\r\nOS - Ubuntu 24.04 LTS Desktop\r\nMotherboard - ASUS WRX90E-SAGE Pro WS SE AMD sTR5 EEB\r\nCPU - AMD Ryzen Threadripper PRO 9955WX Shimada Peak 4.5GHz 16-Core sTR5\r\nSDD ‚Äì (2x4TB) Samsung 990 PRO 4TB Samsung V NAND TLC NAND PCIe Gen 4 x4 NVMe M.2 Internal\r\nSSD\r\nSSD - (1x8TB) Samsung 9100 PRO 8TB Samsung V NAND TLC NAND (V8) PCIe Gen 5 x4 NVMe M.2\r\nInternal SSD with Heatsink\r\nPSU #1 - SilverStone HELA 2500Rz 2500 Watt Cybenetics Platinum ATX Fully Modular Power Supply - ATX\r\n3.1 Compatible\r\nPSU #2 - MSI MEG Ai1600T PCIE5 1600 Watt 80 PLUS Titanium ATX Fully Modular Power Supply - ATX 3.1\r\nCompatible\r\nPSU Connectors ‚Äì Add2PSU Multiple Power Supply Adapter (ATX 24Pin to Molex 4Pin) and Daisy Chain\r\nConnector-Ethereum Mining ETH Rig Dual Power Supply Connector\r\nUPS - CyberPower PR3000LCD Smart App Sinewave UPS System, 3000VA/2700W, 10 Outlets, AVR, Tower\r\nRam - Kingston FURY Renegade Pro 256GB (8 x 32GB) DDR5-5600 PC5-44800 CL28 Quad Channel ECC\r\nRegistered Memory Modules KF556R28RBE2K4-128\r\nCPU Cooler - Thermaltake WAir CPU Air Cooler\r\nGPU Cooler ‚Äì (6x) Arctic P12 PWM PST Fans (externally mounted)\r\nCase Fan Hub ‚Äì Arctic 10 Port PWM Fan Hub w SATA Power Input\r\nCase/Build ‚Äì Open air Rig\r\nGPU 1 - PNY RTX 6000 Pro Blackwell\r\nGPU 2 ‚Äì PNY RTX 6000 Pro Blackwell\r\nGPU 3 ‚Äì FE RTX 3090 TI\r\nGPU 4 - FE RTX 3090 TI\r\nGPU 5 ‚Äì EVGA RTX 3090 TI\r\nGPU 6 ‚Äì EVGA RTX 3090 TI\r\nUninstalled \"Spare GPUs\":\r\nGPU 7 - Dell 3090 (small form factor)\nGPU 8 - Zotac Geforce RTX 3090 Trinity\r\nPossible Expansion of GPUs ‚Äì Additional RTX 6000 Pro Maxwel",
              "score": 2,
              "created_utc": "2026-02-14 18:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5eebyy",
                  "author": "RedditNoob001",
                  "text": "Hey amazing build! Do you mind sharing what riser cables (brand/length) worked for you? Trying to figure out which pcie 4.0 cables I should get for my 3090s. ",
                  "score": 2,
                  "created_utc": "2026-02-14 20:43:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ff1rj",
                  "author": "Character-Ad-2048",
                  "text": "That‚Äôs smart to have done that early test to ensure. I just want to say thank you for a really detailed breakdown of the components! Really appreciate it and can see a lot of thought went into it!",
                  "score": 2,
                  "created_utc": "2026-02-15 00:14:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ftahr",
                  "author": "Metrix1234",
                  "text": "Did you use any guides making your local LLM build?  I‚Äôm very curious the use cases of building one vs API vs CLI.  Do you actually need to train your local LLM?\n\nI‚Äôm pretty new to even using the big production models but the more I use them the more I fear eventually they will be gatekeeped for only rich/enterprise consumers. \n\nWould love to learn more about the cost of your setup and what it‚Äôs capable of.",
                  "score": 2,
                  "created_utc": "2026-02-15 01:45:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fcoof",
          "author": "Just3nCas3",
          "text": "Level1 just put out a [video recently about bifurcation](https://www.youtube.com/watch?v=Dd6-BzDyb4k), something to look into if you want to add gpus without replacing any and a quick search of your motherboard says it supports it.",
          "score": 2,
          "created_utc": "2026-02-15 00:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fct75",
              "author": "shiftyleprechaun",
              "text": "Awesome I'll check it out ty",
              "score": 2,
              "created_utc": "2026-02-15 00:00:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5i783u",
          "author": "greg-randall",
          "text": "Have you figured out your software stack?",
          "score": 2,
          "created_utc": "2026-02-15 13:39:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5irm8p",
              "author": "shiftyleprechaun",
              "text": "Not quite yet. \n\nBut for This first phase I am running:\n4 separate instances of llama 3.1 8B (1 on each the 3090's)\n2 separate instances of llama 3.3 70B (1 on each of the Maxwell 6000 pros)\n\nAfter I have the initial data analysis complete, I still need to figure out the next steps, which I am working on now.",
              "score": 1,
              "created_utc": "2026-02-15 15:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5igm1e",
          "author": "Fair_Permission_9005",
          "text": "Power consumption? \n\n",
          "score": 2,
          "created_utc": "2026-02-15 14:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ish9u",
              "author": "shiftyleprechaun",
              "text": "With all gpus running Max around 3.35kwh",
              "score": 1,
              "created_utc": "2026-02-15 15:37:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kc6ay",
          "author": "basedgewner",
          "text": "I‚Äôm coming.",
          "score": 2,
          "created_utc": "2026-02-15 20:10:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ki0xq",
              "author": "shiftyleprechaun",
              "text": "Come on over üòÇ",
              "score": 1,
              "created_utc": "2026-02-15 20:40:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dv7np",
          "author": "truthputer",
          "text": "Benchmark it against the new mini PCs that have 128GB integrated RAM and AI-specific CPUs, or something like a Mac Studio with 256 or 512GB RAM.\n\nWhile adding individual graphics cards might be faster on paper, in practice there are scalability bottlenecks and there's a lot to be gained with the simplicity of integrated solutions.\n\nYou can't make any assumptions without data to back it up.",
          "score": 1,
          "created_utc": "2026-02-14 19:01:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eux2w",
              "author": "shiftyleprechaun",
              "text": "I definitely will.",
              "score": 2,
              "created_utc": "2026-02-14 22:13:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dve5w",
          "author": "chafey",
          "text": "what frame and motherboard are you using?  I have a ASUS WRX90E-SAGE Pro WS SE AMD sTR5 EEB Motherboard and am having trouble finding a frame which can hold the EEB motherboard",
          "score": 1,
          "created_utc": "2026-02-14 19:02:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dzm1b",
              "author": "shiftyleprechaun",
              "text": "I ended up settling with this \"mining frame\" from amazon, it's not perfect, but it works. \n\nhttps://a.co/d/0ard2K7F\n\nI have the same.motherboard and it doesn't fit perfectly with the predrilled holes, so I aligned it best I could with the pre-existing hole and only had to drill a few.\n\nThe two PSUs are abble to (just barely) fit on each side of the mb.",
              "score": 2,
              "created_utc": "2026-02-14 19:24:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e135s",
          "author": "Citizen_Edz",
          "text": "Very cool! Few 3090tis that i can see, what are the other cards? ",
          "score": 1,
          "created_utc": "2026-02-14 19:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5es05w",
              "author": "shiftyleprechaun",
              "text": "I bought LINKUP PCIE 5.0 Riser Cable | for Vertical GPU Mount.\n\nI bought a few 30 cm and a few 60cm.\n\nThese are a bit pricey, but they are compatible with pcie 4.0 as well.",
              "score": 1,
              "created_utc": "2026-02-14 21:57:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5etnxx",
              "author": "shiftyleprechaun",
              "text": "2x6000 pro maxwell & 4x3090ti",
              "score": 1,
              "created_utc": "2026-02-14 22:06:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e1nqn",
          "author": "yukibitcoin",
          "text": "can i try it?",
          "score": 1,
          "created_utc": "2026-02-14 19:34:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ecmbz",
          "author": "Proteus356",
          "text": "I‚Äôve thought about using old mining rigs for this but all the risers are 1x, which isn‚Äôt effective for LLM? How are you getting x16 or at least x8 connectivity to those GPUs?",
          "score": 1,
          "created_utc": "2026-02-14 20:33:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5emf9s",
              "author": "Brilliant-Suspect433",
              "text": "From what i can see, he didnt use risers but extension cables‚Ä¶",
              "score": 1,
              "created_utc": "2026-02-14 21:27:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eswc9",
                  "author": "shiftyleprechaun",
                  "text": "No I used risers: LINKUP PCIE 5.0 Riser Cable | for Vertical GPU Mount.\n\nCombo of 30 & 60 cm.\n\nI realized today that I need to move the last 3090 into slot 7, rather than 6.\n\nhttps://preview.redd.it/1tgi89f49jjg1.jpeg?width=1034&format=pjpg&auto=webp&s=0fb36e7f733fa61358a83dbc9ac69f51e22a865b",
                  "score": 1,
                  "created_utc": "2026-02-14 22:02:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5etayx",
              "author": "shiftyleprechaun",
              "text": "Also, nothing I bought has been used for mining. The only used components are the 3090s , which of course I cannot guarantee weren't used for mining, but based on when I met each person (buying them off of FB marketplace), they all told me they wernt using for mining, just gaming or photo/video and they upgraded to 5090.\n\nAside from the 3090s everything is brand new and assembled together myself.",
              "score": 1,
              "created_utc": "2026-02-14 22:04:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5eedx0",
          "author": "Weird-Abalone-1910",
          "text": "What did this cost you to put together? I want to build one to try to run kimi2.5",
          "score": 1,
          "created_utc": "2026-02-14 20:43:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etgzn",
              "author": "shiftyleprechaun",
              "text": "Total all in, including new dedicated electrical circuits installed, around $50k.",
              "score": 1,
              "created_utc": "2026-02-14 22:05:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5eu9ag",
                  "author": "Weird-Abalone-1910",
                  "text": "Wow, thanks for sharing.",
                  "score": 2,
                  "created_utc": "2026-02-14 22:10:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5esbr3",
          "author": "ApprehensiveView2003",
          "text": "what did you do to fix when it originally doesnt allow P2P?",
          "score": 1,
          "created_utc": "2026-02-14 21:59:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eufa7",
              "author": "shiftyleprechaun",
              "text": "Enable IOMMU (SVM Mode), set PCIe ACS override, disable ASPM/SR-IOV/C-States, force PCIe Gen 4, and if you're still having issues add pcie_acs_override=downstream,multifunction to your kernel boot parameters. The WRX90 workstation boards need all of this dialed in for clean multi-GPU P2P.",
              "score": 1,
              "created_utc": "2026-02-14 22:11:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ew17n",
          "author": "Correct_Lead_2418",
          "text": "Can I ask your budget for this? For 10k you could get the highest tier Mac studio with 512GB unified memory and 4tb storage",
          "score": 1,
          "created_utc": "2026-02-14 22:20:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f3m5w",
              "author": "shiftyleprechaun",
              "text": "I spent around $50k. But there is no way a mac with 512gb unified memory could do what this can. I can also continue to add to this as needed. Not so easy with Mac.\n\nAlso I hate apple and macs üòâ.",
              "score": 1,
              "created_utc": "2026-02-14 23:04:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g7w46",
                  "author": "lenjet",
                  "text": "For that kind of money you could have got circa 8-10 DGX Spark or an OEM and ended up with 1024-1280gb of unified VRAM‚Ä¶ coupled with the smaller footprint and much lower power consumption.\n\nDid you consider that?",
                  "score": 1,
                  "created_utc": "2026-02-15 03:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5g1tlg",
          "author": "Prigozhin2023",
          "text": "Is it cheaper than 2 x Dgx Spark?",
          "score": 1,
          "created_utc": "2026-02-15 02:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g2ce3",
              "author": "shiftyleprechaun",
              "text": "What I built? No.",
              "score": 1,
              "created_utc": "2026-02-15 02:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g357d",
                  "author": "Prigozhin2023",
                  "text": "Trying got understand which is the more cost effective build. One like yours, vs DGX Spark vs Mac Studio.",
                  "score": 1,
                  "created_utc": "2026-02-15 02:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gbrpv",
          "author": "Big_Championship1291",
          "text": "I know nothing about building an ai workstation and I‚Äôm planning on building my own. What is the highest model you can use with this station with reasonable speed for heavy software development?",
          "score": 1,
          "created_utc": "2026-02-15 03:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gdn91",
              "author": "shiftyleprechaun",
              "text": "With what I built?",
              "score": 1,
              "created_utc": "2026-02-15 04:07:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ge92n",
                  "author": "Big_Championship1291",
                  "text": "Yes",
                  "score": 1,
                  "created_utc": "2026-02-15 04:12:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jpftt",
          "author": "kidflashonnikes",
          "text": "something to consider: with this motherboard, if you want to expand compute, get PCIe lane spitters, going from 16x per one lane (exluding the one pcie 8x lane on this motherboard) and split every 16x pcie gen 5 lane in pcie 8x, which will double your compute. I currently use 4 RTX PRO 6000s, and I am getting another 8 RTX PROs delivered in the next few weeks, and will be doing this. ",
          "score": 1,
          "created_utc": "2026-02-15 18:17:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jyhk4",
              "author": "shiftyleprechaun",
              "text": "And by doing this, you find it running faster? What are you typically doing? Training?",
              "score": 1,
              "created_utc": "2026-02-15 19:01:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jza42",
                  "author": "kidflashonnikes",
                  "text": "I run a team at one of the worlds largest privately funded AI labs. My team works on inserting BCI (brain computer interface) threads into live test subjects (willingly of course) with brain damage to compress brainvwave data in real time via LLMs due to the data being too large and variate with current software ect. I work on personal compute tasks. One of my personal projects is using AI to predict bad thoughts/crime thoughts. I am currently using my set up for trainining and inference. I am almost done with my research, effectively I need more training compute to complete the version 1 of the model where I will be able to successfully predict negative behaviour of humans based on AI throughput. Some people will hate me for doing this, I dont care, I need to feed my family and you would not believe the prices certain US companies are willing to pay for this new LLM ",
                  "score": 2,
                  "created_utc": "2026-02-15 19:05:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5jzsqx",
                  "author": "kidflashonnikes",
                  "text": "as for the motherboard - you can split the 16x lanes into 8x lanes (2 each from 1 16x lane), to double your compute. Dropping from 16x to 8x does not impact inference that much - it does impact training substantially more. My current set up (all housed in a phanteks serve rpro 2 TG case): 1 TB of RAM, 16 TB of NVMe, 96 core CPU (thread ripper pro), 2000 Watt PSU, AIO for the CPU, regular fans to for cooling, 4 RTX PRO 6000s (6 more on the way before I transition to an open rig case), Linux (ubuntu)",
                  "score": 2,
                  "created_utc": "2026-02-15 19:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gfbve",
          "author": "Prize_Wolf_5859",
          "text": "I‚Äôm new to this AI game Brodie how you built this and how much did it cost",
          "score": 0,
          "created_utc": "2026-02-15 04:20:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0swmh",
      "title": "Is Local LLM the next trend in the AI wave?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r0swmh/is_local_llm_the_next_trend_in_the_ai_wave/",
      "author": "G3grip",
      "created_utc": "2026-02-10 06:02:01",
      "score": 99,
      "num_comments": 149,
      "upvote_ratio": 0.9,
      "text": "Suddenly I've been seeing a lot of content and videos centred around the cost of running LLMs vs paying subscriptions.\n\nCouple of months back it was all about Claude Code, very recently it is OpenClaw, now I feel, that by the coming week, everyone would be talking hardware and local LLM setups.\n\nIt will start with people raving about \"how low is the cost of local AI over time\", \"privacy\", \"freedom\", only to be followed by gurus saying \"why did I not do this earlier?\" and dropping crazy money into hardware setups. Then there will be an influx of 1-click setup tools and guides.\n\nHonestly, I've been loving all the exploration and learning with the past couple of trends, but I'll admit, it's a bit much to keep up with. I don't know, maybe I'm just crazy at this point.\n\nThoughts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r0swmh/is_local_llm_the_next_trend_in_the_ai_wave/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4kqfeh",
          "author": "jpinbn",
          "text": "I can't afford to sell an arm and a leg for RAM, so no LLM until prices come back to earth.",
          "score": 19,
          "created_utc": "2026-02-10 07:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kqxdw",
              "author": "corelabjoe",
              "text": "Uses Vram mostly not normal ram. I've run adorable little LLMs on even an old GTX1660 but presently run them on an RTX3060 12gb.",
              "score": 6,
              "created_utc": "2026-02-10 07:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mfjlw",
                  "author": "Prestigious_Ad572",
                  "text": "Running LLMs on RAM with CPU for inference is surprisingly cheap and with good enough performance that it becomes possible to use big models locally.",
                  "score": 3,
                  "created_utc": "2026-02-10 14:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ksm01",
              "author": "G3grip",
              "text": "Don't even get me started on the hardware cost these days. The market has constantly cursed ever since the crypto boom.\n\nFrom there, I've never seen \"normal\" pricing across the right components. It's always something or the other that jacks up the price of memory, GPUs, and whatnot. Then they rarely get down to the regular price.",
              "score": 5,
              "created_utc": "2026-02-10 07:20:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kqdhs",
          "author": "civil_politics",
          "text": "I mean local is always going to be limited by your gear - to have a ‚Äògood‚Äô local LLM you‚Äôre talking 5k minimum to have something reasonable and really 10k+ to get something that is significant value add - and at these prices you can get access to good online models for a long period of time. \n\nFor some usecases, and I think something like OpenClaw is one, there will be people building good local setups, but writ large I don‚Äôt see this trend taking off. I mean even with OpenClaw there are a ton of reviews basically saying it is near useless without hooking it up to one of the large models.",
          "score": 34,
          "created_utc": "2026-02-10 07:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kqxxn",
              "author": "G3grip",
              "text": "Cost is a big barrier to entry. Agree.\n\nUnfortunately, this type of first principle thinking only comes to sense after the wave settles.\n\nHaving that said, I think there's real value in budget, small and specialised LLMs. Use cases that we are yet to see. \n\nThere are always going to trade offs and advantages on both sides.",
              "score": 10,
              "created_utc": "2026-02-10 07:05:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4l0a45",
                  "author": "pot_sniffer",
                  "text": "For coding I've had good results with qwen2.5-coder 32b q4, using a rag with chromadb that contains all the info about the project.\n\nI've also had good results. Maybe better actually using Qwen3-Coder-Next 80b q6 without the rag as it caused it to be too slow. \n\nI'm running a 7950x, 64gb ddr5 and a 9060XT 16gb. Using llama.cpp. the 7950x isn't essential but it definitely helps, a 9070XT would've been better if I had the money at the time but this is working well enough I don't regret getting the 9060. Cost me ¬£1200 just before the ram crisis. Would be a few hundred more now but not out of reach",
                  "score": 9,
                  "created_utc": "2026-02-10 08:33:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n6im1",
              "author": "PeanutButterApricotS",
              "text": "Local ai isn‚Äôt to far from being able to be a quality agent. A lot of it is the process.\n\nI was able to build my own personal Ai agent app. It has worked better then openclaw because openclaw restricts quality items/features behind paywalls.\n\nI was able to make a agent\n\nI told the agent to have a way to check my weather locally\n\nThe agent made a python script after scraping the web and finding an api. It took the script and provided the weather.\n\nIt provided the wrong time zone (defaults to ust)\n\nI told it I needed CST\n\nIt went and fixed the time zone and updated the script.\n\n\n\nI couldn‚Äôt get openclaw to do that because it‚Äôs restricted to paying users.",
              "score": 2,
              "created_utc": "2026-02-10 16:59:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n71n8",
                  "author": "civil_politics",
                  "text": "I mean OpenClaw is just a wrapper - what models are you running locally? \n\nGimmicky things like single page websites or small little single API integration tasks are fun and neat, but they hardly move the needle on being practically viable. That being said getting that built on a local setup that costs less than 5k would be pretty impressive",
                  "score": 1,
                  "created_utc": "2026-02-10 17:01:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nqk7y",
                  "author": "cheechw",
                  "text": "What exactly are you talking about that's behind a paywall? I'm using Openclaw and there's nothing to even pay for right now. Am I missing something huge?\n\nAlso, Openclaw is open source. You can just fork it and change it to do what you want. Frankly, I don't even know if we're talking about the same thing here.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mjsry",
              "author": "danny_094",
              "text": "The situation will change once RAM prices recover. Current development efforts are also focused on allocating RAM to VRAM. AI is not even five years old in this form. Anything concerning the future is speculation.",
              "score": 1,
              "created_utc": "2026-02-10 15:12:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o44et",
                  "author": "stuffitystuff",
                  "text": "RAM manufacturers are selling unbuilt manufacturing capacity for a couple years from now. It's going to be a bit.",
                  "score": 3,
                  "created_utc": "2026-02-10 19:33:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mpnzs",
                  "author": "quantgorithm",
                  "text": "It‚Äôs not estimated to recover until 27 or 28.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:41:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4p02u9",
              "author": "nntb",
              "text": "The thing is I'm quite surprised how many things were able to overcome even though we have small gear or something like that. I never thought I'd be able to run you know one of the super big models without having an entire farm of gpus but then to see that I can not only run it but run it pretty decently using hard drive space on a solid state that's kind of amazing.",
              "score": 1,
              "created_utc": "2026-02-10 22:02:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4p1c24",
                  "author": "civil_politics",
                  "text": "Oh I completely agree it‚Äôs pretty incredible and we will move more and more towards a world where local is accessible, but I‚Äôm not sure if we get to a point where local can run truly impressive models that aren‚Äôt hyper specialized and even then for a decent chunk of change. How often do you need to leverage something that costs 10k to build?",
                  "score": 1,
                  "created_utc": "2026-02-10 22:08:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4pwr9q",
              "author": "alias454",
              "text": "I think using local models can solve the good enough problem very well though. A lot of things don't need SOTA models to perform decently well. The quality of relatively small local models is also getting better.",
              "score": 1,
              "created_utc": "2026-02-11 01:00:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l5pb5",
          "author": "truthputer",
          "text": "I think we‚Äôre coming up on a tipping point where free small local models make more sense than paid cloud models.\n\nThe big cloud models will always be better than the local models, but the gap between them is closing as the big models have diminishing returns on their improvements.\n\nIe: Local models are getting better faster than the cloud models are improving.\n\nLocal models will still be taking big jumps forward in the next 12 months - especially with the next generation of AI trained small LLMs giving similar result to models 10x bigger.\n\nWhereas cloud models might be a bit better? Like how Opus 4.6 is a bit better than 4.5 but some people have decided to stick with 4.5?\n\nWe‚Äôre already at the point where if you care about image generation, you‚Äôre probably running it locally. While cloud stuff is still ‚Äúbetter‚Äù and usually faster, it‚Äôs not as flexible with usage limits and content restrictions.\n\nLike you‚Äôre crazy to pay for online image generation with usage restrictions when you can just download Ollama or llmstudio and make as many images as you want, even tho it might take a few minutes for a high quality result.\n\nI can see in the next 12 months it might make more financial sense to have a $2000 mini PC with 128GB RAM running a local model that you connect Claude Code or KiloCode to vs paying $200 per month for a Claude Code subscription.",
          "score": 11,
          "created_utc": "2026-02-10 09:26:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8ijh",
              "author": "G3grip",
              "text": "Solid perspective. The way you placed it, I think it really makes sense.\n\nI just hope that the hardware pricing comes to its senses soon.",
              "score": 2,
              "created_utc": "2026-02-10 09:54:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4no0uu",
              "author": "chimph",
              "text": "Can local image gen really compare to nano banana pro tho?\nI‚Äôve gone deep into comfy ui in the past and tbf haven‚Äôt revisited for a while but recently have been using nano banana for help designing and renovating a house and it‚Äôs been superb. Nails the images. I feel like I would have to do a lot of tweaking for local and still get mixed results. But yes, not having limitations is nice.",
              "score": 2,
              "created_utc": "2026-02-10 18:19:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4owfhg",
              "author": "NaiveAccess8821",
              "text": "Very interesting take, I wondered what Local LMs are on top of your useage list",
              "score": 1,
              "created_utc": "2026-02-10 21:44:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kv114",
          "author": "BeatTheMarket30",
          "text": "Local LLMs are much less capable with 16-24GB GPU memory limit. We need cheap GPUs with like 128GB memory.",
          "score": 6,
          "created_utc": "2026-02-10 07:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l7y8h",
              "author": "Look_0ver_There",
              "text": "There's already machines we can buy today that have dedicated AI inference acceleration with that much memory for less than the price of a 5090.\n\nI would say that 256 or 512GB may be a better memory target for a very strong capable local machine that would get within spitting distance of the \"big boys\".\n\nThe thing is, that would democratize strong local AI compute to the masses, and that would break this artificial monopolistic bubble that has formed.\n\nI suspect that is the real reason why the tech companies have aligned to push prices sky high. To me, this is remiscent of the 80's to 90's, when vast compute power was in the hands of the few while the masses got scraps. The strong personal PCs of the time cost tens of thousands of dollars in today's value.\n\nThen PC\"s started to actually get good, and now everyone has on their desktop machines that cost a few thousand dollars that outperform machines that used to cost milliona.\n\nTo my mind, it's inevitable that this artificial bubble will break and access to wide scale high quality personal and private AI will start to become available to all. The big boys know it, and they're doing all the can to gate keep it and make money while the sun is shining.",
              "score": 5,
              "created_utc": "2026-02-10 09:48:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ozvul",
                  "author": "NaiveAccess8821",
                  "text": "If we, as a community, (or someone who built a company) can train a collection of agents by using a shared pool of individual data (with consent first), or experiences sampled from such small agents. \n\nThen with such agent swarm work coordinatively, don't we have a path beating those big guys? ",
                  "score": 1,
                  "created_utc": "2026-02-10 22:01:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4l1qvr",
              "author": "G3grip",
              "text": "One can only hope. RAM is expensive as is, V-RAM more so, plus we're at the mercy of OEMs to bundle it within GPUs. So you only get a decent amount alongside beefy, expensive GPUs.",
              "score": 2,
              "created_utc": "2026-02-10 08:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4klbmh",
          "author": "oldboi",
          "text": "I think the maturity of self-hosted LLM's is starting to reach a point of actual usefulness to those with user-end hardware. Tiny, effective MoE models, models under 20gb that match or outperform SOTA models from 12 months ago, more apps to use them with, and then the odd viral use-case like OpenClaw... \n\nI have been tinkering with them for a while but they weren't really that useful or effective until recently. Now I actually *use* them instead of *testing* them.",
          "score": 5,
          "created_utc": "2026-02-10 06:16:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l43l7",
              "author": "RnRau",
              "text": "Yup models have been getting better over the last year. \n\nThere is a paper out there that reckons that the capability per parameter doubles every 3.5 months.\n\nhttps://arxiv.org/abs/2412.04315",
              "score": 4,
              "created_utc": "2026-02-10 09:11:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lbvj4",
                  "author": "G3grip",
                  "text": "2x every 3.5 months is insane growth.",
                  "score": 1,
                  "created_utc": "2026-02-10 10:25:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4klwn0",
              "author": "G3grip",
              "text": "The most useful things that I hope we get out of this is simplified self hosting LLMs and practically skilled local LLMs under budget.",
              "score": 2,
              "created_utc": "2026-02-10 06:21:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kqrla",
                  "author": "corelabjoe",
                  "text": "Simplified? [Oh it's super simplified!](https://corelab.tech/private-chatgpt-local-llm/)\n\nYou can just fire a docker up with access to your gpu, load an LLM module and BAM! Start chatting with it...\n\nIt's been like this for AWHILE already ;)",
                  "score": 2,
                  "created_utc": "2026-02-10 07:03:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kukao",
          "author": "SebastianOpp",
          "text": "Remember to factor in electricity too ‚ö°Ô∏è",
          "score": 4,
          "created_utc": "2026-02-10 07:38:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l25fx",
              "author": "G3grip",
              "text": "And fire safety.\n\nPeople underestimate the effort that goes into uptime. Entire businesses run on maintaining hardware and redundancy.",
              "score": 2,
              "created_utc": "2026-02-10 08:51:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4nozcc",
              "author": "chimph",
              "text": "Again why Mac Minis are great. 5w idle",
              "score": 1,
              "created_utc": "2026-02-10 18:24:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mbj9n",
          "author": "Hector_Rvkp",
          "text": "A Strix halo for 2100$ (128) starts to be capable with large MoE models. It's slower and dumber than a frontier model, but it doesn't have to be slow or dumb. Moe models will obviously keep getting better. Meanwhile, models are running out of data to train on, synthetic data is often but helpful, and model improvement curve looks logarithmic, not linear, let alone exponential. Also, killing a fly with a bazooka isn't a sign of intelligence, so most people don't need a 1tb dense model to find a recipe. \nPrivacy concern is real. \nSkilling/ reskilling concern is real too. \nIt would be intelligent to develop skills and work flows to know when to use local compute, and maybe, when you really do need speed or compute, use the cloud. But running a dense pro model for every prompt sounds dumb, wasteful, and only sensible from the perspective of the cloud provider that's charging you per token irrespective of the slop you make. \nOne fear I have: letting a model running on the cloud overnight working on some multi step task to wake up to a large bill with garbage to show for. If that ran locally, all I've done is heat my place a little bit.",
          "score": 4,
          "created_utc": "2026-02-10 14:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kkk5r",
          "author": "hchen25",
          "text": "Yes, looking forward to build a machine to run local LLM.",
          "score": 3,
          "created_utc": "2026-02-10 06:09:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4krnlc",
              "author": "goobervision",
              "text": "Mac M1 Max Pro 64gb second hand isn't a bad shout on a budget.",
              "score": 3,
              "created_utc": "2026-02-10 07:11:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l4pft",
                  "author": "ScuffedBalata",
                  "text": "I found one of those, busted up Macbook with bad screen (i wasn't using it as a laptop anyway) for $700 recently.",
                  "score": 1,
                  "created_utc": "2026-02-10 09:16:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4klndk",
              "author": "G3grip",
              "text": "Me too! But I'm terrified of being so excited about every new thing all the time.",
              "score": 2,
              "created_utc": "2026-02-10 06:18:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kqpaj",
                  "author": "MrScotchyScotch",
                  "text": "I don't think you need to be scared of being excited",
                  "score": 1,
                  "created_utc": "2026-02-10 07:03:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l699a",
          "author": "hydropix",
          "text": "Local LLM are actually quite expensive.\n\nIn data centers, the hardware is used almost 100% of the time and is much better optimized. If you calculate precisely what it costs to amortize a configuration capable of running AI and its obsolescence after three years. You need to perform inference for at least 6 hours continuously per day (7 days a week) for the local option to be profitable. I advise you to ask an AI to give you the details of the calculations and depreciation costs; it's quite easy to compare different scenarios.\n\nThe difference with GPUs for gaming, where latency is critical, does not apply to the usual use of AI, so again, this does not argue in favor of local processing.\n\nOn the other hand, for those who want to work with AI, having the hardware to perform all the tests is very valuable. For others, unless you have very intensive use or data protection needs, the cloud is a better option.",
          "score": 3,
          "created_utc": "2026-02-10 09:32:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfn1k",
              "author": "profcuck",
              "text": "You're right about all that of course but despite the current blip in RAM prices (and SSDs) a broad version of Moore's Law still very much holds.\n\nIn 2016 \"Intel Core i7 Extreme Edition is the Most Powerful Desktop PC Processor Ever\" - https://www.thurrott.com/hardware/67574/computex-2016-intel-core-i7-extreme-edition-powerful-desktop-pc-processor-ever\n\nIt is slower than an Intel N150 which is now considered appropriate for a very basic mini pc.\n\nWe could dig a bit deeper:\nhttps://medium.com/@cli_87015/the-evolution-of-gpu-pricing-a-deep-dive-into-cost-per-fp32-flop-for-hyperscalers-cbf072b85bb5\n\nMy overall point?  While it is always going to be true that near 100% usage of datacenter hardware means that it will be cheaper to go that way, it won't be long until it's all plenty cheap enough to run good stuff locally.",
              "score": 2,
              "created_utc": "2026-02-10 10:59:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o5pus",
                  "author": "stuffitystuff",
                  "text": "Even now you can get an Nvidia A100 80GB on eBay for \\~$7k and they were around twice that a year ago, IIRC.",
                  "score": 2,
                  "created_utc": "2026-02-10 19:40:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l2g1j",
          "author": "NoobMLDude",
          "text": "LocalLLM is not ‚Äúthe next trend‚Äù , it‚Äôs already ‚Äúthe CURRENT TREND‚Äù practiced by many (including me) for few years .\n\nI‚Äôve been[trying to inform and educate](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) the general public that they do not need to give their money and personal data to big AI companies. We can get Local LLMs running and it might be enough for 90% or users. \n\nWhy would you share your Personal information with AI companies who will use it against you to show you/ manipulate you with Ads and other products !??\n\n\nNot just data, the offerings of AI APIs and services are very questionable:\n1. Customers of Perplexity paid for Pro account (advertised to have limit of 2000 research questions) noticed they have reduced the limit now tor 20. How a company can change the limits of a product after customers paid for  it is still unclear, but it‚Äôs happening.\n\n2. companies change the underlying model running behind the Chat Apps to lower quality model without telling us to save costs. Customers notice degradation in models few weeks after the model has generated buzz and captured attention.\n\nAll of this is happening and  the only way to have some control over your AI experience/access is to run your own model locally.\n\n\nHere‚Äôs some ideas for Private and Local AI uses:\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 5,
          "created_utc": "2026-02-10 08:54:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l5t0q",
              "author": "G3grip",
              "text": "Of course, I know many are doing it and even swear by it. \n\nWhen I say 'next trend,' I‚Äôm talking about a total takeover. I mean the kind of buzz where my phone is literally drowning in notifications from Google and X every single day. The same way everyone (myself included) is obsessed with OpenClaw right now.",
              "score": 2,
              "created_utc": "2026-02-10 09:27:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lolim",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-10 12:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m2dv8",
              "author": "G3grip",
              "text": "Hey, this looks great. \n\nCan I ask, what sort of a model / use case would I be able to run on a simple macbook (if any at all)?",
              "score": 2,
              "created_utc": "2026-02-10 13:40:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4mhm53",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-02-10 15:01:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lqs6l",
          "author": "techlatest_net",
          "text": "Totally feel you on the hype cycle overload‚ÄîClaude coding buzz to OpenClaw blowing up overnight, now everyone's eyeing local setups. Local LLMs are def picking up steam for privacy and no-sub fees long-term (like $1/day amortized vs $20 cloud), but that hardware drop (GPUs, rigs hitting $1k-3k) is gonna suck people in hard. I'm all in on the freedom angle, but yeah, it's exhausting keeping pace‚Äîstick to what works for your workflow and ignore the FOMO gurus!",
          "score": 2,
          "created_utc": "2026-02-10 12:27:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3aug",
              "author": "G3grip",
              "text": "Finally, someone shares my pain!",
              "score": 1,
              "created_utc": "2026-02-10 13:45:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lzay2",
          "author": "pot4scotty20",
          "text": "next trend? if getting squeezed by market to pay the maximum price is something that interests you? the time to build was 12 months ago, best to wait for prices to stabilize after these artificially inflated hype rates come crashing down",
          "score": 2,
          "created_utc": "2026-02-10 13:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3i6z",
              "author": "G3grip",
              "text": "üò≠",
              "score": 1,
              "created_utc": "2026-02-10 13:46:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mirj9",
          "author": "danny_094",
          "text": "I don't see locale execution as a trend. It's more like the logical next step. Locale LLM has existed since GPT. It's just becoming more user-friendly. It will eventually be possible to run large models on less powerful hardware. Not because magic happens, but because AI is still quite new and its development toward greater efficiency is still in its infancy. Tools like Clawbot simply make AI more appealing to the local market, which in turn generates more interest. OpenAI's first AI model was open source and available to everyone back then. So, locally.",
          "score": 2,
          "created_utc": "2026-02-10 15:07:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mk3fl",
          "author": "beedunc",
          "text": "It would have been, but not with these ram prices. \n\nMy $180 96GB sticks (needed to run big local models) are now almost  $1200. That‚Äôs more than I spent on everything else.",
          "score": 2,
          "created_utc": "2026-02-10 15:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mnbld",
          "author": "mr__smooth",
          "text": "The issue is memory. If there wasnt a supply constraint created by that company last year it would have been much more realizable. But now there is a shortage of memory that will impact what the average consumer can buy on their own",
          "score": 2,
          "created_utc": "2026-02-10 15:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n97f2",
              "author": "G3grip",
              "text": "This RAM pricing thing resonates with everyone.",
              "score": 1,
              "created_utc": "2026-02-10 17:11:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4oqs2p",
              "author": "VaporwaveUtopia",
              "text": "The RAM shortage is shitty, but it won't last forever. There's demand and money to be made, so it won't be long before existing manufacturers scale up, or new players get into the space.",
              "score": 1,
              "created_utc": "2026-02-10 21:18:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4msbgv",
          "author": "robbro9",
          "text": "Nah, it's quantum blockchain ai..... With COVID.  Seriously i how your right.  I get the feeling the ai cos are booking up all the hardware needed to run ai locally but they can't sustain that.  Imagine memory prices from a year ago, ai would be so much more accessible with that locally.",
          "score": 2,
          "created_utc": "2026-02-10 15:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4na20n",
              "author": "G3grip",
              "text": "I feel like I should have bulk-ordered RAM before the price hike; I would have been rich. The return on these things has turned out to be better than gold... lol...",
              "score": 1,
              "created_utc": "2026-02-10 17:15:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4msq8t",
          "author": "AppointmentAway3164",
          "text": "Yes. Privacy is important. But also I feel like the cloud based models have hit a wall and aren‚Äôt advancing sufficiently to make their position a requirement for ai. I can get similar results locally, and so the opportunity is present for local models to gain popularity.\n\nAs a dev I really am not interested in constantly improving Anthropic‚Äôs models, which is what you‚Äôre doing when using them. You‚Äôre paying them to reinforcement train their models. And at the end of the day, I want my source code private. \n\nThe nvidia DGX Spark is really nice, and apple would be smart to position themselves as local LLM hosts. We will see if Tim Apple can see the forest for the trees.",
          "score": 2,
          "created_utc": "2026-02-10 15:55:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nbxlz",
              "author": "G3grip",
              "text": "Good points. And with models like Kimi 2.5 available for free, the hardware is the only limitation. However, if you want to run something like that locally and do not already own the hardware to support it, then the monthly subscriptions start to look good.\n\nOnly if you are actually dropping 1000s of dollars on the subs every month, you should then get a machine so powerful (and expensive).\n\n  \nBut my hope is that in time we will have small, specialised and efficient models that most people can run decent personal machines. Leaving the subscription-based ones on for heavy-duty work that not everyone needs all the time.",
              "score": 1,
              "created_utc": "2026-02-10 17:24:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mt8nj",
          "author": "locai_al-ibadi",
          "text": "Local LLMs (and AI in general) has been improving massively with lower computing resources/specs. It is costly to have a \"good\" local LLM but not as much as it used to be 6+ month ago. Honestly, I think for a lot of the daily average use cases of LLMs, people do not need cloud servers running those massive models. ",
          "score": 2,
          "created_utc": "2026-02-10 15:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc367",
              "author": "G3grip",
              "text": "I second this.",
              "score": 2,
              "created_utc": "2026-02-10 17:24:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtaes",
          "author": "mpw-linux",
          "text": "The question is do we really want to have a local super computer to run high-end LLM's? Cost of hardware, cost of power to run the machines. For price of buying all the hardware one can just pay the 30-50 USD a. month to let the cloud computers do all the work for us. Local LLM's are a great for experimentation, learning models, training, etc. \n\nI think that the future will be more powerful quantized versiions of powerful LLM's for local usage like what they are doing at Liquid AI. ",
          "score": 2,
          "created_utc": "2026-02-10 15:58:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ncj27",
              "author": "G3grip",
              "text": "Intrusting stuff, this Liquid AI. First time I got to know. Thanks.\n\nI agree.",
              "score": 1,
              "created_utc": "2026-02-10 17:26:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nmv6a",
          "author": "chimph",
          "text": "Local LLM will become a bigger part of people‚Äôs toolsets for sure. And I think the whole Clawdbot + Mac Mini has made people more aware of local LLM. This years Mac M5 will be known for driving good local models",
          "score": 2,
          "created_utc": "2026-02-10 18:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o9vdq",
          "author": "darkestvice",
          "text": "Given that said AI buildup has skyrocketed the cost of all PC parts required to run not-so-garbage local LLMs, I highly doubt Local LLMs will become trendy any time soon. It's still squarely in an up front cost range that's out of reach for the majority of people.",
          "score": 2,
          "created_utc": "2026-02-10 20:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v1ldh",
              "author": "G3grip",
              "text": "Fair enough, I keep telling myself ‚ÄùPrices have to come down, technology is supposed to get cheaper with time, these are just temporary bumps.\", BUT... they never do!\n\nHow we are going backwards in technology, it's beyond me. I mean, ya, I know why the costs are high, I understand the technical reasons but, like, as a whole, I just can't comprehend it at this point.",
              "score": 1,
              "created_utc": "2026-02-11 20:26:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ozqni",
          "author": "nntb",
          "text": "You're going to find a lot of videos comparing the cost of local versus software as a solution because there's a lot of money being poured into software as a solution right now. And a lot of companies have to recoup that cost so they may be fighting each other and they may be offering stuff for inexpensive for now until the user base gets big enough it can warrant them increasing costs. I've been on this subreddit for a really long time and I've been involved in locally running AI for years I would say that it's not a new thing that local is a popular option. And I would say it's the best option to be 100% honest. Now I know those who don't have hardware of their own will go and use things like Google collab or other services to host theirs and that's fine but really the heart of this entire suburb has been locally running AI",
          "score": 2,
          "created_utc": "2026-02-10 22:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p3i2y",
          "author": "billytryer",
          "text": "I was thinking of buying a M4 Mac mini ¬£900new new with 24gb of ram is that good for ollama and Some of the smaller LLMs , I saw here that the M1 Max has a better performance ? I just don't know",
          "score": 2,
          "created_utc": "2026-02-10 22:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v51ds",
              "author": "G3grip",
              "text": "Ya, one dude did mention that, I think 2 days back. While I don't know much about the M1 and M4 comparison, many folks have told me that smaller models on Apple silicon Macs are no issues.",
              "score": 2,
              "created_utc": "2026-02-11 20:43:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pcf1d",
          "author": "epSos-DE",
          "text": "Yes. Logical Bit Operators on the CPU make it 100% possible.\n\nGPU vector calualtions can assist.\n\nIT is possible to have local LLMs that reduce the decision space with binery trees and run logical operators , instead of vector search. LLM with logical bit operators and some GPU offload can run on one CPU core , if it can prune the decision space correctly.\n\nTraining the Ai is still hard tho ! Needs server farms or p2p bot swarms that share computational load.\n\n\n\nBit Logical LLMs will need to be trained for logical bit operators or the vector space LLM traiing data will need to be converted to bit operators and binary decision trees  for look ups of data.\n\n\n\nTechnically it is possible, but NVIDIA wants to sell more GPUs !!! SO they train vectors all day long, because Bit operations would harm their profits !",
          "score": 2,
          "created_utc": "2026-02-10 23:05:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v5msc",
              "author": "G3grip",
              "text": "I feel dumb after reading your comment. I gotta learn so much about these LLMs.",
              "score": 1,
              "created_utc": "2026-02-11 20:46:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pqcjk",
          "author": "HealthyCommunicat",
          "text": "i was pretty dissapointed because even with 350+ gb of VRAM clustered, it felt hard making any LLM have a fast enough pp/tgs to feel really useful, (20 token/s is not realistically useful for someone who uses a agentic cli tool literally 8+ hrs a day) - but the most recent qwen 3 coder next gives me hope, i really wish companies would focus more on speed, minimax and stepfun are also two that come to mind, but the 200b models seem to be the perfect golden spot for those that want to spend 5-10k to run models from home.",
          "score": 2,
          "created_utc": "2026-02-11 00:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v6hw5",
              "author": "G3grip",
              "text": "With how much VRAM!?!?!  üòµ",
              "score": 1,
              "created_utc": "2026-02-11 20:50:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v0ogy",
          "author": "Hydroskeletal",
          "text": "If you are writing software; maybe? \n\nBut if you're doing stuff where you are ingesting lots and lots of tokens it becomes quite easy to see where the token pricing model becomes prohibitive. So that leaves you with using instance pricing. Well now you need to make sure you're not leaving the thing idle. Even then not hard to see a weekly bill in the hundreds. At that point ownership vs renting is a real discussion and you can see break-even in months not years. \n\nBut the real hinge is if spot instance pricing remains stable. I am skeptical about how much runway there really is for the accelerating hunger.",
          "score": 2,
          "created_utc": "2026-02-11 20:21:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53qdwj",
          "author": "StretchyPear",
          "text": "The real problem is context window size on consumer hardware, even at like 256GB you're not going to get great performance with 256k content windows, that being said optimization is happening all the time and will be available to both hosted and local models, but it comes at a cost of specificity, which you may want as you increase context.",
          "score": 2,
          "created_utc": "2026-02-13 03:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o542o1j",
              "author": "G3grip",
              "text": "Apologies I don't know much around this, but does 1GB RAM= 1k content window?",
              "score": 1,
              "created_utc": "2026-02-13 04:59:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o55qkir",
                  "author": "StretchyPear",
                  "text": "It varies based on model parameter size and because you also need to store the model weights in memory and the tokens can be cached at different specificity.  ",
                  "score": 2,
                  "created_utc": "2026-02-13 13:18:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56pgct",
          "author": "aibyzee",
          "text": "I feel this 100%. Every few months there's a new 'this is the future' wave firstly it was cloud API's then AI Copilots and now local LLMs.   \nI do think local models will grow especially for privacy and cost control long-term but for most people cloud is still more practical. Not everyone wants to manage GPUs and configs just to run a chatbot.  \nMaybe its not about one replacing the other but an another layer in the ecosystem. Use cloud for convinience and local for control.",
          "score": 2,
          "created_utc": "2026-02-13 16:16:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56ygly",
              "author": "G3grip",
              "text": "Quite a balance opinion. Agree.",
              "score": 1,
              "created_utc": "2026-02-13 16:58:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dqvzs",
          "author": "hejj",
          "text": "Of course there will come a point where either consumer grade hardware is powerful enough, and/or AI (note I don't say \"LLM\") technology has been streamlined enough to be highly practical to run locally.  But this is purely speculative;  We'll either slowly trend towards that point of local hardware meeting freely available AI models computing needs over the course of the next 5 to 10 years, or (I think more likely) we'll have a fundamental rethink and reset of how generative AI gets done in a way that less powerful hardware is needed (e.g. NPU vs GPU, analog computing, etc.).",
          "score": 2,
          "created_utc": "2026-02-14 18:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dyfo7",
              "author": "G3grip",
              "text": "As long as there is motivation, eventually there will be a way to do it easily.",
              "score": 1,
              "created_utc": "2026-02-14 19:17:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l1fsp",
          "author": "beryugyo619",
          "text": "Of course it would be great if you could just run Claude Opus(any) locally for free, if that could be done at all. It's not the matter of demand but feasibility.",
          "score": 1,
          "created_utc": "2026-02-10 08:44:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1yzb",
              "author": "G3grip",
              "text": "What if we don't need Opus? What if there's real use in much smaller but specialised and practical models that may run on much cheaper hardware?\n\nI mean ya, it's all speculation but the possibilities are exciting.",
              "score": 1,
              "created_utc": "2026-02-10 08:50:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l4f46",
          "author": "ScuffedBalata",
          "text": "No, unless there's a breakthrough that makes the ever-larger scaling of models breakdown somehow.\n\nThe cloud-based \"frontier\" models will always be (at least in the short/medium term) SIGNIFICANTLY more capable than the local ones.",
          "score": 1,
          "created_utc": "2026-02-10 09:14:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l6pg4",
              "author": "G3grip",
              "text": "Yes, but do we always need the best model at all times? \n\nI mean, think of it this way, I can rent a ridiculously powerful machine on AWS. They will always have servers more powerful than any personal computer that I can go out and buy. \n\nI still own a very low-powered laptop for my personal day-to-day tasks, as I don't need a 128-core CPU for running chrome and checking email (RAM is a different story for chrome... lol...).",
              "score": 2,
              "created_utc": "2026-02-10 09:36:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4mpqk5",
                  "author": "ScuffedBalata",
                  "text": "For Coding?  I feel like \"often\".\n\nI mean even when you have a better model, i find it makes more elegant code, it plans better and architects better for future upgrades and cleaner code.  \n\nA lower quality model might make it work, but it may do it with spaghetti code, or choose a suboptimal library that doesn't account for something.\n\nJust my feeling doing a lot of AI-backed development.\n\nIf you're just chatting or asking about muffin recipies, then no, no you don't.  But even if you're asking about research, the actual concreteness and reduction of \"AI buzz\" (aka mild hallucination and exageration) is better with larger models.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:41:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4li35j",
          "author": "Soft-Dragonfruit6447",
          "text": "Local LLMs feel inevitable long-term, but hardware cost is still the real gate. Once RAM/VRAM prices drop and setup gets easier, adoption will explode. Right now it‚Äôs a power-user game, not mainstream yet.",
          "score": 1,
          "created_utc": "2026-02-10 11:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lmx4e",
          "author": "tcoder7",
          "text": "I do not thunk so. The cartels colluded to price gouge [RAM.So](http://RAM.So) the next waves of PC will have lower RAM. Also the best LLM models are still not available to download locally. There is still some low proba chance of local private LLM winning the race if there is an innovation to dramatically reduce RAM and VRAM needs.  ",
          "score": 1,
          "created_utc": "2026-02-10 11:59:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m1guj",
              "author": "G3grip",
              "text": "Wait, doesn't the RAM pricing impact the \"cartel\" too?",
              "score": 1,
              "created_utc": "2026-02-10 13:35:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m2o3f",
                  "author": "tcoder7",
                  "text": "Not if you are the first buyer.",
                  "score": 1,
                  "created_utc": "2026-02-10 13:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lsne4",
          "author": "LeaderBriefs-com",
          "text": "The hardware you would need to run a competent LOCAL LLM is really cost prohibitive for the majority. \n\nYou will always trade quality for time and money. \nThat doesn‚Äôt really scale in my mind.",
          "score": 1,
          "created_utc": "2026-02-10 12:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3408",
              "author": "G3grip",
              "text": "I think for now, yes.\n\nIn time, smaller models will get better and small setups would be viable.",
              "score": 1,
              "created_utc": "2026-02-10 13:44:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nrdcz",
          "author": "No_Vehicle7826",
          "text": "If they ever make local LLMs efficient enough to run good models of basic hardware, closed ai will just be government ai \n\nUntil then, there's Venice for us broke peeps without a rig lol\n\nLeChat is pretty cool too. There's too much \"safety\" on the ChatGPT, Claude and Gemini... then Grok just seems dumb to me. Hoping Grok 4.20 is cool though, supposed to be 6 trillion parameters",
          "score": 1,
          "created_utc": "2026-02-10 18:34:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nwmyp",
          "author": "Budget-Length2666",
          "text": "Not right now - it will cost you 20k+ to run a similar high end model locally in some cluster just to be sort of on equal playing field with the frontier foundational models. No guarantees your hardware you buy today will be able to compete with the frontier models of 2 years from now. And tps is generally slower locally, even with latency considered. inference is just hard and expensive.",
          "score": 1,
          "created_utc": "2026-02-10 18:58:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o7dqm",
          "author": "Zestyclose_Paint3922",
          "text": "Local will always be behind. I think this is more for corporate \"limited\" applications.",
          "score": 1,
          "created_utc": "2026-02-10 19:48:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4och8n",
          "author": "abe-azam",
          "text": "If you use high end models, local won‚Äôt do.\n\nIf you want something basic the it will pass.",
          "score": 1,
          "created_utc": "2026-02-10 20:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pl2sj",
          "author": "Apprehensive_Gap3673",
          "text": "I don't think it will honestly ever be a trend.¬† Local LLM can't compete with frontier models in terms of performance and most people wouldn't be able to set them up or be willing to pay the upfront costs",
          "score": 1,
          "created_utc": "2026-02-10 23:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v5v1y",
              "author": "G3grip",
              "text": "I would put my money on the other post. But I guess only time will tell.",
              "score": 1,
              "created_utc": "2026-02-11 20:47:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qtpvx",
          "author": "Shoddy_Bed3240",
          "text": "Two Mac Ultra M3 machines with 512GB of memory are probably the best option if you want to run very large local models at a quality level comparable to paid subscriptions ‚Äî but the total cost is around $30,000.",
          "score": 1,
          "created_utc": "2026-02-11 04:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v6rqr",
              "author": "G3grip",
              "text": "The Mac mini studios (or whatever they call them) are cheaper, no?",
              "score": 1,
              "created_utc": "2026-02-11 20:51:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4vcpx1",
                  "author": "Shoddy_Bed3240",
                  "text": "It‚Äôs cheaper, but only the M3 Ultra comes with 512‚ÄØGB of unified memory.",
                  "score": 1,
                  "created_utc": "2026-02-11 21:20:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sj36n",
          "author": "FinancialBandicoot75",
          "text": "With cost as a factor, no.  It‚Äôs fun for small llm but not too practical.  Yes you can use a 3060ti and be ok, but if you want better you have to spend a buck on vid cards and ram and with ram nuts today, good luck.",
          "score": 1,
          "created_utc": "2026-02-11 13:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eqovw",
          "author": "Simple-Fault-9255",
          "text": "Always was.¬†",
          "score": 1,
          "created_utc": "2026-02-14 21:50:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nf0xi",
          "author": "One-Distribution-376",
          "text": "its not the future they want, you'll own nothing and you will be happy!",
          "score": 1,
          "created_utc": "2026-02-10 17:38:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nh713",
              "author": "G3grip",
              "text": "I heard Linus say the same thing today.",
              "score": 1,
              "created_utc": "2026-02-10 17:48:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4knwms",
          "author": "woundedkarma",
          "text": "Yeah... I think it could be a business for a while. I just don't know how to find clients. LOL  There's bound to be people who want local llms but don't know how to do it. \n\nI posted in a local sub and got zero interest. I need more outreach but lol I'm an introvert.",
          "score": 0,
          "created_utc": "2026-02-10 06:38:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kv9ys",
              "author": "Wide_Brief3025",
              "text": "Finding clients for niche tech like local LLMs can be tough, especially if cold outreach is not your thing. Try looking for communities where small businesses or devs vent about AI challenges and jump in to share your knowledge. Tools like ParseStream can help by tracking those discussions across multiple platforms so you do not miss any real opportunities to help.",
              "score": 3,
              "created_utc": "2026-02-10 07:45:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ko9jn",
              "author": "G3grip",
              "text": "These days you can get outreach white sitting in your room. Try posting at multiple places maybe? Also, care to share that post? I'm intrigued to see.",
              "score": 2,
              "created_utc": "2026-02-10 06:41:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kp59l",
                  "author": "woundedkarma",
                  "text": "[https://www.reddit.com/r/kzoo/comments/1qvavf9/fully\\_local\\_ai/](https://www.reddit.com/r/kzoo/comments/1qvavf9/fully_local_ai/) ",
                  "score": 1,
                  "created_utc": "2026-02-10 06:49:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2t35r",
      "title": "Tutorial: Run GLM-5 on your local device!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/1047rus1c2jg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-12 13:13:49",
      "score": 97,
      "num_comments": 62,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r2t35r/tutorial_run_glm5_on_your_local_device/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4z7vu1",
          "author": "No_Clock2390",
          "text": "So you need like a 10K PC to run this?",
          "score": 22,
          "created_utc": "2026-02-12 13:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zgxbe",
              "author": "UseMoreBandwith",
              "text": "yes. Does that surprise you?",
              "score": 6,
              "created_utc": "2026-02-12 14:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zjgue",
                  "author": "No_Clock2390",
                  "text": "I guess not",
                  "score": 2,
                  "created_utc": "2026-02-12 14:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zuwuq",
              "author": "Prudent-Ad4509",
              "text": "Let's see... $700 per 24Gb vram GPU, and you need about 12 of them to run dynamic 2-bit with a little bit of context. That is already $8400. The epyc system with 128 PCI lanes and PCIe 4.0 will set you back another $1000 in current prices, and that is the price \\*without\\* ram or ssd. Well, add another $200 for whatever low ram stick you can find just to run the thing. The remaining $600 will go on connecting all that with x8 bifurcation. This is where you are out of money, but you need 2-3 of more good PSUs and at least one SSD drive.\n\nSo no, 10K PC will not run this. But 11K-12K PC could. Or you can try to run it in ram, but I would not call it \"running\".",
              "score": 5,
              "created_utc": "2026-02-12 15:34:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o513tiu",
                  "author": "Particular-Way7271",
                  "text": "Or in ssd or hdd",
                  "score": 1,
                  "created_utc": "2026-02-12 19:03:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zle3w",
              "author": "Salt-Willingness-513",
              "text": "Just alot of ram should work somewhat too i guess. At least if you dont need high speed. Ima try it on my 850gb ram server",
              "score": 2,
              "created_utc": "2026-02-12 14:47:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zngmg",
                  "author": "No_Clock2390",
                  "text": "What kind of server is it? How much did it cost?",
                  "score": 1,
                  "created_utc": "2026-02-12 14:57:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5264zx",
                  "author": "bLackCatt79",
                  "text": "what speed did you got?",
                  "score": 1,
                  "created_utc": "2026-02-12 22:07:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o514gfg",
              "author": "fallingdowndizzyvr",
              "text": "Nope. I run GLM on my little Strixy with a little help from a Mac and a 7900xtx. I might have to add another 7900xtx for GLM-5 since it's a bit bigger than 4.7.",
              "score": 2,
              "created_utc": "2026-02-12 19:07:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o514qtq",
                  "author": "No_Clock2390",
                  "text": "You're running the 1-bit version?",
                  "score": 1,
                  "created_utc": "2026-02-12 19:08:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bd58k",
                  "author": "Sociedelic",
                  "text": "Really? And how much ram  you'll need?",
                  "score": 1,
                  "created_utc": "2026-02-14 09:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o54t6e2",
              "author": "JacketHistorical2321",
              "text": "No, I can run 4bit on a server I paid a total of $2k. Between 6-8t/s depending on ctx. People just like to be dramatic",
              "score": 1,
              "created_utc": "2026-02-13 08:47:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zn4wi",
          "author": "not-really-adam",
          "text": "I wonder if running this in 1-bit, would provide better local coding results than qwen3-next-coder in 8-bit?",
          "score": 5,
          "created_utc": "2026-02-12 14:56:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50c61w",
              "author": "entr0picly",
              "text": "That‚Äôs genuinely an open question in the field. The quantization vs parameterization curve has suggested that larger models at lower quant may perform better than smaller models at larger (or no) quant. There isn‚Äôt a one size fits all answer. It‚Äôs at the frontier, and you have to test your own use cases yourself. Personally, testing 2bit deepseek R1, I found it generally did better with scientific work than qwen3, however it also tended to drift more quickly and maybe struggle a little more with memory.",
              "score": 7,
              "created_utc": "2026-02-12 16:54:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54wxa7",
              "author": "Septimus4_FR",
              "text": "I can‚Äôt test it myself since I don‚Äôt have the hardware for that setup, but in general 1-bit (and usually 2-bit) quants degrade too much to be great for coding.\n\nOnce you drop that low, models tend to hallucinate more, lose consistency and are not very usable in practice. For coding, that usually shows up as wrong APIs, subtle logic bugs, or broken refactors, invalid json generation. In practice, 4-bit is often considered the lowest ‚Äúcomfortable‚Äù range for usable quants.\n\nThat said, it really depends on the quantization method and how well it‚Äôs done. A very good 3-bit quant of GLM-5 could actually be interesting to try. But I‚Äôd be very skeptical that a typical 1-bit GLM-5 would outperform an 8-bit Qwen coder for real coding work.",
              "score": 2,
              "created_utc": "2026-02-13 09:22:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zvp8o",
          "author": "Jumpy-Requirement389",
          "text": "So.. if I have 192GB of ddr5 and a 5090. I‚Äôll be able to run this?",
          "score": 4,
          "created_utc": "2026-02-12 15:38:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwgzl",
              "author": "robertpro01",
              "text": "Probably, try it and share results :)",
              "score": 1,
              "created_utc": "2026-02-12 15:41:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5143aj",
                  "author": "Ell2509",
                  "text": "I think they might be mocking",
                  "score": 1,
                  "created_utc": "2026-02-12 19:05:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bdvkh",
              "author": "Sociedelic",
              "text": "Does DDR4 vs DDR5 matter when running LLM locally?",
              "score": 1,
              "created_utc": "2026-02-14 09:40:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zzakp",
          "author": "separatelyrepeatedly",
          "text": "honestly what even is the point with such small quants?",
          "score": 4,
          "created_utc": "2026-02-12 15:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o544kd5",
              "author": "yoracale",
              "text": "You can see benchmarks we did for 1-bit DeepSeek-V3.1 which is smaller than GLM: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\n3-bit is very good and surprisingly near full precision \n\nAlso if you don't want to use lower precision, just use higher precision",
              "score": 1,
              "created_utc": "2026-02-13 05:13:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50al9g",
          "author": "Kubas_inko",
          "text": "2-bit and 1-bit are gonna be absolutely worthless. 3-bit might be somewhat usable.",
          "score": 4,
          "created_utc": "2026-02-12 16:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50eksi",
              "author": "silenceimpaired",
              "text": "I have found 2 bit acceptable for my use for GLM 4.7. I suspect for some use cases 2 bit on GLM 5 will beat models at around the same size or a little lower. I prefer GLM 4.7 to GLM Air.",
              "score": 3,
              "created_utc": "2026-02-12 17:05:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50jn4p",
                  "author": "Kubas_inko",
                  "text": "From my own testing, and for my purpose, Q2 GLM 4.7 is worse than Q6 GLM 4.5 Air.",
                  "score": 1,
                  "created_utc": "2026-02-12 17:29:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o514k9j",
              "author": "fallingdowndizzyvr",
              "text": "That's not true at all. I run TQ1, 1 bit, and find it pretty darn usable.",
              "score": 2,
              "created_utc": "2026-02-12 19:07:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o544tmo",
              "author": "yoracale",
              "text": "You can see benchmarks we did for DeepSeek-V3.1 which is smaller than GLM: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\n1-bit is absolutely useable since it's dynamic. And 3-bit is much better though.\n\nAlso if you don't want to use lower precision, just use higher precision",
              "score": 2,
              "created_utc": "2026-02-13 05:15:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57lcgq",
          "author": "dreamer2020-",
          "text": "Many thanks master! \n\nI have couple questions, I have maxed out Mas studio, so 512gb. What I really found difficult is to test the models from unsloth against like glm 4.7 6-bit. I need to dive into how and what it means to be unsloth dynamic. \n\nMaybe stupid question, what is the best model in terms of agentic coding ? Like using it for openclaw ? What should you use ?",
          "score": 2,
          "created_utc": "2026-02-13 18:48:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zbsfr",
          "author": "TimWardle",
          "text": "I wonder if additional languages except from English REAP‚Äôed from the model can reduce the size further while maintaining usability.",
          "score": 2,
          "created_utc": "2026-02-12 13:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52o7x3",
          "author": "minilei",
          "text": "Dam what even is the performance to run this locally with actual consumer hardware.",
          "score": 1,
          "created_utc": "2026-02-12 23:43:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54rhcc",
              "author": "yoracale",
              "text": "With Mac maybe 15 tokens/s. With RAM + VRAM you can get 20 tokens/s. With GPU pure, then 100 tokens/s",
              "score": 1,
              "created_utc": "2026-02-13 08:31:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54ued2",
          "author": "lol-its-funny",
          "text": "I can save you $10k ‚Ä¶ just run the 0b quant. It‚Äôs incredibly fast, as if nothing‚Äôs going on! Must try!",
          "score": 1,
          "created_utc": "2026-02-13 08:58:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54uy4m",
              "author": "yoracale",
              "text": "You can see benchmarks we did for 1-bit DeepSeek-V3.1 which is smaller than GLM: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs)\n\n3-bit is very good and surprisingly near full precision\n\nIf you don't want to use lower precision, just use higher precision.\n\nIf you don't want to run it at all, jsut run smaller models",
              "score": 1,
              "created_utc": "2026-02-13 09:03:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56bn0t",
          "author": "Dr-Coktupus",
          "text": "Makes zero sense to run locally, just run it from a cloud provider",
          "score": 1,
          "created_utc": "2026-02-13 15:09:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56qrna",
              "author": "yoracale",
              "text": "If you have the compute requirements, why not local?",
              "score": 1,
              "created_utc": "2026-02-13 16:22:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o579yr3",
          "author": "lol-its-funny",
          "text": "Why do you guys never publish the K-L divergence of your quants against the ***unquantized model***???",
          "score": 1,
          "created_utc": "2026-02-13 17:54:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59gag8",
              "author": "yoracale",
              "text": "We did do benchmarks for many models previously here: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\nRunning kl divergence benchmarks are expensive and time consuming",
              "score": 1,
              "created_utc": "2026-02-14 00:40:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bfafb",
          "author": "emrbyrktr",
          "text": "Qwen 3 Coder does the same job as next 80b.",
          "score": 1,
          "created_utc": "2026-02-14 09:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bjxi5",
              "author": "Medical_Farm6787",
              "text": "So why this is under GLM post?",
              "score": 1,
              "created_utc": "2026-02-14 10:40:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zmec4",
          "author": "dropswisdom",
          "text": "Yeah.. No. You basically need a server farm to run this locally.",
          "score": 1,
          "created_utc": "2026-02-12 14:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50fa24",
          "author": "squachek",
          "text": "1 bit quant? GTFOH",
          "score": 1,
          "created_utc": "2026-02-12 17:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o514xn8",
              "author": "fallingdowndizzyvr",
              "text": "Try it. I think it runs fine.",
              "score": 3,
              "created_utc": "2026-02-12 19:09:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o544qzo",
              "author": "yoracale",
              "text": "You can see benchmarks we did for DeepSeek-V3.1 which is smaller than GLM: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs\n\n1-bit is absolutely useable since it's dynamic. And 3-bit is much better though.\n\nAlso if you don't want to use lower precision, just use higher precision",
              "score": 2,
              "created_utc": "2026-02-13 05:15:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o549ihy",
                  "author": "squachek",
                  "text": "Just use higher precision he says! Sheesh. I‚Äôll just will 256gb of VRAM into existence! üò≠",
                  "score": 0,
                  "created_utc": "2026-02-13 05:52:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zlvi5",
          "author": "rookan",
          "text": "Which bit do you recommend for software development to be as smart as Claude Opus 4.6?",
          "score": 0,
          "created_utc": "2026-02-12 14:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52hnrz",
              "author": "zekrom567",
              "text": "None locally will get there unless you have a lot of money to fork over. I'm liking the gpt-oss-120b so far agentic programming",
              "score": 1,
              "created_utc": "2026-02-12 23:07:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1jdq3",
      "title": "My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r1jdq3/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/",
      "author": "BetaOp9",
      "created_utc": "2026-02-11 01:14:08",
      "score": 63,
      "num_comments": 42,
      "upvote_ratio": 0.9,
      "text": "I didn't want to buy two systems. That was the whole thing.\n\nI needed a NAS. I also wanted to mess around with local LLMs. And I really didn't want to explain to my wife why I needed a second box just to talk to a chatbot that sometimes hallucinates, I have my father-in-law for that. So when I was specing out my NAS build, I went a little heavier than most people would and crossed my fingers that the system could pull double duty down the road.\n\nHonestly? I was prepared to be wrong. Worst case I'd have an overpowered NAS that never breaks a sweat. I could live with that.\n\nBut it actually worked. And way better than I expected.\n\n**The Build**\n\n* Minisforum N5 Pro\n* AMD Ryzen AI 9 HX PRO 370 (12c/24t, 16 RDNA 3.5 CUs)\n* 96GB DDR5-5600 (2x 48GB SO-DIMMs)\n* 5x 26TB Seagate Exos in RAIDZ2 (\\~70TB usable)\n* 2x 1.92TB Samsung PM983 NVMe (ZFS metadata mirror)\n* TrueNAS SCALE\n\nDay to day it runs Jellyfin with VAAPI hardware transcoding, Sonarr, Radarr, Prowlarr, qBittorrent, FlareSolverr, Tailscale, and Dockge. It was already earning its keep before I ever touched LLM inference.\n\n**The Experiment**\n\nThe model is Qwen3-Coder-Next, 80 billion parameters, Mixture of Experts architecture with 3B active per token. I'm running the Q4\\_K\\_M quantization through llama.cpp with the Vulkan backend. Here's how it actually went:\n\n**3 tok/s** \\- First successful run. Vanilla llama.cpp and Qwen3-Coder-Next Q8 quantization, CPU-only inference. Technically working. Almost physically painful to watch. But it proved the model could run.\n\n**5 tok/s** \\- Moved to Q4\\_K\\_M quantization and started tuning. Okay. Nearly double the speed and still slow as hell...but maybe usable for an overnight code review job. Started to think maybe this hardware just won't cut it.\n\n**10 tok/s** \\- Ran across a note in a subreddit that someone got Vulkan offloading and doing 11 tok/s on similar hardware but when I tried it...I couldn't load the full model into VRAM despite having plenty of RAM. Interesting. I tried partial offload, 30 out of 49 layers to the iGPU. It worked. Now it actually felt usable but it didn't make sense that I had all this RAM and it wouldn't load all of the expert layers.\n\n**15 tok/s** \\- Then the dumb breakthrough. I discovered that `--no-mmap` was quietly destroying everything. On UMA architecture, where the CPU and GPU share the same physical RAM, that flag forces the model to be allocated twice into the same space. Once for the CPU, once for GPU-mapped memory, both pulling from the same DDR5 pool. I couldn't even load all 49 layers without OOM errors with that flag set. Dropped it. All 49 layers loaded cleanly. 46GB Vulkan buffer. No discrete GPU.\n\n**18 tok/s** \\- Still I wanted more. I enabled flash attention. An extra 3 tok/s, cut KV cache memory in half, and significantly boosted the context window.\n\n3 ‚Üí 5 ‚Üí 10 ‚Üí 15 ‚Üí 18. Each step was one discovery away from quitting. Glad I didn't.\n\n**Results (Flash Attention Enabled)**\n\n* Up to 18 tok/s text generation\n* 53.8 tok/s prompt processing\n* 50% less KV cache memory\n* Fully coherent output at any context length\n* All while Jellyfin was streaming to the living room for the kids\n\nCouldn't I just have bought a box purpose built for this? Yep. For reference, a Mac Mini M4 Pro with 64GB runs $2,299 and gets roughly 20-25 tok/s on the same model. Apple's soldered LPDDR5x gives it a real bandwidth advantage. But then it wouldn't run my media stack, store 70TB of data in RAIDZ2. I'm not trying to dunk on the Mac at all. Just saying I didn't have to buy one AND a NAS.\n\nWhich was the whole point.\n\nNo exotic kernel flags. No custom drivers. No ritual sacrifices. Vulkan just works on RDNA 3.5 under TrueNAS.\n\n**Still On the Table**\n\nI've barely scratched the surface on optimization, which is either exciting or dangerous depending on your relationship with optimizing. Speculative decoding could 2-3x effective speed. EXPO memory profiles might not even be enabled, meaning I could be leaving free bandwidth sitting at JEDEC defaults. Thread tuning, KV cache quantization, newer Vulkan backends with RDNA 3.5 optimizations landing regularly, UMA buffer experimentation, different quant formats.\n\nOn top of all that, the model wasn't even designed to run on standard transformer attention. It was built for DeltaNet, a linear attention mechanism that scales way better at long context. There's an active PR implementing it and we've been helping test and debug it. The fused kernel already hits 16 tok/s on a single CPU thread with perfect output, but there's a threading bug that breaks it at multiple cores. When that gets fixed and it can use all 12 cores plus Vulkan offloading, the headroom is significant. Especially for longer conversations where standard attention starts to choke.\n\n18 tok/s is where I am but I'm hopeful it's not where this tops out.\n\n**The Takeaway**\n\nI'm not saying everyone should overbuild their NAS for an LLM machine or that this was even a good idea. But if you're like me, enjoy tinkering and learning, and are already shopping for a NAS and you're curious about local LLMs, it might be worth considering specing a little higher if you can afford it and giving yourself the option. I didn't know if this would work when I bought the hardware, a lot of people said it wasn't worth the effort. I just didn't want to buy two systems if I didn't have to.\n\nTurns out I didn't have to. If you enjoyed the journey with me, leave a comment. If you think I'm an idiot, leave a comment. If you've already figured out what I'm doing wrong to get more tokens, definitely leave a comment.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r1jdq3/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4qdcjw",
          "author": "qwen_next_gguf_when",
          "text": "96GB DDR5 5600.",
          "score": 15,
          "created_utc": "2026-02-11 02:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rwka4",
              "author": "Ill_Shelter4127",
              "text": "he has ram! get him!¬†",
              "score": 13,
              "created_utc": "2026-02-11 10:04:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t8yd9",
                  "author": "BetaOp9",
                  "text": "Grab the pitch forks!",
                  "score": 5,
                  "created_utc": "2026-02-11 15:22:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r4kwt",
          "author": "Firestorm1820",
          "text": "Could you post the exact flags you‚Äôre using with llama.cpp? Curious to see what I can get on my setup.",
          "score": 10,
          "created_utc": "2026-02-11 05:48:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s7nrm",
              "author": "BetaOp9",
              "text": "Yeah, I'll try to remember in the morning to look to see what I stopped at. What hardware are you running? Welcome to message me.",
              "score": 3,
              "created_utc": "2026-02-11 11:41:39",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4trdon",
              "author": "BetaOp9",
              "text": "vanilla llama.cpp, Vulkan backend\n\nllama-server \\\\\n\n\\-m /models/Qwen3-Coder-Next-Q4\\_K\\_M-00001-of-00004.gguf \\\\\n\n\\-t 12 \\\\\n\n\\-c 4096 \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\\\\n\n\\--port 8080 \\\\\n\n\\-ngl 99 \\\\\n\n\\-fa on",
              "score": 3,
              "created_utc": "2026-02-11 16:48:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4usi3i",
                  "author": "Firestorm1820",
                  "text": "Excellent, thank you! My main rig is a RTX 5000 Ada 32GB + 256GB RAM, I‚Äôve also got a variety of mini PCs with Intel iGPU and 4GB NVIDIA GPUs. Smaller models are becoming more and more intelligent particularly when it comes to tool calling, so offloading inference tasks from my main PC to the mini PCs where possible would be awesome. Your post was exactly what I was looking for to start down that path, ty again.",
                  "score": 3,
                  "created_utc": "2026-02-11 19:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qzx2h",
          "author": "leonbollerup",
          "text": "Try running gpt-oss-20b, set up a server.dev mcp search.. you will find that it‚Äôs properly 2x as fast and with search it becomes quite smart",
          "score": 8,
          "created_utc": "2026-02-11 05:11:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r25f9",
              "author": "Icy_Distribution_361",
              "text": "Or possibly even the 120b with this amount of memory. From what I've read the token generation isn't much slower compared to 20b.",
              "score": 2,
              "created_utc": "2026-02-11 05:29:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4t203a",
              "author": "BetaOp9",
              "text": "I'll have to check this out. I may ping you when I do if that's okay.",
              "score": 1,
              "created_utc": "2026-02-11 14:47:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qmaa3",
          "author": "HealthyCommunicat",
          "text": "Its cool to see that the HX chips are decently affordable and are being used everywhere. We‚Äôre boutta all be able to inference on our phones in a few years.",
          "score": 5,
          "created_utc": "2026-02-11 03:36:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rdh1y",
              "author": "cramyzarc",
              "text": "Try MNN Chat, works already quite nicely with small models",
              "score": 2,
              "created_utc": "2026-02-11 07:05:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4rvz9c",
                  "author": "HealthyCommunicat",
                  "text": "i had a ai max 395+ and had to return it, too slow for me, i got the dgx spark and returned that too.",
                  "score": 1,
                  "created_utc": "2026-02-11 09:59:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4s8f00",
              "author": "strosz",
              "text": "With ChatterUI on Android I can already use GPT-OSS-20B with about 5-7 tok/s on 16GB ram phone. Probably good to have if stranded somewhere. Got to try other apps as well",
              "score": 1,
              "created_utc": "2026-02-11 11:47:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4sckps",
              "author": "No_Clock2390",
              "text": "Already can on S25 Ultra",
              "score": 1,
              "created_utc": "2026-02-11 12:18:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4taej7",
          "author": "singh_taranjeet",
          "text": "The --no-mmap point is the real PSA here. UMA + Vulkan is exactly where people shoot themselves in the foot by accidentally doubling allocations.\n\nAlso, 18 tok/s on an 80B MoE while the box is doing NAS and Jellyfin duty is a legit ‚Äúone box to rule them all‚Äù flex. Post your final llama.cpp flags when you can, everyone‚Äôs going to try to replicate this",
          "score": 3,
          "created_utc": "2026-02-11 15:29:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tvewl",
              "author": "BetaOp9",
              "text": "llama.cpp, Vulkan backend\n\nllama-server \\\\\n\n\\-m Qwen3-Coder-Next-Q4\\_K\\_M-00001-of-00004.gguf \\\\\n\n\\-t 12 \\\\\n\n\\-c 4096 \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\\\\n\n\\--port 8080 \\\\\n\n\\-ngl 99 \\\\\n\n\\-fa on",
              "score": 2,
              "created_utc": "2026-02-11 17:07:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tjap0",
          "author": "Kuusj",
          "text": "I have the normal N5 NAS with 32GB‚Äôs, do you still think it can run some models? Not specifically the RAM but the chip overall. I‚Äôm sad I chose 32GB half a year back because why ‚Äòwould I ever need 96GB‚Äô lol.. \n\nAlso, do you not have problems with the hardware fault in the SATA controller?",
          "score": 2,
          "created_utc": "2026-02-11 16:11:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59knfj",
          "author": "agentic_lawyer",
          "text": "I‚Äôve got trueNAS and ollama-ipex running off an intel ARC iGPU and it is very respectable. To top it off, i had to get GPU passthrough from Proxmox to the trueNAS VM. That part was actually the hardest element of the configuration.\n\nOnly allocated 32GB of my 96 GB Beelink to the VM so I‚Äôve crashed the IPEX container a few times but it never seems to affect any of the other applications running on the NAS. And I‚Äôve got about a dozen other apps running including the ARR stack and Emby. I can run 21B models without a problem if speed is not a requirement. The biggest challenge seems to be the first response but once the model is loaded up and spinning then subsequent token output is much faster. \n\nI‚Äôve tried using this set up with 7B models to run Chatwoot customer enquiry bots. Speed is not a problem with the smaller models, but the models are too dumb to stick to a script and handle customer inquiries. \n\nNow that you‚Äôve posted this, I‚Äôm going back to have another look at my setup to see if I can tune more output out of the LLM.",
          "score": 2,
          "created_utc": "2026-02-14 01:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q435h",
          "author": "timbo2m",
          "text": "Good stuff! I did something similar yesterday. I have different hardware but maybe this will get a few extra tokens per second:\n\n\n--threads 16 --batch-size 512\n\n\nThat gave me 2 tokens per second extra. Small, but it means a lot when it's sub 20 tokens.  Of course, fix threads to 1 or 2 less cores than you have available for llama server. In my case flash attention didn't help so I left it off.\n\nAlso, I wonder if the 2 bit XL quant is accurate enough, it will probably give you 10 tokens per second extra over the 4. Play with your context size too, I find 64k is good middle ground.",
          "score": 1,
          "created_utc": "2026-02-11 01:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs7ou",
              "author": "Shoddy_Bed3240",
              "text": "Try running it with 8 threads and pin the process to the performance cores only using taskset.\n\n\n\nOn Intel CPUs, this makes a huge difference when you restrict the workload to P-cores instead of mixing P-cores and E-cores. I suspect it should work similarly on AMD systems with Zen 5 performance cores as well.\n\n\n\nIt‚Äôs definitely worth testing and comparing the results.",
              "score": 2,
              "created_utc": "2026-02-11 04:16:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4qvbu7",
                  "author": "timbo2m",
                  "text": "I'm actually running 2 combos with the same qwen coder next models - an old amd server with a couple of nvidia T4 Tesla cards, comparing with my own gaming machine which is intel i9/4090/32GB, my gaming machine is actually 10 tokens per second faster but it's nice to offload agents to the server. Ideally I can get some Openclaw thing happening when  I get some firewalls in place.",
                  "score": 1,
                  "created_utc": "2026-02-11 04:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4r4303",
              "author": "colin_colout",
              "text": " 768 batch size was the sweet spot on my 780m when i was maining it. \n\nit matches the number of shader cores and improves prompt processing significantly. \n\ni think 16 threads shouldn't be needed since it uses vulkan amd no cpu offload if you set GTT correctly in Linux",
              "score": 1,
              "created_utc": "2026-02-11 05:44:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qqttc",
          "author": "Shoddy_Bed3240",
          "text": "If you want to maximize performance, memory bandwidth is critical. Improving effective memory bandwidth can have a significant impact on overall speed, especially for large models.\n\nThe best tool I‚Äôve found for measuring memory bandwidth is STREAM:  \n[https://github.com/jeffhammond/STREAM](https://github.com/jeffhammond/STREAM)\n\nYou can compile it with:\n\n    gcc -O3 -fopenmp -mcmodel=large -DSTREAM_ARRAY_SIZE=200000000 stream.c -o stream\n    \n\nThen run it to determine your maximum memory bandwidth:\n\n    export OMP_NUM_THREADS=<max_threads>\n    taskset -c <list_of_cores> ./stream\n    \n\nOn my system, I was able to reach around 96‚Äì97 MB/s, which allowed me to run a Q8 model at about 10 tokens per second.\n\nIf you're trying to optimize inference speed, it‚Äôs definitely worth benchmarking your memory bandwidth first.",
          "score": 1,
          "created_utc": "2026-02-11 04:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r20gd",
              "author": "Icy_Distribution_361",
              "text": "And then you find your memory bandwidth, and then what?",
              "score": 2,
              "created_utc": "2026-02-11 05:28:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r2f5v",
                  "author": "Shoddy_Bed3240",
                  "text": "Next, find a way to maximize your memory bandwidth. LLM is sensitive to memory bandwidth‚Äîthe more you have, the faster it can generate tokens.",
                  "score": 1,
                  "created_utc": "2026-02-11 05:31:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r3he1",
          "author": "EbbNorth7735",
          "text": "It's a 3B model. Speculative decoding likely wont get you much.",
          "score": 1,
          "created_utc": "2026-02-11 05:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rqszf",
              "author": "BetaOp9",
              "text": "You're probably right that speculative decoding gains won't be as dramatic as they would on a dense 70B. The active parameter count does limit how much a draft model can accelerate things. I'll be honest, I listed it as an optimization I haven't tried yet, not a guaranteed win. I haven't even looked into if anyone's actually benchmarked spec decode on MoE models and has real numbers.\n\nBut...it is 80B total parameters with 3B active per token. The routing layer is still selecting from the full 80B every forward pass. It's not the same workload as a dense 3B model. The memory bandwidth requirements alone make that obvious since a dense 3B doesn't need 46GB of VRAM. Each step has been about trying it and debugging and seeing what we end up with. ü§∑‚Äç‚ôÇÔ∏è",
              "score": 1,
              "created_utc": "2026-02-11 09:10:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ry0n7",
                  "author": "EbbNorth7735",
                  "text": "Yeah, I guess if you dump a 1B in vram you may see a few tps",
                  "score": 0,
                  "created_utc": "2026-02-11 10:17:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u3z6a",
          "author": "beedunc",
          "text": "This is the future, especially with panther lake coming. Congrats!",
          "score": 1,
          "created_utc": "2026-02-11 17:48:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wltry",
          "author": "__rtfm__",
          "text": "Congrats! That was my dream and I ran into hardware issues with my n5 pro and returned it. Glad it‚Äôs working out for you",
          "score": 1,
          "created_utc": "2026-02-12 01:23:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r6z5d",
          "author": "RiskyBizz216",
          "text": "Absolute beast of a setup, but I agree - a Mac Studio would have been a better value - higher throughput and you would have gotten access to MLX models and 0-day releases.",
          "score": 1,
          "created_utc": "2026-02-11 06:08:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rswjk",
              "author": "BetaOp9",
              "text": "Six months ago if someone said \"just buy a Mac for local inference\" I would have agreed with them. Unified memory, great bandwidth, MLX ecosystem. Nothing else was close.\n\nBut that's not really true anymore. AMD's Strix Halo chips are going toe to toe with Apple Silicon on inference performance. Competitive bandwidth, 128GB of unified memory, and the software stack is catching up fast. My chip is the budget little brother of that architecture and it's still pushing 18 tok/s on an 80B MoE while running my other stuff. I would have spent more, gotten a bit more performance and ended up with a lot less function baked in had I gone with the Mac.",
              "score": 1,
              "created_utc": "2026-02-11 09:30:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rvjhn",
                  "author": "RiskyBizz216",
                  "text": "Since you're going \"NAS first\" I guess that makes sense\n\nBut one correction - if you wanted better LLM performance the mac would hands down be the better solution. That's based on recent benchmarks and test cases.\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1r082v1/qwen3codernext\\_performance\\_on\\_mlx\\_vs\\_llamacpp/](https://www.reddit.com/r/LocalLLaMA/comments/1r082v1/qwen3codernext_performance_on_mlx_vs_llamacpp/)\n\nhttps://preview.redd.it/vd6nbulg7uig1.png?width=1080&format=png&auto=webp&s=bba9bf928b8b1103fb5ef8ea677b5d3285afaeec\n\n  \nAlso there is limited AMD/ROCm support in the LLM community - users are seeing poor performance across the board with this model in particular.\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1qwg58c/qwen3\\_coder\\_next\\_poor\\_performance\\_on\\_r9700s/](https://www.reddit.com/r/LocalLLaMA/comments/1qwg58c/qwen3_coder_next_poor_performance_on_r9700s/)\n\nIf you're just \"tinkering and learning\" and shopping for a NAS, then yeah - a Mac is overkill. Go with Dr. Frankenstein :) ",
                  "score": 2,
                  "created_utc": "2026-02-11 09:55:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qfs5y",
          "author": "UpbeatDraw2098",
          "text": "your a smart dude",
          "score": 1,
          "created_utc": "2026-02-11 02:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q50pi",
          "author": "greatwilt",
          "text": "Nice! This model works well with open claw too, if you can get the context size up above  200 K. Good luck in your optimization journey.",
          "score": 0,
          "created_utc": "2026-02-11 01:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qsixu",
              "author": "timbo2m",
              "text": "I was going to use Kimi K2.5 as the brain and outsource coding and heartbeats to my local qwen 3 coder next 80B. Are you suggesting the brain could be just the local LLM too?!",
              "score": 1,
              "created_utc": "2026-02-11 04:18:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r26mw9",
      "title": "Getting ready to send this monster to the colocation for production.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1r26mw9",
      "author": "Ok_Stranger_8626",
      "created_utc": "2026-02-11 19:14:58",
      "score": 61,
      "num_comments": 16,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r26mw9/getting_ready_to_send_this_monster_to_the/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4us5jd",
          "author": "MinimalDistance",
          "text": "Awesome! Thanks for sharing details of the stack. I wonder about estimate costs of building and running a box like this - can you provide some numbers? Thanks!",
          "score": 8,
          "created_utc": "2026-02-11 19:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v179v",
              "author": "Ok_Stranger_8626",
              "text": "All told, it's about $25K worth of gear at today's prices(largely due to the RAM and disk). We assembled it from some spare parts we had, and the rest was purchased from The Server Store. All in all, we figure $12K-ish when we first assembled it.",
              "score": 4,
              "created_utc": "2026-02-11 20:24:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uz652",
          "author": "Anarchaotic",
          "text": "You've built this for a client I assume? Time to rake in that sweet sweet \"ongoing support\" $ to make sure everything keeps working properly and you can update containers/libraries as needed. \n\nHow easily could you redeploy something like this if you had to? From a business perspective it could make sense to have the projects/scripts all relatively automated so you can turn-key another one of these.",
          "score": 5,
          "created_utc": "2026-02-11 20:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v2rmm",
              "author": "Ok_Stranger_8626",
              "text": "We built it initially to do a proof-of-concept for the \"Experts\". It was purely a technical exercise to begin with. \n\nBut after the last few weeks, we decided to make it a multi-tenant system(we have it tied into our SSO system and realms so clients can log into it and have their employees grouped so data doesn't cross-domains.\n\nBut we have all the setup scripts, python code and injectable prompts safely stored away in our GitLab host so we could easily replicate the setup at any time.\n\nThe big ticket is the all the Federal Statutes we ingested and the related instructions(CFRs) from the federal registry. It took us a couple weeks to get them all in properly, and we update them every weekend now, so it's a pretty valuable asset that we can sell/subscribe to law firms, CPAs or compliance consultants.\n\nWe're also working with a Medical compliance officer to develop a Medical Compliance Expert(HIPAA/GDPR/State Level) at the moment, we hope to have that one done in a month or so.",
              "score": 6,
              "created_utc": "2026-02-11 20:31:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v20oq",
          "author": "Hydroskeletal",
          "text": "for something like Federal Law expert are you basically doing a lot of prompt engineering by sticking the US Code into the vector DB? super curious if you can talk about the high level concepts you are using",
          "score": 4,
          "created_utc": "2026-02-11 20:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v3upn",
              "author": "Ok_Stranger_8626",
              "text": "So, we use some pretty tight prompts, yeah. We basically force the model to ignore it's own \"helpful assistant\" role, and behave more like a professional expert. That helped us get around the hallucination problem by a lot. The other thing we do is only allow each expert to research in their specific \"Title\" of the federal code, which really helps, as it limits their expertise to just that ONE particular statue/instruction \"manual\". For example, the \"Tax Expert\" only has access to Title 26 and the IRS CFR.",
              "score": 6,
              "created_utc": "2026-02-11 20:37:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4x7uiw",
                  "author": "Hefty_Development813",
                  "text": "How do you train one specific expert with an MOE?",
                  "score": 1,
                  "created_utc": "2026-02-12 03:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v4so2",
          "author": "dave-tay",
          "text": "Beautiful. Price?",
          "score": 3,
          "created_utc": "2026-02-11 20:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v57iy",
              "author": "Ok_Stranger_8626",
              "text": "We estimate now that prices have gone up, somewhere between $20K and $25K US. Back when we did it last summer, probably closer to $12k to $16K US.\n\nEDIT: That would be our cost for just the hardware. It would definitely not get sold for that, considering the customized containers, python code, specialty prompts and the RAG database. We've put hundreds of hours into building the software stack, so I'd say, if we were to retail it, we'd probably price the box around $65K US for a functional \"turn-key\" device.",
              "score": 5,
              "created_utc": "2026-02-11 20:43:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4we6s7",
                  "author": "Much-Researcher6135",
                  "text": "Thanks, I was wondering what the market was like for all this stuff. Are customers needy or pretty hands-off? Or do you guys not even do service contracts?\n\nNeat post!",
                  "score": 3,
                  "created_utc": "2026-02-12 00:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xsxy9",
          "author": "ridablellama",
          "text": "very cool post. i love to see real business applications of local",
          "score": 2,
          "created_utc": "2026-02-12 06:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wqm22",
          "author": "chrisbliss13",
          "text": "How much you paying for colo",
          "score": 1,
          "created_utc": "2026-02-12 01:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wtfn0",
              "author": "Ok_Stranger_8626",
              "text": "Right now, it's about $325/mo for the 6U and low power for my other gear. \n\nThis one, when it goes down there will add about $550/mo because it needs way more energy.",
              "score": 2,
              "created_utc": "2026-02-12 02:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zeqpj",
          "author": "Lonely_Love4287",
          "text": "so cool which i had the funds to do  this",
          "score": 1,
          "created_utc": "2026-02-12 14:11:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0x3kn",
      "title": "What would a good local LLM setup cost in 2026?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r0x3kn/what_would_a_good_local_llm_setup_cost_in_2026/",
      "author": "Lenz993",
      "created_utc": "2026-02-10 10:20:12",
      "score": 55,
      "num_comments": 91,
      "upvote_ratio": 0.94,
      "text": "If you had a budget of around $5,000 and wanted to set up a local LLM, what hardware would you go for? Would you go for a good PC with a powerful graphics card like an RTX 5090 or a Mac like the M3 Ultra? And why?\n\nI would like to connect and use my own local LMM with OpenClaw in the course of the year (hopefully more powerful local models will come onto the market in 2026).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r0x3kn/what_would_a_good_local_llm_setup_cost_in_2026/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4mil5k",
          "author": "PraxisOG",
          "text": "Honestly 4x 3090 is a solid way to go as a balance of memory capacity and speed, and run something like Qwen coder next super fast. You could also get more vram with an obscure option like 7x AMD V620 for full gpu offload using GLM4.7 or a lot of context with minimax 2.1 and soon 2.2. Though if you don‚Äôt want a super loud mining rig type thing get a strix halo box for 2k and call it a day, same vram capacity as 4x3090 but slower and whisper quiet in comparison.¬†",
          "score": 13,
          "created_utc": "2026-02-10 15:06:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndwvb",
              "author": "Endhaltestelle",
              "text": "Where can you buy 4 x 3090 for $5000 ?",
              "score": 3,
              "created_utc": "2026-02-10 17:33:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ni0oh",
                  "author": "Doug_Fripon",
                  "text": "How much is a 3090 nowadays? A few weeks ago in Europe you could easily get one for 400‚Ç¨. Now 600.",
                  "score": 5,
                  "created_utc": "2026-02-10 17:52:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nf8gt",
                  "author": "AnonymousCrayonEater",
                  "text": "Ebay?",
                  "score": 2,
                  "created_utc": "2026-02-10 17:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4qtw1d",
              "author": "WonderfulEagle7096",
              "text": "Depending on where you live, 4x 3090 could end up being very costly operationally. Assuming 8 hours per day operation (average work day), 4 √ó 350 W = 1.40 kW or roughly 5000 kWh annually.\n\nWhere I live this will translate to \\~$1800/year in electricity alone, then you'll have another chunk in depreciation. An M3/M4 chip (Macbook Pro/Mac Studio) only consumes <100W and depreciates a lot more gracefully.",
              "score": 2,
              "created_utc": "2026-02-11 04:27:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ozxjv",
              "author": "Citizen_Edz",
              "text": "I don‚Äôt think 4x 3090s is a very realistic setup for 5k? That‚Äôs atleast 3k in gpus. Then you probably need a thredripper, or other workstation/server ship to support that many pcie devices.",
              "score": 3,
              "created_utc": "2026-02-10 22:01:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pl382",
                  "author": "PraxisOG",
                  "text": "You can throw it in a 1k X299 platform and mining frame and save a grand if you want¬†",
                  "score": 3,
                  "created_utc": "2026-02-10 23:54:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wchw6",
              "author": "Small_Bee_4655",
              "text": "Is it 24g version of 3090?",
              "score": 1,
              "created_utc": "2026-02-12 00:27:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xzopm",
                  "author": "PraxisOG",
                  "text": "All models of the 3090 are 24g, some are modded for 48g though but are more expensive¬†",
                  "score": 1,
                  "created_utc": "2026-02-12 07:18:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4msiyb",
          "author": "SimplyRemainUnseen",
          "text": "I'd probably cluster 2 128gb ryzen ai max+ systems. 256gb of unified memory is crazy good. Really good 4bit performance on those systems for LLMs and image generation.\n\nYou can finetune a QAT LoRA with unsloth's workflows to shrink the performance gap for int4 quants. For reference you can get usable speeds on models like GLM 4.7 with int4 quantization.\n\nA friend of mine runs a ComfyUI API on one system and GPT OSS 120B on the other as a house AI system that can do image & video gen. \n\nThat amount of unified memory really opens up a lot of possibilities.",
          "score": 18,
          "created_utc": "2026-02-10 15:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n2dak",
              "author": "Zyj",
              "text": "If you add RDMA capable network cards\nyou speed things up with tensor parallelism",
              "score": 5,
              "created_utc": "2026-02-10 16:40:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o0ken",
                  "author": "jstormes",
                  "text": "Do you know of any posts about this?  I am looking into it for my setup?",
                  "score": 1,
                  "created_utc": "2026-02-10 19:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4q0fs7",
              "author": "flyingbanana1234",
              "text": "i would totally get 2 ryzens if they just had higher bandwidth as i want to mess around with video generation, i might just wait for the m5 256 gb mac studio but im so impatient",
              "score": 3,
              "created_utc": "2026-02-11 01:22:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n21rh",
          "author": "Zyj",
          "text": "2x Strix Halo (2x $2000) + 2 used infiniband network cards (2x $200) + 2x Oculink and a small PSU. Probably around $4500-4600 in\ntotal. Gives you the ability to run 470b models at q4 etc. **with tensor parallelism**\nSpeed won‚Äôt be great but good enough probably.",
          "score": 10,
          "created_utc": "2026-02-10 16:38:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q0nzy",
              "author": "flyingbanana1234",
              "text": "i could be wrong but waiting for the 256 gb m5 mac studio for 5500 seems to be a better move\nhigher bandwidth ü´†ü´†ü´†",
              "score": 2,
              "created_utc": "2026-02-11 01:24:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4szrwq",
                  "author": "Zyj",
                  "text": "Sure, that sounds pretty good too. If the cost doesn‚Äòt go up.",
                  "score": 2,
                  "created_utc": "2026-02-11 14:35:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51suep",
                  "author": "Hector_Rvkp",
                  "text": "Do we know the bandwidth on the next M5 chip? If it's not much higher than 1000gb/s I don't think that ram above 128gb is going to be very usable.",
                  "score": 1,
                  "created_utc": "2026-02-12 21:03:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o54t6da",
                  "author": "parmdhoot",
                  "text": "The 256 version with 4tb of storage is $6599 you are going to want the faster onboard storage it is nicer when switching models.... also all new models are getting much larger now. .... they no longer even fit on the 512 version.  Not worth it in my opinion. Only worth it from a learning perspective. With the machine being on all the time running mid open source models and saturated most the time the payoff period is close to 8 years. That does not include any other costs. Apple silicon is great but there is a reason that NVIDA owns this space. IMO",
                  "score": 1,
                  "created_utc": "2026-02-13 08:47:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4opwh4",
              "author": "Lenz993",
              "text": "That's a great idea too! Thank you very much.",
              "score": 1,
              "created_utc": "2026-02-10 21:14:52",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4youd8",
              "author": "alfons_fhl",
              "text": "And why do you prefer \"AMD Strix Halo\" and not from Nvidia? Like \"Asus GX10\"?",
              "score": 1,
              "created_utc": "2026-02-12 11:19:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5110qz",
                  "author": "Zyj",
                  "text": "Asus GX10 is $3000, AMD Strix Halo is $2000. That's the main reason. ",
                  "score": 1,
                  "created_utc": "2026-02-12 18:50:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4njtm0",
          "author": "Own_Atmosphere9534",
          "text": "Macbook M4 PRO MAX 128 gb comes in around $5K.  Best models for this config:\n\n**Llama 3.3 70B Instruct (MLX / GGUF 4-bit/8-bit**  \nstrong reasoning, writing, and instruction following. Supposedly GPT 4 Class inference on local machine\n\n**Qwen3** coder variants (\\~80B MLX)  \n**Qwen3-235B-A22B (MLX 3-bit or 4-bit)**  \nGood coding\n\nand others I‚Äôm sure. \n\nRTX 5090 32 GB VRAM  come in new around this price point ($5000).\n\n As others have said here you could stack multiple RTX 3090s and get a pretty good system with large VRAM for \\~5K probably more.\n\ni‚Äôve heard that the new Mac chips perform in 3090 class RTX range (irrespective of  ram size).  Maybe someone can confirm or give a better indication of  performance is \n\nThe real question is how large of model  do you want to  run and how comfortable are you with screwing around with hardware. \n\nUntil recently, I was looking at Nvidia based solutions but did some testing on my 24 gig M4  MacBook,  and found that things like GPT-OSS-20B run extremely well for my usage. So I went ahead and bought a M4  PRO MAX 128 gig.\n\n\n\n\n\n\n\n",
          "score": 7,
          "created_utc": "2026-02-10 18:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qgy12",
              "author": "Boring-Attorney1992",
              "text": "what kind of tokens/s are we ranging here with your Mac 128GB?",
              "score": 3,
              "created_utc": "2026-02-11 03:02:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r5b1d",
                  "author": "Own_Atmosphere9534",
                  "text": "Just ordered it. The MAX MacBooks are on backorder. It‚Äôs going to be mid March. There is pretty strong likelyhook that the M5 MAX MacBooks are going to be announced early March. Something to consider if you are looking in this direction. I‚Äôm considering canceling but in all likely hood the M5 MAX will be shipping a month later once they are announced so that would be at least April..",
                  "score": 2,
                  "created_utc": "2026-02-11 05:54:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4q9mgv",
          "author": "BetaOp9",
          "text": "People don't always realize clustering is good for parallel tasks, not single-stream inference. Splitting one model across multiple nodes means every layer boundary ships activations over the network. On 10GbE that's about 1.25 GB/s. Your nodes spend more time waiting on each other than doing math. You end up slower than a single fat node. \n\nThat said someone in this thread mentioned InfiniBand between two Strix Halo boxes and that actually offloads this bottleneck significantly. Used ConnectX-4/5 EDR cards run about $150-200 each and give you 100 Gbps, roughly 11 GB/s actual throughput with RDMA. That's 9x faster than 10GbE and makes tensor parallelism viable. 256GB unified memory across two nodes, 470B models at Q4, about $4,500 total. Interesting build. \n\nBut for a single $5K box? Mac Studio M4 Max. 128GB. $3,499.\n\n400 GB/s memory bandwidth, 128GB unified so no VRAM ceiling, silent, low power. Q3CN (80B MoE) would do roughly 40-60 tok/s. Most open models through 2026 will fit in 128GB quantized.\n\nThe RTX 5090 has insane bandwidth (about 1,800 GB/s) but only 32GB VRAM. Anything that fits in 32GB will scream. Anything that doesn't, and that's most of the interesting models, won't run at all without spilling to system RAM and tanking performance.\n\nThe M3 Ultra is last gen. Wait for the M4 Ultra if you want 256GB+ unified but that'll blow past $5K.\n\nFor context I'm running Q3CN on a Minisforum N5 Pro (Ryzen AI 9 HX PRO 370, 96GB DDR5, 16 RDNA 3.5 CUs) and hitting 18 tok/s on the iGPU via Vulkan at about 95 GB/s bandwidth. No discrete GPU. Token gen is bandwidth-bound so it scales roughly linear. A Strix Halo box at 212 GB/s would be around 40 tok/s. The M4 Max at 400 GB/s would push 75+.\n\nSave the $1,500 left over for when the M4 Ultra drops. Sell the Max and upgrade to 256GB at 800 GB/s. That's the real next step for local inference in a tight little box.",
          "score": 6,
          "created_utc": "2026-02-11 02:18:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mb83w",
          "author": "I_like_fragrances",
          "text": "At the moment the rtx 5090 is around 4-4.5k and a decent amount of ram bumps it up higher. For around $7k-$8k id do that. With $4-$5k id do a mac studio or dgx spark.",
          "score": 4,
          "created_utc": "2026-02-10 14:28:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mc2ox",
              "author": "eribob",
              "text": "I would do 4xRTX 3090 in an AM4 system",
              "score": 7,
              "created_utc": "2026-02-10 14:32:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4pen8q",
              "author": "Kaokien",
              "text": "I got a pre-built from cyber power for 4.2k so definitely doable, within the 5k range 64gb ram",
              "score": 1,
              "created_utc": "2026-02-10 23:17:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nyhya",
          "author": "Hector_Rvkp",
          "text": "A very clean option would be a mac studio with 128gb ram, 1 or 2tb SSD, and you max out the chip / bandwidth to fit your budget. I don't think 256gb ram makes sense on apple because the bandwidth is 1000gb/s max, which makes models 128+gb too slow to be useful. (Bandwidth/model size). But a good MoE model fitting inside 128gb would be very usable I believe.",
          "score": 5,
          "created_utc": "2026-02-10 19:07:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ojcmq",
              "author": "Lenz993",
              "text": "So you mean the Apple M4 Max (16-core CPU), 128 GB RAM, 1024 GB SSD?",
              "score": 2,
              "created_utc": "2026-02-10 20:44:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4p8f7j",
                  "author": "mikestanley",
                  "text": "That‚Äôs what I bought last week. Setup changedetection.io and pointed it at the Apple Certified Refurb store. Bought that config 19 hours later. With the additional Veteran discount I paid $2825 before tax.",
                  "score": 4,
                  "created_utc": "2026-02-10 22:44:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51s27s",
                  "author": "Hector_Rvkp",
                  "text": "What I meant is what I wrote: at a given budget, pick the generation/model that has 128gb ram, save your money by only getting 1tb (then add external storage for cheap) and spend your money on the fastest chip and fastest bandwidth. Prices vary wildly depending on jurisdiction, second hand / refurb availability and so on. Where I live, a M1 ultra is still expensive AF, while Americans have reported getting them for Uber cheap.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o54tw0n",
              "author": "parmdhoot",
              "text": "100% agree, people see the performance of the 128gb ram macbooks and assume the m3 ultra with 512gb will do great but prefill times are super slow with larger models...  between 10 and 45 seconds for 10k to 40k input tokens and, and tokens per second on larger models are about 20 tokens per second down to about 10 tokens per second as context fills up. The 128gb macs however are a great value as long as the context windows are small.",
              "score": 1,
              "created_utc": "2026-02-13 08:53:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55rves",
                  "author": "Hector_Rvkp",
                  "text": "Yep. I wonder the bandwidth on the next M5 chip, because quantized frontier model do not fit on 128 ram, so you need 256 or 512. But the active agents on these models also are quite chonky, so if you spend quite a few thousands on hardware, you can't accept 10 tokens/s, performance has to make sense. That said, there's basically only 1 Nvidia retail GPU that has bandwidth above 1000gb/s, so 1000 is fast. Just not fast enough for huge ass models. That and apple software isn't nearly as optimized as Nvidia chips.",
                  "score": 1,
                  "created_utc": "2026-02-13 13:25:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4o15np",
          "author": "NoobMLDude",
          "text": "Start FREE FIRST.\n\nIf you have not tried it yet, get a taste of Local LLM on whichever device you have.\n\nOnly if you notice issues in the model you run should you put money for hardware. For most users they won‚Äôt notice a huge difference. \n\nFor OpenClaw you might need a big model.\nHowever there are some Local LLM pipelines you could try first which have some assistant features like OpenClaw:\n- [Personal Meeting Assistant](https://youtu.be/cveV7I7ewTA)\n- [Personal Talking Assistant](https://youtu.be/2VHzYy45kPw)\n- Tools for coders- Code Assistant, Terminal, Local API, etc\n\nTry out some of the above setups first as a way to check if you could utilize the expensive hardware.",
          "score": 5,
          "created_utc": "2026-02-10 19:19:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o6ckj",
          "author": "Lenz993",
          "text": "Most AI systems recommended the Mac M4 Max with 128 GB because it fits perfectly into the budget and can run large LLMs with 128 GB. That sounds logical to me. In addition, Apple's RAM is supposed to be particularly fast.",
          "score": 5,
          "created_utc": "2026-02-10 19:43:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4mkoka",
          "author": "mpw-linux",
          "text": "# what about a NVIDIA DGX Spark, 3,999.00 ? There was a review of it I saw on YT it looking pretty impressive, runs a version of LInux",
          "score": 8,
          "created_utc": "2026-02-10 15:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mtwsb",
              "author": "InfraScaler",
              "text": "You can get AMD AI MAX+ 395 with 128GB of unified RAM for half that... But also, someone else mentioned 4x3090 and that's probably the way to go to maximise your 5k",
              "score": 8,
              "created_utc": "2026-02-10 16:01:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4njthm",
                  "author": "Look_0ver_There",
                  "text": "Adding to your comment, there's some benchmarks here: [https://github.com/lhl/strix-halo-testing/](https://github.com/lhl/strix-halo-testing/)  \nThis guy compares the DGX Spark to the Strix Halo.  For Prompt Processing the Spark runs at about twice the speed of the Strix Halo, but for token generation the Spark falls back to just \\~10% faster.  This closely reflects both the compute speed differences (ARM vs AMD64) and the memory speed differences between the two machine.   At $3999 vs $2000, you'd want to mostly be doing prompt processing full-time to justify paying twice as much.",
                  "score": 3,
                  "created_utc": "2026-02-10 18:00:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4n8adp",
                  "author": "Expert_Mulberry9719",
                  "text": "be aware that the AMD ones can only use 50% of the memory for the GPU, Apple ones can use 75% of the memory for the GPU.",
                  "score": -9,
                  "created_utc": "2026-02-10 17:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nflsy",
              "author": "SpecialistNumerous17",
              "text": "Asus Ascent GX10 is a clone of the DGX Spark that is $1000 cheaper. I bought this and am very happy with it. The internals are mostly identical to the Spark, except for smaller disk, and so I just attached an external 2TB disk that I had already and it's been great so far.",
              "score": 4,
              "created_utc": "2026-02-10 17:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q0y75",
                  "author": "flyingbanana1234",
                  "text": "have you tried any video generation ?",
                  "score": 1,
                  "created_utc": "2026-02-11 01:26:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ypjmv",
                  "author": "alfons_fhl",
                  "text": "And why do you bought from Nvidia and not from AMD like \"Halo Strix\"?",
                  "score": 1,
                  "created_utc": "2026-02-12 11:25:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n5rtg",
              "author": "AppointmentAway3164",
              "text": "Yep I‚Äôm shopping these. Really great solution from nvidia.",
              "score": 2,
              "created_utc": "2026-02-10 16:55:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4no0e0",
              "author": "Financial-Source7453",
              "text": "$3k only for Asus Gx10. 1TB drive instead of 4TB. Copper radiator and plastic body instead of vapor chamber and metal foam case, but totally worth it.",
              "score": 2,
              "created_utc": "2026-02-10 18:19:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4r27hc",
              "author": "J673hdudg",
              "text": "I love my DGX Spark!! I do training runs (e.g. nanochat) as well as inference. Power usage is 90W max. The GB10 GPU does about \\~55 tok/s (over 1000 tok/s batched) for Qwen3-VL-30B and is my workhorse. Same Spark is running 3 other models and Flux for image gen. I still have a 2x3090 rig that can only do a fraction of that and uses 10x as much power.   \n  \nI also have a Strix Halo, but it came pre-installed with Windows that took a few hours to set up w/ a monitor (puke!) and haven't tried to re-image it with Linux yet. The Spark is Linux native and will setup headless out of the box. Amazing.",
              "score": 2,
              "created_utc": "2026-02-11 05:29:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ypp0v",
                  "author": "alfons_fhl",
                  "text": "And why from Nvidia and not from amd like halo Strix?",
                  "score": 1,
                  "created_utc": "2026-02-12 11:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n91h9",
          "author": "LSU_Tiger",
          "text": "Apple M4 Mac Studio Max with 128GB of ram.",
          "score": 4,
          "created_utc": "2026-02-10 17:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q138a",
              "author": "flyingbanana1234",
              "text": "this maybe the 256 gb option. I have an m4 128 gb ready for pickup but seriously debating waiting for the mac studio m5 for the higher bandwidth",
              "score": 2,
              "created_utc": "2026-02-11 01:26:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4svijy",
                  "author": "LSU_Tiger",
                  "text": "I was in the same boat but decided against waiting for the M5. I didn't want to wait an undetermined amount of time for an unknown better level of performance at an unknown price point, so I pulled the trigger and have been enjoying my M4 Max / 128GB for months now. It's a beast.",
                  "score": 2,
                  "created_utc": "2026-02-11 14:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4p9a01",
          "author": "mikestanley",
          "text": "This is what I bought last week. Paid $2825 from Apple as certified refurb with veteran discount.\n\nhttps://preview.redd.it/4pd30mfixqig1.jpeg?width=1219&format=pjpg&auto=webp&s=7e094fd32183651e064d0b82aa1062d570fd2091\n\nI‚Äôm still getting started but it can run my large models than my M1 Max 32GB and faster. And I wanted a new Mac anyway.",
          "score": 2,
          "created_utc": "2026-02-10 22:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q1kvb",
              "author": "flyingbanana1234",
              "text": "dang anytime i check it out they only have m1 and m2 in stock !!!",
              "score": 1,
              "created_utc": "2026-02-11 01:29:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q8qor",
                  "author": "mikestanley",
                  "text": "That's what happened to me for a week. The I setup [changedetection.io](http://changedetection.io) in a Docker container on my NAS and pointed it at the Apple Certified Refurb store. Bought that config 19 hours later. I left the query running for a few days - that config shows up regularly, it is just bought within 1-2 hours.",
                  "score": 2,
                  "created_utc": "2026-02-11 02:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4spv36",
              "author": "Remote-Reception-958",
              "text": "Thatsndo damn cheap, in south america is at least 1000 more¬†",
              "score": 1,
              "created_utc": "2026-02-11 13:41:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n5kk5",
          "author": "AppointmentAway3164",
          "text": "Nvidia DGX Spark.",
          "score": 2,
          "created_utc": "2026-02-10 16:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mspx8",
          "author": "MimosaTen",
          "text": "I would think about 5/6.000 ‚Ç¨",
          "score": 1,
          "created_utc": "2026-02-10 15:55:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n6k5u",
          "author": "esmurf",
          "text": "Gfx card alone 1-2000 usd. More if you have the money.",
          "score": 1,
          "created_utc": "2026-02-10 16:59:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4skl5b",
          "author": "Glad_Middle9240",
          "text": "DGX Spark.  128gb unified ram.  nvidia stack so you don‚Äôt have to mess with Mac/rocm.  Room to grow with 200gb infiniband networking built in.   Add a second when you want more capability.",
          "score": 1,
          "created_utc": "2026-02-11 13:11:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ypsgg",
              "author": "alfons_fhl",
              "text": "why from Nvidia an not from AMD?",
              "score": 1,
              "created_utc": "2026-02-12 11:27:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4z6611",
                  "author": "Glad_Middle9240",
                  "text": "To each their own but:  \n\\#1 CUDA > ROCm all day every day.  Support is just more mature  \n\\#2 Built in 200GB infiniband networking for clustering  \n\\#3 Complete ecosystem preinstalled + playbooks at [build.nvidia.com/spark](http://build.nvidia.com/spark)  \n\\#4 Prompt processing on the GB10 chip blows away Max 395+.  Becomes important when you get beyond simple chats.\n\nThe AMD product is great for the price, but it has some limitations.  Spark does as well -- but at least you are dealing with with NVIDIA hardware so compatibility is better.",
                  "score": 1,
                  "created_utc": "2026-02-12 13:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4te167",
          "author": "TheRiddler79",
          "text": "My decision would come down to what model I wanted to run.\n\nPersonally, if you shop it exactly correct, you could get between 5 and 8 of the 32 GB V100, on a server board that doesn't require pcie, just the board is where they sit.\n\nThe reason I would go that route is because for the money, there's probably not a way to get more vram, and the limitations of what you run are basically entirely dependent upon how much fits into the ram or vram.\n\nIf you shop exactly right and get a little lucky and find eight of those in a board, you can run Minimax 2.1, you would feel like you had the most powerful AI on Earth. And it'd be able to run 50 tokens a second to 10 users at once",
          "score": 1,
          "created_utc": "2026-02-11 15:46:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yvnuv",
              "author": "running101",
              "text": "where is a good place to procure the 32gb v100 at a good price? \n\nAlso I thought it is better to get one big card vs many small cards for inference?",
              "score": 2,
              "created_utc": "2026-02-12 12:14:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dsnlv",
                  "author": "TheRiddler79",
                  "text": "Ebay. And if you can afford one big  card, that's always a good idea, but if you can get 2-3x vram on multiple cards, in particular v100 or other enterprise gear, it's going to do the job perfectly.",
                  "score": 1,
                  "created_utc": "2026-02-14 18:48:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56m00w",
          "author": "Rain_Sunny",
          "text": "Consider:  \n  \n1. NPU solution: AMD AI Max+ 395 CPU+128 GB(RAM 32GB+VRAM 96 GB)+ 2 TBÔºàSSDÔºâ,Support 120B(GPT-OSS). Price 4000-5000 USD.\n\n2.NPU solution: DXG Spark: 128 GB+2TB,Support 200B LLMs. Price: 5000 USD.\n\nINT4.",
          "score": 1,
          "created_utc": "2026-02-13 15:59:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n4m5x",
          "author": "fasti-au",
          "text": "Depend on uostres.  Naked you want qwen 30b with decent cintext as a bench but it was different when you had uostreamnipen ai etc not allowing agentvaccess  with subs",
          "score": 0,
          "created_utc": "2026-02-10 16:50:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4naja7",
          "author": "That-Cost-9483",
          "text": "No, the best thing for LLM solely would be to have the GPU/CPU/systemRAM/vRAM all tied together. Just one big soldered heap. This is kind of the direction apple has been going for awhile but obviously for AI you want cuda, so the spark is the best option. Other options would be to build a something‚Ä¶ you have quite a large budget. I‚Äôve seen wild 3090 builds, these things are zip tied to racks and feeding into servers to utilize the enterprise ram and power supplies.\n\n5090 in a gaming PC is way more practical. We will have to see what bolt graphics (Zeus) brings us‚Ä¶ being able to add memory to a graphics processor is either the greatest thing that will ever happen or those guys are stealing everyone‚Äôs money. Then we will have to wait for nvida to steal their research and give us the RTX7095 vRAM 64gb extendable to 256 üôè",
          "score": 0,
          "created_utc": "2026-02-10 17:17:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r220hc",
      "title": "ACE Step 1.5 is here: beats Suno on common eval metrics",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/vkrplb696wig1",
      "author": "North-Jeweler-8699",
      "created_utc": "2026-02-11 16:30:07",
      "score": 53,
      "num_comments": 30,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r220hc/ace_step_15_is_here_beats_suno_on_common_eval/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4u2wkg",
          "author": "sleepy_roger",
          "text": "Everyone saying it beats Suno clearly doesn't use Suno. This is great to have locally and I'm excited for 2.0 but saying it beats Suno is just dumb honestly.",
          "score": 38,
          "created_utc": "2026-02-11 17:43:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4udnfp",
              "author": "Decaf_GT",
              "text": "It's just like all the other LinkedIn style posts here...\"LLM #243 just came out and it BEATS OPUS 4.6 no seriously\"\n\nThe *fuck* it does. As fun as local models are, pretending for a moment that the garbage, anime-style generic pop song is anything close to what Suno can produce is utterly deluded. \n\nAs much fun as I have tinkering with local models, I've become so *tired* of the hyperbolic claims here from people who *desperately* want to show that \"local models are always better and they're the future\". \n\nMaking up some bullshit about \"evals\" (really? evals...for something as subjective as music?) and then using an AI to create a bunch of nonsense \"questions\" and throwing it into a Reddit post...ugh.",
              "score": 10,
              "created_utc": "2026-02-11 18:32:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wjoxq",
                  "author": "ptear",
                  "text": "+1 let's definitely stay realistic in this sub, we're all supportive of local models and this is an amazing constantly iterating space.\n\n\nRight now I'm all about right sizing use cases by model. If I can do something locally I love it since I can just depend on my own systems. We can't do everything as great local today, and I totally use hosted models and also have fun experimenting with products like Suno too!",
                  "score": 3,
                  "created_utc": "2026-02-12 01:10:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uek8x",
                  "author": "sleepy_roger",
                  "text": "Yeah it's definitely tiring. I sound paranoid I know but it seems as if a few of these model providers have a marketing and or affiliate scheme setup so you see reddit subs hit with all these stupid posts. Kimi, minimax, and z.ai are the biggest offenders in the llm space by far.",
                  "score": 3,
                  "created_utc": "2026-02-11 18:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4u4nen",
              "author": "mintybadgerme",
              "text": "True that.",
              "score": 5,
              "created_utc": "2026-02-11 17:51:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ucu7a",
          "author": "SanDiegoDude",
          "text": "lol no it doesn't, and it's not even close. Don't get me wrong, AceSTEP 1.5 is great for the latest best in breed OSS music generator, but even it's very best sounds like poo compared to just the average Suno output.",
          "score": 7,
          "created_utc": "2026-02-11 18:29:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4udtto",
              "author": "Decaf_GT",
              "text": "It sounds like Suno V3 at best, and Suno is now on V5.",
              "score": 5,
              "created_utc": "2026-02-11 18:33:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uede7",
                  "author": "SanDiegoDude",
                  "text": "Yea, mostly robotic voices, very simple layering of instruments and sounds, and it will still sometimes get lost and lose the key, and good lord I don't think it's ever actually honored the BPM I set for it. I like AceSTEP, it's a lot of fun to take lyrics from songs you like and twist them into new genres, but it's still a toy - nobody is going to release ANYTHING out of AceSTEP over what you can get from the same prompts from the modern cloud music generators, the quality gap is just too wide.",
                  "score": 3,
                  "created_utc": "2026-02-11 18:36:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o500b23",
          "author": "emperorofrome13",
          "text": "It is fun to use. Trolled patriots fans at the superbowl party",
          "score": 2,
          "created_utc": "2026-02-12 15:59:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tqn6v",
          "author": "tim_dude",
          "text": "It's good for making pop. That's it.",
          "score": 2,
          "created_utc": "2026-02-11 16:45:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uymda",
              "author": "nntb",
              "text": "Out of the box It's crazy good at Japanese enka. It's not good at cat sounds,unless you train a Lora which is quite easy then it's amazing at it. There is nothing it does bad when trained.",
              "score": 2,
              "created_utc": "2026-02-11 20:11:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4twswq",
              "author": "FaceDeer",
              "text": "I've been able to get a wide variety of genres out of it.",
              "score": 2,
              "created_utc": "2026-02-11 17:14:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tygrt",
                  "author": "tim_dude",
                  "text": "Yes but is it good?",
                  "score": 2,
                  "created_utc": "2026-02-11 17:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4unopc",
          "author": "raysar",
          "text": "so benchmark suck. Zero humain prefer ace quality against suno v5.",
          "score": 1,
          "created_utc": "2026-02-11 19:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vsej6",
          "author": "MrWeirdoFace",
          "text": "It's a little rough around the edges, to be honest, but I look forward to it improving with future versions.",
          "score": 1,
          "created_utc": "2026-02-11 22:36:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wf3z8",
          "author": "uti24",
          "text": ">Speed: full song under 2s on A100\n\nIt would be great for images, or for text (where you can utilize speed that is faster than information consuming) but what's the point to generate music in under 2s? Can we have like 10 second or 30, but better quality?\n\n>Quality: beats Suno on common eval scores\n\nWhat does it mean? It sounds not better than Suno, not even better than older versions, like, 2 years ago versions? Realisticly? Like Suno 2. (I thing actual version of Suno is 4 or 4.5, or something like that and every new version sonds better than previous, so much better one would not want to return to previous version)\n\nDon't get me wrong, I still love that we have Suno at home, though, that's a lot, it don't have to beat Suno to be great.",
          "score": 1,
          "created_utc": "2026-02-12 00:42:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wkqtf",
              "author": "Creepy-Bell-4527",
              "text": "The reason the generation speed is so important is because you need to generate batches of 10 just to get 1 track which follows the lyrics and doesn't skip entire verses.",
              "score": 3,
              "created_utc": "2026-02-12 01:16:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ylz0i",
                  "author": "uti24",
                  "text": "And to listen to a single generation I need like 30 seconds, so how am I utilizing 2 seconds song generation?",
                  "score": 1,
                  "created_utc": "2026-02-12 10:54:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wjsk0",
          "author": "Technical_Ad_440",
          "text": "not anywhere close i have used it. after 2 generations of the same prompt but random seed it has almost 0 variation. maybe when it gets variation. i used it for 1 night then gave up on it",
          "score": 1,
          "created_utc": "2026-02-12 01:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wwans",
          "author": "Mods_Are_Fatties",
          "text": "What was the prompt used for this? sounds good to me\n\nWould be interesting to see what suno generates with the same prompt",
          "score": 1,
          "created_utc": "2026-02-12 02:26:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50dnmv",
          "author": "AppealThink1733",
          "text": "Does he already have his gguf file to use in comfyui?",
          "score": 1,
          "created_utc": "2026-02-12 17:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vrmmd",
          "author": "Aggravating_Fun_7692",
          "text": "Maybe make your own music, how about that?",
          "score": -2,
          "created_utc": "2026-02-11 22:32:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wklvs",
              "author": "Creepy-Bell-4527",
              "text": "You must be new here",
              "score": 2,
              "created_utc": "2026-02-12 01:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yzuxz",
                  "author": "Aggravating_Fun_7692",
                  "text": "Sorry I hate soulless music, sue me",
                  "score": 0,
                  "created_utc": "2026-02-12 12:43:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4vp1d",
      "title": "Best program and model to make this an actual 3d model?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/41y95nmiwijg1.png",
      "author": "Kolpus",
      "created_utc": "2026-02-14 20:56:37",
      "score": 50,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4vp1d/best_program_and_model_to_make_this_an_actual_3d/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5euie8",
          "author": "chevellebro1",
          "text": "Hunyuan3D is the best local model I‚Äôve found. I run using the template workflow in ComfyUI. If you want best quality, I‚Äôd recommend looking at Meshy.ai. The quality is impressive from their newer models",
          "score": 10,
          "created_utc": "2026-02-14 22:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gyxri",
              "author": "Dadda9088",
              "text": "Yeah, use an img2img model to get different views and hunyuan3d to make it in 3D is the way to go locally. The limitation could be missing textures and output format.",
              "score": 2,
              "created_utc": "2026-02-15 07:06:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5eitp3",
          "author": "sumane12",
          "text": "You tried the latest trellis?",
          "score": 7,
          "created_utc": "2026-02-14 21:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ew2ko",
              "author": "Kolpus",
              "text": "Thank you I just tried it with default settings and result is way better than other models I tried. Will try again with maxed settings.\n\nFor other that are interested I used this easy installer: [https://github.com/IgorAherne/trellis-stable-projectorz/releases/tag/latest](https://github.com/IgorAherne/trellis-stable-projectorz/releases/tag/latest)\n\n",
              "score": 11,
              "created_utc": "2026-02-14 22:20:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ewasu",
                  "author": "runlinux",
                  "text": "Oooo? I‚Äôm interested in this too! Thanks for finding this.",
                  "score": 1,
                  "created_utc": "2026-02-14 22:21:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5eljwl",
              "author": "o5mfiHTNsH748KVq",
              "text": "This is the answer\n\nhttps://microsoft.github.io/TRELLIS.2/",
              "score": 9,
              "created_utc": "2026-02-14 21:22:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fshzh",
                  "author": "o5mfiHTNsH748KVq",
                  "text": "for funsies, i put your source image into trellis\n\nhttps://preview.redd.it/hraavaeybkjg1.png?width=1684&format=png&auto=webp&s=27403c19c2b8a52ed6259611a6eff59d591fd6b7\n\nsettings can probably be tweaked",
                  "score": 5,
                  "created_utc": "2026-02-15 01:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ehsc4",
          "author": "Dramatic_Entry_3830",
          "text": "I want to know too",
          "score": 3,
          "created_utc": "2026-02-14 21:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gryya",
          "author": "an80sPWNstar",
          "text": "Trellis 2 is the way. Be sure to run it in a repair app first to make sure all is well. There's a free one that's popular with the 3d modeling world",
          "score": 2,
          "created_utc": "2026-02-15 06:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f4rfq",
          "author": "KiwiNFLFan",
          "text": "Build it from scratch in Blender?",
          "score": 3,
          "created_utc": "2026-02-14 23:10:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f61mm",
              "author": "frogsarenottoads",
              "text": "https://preview.redd.it/dlee5c0rmjjg1.jpeg?width=236&format=pjpg&auto=webp&s=dcde1aa0f34cab3a20d8e506fb6937bae0ebc3b4\n\nFeel free to fill this in yourself I'm far too lazy",
              "score": 17,
              "created_utc": "2026-02-14 23:18:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5i9ug9",
              "author": "MixeroPL",
              "text": "Learn a new skill? Nah we got ai slop",
              "score": -2,
              "created_utc": "2026-02-15 13:55:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3u81s",
      "title": "GLM5",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/1p89zxhfjajg1.png",
      "author": "I_like_fragrances",
      "created_utc": "2026-02-13 16:45:37",
      "score": 48,
      "num_comments": 10,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r3u81s/glm5/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o58vv89",
          "author": "FuzzeWuzze",
          "text": "Lol i want to meet the chad downloading a 1.51 Terabyte file from HF.\n\nOnly to get to the end get a checksum error...",
          "score": 19,
          "created_utc": "2026-02-13 22:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d0zma",
              "author": "Aggressive-Fan-3473",
              "text": "I believe HF uses tls for model downloads so that wouldn‚Äôt happen. The tls would not be able to decrypt and would request the corrupted packet again",
              "score": 1,
              "created_utc": "2026-02-14 16:30:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dak87",
                  "author": "FuzzeWuzze",
                  "text": "Maybe it's a lm studio issue then because it most definitely happens",
                  "score": 1,
                  "created_utc": "2026-02-14 17:18:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o591yqc",
          "author": "LA_rent_Aficionado",
          "text": "I assume this is unsloth‚Äôs repo?\n\n‚ÄúXL‚Äù on unsloth is usually how they categorize their UD dynamic quants, not because they are XL but because there is no separate/better way to fit them into the standard categories.",
          "score": 8,
          "created_utc": "2026-02-13 23:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58k43j",
          "author": "Pixer---",
          "text": "I found that the M model performs better at longer context. The XL variant falls of quicker for programming. It‚Äôs quite noticeable in longer conversations",
          "score": 6,
          "created_utc": "2026-02-13 21:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58tb32",
          "author": "qwen_next_gguf_when",
          "text": "Can't run the smallest\n\n![gif](giphy|uqmtVo5zcIBXJq1rGX)",
          "score": 3,
          "created_utc": "2026-02-13 22:26:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b9qgl",
          "author": "beefgroin",
          "text": "I need Q0.1",
          "score": 2,
          "created_utc": "2026-02-14 08:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o577p96",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 3,
          "created_utc": "2026-02-13 17:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58kuc1",
              "author": "inevitabledeath3",
              "text": "Quantisation doesn't change things like model structure, number of layers, embedding size, or parameters. Llama is lying to you.",
              "score": 4,
              "created_utc": "2026-02-13 21:44:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b92g6",
          "author": "emrbyrktr",
          "text": "Is it better than Qwen 3 Coder Next?",
          "score": 1,
          "created_utc": "2026-02-14 08:52:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzsynx",
      "title": "Qwen3 Coder Next on M3 Ultra v.s. GX10",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "author": "Imaginary_Ask8207",
      "created_utc": "2026-02-09 03:11:13",
      "score": 46,
      "num_comments": 21,
      "upvote_ratio": 0.94,
      "text": "https://preview.redd.it/rg9mxsm7zdig1.png?width=2294&format=png&auto=webp&s=7400c7c54428910158a160e5e407022cbba24947\n\n[Qwen3-Coder-Next served on GX10](https://reddit.com/link/1qzsynx/video/cfad1dooxdig1/player)\n\n[Qwen3-Coder-Next served on M3 Ultra 512GB](https://reddit.com/link/1qzsynx/video/sawsb3kpxdig1/player)\n\nI'm currently exploring CLI-based coding tools as an alternative to GitHub Copilot.\n\nRight now I'm testing **opencode** (seems to be the most popular open-source option at the moment), and I paired it with the new **Qwen3-Coder-Next** model. It's decent!\n\nThe tasks I tested were pretty simple, just some small refactoring in my toy project and fixing one bug. But that's exactly the point, for these kind of everyday coding tasks, you ain't gonna need Opus 4.6 level of intelligence, a \"small and sweet\" open source model is usually good enough.\n\nI feel like 80b model is a sweet spot for GX10 with 128GB of GPU memory, 8-bit quantized models could be comfortably loaded into the GPU. For comparison between the two devices, you can clearly see M3 ultra gives a higher throughput, but it's 3x the price of GX10.\n\nDo you think going local will be the coming trend?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4df22x",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 9,
          "created_utc": "2026-02-09 03:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4diz60",
              "author": "Imaginary_Ask8207",
              "text": "Great suggestion! I don't think GX10 will be able to load Step-3.5-Flash, I'll try on M3 ultra.",
              "score": 2,
              "created_utc": "2026-02-09 03:47:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dr3e2",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-02-09 04:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dmfxb",
          "author": "NoobMLDude",
          "text": "Going local is already the current trend !!\nYou are in the right subreddit for it. üòâ\n\n\nI‚Äôve gone Local since a long time for most of my tasks that are Private.\n\nI‚Äôm also trying to educate the community that you don‚Äôt need to give your money and personal data to big AI companies. You can download Free and OpenSource models and they are good enough for 90% of users. \n\n\nHere are few examples of Local AI workflows that I use:\n- [Local Meeting Assistant](https://youtu.be/cveV7I7ewTA)\n- [Local Talking Assistant](https://youtu.be/2VHzYy45kPw)\n- [Terminal with AI Context support](https://youtu.be/_sDJBosDznI)\n\n\nI added a few more in the [LocalAI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)\n\nOwen3-Coder-Next looks like a solid model for Coding if you can run a 80B model on your hardware.",
          "score": 6,
          "created_utc": "2026-02-09 04:10:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4e5mlg",
          "author": "HumanDrone8721",
          "text": "As I know someone using a similar DGX Spark setup, they use a specialized replacement for the otherwise excellent nvtop that is not really optimized for Sparks that is called dgxtop:\n\nhttps://github.com/GigCoder-ai/dgxtop\n\nIs vibe coded, but one of the good ones IMHO:",
          "score": 3,
          "created_utc": "2026-02-09 06:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e7gw8",
              "author": "Imaginary_Ask8207",
              "text": "Thanks for sharing! This definitely looks better than nvtop. Is it stable and well-tested?",
              "score": 1,
              "created_utc": "2026-02-09 06:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e884x",
                  "author": "HumanDrone8721",
                  "text": "AFAIK they have terminals open running it on both of their Spark cluster nodes as it shows also the network traffic over the high-speed interfaces (these are indeed FAST), along with the CPU and GPU temps, for me is fascinating how they correlate while doing complex inference. So far I've heard no complains, but I'm a poor so I don't own a Spark or two ;).",
                  "score": 2,
                  "created_utc": "2026-02-09 06:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4eorv5",
              "author": "eleqtriq",
              "text": "Nvtop has been updated and works now",
              "score": 1,
              "created_utc": "2026-02-09 09:38:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h70nx",
                  "author": "HumanDrone8721",
                  "text": "Super, thanks, I will let the \"sparkers\" know about it-",
                  "score": 1,
                  "created_utc": "2026-02-09 18:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gqa1r",
          "author": "kingcodpiece",
          "text": "I can say that it's running a fair bit faster than that Mac example on my GX10 using an FP4MX quant which the Grace Blackwell machines love (I don't have the exact numbers but it was like 65 t/s outout and 370ish prefill) . That's using llama.cpp BTW. It's the first model that's had an acceptable speed vs quality tradeoff on this machine for me.",
          "score": 2,
          "created_utc": "2026-02-09 17:21:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jskzh",
              "author": "Imaginary_Ask8207",
              "text": "How does llama.cpp compare with vLLM on GX10?   \nI am used to serving models through vLLM, but nvfp4 MoE models will crash intermittently, which is quite annoying.",
              "score": 1,
              "created_utc": "2026-02-10 02:54:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k9i9t",
                  "author": "kingcodpiece",
                  "text": "vLLM was a pain to get running. Llama.cpp was much easier to get running and it's faster and so far very stable.\n\nEdit: For anybody interested, the architecture isn't properly implemented in stable pytorch yet (CUDA version is too high, plus ARM CPU is a pain to workaround).",
                  "score": 1,
                  "created_utc": "2026-02-10 04:44:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m0a9i",
          "author": "techlatest_net",
          "text": "M3 Ultra 512GB outpacing GX10 on Qwen3-Coder-Next throughput is no surprise‚ÄîApple's unified memory crushes fragmented GPU loads, especially for 80B 8-bit quant that saturates 128GB cleanly.\n\n**Hardware showdown**:\n- **M3 Ultra wins**: Higher t/s across refactors/bug fixes (your vids prove it). Unified 512GB = zero swapping, CLI tools like opencode scream.\n- **GX10 value king**: 1/3 price for \"good enough\" 80B coding. nVidia ecosystem + CUDA upgrades future-proof it over Apple's walled garden.\n\n**Local coding trend locked in**:\n- Opencode + Qwen3-Coder-Next already beats Copilot for toy projects/real work‚Äîyour \"small and sweet\" thesis is spot on.\n- 80B sweet spot confirmed: Everyday tasks don't need Opus-level IQ, just reliable context handling.\n\n**Stick with GX10** if budget > peak perf. M3 Ultra's premium is for Mac diehards who refuse Docker. Local CLI stacks (opencode/aider) are obliterating SaaS subscriptions‚Äîyour benchmarks prove the economics work today.",
          "score": 2,
          "created_utc": "2026-02-10 13:28:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qqxcg",
          "author": "catplusplusok",
          "text": "Just saying, NVIDIA Thor Dev Kit is slightly cheaper than DGX Spark and seems to be faster both on paper (twice the compute speed) and practice (less crusty NVFP4 support now after a rough start). Now idea about vs SOTA Mac Studio, but those are expensive.",
          "score": 2,
          "created_utc": "2026-02-11 04:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4el8of",
          "author": "HallTraditional2356",
          "text": "Amazing! How did you host it on the Apple Silicon M3 Ultra to Connect opencode to it ?",
          "score": 1,
          "created_utc": "2026-02-09 09:02:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4es3zv",
              "author": "Imaginary_Ask8207",
              "text": "I server the model with LM Studio, which gives OpenAI compatible endpoints.",
              "score": 3,
              "created_utc": "2026-02-09 10:11:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jk42v",
                  "author": "nickollie_",
                  "text": "How are you finding the latest version of LM Studio‚Äôs server?",
                  "score": 1,
                  "created_utc": "2026-02-10 02:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fnvqr",
          "author": "bakawolf123",
          "text": "You are comparing GX10 to maxed M3 Ultra though, you can get one with 256GB (alas there's no 128GB option, only 96GB which seems just shy of decent space for context), it's more like 2x price then and you get an actual working environment on top of being able to run a model in background.\n\nOr you can go even cheaper with AMD AI Max+ 395 (TG is about same as gx10 according to llama.cpp benchmarks, but prefill is slower)",
          "score": 1,
          "created_utc": "2026-02-09 14:09:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g5a24",
          "author": "Vozer_bros",
          "text": "I actually just need a good architecture, and a local model with good tool calling and SWE bench a bit better than 60%.",
          "score": 1,
          "created_utc": "2026-02-09 15:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kj5uo",
          "author": "taiphamd",
          "text": "Should try to run the DGX spark with flashinfer as the attention backend using vLLM. Furthermore vLLM also has MoE tuning which might be able to squeeze out some more tps. See this issue : https://github.com/vllm-project/vllm/issues/17619 but you basically launch benchmark_moe.py and remember to specify max batch_size or else it will try to tune up to 4096 and that might take a week to run.",
          "score": 1,
          "created_utc": "2026-02-10 05:58:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2vcgi",
      "title": "Mac M4 vs. Nvidia DGX vs. AMD Halo Strix",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r2vcgi/mac_m4_vs_nvidia_dgx_vs_amd_halo_strix/",
      "author": "alfons_fhl",
      "created_utc": "2026-02-12 14:48:28",
      "score": 44,
      "num_comments": 103,
      "upvote_ratio": 0.89,
      "text": "Has anyone experiences or knowledge about:\n\n**Mac M4** vs. **Nvidia DGX** vs. **Amd Halo Strix**\n\n\n\n\\-each with **128gb**\n\n\\-to **run LLM's**\n\n\\-**not** for tune/train\n\n\n\nI cant find any good reviews on youtube, reddit...\n\n\n\nI heard that Mac is much faster (t/s), but not for train/tune (so fine for me)\n\nIs it true?\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r2vcgi/mac_m4_vs_nvidia_dgx_vs_amd_halo_strix/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4zpv10",
          "author": "Miserable-Dare5090",
          "text": "Look up alex ziskind on youtube. He has tested all 3 head to head recently‚Äîwhich is important bc the benchmarks from october are stale now.\n\nBy the way, you don‚Äôt want to run LLMs. You want to use coding agents. And that is a different factor to add. \n\nThe prompts on coding agents are very large, it‚Äôs not just what you want to code but also instructions. I suggest you look into it with that in mind, and with agentic use, concurrencies matter too (Number of requests in parallel).",
          "score": 20,
          "created_utc": "2026-02-12 15:09:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ztvzf",
              "author": "alfons_fhl",
              "text": "Okay, I'm planning to use up to 256k context tokens. So its very large.\n\nYes I heard about the agents.\n\nSo with a good (software/tool) like ClaudeCode/OpenClaude...\n\n  \nWich software/tool and wich device you would recommend?",
              "score": 3,
              "created_utc": "2026-02-12 15:29:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o505cxc",
                  "author": "Longjumping-Boot1886",
                  "text": "in case of Macs, everyone waiting M5 Pro/Max/Ultra for that, because their main change - improved prompt processing time, and thats the main thing when you are putting in 100k context message.",
                  "score": 4,
                  "created_utc": "2026-02-12 16:23:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50aqf4",
                  "author": "jinnyjuice",
                  "text": "The context token size is a bit more model dependent. For example, official max digits may be in the 6 digits for GPT OSS 120B, but its performance degrades after about 30k or so.",
                  "score": 3,
                  "created_utc": "2026-02-12 16:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o59u1qr",
              "author": "DopePedaller",
              "text": "I think Alex's video about concurrency (on DGX and other platforms) is a must watch for anyone interested in local llms. I was a bit underwhelmed by the benchmarks until seeing what can be achieved with the right HW + concurrent use on vLLM. [LINK](https://youtu.be/Ze5XLooTt6g)",
              "score": 2,
              "created_utc": "2026-02-14 02:07:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54s0tx",
              "author": "alfons_fhl",
              "text": "\"alex ziskind\", tested an M4 Pro Mini with 128gb... But the M4 Pro Mini with 128gb isn't available...\n\n128gb only works with Mac Studio, like the Mac Studio M4 Max...\n\nOr does I understand something wrong?.. :/ ",
              "score": 1,
              "created_utc": "2026-02-13 08:36:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50a5dh",
          "author": "Grouchy-Bed-7942",
          "text": "I‚Äôve got a Strix Halo and 2x GB10 (Nvidia DGX Spark, but the Asus version).\n\nFor pure AI workloads, I‚Äôd go with the GB10. For example, on GPT-OSS-120B I‚Äôm hitting \\~6000 pp and 50‚Äì60 t/s with vLLM, and I can easily serve 3 or 4 parallel requests while still outperforming my Strix Halo, which struggles to reach \\~700 pp and \\~50 t/s with only a single concurrent request!\n\nExample with vLLM on the GB10 ([https://github.com/christopherowen/spark-vllm-mxfp4-docker](https://github.com/christopherowen/spark-vllm-mxfp4-docker)):\n\n|model|test|t/s|peak t/s|peak t/s (req)|ttfr (ms)|est\\_ppt (ms)|e2e\\_ttft (ms)|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|gpt-oss-120b|pp512|2186.05 ¬± 17.36|||235.38 ¬± 1.85|234.23 ¬± 1.85|284.43 ¬± 2.11|\n|gpt-oss-120b|tg32|63.39 ¬± 0.07|65.66 ¬± 0.08|65.66 ¬± 0.08||||\n|gpt-oss-120b|pp512|2222.35 ¬± 10.78|||231.55 ¬± 1.12|230.39 ¬± 1.12|280.76 ¬± 1.14|\n|gpt-oss-120b|tg128|63.44 ¬± 0.07|64.00 ¬± 0.00|64.00 ¬± 0.00||||\n|gpt-oss-120b|pp2048|4888.74 ¬± 36.61|||420.10 ¬± 3.13|418.95 ¬± 3.13|469.42 ¬± 2.85|\n|gpt-oss-120b|tg32|62.38 ¬± 0.08|64.62 ¬± 0.08|64.62 ¬± 0.08||||\n|gpt-oss-120b|pp2048|4844.62 ¬± 21.71|||423.90 ¬± 1.90|422.75 ¬± 1.90|473.38 ¬± 2.10|\n|gpt-oss-120b|tg128|62.65 ¬± 0.08|63.00 ¬± 0.00|63.00 ¬± 0.00||||\n|gpt-oss-120b|pp8192|6658.41 ¬± 30.91|||1231.51 ¬± 5.73|1230.35 ¬± 5.73|1283.13 ¬± 5.97|\n|gpt-oss-120b|tg32|60.39 ¬± 0.14|62.56 ¬± 0.14|62.56 ¬± 0.14||||\n|gpt-oss-120b|pp8192|6660.84 ¬± 38.83|||1231.08 ¬± 7.13|1229.92 ¬± 7.13|1281.95 ¬± 6.97|\n|gpt-oss-120b|tg128|60.76 ¬± 0.03|61.00 ¬± 0.00|61.00 ¬± 0.00||||\n|gpt-oss-120b|pp16384|5920.87 ¬± 13.29|||2768.33 ¬± 6.23|2767.18 ¬± 6.23|2821.06 ¬± 6.16|\n|gpt-oss-120b|tg32|58.12 ¬± 0.13|60.21 ¬± 0.13|60.21 ¬± 0.13||||\n|gpt-oss-120b|pp16384|5918.04 ¬± 8.14|||2769.65 ¬± 3.81|2768.49 ¬± 3.81|2823.16 ¬± 3.66|\n|gpt-oss-120b|tg128|58.14 ¬± 0.08|59.00 ¬± 0.00|59.00 ¬± 0.00||||\n|gpt-oss-120b|pp32768|4860.07 ¬± 8.18|||6743.46 ¬± 11.34|6742.30 ¬± 11.34|6800.08 ¬± 11.34|\n|gpt-oss-120b|tg32|54.05 ¬± 0.14|55.98 ¬± 0.14|55.98 ¬± 0.14||||\n|gpt-oss-120b|pp32768|4858.40 ¬± 5.92|||6745.77 ¬± 8.22|6744.62 ¬± 8.22|6802.72 ¬± 8.15|\n|gpt-oss-120b|tg128|54.18 ¬± 0.09|55.00 ¬± 0.00|55.00 ¬± 0.00||||\n\nllama-benchy (0.3.0) date: 2026-02-12 13:56:46 | latency mode: api\n\nNow the Strix Halo with llama.cpp (the GB10 with llama.cpp is also faster, around 1500‚Äì1800 pp regardless of context):\n\nggml\\_cuda\\_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n\n|model|size|params|backend|ngl|type\\_k|type\\_v|fa|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp512|649.94 ¬± 4.23|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp2048|647.72 ¬± 1.88|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp8192|563.56 ¬± 8.42|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp16384|490.22 ¬± 0.97|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|pp32768|388.82 ¬± 0.02|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|tg32|51.45 ¬± 0.05|\n|gpt-oss 120B MXFP4 MoE|59.02 GiB|116.83 B|ROCm|999|q8\\_0|q8\\_0|1|tg128|51.49 ¬± 0.01|\n\nbuild: 4d3daf80f (8006)\n\nThe noise difference is also very noticeable: the GB10 at full load is just a light whoosh, whereas the Strix Halo (MS S1 Max in my case) spins up quite a bit.\n\nSo if you‚Äôve got ‚Ç¨3k, get a GB10. If you don‚Äôt want to spend that much, a Bossgame at ‚Ç¨1500‚Äì1700 will also do the job, just with lower performance. But if you‚Äôre looking to run parallel requests (agents or multiple users), the GB10 will be far more capable. Same thing if you want to run larger models: you can link two GB10s together to get 256 GB of memory, which can let you run MiniMax M2.1 at roughly Q4 equivalent without issues using vLLM.\n\nI don‚Äôt have a Mac, but in my opinion it‚Äôs not worth it, except for an M3 Ultra with 256 GB / 512 GB of RAM.",
          "score": 13,
          "created_utc": "2026-02-12 16:45:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51592c",
              "author": "fallingdowndizzyvr",
              "text": "> outperforming my Strix Halo, which struggles to reach ~700 pp and ~50 t/s with only a single concurrent request!\n\nIf you are only getting 700PP on your Strix Halo. Then you are doing something wrong. I get ~1000PP.\n\n    ggml_cuda_init: found 1 ROCm devices:\n      Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | n_batch | n_ubatch | fa | mmap |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -------: | -: | ---: | --------------: | -------------------: |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | ROCm,Vulkan |  99 |    4096 |     4096 |  1 |    0 |          pp4096 |       1012.63 ¬± 0.63 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | ROCm,Vulkan |  99 |    4096 |     4096 |  1 |    0 |           tg128 |         52.31 ¬± 0.05 |",
              "score": 3,
              "created_utc": "2026-02-12 19:10:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o535puz",
                  "author": "Grouchy-Bed-7942",
                  "text": "Do you have the details (OS, ROCM version, llamacpp arguments? I'm interested. I was able to reach 1000 PP with Rocm 6.4.4, here the benchmark is with 7.2).",
                  "score": 1,
                  "created_utc": "2026-02-13 01:26:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o54cdq4",
                  "author": "alfons_fhl",
                  "text": "If you have the choice again, will you buy the Strix halo again?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-13 06:16:05",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5kzhb7",
                  "author": "Miserable-Dare5090",
                  "text": "I see your posts often. I know you are a big fan and I also got a bosgame m5. But the honest response is that the GB10 chip is superior, hands down. I have both and i wish that I could say AMD made a better price Spark, but‚Ä¶did you notice the difference at 4096 tokens? 1000 vs 4888. \n\nNow, for the actual question being asked, of agentic coding, which needs long contexts. What is your strix halo doing at 32000 tokens? Is it still processing 1000 tokens per second?",
                  "score": 1,
                  "created_utc": "2026-02-15 22:10:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50p8d8",
              "author": "NeverEnPassant",
              "text": "There must be some mistake with your PP numbers. They are close to what you would see with a RTX 6000 Pro.",
              "score": 1,
              "created_utc": "2026-02-12 17:56:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51p9en",
                  "author": "Grouchy-Bed-7942",
                  "text": "That repo ships a pre-tuned Docker image for DGX Spark/GB10 that bundles the right vLLM build + MXFP4/CUTLASS kernels (and attention stack like FlashInfer) so you don‚Äôt fall back to slower generic paths.\nBecause the whole software stack is locked (libs, flags, backends, caching), you get near-best-case throughput out of the box, which can make Spark numbers look surprisingly close to an RTX 6000 Pro on the same low-precision inference workload.",
                  "score": 2,
                  "created_utc": "2026-02-12 20:46:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50chqb",
              "author": "alfons_fhl",
              "text": "Thanks!\n\nIts really helpful.\n\nSo the Asus is around 15% better... But paying the 1,8x for it... idk...\n\nBut you told the pp... I guess for big context like 200k it is a big problem for \"halo Strix\" and only work on Nvidia, right?\n\n",
              "score": 0,
              "created_utc": "2026-02-12 16:56:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50fs2u",
                  "author": "Grouchy-Bed-7942",
                  "text": "Nope, you didn‚Äôt read everything.\n\nWith vLLM, the Asus is about 5√ó faster at prompt processing (pp). vLLM on Strix Halo is basically a non-starter, performance is awful. It‚Äôs also roughly 15% faster on token generation/writing (tg).\n\nTo make it concrete: if you‚Äôre coding with it using opencode, opencode injects a 10,000-token preprompt up front (tooling, capabilities, etc.). Add ~5,000 input tokens for a detailed request plus one or two context files, and you‚Äôre quickly at ~15,000 input tokens.\n\nOn that workload, the Asus GB10 needs under ~3 seconds to process the ~15k-token input and then starts generating at roughly 55‚Äì60 tok/s. The Strix Halo, meanwhile, takes just under ~30 seconds before it even begins generating, at around ~50 tok/s. You see the difference?\n\nIn other words, the GB10 can read 15,000 tokens and generate a bit more than 1500 tokens of output before the Strix Halo has started writing anything.\n\nAnd that‚Äôs where the GB10 really shines with VLLM : if, for example, someone is chatting with GPT-OSS-120B while you‚Äôre coding, performance doesn‚Äôt get cut in half. It typically drops by only a few percent.",
                  "score": 5,
                  "created_utc": "2026-02-12 17:11:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zpr9y",
          "author": "Creepy-Bell-4527",
          "text": "Wait for the M5 max / ultra. That will eat the others lunch.",
          "score": 4,
          "created_utc": "2026-02-12 15:09:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zquai",
              "author": "alfons_fhl",
              "text": "But I guess this one cost much much more... ",
              "score": 0,
              "created_utc": "2026-02-12 15:14:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zrp4b",
                  "author": "Creepy-Bell-4527",
                  "text": "I would hope it would cost as much as m3 max and ultra but you never know, with memory pricing being what it is.",
                  "score": 2,
                  "created_utc": "2026-02-12 15:18:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o507b8f",
          "author": "flamner",
          "text": "Honest opinion: if you need an agent to assist with coding, it‚Äôs not worth spending money on hardware to run local models. They will always lag behind cloud models or tools like Claude, or Codex. Anyone claiming otherwise is fooling themselves. Local models are fine for simpler tasks like data structuring or generating funny videos.",
          "score": 4,
          "created_utc": "2026-02-12 16:32:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51yjf5",
              "author": "AshKetchum1011",
              "text": "I totally agree with this. I have a M4 Max.",
              "score": 1,
              "created_utc": "2026-02-12 21:30:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54b25j",
              "author": "alfons_fhl",
              "text": "not really... plan is to do a 24/7 automatically coding system... So with cloud LLM its to expensive... for 24/7... ",
              "score": 1,
              "created_utc": "2026-02-13 06:05:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o54fah1",
                  "author": "flamner",
                  "text": "Mate, you‚Äôre operating on some unrealistic expectations. Companies invest billions of dollars in infrastructure to run systems like this, and you think you can achieve similar results on a local home computer? Don‚Äôt focus only on token generation speed, in coding, that‚Äôs not a meaningful metric. What really matters is the quality of the code the model produces. Have you even checked that? Have you looked at what kind of code the model you want to use actually generates? And have you calculated the electricity costs this setup will rack up?",
                  "score": 2,
                  "created_utc": "2026-02-13 06:40:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4znlj4",
          "author": "Look_0ver_There",
          "text": "This page here measures the Strix Halo vs the DGX Spark directly. [https://github.com/lhl/strix-halo-testing/](https://github.com/lhl/strix-halo-testing/)\n\nMind you, that's about 5 months old now and things have improved on the Strix Halo since, and likely on the DGX Spark too.  The DGX is going to always be faster for token processing due to the ARM based architecture, but the DGX only has about 7% faster memory than the Strix Halo, so token generation speeds are always going to be at about that ratio of a difference.\n\nFrom what I've read, the 128GB M4 Mac has the same memory bandwidth as the DGX Spark, so it's also going to generate at about the same speed as the Spark (with both being \\~7% faster than the Strix on average).  I don't know what the processing speeds are like on the Max though.  Both the Max and the DGX costs around twice as much as the Strix Halo solutions though, and if you ever plan to play games on your boxes, then the Strix Halo's are going to be better for that.",
          "score": 2,
          "created_utc": "2026-02-12 14:58:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zoqew",
              "author": "alfons_fhl",
              "text": "The biggest problem is, every video, say/show different results...\n\nThanks for the GitHub \"test\".\n\n\n\nRight now idk, what I should buy... \n\n\n\nSo AMD Halo Strix only 7% slower... is it more worth than an DGX or a Mac  \n\n\nPrices in EURO:\n\nMac 3.400‚Ç¨\n\nNvidia DGX (or Asus Ascent GX10): 2.750‚Ç¨\n\nAMD 2.200‚Ç¨",
              "score": 2,
              "created_utc": "2026-02-12 15:04:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zq1j9",
                  "author": "Look_0ver_There",
                  "text": "The big thing to watch out for is whether or not the tester is using a laptop based Strix Halo, vs a MiniPC one, and even then, which MiniPC exactly.  Pretty much all the laptops, and some of the MiniPC's won't give a Strix Halo the full power budget that it wants, and so one review may show it running badly, while another shows it running well.\n\n2.200‚Ç¨ for a Strix Halo seems surprisingly high priced.  You should be able to get the 128GB models for around the US$2000 mark, so whatever that converts to in Euro (\\~1700EUR)",
                  "score": 1,
                  "created_utc": "2026-02-12 15:10:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51a26z",
                  "author": "fallingdowndizzyvr",
                  "text": "> AMD 2.200‚Ç¨\n\nYou are overpaying if you pay that much for a Strix Halo. 1.770‚Ç¨\n\nhttps://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395",
                  "score": 1,
                  "created_utc": "2026-02-12 19:33:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zp93y",
              "author": "Miserable-Dare5090",
              "text": "This is very old. I‚Äôd say as an owner of both, not representative of current optimizations on each system. \n\nI also have a mac ultra chip studio. the spark is by far much faster prompt processing, which makes sense bc PP is compute bound, Inference is bandwidth bound. But no question that even the mac will choke after 40k tokens, as will the strix halo, but the spark is very good even at that size of context.",
              "score": 1,
              "created_utc": "2026-02-12 15:06:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zt15u",
                  "author": "alfons_fhl",
                  "text": "Okay yea up to 256k context would be perfect. So than you recommend the Spark.\n\nIs it still fast with the spark? ",
                  "score": 1,
                  "created_utc": "2026-02-12 15:25:12",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o508u1t",
                  "author": "alfons_fhl",
                  "text": "Is the context token problem only on m3 (ultra), what about the m4 max?",
                  "score": 1,
                  "created_utc": "2026-02-12 16:39:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50b52m",
              "author": "ScuffedBalata",
              "text": "> 128GB M4 Mac\n\nSpecifying this without specifying whether its Pro/Max/Max+/Ultra is weird.\n\nBecause the memory bandwidth of those are (roughly)...  240/400/550/820 GB/s  \n\nThe Ultra is double the Max and nearly 4x the Pro.",
              "score": 1,
              "created_utc": "2026-02-12 16:49:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50rfjo",
                  "author": "rditorx",
                  "text": "It's unlikely to be an M4 Ultra, and I think the only M4 with 128GB RAM is an M4 Max which would have 546 GB/s, so 2x the DGX Spark (273 GB/s) and more than 2x the Strix Halo (256 GB/s) and also faster than M3 Max (300 GB/s for the lower-core, 400 GB/s for the higher-core variant)",
                  "score": 4,
                  "created_utc": "2026-02-12 18:06:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51klf4",
                  "author": "po_stulate",
                  "text": "We kinda just assume it's M4 Max when we see people refer to 128GB M4 as it's the only M4 variant that has 128GB RAM configurations. Also, M4 Ultra isn't a thing.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:24:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zmft8",
          "author": "eleqtriq",
          "text": "My Mac M4 Pro is substantially slower than my DGX.  The prefill rate is atrocious.   My Max wasn‚Äôt really any better.   I‚Äôve been waiting patiently for the M5 Pro/Max. \n\nIf you‚Äôre just chatting, it‚Äôs fine. But if you want to upload large documents or code, Mac isn‚Äôt the way to go.",
          "score": 1,
          "created_utc": "2026-02-12 14:52:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zncar",
              "author": "alfons_fhl",
              "text": "My plan is to use it for **coding**.\n\nSo for example **qwen3-coder-next-80b.**\n\nAnd why do you bought the DGX? Do you use it for train LLM or to tune? or only to run?\n\nWhy do you not bought an AMD Strix Halo? :) ",
              "score": 1,
              "created_utc": "2026-02-12 14:57:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4znyzi",
                  "author": "spaceman_",
                  "text": "Strix Halo user here. Strix Halo's party trick is big memory. Prefill rates are terrible, decode (token generation) rates are below average. Long, agentic, tool using chains are pretty unusable in a lot of cases.",
                  "score": 3,
                  "created_utc": "2026-02-12 15:00:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5138p7",
                  "author": "ConspiracyPhD",
                  "text": "Is it worth it though?  Have you used qwen3-coder-next-80b for coding anything yet?  If you haven't, you might want to try build.nvidia.com's Qwen3-Coder-480B-A35B-Instruct (which is the larger version of it) with something like opencode or kilocode and see if it's worth investing in local hardware (which I have a feeling might be obsolete in a short time) versus just paying $10 a month for something like a github copilot pro plan for 300 requests a month (and then $0.04 per additional request).  That goes a long way.",
                  "score": 2,
                  "created_utc": "2026-02-12 19:01:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4zsq8o",
                  "author": "eleqtriq",
                  "text": "I fine tune and inference.  I also like to run image generation.  Since it‚Äôs nvidia, a lot of stuff just works.  Not everything, but a lot.",
                  "score": 1,
                  "created_utc": "2026-02-12 15:23:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o55u8sx",
                  "author": "Professional_Mix2418",
                  "text": "I went DGX spark as well bouw strix halo is so much more expensive. The difference in price isn‚Äôt as much and you get actually cuda cores.",
                  "score": 1,
                  "created_utc": "2026-02-13 13:38:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zwx7d",
          "author": "Soft_Syllabub_3772",
          "text": "Which iz the best for coding? :)",
          "score": 1,
          "created_utc": "2026-02-12 15:43:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50doob",
              "author": "Grouchy-Bed-7942",
              "text": "DGX Spark/GB10",
              "score": 2,
              "created_utc": "2026-02-12 17:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54m2nh",
                  "author": "alfons_fhl",
                  "text": "But why?...\n\nLike for example the LLM qwen3-coder-next-80b is very good for coding.\n\nBut why the DGX Spark, and not Mac or \"Halo Strix\"?",
                  "score": 1,
                  "created_utc": "2026-02-13 07:41:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5577pe",
          "author": "Osi32",
          "text": "My Mac M1 Max doesn‚Äôt ‚Äúdo‚Äù fp8, fp16 only. Which is a bit of a complexity in some situations. It means you can‚Äôt efficiently use all models. Nobody else has said this here. If you‚Äôre using LM Studio or something similar you won‚Äôt notice much of an issue, but if you‚Äôre in comfyui and you‚Äôre selecting and configuring your models it can be a bit annoying when you‚Äôre using a template and it‚Äôs all fp8.\nOf course, everything I just said applies to pytorch, other libraries may not have the same challenges.",
          "score": 1,
          "created_utc": "2026-02-13 10:58:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55b1cc",
              "author": "alfons_fhl",
              "text": "Yes, nobody has said this before.\n\nAhh okay so it is a software based problem..\n\n",
              "score": 2,
              "created_utc": "2026-02-13 11:30:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o58fllw",
                  "author": "Osi32",
                  "text": "Actually, its hardware. Nvidia does fp8, but not fp16 (floating point 16 bit). Apple does, but most of the hardware out in the wild is nvidia... so yes, the software manifesting the limitation, but its actually hardware driving the problem.",
                  "score": 1,
                  "created_utc": "2026-02-13 21:18:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5879eh",
          "author": "catplusplusok",
          "text": "I have an NVIDIA Thor which is slightly cheaper and faster, I think ASUS has an even cheaper SB10 box. Bottom line all of them will get similar performance if you go NVIDIA unified memory route. Great for quantized sparse MOE models, not enjoyable for big dense ones.",
          "score": 1,
          "created_utc": "2026-02-13 20:37:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h6822",
          "author": "fallingdowndizzyvr",
          "text": "Did you see this?\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/",
          "score": 1,
          "created_utc": "2026-02-15 08:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i6blu",
              "author": "alfons_fhl",
              "text": "90% of people say CUDA is very good and an very good option. So yea... :/ ",
              "score": 1,
              "created_utc": "2026-02-15 13:34:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5k6u0p",
                  "author": "fallingdowndizzyvr",
                  "text": "LOL. So you didn't see that? Since that says this.\n\n\"NVIDIA DGX Spark has **terrible CUDA** & software compatibility\"",
                  "score": 1,
                  "created_utc": "2026-02-15 19:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4yim5",
      "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4yim5/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "author": "Educational_Cry_7951",
      "created_utc": "2026-02-14 22:56:06",
      "score": 38,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n>**Please think of giving the Github repo a STAR if you like it :)**\n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n>\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations**\n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4yim5/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5fabnd",
          "author": "PomeloSweet926",
          "text": "Just tried this with Gemma3-27B-it (the NVFP4 version) and shit it fits and runs smoothly this is genuinely useful, nice work OP",
          "score": 3,
          "created_utc": "2026-02-14 23:45:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fao32",
              "author": "Educational_Cry_7951",
              "text": "Thank you, Next step for me is to integrate Qwen-Next and make sure it fits and runs fast on a low VRAM budget",
              "score": 1,
              "created_utc": "2026-02-14 23:47:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f4p9m",
          "author": "DataGOGO",
          "text": "Really interesting, what weight format does it expect?",
          "score": 1,
          "created_utc": "2026-02-14 23:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f62bk",
              "author": "Educational_Cry_7951",
              "text": "Thank you! NVFP4 using [https://github.com/NVIDIA/Model-Optimizer.git](https://github.com/NVIDIA/Model-Optimizer.git) \n\nhere is an example:\n\n    python hf_ptq.py \\\n        --pyt_ckpt_path google/gemma-3-27b-it \\\n        --qformat nvfp4 \\\n        --kv_cache_qformat fp8 \\\n        --export_fmt hf \\\n        --export_path ./gemma-3-27b-it-nvfp4-fp8kv \\\n        --calib_size 512 \\\n        --trust_remote_code",
              "score": 2,
              "created_utc": "2026-02-14 23:18:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5f6ffz",
                  "author": "DataGOGO",
                  "text": "Does this use the compressed tensors? Does it use modelopt¬†format? Fused scales?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:21:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5femea",
          "author": "Suitable-Still8379",
          "text": "neat, how does output quality compare to running the same base model in Q4\\_K\\_M via llama.cpp?",
          "score": 1,
          "created_utc": "2026-02-15 00:12:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r26ena",
      "title": "GLM 5 is out now.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u9z1nezwywig1.png",
      "author": "Cultural-Arugula-894",
      "created_utc": "2026-02-11 19:06:37",
      "score": 31,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r26ena/glm_5_is_out_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o51j942",
          "author": "Donmeusi",
          "text": "I¬¥m using it with Openclaw since a few hours. Kimi-k2.5 was good. GLM-5 is awesome. ",
          "score": 1,
          "created_utc": "2026-02-12 20:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5aun2e",
              "author": "Sociedelic",
              "text": "What hardware do you have?",
              "score": 1,
              "created_utc": "2026-02-14 06:36:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5c8fpj",
              "author": "Donmeusi",
              "text": "I have a Mac Mini 4 with 16GB. Its not possible to run this Model local so I use the Cloud Version with Ollama.",
              "score": 1,
              "created_utc": "2026-02-14 13:54:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52bbzt",
          "author": "IllFirefighter4079",
          "text": "Been using it in kilo code with great success. Can't wait for it to be available through Claude code on my Z.AI plan",
          "score": 1,
          "created_utc": "2026-02-12 22:33:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2xb8j",
      "title": "QLoRA - Fine Tuning a Model at Home",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/t0b4vtn473jg1.png",
      "author": "mac10190",
      "created_utc": "2026-02-12 16:03:01",
      "score": 29,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r2xb8j/qlora_fine_tuning_a_model_at_home/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o50rrp6",
          "author": "Ryanmonroe82",
          "text": "I use Easy Dataset to make the datasets and Transformer Lab or LlamaFactory to fine tune and have had great results.  I'd say go for it.",
          "score": 2,
          "created_utc": "2026-02-12 18:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50zgjv",
          "author": "mp3m4k3r",
          "text": "It might be worth tinkering with the chat templates as well, since at least i am able to make tool calls with nemotron a lot and it seems to adapt a bit via its template vs training.\n\nThis for example i run with tool_calls and seems to work well other than with continue.dev at the moment. https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF\n\nThe chat template this model comes with is kinda interestingly formatted",
          "score": 2,
          "created_utc": "2026-02-12 18:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51cdvj",
              "author": "mac10190",
              "text": "Good to know! I'll look into chat templates for Nemotron. I was using the unsloth Nemotron and for the life of me I couldn't get it to stop putting XML tags around the JSON tool calls which caused n8n to not recognize the call.",
              "score": 1,
              "created_utc": "2026-02-12 19:45:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o52rjuz",
              "author": "mac10190",
              "text": ">The chat template this model comes with is kinda interestingly formatted\n\nThis \\^\\^ it was exactly this. Went digging into the unsloth version of Nemotron and it turns out unsloth included a tool call template in their tokenizer.chat\\_template. You were spot on! Thank you kindly. Going to see about creating my own model file and building customized version of that model.\n\nThank you u/mp3m4k3r !!!",
              "score": 1,
              "created_utc": "2026-02-13 00:03:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o52t97u",
                  "author": "mp3m4k3r",
                  "text": "Sounds fun! Iirc I had issues with one of the templates but as i am using llama.cpp you can just have the chat templates in a file and launch it as a parameter (or in my case an environment variable)",
                  "score": 2,
                  "created_utc": "2026-02-13 00:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4x41x",
      "title": "Qwen3 8b-vl best local model for OCR?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4x41x/qwen3_8bvl_best_local_model_for_ocr/",
      "author": "BeginningPush9896",
      "created_utc": "2026-02-14 21:56:19",
      "score": 29,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "For all TLDR:\n\n Qwen3 8b-vl is the best in its weight class for recognizing formatted text (even better than Mistral 14b with OCR).\n\nFor others:\n\nHi everyone, this is my first post. I wanted to discuss my observations regarding LLMs with OCR capabilities.\n\nWhile developing a utility for automating data processing from documents, I needed to extract text from specific areas of documents. Initially, I thought about using OCR, like Tesseract, but I ran into the issue of having no control over the output. Essentially, I couldn't recognize the text and make corrections (for example, for surnames) in a single request.\n\nI decided to try Qwen3 8b-vl. It turned out to be very simple. The ability to add data to the system prompt for cross-referencing with the recognized text and making corrections on the fly proved to be an enormous killer feature. You can literally give it all the necessary data, the data format, and the required output format for its response. And you get a response in, say, a JSON format, which you can then easily convert into a dictionary (if we're talking about Python).\n\nI tried Mistral 14b, but I found that its text recognition on images is just terrible with the same settings and system prompt (compared to Qwen3 8b-vl). Smaller models are simply unusable. Since I'm sending single requests without saving context, I can load the entire model with a 4k token context and get a stable, fast response processed on my GPU.\n\nIf people who work on extracting text from documents using LLMs (visual text extraction) read this, I'd be happy to hear about your experiences.\n\nFor reference, my specs:\nR7 5800X\nRTX 3070 8GB\n32GB DDR4",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4x41x/qwen3_8bvl_best_local_model_for_ocr/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5f4nji",
          "author": "sinan_online",
          "text": " Not only do I agree , I also wrote an evaluation script, evaluated a bunch of models, and Qwen3 VL 8B came out on top, passing even Pixtral. I have a Medium article about it.",
          "score": 9,
          "created_utc": "2026-02-14 23:10:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fcqa4",
          "author": "beedunc",
          "text": "Shhhh. Don‚Äôt tell anybody. \n\nI‚Äôve gotten even the 4B/4q Ti do things that no other VL model can. There‚Äôs none better.",
          "score": 5,
          "created_utc": "2026-02-15 00:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ffrww",
              "author": "BeginningPush9896",
              "text": "What do you think could be the reason that large models perform worse at text recognition than Qwen3-VL?",
              "score": 3,
              "created_utc": "2026-02-15 00:19:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fk054",
                  "author": "beedunc",
                  "text": "I don‚Äôt know enough to comment on that, I just run ‚Äòem. Enjoy!",
                  "score": 5,
                  "created_utc": "2026-02-15 00:45:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ff6up",
          "author": "International-Lab944",
          "text": "I agree , it‚Äôs my go-to local VL/OCR model. When I was doing my evaluations I tested many models and Qwen 8B VL was the top model. I also tested Gemma 27B and Qwen 32B and few more and the small 8B came on top. When it‚Äôs not good enough I normally use Qwen3 235B VL through Openrouter.",
          "score": 4,
          "created_utc": "2026-02-15 00:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ffujr",
          "author": "greatwilt",
          "text": "ZwZ 8B has entered the chat.\n\nhttps://huggingface.co/inclusionAI/ZwZ-8B",
          "score": 3,
          "created_utc": "2026-02-15 00:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hpnnj",
              "author": "BeginningPush9896",
              "text": "I will test this model later and be sure to write about my impressions. Besides, it says it can automatically recognize areas. Currently, I'm doing the document cropping myself.",
              "score": 1,
              "created_utc": "2026-02-15 11:23:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5frrzj",
          "author": "boyobob55",
          "text": "Yes it kicks ass I used it in this project too!: https://github.com/boyobob/OdinsList",
          "score": 2,
          "created_utc": "2026-02-15 01:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fgqs3",
          "author": "nunodonato",
          "text": "4B here :)",
          "score": 1,
          "created_utc": "2026-02-15 00:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gbpjs",
          "author": "michael_p",
          "text": "Currently using pixtral 12b 4bit mlx and going to swap to qwen 3 8b-vl per your rec! I love the qwen family. Qwen3 32b mlx is my favorite local model. Feed it good prompts and configure it right and get (imo) opus quality thinking",
          "score": 1,
          "created_utc": "2026-02-15 03:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ijruo",
          "author": "gabriel0123m",
          "text": "For a customer I have made a similar use case like yours and tried multiple models and ways to run them (ollama, vllm) but with qwen 2.5 VL 7B and qwen 3 vl 4B i got the better quality / performance / price to run, but using multiple agents (think like a workflow) to handle some steps and errors with text extraction + using ocr as a fallback step / comparator and if i had evaluated it was possible to be used extract and use text directly (for txt or pdf that are computer generated...). From v2.5 to 3  I had seen better results for OCR in general with the 3 version, but only using a 4B model to make it follow some complex instructions (like following some rules to handle json structured output) wasn't always correct... So using right now the 8B version in prod! I am evaluating as a side GLM-ocr but I have not used as much as qwen vl, ...",
          "score": 1,
          "created_utc": "2026-02-15 14:52:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j4xa0",
          "author": "damirca",
          "text": "for HA it's not so good I'd say\n\nhttps://preview.redd.it/qf321rf3sojg1.png?width=1190&format=png&auto=webp&s=76ed1e8e195c4a44138448104cebb904215f39f9\n\n",
          "score": 1,
          "created_utc": "2026-02-15 16:38:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j73qq",
              "author": "BeginningPush9896",
              "text": "In my workflow, I work with drawings where I can manually crop all the fields I need, so I don't need to rely on searching for a field with specific text.\n\nI would be very interested to know if there are models that can match surnames with signature fields in documents automatically, without prior cropping.",
              "score": 1,
              "created_utc": "2026-02-15 16:48:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jegwf",
          "author": "l_Mr_Vader_l",
          "text": "If you want custom extraction from documents go with qwen 3 vl, but if you just wanna do really good ocr (proper page to markdown, tables and everything) there are smaller dedicated ones which are better even, than the 8b qwen3 vl\n\nMineru2.5 ocr\nLighton ocr\nPaddleOCR vl \n\n and these are just ~1B models just trained to do ocr, they're definitely better in all ways (size, speed and accuracy) \n\nGlm ocr also does custom extraction, but it's main ocr pipeline was pretty underwhelming",
          "score": 1,
          "created_utc": "2026-02-15 17:24:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r05z64",
      "title": "Claude Code vs OpenCode vs Cline vs Qwen Coder",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r05z64/claude_code_vs_opencode_vs_cline_vs_qwen_coder/",
      "author": "ShowTimezz",
      "created_utc": "2026-02-09 14:42:28",
      "score": 28,
      "num_comments": 25,
      "upvote_ratio": 0.94,
      "text": "Hi all, \n\n  \nI'm curious if anyone knows of or if anyone here did a deep dive with different agentic AI tooling based on concrete, measurable metrics? \n\nFor example, let's say someone gave the same prompt to the same coding model, across several different tools. \"Create a flappy bird clone\" to GLM-4.7-Flash running in Claude Code, GLM running in OpenCode, GLM running in Cline, etc. Some more important prompts and requests could be: \"Here's an attached CSV file with unstructured data. Extrapolate meaningful entries in the file, and create a dashboard using NextJS that displays that data in a meaningful format\".\n\nI did try to find something along these lines, but the vast majority of the results have just been slop, comparing features of the tooling without actually going into a deep dive on how the tooling performs for concrete, measurable instructions, or just displaying the \n\nSomething like this could allow us to actually have a tooling comparison instead of a model comparison per se. \n\nMy gut is telling me that this analysis will have a surprising conclusion, with clear winners on what model compared with what tool provides the best results. i.e GLM4.7-flash works best when paired with Claude Code and worst when paired with Cline based on <metric that was used to evaluate all model/tooling combos>. Or, gpt-oss works best when paired with OpenCode and worst with Qwen Coder.\n\n  \nI'm open to doing the research myself, but was wondering if anyone before bothered doing something like this before I spend the time. \n\n  \nThank you",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r05z64/claude_code_vs_opencode_vs_cline_vs_qwen_coder/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4g44g8",
          "author": "see_spot_ruminate",
          "text": "Don't forget our baguette shaped, cheese dealing partners with mistral-vibe. Maybe not what most people use, but tool calls work well with all the models I put with it.\n\nedit (a joke), can it really be called \"vibe\" coding if it is not from the vibe region in france?",
          "score": 11,
          "created_utc": "2026-02-09 15:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gifub",
              "author": "cmndr_spanky",
              "text": "And Cursor (which is pretty great in my experience).",
              "score": 1,
              "created_utc": "2026-02-09 16:43:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gcxyw",
          "author": "qubridInc",
          "text": "There‚Äôs no serious, metric-driven comparison of **agentic tools** yet and most write-ups compare features, not outcomes. Tooling behavior (planning loops, retries, permissions, context handling) often matters more than the model itself, so the same model can perform very differently across tools. Your intuition is likely right, but this gap is still largely unexplored.",
          "score": 5,
          "created_utc": "2026-02-09 16:17:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gi17m",
              "author": "cmndr_spanky",
              "text": "If that‚Äôs true, what a missed opportunity. Are you telling me with all the hapless and desperate content creators scrambling to remain relevant, with all the noise they are vomiting out to social media‚Ä¶ nobody has through to compare coding agent wrappers for the same model??\n\nWhat an idiotic missed opportunity. \n\nFWIW I agree with op that this is a critical thing. It‚Äôs literally the only reason Cursor (to pick my go-to tool) would even hope to compete or have any relevance.  They should (in theory) have some secret sauce that manages code base and context better than opus being used inside something free like VSCode + Roocode for example.",
              "score": 1,
              "created_utc": "2026-02-09 16:41:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fvpnm",
          "author": "calben99",
          "text": "This is a really interesting research idea! I haven't seen anyone do a systematic comparison like this across the same model with different tooling. I'd be curious to see the results if you end up running the tests. You might also want to track things like token usage and execution time as secondary metrics. Good luck with the research!",
          "score": 2,
          "created_utc": "2026-02-09 14:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g984e",
          "author": "Deep_Traffic_7873",
          "text": "I have a private benchmark like this, but it don't do it on full projects, just questions i can measure",
          "score": 2,
          "created_utc": "2026-02-09 15:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gz0q2",
          "author": "i-eat-kittens",
          "text": "Qwen Coder is their model. The tool name doesn't have an 'r'.",
          "score": 2,
          "created_utc": "2026-02-09 18:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i5iub",
          "author": "Crafty-Diver-6948",
          "text": "you are forgetting pi.\n\npi is better than all of them.",
          "score": 2,
          "created_utc": "2026-02-09 21:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fw0sy",
          "author": "Total-Context64",
          "text": "I'd love to have [CLIO](https://github.com/SyntheticAutonomicMind/CLIO) grouped into this if that's interesting to you, it's a small project with very few users but I'd still like to know how it stacks up.  No worries if it isn't interesting enough, figured it never hurts to try.  :D",
          "score": 1,
          "created_utc": "2026-02-09 14:54:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fy7kd",
          "author": "xLRGx",
          "text": "Probably not. Most everyone prefers the slight intelligence boost of the frontier models to power their coding agents and then muscle memory + the UX just brings us to pair them with each other. Most people using Claude code are just using opus. \n\nIt‚Äôd be valuable research but I‚Äôm not sure how useful it would really be in the AI landscape. It would deprecate quickly due to the pace of change the big 3 are shipping new models. By the time you finish your first rigorous benchmark openAI, Google, and Anthropic will probably have a new model ready to be launched. \n\nI would add a caveat, I think you‚Äôre burying the lead a little bit or maybe you haven‚Äôt considered it - the memory costs of AI and AI token generation of enterprise and consumers is going to likely 100x in 2026 compared to just last year. That‚Äôs a real problem and finding efficiency gains like this might matter more than you think. Right it‚Äôs not just humans increasing the cost of AI now it‚Äôs Agents who can out produce humans in terms of pure token generation with workflows. It could honestly be 1000x in the aggregate. The real question becomes how do we train the Agents to operate in the ‚Äúefficient frontier‚Äù of 2026 and in the future. And it seems like the big 3 are aware that‚Äôs what needs to happen it‚Äôs just that they CANT STOP SCALING even if it‚Äôs looking flat now.",
          "score": 1,
          "created_utc": "2026-02-09 15:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g9gxm",
          "author": "Corosus",
          "text": "I'm confused why this isn't already being done by other people publicly, thats essentially what I do for when I'm trying to figure out good agent/model combos that aren't broken out of the box or just unusably slow.\n\n> \"Here's an attached CSV file with unstructured data. Extrapolate meaningful entries in the file, and create a dashboard using NextJS that displays that data in a meaningful format\".\n\nLiterally what I've been doing too but with a dump of discord message payload json objects.\n\nIf we could have a matrix comparison with agents on the top, and models on the left, where you can get a gist of the thuroughness of their solution + being able to quickly browse the results it'd be amazing.\n\nI guess where it gets a bit more complicated is what quants do you use, I guess try popular ones, ones at various VRAM/RAM tiers. And then on top of that the params you feed into llama.cpp etc. I guess again cherry picking common sensible usage might help there.\n\nI guess like others have said the info would deprecate quickly, the space moves so fast theres not much time for long standing data, so maybe people don't find it worth doing.",
          "score": 1,
          "created_utc": "2026-02-09 16:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gjusi",
              "author": "cmndr_spanky",
              "text": "Slapping a dashboard ontop of a data structure is such a pedestrian task these days, I don‚Äôt think this will be an affective test. \n\nThe secret sauce of these different agentic wrappers is in how efficiently they use context through multiple loops of reasoning, and they index and manage / read a large code base, and the ‚Äúbag of tricks‚Äù / bonus features like rules, skills, MCP access, built-in AI controllable browser etc.\n\nYou‚Äôd be better off finding a sizable code base on GitHub, prompt the agent to understand it well enough to implement feature x and validate its working correctly.  And again, avoid simple ‚Äúdashboard on data‚Äù use cases.. it‚Äôs too common, too small and too easy",
              "score": 2,
              "created_utc": "2026-02-09 16:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h1f78",
                  "author": "Corosus",
                  "text": "Agreed yeah, I guess a couple reasons why I am using the test case I mentioned is because \n\n1. It was part of something I already had claude setup for me (viewing a feed of as many discord channels I want on a single page you can look at without having to navigate around to each channel)\n\n2. It doesn't take a very long time to find out if the results of the test case I am trying is total garbage or not, even that simple task it can range from 5 minutes to 30 minutes depending on context size/model distribution in ram/vram. So in a way its main purpose is an initial filter to see if it's worth testing further at all\n\nMy next step after that was basically what you said, got a lot of work that involves making use of a specific API that isn't used widely and seeing if it can implement it correctly.",
                  "score": 1,
                  "created_utc": "2026-02-09 18:13:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gfovi",
          "author": "MrMisterShin",
          "text": "Qwen3-coder-next did something similar in there tech report (look for the scaffolding table).  https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf?spm=a2ty_o06.30285417.0.0.3bdec921sAzKJK&file=qwen3_coder_next_tech_report.pdf",
          "score": 1,
          "created_utc": "2026-02-09 16:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gj0zz",
              "author": "cmndr_spanky",
              "text": "You completely missed the point. He wants to compare the quality of different agentic wrappers for the same model.  Nobody needs another dumb benchmark comparing the LLMs",
              "score": 1,
              "created_utc": "2026-02-09 16:46:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gkv6m",
                  "author": "MrMisterShin",
                  "text": "That‚Äôs in the paper if you READ it. It literally measures the same model across various scaffolding. It also measure many models against them also.\n\nThe only thing it doesn‚Äôt do is explicitly detail which scaffolding achieved the highest marks for the models.\n\nhttps://preview.redd.it/tjhxjiv81iig1.jpeg?width=1027&format=pjpg&auto=webp&s=7b2a4e50dff2f2821fa86fad92f34946fa8c603e",
                  "score": 3,
                  "created_utc": "2026-02-09 16:55:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l5tlh",
          "author": "Charming_Support726",
          "text": "My Opinion is: (There are rarely real benchmarks out there)\n\n1 . Overall it simply doesn't matter at large. They are human interfaces.\n\n2. Harness and Scaffold don't have large impact on the model - But depending on the infrastructure they can make models perform only worse, never better.\n\n3. Many of these scaffolds have got terrible prompts (T - E - R - R - I - B - L - E). E.g look at the discussion in Opencode about the codex prompt - and their new codex prompt when OpenAI entered the game. To many contradictionary hints, restriction, example and so on. And far to long.\n\n4. Either prompts or tools will degrade performance.\n\nIf that is more or less o.k. you could switch scaffold without notice. I am using multiple of them mostly Zed, Opencode and Codex ( sometimes Antigravity ). I manage the access to the models using CLIProxy - so every harness gets the same provider chain. I feel no difference .",
          "score": 1,
          "created_utc": "2026-02-10 09:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lyscw",
          "author": "techlatest_net",
          "text": "No comprehensive head-to-head benchmarks exist yet comparing those exact tool+model combos on concrete tasks like Flappy Bird or CSV‚ÜíNextJS dashboard. \n\n**Gut ranking from scattered tests**:\n- **Claude Code + Sonnet 4.5/Opus**: Still king for complex reasoning+autonomy. Best at \"think ‚Üí plan ‚Üí code ‚Üí test ‚Üí fix\" loops on unfamiliar stacks. SWE-bench leader.\n- **OpenCode + GLM-4.7**: Closest open-source contender. 73% SWE-bench (vs Claude's 72%) + zero lock-in. Great for multi-model routing.\n- **Cline**: Solid but tool-heavy‚Äîshines on file ops, weaker on pure generation. Agent scaffolding overhead kills simple prompts.\n- **Qwen Coder**: Fastest raw speed, surprisingly good UI/dashboard code, but needs tight scaffolding for full autonomy.\n\n**Surprise winner**: OpenCode+GLM-4.7 often beats Claude Code on price/performance for production pipelines. The tooling matters more than raw model for agentic flows.\n\n**Your test methodology is perfect**‚Äîsame prompt/model across tools reveals the orchestration gap. I'd run:\n1. Flappy Bird (game logic + Canvas)\n2. CSV‚ÜíRecharts dashboard (data parsing + React)\n3. GitHub issue reproduction (multi-file edit)\n\nTrack: **task completion rate, edit iterations, token usage, wall-clock time**. \n\nDo it‚Äîcommunity needs this data desperately. My bet: Claude Code edges out, but OpenCode closes gap 80%.",
          "score": 1,
          "created_utc": "2026-02-10 13:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nl1gq",
          "author": "Lonely-Ad-3123",
          "text": "this is a really interesting question and I've been wondering the same thing about agentic tooling benchmarks. The reality is most comparisons are just feature checklists without actually measuring output quality or success rates on real tasks. If you're going to do this research yourself, here's how I'd approach it: 1.\n\n\nPick 3-5 concrete tasks with different complexity levels (the flappy bird clone is good for basic generation, the CSV dashboard is good for multi-file coordination, maybe add something like refactoring a legacy codebase or fixing a subtle bug across multiple repos) 2. Define your success metrics upfront. Not just does it run but things like: lines of code changed, build errors introduced, how many iterations needed, time to completion, whether it actually solves the stated problem vs going off track 3.\n\n\nFor the validation step, you might want to look at something like Zencoder Zenflow since it has spec-driven workflows with built-in verification loops that can measure drift between requirements and implementation. That could give you actual quantifiable data on how far each tool/model combo strays from the original prompt 4. Document everything in a spreadsheet with timestamps and iteration counts.\n\n\nScreenshots help too for when things go sideways 5. Run each combo at least 3 times since LLMs can be inconsistent The hardest part is gonna be keeping variables controlled. Same prompt wording, same model temp settings, same time of day even (API performance varies).\n\n\nBut yeah if you do this please share the results because the community desperately needs this kinda analysis instead of just vibes-based recommendations.",
          "score": 1,
          "created_utc": "2026-02-10 18:06:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gw0te",
          "author": "Ok_Rough5794",
          "text": "There are evals and benchmarks and youtubers doing this.. all over the place",
          "score": 0,
          "created_utc": "2026-02-09 17:48:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4oava",
      "title": "Hardware constraints and the 10B MoE Era: Where Minimax M2.5 fits in",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r4oava/hardware_constraints_and_the_10b_moe_era_where/",
      "author": "Fragrant_Occasion276",
      "created_utc": "2026-02-14 16:03:02",
      "score": 25,
      "num_comments": 28,
      "upvote_ratio": 0.86,
      "text": "We need to stop pretending that 400B+ models are the future of local-first or sustainable AI. The compute shortage is real, and the \"brute force\" era is dying. I've been looking at the Minimax M2.5 architecture - it's a 10B active parameter model that's somehow hitting 80.2% on SWE-Bench Verified. That is SOTA territory for models five times its size. This is the Real World Coworker we've been waiting for: something that costs $1 for an hour of intensive work. If you read their RL technical blog, it's clear they're prioritizing tool-use and search (76.3% on BrowseComp) over just being a \"chatty\" bot. For those of us building real systems, the efficiency of Minimax is a far more interesting technical achievement than just adding more weights to a bloated transformer.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r4oava/hardware_constraints_and_the_10b_moe_era_where/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o5d0ni9",
          "author": "Panometric",
          "text": "At what quantization?",
          "score": 7,
          "created_utc": "2026-02-14 16:28:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e3w93",
              "author": "DataGOGO",
              "text": "FP8, it degrades rapidly at anything below that¬†",
              "score": 7,
              "created_utc": "2026-02-14 19:46:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ektea",
                  "author": "SpicyWangz",
                  "text": "That‚Äôs good to know. I hope we can get some high quality quants soon, but it might just be that they‚Äôre maximizing the most out of every parameter and there‚Äôs no way to achieve a good quant on this model",
                  "score": 2,
                  "created_utc": "2026-02-14 21:18:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d11ol",
          "author": "Tema_Art_7777",
          "text": "\nFor those building real products, why are you so focused on cost of tokens? Frontier models are amazing, and they really do not cost much. If you are worried $10 vs $1 for an hour of work, what is your business model that is so sensitive to that? Genuinely want to know.",
          "score": 6,
          "created_utc": "2026-02-14 16:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d68gi",
              "author": "sooodooo",
              "text": "I‚Äôll point out the ‚Äúsustainable‚Äù part first. We know the big providers all burn through investor money by providing AI at the current price. It‚Äôs 10$ now, but without more efficient models that‚Äôs not sustainable and investors want to see some honey at some point.\n\nSecond I don‚Äôt know what metric ‚Äú$10 per hour work‚Äù is, but I‚Äôd rather pay $10 and have the same work done in a minute. I don‚Äôt understand how price could not be THE deciding factor in the age of cloud computing, if I can get the same work done at 1/10 the of the price then you can scale 10x bigger and faster.",
              "score": 9,
              "created_utc": "2026-02-14 16:56:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dx06y",
                  "author": "Karyo_Ten",
                  "text": "Companies are often bottlenecked by humans, within or outside, say verification, quality control, certification, a signature.\n\nProducing meh output fast is not interesting because work will be sent back.\n\nIt's interesting for iteration/prototyping but a model that does well from the get go might end up costing less even if bigger / more expensive per token",
                  "score": 1,
                  "created_utc": "2026-02-14 19:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ei0r9",
              "author": "evilbarron2",
              "text": "Are you building a product that‚Äôs only good for business use cases? That‚Äôs fine, but it isn‚Äôt where the majority of ai stuff is gonna happen. Instead of focusing on what the user is or isn‚Äôt doing, just make clear what the requirements/ costs of *your* product is. Users aren‚Äôt gonna use your product the way you think they will.",
              "score": 1,
              "created_utc": "2026-02-14 21:03:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eu8mi",
                  "author": "Tema_Art_7777",
                  "text": "It is a mix, both business and personal use cases like home automation. to expand further, $10 for mm tokens don‚Äôt move the needle for business and I don‚Äôt need mm of tokens for personal stuff‚Ä¶. So still not sure why everyone is hyped up about 50cents for mm tokens. So I must be missing a use-case people have‚Ä¶",
                  "score": 1,
                  "created_utc": "2026-02-14 22:10:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5fi1kd",
              "author": "former_farmer",
              "text": "You are on LocalLLM my brother. Not on \"consume LLMs on the cloud for cheap\" subreddit.",
              "score": 1,
              "created_utc": "2026-02-15 00:33:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fkh5q",
                  "author": "Tema_Art_7777",
                  "text": "Yes except the OP‚Äôs post said: ‚ÄúThis is the Real World Coworker we've been waiting for: something that costs $1 for an hour of intensive work.‚Äù. So I‚Äôm questioning the use-cases for the intensive focus on price.",
                  "score": 1,
                  "created_utc": "2026-02-15 00:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ew29b",
          "author": "Dependent-Example930",
          "text": "There is a string of these style posts. Feels spam/fake pushing minimax 2.5",
          "score": 3,
          "created_utc": "2026-02-14 22:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d0btt",
          "author": "Pixer---",
          "text": "What local system do you have ?",
          "score": 2,
          "created_utc": "2026-02-14 16:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e5n6a",
              "author": "starkruzr",
              "text": "unfortunately chances are good OP is just promoting.",
              "score": 1,
              "created_utc": "2026-02-14 19:55:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e0bjy",
          "author": "No-Leopard7644",
          "text": "üíØ My focus and expertise is local ai with open source models and tools - for enterprises as well as personal use. That being said, the element of tokenomics, sovereign data and model freedom is paramount for highly regulated industries. I spend my day job straddling cloud as well as architecting sovereign local AI infrastructure with the ability to burst to the cloud.",
          "score": 1,
          "created_utc": "2026-02-14 19:27:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gc6bq",
          "author": "Euphoric_Emotion5397",
          "text": "  \nSo, in actual practise, my local setup that can get me the same output every single time is temp <= 0.3, tool use, searxng, playwright, local context memory to get real world updates in multistage workflow with different prompts at different stage. \n\nWe just need a good enough multimodal LLM (using qwen 3 VL 30b) and a good enough embedding model (using Gemma embedding) to understand what we are feeding it and then use the available data (we give them) to come out with an analysis in json output format.\n\nThen i feed the output to Gemini Pro to verify and it's rated highly against their own.\n\nYou need to have a workflow usecase that you use frequently which you can vibecode first.\n\nFor other things generic , then it's better to just go online and use the frontier models to chat and get your answer.",
          "score": 1,
          "created_utc": "2026-02-15 03:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h18yx",
          "author": "Exciting_Square_4593",
          "text": "The obsession with \"more parameters = more better\" is just cope for companies that can't figure out efficient architecture. M2.5 hitting 80.2% on SWE-Bench Verified with only 10B active parameters is a slap in the face to every bloated 400B model out there. If you're still paying for compute-heavy giants that lag every time you try to run a simple agentic loop, you're literally just subsidizing bad engineering.",
          "score": 1,
          "created_utc": "2026-02-15 07:28:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h8696",
          "author": "Monty756",
          "text": "$1 an hour for a real-world coworker that doesn't hallucinate like a high schooler on caffeine. Finally, a model that respects my time and my wallet.",
          "score": 1,
          "created_utc": "2026-02-15 08:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h8aym",
          "author": "Critical-Raccoon-926",
          "text": "It's the difference between a junior dev who yaps and a senior who just pushes the fix.",
          "score": 1,
          "created_utc": "2026-02-15 08:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hi7lw",
          "author": "Previous-Shop6033",
          "text": "We are hitting the physical limits of H100 clusters and people still think scaling is the only way up. M2.5 is proof that you can reach SOTA by being smart about tool-use and search (that 76.3% on BrowseComp is no joke) rather than just throwing more VRAM at the problem.",
          "score": 1,
          "created_utc": "2026-02-15 10:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i0nlw",
          "author": "Nervous-Dig-6383",
          "text": "Imagine being a \"frontier\" model that's 5x the size and still losing to a 10B active MoE on actual engineering tasks. Embarrassing.",
          "score": 1,
          "created_utc": "2026-02-15 12:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i63m8",
          "author": "BetterInternet63",
          "text": "Finally, someone mentions the BrowseComp score. Everyone fixates on SWE-Bench, but if your \"coworker\" can't navigate documentation or search for a specific library error without getting lost, it's useless. M2.5 actually feels like it knows how to use a browser.",
          "score": 1,
          "created_utc": "2026-02-15 13:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i9ckk",
          "author": "Greedy_Sandwich_1839",
          "text": "100 TPS at 1/10th the cost of Claude. If you're not using this for your CI/CD pipelines yet, you're basically just lighting money on fire.",
          "score": 1,
          "created_utc": "2026-02-15 13:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ic5f6",
          "author": "MarinaCER87",
          "text": "Tired of these \"chatty\" bots that spend 500 tokens apologizing before they even look at the code. M2.5's focus on grounded tool-use over \"personality\" is exactly what the industry needs right now. We don't need a friend; we need an engineer.",
          "score": 1,
          "created_utc": "2026-02-15 14:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5idwlo",
          "author": "n521n",
          "text": "Everyone is mourning the \"compute shortage\" while MiniMax is just out here building the most sustainable architecture of the 10B MoE era. This is the future, whether the hardware snobs like it or not.",
          "score": 1,
          "created_utc": "2026-02-15 14:19:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1g5m0",
      "title": "Best model to run local on a \"normal pc\"",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r1g5m0/best_model_to_run_local_on_a_normal_pc/",
      "author": "MaxsBosch",
      "created_utc": "2026-02-10 22:59:11",
      "score": 24,
      "num_comments": 19,
      "upvote_ratio": 0.86,
      "text": "Hey guys, I'm pretty sure that this post has been made multiple times by other people. But with the fast pace of LLM's right now I'm getting overwhelmed in all the options. Im getting back into coding and I want to experiment with using AI in my workflow.\n\nI do have the gemini pro subscription (I get it for free from work) but I notice that it lacks alot in coding\n\nI recently bought a new pc with a Ryzen 7 9700X, rxt 5070 and 32gb of ram. I'm fascinated by the models from claude but I'm not willing to pay 200 euros a month right now. I also read about other models like kimi k2.5 and GLM. But as I said I'm overwhelmed with how many options there are.\n\nWhat models would you guys recommend for me to run locally right now? Or will I get worse results than Gemini when running locally?\n\nI am new to all of this, but I'm sure I will be able to figure out how to set everything up correcty as soon as I know whichone I should go for. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r1g5m0/best_model_to_run_local_on_a_normal_pc/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4pd4xc",
          "author": "SimilarWarthog8393",
          "text": "You'll only come close to Gemini results when running huge models like Kimi K2.5 (1T parameters) but your PC can't handle models bigger than maybe 30B parameters (Qwen3 Coder 30B A3B). Reduce your expectations and enjoy the privacy you gain from running locally~",
          "score": 11,
          "created_utc": "2026-02-10 23:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pd653",
          "author": "seedsofchaos",
          "text": "My suggestion?  Download LM Studio.  Join huggingface.co.  Search this sub-Reddit for your specific use case (ex. Python coding).  Download those models off huggingface.co.  Try those models with your use cases and see what happens and which you like better.  As someone who just started down the same path on Friday morning, that'd be my suggestion.  Now that I'm that far, I'm looking into what options I have to train a RAG for my particular use cases but haven't figured those steps out yet..\n\nYour rig is about the same specs as mine so it'll do just fine.  ",
          "score": 15,
          "created_utc": "2026-02-10 23:09:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q0ut1",
              "author": "low_v2r",
              "text": "I'm in somewhat of the same boat, although I think I will give llamma.cpp a shot on my strix machine (128Gb unified memory).  \n\nStill have to learn how to do the RAG part - likely will just ask gemini LOL",
              "score": 2,
              "created_utc": "2026-02-11 01:25:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pd2qf",
          "author": "ai_hedge_fund",
          "text": "You will absolutely get worse results than Gemini - but that should not stop you\n\nThe first model should be something small so you can get all your other little pieces in place to be able to run it successfully\n\nMaybe a small Granite gguf formatted model\n\nFrom there you can start pushing hardware and testing capabilities",
          "score": 6,
          "created_utc": "2026-02-10 23:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q925w",
          "author": "ScuffedBalata",
          "text": "You won't get anything anywhere close to Gemini (not even in the ballpark) on such a machine. \n\nKimi 2.5 requires a TERABYTE of high-speed unified memory.  You can get a quantized (shrunk) version of it to run on a ultra-high-spec Mac Studio M4 Ultra with 512GB of unified RAM, but that's a $15k machine.\n\nThe closest normal human model that's kinda/sorta in the ballpark of Gemini might be the new **Qwen3-Coder-Next-80B** that came out last week.  It requires 60-80GB of VRAM.  I think your RTX5070 has 12GB.  The system ram on AMD/Intel systems is too slow for AI work, so it's just not going to work on anything short of a $4k machine.  My friend has a dual-3090 GPU setup and it's basically minimum tolerable speed on that.\n\nI mean if you had 64GB of system RAM, it might RUN, but it'll be AWFUL (like waiting 5 minutes for a simple prompt).\n\nI have it running on a Mac with a M1 Max processor and 64GB of unified memory.  That's the minimum-minimum spec, I have to keep the system stripped down and running nothing but the LLM to have enough RAM, but it works.\n\nFor the most capable local \"coder\" that will work in your system as a hobby/plaything would be maybe Qwen3-Coder 30B with heavy CPU offload.\n\nIt's pretty easy to try.  Download LMStudio, it's point and click.  Then search for \"coder\" models and stay below 30b.  It'll bitch and moan about even that big a model (that's a 20gb download I think), but it will probably work if you slide the offload slider a bunch.\n\nI think as fast a system as you have, it'll run... slow but maybe tolerably well.  But won't sniff gemini for quality and capability.\n\nIf you want something quick, stick to like a 8b model on your system that will fit fully in VRAM with decent context.  Problem is those models aren't that smart and will be a bit forgetful and not as skilled at error-free code.",
          "score": 3,
          "created_utc": "2026-02-11 02:14:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4peyck",
          "author": "aPenologist",
          "text": "Ask Gemini. With a 5070ti I can run a 20B model on vram v nicely. If i was looking for a new model, qwen would be in the equation but id definitely want to make sure whether the code is clean after some 'news' I brushed over today. \n\nFor a first try, just follow instructions to the letter, it will ask all the relevant questions.",
          "score": 2,
          "created_utc": "2026-02-10 23:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pm48i",
              "author": "snozzberrypatch",
              "text": "What 'news' did you brush over today?",
              "score": 1,
              "created_utc": "2026-02-10 23:59:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4po6gh",
                  "author": "aPenologist",
                  "text": "I put it in quotes because it was the title of a reddit post, which i cant now find, and I didnt read beyond the possibly anti-hype bait title. Up-to-date Due dilligence is always a good idea though regardless.",
                  "score": 1,
                  "created_utc": "2026-02-11 00:11:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pcs0f",
          "author": "tomopenworldai",
          "text": "You'll get worse results running locally than with Gemini. But Gemini's pretty good for coding in my opinion.",
          "score": 3,
          "created_utc": "2026-02-10 23:07:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pe1is",
          "author": "rerorerox42",
          "text": "Ministral-3:8b is one that i prefer for non coding, also as someone else mentioned Qwen 3.\n\nOtherwise there are other older coding models. Unsure how for instance a devstrall small or similar will fit as a coding agen in an IDE or something.",
          "score": 1,
          "created_utc": "2026-02-10 23:14:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ph7e1",
          "author": "former_farmer",
          "text": "Cursor has 20 usd per month plans. Sometimes you get free usage of composer model after you run out. There are also other providers of qwen or deepseek that cost little.\n\nRunning locally you will suffer a lot of quality degradation probably. You will be able to run models from 1B to 20B, maybe 30B at very slow speed. Anything above 10B might be slow. Like.. 5% the speed you are used to.\n\nThese things demand a lot of infrastructure.",
          "score": 1,
          "created_utc": "2026-02-10 23:32:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4punks",
          "author": "boyobob55",
          "text": "Download Opencode, they have a handful of free really capable models right now",
          "score": 1,
          "created_utc": "2026-02-11 00:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rbmvw",
          "author": "andrew-ooo",
          "text": "For coding on your 5070 (12GB VRAM), Qwen 2.5 Coder 14B at Q4_K_M quantization is probably your sweet spot ‚Äî fast inference and genuinely useful for code completion and refactoring. If you want to push it, you can offload some layers to RAM and run the 32B version, but expect slower speeds. Start with Ollama or LM Studio for easy setup; both handle model downloads and GPU offloading automatically.",
          "score": 1,
          "created_utc": "2026-02-11 06:48:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4s12dy",
          "author": "Hefty-Significance91",
          "text": "Any llama model",
          "score": 1,
          "created_utc": "2026-02-11 10:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4suvck",
          "author": "eXl5eQ",
          "text": "I think Qwen 30B-A3B is the best you can get for 32GB. At least that's what I'm playing with.",
          "score": 1,
          "created_utc": "2026-02-11 14:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tk8zt",
          "author": "Express_Ad3626",
          "text": "Have you tried using AntiGravity, Gemini CLI, or the Gemini VScode extension? These (I would assume) will have a slightly different LoRA for coding tasks vs standard Gemini App. I would take a swing at this and see the results you get. \n\nI dont think you're going to be able to run Nemotron or Qwen Code on the 5070. You might be able to run GPT-OSS 20B but might spill over. It's no where close to other models, but I like for simple tasks, that provide concise responses. ",
          "score": 1,
          "created_utc": "2026-02-11 16:15:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qj7kb",
          "author": "No_Foot_7465",
          "text": "Hello guys !, tengo una pregunta similar al del post , que modelos enfocados a codigo puedo correr de manera local ?\n\nexpec:  \nRyzen 5 5600G , Nvidia 3060 12Gb y 16Gb de ram",
          "score": 0,
          "created_utc": "2026-02-11 03:16:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rlt7j",
              "author": "DertekAn",
              "text": "In english nowwwwwwww",
              "score": 1,
              "created_utc": "2026-02-11 08:23:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3lx38",
      "title": "Google Releases Conductor",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r3lx38/google_releases_conductor/",
      "author": "techlatest_net",
      "created_utc": "2026-02-13 10:39:03",
      "score": 24,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "# Google Releases Conductor: a context-driven Gemini CLI extension that stores knowledge as Markdown and orchestrates agentic workflows\n\nLink: [https://github.com/gemini-cli-extensions/conductor](https://github.com/gemini-cli-extensions/conductor)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r3lx38/google_releases_conductor/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o55c7pi",
          "author": "Super-Jackfruit8309",
          "text": "about time really, having to do this manually made no sense and often got in the way of work. Looking forward to trying it out.",
          "score": 3,
          "created_utc": "2026-02-13 11:40:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b7x86",
          "author": "trentard",
          "text": "conductor we have a problem\nconductor we have a problem",
          "score": 1,
          "created_utc": "2026-02-14 08:41:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}