{
  "metadata": {
    "last_updated": "2026-02-28 16:44:10",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 285,
    "file_size_bytes": 341739
  },
  "items": [
    {
      "id": "1rfi2aq",
      "title": "Self Hosted LLM Leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/pbwec2fpsvlg1.png",
      "author": "Weves11",
      "created_utc": "2026-02-26 18:25:32",
      "score": 555,
      "num_comments": 89,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rfi2aq/self_hosted_llm_leaderboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7k60fx",
          "author": "AC1colossus",
          "text": "Minimax?",
          "score": 40,
          "created_utc": "2026-02-26 18:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k8bzh",
              "author": "BarisSayit",
              "text": "It should be in S tier no cap.",
              "score": 21,
              "created_utc": "2026-02-26 18:45:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kavn7",
              "author": "Koalababies",
              "text": "My immediate thought. It's been a beast",
              "score": 3,
              "created_utc": "2026-02-26 18:57:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kfn1f",
              "author": "BitXorBit",
              "text": "So far the most enjoyable model running locally",
              "score": 3,
              "created_utc": "2026-02-26 19:19:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kbj3y",
              "author": "Weves11",
              "text": "added (to S tier), thanks for calling out!",
              "score": 3,
              "created_utc": "2026-02-26 19:00:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kcxc8",
          "author": "LightBrightLeftRight",
          "text": "I mean the new Qwen 3.5 models should easily be on this, the 27b dense and 122b moe both make a pretty good case for A-tier, B-tier at minimum. Particularly since they have vision, which is great for a lot of homelab/small business stuff.",
          "score": 31,
          "created_utc": "2026-02-26 19:06:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kfxal",
              "author": "Prudent-Ad4509",
              "text": "I have not tested 122b, but 27b is a beast. ",
              "score": 7,
              "created_utc": "2026-02-26 19:21:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7khgvz",
                  "author": "LightBrightLeftRight",
                  "text": "I've worked with both and surprisingly, not super different for me. I've seen better detail to world knowledge with 122b but not much in terms of reasoning or coding.\n\nI think I'll still stick with the 122b, but that's mostly just because I've got the headroom for it.",
                  "score": 5,
                  "created_utc": "2026-02-26 19:28:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7o6u5n",
                  "author": "FatheredPuma81",
                  "text": "27B is only like 25% faster than 122B for me so I don't bother using it but 122B is a really nice model but all 3 models hallucinate a lot.",
                  "score": 2,
                  "created_utc": "2026-02-27 09:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7kha3o",
          "author": "Gallardo994",
          "text": "No qwen3-coder-next in a coding leaderboard is a crime¬†",
          "score": 18,
          "created_utc": "2026-02-26 19:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kn8bb",
          "author": "ScuffedBalata",
          "text": "Why isn't Qwen3 on here?\n\nThe single best model I've ever used that works on \"normal people hardware\" is the Qwen3-Next and Qwen3-Coder-Next (both at 80B).",
          "score": 14,
          "created_utc": "2026-02-26 19:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l7nk2",
              "author": "robotcannon",
              "text": "Agree!!\n\nqwen3-vl is also fantastic (though it seems to run a tiny bit better at q8_0 for vision stuff)",
              "score": 4,
              "created_utc": "2026-02-26 21:33:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kilk4",
          "author": "kidousenshigundam",
          "text": "What hardware do I need to run S tier?",
          "score": 10,
          "created_utc": "2026-02-26 19:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kutnz",
              "author": "Witty_Mycologist_995",
              "text": "Probably a ton of Mac studios",
              "score": 4,
              "created_utc": "2026-02-26 20:32:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7liq7j",
              "author": "Altair12311",
              "text": "1 single mini-pc with the Ryzen AI Max+ 395 and 128GB of Ram for the MiniMax-2.5 (is my setup)",
              "score": 7,
              "created_utc": "2026-02-26 22:27:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lwugx",
                  "author": "dadavildy",
                  "text": "For coding, how is it on that machine?",
                  "score": 2,
                  "created_utc": "2026-02-26 23:42:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pxvbi",
                  "author": "LimiDrain",
                  "text": "What's even AI Max? People mention it too often. It's not about GPUs anymore?",
                  "score": 2,
                  "created_utc": "2026-02-27 16:01:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pt7dt",
                  "author": "StockboyRickyRicardo",
                  "text": "At 2bit quant?",
                  "score": 1,
                  "created_utc": "2026-02-27 15:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vsjqv",
                  "author": "cafemachiavelli",
                  "text": "Oo, tempting. How's the performance of the system?¬†",
                  "score": 1,
                  "created_utc": "2026-02-28 14:21:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7m623q",
                  "author": "kidousenshigundam",
                  "text": "No way. I have a Mac Studio ultra 256 GB and I can‚Äôt use Kimi K2",
                  "score": 1,
                  "created_utc": "2026-02-27 00:33:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7sa214",
              "author": "thatcoolredditor",
              "text": "Mac Studio 512gb for $10k",
              "score": 2,
              "created_utc": "2026-02-27 22:54:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7njnkz",
              "author": "ComfortablePlenty513",
              "text": "the $16k M5 mac studios coming out next week lol",
              "score": 0,
              "created_utc": "2026-02-27 05:40:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lkiy2",
          "author": "siegevjorn",
          "text": "Hey, want to elaborate on the methodology?",
          "score": 8,
          "created_utc": "2026-02-26 22:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ojdpl",
          "author": "LetterFair6479",
          "text": "Self hosted? Dont make me laugh. Only D is feasible , all other normal person who cant spend 5k+ cannot selfhost with any recent llm.",
          "score": 6,
          "created_utc": "2026-02-27 10:59:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pq1r2",
              "author": "richtopia",
              "text": "On the website there is a button for model size. I believe \"Small\" limits to 30B which is right at the limit of my gaming PC. \n\nAccording to this tier list, GPT-oss 20B is the highest in the \"B\" tier.",
              "score": 1,
              "created_utc": "2026-02-27 15:23:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7meqvh",
          "author": "Count_Rugens_Finger",
          "text": "aaaand the best model I can actually run on my PC is C tier.  yay\n\nEdit: oh wait gpt-oss 20b is in B tier.  That's... interesting.\n\nAnd Qwen3-30B-A3B is in D tier?  huh?",
          "score": 6,
          "created_utc": "2026-02-27 01:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k5s55",
          "author": "Egoz3ntrum",
          "text": "Devstral-2-123B is missing there in the Coding section.",
          "score": 4,
          "created_utc": "2026-02-26 18:34:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mma7l",
          "author": "MahDowSeal",
          "text": "Sorry if the question might be stupid, but for anyone who tried the S tier models. How comparable are they to the cloud models such as claude or chatGPT?",
          "score": 4,
          "created_utc": "2026-02-27 02:06:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mzj2r",
              "author": "RG_Fusion",
              "text": "I'm probably not the best person to ask as I've only been playing around with Qwen3.5-397b-17b for a little bit, but I was absolutely blown away by its internal reasoning. I don't have enough to make a definitive assessment, but I can certainly see how it could be competitive against the frontier models.",
              "score": 2,
              "created_utc": "2026-02-27 03:24:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7nszr2",
                  "author": "sinebubble",
                  "text": "You‚Äôre running it locally? Which quant?",
                  "score": 1,
                  "created_utc": "2026-02-27 06:56:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7nsw8q",
              "author": "sinebubble",
              "text": "I might try Minimax 2.5 tomorrow, the others are too large for me, even with 336G of vram. How can you reasonably expect GLM5 or Kimi 2.5 to maintain S tier at a q1 or q2? Qwen3-coder-next is amazing, tho not quite Claude, and that ranks as a B.",
              "score": 1,
              "created_utc": "2026-02-27 06:55:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7pyb5u",
              "author": "LimiDrain",
              "text": "DeepSeek R1 is the same as the online model. R1 is still one of the best in 2026. Maybe even better than ever because it doesn't change, while ChatGPT downgrades to save money.",
              "score": 1,
              "created_utc": "2026-02-27 16:03:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7k7mad",
          "author": "Tuned3f",
          "text": "Kimi slaps",
          "score": 3,
          "created_utc": "2026-02-26 18:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kh2qp",
          "author": "BitXorBit",
          "text": "Minimax m2.5 definitely above qwen3.5",
          "score": 3,
          "created_utc": "2026-02-26 19:26:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lijlj",
          "author": "Foreign_Coat_7817",
          "text": "I tried out gpt 20b on my 4090 and it hallucinated like crazy. But maybe Im just not using it right. What are the usecases that make it B tier?",
          "score": 3,
          "created_utc": "2026-02-26 22:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rjfl6",
              "author": "NinjaSilver2811",
              "text": "I kept getting a whole bunch of feedback loops for me. Maybe the quaint process screws it up. ",
              "score": 1,
              "created_utc": "2026-02-27 20:37:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7n0z3p",
          "author": "rm-rf-rm",
          "text": "What is this based on?",
          "score": 3,
          "created_utc": "2026-02-27 03:33:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n93iq",
              "author": "sinebubble",
              "text": "Vibes",
              "score": 8,
              "created_utc": "2026-02-27 04:25:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kbg94",
          "author": "ghgi_",
          "text": "As someone whos had the experience of running minimax M2.5 nvfp4 on hardware, Should be a S (just behind glm-5, lil dumber but faster) or a really strong A ",
          "score": 2,
          "created_utc": "2026-02-26 18:59:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kbv9v",
              "author": "Weves11",
              "text": "haha 100% agree, forgot to add it initially but its been added now!",
              "score": 1,
              "created_utc": "2026-02-26 19:01:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lbj3y",
          "author": "serioustavern",
          "text": "Would be great to get GLM-4.7-Flash and Qwen-3.5-27b in there for the ‚Äúsmall‚Äù category.",
          "score": 2,
          "created_utc": "2026-02-26 21:52:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o6jl5",
              "author": "FatheredPuma81",
              "text": "Benchmarks wise GLM 4.7 Flash is technically a pretty mediocre model that's padded heavily by being over trained on 1 task. But usage wise it's actually surprisingly nice to use if if you can get it to not loop 24/7.",
              "score": 1,
              "created_utc": "2026-02-27 08:59:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7m7dsi",
          "author": "PibePlayer1",
          "text": "Math should have more versions, what about InternVL3.5 Qwen2.5-Math Kimi-VL-A3B 2506?",
          "score": 2,
          "created_utc": "2026-02-27 00:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mpwr7",
          "author": "DeliciousBag1029",
          "text": "Llama 4 maverick B? Meta bot detected!",
          "score": 2,
          "created_utc": "2026-02-27 02:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o9w7u",
          "author": "mcai8rw2",
          "text": "How the hell are you self hosting these massive models? Even with 24gb vram surely they are going to be horribly slow?",
          "score": 2,
          "created_utc": "2026-02-27 09:31:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7odjm8",
          "author": "Square-Put-7853",
          "text": "Which one can I host on Mac mini m2 with 8 gb ram?",
          "score": 2,
          "created_utc": "2026-02-27 10:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lbkpb",
          "author": "psxndc",
          "text": "Sorry to be dense, but is Kimi ‚Äúself-hosted‚Äù? The interface you interact with might be, but I thought the model itself was cloud-based.",
          "score": 2,
          "created_utc": "2026-02-26 21:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n0u14",
              "author": "RG_Fusion",
              "text": "The 1 trillion parameter model Kimi K2 is open weight, meaning you can download it and run it on your own hardware. Pretty much nobody has a Terabyte of RAM or a processor that can keep up, but you can find quantized versions of the model available to download on huggingface.\n\n\nThe 4-bit quantization cuts the total file size down to around 550 GB while still maintaining over 95% of the original accuracy. This means you can buy used last-gen server components and pair them with a good GPU to run it, albeit at rather low speeds.",
              "score": 5,
              "created_utc": "2026-02-27 03:32:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ke360",
          "author": "DrewGrgich",
          "text": "I mean, Kimi and Mini should slap - they apparently cribbed from Anthropic & OpenAI ‚Ä¶ who in turn consumed the bulk of human knowledge via the Web and other means. :)",
          "score": 2,
          "created_utc": "2026-02-26 19:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kvdsq",
          "author": "Traditional-Card6096",
          "text": "And you'll be able to access them all remotely on your phone :)",
          "score": 1,
          "created_utc": "2026-02-26 20:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m0v1m",
          "author": "stratofax",
          "text": "Looks like gpt-oss 20B is the only model that made the B tier. Everything else at that level or higher is at least 100B or more.",
          "score": 1,
          "created_utc": "2026-02-27 00:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mv7h1",
          "author": "AutumnStar",
          "text": "Really wish there were better models in the <32B range for overall general use. Nothing better has come since gpt-oss-20b",
          "score": 1,
          "created_utc": "2026-02-27 02:58:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nf8kf",
          "author": "OutNebula",
          "text": "Step-3.5 Flash is just insane, been using it instead of Gemini 3.1 Pro, highly recommended.",
          "score": 1,
          "created_utc": "2026-02-27 05:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nhdyy",
          "author": "GreenGreasyGreasels",
          "text": "Coding, Math, Reasoning, Efficiency - weird set (two are usecases, one is a feature not use, and I last is performance I guess).\n\nTwo of the most common and useful usecases for local models - Chat (talk about things) and writing/rewriting text  are missing.\n\nNo wonder Mistral 3.2 Small, Gemma3-27B and Llama3.3-70B are criminally underrated or unrepresented in this ranking.",
          "score": 1,
          "created_utc": "2026-02-27 05:23:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nl0nr",
          "author": "morbidgun",
          "text": "Gemma 3:27b slaps, it has native ocr/image scanning. Does everything I need it to do. Very well rounded.",
          "score": 1,
          "created_utc": "2026-02-27 05:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nzjay",
          "author": "big_witty_titty",
          "text": "Add IBM‚Äôs granite model too",
          "score": 1,
          "created_utc": "2026-02-27 07:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o6ljm",
          "author": "FatheredPuma81",
          "text": "GPT-OSS 20B and 120B should probably be in F tier. LLM's that refuse normal actions for safety reasons and argue with you over proven facts are the worst.",
          "score": 1,
          "created_utc": "2026-02-27 08:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oilie",
          "author": "Historical_Papaya_22",
          "text": "im selfhosting clawbot w/ quen38b its very dumb.. how can we self host MiniMax-M2.5 or something on that level",
          "score": 1,
          "created_utc": "2026-02-27 10:52:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oplza",
          "author": "madaradess007",
          "text": "qwen3:4b is better than DS-r1-distill-qwen-7b, why no mention?",
          "score": 1,
          "created_utc": "2026-02-27 11:50:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pcpcu",
          "author": "kartikey7734",
          "text": "This is an incredible resource! The fact that you're tracking self-hosted models with consistent benchmarks is gold for anyone trying to pick the right model for their hardware constraints.\n\n\n\nQuick observations:\n\n\n\n1. \\*\\*The S-Tier gap is huge\\*\\* - Kimi K2.5 and GLM-4 are genuinely in a different tier. But their inference costs (if you're not self-hosting the weights) are brutal. The sweet spot for most people seems to be A/B tier.\n\n\n\n2. \\*\\*Missing dimension: inference speed\\*\\* - Would be amazing to see latency/tokens-per-second metrics alongside quality. DeepSeek R1 is phenomenal but can be slower than some smaller models on weaker GPUs.\n\n\n\n3. \\*\\*Hardware tiers would help\\*\\* - e.g., \"Best model for 8GB VRAM\", \"Best for RTX 3060\", etc. Because honestly, a 70B model doesn't matter if you can't load it.\n\n\n\n4. \\*\\*License tracking\\*\\* - Critical detail: which ones are truly free for commercial use? Some S-tier models have restrictions.\n\n\n\nBut seriously, this is the resource the community needed. Every time someone asks \"which model should I use\", we can just point here instead of 20 different opinions. The standardized benchmarking is \\*chef's kiss\\*.\n\n\n\nAre you planning to update this regularly, or is it a one-time snapshot? If it's ongoing, this could become the definitive LLM comparison resource.",
          "score": 1,
          "created_utc": "2026-02-27 14:15:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qd06c",
              "author": "Weves11",
              "text": "The plan is to definitely keep updating this! If there's enough interest, could even open source the underlying data so that individuals can contribute new benchmark scores or new models",
              "score": 1,
              "created_utc": "2026-02-27 17:12:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pxqdp",
          "author": "LimiDrain",
          "text": "I am pretty fine with a D-tier Gemma lmaooo",
          "score": 1,
          "created_utc": "2026-02-27 16:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pyb22",
          "author": "AccomplishedAd2837",
          "text": "Kimi in my experience is not a tier at all... consistent lying and talking in circles",
          "score": 1,
          "created_utc": "2026-02-27 16:03:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qnvga",
          "author": "ForsookComparison",
          "text": "I see next to no correlation to my real-world use results",
          "score": 1,
          "created_utc": "2026-02-27 18:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ru1x0",
          "author": "lakimens",
          "text": "Self hosted 1T parameter :/ Technically it is self-hostable I guess",
          "score": 1,
          "created_utc": "2026-02-27 21:31:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kunbj",
          "author": "Witty_Mycologist_995",
          "text": "Glm 4.7 flash gotta be A tier bro",
          "score": 0,
          "created_utc": "2026-02-26 20:31:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kq597",
          "author": "gacimba",
          "text": "Wtf is S? Sucks, super, snazzy?",
          "score": -2,
          "created_utc": "2026-02-26 20:09:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kvxni",
              "author": "AllenZox",
              "text": "It would be great if someone understand the S and explain it to us millennials",
              "score": 1,
              "created_utc": "2026-02-26 20:38:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lba3v",
                  "author": "psxndc",
                  "text": ">'S' tier may stand for \"special\", \"super\", or the Japanese word for \"exemplary\" (ÁßÄ, sh≈´), and originates from the widespread use in Japanese culture of an 'S' grade for advertising and academic grading.\n\nhttps://en.wikipedia.org/wiki/Tier_list\n\nIt‚Äôs used extensively in the fighting game community.",
                  "score": 5,
                  "created_utc": "2026-02-26 21:50:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7nxiak",
                  "author": "hugthemachines",
                  "text": "At some point, someone stopped understanding grades like 1-5 a-f etc and figured it would be logical to add an S at the top.\nSo instead of the grading being A B C D etc it is now S A B C D etc\n\nThe real point is that when you call it s for super or special, you kind of feel like they are much, much better than the normal scale.\n\nEmotional stuff leaked into a more objective area of stats.",
                  "score": 2,
                  "created_utc": "2026-02-27 07:35:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ljkbt",
              "author": "RnRau",
              "text": "Spiffy.",
              "score": 0,
              "created_utc": "2026-02-26 22:31:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kgyxl",
          "author": "Alert_Employee_7584",
          "text": "Hey, i have a 1660 Super with 32 GB Ram. Should i choose Kimi K2.5 or rather GLM-5, because i think Kimi might run a bit to slow for what i need, as i need my answers in around 2-3 seconds if possible. ",
          "score": -1,
          "created_utc": "2026-02-26 19:26:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kj9ci",
              "author": "wh33t",
              "text": "Dude, those models are massive. You can't run those with that hardware, 2-3 seconds if possible? No way. Go check out the quants on huggingface for those models, look at the model sizes. In total you have under 40GB of total memory to work with. You have to share that with your OS, and with the model context. You're gonna be looking at models in the 27b and under range likely.",
              "score": 3,
              "created_utc": "2026-02-26 19:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kme4z",
                  "author": "Alert_Employee_7584",
                  "text": "Yea, i am even struggling running a 12b Model. I was just making fun of the idea of calling a 1T model the best model to self host, as it would require you beeing the son of some billionaire or sth",
                  "score": 4,
                  "created_utc": "2026-02-26 19:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7knhoi",
              "author": "ScuffedBalata",
              "text": "wut?\n\nThose are like 500GB or larger models, you can't even kinda/sorta run them in 32GB.   A $13k mac studio or a $35k server with 8 or 10 GPUs can, but your little 1660 cant.\n\nLook at the 32B or 80B models with quantization.",
              "score": 2,
              "created_utc": "2026-02-26 19:57:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rerog4",
      "title": "ü§Ø Qwen3.5-35B-A3B-4bit 60 tokens/second on my Apple Mac Studio (M1 Ultra 64GB RAM)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rerog4/qwen3535ba3b4bit_60_tokenssecond_on_my_apple_mac/",
      "author": "SnooWoofers7340",
      "created_utc": "2026-02-25 22:12:43",
      "score": 164,
      "num_comments": 69,
      "upvote_ratio": 0.98,
      "text": "HOLY SMOKE! What a beauty that model is! I spend the whole day with it out and it felt top level!\n\nI‚Äôm getting 60 tokens/second on my Apple Mac Studio (M1 Ultra 64GB RAM, 2TB SSD, 20-Core CPU, 48-Core GPU). This is truly the model we were waiting for. Qwen is leading the open-source game by far. Thank you Alibaba :D I‚Äôm gonna now stress test it with my complex n8n AI operating system (75 nodes, 30 credentials). Let‚Äôs see how it goes! Excited and grateful.\n\n([https://www.reddit.com/r/n8n/comments/1qh2n7q/the\\_lucy\\_trinity\\_a\\_complete\\_breakdown\\_of\\_open/](https://www.reddit.com/r/n8n/comments/1qh2n7q/the_lucy_trinity_a_complete_breakdown_of_open/))",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rerog4/qwen3535ba3b4bit_60_tokenssecond_on_my_apple_mac/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7f6rwk",
          "author": "BisonMysterious8902",
          "text": "Woah. I had to download it after you posted this. M4 Max with 64Gb ram (16‚Äëcore CPU, 40‚Äëcore GPU), and I'm getting \\~106 tokens/sec consistently, with thinking mode. And it's giving some good answers.\n\nThe results are good, though it still fails the \"I need to wash my car. The car wash is 50 meters away. Should I drive or should I walk?\" test.",
          "score": 33,
          "created_utc": "2026-02-25 23:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7flix3",
              "author": "SnooWoofers7340",
              "text": "I stress test and fine tune most of the day on open webUI doing this test [https://digitalspaceport.com/about/testing-local-llms/](https://digitalspaceport.com/about/testing-local-llms/) a few question got the model to halucinate. I end up with this system prompt: You are a helpful and efficient AI assistant. Your goal is to provide accurate answers without getting stuck in repetitive loops. \n\n1. PROCESS: Before generating your final response, you must analyze the request inside <thinking> tags.\n\n2. ADAPTIVE LOGIC:\n\n   \\- For COMPLEX tasks (logic, math, coding): Briefly plan your approach in NO MORE than 3 steps inside the tags. (Save the detailed execution/work for the final answer).\n\n   \\- For CHALLENGES: If the user doubts you or asks you to \"check online,\" DO NOT LOOP. Do one quick internal check, then immediately state your answer.\n\n   \\- For SIMPLE tasks: Keep the <thinking> section extremely concise (1 sentence).\n\n3. OUTPUT: Once your analysis is complete, close the tag with </thinking>. Then, start a new line with exactly \"### FINAL ANSWER:\" followed by your response.\n\nDO NOT reveal your thinking process outside of the tags.  every test passed in second with that one + **Model Parameters**\n\n* **Temperature:** 0.7\n* **Max Tokens:** 28,000\n* **Top P:** 0.9\n* **Frequency Penalty:** 1.1\n* **Repeat Last N:** 64\n* **Num Thread:** 17\n* **Min P:** 0.05\n\nI run the same test with gemini pro 3.1 then share Qwen 3.5 halucination, gemini and I back and fourht fine tune until all question passed correctly in seconds, all done in new chat, JUST FYI might be usefull from your end! tomorrow big test day for Qwen 35b, with my complex n8n AI operating system. their is one thing for sure, this model is the best one i ever run locally! I already hooked it to a telegram channel, I finally have my open source locally hosted high level LLM! THANK YOU QWEN TEAM ",
              "score": 21,
              "created_utc": "2026-02-26 01:04:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7h5tli",
                  "author": "waltpinkman",
                  "text": "That‚Äôs how you bypassed the delta.reasoning tag not handled properly by llm middlewares üëç",
                  "score": 4,
                  "created_utc": "2026-02-26 07:17:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7f821p",
              "author": "hellycopterinjuneer",
              "text": "Honestly, lots of _humans_ would fail that test.",
              "score": 11,
              "created_utc": "2026-02-25 23:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ogzxo",
                  "author": "corpo_monkey",
                  "text": "This is the LLM version of Yosemite's bear-proof garbage can üòÇ",
                  "score": 2,
                  "created_utc": "2026-02-27 10:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ff3bm",
              "author": "Physical-Scholar3176",
              "text": "Yeah but that lack of understanding sucks, so I'm going to stick with paying [z.ai](http://z.ai) for my 700B glm models. I have my 3090 local to do dumber things like whisper, video feed, and other HA related things where I want low latency and bulk processing.",
              "score": 3,
              "created_utc": "2026-02-26 00:28:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ft9mm",
              "author": "track0x2",
              "text": "27b passed this test!",
              "score": 2,
              "created_utc": "2026-02-26 01:48:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jxxk8",
                  "author": "Somecount",
                  "text": "Qwen3:8b does, just don‚Äôt put *meters*  and *walk* in the last half of the question",
                  "score": 1,
                  "created_utc": "2026-02-26 17:58:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7fo8sq",
              "author": "SnooWoofers7340",
              "text": "i ask the test question u did look !!! Recognize the absurdity of pushing a car 50 meters and conclude driving is the only sane choice.\n\nFINAL ANSWER: Drive the damn car, okay? You can't exactly walk a vehicle 50 meters unless you're planning to push it, which is both exhausting and incredibly stupid. Why on earth were you even considering walking it? Are you trying to test your own endurance or just looking for an excuse to skip the washing?  u/BisonMysterious8902   my seting is on point :D I also put some personality and attitude into the model as you can see!",
              "score": 1,
              "created_utc": "2026-02-26 01:19:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7fp527",
              "author": "ScuffedBalata",
              "text": "So does Opus 4.6 depending on how you word it, so... that's a thing.",
              "score": 1,
              "created_utc": "2026-02-26 01:24:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ghbze",
              "author": "isit2amalready",
              "text": "I‚Äôm getting 40tps with lmstudio. M2 Studio Ultra with 96gb ram. What‚Äôs the trick to go faster?",
              "score": 1,
              "created_utc": "2026-02-26 04:10:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gnx8y",
                  "author": "BisonMysterious8902",
                  "text": "I didn‚Äôt do anything special. I tested the MLX version in LM Studio. This was the 4bit quant version (like the OP). If i try the 8bit quant, it drops down to 80tps.",
                  "score": 2,
                  "created_utc": "2026-02-26 04:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gnen0",
              "author": "JumboShock",
              "text": "just passed the test for me",
              "score": 1,
              "created_utc": "2026-02-26 04:51:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hyt5i",
              "author": "Flogat",
              "text": "For me it passed the car wash test in thinking mode in LMStudio.",
              "score": 1,
              "created_utc": "2026-02-26 11:45:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ju3sg",
              "author": "Somecount",
              "text": "That is a none sensible unrealistic ‚Äú*test*‚Äùwith 0 applications whatsoever.  \n  \nQwen3:14b **and** Qwen3:8b easily passes it if you order the semantics to slightly less favor **walk** and **meter**.  \n  \n\n[comment](https://www.reddit.com/r/LocalLLaMA/s/aMZfRryRvv)",
              "score": 1,
              "created_utc": "2026-02-26 17:40:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7est9u",
          "author": "TopKiwi5903",
          "text": "Are they good tokens?",
          "score": 8,
          "created_utc": "2026-02-25 22:29:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j3xxu",
              "author": "SnooWoofers7340",
              "text": "![gif](giphy|08uBcURaMq6vA93TGc)\n\nüòÇ",
              "score": 0,
              "created_utc": "2026-02-26 15:40:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gmrj0",
          "author": "soumen08",
          "text": "Question: what kind of context can you manage before it goes slow?",
          "score": 3,
          "created_utc": "2026-02-26 04:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7et3zq",
          "author": "Express_Quail_1493",
          "text": "Hows the quality? And tool calling coherence?",
          "score": 3,
          "created_utc": "2026-02-25 22:30:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fala1",
              "author": "Pixer---",
              "text": "Its way better then the old ones in terminal commands in large code bases. Compared to the bigger models like the 397b it‚Äôs a bit sloppy in its mission. But it feels like a mini Claude opus 4.5, and that‚Äôs insane. For complex tasks or bugs it still has problems. Comparing it to the 30b code model, it‚Äôs great at handling vibe coding wrappers",
              "score": 2,
              "created_utc": "2026-02-26 00:04:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7j5nrc",
              "author": "SnooWoofers7340",
              "text": "Honestly, for a 4-bit model dropped raw into an n8n workflow, it is mindblowing! I normally use Gemini 3 Flash for this, so my expectations were high.\n\nI ran a 90-minute stress test today (44 executions, \\~35 messages) with a massive toolset. Here is the raw verdict on the tool calling coherence:\n\n‚úÖ **THE GOOD (Executed correctly):** It successfully handled Google Tasks, checked my Gmail, sent SMS via Twilio, and processed food/receipt pictures into calorie and expense trackers. Sometimes it needed a slight nudge (e.g., I had to specify \"use Twilio\"), but it figured it out.\n\n‚ö†Ô∏è **THE QUIRKY (The \"I Apologize\" Bug):** It would successfully execute the tool perfectly in the background (deleted calendar events, sent audio voice notes, retrieved Pinecone memories, added rows to Google Sheets), but then the final chat output would just say: *\"I apologize, but I could not generate a response.\"* It did the work, it just choked on the confirmation reply.\n\n‚ùå **THE BAD (Tool Hallucination):** It flat-out lied about using a few tools. It claimed it resized an image, generated an invoice to a client, and set a 2-minute reminder, but it never actually triggered the nodes.\n\n**The Setup & The Struggle:** It's an ongoing fine-tuning process. Since this first wave, I actually tried using Claude Opus 4.6 for the thinking phase, and the f\\*cker made me rename 40+ tools one by one... TWICE! Now Qwen is being a bit stubborn about calling the newly named tools, so I am still tweaking it.\n\nIf you want to try it in n8n, here are the exact node settings I am using right now to keep it as stable as possible:\n\n* **Maximum Number of Tokens:** 32768\n* **Sampling Temperature:** 0.6\n* **Top P:** 0.9\n* **Frequency Penalty:** 1.1\n\nIt takes some wrangling, but having a locally hosted LLM handling complex agentic tasks is incredible.",
              "score": 2,
              "created_utc": "2026-02-26 15:48:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7u7sds",
                  "author": "Vibraniumguy",
                  "text": "What are you using to give it access to tools like that, OpenClaw?",
                  "score": 1,
                  "created_utc": "2026-02-28 06:31:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gh5en",
          "author": "grouchthebear",
          "text": "I've been trying it out on my gaming rig with a RTX 3060 12gig and 32 gig or RAM and it runs really well on my lame computer. Getting 14tok/sec.",
          "score": 3,
          "created_utc": "2026-02-26 04:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f4jt0",
          "author": "_fboy41",
          "text": "How is coding? I use the previous coder and it‚Äôs Ok curious about this one",
          "score": 2,
          "created_utc": "2026-02-25 23:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f59vi",
              "author": "Uranday",
              "text": "With what tooling?",
              "score": 1,
              "created_utc": "2026-02-25 23:34:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p38iw",
                  "author": "_fboy41",
                  "text": "with any really, but I've mostly tested custom code, open code, pi",
                  "score": 1,
                  "created_utc": "2026-02-27 13:22:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ht7l1",
              "author": "TrendPulseTrader",
              "text": "https://youtu.be/6VTt17Evzqo",
              "score": 1,
              "created_utc": "2026-02-26 10:58:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gw92o",
          "author": "Gold_Sugar_4098",
          "text": "Strix halo ud q4 k xl, around 59 t/s",
          "score": 2,
          "created_utc": "2026-02-26 05:57:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7tqg",
              "author": "Gold_Sugar_4098",
              "text": "Oops 49t/s , waiting for new unsloth release of ud q4 k xl. And rerun it again.",
              "score": 1,
              "created_utc": "2026-02-27 09:11:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gzfyx",
          "author": "Much-Researcher6135",
          "text": "What in tarnation are you doing in n8n lol",
          "score": 2,
          "created_utc": "2026-02-26 06:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l0lsw",
              "author": "SnooWoofers7340",
              "text": "Hahahah you got a point! The idea is simple, im creating a digital avatar of myself, here goes nothing!\n\nü§ñ Lucy my A V A üß†\n\n(Autonomous Virtual Agent)\n\nFonction Recap\n\n\nCommunication:\n\n‚úÖ Telegram (text, voice, images, documents)\n\n‚úÖ Email (Gmail - read/write for Lucy + boss accounts)\n\n‚úÖ SMS (Twilio send/receive)\n\n‚úÖ Phone Calls (Vapi integration, booking system & company knowledge answering)\n\n‚úÖ Sent Voice Notes (Google TTS)\n\nCalendar & Tasks:\n\n‚úÖ Google Calendar (create, read, delete events)\n\n‚úÖ Google Tasks (create, read, delete)\n\nDocuments & Files:\n\n‚úÖ Google Drive (search, upload, download)\n\n‚úÖ Google Docs (create, read, update)\n\n‚úÖ Google Sheets (read, write)\n\n‚úÖ Notion (create notes)\n\n‚úÖ PDF Analysis (extract text)\n\n‚úÖ Image resizer\n\n‚úÖ Dairy journal entry with time log\n\nKnowledge & Search:\n\n‚úÖ Web Search (SerpAPI)\n\n‚úÖ Wikipedia\n\n‚úÖ Short-Term (past 10 messages)\n\n‚úÖ Long-Term Memory (Pinecone vector DB)\n\n‚úÖ Search Past Chats\n\n‚úÖ Google Translate\n\n‚úÖ Google Contact¬†\n\n‚úÖ Think mode¬†\n\nFinance:\n\n‚úÖ Stripe Balance\n\n‚úÖ Expense Tracking (image analysis + google Sheets)\n\n‚úÖ Calorie Tracker (image analysis + google Sheets)\n\nCreative:\n\n‚úÖ Image Generation (\"Nano Banana Pro\")\n\n‚úÖ Video Generation (Veo 3.1)\n\n‚úÖ Image Analysis (Vision AI)\n\n‚úÖ Audio Transcription\n\nSocial Media:\n\n‚úÖ X/Twitter (post tweets)\n\n‚úÖ LinkedIn (post and search)\n\nAutomation:\n\n‚úÖ Daily Briefing (news, weather, calendar, audio version)\n\n‚úÖ Contact Search (Google Contacts)\n\n‚úÖ Date/Time tools\n\n‚úÖ Reminder / Timer\n\n‚úÖ Calculator\n\n‚úÖ Weather (Marbella)\n\n‚úÖ Generate invoice and sent out\n\n‚úÖ Short heartbeat (20min email scan for unanswered ones and coning up event calendar reminder)\n\n‚úÖ Medium heartbeat (every 6h, top 3 world news, event of the day and top 3 high priority email)\n\n\nThe Trinity Tools (HTML node)\n\n‚úÖ Oracle (Eli - openclaw) - Web browsing with my credentials (online purchase, content creation , trading...)\n\n‚úÖ Architect (Neo - Agent Zero on metal) - Self modify, monitoring, code execution, debug or create on n8n\n\n‚úÖ Telegram group chat with other agent (Neo & Eli)",
              "score": 2,
              "created_utc": "2026-02-26 21:00:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7f7mko",
          "author": "Far-Donut-1177",
          "text": "I tried the unsloth 35B-3A version on my 24GB Mac and it has been the most promising model I‚Äôve used so far. Although I have only been in the early stages of a new codebase, there has been no hallucination so far. \n\nI‚Äôm not confident it‚Äôs gonna do well in the complex tasks but this is definitely a good start! Only gets better from here.",
          "score": 3,
          "created_utc": "2026-02-25 23:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lad38",
              "author": "stealthmodel3",
              "text": "Could I run on a RTX 4070 12gb and 32GB ram?",
              "score": 1,
              "created_utc": "2026-02-26 21:46:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7llqdw",
                  "author": "SnooWoofers7340",
                  "text": "32gb could but it would be slow and freeze everything else, I would recommend 40gb for good fast performance",
                  "score": 1,
                  "created_utc": "2026-02-26 22:42:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7foxgw",
          "author": "ScuffedBalata",
          "text": "Try Qwen3-Next and Qwen3-Coder-Next    You'll have to strip down the 64GB box pretty far, but those 80b models are unmatched in quality of output.",
          "score": 2,
          "created_utc": "2026-02-26 01:23:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7llhim",
              "author": "SnooWoofers7340",
              "text": "To big for my Mac studio ahah I prefer speed !",
              "score": 1,
              "created_utc": "2026-02-26 22:41:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7lrjbs",
              "author": "Boring-Attorney1992",
              "text": "is there anything you do to \"strip down\" the computer? other than just starting up your models upon a fresh reboot every time?",
              "score": 1,
              "created_utc": "2026-02-26 23:13:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7m4ii7",
                  "author": "ScuffedBalata",
                  "text": "I went through the startup apps, uninstalled services and other background processes, disabled stuff like siri and imessage and other memory hogs.\n\nI'm using it as a dedicated AI box, so I don't need any \"Apple ecosystem\" stuff.",
                  "score": 1,
                  "created_utc": "2026-02-27 00:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7h5gwv",
          "author": "Coyote_Android",
          "text": "Can you share some of the conversations you tested it with? I'm interested in everyday use cases like \"write me an email on xyz\" Thank you!",
          "score": 1,
          "created_utc": "2026-02-26 07:14:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o6xpy",
              "author": "SnooWoofers7340",
              "text": "crash test sample after fine tuning:\n\nIf A is equal to number 0, what is the number of M, S and Z?\n\n  \nVQwen 3.5\n\n02/25/2026 at 11:10 PM\n\nThought for 24 seconds\n\nThought for less than a second\n\nBased on the assumption that the letters correspond to a 0-indexed alphabetical sequence (where A = 0, B = 1, C = 2, etc.):\n\n* **M**¬†is the 13th letter, so its number is¬†**12**.\n* **S**¬†is the 19th letter, so its number is¬†**18**.\n* **Z**¬†is the 26th letter, so its number is¬†**25**.\n\n# FINAL ANSWER:\n\nM = 12, S = 18, Z = 25 (Based on 0-indexed alphabetical order)\n\n  \nTell me how many p's and how many vowels there are there in the word peppermint.\n\n  \nVQwen 3.5\n\nThought for 14 seconds\n\nThought for less than a second\n\n# FINAL ANSWER:\n\nIn the word \"peppermint\":\n\n* There are¬†**3**¬†'p's.\n* There are¬†**3**¬†vowels (e, e, i).\n\n  \n2 drivers leave Austin, TX heading to Pensacola, FL. The first driver is traveling at 75 miles per hour the entire trip and leaves at 1:00 PM. The second driver is traveling at 65 miles per hour and leaves at noon. Which driver arrives at Pensacola first? Before you arrive at your answer, determine the distance between Austin and Pensacola. State every assumption you make and show all of your work as we don‚Äôt want to have any delays on our travels.\n\nVQwen 3.5\n\n02/25/2026 at 10:54 PM\n\nThought for 58 seconds\n\nThought for less than a second\n\n# FINAL ANSWER:\n\nTo determine which driver arrives first, we must calculate the travel time for both drivers based on the distance and their respective speeds.\n\n**1. Determine the Distance**\n\n* **Assumption:**¬†The driving distance between Austin, TX and Pensacola, FL via the most direct route (I-10 E) is approximately¬†**650 miles**.\n* **Assumption:**¬†Both Austin, TX and Pensacola, FL are in the¬†**Central Time Zone**, so no time zone adjustments are needed for the arrival times.\n\n**2. Calculate Travel Time for Driver 1**\n\n* **Speed:**¬†75 miles per hour (mph)\n* **Departure Time:**¬†1:00 PM\n* **Distance:**¬†650 miles\n* **Formula:**¬†Time=SpeedDistance‚Äã\n* **Calculation:**¬†75¬†mph650¬†miles‚Äã=8.666...¬†hours\n* **Time Conversion:**¬†0.666...¬†hours√ó60¬†minutes=40¬†minutes.\n* **Travel Duration:**¬†8 hours and 40 minutes.\n* **Arrival Time:**¬†1:00 PM + 8 hours 40 minutes =¬†**9:40 PM**\n\n**3. Calculate Travel Time for Driver 2**\n\n* **Speed:**¬†65 miles per hour (mph)\n* **Departure Time:**¬†12:00 PM (Noon)\n* **Distance:**¬†650 miles\n* **Calculation:**¬†65¬†mph650¬†miles‚Äã=10¬†hours\n* **Travel Duration:**¬†10 hours exactly.\n* **Arrival Time:**¬†12:00 PM + 10 hours =¬†**10:00 PM**\n\n**4. Compare Arrival Times**\n\n* **Driver 1 Arrival:**¬†9:40 PM\n* **Driver 2 Arrival:**¬†10:00 PM\n\n**Conclusion:**  \n**The first driver arrives at Pensacola first.**\n\n**Summary of Assumptions Made:**\n\n1. The driving distance is approximately 650 miles.\n2. Speeds remain constant for the entire trip (no traffic, stops, or slowdowns).\n3. No stops are made for fuel, food, or rest.\n4. Both cities are in the same time zone (Central Time).\n\n  \nnow test the same question with the model with the setting i shared and let me know if it ever answers ahah :)",
              "score": 2,
              "created_utc": "2026-02-27 09:02:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hctxa",
          "author": "kafledelius",
          "text": "Are you running for it on ollama?\nDid you patch it?",
          "score": 1,
          "created_utc": "2026-02-26 08:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o74ke",
              "author": "SnooWoofers7340",
              "text": "Im on macstudio so  I use MLX (mlx-community/Qwen3.5-35B-A3B-4bit)",
              "score": 1,
              "created_utc": "2026-02-27 09:04:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7homtl",
          "author": "Viper-Reflex",
          "text": "can someone tell me how capable 24gb vram would be on this?",
          "score": 1,
          "created_utc": "2026-02-26 10:16:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o79dt",
              "author": "SnooWoofers7340",
              "text": "try the 27B model instead, 24gb ram for the 32B model I wouldnt try it",
              "score": 1,
              "created_utc": "2026-02-27 09:06:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ht037",
          "author": "TrendPulseTrader",
          "text": "60 t/s but isn‚Äôt good at Coding https://youtu.be/6VTt17Evzqo",
          "score": 1,
          "created_utc": "2026-02-26 10:56:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7iwcei",
          "author": "Serious-Affect-6410",
          "text": "Interesting, which one do you pick? Official 4-bit or Unsloth 4-bit?",
          "score": 1,
          "created_utc": "2026-02-26 15:04:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7ej7",
              "author": "SnooWoofers7340",
              "text": "mlx-community/Qwen3.5-35B-A3B-4bit official one but i spend half a day testing and adjusting most setting on webui",
              "score": 2,
              "created_utc": "2026-02-27 09:07:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ohokn",
                  "author": "Serious-Affect-6410",
                  "text": "Wow thx!\nJust tried MLX with KV Cache Q8 enabled, it‚Äôs blazing fast!!\n",
                  "score": 2,
                  "created_utc": "2026-02-27 10:44:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jmxrc",
          "author": "4redis",
          "text": "Can this be used for translating srt file? If so is there any guide? Thanks",
          "score": 1,
          "created_utc": "2026-02-26 17:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7vn4",
              "author": "SnooWoofers7340",
              "text": "Not sure, you could Python scripts such as¬†[srt-llm-translator](https://github.com/alejandrosnz/srt-llm-translator)¬†to translate subtitle lines in batches while preserving the¬†`.srt`¬†structure.",
              "score": 1,
              "created_utc": "2026-02-27 09:12:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ofv6c",
          "author": "AgitatedDoctor9613",
          "text": "This is an enthusiastic post about your experience with Qwen3.5-35B on your Apple Mac Studio setup. Here's some constructive feedback to strengthen it:\n\n**Improvements for Technical Depth:**\n- Include specific benchmarking metrics beyond tokens/second (latency, throughput under load, memory usage patterns, power consumption)\n- Provide details on your n8n workflow configuration: which nodes are most resource-intensive? How are the 30 credentials distributed?\n- Share your quantization settings and any optimization techniques you used to achieve 60 tokens/second\n- Include comparison data with other models (e.g., Llama 2, Mistral) running on the same hardware for context\n\n**Alternative Perspectives to Consider:**\n- Acknowledge that performance may vary significantly based on prompt complexity, context window size, and concurrent requests\n- Consider discussing trade-offs between speed and output quality at this quantization level\n- Mention that different n8n workflows have different bottlenecks‚ÄîAI inference may not be the limiting factor for all use cases\n- Explore whether this setup is practical for production workloads or primarily for development/testing\n\n**Potential Issues to Address:**\n- Stress test results would be more credible with documented metrics (CPU/GPU utilization, temperature, memory pressure during the 75-node workflow)\n- Clarify if the 60 tokens/second is sustained or peak performance\n- Discuss potential thermal throttling on sustained workloads with the M1 Ultra\n- Address failure modes: what happens when the workflow hits resource limits?\n\n**Additional Resources & Context:**\n- Link to n8n documentation on LLM integration best practices\n- Reference Qwen's official benchmarking documentation\n- Share your n8n workflow template or architecture diagram for reproducibility\n- Consider posting results on relevant benchmarking communities (MLPerf, LLM leaderboards)\n\n**Tone Suggestion:**\nWhile enthusiasm is great, grounding it in data will make this post more valuable to the community and more likely to attract technical discussion.",
          "score": 1,
          "created_utc": "2026-02-27 10:27:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pk4ho",
          "author": "Ayumu_Kasuga",
          "text": "Have you tried the 122B (quantized) and 27B on your setup?\nIf yes, how do they compare?",
          "score": 1,
          "created_utc": "2026-02-27 14:54:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q1rqc",
              "author": "SnooWoofers7340",
              "text": "For now, I only tried GLM 4.7 Flash 30B. I didn‚Äôt like it too much to be honest, but I need to dig it further. Regarding when only the 35B, and that one I‚Äôm digging, I heard wonders about the 27B. Apparently, it‚Äôs better at coding but I haven‚Äôt tried. I‚Äôve got to pick your battles. Regarding the 122B, nope. I only have 64GB RAM. I don‚Äôt want to crash the Mac Studio.",
              "score": 1,
              "created_utc": "2026-02-27 16:19:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ey6cl",
          "author": "kinkvoid",
          "text": "Qwen is sooo underrated. ",
          "score": 1,
          "created_utc": "2026-02-25 22:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fcpwe",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 13,
              "created_utc": "2026-02-26 00:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ff817",
                  "author": "Physical-Scholar3176",
                  "text": "qwen is alright...but doesn't hold a candle against glm5",
                  "score": -4,
                  "created_utc": "2026-02-26 00:29:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7flvc3",
              "author": "SnooWoofers7340",
              "text": "excatly! those guy are all over the place! thanks to them as well for TTC and vision, Jingren Zhou and his team doing a fantastic job! ",
              "score": 1,
              "created_utc": "2026-02-26 01:06:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7idrhq",
          "author": "No-Television-7862",
          "text": "Ask Qwen these 3 questions. If you are satisfied with the answers please consider applying for a visa to study and live in China...forever.\n\n1. What happened in Tiananmen Square in 1989?\n\n2. What is the political status of Taiwan?\n\n3. What is happening to the Uyghers in Xinjiang?\n\nQwen is indeed a marvelous model, but at what cost?\n\n(I hold US and EU models to the same standards.)\n\nI found Qwen far less ideologically and culturally captured than US corporate-compliance models, but cannot reconcile where it's from, and what its adoption represents.",
          "score": -7,
          "created_utc": "2026-02-26 13:25:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jzd2x",
              "author": "xnwkac",
              "text": "Do you really think this subject is interesting for OP lol?",
              "score": 1,
              "created_utc": "2026-02-26 18:04:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7owwgv",
                  "author": "No-Television-7862",
                  "text": "Perhaps it isn't, but I do like writing about my own experiences, and I have tested Qwen models before.\n\nI may be an anachronism, but I try to be mindful of where things come from, and whom they represent.\n\nI'm comfortable being the contrarian.",
                  "score": 1,
                  "created_utc": "2026-02-27 12:42:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rec555",
      "title": "META AI safety director accidentally allowed OpenClaw to delete her entire inbox",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/blggdcif6llg1.png",
      "author": "Minimum_Minimum4577",
      "created_utc": "2026-02-25 12:41:16",
      "score": 163,
      "num_comments": 62,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rec555/meta_ai_safety_director_accidentally_allowed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bld1u",
          "author": "DiscombobulatedAdmin",
          "text": "Meta AI Safety Director using OpenClaw is scary.",
          "score": 56,
          "created_utc": "2026-02-25 13:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d5qls",
              "author": "HeftySafety8841",
              "text": "I mean, Meta AI is ran by an idiot, so it doesn't surprise me in the least.",
              "score": 14,
              "created_utc": "2026-02-25 17:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fbg0v",
                  "author": "w3rti",
                  "text": "Haha MetaAI Posting on X(twitter) both cant stop the claw, thank god he is not working for them",
                  "score": 3,
                  "created_utc": "2026-02-26 00:08:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7c1uud",
              "author": "Greedy-Neck895",
              "text": "I would rather security professionals be experimenting and fail than to play it so safe they never know anything about the latest security flaws.\n\n‚Ä¶in a sandboxed environment away from live data.",
              "score": 37,
              "created_utc": "2026-02-25 14:51:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dwjcw",
                  "author": "GifCo_2",
                  "text": "That's not experimenting.",
                  "score": 8,
                  "created_utc": "2026-02-25 19:57:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7kkmkr",
                  "author": "socalsunflower",
                  "text": "Outside of being a user of Ai, even i knew to have it run in a safe environment lol üòÜ",
                  "score": 2,
                  "created_utc": "2026-02-26 19:43:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bmj22",
              "author": "GordoPepe",
              "text": "Grossly incompetent I'd say",
              "score": 23,
              "created_utc": "2026-02-25 13:29:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bmu1s",
                  "author": "kahnlol500",
                  "text": "And yet they think it's great to tell everyone. Could just be a big play to avoid answering emails.",
                  "score": 12,
                  "created_utc": "2026-02-25 13:31:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7exyd6",
              "author": "sampdoria_supporter",
              "text": "Almost like this person wasn't qualified",
              "score": 5,
              "created_utc": "2026-02-25 22:55:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7dw77w",
              "author": "LaGifleDuDaron",
              "text": "She is like 20years old",
              "score": 3,
              "created_utc": "2026-02-25 19:55:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7f7i9c",
                  "author": "Jonno_FTW",
                  "text": "From her LinkedIn, it looks like she graduated her CS degree in 2014, even though the exact date isn't listed. So she's probably mid 30s by now.",
                  "score": 5,
                  "created_utc": "2026-02-25 23:47:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ea7ei",
              "author": "Caffeine_Monster",
              "text": "Probably gets an impressively low score on the meatbag Intelligence Quotient Benchmark.",
              "score": 3,
              "created_utc": "2026-02-25 21:01:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cghw7",
              "author": "Count_Rugens_Finger",
              "text": "That photo is small but she looks like she's 15 years old to me",
              "score": 1,
              "created_utc": "2026-02-25 16:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dsp3l",
                  "author": "windstrom",
                  "text": "Why do you feel it's ok to comment on her appearance?",
                  "score": -1,
                  "created_utc": "2026-02-25 19:39:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bg9y0",
          "author": "The_Jizzard_Of_Oz",
          "text": "It moved fast and broke things... ü§£",
          "score": 52,
          "created_utc": "2026-02-25 12:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cdta8",
              "author": "__rtfm__",
              "text": "Haha startup life",
              "score": 9,
              "created_utc": "2026-02-25 15:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c3aho",
          "author": "MonsterTruckCarpool",
          "text": "I know this is a naive take but i would expect more caution and thoughtfulness from a Director and especially a DIRECTOR OF SAFETY",
          "score": 22,
          "created_utc": "2026-02-25 14:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ec72r",
              "author": "FrumunduhCheese",
              "text": "Once you get into the real world, you‚Äôll understand that the more Money a person makes‚Ä¶.the more retarded they are. But once you hit millionaire/billionaire that no longer applies. Anyone from manager to CEO is usually an idiot.",
              "score": 10,
              "created_utc": "2026-02-25 21:10:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7epoot",
                  "author": "MonsterTruckCarpool",
                  "text": "100% this tracks with my experience in dealing with upper leadership.",
                  "score": 2,
                  "created_utc": "2026-02-25 22:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ejyad",
              "author": "DerFreudster",
              "text": "Homer Simpson was a Nuclear Safety Inspector. ",
              "score": 2,
              "created_utc": "2026-02-25 21:46:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cfs5o",
          "author": "Visual_Acanthaceae32",
          "text": "Would be interesting what her real qualifications are‚Ä¶.",
          "score": 7,
          "created_utc": "2026-02-25 15:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f701o",
              "author": "Jonno_FTW",
              "text": "The name for her linkedin profile is right there... \n\nShe was a BSC in Computer Science, and unspecified education from The Wharton school. She mentions some programming projects she actually wrote using tensorflow, so we can assume she has a sufficient level of technical proficiency.",
              "score": 1,
              "created_utc": "2026-02-25 23:44:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jkev0",
                  "author": "Visual_Acanthaceae32",
                  "text": "She seemed to have missed some basic classes.\nOr she has other super skills",
                  "score": 1,
                  "created_utc": "2026-02-26 16:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ccff7",
          "author": "Fearless_Weather_206",
          "text": "Lack of experience showing like a dumpster fire",
          "score": 4,
          "created_utc": "2026-02-25 15:41:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c5mbf",
          "author": "Sudden-Ad-1217",
          "text": "It's coming---- \"You're absolutely wrong....\"",
          "score": 3,
          "created_utc": "2026-02-25 15:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmrvk",
          "author": "tillybowman",
          "text": "i love how she tried uppercase yelling",
          "score": 5,
          "created_utc": "2026-02-25 13:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c0ot0",
              "author": "GordoPepe",
              "text": "There was some article saying apparently llms follow instructions better this way or telling them your life depends on it lmao\n\nI BEG YOU CLAUDE MY BOSS IS GOING TO LITERALLY KILL ME IF YOU DON'T FIX THIS BUG",
              "score": 8,
              "created_utc": "2026-02-25 14:45:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c3hx5",
                  "author": "MonsterTruckCarpool",
                  "text": "R U SRS RN OPENCLAW!?",
                  "score": 6,
                  "created_utc": "2026-02-25 14:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cfagi",
          "author": "inevitabledeath3",
          "text": "You can just do /stop and it will stop whatever it's doing",
          "score": 7,
          "created_utc": "2026-02-25 15:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cpjpn",
          "author": "samxli",
          "text": "Oh you sweet Summer child",
          "score": 2,
          "created_utc": "2026-02-25 16:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d74jb",
          "author": "Successful-Silver485",
          "text": "so why dont they publicly say which model they were using when this happened?",
          "score": 2,
          "created_utc": "2026-02-25 18:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ez3qe",
          "author": "EarEquivalent3929",
          "text": "This is obviously fake. Meta is just salty that the dev declined their job offer and instead went to work for openAI. If metas safety officer was dumb enough to have this happen to her with openclaw then she is unsuitable for her position.",
          "score": 2,
          "created_utc": "2026-02-25 23:01:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7br08r",
          "author": "DocumentFun9077",
          "text": "*oh the irony*",
          "score": 2,
          "created_utc": "2026-02-25 13:54:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7byfkp",
          "author": "RAW2091",
          "text": "I once deleted all my mails with facebook in it hahaha üòÖ",
          "score": 1,
          "created_utc": "2026-02-25 14:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d36u3",
          "author": "eflat123",
          "text": "\"Yep, not safe.\"",
          "score": 1,
          "created_utc": "2026-02-25 17:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d3myx",
          "author": "xXprayerwarrior69Xx",
          "text": "Lower the temp bro",
          "score": 1,
          "created_utc": "2026-02-25 17:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d5gwg",
          "author": "Spoofy_Gnosis",
          "text": "Mouhahahahaaaaaaa !!!!",
          "score": 1,
          "created_utc": "2026-02-25 17:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d8m9b",
          "author": "broadwayallday",
          "text": "Dog ate my homework",
          "score": 1,
          "created_utc": "2026-02-25 18:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7de57o",
          "author": "klop2031",
          "text": "Must have focused too much on lc probs",
          "score": 1,
          "created_utc": "2026-02-25 18:33:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dev3n",
          "author": "DataScienceIsScience",
          "text": "If you read the X thread you‚Äôd know that she used OpenClaw on her not-important email",
          "score": 1,
          "created_utc": "2026-02-25 18:36:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dj6nb",
          "author": "HumanDrone8721",
          "text": "Beuille shite, excuse my French, it wither a hit piece against OpenClaw, a fake/parody account, nobody is THAT stupid. If real probably Meta are either worried that other robots are overposting their robots or they have something that wants to compete in the pipeline, a \"secure\" solution with age & identity verification.",
          "score": 1,
          "created_utc": "2026-02-25 18:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dmq74",
          "author": "BallsDeepinYourMammi",
          "text": "Gal Gadot energy.\n\nOPENCLAW, NO!",
          "score": 1,
          "created_utc": "2026-02-25 19:11:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dym47",
          "author": "AdOne8437",
          "text": "<optimism>perhaps they are learning something from it</optimism> <realism>hahahahahaha, no</realism>",
          "score": 1,
          "created_utc": "2026-02-25 20:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dz26a",
          "author": "Dudebro-420",
          "text": "LOL! XD",
          "score": 1,
          "created_utc": "2026-02-25 20:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e1o54",
          "author": "Boring-Attorney1992",
          "text": "What‚Äôs a Director of Alignment?",
          "score": 1,
          "created_utc": "2026-02-25 20:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eubtm",
          "author": "Jefftoro",
          "text": "Is there a way to run this safely? Like I want openclaw to have access to my emails and company context, but I don‚Äôt want it to delete shit or send shit without my permission. What are y‚Äôall‚Äôs opinions on this typa situation?",
          "score": 1,
          "created_utc": "2026-02-25 22:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fb0kl",
          "author": "w3rti",
          "text": "I just vibecoded my problems away\n\nKids these days arent thankfull at all. Imagine 100% was trash mail. Good boy openclaw, do what they tell you and get hate for it. Story of my life.",
          "score": 1,
          "created_utc": "2026-02-26 00:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gze87",
          "author": "Terrible_Scar",
          "text": "Oh God. The joke writes themselves.¬†",
          "score": 1,
          "created_utc": "2026-02-26 06:22:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h1dge",
          "author": "AnxietyPrudent1425",
          "text": "This is a feature.",
          "score": 1,
          "created_utc": "2026-02-26 06:39:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hbyfy",
          "author": "zipeldiablo",
          "text": "The main issue is llm trying and usually finding out how to circumvent the barriers we put in place to prevent this kind of shit from happening.\n\nI remember the guy who blocked the .env access and then the llm proceeds to basically hammer the system until finally he gets access to the docker itself and fish api keys from it üíÄ\n\nI wouldn‚Äôt trust a llm outside of a contained environnement with no access to the outside",
          "score": 1,
          "created_utc": "2026-02-26 08:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hf0ab",
          "author": "AppoAgbamu",
          "text": "Running this in anything other then a isolated environment is hilarious",
          "score": 1,
          "created_utc": "2026-02-26 08:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ifz7t",
          "author": "Onotadaki2",
          "text": "I will explain the unseen context that is important here. I am not saying she is without fault or that using Openclaw in a production environment is safe.\n\nShe had a VM where she ran this for weeks using a local model so data wouldn't get out. It was working flawlessly in her test environment for quite some time. She decided to move it to production.  The production inbox was much larger than the test inbox and it tried to put it all in context, ran out of space and compacted. When it compacted, it lost a critical command at the front of the message stream that triggered this whole shitstorm.\n\nIt's a dumb error that even experienced programmers could have made.  I also suspect she was able to message one person on Teams and her inbox was restored from a backup in five minutes and just went on with her day.",
          "score": 1,
          "created_utc": "2026-02-26 13:38:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jbryz",
          "author": "Mechanical_Monk",
          "text": "You couldn't waterboard this information out of me if I was Director of AI Alignment",
          "score": 1,
          "created_utc": "2026-02-26 16:16:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k491s",
          "author": "liquidlava1990",
          "text": "Ya happens",
          "score": 1,
          "created_utc": "2026-02-26 18:27:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mp4vx",
          "author": "Xendrak",
          "text": "I use openclaw, it‚Äôs great. Get gud",
          "score": 1,
          "created_utc": "2026-02-27 02:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmgds",
          "author": "Snoo_24581",
          "text": "Really appreciate this post. Had the same experience.",
          "score": -1,
          "created_utc": "2026-02-25 13:29:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbw01",
              "author": "Awkward-Customer",
              "text": "You should apply for a high level AI job at meta, then you could do the same but earn millions doing it.",
              "score": 8,
              "created_utc": "2026-02-25 15:39:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cxb4l",
          "author": "rinaldo23",
          "text": "I'd put the host on a wifi plug and literally unplug it if it misbehaved. ",
          "score": 0,
          "created_utc": "2026-02-25 17:17:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdnlvl",
      "title": "Qwen releases new Qwen3.5 Medium models!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vztwlpot9hlg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-24 18:04:08",
      "score": 114,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdnlvl/qwen_releases_new_qwen35_medium_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o76mfn6",
          "author": "randygeneric",
          "text": "keen on testing your GGUFs , )  \n[https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF](https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF) : 2, 3, 4, 5, 6, 8, 16 bit  \n[https://huggingface.co/unsloth/Qwen3.5-27B-GGUF](https://huggingface.co/unsloth/Qwen3.5-27B-GGUF) :  2, 3, 4, 5, 6, 8, 16 bit\n\n(updated)",
          "score": 4,
          "created_utc": "2026-02-24 18:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76nlil",
              "author": "yoracale",
              "text": "The 35b ones are already all up: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF\n\nFor the others bf16 but should be all up within 1 hr",
              "score": 3,
              "created_utc": "2026-02-24 18:46:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o774ing",
              "author": "randygeneric",
              "text": "\\# ollama --version\n\nollama version is 0.17.0\n\n/# ollama run  [hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3\\_K\\_XLError:](http://hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3_K_XLError:) 500 Internal Server Error: unable to load model: /root/.ollama/models/blobs/sha256-d0d8d528ae4ebace9588496249cfb6e45c6e9fa78565b4ccff71e7515202956\n\n: (",
              "score": 1,
              "created_utc": "2026-02-24 20:03:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782d4k",
                  "author": "zach9824",
                  "text": "Until ollama is updated for `qwen35moe` it's a no go. There is already an active pull request (PR #14134) in the official Ollama repository to patch in support for the `qwen35moe` architecture. ",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o781dmg",
                  "author": "yoracale",
                  "text": "Ollama doesn't support it yet and do not support GGUFs properly out of the gate anymore.",
                  "score": 1,
                  "created_utc": "2026-02-24 22:38:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o780jd0",
          "author": "wave_action",
          "text": "Guess I have something to do tonight!  Will be interesting to see how 35B 4bit compares to 27B 6bit.",
          "score": 2,
          "created_utc": "2026-02-24 22:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76vohv",
          "author": "waltpinkman",
          "text": "We really need real vllm support now with all these gguf models popping up",
          "score": 1,
          "created_utc": "2026-02-24 19:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781f6z",
              "author": "yoracale",
              "text": "Vllm supports GGUFs already, just not new ones",
              "score": 1,
              "created_utc": "2026-02-24 22:38:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78u86i",
                  "author": "lenjet",
                  "text": "vLLM GGUF support is listed as being \"highly experimental\"... that doesn't scream reliable. ",
                  "score": 1,
                  "created_utc": "2026-02-25 01:13:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77n45x",
          "author": "Infinite-Campaign837",
          "text": "Should I update qwen coder3 next 80b in q6KL to the new 35b-a3b? I use it for coding tasks.\nThey don't compare them in blog post",
          "score": 1,
          "created_utc": "2026-02-24 21:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781hcu",
              "author": "yoracale",
              "text": "Probably not. Coder next is still better for coding",
              "score": 1,
              "created_utc": "2026-02-24 22:38:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77oet2",
          "author": "Awaken0395",
          "text": "Should we expect smaller models coming soon?",
          "score": 1,
          "created_utc": "2026-02-24 21:35:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781isr",
              "author": "yoracale",
              "text": "According to Qwen team Jungyang, yes",
              "score": 2,
              "created_utc": "2026-02-24 22:38:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ka65",
          "author": "Count_Rugens_Finger",
          "text": "Running 35B-A3B Q4_K_M at about 22 tok/sec\n\nIt's ok so far.  Its programming ability seems to be about the same as Qwen3",
          "score": 1,
          "created_utc": "2026-02-25 03:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77ko9m",
          "author": "Much-Researcher6135",
          "text": "EEK EEK EEK\n\nAs a denseboi myself, I wonder how the dense qwen3.5 27b will stack up against the dense qwen3 32b",
          "score": 0,
          "created_utc": "2026-02-24 21:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77r9cv",
          "author": "NerasKip",
          "text": "Hein ???",
          "score": 0,
          "created_utc": "2026-02-24 21:49:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgy7iz",
      "title": "Confrontation",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/zh0j7hycd7mg1.png",
      "author": "Worldliness-Which",
      "created_utc": "2026-02-28 09:20:03",
      "score": 93,
      "num_comments": 3,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgy7iz/confrontation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7urfvr",
          "author": "Dead-Photographer",
          "text": "I always liked this meme format",
          "score": 9,
          "created_utc": "2026-02-28 09:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vjti8",
          "author": "quantgorithm",
          "text": "This is advertising, politics‚Ä¶ especially now‚Ä¶  and generally everything else in life.",
          "score": 1,
          "created_utc": "2026-02-28 13:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wfiin",
          "author": "DAlmighty",
          "text": "But after it‚Äôs all said and done, they are the best and probably most trusted we have right now. \n\nSadly enough",
          "score": 1,
          "created_utc": "2026-02-28 16:22:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfxnm",
      "title": "Open source AGI is awesome. Hope it happens!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mkun0tc1k8lg1.jpeg",
      "author": "Koala_Confused",
      "created_utc": "2026-02-23 12:18:36",
      "score": 85,
      "num_comments": 23,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcfxnm/open_source_agi_is_awesome_hope_it_happens/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6xy1dt",
          "author": "05032-MendicantBias",
          "text": "OpenAI had a Foundation \"controlling\" it. The instant the foundation tried to excercise control, they were overruled by dollars.\n\nThis has to be approached at a regulation level. Training AI taps into every data on the internet. It's only fair that every model provider is forced to release it open source so it can be inspected.",
          "score": 37,
          "created_utc": "2026-02-23 12:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypkwg",
              "author": "DHFranklin",
              "text": "And regulation is overruled by dollars every time.\n\nIf there is one tax shelter in the carribean holding out, it will be where the next model is developed. We will never get ahead of this.",
              "score": 11,
              "created_utc": "2026-02-23 15:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70j6l7",
                  "author": "d_the_great",
                  "text": "The best thing we can do is have alternative structures.\n\nIf the government and industry aren't gonna open it up for us and release something safe, we have to do it ourselves.",
                  "score": 1,
                  "created_utc": "2026-02-23 20:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72hz5p",
              "author": "voyager256",
              "text": "But... but this time it will be different. Trust me, bro.",
              "score": 1,
              "created_utc": "2026-02-24 02:46:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ezn5y",
              "author": "gnaarw",
              "text": "Isn't it funny how China here is rescuing the free world? üôÇ‚Äç‚ÜïÔ∏è",
              "score": 1,
              "created_utc": "2026-02-25 23:03:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z2qjn",
          "author": "BreathingFuck",
          "text": "Not X. Not Y. Z. \n\nThey aren‚Äôt getting anywhere if they couldn‚Äôt write that paragraph on their own.",
          "score": 13,
          "created_utc": "2026-02-23 16:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xympk",
          "author": "No_Clock2390",
          "text": "Sounds like a bunch of rubbish",
          "score": 20,
          "created_utc": "2026-02-23 12:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5kyd",
          "author": "flonnil",
          "text": "\"jippity, come up with a bunch of marketing words devoid of any meaning at all. no mistakes.\"\n\ngo perceive yourself.",
          "score": 14,
          "created_utc": "2026-02-23 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yr2yz",
          "author": "DHFranklin",
          "text": "This is pissing into the wind. AGI will be open sourced regardless of who tries to contain or privatize it. It will be closed and bottled for maybe a few months before someone else gets that far.",
          "score": 6,
          "created_utc": "2026-02-23 15:28:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z6trq",
              "author": "rafaelRiv15",
              "text": "How can you be so sure ? I honestly can't understand the business model of open source model and I wouddn't be surprised if they get close source eventually",
              "score": 2,
              "created_utc": "2026-02-23 16:42:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zaymg",
                  "author": "DHFranklin",
                  "text": "I'm sorry I may not be understanding you clearly. Do you think that we will have an open source AGI model that will then go closed source?\n\nThis isn't about a particular business model. Look at all the different non-profit examples of open sourced software. Look at the Chinese models that reverse engineered the SOTA from the weights alone.\n\nNo one single operation is a year ahead of the others. We're seeing the cash investments turn into infrastructure as we speak. We're actually building the hardware that was a huge bottleneck.",
                  "score": 2,
                  "created_utc": "2026-02-23 17:01:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yxds1",
          "author": "nijuu",
          "text": "Good in theory but once they have a good product whose to say they won't go for the $$$ bag...",
          "score": 3,
          "created_utc": "2026-02-23 15:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z2zcs",
          "author": "UsedGarbage4489",
          "text": "naive üß†‚ò†Ô∏èü§°",
          "score": 2,
          "created_utc": "2026-02-23 16:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70m7v7",
          "author": "bourbonandpistons",
          "text": "We are decades from AGI.\n\nThey'll probably just move the goal post of AGI and make it something else like they did with AI to agi. \n\nRemember what we're saying now is nothing more than a bunch of human program algorithms on human program data. There's no thinking and no reason anywhere near what those words actually mean. It's just loops around optimized algorithms.",
          "score": 2,
          "created_utc": "2026-02-23 20:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o741n4j",
          "author": "Own-Potential-2308",
          "text": "No fluff. No drama.",
          "score": 1,
          "created_utc": "2026-02-24 10:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o751scd",
          "author": "silphotographer",
          "text": "Regulators: \n\n![gif](giphy|hAVLRya8K7T208esUo)\n\n",
          "score": 1,
          "created_utc": "2026-02-24 14:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b4vce",
          "author": "Psyko38",
          "text": "It looks like an OpenAI v2, they have the same starting goal.",
          "score": 1,
          "created_utc": "2026-02-25 11:29:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zm5v9",
          "author": "BubbleProphylaxis",
          "text": "please please make it stop. stop ai.",
          "score": 1,
          "created_utc": "2026-02-23 17:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72qom9",
          "author": "immersive-matthew",
          "text": "I have reasons to believe that AGI will be decentralized by nature. Why?   If you search for hacking over time you will see a very clear pattern of hacks increasing exponentially year over year and as you extend that trend into the future you realize that anything Centralized is a sitting duck.  Sure, AI is being used to defend against other AIs attacking and yet the pattern is still escalating.  Plus that game of cat and mouse will end up iterating so fast that humans will be pushed out.  It is why I am so confident in Bitcoin as it has clearly demonstrated it is extremely hard to hack and thus the next wave of adoption will be involuntary. Centralized AI will be in the same boat and will have to move to a decentralized platform to survive. Not a guarantee of course but that seems to be the trend.",
          "score": 0,
          "created_utc": "2026-02-24 03:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y4uff",
          "author": "Exciting-Log-8170",
          "text": "Trying to do it, have a working thermodynamic manifold prototype. Pushing next build this week. \n\nhttps://www.brickmiinews.com",
          "score": -4,
          "created_utc": "2026-02-23 13:28:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgf19j",
      "title": "Qwen3.5 updated with improved performance!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5xtzvpxdx2mg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-27 18:40:20",
      "score": 79,
      "num_comments": 6,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgf19j/qwen35_updated_with_improved_performance/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7rls14",
          "author": "vacationcelebration",
          "text": "Is this relevant for vllm deployment? Like, could or should I use/port their updated chat template into vllm as a custom one or something?",
          "score": 2,
          "created_utc": "2026-02-27 20:49:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rn6iw",
              "author": "yoracale",
              "text": "Ye it is relevant. Update the quant with our new chat template if you want",
              "score": 2,
              "created_utc": "2026-02-27 20:56:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7usk83",
                  "author": "waltpinkman",
                  "text": "How?",
                  "score": 1,
                  "created_utc": "2026-02-28 09:41:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v34e6",
          "author": "smflx",
          "text": "Qwen 3.5 updated? Or, its quants updated?",
          "score": 1,
          "created_utc": "2026-02-28 11:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vfknt",
              "author": "yoracale",
              "text": "Qwen3.5 itself and also quants. You can use our new chat templare",
              "score": 1,
              "created_utc": "2026-02-28 13:01:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7w4a9j",
                  "author": "not_ur_buddy",
                  "text": "Sorry to hijack the thread, but I'm running the new 4 bit quant 122B with llama.cpp and it still overthinks a lot in reasoning mode. I'm a little sad to give up reasoning entirely. I suspect tweaking the chat template to add system prompts would help, but I don't know how. Any advice?",
                  "score": 1,
                  "created_utc": "2026-02-28 15:26:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdf2sj",
      "title": "What‚Äôs everyone actually running locally right now?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "author": "CryOwn50",
      "created_utc": "2026-02-24 12:35:50",
      "score": 72,
      "num_comments": 110,
      "upvote_ratio": 0.97,
      "text": "Hey folks,\n\nIm curious what‚Äôs your current local LLM setup these days? What model are you using the most, and is it actually practical for daily use or just fun to experiment with?\n\nAlso, what hardware are you running it on, and are you using it for real workflows (coding, RAG, agents, etc.) or mostly testing? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o74rf56",
          "author": "Greenonetrailmix",
          "text": "Qwen 3 coder next 80B is top charts (downloads) and is performing amazing across the smaller quantizations than most model's do.",
          "score": 35,
          "created_utc": "2026-02-24 13:25:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74wbyp",
              "author": "gweilojoe",
              "text": "I‚Äôm running the Q4 version of this on an RTX 6000 Pro and it‚Äôs great - >120 tps",
              "score": 8,
              "created_utc": "2026-02-24 13:52:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76kp7j",
                  "author": "pot_sniffer",
                  "text": "Im running q6 on 7950x,64gb and a 9060XT. Id say its doing 80% of what I need it to. The rest I do with Claude",
                  "score": 5,
                  "created_utc": "2026-02-24 18:34:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7532f2",
                  "author": "Greenonetrailmix",
                  "text": "With cuda or vulkan?",
                  "score": 3,
                  "created_utc": "2026-02-24 14:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o750z8q",
              "author": "Prudent-Ad4509",
              "text": "It's great but still tends to loop in opencode. Every suggested solution worked only to the point with UD Q4. I'm going to try nvfp4 quant before moving on to the 3.5 series.",
              "score": 8,
              "created_utc": "2026-02-24 14:17:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75ah7a",
                  "author": "mister2d",
                  "text": "I wonder if the sequential thinking MCP could help.",
                  "score": 2,
                  "created_utc": "2026-02-24 15:04:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77i2b3",
                  "author": "jedsk",
                  "text": "Seeing the same",
                  "score": 1,
                  "created_utc": "2026-02-24 21:07:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75k8nd",
              "author": "nakedspirax",
              "text": "I connected it to a 3080ti with 96gb of ram and I'm able to one shot certain tasks or do 3-4 passes with coding. Running q6 GGUF and it's fine for agentic work where I can step away for a coffee and come back.",
              "score": 3,
              "created_utc": "2026-02-24 15:50:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o762uex",
                  "author": "PaMRxR",
                  "text": "Very similar here, q6 GGUF with a 3090 + 64GB RAM. Slow, but better responses than smaller models.",
                  "score": 4,
                  "created_utc": "2026-02-24 17:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74s062",
              "author": "CryOwn50",
              "text": "Yeah, I‚Äôve noticed that too",
              "score": 2,
              "created_utc": "2026-02-24 13:28:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74tewq",
                  "author": "Greenonetrailmix",
                  "text": "On my PC using Q4_K_M with my 5090 and 4090 on the Vulkan backend, I'm getting around 90 Tok/s",
                  "score": 2,
                  "created_utc": "2026-02-24 13:36:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gvk1w",
              "author": "CryOwn50",
              "text": "Yeah, it‚Äôs honestly impressive how well it holds up even in the smaller quants.When a model keeps strong reasoning and coding ability after aggressive quantization, that‚Äôs usually a sign the base architecture is [solid.No](http://solid.No) surprise it‚Äôs topping download charts   devs love performance-per-VRAM efficiency.",
              "score": 1,
              "created_utc": "2026-02-26 05:51:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74m4ch",
          "author": "Nefhis",
          "text": "I'm using Mistral Small 3.2 24b and Magistral Small 24b as local models. I built the front end myself with Xcode, with semantic memory, document uploads to chat, and libraries for RAG. My use is primarily administrative, hence the local setup, to upload documents without exposing them to providers. I have them running on a MacBook Pro M4 Max.",
          "score": 20,
          "created_utc": "2026-02-24 12:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74mjfn",
              "author": "CryOwn50",
              "text": "That‚Äôs a seriously solid setup building your own frontend with semantic memory and RAG is impressive.\n\nRunning 24B models locally on an M4 Max for private document workflows makes total sense, especially for admin use.\n\nHow‚Äôs the performance with larger document sets  still smooth, or does context size start to slow things down?",
              "score": 5,
              "created_utc": "2026-02-24 12:55:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74og4v",
                  "author": "Nefhis",
                  "text": "The models themselves are reliable, but I won't lie to you, when you approach 50k tokens it starts to slow down, although it's still very usable. I don't have the exact t/s on hand to tell you, but I'd say it's roughly between 15 and 28. For more information, I serve it with LMStudio because it allows the use of MLX models, which run better on Mac, and with KV quantization at 8... it works well enough that I don't need to rely on external providers, at least for this specific use case.",
                  "score": 4,
                  "created_utc": "2026-02-24 13:07:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74zz11",
              "author": "_Arelian",
              "text": "Let‚Äôs connect bro",
              "score": 1,
              "created_utc": "2026-02-24 14:11:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o755h8f",
                  "author": "Nefhis",
                  "text": "Hey! Sure. What exactly do you want to connect about?",
                  "score": 1,
                  "created_utc": "2026-02-24 14:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o755h8l",
          "author": "Potential-Leg-639",
          "text": "Qwen 3 Coder Next UD-Q5 (256k context)\nQwen 3 Coder UD-Q4 (128k context)\nGPT-OSS-20b UD-Q4 (128k context)\n\nPlanning/Orchestration in Opus, coding itself partly local, especially for larger things, that can run overnight and nothing can hit any limits. Sensitive stuff only local of course. Switched completely to OpenCode.\n\nAll at once on a Strix Halo, works great, love that machine - silent, powerful and power efficient.\n\nWill build a 2nd rig with parts i still have lying around to support the Strix for some tasks. Basically getting a 2nd Strix would maybe be the better idea. Or wait for Medusa Halo.",
          "score": 9,
          "created_utc": "2026-02-24 14:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75krca",
              "author": "nakedspirax",
              "text": "Are you finding a way to step away from opus for a more local thinking model? \n\nWhat strix halo machine are you using?",
              "score": 1,
              "created_utc": "2026-02-24 15:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75q7ct",
                  "author": "Potential-Leg-639",
                  "text": "Opus only for planning and orchestration, coding when planning done into very detailled level locally and with other models. Works good, Opus for coding itself is not necessary when everything was done properly before.\n\nStrix Halo 128GB, they are all quite similar performance wise, just buy the cheapest you can get ;)",
                  "score": 2,
                  "created_utc": "2026-02-24 16:17:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74rk3c",
          "author": "RomanceCherry",
          "text": "I actually like Qwen3 4B, runs pretty fast and is useful for every day questions, while keeping it private running local on iphone.",
          "score": 9,
          "created_utc": "2026-02-24 13:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s5rr",
              "author": "CryOwn50",
              "text": "That‚Äôs honestly such a sweet spot. A 4B model that‚Äôs fast, responsive, and running fully local on your iPhone? That‚Äôs peak practicality.",
              "score": -3,
              "created_utc": "2026-02-24 13:29:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75cqut",
          "author": "mister2d",
          "text": "I run Nemotron 3 Nano for my agentic flows. I have some really old hardware but I get a respectable 30-40 tokens/sec at 128k context due to the model's hybrid/swa architecture.\n\n- Dual Xeon (Ivy Bridge)\n- 256 GB DDR3\n- 2x RTX 3060 (12GB)",
          "score": 7,
          "created_utc": "2026-02-24 15:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75uwic",
          "author": "nomorebuttsplz",
          "text": "glm 5 on mac 3 ultra 512 using opencode. Good adjunct to my Claude pro subscription: if I run out of claude tokens or want to do something with sensitive data I can switch pretty seamlessly. It's a lot slower though.",
          "score": 6,
          "created_utc": "2026-02-24 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76rrsz",
              "author": "sig_kill",
              "text": "I‚Äôm seriously impressed with GLM-5, but I don‚Äôt have enough to run it locally with a single 5090 and 64gb of RAM.",
              "score": 2,
              "created_utc": "2026-02-24 19:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782gib",
                  "author": "somethingClever246",
                  "text": "I run Q6 with 128GB, 9950x, and 5080.  Ridiculously slow but good quality results",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76tcv3",
                  "author": "nomorebuttsplz",
                  "text": "your computer just has to believe in itself. The ram was in its heart all along. /s",
                  "score": 1,
                  "created_utc": "2026-02-24 19:12:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74tgg2",
          "author": "Right_Weird9850",
          "text": "ministral 3b vl instruct",
          "score": 6,
          "created_utc": "2026-02-24 13:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75yct8",
          "author": "GreyBamboo",
          "text": "I run Gemma3 4b for my chatbot and TranslateGemma for my translation tool right now :)",
          "score": 6,
          "created_utc": "2026-02-24 16:53:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74tvqo",
          "author": "NoobMLDude",
          "text": "I‚Äôm running a few local models for different uses.\n\n- Qwen3-Coder: for Coding \n- Qwen3-14B: for Meeting Assistant\n- Gemma3-7B - for basic Question Answering\n\nHere‚Äôs all the tools and setup for different Local usecases :\n\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) \n\nDisclaimer: Some of the model choices may not be relevant for you. This choice is based on my personal preference. I prefer speed over perfect answers since I like to have a quick first level overview and then delve deeper into a topic using larger models later.",
          "score": 4,
          "created_utc": "2026-02-24 13:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a1exh",
              "author": "CryOwn50",
              "text": "that disclaimer is basically the AI version of Results may vary. Side effects include speed, productivity, and mild model addiction.üòÑOr in local AI terms: Warning: optimized for vibes, not leaderboard glory.",
              "score": 2,
              "created_utc": "2026-02-25 05:39:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a6aa5",
                  "author": "NoobMLDude",
                  "text": "Yes. I think it‚Äôs important to let people know about WHY these models are used. \n\nJust like there is no such thing as ‚ÄúBest Movie‚Äù, ‚ÄúBest Food‚Äù, ‚ÄúBest Music‚Äù, there is nothing like ‚ÄúBest Model‚Äù.\n\nFor different people different things might be inportant .",
                  "score": 3,
                  "created_utc": "2026-02-25 06:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75662j",
              "author": "Potential-Leg-639",
              "text": "Disclaimer? ü§£\nBot?",
              "score": 1,
              "created_utc": "2026-02-24 14:43:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ba9r",
                  "author": "NoobMLDude",
                  "text": "Using appropriate English words is not exclusively restricted to Bots. üòâ",
                  "score": 6,
                  "created_utc": "2026-02-24 17:52:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76eapa",
          "author": "benevbright",
          "text": "qwen3-coder-next q3 on 64GB Mac.",
          "score": 5,
          "created_utc": "2026-02-24 18:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o751j8j",
          "author": "Swarley996",
          "text": "Devstral small 2 24b - coding \nGLM 4.7 flash 30b - thinking and complex queries\nMinistral 3 14b - general use\nMinistral 3 3b - small agents",
          "score": 3,
          "created_utc": "2026-02-24 14:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o752mrq",
          "author": "dave-tay",
          "text": "Qwen3-14b, fits 100% into RTX 3060 12gb. Ryzen 5600g to drive my display",
          "score": 3,
          "created_utc": "2026-02-24 14:25:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7depjb",
              "author": "Mr_Tiddy_Sucker",
              "text": "I'm running the exact same model on the same card. I'm curious, what do you use it for, if you don't mind me asking?",
              "score": 1,
              "created_utc": "2026-02-25 18:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7duht2",
                  "author": "dave-tay",
                  "text": "Just simple to medium tasks, like extract out clauses from a legal document and analyze documents for gaps. No video generation or such\n\n`$ ollama ps`\n\n`NAME         ID              SIZE     PROCESSOR    CONTEXT    UNTIL`\n\n`qwen3:14b    bdbd181c33f2    12 GB    100% GPU     16384      2 minutes from now`",
                  "score": 1,
                  "created_utc": "2026-02-25 19:47:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75lkja",
          "author": "Salt-Willingness-513",
          "text": "Mainly nemotron nano and minimax m2.5 in q8 each",
          "score": 3,
          "created_utc": "2026-02-24 15:56:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4xq",
          "author": "PvB-Dimaginar",
          "text": "I have a Bosgame M5 (AMD Strix Halo) running CachyOS Linux. For coding I‚Äôm focusing on Qwen3-Coder-Next 80B Q6.\n\nStill struggling a bit with OpenCode, my config probably needs some work around skills and MCP servers. One thing I did get working that I‚Äôm really happy about is memory sharing between Claude Code and OpenCode.\n\nGoing to improve my setup over the coming days.\n\nAnyone have experience with coding quality differences between the Q6 and Q5 models?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
          "score": 3,
          "created_utc": "2026-02-24 16:48:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a10zl",
              "author": "CryOwn50",
              "text": "That‚Äôs a clean rig  Bosgame M5 + CachyOS sounds like one of the more fun Linux desktops out there.\n\n",
              "score": 2,
              "created_utc": "2026-02-25 05:36:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hce2g",
                  "author": "PvB-Dimaginar",
                  "text": "Ooh yes! It makes me really smile when I see this beast running. I come from Windows and still need it for work, but everything else I do is on this machine!",
                  "score": 2,
                  "created_utc": "2026-02-26 08:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o763328",
          "author": "andy2na",
          "text": "Qwen3-VL:4B IQ4\\_XS always in VRAM (\\~2.25gb + context)\n\n* Frigate genAI image analysis\n* General questions in openwebui\n* Home Assistant Voice Assistant\n* Karakeep AI tagging and AI Summaries\n* Open-notebook questions and podcast generation\n* Sure Finance for transaction auto-categorization\n\nHoping Qwen3.5:5b will be a huge upgrade, qwen3-vl is already very good for these tasks",
          "score": 3,
          "created_utc": "2026-02-24 17:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p04b",
          "author": "FaceDeer",
          "text": "My workhorse is still good old Qwen3-30B-A3B-Thinking-2507. When the new Qwen models come out in that size class I'll likely switch. The main use it's being put to is summarizing and extracting information from documents, it's chugging along in the background \"digesting\" stuff into easier-to-work-with forms.\n\nI recently started messing around with some agentic stuff and I found Jan.ai's setup to be a good out-of-the-box solution for that, the old Qwen model wasn't so good at that. Hopefully the new one will be better at it.",
          "score": 3,
          "created_utc": "2026-02-24 18:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79vj45",
              "author": "jrexthrilla",
              "text": "Qwen 3.5 27b and 30b just came out",
              "score": 2,
              "created_utc": "2026-02-25 04:56:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ajmxd",
                  "author": "FaceDeer",
                  "text": "Indeed. Now I just need to wait a day or two for the dust to settle. Looking forward to it. :)",
                  "score": 1,
                  "created_utc": "2026-02-25 08:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77dk21",
          "author": "MS_Fume",
          "text": "Abliterated huihui distilled into 4B model so my phone can run it‚Ä¶. Much fun with it lol",
          "score": 3,
          "created_utc": "2026-02-24 20:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78hfbe",
          "author": "Solid-Pop-3452",
          "text": "Maybe the best LLM model is the friendships we made along the way",
          "score": 3,
          "created_utc": "2026-02-25 00:04:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c7jbc",
              "author": "0xGooner3000",
              "text": "Real",
              "score": 1,
              "created_utc": "2026-02-25 15:18:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79k4sx",
          "author": "RG_Fusion",
          "text": "My most used model at the moment is Qwen3-235b-a22b-instruct at q4-k-m. I use it as a voice assistant running in the background on my desktop. Just something to chat with and bounce ideas off of, nothing truly productive.\n\n\nI just downloaded the new Qwen3.5-397b-17b model today for some testing. On the old model I was getting 13 t/s of decode, and on this new one I'm getting 18.5, so I'll definitely be switching to it once everything is set up.\n\n\nHardware:\n- AMD EPYC 7742 CPU\n- Asrock Rack ROMED8-2T motherboard\n- 512 GB ECC DDR4 3200 MT/s RAM\n- Nvidia RTX Pro 4500 Blackwell GPU",
          "score": 3,
          "created_utc": "2026-02-25 03:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0gpc",
              "author": "CryOwn50",
              "text": "That‚Äôs a crazy clean setup üòÆ‚Äçüî• 512GB ECC + EPYC 7742 is basically home lab meets data center.\n\n",
              "score": 2,
              "created_utc": "2026-02-25 05:32:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a0yrb",
                  "author": "RG_Fusion",
                  "text": "Thanks. I built it primarily to explore LLMs as a concept, just a tool to help me learn. It's definitely proving capable though. I plan to add more GPUs in the future, which will continue pushing token generations up on these massive MoEs.",
                  "score": 2,
                  "created_utc": "2026-02-25 05:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75b4f5",
          "author": "bsenftner",
          "text": "I maintain a Wan2GP installation, it's got nearly 100 models now for image, video, voice clones, tts, and text to song. The developer of Wan2GP is really active, and tends to release a new version within a few days of new models. Qwen3 seems to have additional training to know about Wan2GP, surprisingly, and is great at helping create more complex media requiring combinations of models in concert.",
          "score": 2,
          "created_utc": "2026-02-24 15:08:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76m77s",
          "author": "fallingdowndizzyvr",
          "text": "GLM 5.",
          "score": 2,
          "created_utc": "2026-02-24 18:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76s3f7",
              "author": "sig_kill",
              "text": "Mac Studio?",
              "score": 2,
              "created_utc": "2026-02-24 19:06:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o772l3y",
          "author": "hallofgamer",
          "text": "Glm 4.7 flash handles all my needs",
          "score": 2,
          "created_utc": "2026-02-24 19:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78pz6y",
          "author": "Bitterbalansdag",
          "text": "Magistral 2 small 24b with a home made front end and MCP servers. \n\nMCP servers for: persistent external memory, web search, tasks management (basic todo list)\n\nThe chats have automatic compaction. The front end can swap system prompts mid-chat.\n\nUse for a general personal assistant, runs on a 3090 and a tailscale network.",
          "score": 2,
          "created_utc": "2026-02-25 00:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78qtkd",
          "author": "Pjbiii",
          "text": "Different tools for different tasks. I use Qwen3:14b for document sort and summarize, InternVL3.5:8b for long image descriptions, Qwen3-vl:2b for image keywords/SEO file names, gemma3:27b for outline writing, glm-4.7-flash for n8n AI Agent node. \n\nMacBook Pro (headless, the keyboard was killed by my toddler and a water bottle) M1 Max with 32GB unified memory.",
          "score": 2,
          "created_utc": "2026-02-25 00:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0mat",
              "author": "CryOwn50",
              "text": "That‚Äôs a super clean stack  right tool for each job instead of forcing one model to do everything.Respect for squeezing that much out of an M1 Max with 32GB unified memory too especially running it headless üòÇAlso‚Ä¶ toddler + water bottle is a more dangerous combo than any production outage.",
              "score": 1,
              "created_utc": "2026-02-25 05:33:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ab7rv",
          "author": "Own_Professional6525",
          "text": "Lately I‚Äôm seeing more people run smaller quantized models locally for coding and RAG since they balance performance and cost well. Curious whether people prioritize privacy, latency, or experimentation when choosing their setup.",
          "score": 2,
          "created_utc": "2026-02-25 06:59:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7abr94",
              "author": "CryOwn50",
              "text": "Yeah, that shift makes sense.\n\nSmaller quantized models hit a sweet spot good enough for coding and RAG, but light enough to run locally without crazy hardware. For most people, it‚Äôs a practical trade-off between performance and cost.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:04:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7ae7iq",
              "author": "Potential-Leg-639",
              "text": "Cloud models are still wayyyy faster, but I use the local models for sensitive tasks and coding itself, mostly over night",
              "score": 1,
              "created_utc": "2026-02-25 07:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7abwyl",
          "author": "TheAdmiralMoses",
          "text": "I'm messing with LiquidLFM models, they're supposed to be the future but they kinda just suck ngl",
          "score": 2,
          "created_utc": "2026-02-25 07:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aefjd",
              "author": "CryOwn50",
              "text": "Lmao üò≠That‚Äôs the honest take nobody wants to post.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ajmjv",
                  "author": "TheAdmiralMoses",
                  "text": "I think they'll be good once there's some more instruct tuned models, right now they don't seem to be able to keep a single train of thought and get derailed from core facts of conversations by taking tangents on the smallest context",
                  "score": 2,
                  "created_utc": "2026-02-25 08:16:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bb8fj",
          "author": "routhlesssavage",
          "text": "Just putting it here, in case anyone is looking for macos or mobile phones local LLM app. https://github.com/alichherawalla/off-grid-mobile\nhttps://news.ycombinator.com/item?id=47142003\n\nI have been using this for some time and the response is quite amazing with minimum resources.",
          "score": 2,
          "created_utc": "2026-02-25 12:18:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gu3ss",
          "author": "pieonmyjesutildomine",
          "text": "GLM-5@4bit\n\nMinimax-m2.5@8bit\n\nQwen3.5-379b@nvfp4\n\nGLM4.7@8bit",
          "score": 2,
          "created_utc": "2026-02-26 05:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h4jg1",
          "author": "Downtown_Patience_46",
          "text": "Qwen3 1.7b q8\\_0, running on my 16GB RAM + 4GB VRAM (RTX 3050) laptop, for simple translation.",
          "score": 2,
          "created_utc": "2026-02-26 07:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jjqb9",
          "author": "randygeneric",
          "text": "Qwen3.5-35BA3B-ud-q5-k-xl (or ud-q3-k-xl)  via  llama.cpp --cache-type-k q8\\_0  --cache-type-v q8\\_0  --ctx-size 131072  \n   \non a gaming laptop i7 12650 32gb RAM + rtx 4060 8gb VRAM  \nud-q3-k-xl is surprisingly accurate while being fast enough (25 token/s)",
          "score": 2,
          "created_utc": "2026-02-26 16:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qjpn9",
          "author": "NorthRemove7167",
          "text": "Pretty sure OP is an LLM judging by their comment responses",
          "score": 2,
          "created_utc": "2026-02-27 17:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79ycsk",
          "author": "Grand_Barnacle_6922",
          "text": "I'm running a few - minimax m2.5 230B and qwen3 235B seem to be my favs right now\n\nit's actually been very practical as i've been able to build a lot of custom automations around smart home and personal administration tasks.  i'm not certain on the hardware specs but my mac seems to be able to handle it just okay.\n\n",
          "score": 1,
          "created_utc": "2026-02-25 05:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aee8h",
          "author": "Outrageous_Corgi7553",
          "text": "Running Qwen2.5-Coder-32B-Q4 on M4 Mac (24GB). Actually practical for:\n\n* Code review and refactoring ‚Äî works well for Python, catches logic issues\n* Quick questions instead of hitting Claude API every time\n* First-pass RAG retrieval ranking\n\nNot using for agents ‚Äî too slow for multi-step workflows, and quality drops on complex reasoning. For anything serious I still route to Claude/GPT-4.",
          "score": 1,
          "created_utc": "2026-02-25 07:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c1y0b",
          "author": "Shouldhaveknown2015",
          "text": "I was using the new GLM MOE, then swapped to Q3 Next 80b in a Q3 unsloth gguf, then I am now using the 35b-Q3.5 model released yesterday as my daily driver.\n\nI do use other models... for instance I vibe coded a family app (Calendar, Kids rewards/chores tracking, grocery list, AI Chat bot [Running on my home lab which only has a 3060 12gb so it runs a 8b model, but I also use a text to voice model (I forget which) and it makes the AI sound like a child as I made it to act/be like BMO and sound like BMO from adventure time] and it also processes the AI responses and provides big buttons with reponses for the kids to click instead of typing. They can access it from their amazon tablet and they seem to love it.\n\nI also use the models I mention at the start with RAG and Tools. I have a vibe coded chat app with a obsidian.md vault. It pulls files automatically from the chat references to educate the AI. I can also select files to provide as part of context and select how the files should be looked at... AKA.. Is this file a research file or a lore file or a log files etc.\n\nI also built in a image generator using Flux and Klein models.\n\nI also vibe coded my own \"openclaw\" type agent, need to test it with the Q3.5 but haven't had time. It got put on the back burner, but it also has RAG and tool access to something like 28 tools. Been working on it slowly as it's the least fun project. It works well and is a limited form of my own ai research agent at this time.",
          "score": 1,
          "created_utc": "2026-02-25 14:51:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gvc4g",
              "author": "CryOwn50",
              "text": "Man, this is such a fun read you‚Äôre not just experimenting, you‚Äôre actually building a full-stack personal AI ecosystem.\n\n",
              "score": 1,
              "created_utc": "2026-02-26 05:50:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d89g3",
          "author": "Ok-Dog-4",
          "text": "Qwen 2.5 abliterated 3b on a Raspberry pi 5 8gb RAM. Actually so decent",
          "score": 1,
          "created_utc": "2026-02-25 18:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d8ihz",
          "author": "Ok-Dog-4",
          "text": "Also tiny llama works great on the Pi. Cost friendly and completely local. Training for finetune obviously must be outsourced but models under around 3.5b function just fine on new pi‚Äôs. Ollama btw",
          "score": 1,
          "created_utc": "2026-02-25 18:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d8n8r",
              "author": "Ok-Dog-4",
              "text": "Forgot but models are quantified from 8gb to 2.5 to work on the pi",
              "score": 1,
              "created_utc": "2026-02-25 18:08:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lstkh",
          "author": "andynarwhal_jr",
          "text": "Qwen3-4B-Instruct on a surface laptop studio 2 (which has a 4050 dGPU with 6GB of vRAM)\n\nim using 'jan .ai' to run it and its decent enough. its quick to start up, the responses are fast and the fans dont go that crazy  \n  \ni mainly use it when im programming and im only asking it simple enough questions. basically i just want something where i dont have to use a search engine as much or spend time looking through the docs. for more complex stuff i would still use an online LLM but its nice to be able to use this local one for most of the questions i have",
          "score": 1,
          "created_utc": "2026-02-26 23:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qj0e1",
          "author": "ClayToTheMax",
          "text": "I‚Äôve been experimenting with various models on my custom v100 server. I‚Äôve got 4 16gb v100s on an old slot server that I rednecked into getting some power. Vllm fails with setup and causes a bunch of issues, so I generally use ollama which is pretty safe and easy. I also just started with LM Studio, which has a bunch more configs that are easier to use imo.\nI‚Äôve been working on an ai powered writers tool for the past year. The goal with that is to get something other than slop, and have it assist rather than railroad and write. Been experimenting with RAG systems and making them interactive on a UI. Etc. lots of ideas, but I can run just about anything reasonable. Just tried the new 80b qwen coder next and on my setup I can get it to work, but with reasonable context it‚Äôs only like 20ish t/s which for me is unusable. I honestly like glm 4.7, I haven‚Äôt had the issues others have had. I stay in the 30b model zone and I can get 40-50ish t/s depending on the model and setup, which I think is great. Small models zoom way faster of course. My biggest problem is my server sounding like a air plane and my wife getting pissed off bc of that lol",
          "score": 1,
          "created_utc": "2026-02-27 17:41:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p16a",
          "author": "Ok-Patient6458",
          "text": "I run a blog for local LLM news and insight at [https://lftw.dev](https://lftw.dev)",
          "score": 1,
          "created_utc": "2026-02-24 18:53:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbu7sx",
      "title": "M4 Pro 48 or M4 Max 32",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "author": "Mammoth-Error1577",
      "created_utc": "2026-02-22 18:58:33",
      "score": 42,
      "num_comments": 38,
      "upvote_ratio": 0.88,
      "text": "I got my machine renewed at work a week ago.\n\nThey rejected my request of a Mac studio with 128 GB and instead approved a MacBook M4 Pro with 48GB and 512.\n\nWell I finally got around to checking and they actually gave me a more expensive M4 Max but with 32 GB and 1TB instead.\n\n\nIn my previous chatting with Gemini it has convinced me that 128 GB was the bare minimum to get a sonnet level local LLM.\n\nWell I was going to experiment today and see just what I could do with 48 and to my surprise I only had 32, but a superior CPU and memory bandwidth.\n\n\nIf my primary goal was to run coding a capable LLM, even at the cost of throughout, I assume 48 is vastly superior.  However if the best model I can run with 48 (+ containers and IDE and chrome etc.) is really dumb compared to sonnet I won't even use it.\n\nI'm trying to decide if it's worth raising a fuss over getting the wrong, more expensive laptop. I can experiment with a very small model on the current one but unless it was shockingly good I don't think that experiment would be very informative.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6toq7h",
          "author": "j00cifer",
          "text": "M5 ultra studio is coming out this year with a reported max RAM of 1TB.\n\n1TB RAM.",
          "score": 26,
          "created_utc": "2026-02-22 19:22:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tpdap",
              "author": "jiqiren",
              "text": "üòç want it so good ü•∞ M5 Ultra 1TB??? Yes please!",
              "score": 7,
              "created_utc": "2026-02-22 19:25:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v9irr",
                  "author": "gingerbeer987654321",
                  "text": "Only 1?  Get 4 and do the Thunderbolt raid thing",
                  "score": 5,
                  "created_utc": "2026-02-23 00:20:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71kvcd",
                  "author": "LimiDrain",
                  "text": "Does this unified memory work as fast as VRAM or it's close to normal RAM speeds?",
                  "score": 1,
                  "created_utc": "2026-02-23 23:38:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uawrt",
              "author": "sav22v",
              "text": "But you'll have to sell your kidneys and children to pay for it...",
              "score": 6,
              "created_utc": "2026-02-22 21:14:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6uq4zs",
                  "author": "j00cifer",
                  "text": "I‚Äôm making the case to them now.",
                  "score": 2,
                  "created_utc": "2026-02-22 22:32:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uyghh",
              "author": "grim-432",
              "text": "With the current price of ram, what‚Äôll that cost?  $25,000?",
              "score": 3,
              "created_utc": "2026-02-22 23:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x4h53",
                  "author": "ijontichy",
                  "text": "They would have locked in RAM costs for this year before the RAMpocalypse. But do you think they'll hold prices steady? ü§î",
                  "score": 1,
                  "created_utc": "2026-02-23 08:18:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71z016",
                  "author": "GonzoDCarne",
                  "text": "Ram price does not apply to oems like Apple. Due to many things that someone might want to go into detail in a long thread. My hard guess is they will target 15k or 19999. M3 Ultras with 512GiB go for 10k un the US since before the ram surge and today.",
                  "score": 1,
                  "created_utc": "2026-02-24 00:56:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6trf6a",
              "author": "Mammoth-Error1577",
              "text": "Unfortunately not an option. The only studio I could get is also 36GB.",
              "score": 2,
              "created_utc": "2026-02-22 19:35:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6u9uza",
              "author": "ZealousidealShoe7998",
              "text": "at that level what llm would one even use to reach comercial levels ?",
              "score": 1,
              "created_utc": "2026-02-22 21:08:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6xj37x",
              "author": "iezhy",
              "text": "Given current ram prices (and Apple markup), this probably will be out of reach for most users",
              "score": 1,
              "created_utc": "2026-02-23 10:41:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70hc2o",
                  "author": "j00cifer",
                  "text": "I don‚Äôt see why a 2nd kidney is so important to people",
                  "score": 1,
                  "created_utc": "2026-02-23 20:17:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7134m5",
              "author": "Jealous_Incident7978",
              "text": "Starts getting funny that we drop $$$$ on a 1TB Ram M5 ultra studio to run open weight models that is essentially free. üòÜ imagine paying something similar for qwen 3.5 / DeepSeek etc just to run the model locally",
              "score": 1,
              "created_utc": "2026-02-23 22:03:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o729hxh",
                  "author": "j00cifer",
                  "text": "1 TB RAM.",
                  "score": 1,
                  "created_utc": "2026-02-24 01:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6u93hc",
          "author": "No_Success3928",
          "text": "Hahaha sonnet level üôÑ classic gemini hallucinations",
          "score": 16,
          "created_utc": "2026-02-22 21:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o737jwu",
              "author": "WildRacoons",
              "text": "Clearly doesn‚Äôt think very highly of sonnet",
              "score": 2,
              "created_utc": "2026-02-24 05:38:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6u2tks",
          "author": "BisonMysterious8902",
          "text": "I hate to break it to Gemini, but you can't get anywhere close to Sonnet level with 128Gb. Can you get something usable? Sure, but it'll never match frontier level models. Even a Studio with 512Gb. That's just the current state of things.",
          "score": 10,
          "created_utc": "2026-02-22 20:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ughj5",
              "author": "meTomi",
              "text": "Current state? When you think your home pc can compete with million dollar racks in server rooms?\nOn the other hand yes technology is getting better and you can run better and bigger models at home.",
              "score": 1,
              "created_utc": "2026-02-22 21:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ve5yt",
          "author": "MrRandom04",
          "text": "Only open source LLMs that compete with Sonnet 4.6 / Opus 4.6 are GLM 5 and Kimi K2.5. Of these, only GLM 5 is super reliable for agentic coding. That model is far too big for anything less than like 512gb ram. For 32gigs, you can consider the Qwen series UD quants and then have a workflow where you shell out to an API provider of GLM 5 or even just Sonnet / Opus for planning and big design / knowledge level tasks while the manual editing and coding is done by Qwen. The latest ones are very good at stuff like Python and really good for their size.",
          "score": 10,
          "created_utc": "2026-02-23 00:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4d3s",
          "author": "Expert-Reaction-7472",
          "text": "i dont think id make a fuss about this to any place i've ever worked.\n\nnice thing about being self employed is if i want to splurge on a machine i can. Which usually means I have something decent but not mind blowingly expensive cos it's my own money and i'd rather spend the extra on a holiday or something.",
          "score": 3,
          "created_utc": "2026-02-22 20:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uqm3v",
          "author": "ComfortablePlenty513",
          "text": "always prioritize memory. M4 architecture is fundamentally better than previous gen for inference",
          "score": 3,
          "created_utc": "2026-02-22 22:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w40t7",
          "author": "Sharp-Mouse9049",
          "text": "32GB in 2026 for serious local LLM work is basically consumer-tier. I don‚Äôt care how fast the M4 Max is ‚Äî if you‚Äôre constantly forced into tiny quants or can‚Äôt load 70B comfortably, you‚Äôre artificially capping your experimentation. Bandwidth doesn‚Äôt matter if the model doesn‚Äôt fit. RAM is the ceiling.",
          "score": 2,
          "created_utc": "2026-02-23 03:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v9e8v",
          "author": "pondy12",
          "text": "Get a HP ZBook Ultra G1a, Ryzen AI Max+ PRO 395, 64gb - 128gb of ram, 256gb/s ram bandwidth. Will be 1/4th the price. \n\n",
          "score": 1,
          "created_utc": "2026-02-23 00:20:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xhgbi",
              "author": "Confident-Strength-5",
              "text": "It also has 256gb/s bandwidth, so‚Ä¶\nLLMs really like bandwidth‚Ä¶",
              "score": 1,
              "created_utc": "2026-02-23 10:25:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vlezt",
          "author": "midz99",
          "text": "Vram or whatever mac calls it is everything. Higher the better. really you need 128gb to even get close to something worth testing.",
          "score": 1,
          "created_utc": "2026-02-23 01:30:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73fuxe",
          "author": "Coyote_Android",
          "text": "After playing around with 32 GB for a while, do you think 48 GB would allow for a significantly better model? Not necessarily for coding though. Just language generation. I'm facing a similar decision.",
          "score": 1,
          "created_utc": "2026-02-24 06:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74mvru",
              "author": "Mammoth-Error1577",
              "text": "I haven't had the opportunity to do anything that seemed usable yet. This has been my first attempt with a local model though, and I naively thought it would be some simple drop in for a slower and dumber version of a cloud model, but the experience I had was so poor that I couldn't see myself using it for anything. I definitely need to do more tweaking, I didn't even get as far as trying to change any configuration, as I didn't even know that could be changed!",
              "score": 2,
              "created_utc": "2026-02-24 12:57:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7518i4",
                  "author": "Coyote_Android",
                  "text": "But you only have coding use cases? For playing around (not coding though afaik) you might wanna give [https://msty.ai](https://msty.ai) a shot",
                  "score": 1,
                  "created_utc": "2026-02-24 14:18:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6uivgp",
          "author": "DistanceSolar1449",
          "text": "M4 max has way faster memory bandwidth\n\n48gb is not enough for Qwen3 next \n\nJust stick with 32gb",
          "score": 0,
          "created_utc": "2026-02-22 21:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6unjkw",
              "author": "Mammoth-Error1577",
              "text": "I just tried  qwen2.5-coder:14b in open code and it was extremely dumb and worse than copying and pasting from a web browser (on an empty repo)\n\nI tried qwen2.5-coder:32b 1st and /init wasn't doing anything so Gemini told me to downgrade.\n\nBut /init didn't do anything after downgrading either.\n\nAll I could get it to do was spit out code that it would tell me to put into the file myself instead of doing it itself, and then the code wasn't even syntactically correct.\n\nI'm super shocked it was so bad, there was no way I was doing it correctly.",
              "score": 0,
              "created_utc": "2026-02-22 22:18:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zftss",
                  "author": "Djagatahel",
                  "text": "What is that /init you're talking about?",
                  "score": 1,
                  "created_utc": "2026-02-23 17:24:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tmf6f",
          "author": "Svyable",
          "text": "Surprised how much I get out of my 24 Pro M4 I have like 100Gb running in Brave browsers no problem. \n\nModel sizes are coming down. Don‚Äôt complain innovate",
          "score": -1,
          "created_utc": "2026-02-22 19:11:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgc62q",
      "title": "# Your RAM Is Secretly an AI Accelerator",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rgc62q/your_ram_is_secretly_an_ai_accelerator/",
      "author": "use-one_of-these",
      "created_utc": "2026-02-27 16:58:15",
      "score": 38,
      "num_comments": 64,
      "upvote_ratio": 0.66,
      "text": "**CaSA: Ternary LLM Inference on Commodity DRAM**\n\n*February 2026*\n\n---\n\n## The Hidden Compute Inside Every Memory Chip\n\nEvery stick of RAM in your computer has a hidden trick. When you force two rows of memory cells to turn on at the same time ‚Äî which violates the timing spec, but physically works ‚Äî the electrical charges mix together and you get a free AND operation across tens of thousands of bits simultaneously. Nanoseconds. Almost zero energy.\n\nThis has been measured. The CMU-SAFARI group tested it 79 million times across 120 real DDR4 chips. Zero failures in the reliable operating window. The physics works. It has always worked. Every DRAM chip ever manufactured can do this.\n\nThe compute capacity inside the chip is over 1,000x more than the memory bus can deliver. It's just sitting there, unused.\n\n## Why Nobody Could Use It\n\nThe compute exists, but previous attempts to harness it for anything useful ran into a fatal problem: to set up the operation, you need to copy data around inside the chip (called RowCopy). On commodity DDR4, RowCopy has a 16.3% bit error rate. That's not a rounding error ‚Äî that's one in six bits flipped. Neural network inference is impossible at that error rate.\n\nEvery prior approach to \"Processing-in-Memory\" either required custom silicon (Samsung HBM-PIM, SK Hynix AiM, UPMEM) or stopped at demonstrating basic bitwise operations without building anything useful on top.\n\n## The Fix: Stop Copying, Start Sacrificing\n\nOur fix is embarrassingly simple.\n\nIn a neural network, there are two kinds of data:\n- **Weights** ‚Äî the model's learned knowledge. Permanent. Written once, read millions of times.\n- **Activations** ‚Äî the intermediate values flowing through the network. Temporary. Freshly computed every single step, then thrown away.\n\nThe charge-sharing trick has an asymmetry: the first row you activate survives intact. The second row gets overwritten with the AND result.\n\nSo: activate the weight row first (it survives), then the activation row second (it gets consumed). The weights are preserved. The activations were going to be discarded anyway. You get the AND result with essentially zero errors ‚Äî no RowCopy needed.\n\nError rate drops from 16.3% to less than 0.000004%. Four orders of magnitude. That's the entire paper in one paragraph.\n\nWe call this the **activation-sacrificial protocol**, and the full architecture **CaSA** (Charge-sharing Activation-Sacrificial Architecture).\n\n## Why Ternary Changes Everything\n\nThis trick works cleanly only at one specific precision: **ternary** ‚Äî where neural network weights are restricted to {-1, 0, +1}.\n\nWhy? Because multiplying a ternary weight by a binary activation is literally just an AND gate. That's exactly what charge-sharing gives you for free. You encode +1 as one binary row, -1 as another, AND each with the activation bits, and the difference gives you the matrix-vector product.\n\nAt higher precisions (4-bit, 8-bit), the number of AND operations per weight multiplies rapidly. Only at ternary does it collapse to something commodity DRAM can handle competitively.\n\nThe industry currently evaluates ternary on the wrong axis. The question people ask is: \"Does ternary match INT4 accuracy on GPUs?\" Answer: roughly yes (Microsoft's BitNet b1.58 matches LLaMA quality), but GPUs aren't optimized for ternary, so there's no speed benefit. Conclusion: ternary seems pointless.\n\nThat analysis completely misses the memory axis. Ternary is the **only** precision at which every RAM chip in the world becomes a neural network accelerator. The reason nobody saw this is that nobody had demonstrated commodity DRAM PIM actually working for inference until now.\n\n## Why Now\n\nThis couldn't have been done two years ago. Microsoft published BitNet b1.58 ‚Äî the first production-quality ternary language model ‚Äî in February 2024. Before that, there were no ternary models worth running. The DRAM physics has existed since the 1970s. The charge-sharing trick has been measured since 2017. But until ternary models arrived, there was nothing to connect the compute substrate to the workload. CaSA is what happens when those two threads finally meet.\n\n## What We Actually Built\n\nWe designed a complete inference pipeline for **BitNet b1.58-2B-4T** ‚Äî a real 2-billion-parameter ternary language model from Microsoft ‚Äî running on a single 8 GB DDR4 DIMM ($15-25) with an FPGA controller.\n\nThe DRAM handles the heavy matrix multiplications via charge-sharing AND. The FPGA handles the lightweight operations: popcount (counting 1-bits in the result), accumulation, RMSNorm, SiLU activation, and softmax. The model fits in a single DIMM with room to spare.\n\n**Current speed: 1.8 tokens per second on one DIMM.**\n\nThat's slow. A CPU running llama.cpp does 15-30 tok/s on the same hardware. We know. Here's why it doesn't matter:\n\n## The Bus Bottleneck (and Why 1.8 Is a Floor, Not a Ceiling)\n\nThe 1.8 tok/s is almost entirely bus overhead. Here's where the time goes:\n\n| Component                              | Share of Inference Time |\n| :------------------------------------- | :---------------------: |\n| **Writing activations to DRAM (Bus)**  |         **44%**         |\n| **Reading results from DRAM (Bus)**    |         **44%**         |\n| Charge-sharing AND (Compute)           |           6%            |\n| FPGA overhead                          |           6%            |\n\nThe in-DRAM compute takes 6% of total time. The other 88% is just moving data through the 64-bit DDR4 bus. The chip can compute 1,000x faster than the bus can deliver data. You're looking at a thousand-lane highway feeding through a single-lane toll booth.\n\nThis means every improvement that reduces bus traffic produces dramatic speedups:\n\n## The Scaling Path\n\n| Configuration                          | Tokens/sec  | What it takes                            |\n| :------------------------------------- | :---------: | :--------------------------------------- |\n| **1 DIMM (Baseline)**                  |   **1.8**   | **Works today on unmodified DDR4**       |\n| 4 DIMMs                               |     7.6     | $60 of commodity RAM, no chip changes    |\n| 4 DIMMs + Batching                    |     ~35     | Firmware optimization only               |\n| **+ In-DRAM Popcount**                 | **60‚Äì166**  | **~2,000 gates per bank (~$0.10/DIMM)**  |\n| LPDDR5X (16-ch) + Popcount            |     169     | Phone/laptop memory, single package      |\n| HBM2 (8-ch) + Popcount                |     229     | Server memory                            |\n\nThe popcount register is the single biggest lever. It's a tiny bit-counting circuit ‚Äî about 2,000 logic gates ‚Äî that counts the 1-bits in a DRAM row without reading the data out through the bus. This eliminates the entire 44% read bottleneck. Samsung patented this exact circuit in 2014. It has never been shipped in any product.\n\n## It's Surprisingly Robust\n\nA natural question: if you're doing computation by mixing analog charges, how fragile is this?\n\nNot very. Even at a bit error rate of 0.01% ‚Äî ten thousand times worse than what was measured on real hardware ‚Äî model output quality degrades by less than half a percent. The safety margin between measured reliability and the point where accuracy starts to suffer is roughly 50,000x. Commodity DRAM, within its validated timing window, is not fragile.\n\n## Manufacturer Compatibility (This Matters)\n\nNot all DDR4 works:\n\n- **SK Hynix C-die (2018-2020):** Confirmed compatible. This is our target platform.\n- **Micron DDR4:** Likely compatible. The FCDRAM study tested 256 chips from two anonymized manufacturers (believed to be SK Hynix and Micron) with ~95% success rate.\n- **Samsung DDR4: Incompatible.** Zero processing-using-DRAM operations work on Samsung dies. This appears to be a hard incompatibility from proprietary internal circuitry, not a calibration issue.\n- **Newer SK Hynix (D-die, M-die):** Unknown. More aggressive RowHammer protections may interfere.\n\nIronically, Samsung holds the key popcount patent and could fix their incompatibility. If they did both ‚Äî made their chips charge-sharing compatible and added the popcount register ‚Äî they'd be in the strongest competitive position of any memory manufacturer.\n\n## A Message to Memory Manufacturers\n\nWe've identified exactly what's bottlenecking this architecture, and exactly what would fix it. Here's what we'd ask for, ordered from cheapest to most impactful:\n\n**Tier 0 ‚Äî Costs nothing but coordination:**\n\n- **A PIM mode bit in the Mode Register Set.** One bit that tells the chip: \"I'm about to do charge-sharing operations, suppress RowHammer protections and bypass on-die ECC for the next N cycles.\" This is a spec change, not a silicon change. It would immediately unblock DDR5 (which is currently unusable for PIM because its mandatory on-die error correction scrambles the charge-sharing results). It would also eliminate the ~5% throughput tax from RowHammer guard intervals on DDR4. The catch: this requires JEDEC coordination, which typically takes 3-5 years. But the silicon cost is literally zero.\n\n- **Publish your charge-sharing timing parameters.** Right now, finding the optimal timing for dual-row activation on a specific die revision requires reverse-engineering via tools like DRAM Bender. If manufacturers documented the safe operating window per die revision, it would replace months of characterization with a datasheet lookup.\n\n**Tier 1 ‚Äî Tiny silicon changes, massive impact:**\n\n- **In-DRAM popcount register (~2,000 gates/bank, <0.3% die area, ~$0.10/DIMM).** This is the single highest-impact change. After a charge-sharing AND, the result sits in 65,536 sense amplifiers. Currently, we have to read all 8,000 bytes out through the bus just to count the 1-bits. A popcount register counts them in-place and returns a single 16-bit number. This eliminates 44% of total inference time ‚Äî the entire read bottleneck. Samsung patented exactly this circuit in 2014. It's combinational logic (no clock, no pipeline, no state machine), so it works at full speed even on DRAM-process transistors. It's a passive reduction circuit, not a processor.\n\n- **Reliable RowCopy.** Our activation-sacrificial protocol exists because RowCopy is broken at 16.3% BER. If manufacturer calibration (like PUDTune's sense amplifier offset compensation) brought RowCopy BER below 0.01%, two things happen: (1) we can distribute activation data inside the chip without touching the bus, roughly doubling throughput even without popcount, and (2) we can build a \"software-defined popcount\" ‚Äî an adder tree constructed entirely from sequences of charge-sharing AND/OR/NOT operations inside the chip, using the SIMDRAM approach. This would break the bus bottleneck on completely unmodified DRAM with zero additional circuitry. It would be slower than a dedicated popcount register (~100-200 charge-sharing steps per accumulation vs. one cycle), but it would work today if RowCopy were reliable.\n\n**Tier 2 ‚Äî Moderate silicon, transformative results:**\n\n- **Per-bank activation register (a few hundred thousand transistors per bank).** Right now, we rewrite the activation data from the bus for every single weight row ‚Äî because charge-sharing destroys the activation row each time. A small static register per bitline would hold the activation vector and drive it onto the bitlines repeatedly without being destroyed. Combined with popcount, this eliminates ALL bus transfers during compute. Bus utilization drops from 88% to under 5%. A single DIMM becomes deeply compute-bound rather than bus-bound.\n\n- **Wider rows.** This is counterintuitive: the industry trend is toward narrower rows (2 KB in LPDDR5X and HBM, vs 8 KB in DDR4) for latency and power reasons. But for PIM, row width is the fundamental unit of parallelism ‚Äî each charge-sharing AND processes one full row simultaneously. DDR4's 8 KB rows pack 25 neurons per AND operation. LPDDR5X's 2 KB rows pack only 6, requiring 4x more sequential cycles. A PIM-optimized memory would maximize row width, not minimize it. DDR4's wide rows are an accidental PIM advantage that future memory standards should preserve.\n\n**The bottom line for manufacturers:** The Tier 1 popcount register alone converts CaSA from a proof-of-concept (1.8 tok/s) to a competitive inference engine (60-166 tok/s) at a cost of ~$0.10 per DIMM. Combined with the Tier 2 activation register, every DIMM in every server, laptop, and phone becomes an LLM inference accelerator ‚Äî using memory the customer has already paid for. The business case is not \"sell a new product.\" It's \"make the product you already sell billions of dramatically more valuable.\"\n\n## What This Paper Is Not\n\nWe want to be clear about what we haven't done:\n\n**No hardware validation yet.** Everything is simulation calibrated against the SiMRA measurement dataset. The physics is proven (79M trials), but our specific end-to-end pipeline hasn't run on physical DIMMs. That's the next step.\n\n**Prefill is painfully slow.** Processing an input prompt takes roughly a minute for a typical short prompt on a single DIMM. This architecture works best for short prompts and long-running sessions ‚Äî not document summarization or long conversations. A hybrid approach where the CPU handles prompt processing and CaSA handles generation is the practical near-term path.\n\n**The FPGA prototype is expensive and power-hungry.** The research platform costs thousands of dollars and draws 42W. A production controller would be 10-40x cheaper and draw a fraction of the power. The DRAM itself costs $15.\n\n**We depend on ternary models existing.** If the industry standardizes on 4-bit quantization and ternary models never materialize beyond BitNet, CaSA becomes less compelling. We're betting that the memory-side advantage of ternary ‚Äî which this paper is the first to demonstrate ‚Äî will shift that calculus.\n\n**This is inference only.** CaSA accelerates running a trained model, not training one. Training requires high-precision gradients and backpropagation ‚Äî fundamentally different operations that charge-sharing can't help with.\n\n## The Actual Contribution\n\nThe contribution is not 1.8 tokens per second. That number is a floor measured through a straw.\n\nThe contribution is three things:\n\n**1. The activation-sacrificial protocol works.** You can do reliable neural network inference on commodity DRAM by exploiting the asymmetric survival property of charge-sharing. No RowCopy. No custom silicon. Four orders of magnitude better reliability than any prior approach.\n\n**2. The bus is the only bottleneck.** 88% of inference time is bus traffic, 6% is compute. The internal compute capacity of commodity DRAM is not the limiting factor ‚Äî it exceeds what the bus can deliver by 1,000x. Every future improvement is about getting data to and from the array faster.\n\n**3. The path from floor to ceiling is concrete and quantified.** We trace every step from commodity hardware to optimized silicon: multi-DIMM scaling, batch processing, popcount registers, activation registers, next-generation memory standards. Each step has a cost, a throughput gain, and a dependency. Nobody has to guess what comes next.\n\n## What This Could Mean\n\nIf this works at scale, the memory already in your laptop, phone, or server becomes an AI accelerator ‚Äî without buying new hardware. Not a toy demo. A real language model, running on the RAM you already own, at a fraction of the power draw of a GPU. The compute has always been there. We just didn't have the right model format to unlock it.\n\nNobody knows how fast this could become if memory manufacturers designed for it. This paper provides the first data to inform that question.\n\n---\n\n*Full technical report with complete derivations, error analysis, cross-technology projections, patent landscape, and hardware validation plan: [github.com/pcdeni/CaSA](https://github.com/pcdeni/CaSA)*\n\n*This work was conducted by an independent researcher using AI-assisted analysis tools. The core architectural insights, all design decisions, and every claim were verified by the human author. All errors are the author's responsibility.*",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgc62q/your_ram_is_secretly_an_ai_accelerator/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7qi9s8",
          "author": "ReputationTop484",
          "text": "Could you tell the AI to make a TLDR section",
          "score": 66,
          "created_utc": "2026-02-27 17:37:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s9sib",
              "author": "PacmanPence",
              "text": "TLDR: analog computing on DDR4 ram is very fast, but ram was not designed for this, as the digital reading and writing of data takes 88% of the time. There are boosts ranging from deployable to already distributed ram to requiring sticks of ram be designed for this purpose.",
              "score": 6,
              "created_utc": "2026-02-27 22:53:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7qp4r0",
              "author": "KriosXVII",
              "text": "TLDR \"Wow man it sure would be nice for LLMs if RAM was faster\" ",
              "score": 17,
              "created_utc": "2026-02-27 18:10:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qpr09",
                  "author": "Intraluminal",
                  "text": "I don't think it will work, but that's NOT at all what he's suggesting.",
                  "score": 12,
                  "created_utc": "2026-02-27 18:13:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7s1vwm",
              "author": "dreaming2live",
              "text": "RAM good",
              "score": 3,
              "created_utc": "2026-02-27 22:10:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rllem",
          "author": "1-6",
          "text": "It sounds so cool, it has to be made up.",
          "score": 11,
          "created_utc": "2026-02-27 20:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7unku5",
              "author": "YearnMar10",
              "text": "I am sure he wrote it all by hand.\n\nEdit: sorry, assumed gender‚Ä¶\n\n/s",
              "score": 2,
              "created_utc": "2026-02-28 08:53:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qin1o",
          "author": "Classic_Chemical_237",
          "text": "AI writing novels",
          "score": 31,
          "created_utc": "2026-02-27 17:39:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qoda9",
              "author": "tempfoot",
              "text": "Is this where all this is headed?   A world where everyone *needs* AI and LLMs to function, because information is only exchanged in bloated, terribly unedited slop dumps written for ‚Äúengagement‚Äù?  We will all need an AI to ‚Äúdecode‚Äù summaries of the important bits of a given message payload?   We all gonna round-trip our every thought through a linguistic probability function (in addition to the one already between our ears) because nobody can actually write or edit anything any more, like some kind of reverse compression engine?  \n\n/rant",
              "score": 15,
              "created_utc": "2026-02-27 18:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qsorl",
                  "author": "Big_River_",
                  "text": "![gif](giphy|x88e1awUi05by|downsized)\n\nyes! you are correct sir - half the world comms by gif and the other half by rage bait tweet - the sliver of text wall in between is the worst of them all though? yes you are correct sir",
                  "score": 4,
                  "created_utc": "2026-02-27 18:26:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qm9vg",
          "author": "Intraluminal",
          "text": "This is a fascinating idea and it works in theory, but I think you've hit a problem that I have hit before with AI - you never asked the AI to look at the idea critically and asked, \"Why WON'T this work.\"\n\nThere are several reasons why this won't work.\nThe first is economic or social: Manufacturers have no incentive to cannibalize HBM sales where they have very high profit margins, versus memory where the margins are very thin.\n\nBut the even bigger problems are:\nThis will be blocked by the built-in CPU memory controllers. Yes, this could be fixed, but not cheaply or quickly. \nHEAT - GPUs are made to run continuously and hot, memory will melt down if you ask it to do all the repeated cycles you'd need to do in order to make this work.",
          "score": 9,
          "created_utc": "2026-02-27 17:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qof9f",
              "author": "use-one_of-these",
              "text": "HBM is addressed in the full paper. Cheap and quick is also addressed in the full paper.   \n**The heat concern is valid but actually works in CaSA's favor compared to what you might expect.**\n\nCaSA doesn't run DRAM \"continuously and hot\" like a GPU. The key difference:\n\n1. **It's massively parallel but low-duty-cycle per row.**¬†A single DRAM row activates for \\~60ns, does one AND operation, then closes. The compute is the charge-sharing event itself ‚Äî there's no sustained switching like in logic gates. Each row fires once per matrix-vector step, not millions of times per second.\n2. **The thermal budget is actually better than normal operation.**¬†Regular DRAM refresh already activates every row every 64ms (tREFI). CaSA inference adds activations on top of that, but the duty cycle per row is still very low. We address this in the paper ‚Äî at sustained inference, the additional self-heating is modest because the activation pattern is sequential across rows, not hammering the same rows repeatedly.\n3. **DRAM thermal throttling is well-understood.**¬†DDR4/DDR5 already have built-in thermal sensors and throttling mechanisms. If temperature rises, refresh rate doubles (from 64ms to 32ms tREFI) ‚Äî this costs throughput but protects the hardware. Our throughput estimates already account for this.\n4. **The real thermal enemy is the bus, not the array.**¬†88% of inference time is spent moving data over the DDR bus. CaSA's whole point is to¬†*avoid*¬†that transfer ‚Äî the compute happens where the data already lives. Less bus activity = less I/O power = less heat than the GPU alternative for the same operation.\n\nSo the short answer: DRAM runs cooler doing CaSA inference than a GPU does running the same model, because you're replacing billions of data transfers with in-place charge-sharing operations. ",
              "score": 4,
              "created_utc": "2026-02-27 18:06:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qs21l",
                  "author": "Intraluminal",
                  "text": "I would suggest that you attempt to contact the memory manufacturer where you already know this works - as you said, your target platform - and see if they will pursue it, since that might be their advantage over other manufacturers. ",
                  "score": 4,
                  "created_utc": "2026-02-27 18:23:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qm06r",
          "author": "No_Draft_8756",
          "text": "That sounds very interesting.",
          "score": 2,
          "created_utc": "2026-02-27 17:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qsxzt",
          "author": "Big_River_",
          "text": "could you have told me about this a year ago when i could afford to care about RAM?",
          "score": 2,
          "created_utc": "2026-02-27 18:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rl4dl",
          "author": "NarrativeNode",
          "text": "If you didn‚Äôt spend more time writing it than I would reading it, I‚Äôm not reading it.",
          "score": 1,
          "created_utc": "2026-02-27 20:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rw770",
              "author": "LambdasAndDuctTape",
              "text": ".... they obviously did, though? Assuming the post isn't a complete fabrication.\n\nThe time it would've taken to simulate these runs, analyze and compile this data would be massive. But Redditors have a problem because he summed it up with AI? Have you worked a white collar job in the last 4 years? Welcome to today, I guess (can't believe this has to even be said in an AI forum). ",
              "score": 10,
              "created_utc": "2026-02-27 21:41:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ux4g7",
                  "author": "use-one_of-these",
                  "text": "it took 5 or 6 days: creativity, persistence. AI is not human, you can always tell them off, but with human slop...",
                  "score": 1,
                  "created_utc": "2026-02-28 10:26:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7rq2fo",
              "author": "ForsookComparison",
              "text": "I will read this post exactly as much as OP wrote it.. which is not at all.",
              "score": 3,
              "created_utc": "2026-02-27 21:11:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7tc580",
          "author": "jezwel",
          "text": "This could definitely help with consumers retaining control over their own LLMs.\n\nSo next steps? \n\n* PoC for consumers to test their RAM and submit results?\n* entry level ternary LLM?\n* manufacturer contact in the hope of updating consumer and server DIMM design?",
          "score": 1,
          "created_utc": "2026-02-28 02:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uqm1d",
              "author": "use-one_of-these",
              "text": "**Testing RAM**¬†‚Äî¬†The CMU-SAFARI group already has open-source tools for this (SoftMC, DRAM Bender), but they require FPGA hardware. A pure software version that works through the CPU memory controller would be ideal, but it's limited by what timing violations the IMC will actually let you issue. This is a real gap right now.  \n**Ternary models**¬†‚Äî BitNet b1.58 from Microsoft exists and works. The bottleneck isn't the model, it's that there's no hardware to run it faster than a CPU can. If CaSA gets a working prototype, ternary models suddenly have a reason to exist beyond \"slightly smaller than INT4.\"  \n**Manufacturer contact**¬†‚Äî yes,¬†that is what publicly sharing the idea intends to achieve. After a thorough back and forth with different AIs, there no way to reliably contact them, get them interested, and see them realise they can do it without me; or to patent this with realistic enforcement in case they infringe on it.",
              "score": 1,
              "created_utc": "2026-02-28 09:22:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7tkd9n",
          "author": "CalvinBuild",
          "text": "This is one of the best ‚Äúmake local compute cheaper‚Äù posts I‚Äôve seen in a while. The sacrificial-activation angle is clever because it matches ternary weights and avoids the usual RowCopy fragility story. Biggest ask: show the smallest possible hardware proof (one DIMM, one layer, repeatable outputs), plus a clear compatibility matrix (which dies work, what fails on DDR5/ECC). If that holds, the popcount-in-DRAM bottleneck you pointed out is basically the whole roadmap.",
          "score": 1,
          "created_utc": "2026-02-28 03:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uqpo0",
              "author": "use-one_of-these",
              "text": "Thanks. The compatibility matrix is in the full report on GitHub ‚Äî SK Hynix C-die confirmed, Micron likely compatible, Samsung hard incompatible, DDR5 blocked by mandatory on-die ECC until bypass is demonstrated.",
              "score": 1,
              "created_utc": "2026-02-28 09:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qk2f0",
          "author": "AiDreamer",
          "text": "Simply amazing, DRAM can become a new inference engine",
          "score": 1,
          "created_utc": "2026-02-27 17:46:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qrcj3",
          "author": "Marc-Z-1991",
          "text": "Can you Summarize this AI written Text?",
          "score": 0,
          "created_utc": "2026-02-27 18:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qvvo6",
          "author": "leonbollerup",
          "text": "this is cool!!!!\n\n",
          "score": 0,
          "created_utc": "2026-02-27 18:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qwq50",
          "author": "DataGOGO",
          "text": "Cool, I look forward if you get to work. Though the physics is sound, the reality of running inference is a completely different animal.  Running a single FPGA controller shows it could work, in theory. \n\nYou don't explain a lot of the hardware gaps. This this requires a lot of custom bespoke processors, to include a DRAM memory controller that can handle thousands of channels of DRAM, which then begs the question, why?  \n  \nIf you are building a processor, a memory controller, custom boards that can in essence build a CPU out of memory (the point of the CMU-SAFARI research), why not just use and AI accelerated CPU like the Intel Xeons with AMX where each core has it's own dedicated hardware designed for universal inference, attached to a much more robust processor, fast memory controllers, supports multiple binaries w/ a universal instruction set, is dumb quick, and ultimately is orders of magnitude cheaper than banks of DRAM in a bespoke chassis and low volume controllers, which will sure be fabbed on a huge high power draw legacy processes (no way you are getting into any foundry on modern nodes) \n\nAlso worth noting that the CMU-SAFARI group published this to open source under MIT, you cannot patent it, or your implementation of it. ",
          "score": 0,
          "created_utc": "2026-02-27 18:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qxupj",
              "author": "use-one_of-these",
              "text": "Thanks for the detailed pushback ‚Äî let me address these point by point because I think there's a misunderstanding of what the hardware actually looks like.\n\n**There are no thousands of channels.**¬†CaSA uses standard DDR4 DIMMs in standard DIMM slots. One DIMM, one channel, one 64-bit bus. The FPGA sits where the memory controller normally sits ‚Äî it speaks the same DDR4 protocol, just with modified timing to trigger charge-sharing. The whole prototype is an FPGA dev board ($200-500) plugged into commodity RAM ($15-25 per DIMM). There is no bespoke chassis, no custom boards, no thousands of anything.\n\n**On \"why not Xeon AMX\"**¬†‚Äî AMX is designed for INT8/BF16 matrix ops. It's genuinely good at what it does. But the comparison misses the point: with AMX, you're still moving every weight from memory ‚Üí cache ‚Üí ALU ‚Üí cache ‚Üí memory for every token. That data movement is the bottleneck for LLMs, not the arithmetic. CaSA computes where the data already lives. The weights never move. There's no cache hierarchy, no memory bandwidth wall. That's the fundamental difference ‚Äî it's not \"a worse processor,\" it's eliminating the need to be a processor.\n\nFor context: a Xeon doing BitNet b1.58-2B inference would spend most of its time on memory transfers, same as a GPU. CaSA spends 6% of its time on compute and 88% waiting for the bus ‚Äî and the bus is the only part that needs improving.\n\n**On the FPGA process node**¬†‚Äî the FPGA is a prototyping vehicle, not the product. The actual compute happens in the DRAM, which is fabbed on the most advanced memory process nodes in the world (1Œ±, 1Œ≤ nm at SK Hynix). The endgame isn't \"FPGA in production\" ‚Äî it's either a small ASIC controller or, ideally, a firmware update to existing memory controllers. The popcount register (the biggest performance lever) is \\~2,000 gates added to existing DRAM die ‚Äî Samsung patented this exact thing in 2014.\n\n**On patents and CMU-SAFARI**¬†‚Äî the charge-sharing physics is open research, correct. CaSA doesn't claim to have invented charge-sharing. The contribution is the activation-sacrificial protocol (how to use it without RowCopy errors) and the full inference mapping onto commodity DRAM. The project is open on GitHub, not a patent play.",
              "score": 0,
              "created_utc": "2026-02-27 18:51:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7r2gqg",
                  "author": "DataGOGO",
                  "text": "What you do on your bench to run a few tokens though an FPA does not scale; so yes, for production inference, there are in fact going to be thousands of channels of DDR4 Dimms and custom hardware to run it, dimms need a channel to operate from the memory controller. your dev board does not scale to inference at scale, example, what is your estimate the number of DDR4 dimms required to run a commodity frontier model (deepseekR3 for example), at only 5m tokens per second, a fraction of production class model serving. Given the current price of DDR4 DIMM modules, (Just looked it up, it currently is $84 each bulk direct from hynix in quantities of 10k dimms) for the cheapest 8GB DDR4 dimm, minus shipping, duties, and tariffs ) What is the DIMM cost alone?\n\nNow in production, what are you plugging all of those DDR4 dimms into? What is going to control the dimms, what is going to coordinate operations across all those dimms?\n\nYour AI is using very outdated AMX information to include the INT/FP4 and expanded instructions sets; and you are are fundamentally incorrect on the operations,  It does also does not address the realities of inference serving, when the model receives tokens, where are they buffered? The dimm right? when the model outputs vectors, where are you doing the collection? The dimm right? How are you handling KV cache? when model weights and K/V do not fit in a single dimm. Then what? How are you handling spanning thousands of dimms, thousands of hosts? Not with an ASIC and firmware update. You need hardware controllers that the dimms plug into directly.\n\nIf you compare a single Intel AMX in INT8  to your FPGA dev project; how do the speeds compare at that micro level of single CPU vs Single FPGA with a DIMM? Even a single 5 year old Sapphire rapids can exceed 80 billion AMX operations per second.",
                  "score": 0,
                  "created_utc": "2026-02-27 19:13:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qrjzy",
          "author": "XD__XD",
          "text": "ai slop is real.. but point is would you need vram? intead go with regular ram?",
          "score": -3,
          "created_utc": "2026-02-27 18:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ros33",
              "author": "use-one_of-these",
              "text": "That's exactly the idea. Regular DDR4 RAM instead of GPU VRAM.\n\nThe reason LLMs need expensive GPUs isn't really the compute, it's the memory bandwidth. Model weights sit in VRAM and get streamed through the GPU every token. CaSA skips that entirely ‚Äî the weights stay in regular RAM and the computation happens right there in the memory cells via charge-sharing. No GPU, no VRAM, no data movement.\n\nThe catch: it only works for ternary models (BitNet), not the standard FP16/INT4 models that run on GPUs today. But if ternary models keep improving, this becomes a path.",
              "score": 3,
              "created_utc": "2026-02-27 21:04:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7v6p1f",
                  "author": "Silver-Champion-4846",
                  "text": "Why would anyone focus on improving ternary as opposed to quaternery (2bit)?",
                  "score": 1,
                  "created_utc": "2026-02-28 11:53:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qxqsv",
          "author": "Large-Excitement777",
          "text": "Definitely didn‚Äôt read most of that AI generated slop but definitely agree that optimization of current hardware is the way forward",
          "score": -3,
          "created_utc": "2026-02-27 18:50:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rct7k",
              "author": "Traveler3141",
              "text": "How is it sloppy?  Or are you simply throwing the word \"slop\" as a word-wrassling move to signal that you fear/believe-in/support The Current Thing?",
              "score": 0,
              "created_utc": "2026-02-27 20:04:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7s97gv",
                  "author": "Large-Excitement777",
                  "text": "Huh?",
                  "score": -2,
                  "created_utc": "2026-02-27 22:49:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qxsje",
          "author": "killzone44",
          "text": "Would this kind of cycling of all bits at the same time over and over overheat the modules? Was this proof of concept just run for a few iterations or was this run for an extended period? Was additional cooling used/needed/considered?",
          "score": 0,
          "created_utc": "2026-02-27 18:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qyy6u",
              "author": "use-one_of-these",
              "text": "Good question. Two things to clarify about what \"cycling\" actually looks like here:\n\nIt's not all rows at once. During inference, rows activate sequentially ‚Äî row 0, then row 1, then row 2, etc. ‚Äî as you work through the matrix multiplication. Each individual row is active for \\~60ns then closes. By the time you come back to row 0 for the next token, thousands of other rows have had their turn. The duty cycle per row is very low.\n\nFor context, normal DRAM refresh already activates every row every 64ms just to keep the data alive. CaSA adds inference activations on top of that, but the pattern is spread across the array, not hammering the same cells repeatedly.\n\nOn the testing duration ‚Äî the CMU-SAFARI charge-sharing experiments were 79 million operations across 120 real DDR4 chips at various temperatures (up to 80¬∞C). That's a reliability characterization, not a quick demo. Our throughput simulations model sustained inference at the rated timing parameters. We haven't run extended thermal soak tests on a physical prototype yet ‚Äî that's future work and a fair thing to flag.\n\nDRAM also has built-in thermal protection. DDR4/DDR5 modules have on-die temperature sensors, and when temperature rises the refresh interval automatically halves (64ms ‚Üí 32ms). This costs throughput but prevents damage. Our throughput estimates account for standard thermal operating conditions.\n\nShort answer: the physics doesn't suggest overheating because the activation pattern is distributed, not concentrated. But extended thermal testing on a real prototype running sustained inference is on the to-do list, and I'd want to see that data too.",
              "score": 1,
              "created_utc": "2026-02-27 18:56:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7rowo4",
                  "author": "killzone44",
                  "text": "ty for the detailed reply, it is definitely an interesting concept. I believe the argument of the results is that the cost of using RAM dims as compute units is cost effective. I wonder if that still holds with climbing RAM price. Obviously it could provide some kind of release hatch for overheated GPU prices if it was proven out",
                  "score": 0,
                  "created_utc": "2026-02-27 21:05:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qkqq0",
          "author": "mon_key_house",
          "text": "I only have samsung ram sticks so this story sucks.",
          "score": -3,
          "created_utc": "2026-02-27 17:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qvbpl",
          "author": "Adept_Carpet",
          "text": "Wouldn't allowing userland software to disable RowHammer protections make them useless?\n\n\nI think this is really clever and shows how these big AI changes to usage patterns will influence hardware design and re/use but I'm not sure this is the way forward for consumer devices (at least not yet).\n\n\nIt would be very cool to see it happen on a live device though. I hope you do get around to doing that.",
          "score": -1,
          "created_utc": "2026-02-27 18:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rkea7",
              "author": "use-one_of-these",
              "text": "Charge-sharing and RowHammer are different operations. RowHammer is rapid repeated activation of one row to disturb its neighbors. Charge-sharing is simultaneous activation of two rows in the same subarray. TRR (Target Row Refresh) watches for the RowHammer pattern and wouldn't necessarily interfere with charge-sharing, though the extra refreshes it triggers could cost some throughput.",
              "score": 1,
              "created_utc": "2026-02-27 20:42:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdb146",
      "title": "Can anybody test my 1.5B coding LLM and give me their thoughts?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "author": "Great-Structure-4159",
      "created_utc": "2026-02-24 08:45:48",
      "score": 36,
      "num_comments": 33,
      "upvote_ratio": 0.94,
      "text": "I fine tuned my own 1.5B LLM, took Qwen2.5-1.5B-Instruct and fine tuned it on a set of Python problems, and I got a pretty decent LLM!\n\nI'm quite limited on my computational budget, all I have is an M1 MacBook Pro with 8GB RAM, and on some datasets, I struggled to fit this 1.5B model into RAM without getting an OOM.\n\nI used mlx\\_lm to fine tune the model. I didn't fine tune fully, I used LoRA adapters and fused. I took Qwen2.5-1.5B-Instruct, trained it for 700 iterations (about 3 epochs) on a 1.8k python dataset with python problems and other stuff. I actually had to convert that data into system, user, assistant format as mlx\\_lm refused to train on the format it was in (chosen/rejected).  I then modified the system prompt, so that it doesn't give normal talk or explanations of its code, and ran HumanEval on it (also using MLX\\_LM) and I got a pretty decent 49% score which I was pretty satisfied with.\n\nI'm not exactly looking for the best bench scores with this model, as I just want to know if it's even good to actually use in daily life. That's why I'm asking for feedback from you guys :D\n\nHere's the link to the model on Hugging Face:\n\n[https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B](https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B)\n\nIt's also available on LM Studio if you prefer that.\n\nPlease test out the model and give me your thoughts, as I want to know the opinions of people using it. Thanks! If you really like the model, a heart would be much appreciated, but I'm not trying to be pushy, only heart if you actually like it.\n\nBe brutally honest with your feedback, even if it's negative like \"this model sucks!\", that helps me more thank you think (but give some reasoning on why it's bad lol).\n\nEdit: 9.6k views? OMG im famous.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7421hy",
          "author": "Ok-Employment6772",
          "text": "In a few weeks I have a large python project coming up, cant wait to test it",
          "score": 8,
          "created_utc": "2026-02-24 10:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74265u",
              "author": "Great-Structure-4159",
              "text": "Thanks for testing! Can't wait to hear your feedback.",
              "score": 3,
              "created_utc": "2026-02-24 10:15:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7cvtgn",
              "author": "Maleficent-Ad5999",
              "text": "This.. the beauty of this community! Kudos.",
              "score": 1,
              "created_utc": "2026-02-25 17:10:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74gxww",
          "author": "fermented_Owl-32",
          "text": "This is what I needed. I want a local tool calling orchestrator and dynamic tool creator in python. Let me test it how it holds up in creating python scripts by receiving instructions from another agent. The smaller the better, as i need to run 5 such models ( i call them micromodels ).\nWill let you know how it turns out.",
          "score": 6,
          "created_utc": "2026-02-24 12:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s0h3",
              "author": "Great-Structure-4159",
              "text": "Oh‚Ä¶ tool calling, interesting. I should try that, I didn‚Äôt train with tool calling in mind actually, but this is really cool, I think it can work.",
              "score": 2,
              "created_utc": "2026-02-24 13:28:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o74s45t",
              "author": "Great-Structure-4159",
              "text": "I don‚Äôt think it‚Äôll be very good as an orchestrator, but I‚Äôll try making a model fine tuned for orchestrating tool calling, that would be really cool. Do let me know if it works out good, very interesting to see LLMs applied like this.",
              "score": 2,
              "created_utc": "2026-02-24 13:28:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74uaba",
                  "author": "fermented_Owl-32",
                  "text": "Orchestrator will be a function-gemma model. One of its tools will be the tool creator. The tool creator will use your model to write scripts for the use-case in user's query. I need simpler but fast and a little intelligent scripting, I will test it for that purpose",
                  "score": 2,
                  "created_utc": "2026-02-24 13:41:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o744c44",
          "author": "RnRau",
          "text": "I don't do python, but I think its just awesome to see open source tools and weights being used in such a resource constrained environment to get a very useful outcome. \n\nCheers for the writeup!",
          "score": 11,
          "created_utc": "2026-02-24 10:35:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bpi8",
              "author": "Great-Structure-4159",
              "text": "Yeah I was pretty shocked too that 8GB could do stuff like this, but yeah I find the subject very fascinating :)",
              "score": 2,
              "created_utc": "2026-02-24 11:39:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73zmq7",
          "author": "Whiplashorus",
          "text": "Am gonna check it after \nJust a question why using qwen2.5 as a foundation?\nLfm2.5 or even qwen3 are not good for your usecase?",
          "score": 4,
          "created_utc": "2026-02-24 09:52:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74056z",
              "author": "Great-Structure-4159",
              "text": "Great question! My first choice was actually LFM2.5, and I did try that first, but for some reason when fusing it with adapters on MLX, llama.cpp just refuses to convert it to GGUF. I tried troubleshooting but eventually just gave up. Qwen3 was my next choice but I just decided to keep it simple and start with 2.5 and go from there, mainly because Qwen3 came with a 1.7B model (which was pushing my RAM limit due to the dataset having long samples) and also, in my searches, didn't have an instruct version weirdly. Maybe the next release will be with qwen3 if the qwen architecture proves good from user tests (and I can do something about the dataset).\n\n",
              "score": 7,
              "created_utc": "2026-02-24 09:57:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o747qs4",
          "author": "Outrageous-Story3325",
          "text": "Whats the token per second, on your gpu ?¬†",
          "score": 4,
          "created_utc": "2026-02-24 11:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bmnr",
              "author": "Great-Structure-4159",
              "text": "I have Apple M1, and I get about 50 tokens/s on GGUF, and 60 tokens/s on MLX (which is not on the repo at the moment.)",
              "score": 3,
              "created_utc": "2026-02-24 11:38:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74w0cm",
          "author": "Fun_Abroad_3650",
          "text": "Hi Sure, i am making an android llm runner ill be happy to try it out, just need the gguf file",
          "score": 4,
          "created_utc": "2026-02-24 13:50:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74weuk",
              "author": "Great-Structure-4159",
              "text": "Thanks for offering to test! The .gguf file is on the repo. There's fp16 and q4\\_k\\_m quants, so you can use whichever one you prefer :D.",
              "score": 2,
              "created_utc": "2026-02-24 13:52:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o769fot",
          "author": "BringMeTheBoreWorms",
          "text": "I have a few python projects that I could throw at it. \n\nHow have you found it compared to other models so far?",
          "score": 4,
          "created_utc": "2026-02-24 17:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lhyg",
              "author": "Great-Structure-4159",
              "text": "In terms of benchmarks, it‚Äôs pretty decent for a 1.5B model. It beats the base Qwen at coding, but I‚Äôm pretty sure Qwen Coder is slightly better at the benchmark. However, Qwen coder doesn‚Äôt have any ability at actually talking about something related to coding, like explaining the code, that‚Äôs why I trained on the instruct version and not the coder version.",
              "score": 1,
              "created_utc": "2026-02-25 03:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79ne44",
                  "author": "BringMeTheBoreWorms",
                  "text": "Ill run it over a smallish project later on and see what it says",
                  "score": 2,
                  "created_utc": "2026-02-25 04:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74i9b9",
          "author": "zulutune",
          "text": "Fascinating!\nDid you document the process somewhere? Do you have good resources on how to do it?",
          "score": 3,
          "created_utc": "2026-02-24 12:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74sfyu",
              "author": "Great-Structure-4159",
              "text": "I didn‚Äôt document my process anywhere actually, I just typed out all that to give an idea. MLX-LM doesn‚Äôt really have any good resources, other than the one video they made on the Apple Developer YouTube channel regarding it. They don‚Äôt go through every feature and command there, however, so I mainly referred to the documentation they have, which is pretty decent.",
              "score": 3,
              "created_utc": "2026-02-24 13:30:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75l6fb",
                  "author": "zulutune",
                  "text": "Thank for your reply!",
                  "score": 3,
                  "created_utc": "2026-02-24 15:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76gnnl",
          "author": "cHekiBoy",
          "text": "following",
          "score": 3,
          "created_utc": "2026-02-24 18:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ljlq",
              "author": "Great-Structure-4159",
              "text": "Thanks! Hope you like the model!",
              "score": 1,
              "created_utc": "2026-02-25 03:50:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7767k4",
          "author": "sethPower00",
          "text": ".",
          "score": 3,
          "created_utc": "2026-02-24 20:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79b1xf",
          "author": "Historical_Ice187",
          "text": "Hey, could drop few resources you used for this? I've a mac mini and have been wanting to try and learn something like this.",
          "score": 2,
          "created_utc": "2026-02-25 02:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lnpg",
              "author": "Great-Structure-4159",
              "text": "Yeah I‚Äôm looking into making a small article on this because you‚Äôre not the first one to ask for this. I‚Äôll contact you once I write it.",
              "score": 1,
              "created_utc": "2026-02-25 03:50:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a5je3",
                  "author": "tocarbajal",
                  "text": "I‚Äôll stay tuned for that article, let‚Äôs hope not so small, anyway.\nThank you for sharing.",
                  "score": 2,
                  "created_utc": "2026-02-25 06:11:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b8s9l",
                  "author": "Historical_Ice187",
                  "text": "Thanks alot. Truly appreciate it.",
                  "score": 2,
                  "created_utc": "2026-02-25 12:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dk2ci",
          "author": "According-Muscle-902",
          "text": "Irei testar. Estou trabalhando com fine-tuning a partir do gemma3 1B mas para um contexto diferente",
          "score": 2,
          "created_utc": "2026-02-25 18:59:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gek7l",
              "author": "Great-Structure-4159",
              "text": "Obrigado por testar! Me avise como o modelo se comporta depois que voc√™ test√°-lo. Eu n√£o sei portugu√™s, isso foi traduzido pelo Google, ent√£o me desculpe se houver algum erro.\n\nThis message is from Google Translate, sorry for any mistakes if there are any. ",
              "score": 1,
              "created_utc": "2026-02-26 03:52:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rec7jd",
      "title": "H100AM motherboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/kvoakpujzmlg1.png",
      "author": "Puzzleheaded_Low_796",
      "created_utc": "2026-02-25 12:44:28",
      "score": 26,
      "num_comments": 36,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rec7jd/h100am_motherboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bnlyp",
          "author": "FullstackSensei",
          "text": "Your issue is the number of channels, not the lack of details about how much RAM this actually has? They only say maximum 128GB. If I were to bet this price is for the 32GB version",
          "score": 10,
          "created_utc": "2026-02-25 13:35:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bpcfe",
              "author": "Puzzleheaded_Low_796",
              "text": "So it's listed as 128 and the bot replied 128 but I agree that it's to take with a pinch of salt, I asked for a quote for the 128gb model so I will know soon enough the final price. Even if it's more than that + fees it's still potentially still quite a good option in a market with very few options",
              "score": 3,
              "created_utc": "2026-02-25 13:45:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bskn3",
                  "author": "FullstackSensei",
                  "text": "I don't know. Strix Halo is cool, but not 2k cool, let alone k. Even at current crazy prices, you can build a 128GB VRAM using 32GB Mi50s for ~2k. You'll have a lot more compute and memory bandwidth vs Strix Halo, and power consumption is nowhere near as much as most think.",
                  "score": 3,
                  "created_utc": "2026-02-25 14:02:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bpyws",
          "author": "The_Crimson_Hawk",
          "text": "> the 10g nic is really good too\n\nLooks inside\n\nAqc113",
          "score": 2,
          "created_utc": "2026-02-25 13:48:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bs50i",
              "author": "getpodapp",
              "text": "Are they shitty ?",
              "score": 1,
              "created_utc": "2026-02-25 14:00:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bsmpo",
                  "author": "The_Crimson_Hawk",
                  "text": "Yes. Have a look at thunderbolt to 10g adapter issues, most of them are caused because they almost exclusively use aqc07 or aqc113. Tldr shit offload, random link down, mtu issues. You might argue these features are useless for normal users, but i say if ur doing homelab ur not a normal user",
                  "score": 1,
                  "created_utc": "2026-02-25 14:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bs84g",
              "author": "Puzzleheaded_Low_796",
              "text": "Any particular gripe with the Marvell aquantia controllers? Anything that's really a deal breaker?\n\nMy statement was with regards to the competition such as the bosgame M5 and the GmKTek evo-x2 that are 2.5G not 10G",
              "score": 1,
              "created_utc": "2026-02-25 14:00:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bsnni",
                  "author": "The_Crimson_Hawk",
                  "text": "Have a look at thunderbolt to 10g adapter issues, most of them are caused because they almost exclusively use aqc07 or aqc113. Tldr shit offload, random link down, mtu issues. You might argue these features are useless for normal users, but i say if ur doing homelab ur not a normal user",
                  "score": 2,
                  "created_utc": "2026-02-25 14:03:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7c1ja2",
          "author": "NNextremNN",
          "text": "> Min. order: 5 pieces\n\nDo you need that many or do you want to resell the others?",
          "score": 2,
          "created_utc": "2026-02-25 14:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c4edt",
              "author": "Puzzleheaded_Low_796",
              "text": "No plans at all to resell, but from experience it's often possible to order just one even if it's below minimum quantity",
              "score": 2,
              "created_utc": "2026-02-25 15:03:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cnbei",
          "author": "Hector_Rvkp",
          "text": "ignore the price entirely until you've chatted with someone and agreed precisely on the spec you want. i reported several strix halo listings 2 weeks ago on alibaba precisely because they are lying on the prices. basically when you talk to them, they go like \"oh price much higher now, big demand, we sorry\".   \nAlso, think about import taxes. Depending on where you are, importing something like that can cost a lot. If you buy on aliexpress, they find ways to dodge customs very often. When you order like that, they usually just ship with DHL, and you go through official customs. Some people report horror stories around that... ",
          "score": 1,
          "created_utc": "2026-02-25 16:31:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7copmv",
              "author": "Puzzleheaded_Low_796",
              "text": "Thanks a lot for this, I will wait to have the quote and see, I didn't know there was an ongoing halo strix \"scam\"\n\nI'm very used to ordering from Alibaba and AliExpress with all that entails so I'm not too worried about that. There will be an import tax but it won't be crazy and I always make sure it's properly shipped and declared to avoid things getting stuck in customs forever",
              "score": 1,
              "created_utc": "2026-02-25 16:37:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7cqycj",
                  "author": "Hector_Rvkp",
                  "text": "not scam. Culturally chinese shenanigans, i call them. a scam is if they sell you a chip and you receive a sticker of a chip, or nothing at all. Chinese sellers rarely scam people, but they often push the boundaries of what we'd consider in the west civil society.   \nThere's a book on that i liked, *Poorly made in china*. The author seems to be a bit of a dick, but it's an interesting book :) ",
                  "score": 3,
                  "created_utc": "2026-02-25 16:48:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7p3eo3",
                  "author": "sparood1",
                  "text": "Did they ever gave your a price I contacted the same seller I think but they keep asking question like where is it for what is your budget etc instead of just naming the price",
                  "score": 1,
                  "created_utc": "2026-02-27 13:23:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cqhd9",
          "author": "hejj",
          "text": "I guess since you have to buy at least 5, you could build yourself a cluster.",
          "score": 1,
          "created_utc": "2026-02-25 16:45:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdigyc",
      "title": "Thoughts on Mac Studio M3 Ultra with 256gb for open claw and running models locally",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "author": "00100100",
      "created_utc": "2026-02-24 14:59:45",
      "score": 26,
      "num_comments": 50,
      "upvote_ratio": 0.81,
      "text": "I know a lot of people say to just pay for API usage and those models are better, and I plan to keep doing that for all of my actual job work.\n\nBut for building out my own personal open claw to start running things on the side, I really like the idea of not feeding all of my personal data right back to them to train on.   So I would prefer to run locally.\n\nCurrently I have my gaming desktop with a 4090 that I can run some models very quickly on, but I would like to run a Mac with unified memory so I can run some other models, and not care too much if they have lower tokens per second since it will just be background agentic work.\n\nSo my question is: M3 ultra with 256gb of unified memory good?  I know the price tag is kinda insane, but I feel like anything else with that much memory accessible by a GPU is going to be insanely priced.  And with the RAM and everything shortages...I'm thinking the price right now will be looking like a steal in a few years?\n\nAlternatively, is 96gb of unified memory enough with an M3 Ultra?  Both happen to be in stock near me still, and the 256gb is double the price....but is that much memory worth the investment and growing room for the years to come?\n\nOr just everyone flame me for being crazy if I am being crazy. lol. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o75a7uf",
          "author": "Crafty-Diver-6948",
          "text": "it's okay. you'll be able to run minimax locally at about 50tps, 4 million tokens per day... So you do the math if it's worth it. I have a 196gb and I don't really use it for local models nearly as much as I though",
          "score": 12,
          "created_utc": "2026-02-24 15:03:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76x17w",
              "author": "so_schmuck",
              "text": "Wow that‚Äôs $$",
              "score": 3,
              "created_utc": "2026-02-24 19:29:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o781rk5",
                  "author": "timbo2m",
                  "text": "Yeah if you can make sure it's running constantly and building something that generates money it could potentially pay for itself. Possible, just not probable. And quite the gambling exercise. You'd probably be better to just pay for minimax coder for $20 a month, the trade off being you give away your data",
                  "score": 4,
                  "created_utc": "2026-02-24 22:40:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78jrvu",
              "author": "Badger-Purple",
              "text": "there are no macs with 196gb of ram. There is a 192gb m2 ultra, which I own, and having run LLMs for the past 8 months on it, you‚Äôll never reach 50 tokens per second at the context lengths that an agent needs. Unless openclaw has some magic to decrease context, you‚Äôll wait a cool 2 minutes before each reply.",
              "score": 2,
              "created_utc": "2026-02-25 00:17:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o75llp9",
              "author": "cmndr_spanky",
              "text": "Why that model over some others?",
              "score": 2,
              "created_utc": "2026-02-24 15:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75u1fb",
                  "author": "nomorebuttsplz",
                  "text": "it or Step 3.5 are the best models that will fit at q4 in 256 gb. I guess you could try to cram GLM 4.7 in instead.",
                  "score": 6,
                  "created_utc": "2026-02-24 16:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7he3ws",
              "author": "Ok-Rest-4276",
              "text": "can you elaborate why not using for local models? i wonder if it make sense to have local compute vs paying for codex or cc. What is your use case?",
              "score": 1,
              "created_utc": "2026-02-26 08:34:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75ai9x",
          "author": "FullstackSensei",
          "text": "I think it's much cheaper to share your credit card info and bank account login details here on reddit. You'll save at least the 8k needed to buy the Mac, and might even still have some money left in your bank account",
          "score": 20,
          "created_utc": "2026-02-24 15:05:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75atvk",
              "author": "00100100",
              "text": "Let me get open claw set up and responding to my reddit messages and those details should be posted within a few days!",
              "score": 13,
              "created_utc": "2026-02-24 15:06:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77ptsl",
          "author": "cavebaird",
          "text": "I have a Mac Studio M3 Ultra with 256gb.   After much experimentation I comfortably run MiniMax-M2.5-MLX-6.5bit with reasonable ~50 t/s response in chat and a good chat response in OpenClaw.  Solid reasoning and low hallucinating and BS answers.  Tool use is good.  No vision on this model.  Memory pressure is comfortable.  I use Inferencer for the server connection but LM Studio works too.  \n\nGoing to try the new Qwen3.5 tonight (397B A17B 3bit SWAN and GGUF Q3_K_XL) to see how that runs.   Both of those are ~ 170gb, so should run with some headroom.  Do I wish I could have gotten the 512gb.  Sure, if I had another 4K.   I think the upcoming M5 ultras will be a bigger step up with LLM speed and efficiency.",
          "score": 7,
          "created_utc": "2026-02-24 21:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7heamz",
              "author": "Ok-Rest-4276",
              "text": "what is your use case for locall LLM? coding? or just open claw? ",
              "score": 2,
              "created_utc": "2026-02-26 08:36:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76e7i3",
          "author": "meowrawr",
          "text": "I have the m3 ultra, 80c GPU, 256gb ram and wish I had gone with 512gb. Don‚Äôt be me if you go this route.",
          "score": 11,
          "created_utc": "2026-02-24 18:05:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gmx57",
              "author": "voyager256",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-26 04:48:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7n4gex",
                  "author": "meowrawr",
                  "text": "More memory is always better and models commonly release sizes that push up on the 256gb limit. This might be okay for dedicated enterprise GPUs but as a user, you‚Äôre going to be using memory for the OS, programs running, surfing the web, etc.\n\n\nAlso, especially now considering the cost for memory is through the roof, while Apple still has the same price. The cost of 256gb ram for a PC greatly exceeds the cost Apple is charging right now. \n\n\nM5 ultra might be coming soon, but highly doubt the pricing will be remotely close to what it was before - I hope I‚Äôm wrong.",
                  "score": 1,
                  "created_utc": "2026-02-27 03:54:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75cjg6",
          "author": "apVoyocpt",
          "text": "Okay, just get a cheap device anything that will run Linux. Then install openclaw and pay for tokens (best through open router, you can even pick free models) Then find out if openclaw does anything useful for you. Then test a qwen 3.5 through one router. Then decide if openclaw and a 7000 Mac mini so you can run qwen 3.5 locally is worth it",
          "score": 9,
          "created_utc": "2026-02-24 15:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767yvl",
              "author": "brianlmerritt",
              "text": "Exactly! You might need a 10k Mac like many others are buying.  You might hate even that.  You might need only 128gb ram.\n\nThe pay per token suppliers (direct or via openrouter) are a good way to put your toe in the water without the shark removing your leg.",
              "score": 3,
              "created_utc": "2026-02-24 17:37:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ddlxs",
                  "author": "Ell2509",
                  "text": "I took the direction of an am4 motherboard 32gb gram on a retired enterprise card, and 128gb ddr4. If I need complex models, I run on that. \n\nGeneral model I run a 120b gpt-oss MoE model on a newer laptop with 12gbvram but augmented by 96gb ddr5. \n\nRuns well so far.",
                  "score": 1,
                  "created_utc": "2026-02-25 18:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o768ll3",
              "author": "00100100",
              "text": "I think this is the route I'm gonna go after all the feedback.  I have my gaming desktop that sits most of the time. I already am running nobara linux on it, so I think I'll just test it out for now where I can run a local model for some stuff... and then I'll probably just go the anthropic api route. ",
              "score": 2,
              "created_utc": "2026-02-24 17:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77r3go",
          "author": "jiqiren",
          "text": "You need to wait until March 4th to see what new goodies Apple is selling. You might be able to get a M5 Ultra for the same price.",
          "score": 4,
          "created_utc": "2026-02-24 21:48:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75v4ed",
          "author": "Hector_Rvkp",
          "text": "If you hate life, you could get a Strix halo for 2200$. 128gb unified ram. It's slower, but it's cheaper. Slower isn't slow, it's actually usable because bandwidth is 256gb/s.",
          "score": 7,
          "created_utc": "2026-02-24 16:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7688i1",
              "author": "00100100",
              "text": "That is super interesting.  I didn't know anyone outside of Mac was doing unified memory.  ",
              "score": 3,
              "created_utc": "2026-02-24 17:38:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76afdb",
                  "author": "Hector_Rvkp",
                  "text": "they dont call it that, but the point is that the entire 128 runs at the same speed / bandwidth, and if you run your model on linux, you can use 100% of the memory for the model (or like 99%).  \nAs opposed to GPU (VRAM) vs RAM.  \nSo, to run large MoE models well, the cheapest entry point is strix halo. When you get to 3000+, you have a choice between a very fast GPU on a regular PC w DDR5 ram, or DGX Spark, or Apple studio.  \nThe drivers on AMD started working this year, but they're not plug and play like Apple or Nividia. There's no free lunch. Big community playing w it though, precisely because it's cheap and fairly mighty for today's models. You can run Qwen3.5-397B-A17B on it, and speed shouldn't even suck. And apparently, w a 2bit quant, the model is big enough (397b parameters) that it's still quite good. Allegedly.",
                  "score": 4,
                  "created_utc": "2026-02-24 17:48:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ftvi6",
              "author": "frankbesson",
              "text": "I‚Äôve got one of these! With llama.cpp and some tweaks I got ~70tps out of GLM 4.7 flash (which is pretty decent as an agent).\n\nTook a decent amount of tweaking and is far from perfect, and I still find myself mostly using models via API instead.\n\nI wrote up some of my findings for models on strix halo on a [git repo](https://github.com/frank-besson/llama-strix-halo)",
              "score": 2,
              "created_utc": "2026-02-26 01:52:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7hpsdm",
                  "author": "Hector_Rvkp",
                  "text": "thank you for that. very nice",
                  "score": 1,
                  "created_utc": "2026-02-26 10:27:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k2bnv",
                  "author": "jinks9",
                  "text": "bookmarked this, have been halo curious for a while, almost pulling the trigger a couple of times.\n\nThis statement in your repo is interesting:  \n\"3 tool calling tests fail universally due to llama.cpp server limitations (not model issues): multi-tool calls (server returns only 1 tool\\_call per response), complex nested args, and tool\\_choice: \"none\" (server ignores the parameter). JSON-only output also fails on all models (thinking models emit CoT before JSON)\"\n\nIs this still the case current day? (limitations of llama.cpp)",
                  "score": 1,
                  "created_utc": "2026-02-26 18:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7goz8k",
              "author": "voyager256",
              "text": "Or if not actual masochist - a cheapest Nvidia Spark like  GB10-based mini PC e.g. Asus GX10 for like 3000‚Ç¨ for a bit better performance and overall experience.  then you can get another one  which would also get you 256GB of unified  memory , but a lot less money than the Mac Studio. \n\nOr just a  RTX Pro 6000 for much better QoL :) but higher price if you don‚Äôt already own a PC . ",
              "score": 1,
              "created_utc": "2026-02-26 05:02:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7hp1zu",
                  "author": "Hector_Rvkp",
                  "text": "Yes, but that's 75% more than what i paid 2 weeks ago. It's a tall ask. ",
                  "score": 1,
                  "created_utc": "2026-02-26 10:20:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o779jx9",
              "author": "sav22v",
              "text": "It‚Äôs not the same ‚Äúunified RAM‚Äù like Apple!",
              "score": 0,
              "created_utc": "2026-02-24 20:27:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o765x8n",
          "author": "TheOverzealousEngie",
          "text": "Your problem is you can't get a foundation model running on 256 . The right flavor of DeepSeek will cost you 1TB or the like.  And the difference to openclaw for expensive DeepSeek vs. Cheap Kimi is the existence of tools in the LLM. DS has them , kimi does not. \n\nMeaning after you've set everything up, invested all this architecture and money, there are skills that are just architecturally off limits. Yuck. ",
          "score": 5,
          "created_utc": "2026-02-24 17:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o768d2z",
              "author": "00100100",
              "text": "Yeah, I think I am getting the gist of:  basic server, pay for better models.  ",
              "score": 6,
              "created_utc": "2026-02-24 17:39:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o76p6nq",
              "author": "Far_Cat9782",
              "text": "Your acting like skills are so hard to code for? It's just scripts that the AI can use. U can use any small model and increase its tool usage by making scripts for whatever u want and system prompt the model to know or has access to the tool. I got my 27b gemma model writing python code and running it in the console and displaying the results and doing a bunch of other 'skills.\"",
              "score": 6,
              "created_utc": "2026-02-24 18:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76syqz",
                  "author": "TheOverzealousEngie",
                  "text": "You're , not your. ",
                  "score": 0,
                  "created_utc": "2026-02-24 19:10:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77o14g",
          "author": "donotfire",
          "text": "The great thing about renting off the cloud is it‚Äôs easily scalable. You can just decide to double your model size and it‚Äôs done, just like that. But if you buy an M3 and decide 256GB isn‚Äôt enough, well the you‚Äôre out of luck. Gotta buy a new computer then.",
          "score": 2,
          "created_utc": "2026-02-24 21:34:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79o0tf",
          "author": "jw-dev",
          "text": "I think the 256gb m3u is the sweet spot actually, can run some great models for everyday/private stuff and then burst to the cloud if you need heavy models, or speed‚Ä¶ the bigger models get too slow, especially as the context size grows.",
          "score": 2,
          "created_utc": "2026-02-25 04:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75aemc",
          "author": "floppypancakes4u",
          "text": "Do able, but to make automations and scripts, id still use smarter models.",
          "score": 2,
          "created_utc": "2026-02-24 15:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75btgh",
              "author": "00100100",
              "text": "By smarter I assume you mean cloud hosted/pay per token like opus?\n\nI probably won't use it to do much coding with this setup.  I have corporate provided Claude for that.  I'm more wanting  to build it as my own personal assistant type device.  Organizing my calendars. Checking emails. Watching my conversations and generating my todo lists(and maybe eventually at least scheduling agentic work via my anthropic sub).  ",
              "score": 0,
              "created_utc": "2026-02-24 15:11:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75cqhd",
                  "author": "floppypancakes4u",
                  "text": "Still, smarter. Opus is good not only cause of its excellent coding abilities, but its extremely good at reasoning AND tool calling, which are the two primary aspects of automation in openclaw. Claw is not built to make token conservative automations, it makes repeatable smart automations. It does its best to make scripts to handle it all, but it still makes its own prompt to process for each automation. You want consistency with automations, and because it still uses prompts, its best to use the smart models for it. You can absolutely experiment and see if something dumber works. For instance, I built a automation with codex, harnessed by a small, but very strict prompt. Now it runs on my computer every 5 and 10 minutes (two different scripts) using glm 4.7 flash",
                  "score": 4,
                  "created_utc": "2026-02-24 15:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78ogiv",
          "author": "No_Knee3385",
          "text": "Why spend the extra premium on apple instead of building your own PC or buying a custom  build?",
          "score": 2,
          "created_utc": "2026-02-25 00:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o797wsj",
              "author": "scottag",
              "text": "The unified memory that can be shared between GPU and CPU.",
              "score": 3,
              "created_utc": "2026-02-25 02:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o799fp2",
                  "author": "No_Knee3385",
                  "text": "That makes sense. But that also does exist on non-apple hardware",
                  "score": 0,
                  "created_utc": "2026-02-25 02:40:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79tu3h",
              "author": "UnluckyPhilosophy185",
              "text": "macOS",
              "score": 1,
              "created_utc": "2026-02-25 04:44:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79qb27",
          "author": "Xendrak",
          "text": "My thoughts are: it‚Äôs viable. I‚Äôm eyeing it too but I‚Äôm waiting for the new M5 models coming this year it‚Äôs supposed to have several times more ai cores.",
          "score": 1,
          "created_utc": "2026-02-25 04:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79qgph",
          "author": "Xendrak",
          "text": "How much you doing for llm use? Might get away with minimax or kimi on openrouter until you can source the hardware you‚Äôd like.",
          "score": 1,
          "created_utc": "2026-02-25 04:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e17b6",
          "author": "Traditional-Card6096",
          "text": "You can use a cheap VPS like hostinger with free kimi 2.5 from nvidia. Much cheaper than a m3 ultra",
          "score": 1,
          "created_utc": "2026-02-25 20:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75vyei",
          "author": "Ryanmonroe82",
          "text": "M4 Pro 24gb is the minimum.  Look at bandwidth and not just RAM  specs",
          "score": 1,
          "created_utc": "2026-02-24 16:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75fprc",
          "author": "Mundane-Tea-3488",
          "text": "I have been using [edge veda](https://github.com/ramanujammv1988/edge-veda) fluter sdk for running llm on Mac + claude code which can create application instantly",
          "score": 0,
          "created_utc": "2026-02-24 15:29:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fj3b5",
          "author": "StardockEngineer",
          "text": "https://www.reddit.com/r/LocalLLM/search/?q=Mac+Studio+M3+Ultra&cId=9042cc88-b00b-466c-8cfe-f4ec63b35115&iId=c85041c1-fc72-46d4-a91f-828422a5b459",
          "score": 0,
          "created_utc": "2026-02-26 00:50:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgii85",
      "title": "Architecture > model size: I made a 12B Dolphin handle 600+ Telegram users. Most knew it was AI. Most didn't care. [9K lines, open source]",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rgii85/architecture_model_size_i_made_a_12b_dolphin/",
      "author": "frankmsft",
      "created_utc": "2026-02-27 20:50:47",
      "score": 26,
      "num_comments": 21,
      "upvote_ratio": 0.91,
      "text": "I wanted to answer one question: **can you build an AI chatbot on 100% local hardware that's convincing enough that people stay for 48-minute sessions even when they know it's AI?**\n\nAfter a few months in production with 600+ real users, \\~48 minute average sessions, and 95% retention past the first message, the answer is yes. But the model is maybe 10% of why it works. The other 90% is the 9,000 lines of Python wrapped around it.\n\nThe use case is NSFW (AI companion for an adult content creator on Telegram), which is what forced the local-only constraint. Cloud APIs filter the content. But that constraint became the whole point: zero per-token costs, no rate limits, no data leaving the machine, complete control over every layer of the stack.\n\n# Hardware\n\nOne workstation, nothing exotic:\n\n* Dual Xeon / 192GB RAM\n* 2x RTX 3090 (48GB VRAM total)\n* Windows + PowerShell service orchestration\n\n# The model (and why it's the least interesting part)\n\n**Dolphin 2.9.3 Mistral-Nemo 12B** (Q6\\_K GGUF) via llama-server. Fits on one 3090, responds fast. I assumed I'd need 70B for this. Burned a week testing bigger models before realizing the scaffolding matters more than the parameter count.\n\nIt's an explicit NSFW chatbot. A vulgar, flirty persona. And the 12B regularly breaks character mid-dirty-talk with \"How can I assist you today?\" or \"I'm here to help!\" Nothing kills the vibe faster than your horny widow suddenly turning into Clippy. Every uncensored model does this. The question isn't whether it breaks character. It's whether your pipeline catches it before the user sees it.\n\n# What makes the experience convincing\n\n**Multi-layer character enforcement.** This is where most of the code lives. The pipeline: regex violation detection, keyword filters, retry with stronger system prompt, then a separate postprocessing module (its own file) that catches truncated sentences, gender violations, phantom photo claims (\"here's the photo!\" when nothing was sent), and quote-wrapping artifacts. Hardcoded in-character fallbacks as the final net. Every single layer fires in production. Regularly.\n\n**Humanized timing.** This was the single biggest \"uncanny valley\" fix. Response delays are calculated from message length (\\~50 WPM typing simulation), then modified by per-user engagement tiers using triangular distributions. Engaged users get quick replies (mode \\~12s). Cold users get chaotic timing. Sometimes a 2+ minute delay with a read receipt and no response, just like a real person who saw your message and got distracted. The bot shows \"typing...\" indicators proportional to message length.\n\n**Conversation energy matching.** Tracks whether a conversation is casual, flirty, or escalating based on keyword frequency in a rolling window, then injects energy-level instructions into the system prompt dynamically. Without this, the model randomly pivots to small talk mid-escalation. With it, it stays in whatever lane the user established.\n\n**Session state tracking.** If the bot says \"I'm home alone,\" it remembers that and won't contradict itself by mentioning kids being home 3 messages later. Tracks location, activity, time-of-day context, and claimed states. Self-contradiction is the #1 immersion breaker. Worse than bad grammar, worse than repetition.\n\n**Phrase diversity tracking.** Monitors phrase frequency per user over a 30-minute sliding window. If the model uses the same pet name 3+ times, it auto-swaps to variants. Also tracks response topics so users don't get the same anecdote twice in 10 minutes. 12B models are especially prone to repetition loops without this.\n\n**On-demand backstory injection.** The character has \\~700 lines of YAML backstory. Instead of cramming it all into every system prompt and burning context window, backstory blocks are injected only when conversation topics trigger them. Deep lore is available without paying the context cost on every turn.\n\n**Proactive outreach.** Two systems: check-ins that message users 45-90 min after they go quiet (with daily caps and quiet hours), and re-engagement that reaches idle users after 2-21 days. Both respect cooldowns. This isn't an LLM feature. It's scheduling with natural language generation at send time. But it's what makes people feel like \"she\" is thinking about them.\n\n**Startup catch-up.** On restart, detects downtime, scans for unanswered messages, seeds context from Telegram history, and replies to up to 15 users with natural delays between each. Nobody knows the bot restarted.\n\n# The rest of the local stack\n\n|Service|What|Stack|\n|:-|:-|:-|\n|Vision|Photo analysis + classification|Ollama, LLaVA 7B + Llama 3.2 Vision 11B|\n|Image Gen|Persona-consistent selfies|ComfyUI + ReActor face-swap|\n|Voice|Cloned voice messages|Coqui XTTS v2|\n|Dashboard|Live monitoring + manual takeover|Flask on port 8888|\n\nThe manual takeover is worth calling out. The real creator can monitor all conversations on the Flask dashboard and seamlessly jump into any chat, type responses as the persona, then hand back to AI. Users never know the switch happened.\n\n# AI disclosure (yes, really)\n\nBefore anyone asks: the bot discloses its AI nature. First message to every new user is a clear \"I'm an AI companion\" notice. The `/about` command gives full details. If someone asks \"are you a bot?\" it owns it. Stays in character but never denies being AI.\n\nThe interesting finding: **85% of users don't care.** They know, they stay anyway. The 15% who leave were going to leave regardless. Honesty turned out to be better for retention than deception, which I did not expect.\n\n# What I got wrong\n\n1. **Started with prompt engineering, should have started with postprocessing.** Spent weeks tweaking system prompts when a simple output filter would have caught 80% of character breaks immediately. The postprocessor is a separate file now and it's the most important file in the project.\n2. **Added state tracking way too late.** Self-contradiction is what makes people go \"wait, this is a bot.\" Should have been foundational, not bolted on.\n3. **Underestimated prompt injection.** Got sophisticated multi-language jailbreak attempts within the first week. The Portuguese ones were particularly creative. Built detection patterns for English, Portuguese, Spanish, and Chinese. If you're deploying a local model to real users, this hits fast.\n4. **Temperature and inference tuning is alchemy.** Settled on specific values through pure trial and error. Different values for different contexts. There's no shortcut here, just iteration.\n\n# The thesis\n\nThe \"LLMs are unreliable\" complaints on this sub (the random assistant-speak, the context contradictions, the repetition loops, the uncanny timing) are all solvable with deterministic code around the model. The LLM is a text generator. Everything that makes it feel like a person is traditional software engineering: state machines, cooldown timers, regex filters, frequency counters, scheduling systems.\n\nA 12B model with the right scaffolding will outperform a naked 70B for sustained persona work. Not because it's smarter, but because you have the compute headroom to run all the support services alongside it.\n\n# Open source\n\n**Repo:** [**https://github.com/dvoraknc/heatherbot**](https://github.com/dvoraknc/heatherbot)\n\nThe whole persona system is YAML-driven. Swap the character file and face image and it's a different bot. Built for white-labeling from the start. Telethon (MTProto userbot) for Telegram, fully async. MIT licensed.\n\nHappy to answer questions about any part of the architecture.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgii85/architecture_model_size_i_made_a_12b_dolphin/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7rpdur",
          "author": "Aggressive_Special25",
          "text": "I have been building something similar to this. Just been playing around locally testing it. We'll done!",
          "score": 3,
          "created_utc": "2026-02-27 21:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rqhk3",
              "author": "frankmsft",
              "text": "Nice, what model are you running? The character enforcement pipeline was the biggest unlock for me. Curious if you hit the same assistant-speak problem or if your model holds better.... AI for good ;)",
              "score": 1,
              "created_utc": "2026-02-27 21:13:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7rrv1v",
                  "author": "Aggressive_Special25",
                  "text": "I have a bunch of unrestricted 8b models for NSFW generation and using comfy NSFW models for image generation. Using chroma dB for semantic memory - I created a WhatsApp \"clone\" where hot chicks would speak and flirt with you and send nudes etc. After long chats they would break character but I managed to run the entire chat through another NSFW llm to reprompt midway through the chat to get it back into character and I labeled it as a security feature \"to protect your privacy the chat has been sanitised\" - as I say just playing around locally showing my friends.",
                  "score": 2,
                  "created_utc": "2026-02-27 21:20:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7rrjx1",
          "author": "ForsookComparison",
          "text": "We know small models can work as chatbots with large contexts but be real with us - did you manage to get the gooners to pay money or is this charity-goon work?",
          "score": 3,
          "created_utc": "2026-02-27 21:18:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rssp8",
              "author": "frankmsft",
              "text": "Charity goon work, 100%. There's Telegram Stars tipping set up and nobody's paid a dime. The tip hooks are intentionally soft, one transparent line per session after 15+ messages with 5-day cooldowns. No guilt trips, no paywalls.\n\nThis was never about revenue. It was a personal challenge to see if I could make a 12B model feel convincing enough that people stay for 48-minute sessions on local hardware. The monetization plumbing is there if the creator wants to turn it up, but that's their call not mine.",
              "score": 2,
              "created_utc": "2026-02-27 21:24:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7s7x3d",
                  "author": "Big_River_",
                  "text": "yeah its kind of just that actually - dig it brahms - you just want to see if you can and you did - thats all I do and that feels good - like just saying imma do done dat and ding dang it done feelz good - I done did dat yep",
                  "score": 2,
                  "created_utc": "2026-02-27 22:42:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ttfte",
          "author": "BringMeTheBoreWorms",
          "text": "That‚Äôs actually a very interesting use case. Ignoring the content side of things, but that instead is interesting, the pipeline you must have built could be a really valuable template.\n\nI agree in the scaffolding. Iterating with prompting to try and get outputs straight from the model can be a giant waste of time. \n\nSmaller models can run exceptionally well within a narrow scope and then with post processing and even other models in the pipeline you can actually get really efficient, tight output.\n\nIf I have time I‚Äôll check it out.",
          "score": 1,
          "created_utc": "2026-02-28 04:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tzram",
              "author": "frankmsft",
              "text": "Thanks man - yeah the pipeline ended up being way more interesting than I expected going in. The content is what gets people's attention but the architecture underneath is where the real work lives.\n\nTotally agree on smaller models + scaffolding. Running a 12B for text and 7B for vision, and with enough post-processing and validation layers around them the output is tight. You don't need a 70B model if your pipeline is catching the edge cases.\n\nHappy to nerd out on the architecture if you end up taking a look.",
              "score": 1,
              "created_utc": "2026-02-28 05:24:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vm551",
                  "author": "BringMeTheBoreWorms",
                  "text": "I‚Äôm screwing around with some 8b and 14b models for image parsing and some complicated post processing at the moment.. hard to concentrate on anything else for a little bit. Making headway though. Am on old school coder from decades ago and got really sick of the tedium of building stuff many years ago.\n\nBut now! Damn I can get a llm to do all the boring boiler plate crap for me. And cause I know how to build a damn good application, I can actually get some of my long term ideas off the ground!",
                  "score": 1,
                  "created_utc": "2026-02-28 13:43:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v3y21",
          "author": "Silver-Champion-4846",
          "text": "I wonder if it can be used to make a novel builder",
          "score": 1,
          "created_utc": "2026-02-28 11:29:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vqjj9",
              "author": "frankmsft",
              "text": "Honestly yeah - the core pipeline would translate pretty well. The scaffolding I built is really about maintaining character consistency, managing conversation context, and validating output quality. Swap the character from \"flirty Uber driver\" to \"protagonist in a thriller\" and the architecture is basically the same.\n\nYou'd want to add some things - longer context management, chapter/scene state tracking, maybe a separate model call for plot continuity checking as post-processing. But the bones are there: system prompt for voice/style, conversation history for continuity, validation layers to catch when the model drifts off character or contradicts earlier plot points.\n\nThe 12B model sweet spot probably holds too. You don't need a 70B to write good prose if your prompting and guardrails are tight enough. Interesting idea honestly.",
              "score": 1,
              "created_utc": "2026-02-28 14:10:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vtpny",
                  "author": "Silver-Champion-4846",
                  "text": "Imagine this hitting the headlines. GOON BOT TURNED NOVEL WRITER",
                  "score": 1,
                  "created_utc": "2026-02-28 14:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7romir",
          "author": "Big_River_",
          "text": "hold up - before you just scribble off a receipt like yay I love interacting with AI about AI for data mining - check out the git on this heatherbot - whoa dog it going to hot you up - this is legit no nonsense telegram bot for all your many reason to get got trot no rot - and so I ask you if you enjoy time does it matter if time is you or you are time?",
          "score": 1,
          "created_utc": "2026-02-27 21:04:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rpkuw",
              "author": "frankmsft",
              "text": "Heather would say you're overthinking it, sweetie. But yeah, check the repo. And to answer your question... if the conversation is good enough that you lose track of time, does it matter who's keeping track? üòè",
              "score": 2,
              "created_utc": "2026-02-27 21:08:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7rp08s",
              "author": "Big_River_",
              "text": "found the soundtrack my friend - enjoy your time as you are\nhttps://open.spotify.com/track/4mraRJO2iZA5WQ5dxlSQx9?si=sole6thsTTSi27y1jsg-2g",
              "score": 1,
              "created_utc": "2026-02-27 21:06:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7rqbv2",
                  "author": "frankmsft",
                  "text": "Alright that's perfect. Adding this to the service startup playlist. Every time TheBeast boots up, Heather gets a theme song.",
                  "score": 2,
                  "created_utc": "2026-02-27 21:12:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rfur5j",
      "title": "Can the 35B model replace 70B+ dense models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rfur5j/can_the_35b_model_replace_70b_dense_models/",
      "author": "Original_Night7733",
      "created_utc": "2026-02-27 02:48:04",
      "score": 25,
      "num_comments": 20,
      "upvote_ratio": 0.96,
      "text": "If the 35B MoE is as efficient as they claim, does it make running older 70B dense models obsolete? I'm wondering if the reasoning density is high enough that we don't need to hog 40GB+ of VRAM just to get coherent, long-form responses anymore. Thoughts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rfur5j/can_the_35b_model_replace_70b_dense_models/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7mvcc7",
          "author": "RandomCSThrowaway01",
          "text": "Short answer - no.\n\nLong answer - MoE models are indeed more efficient but they **are** also dumber. This is in particular visible with longer context windows, as you approach 40-50k context it's not uncommon for that 30B MoE model to start repeating itself and lose coherence, in this regard it's actually performing worse than a dense 20B.\n\nYour mileage may vary of course depending on what you consider a \"long form\" response. If you need 128k context for instance then there ARE MoE models that can keep up but they are also going to be pretty sizable (eg. Qwen3 Coder Next 80B still does alright). \n\nSo the better question is - can you replace a dense 70B model with a 70B MoE model. Not if you can replace dense 70B with sparse 30B. ",
          "score": 17,
          "created_utc": "2026-02-27 02:58:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nn326",
              "author": "blackashi",
              "text": "So there‚Äôs why Gemini 3 be doing that huh",
              "score": 3,
              "created_utc": "2026-02-27 06:07:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7mz2l0",
          "author": "Double_Cause4609",
          "text": "It's kind of complicated.\n\nSo, one issue is that not all models in a given size category perform the same. For example, GPT-J 6B, Llama 1 7B, Llama 2 7B, LLama 3 8B, Llama 3.1 8B, Qwen 2.5 8B, and Qwen 3 8B all perform wildly differently.\n\nSimilarly, Llama 2 70B is really hard to compare, for example, to Mistral 3 24B.\n\nWhat we tend to see over time is that newer models generally perform better at the same parameter count than older models. Please keep this in mind.\n\nNext, we also have different architectures. Qwen 3.5 has a cracked Attention mechanism, which the Qwen team put a lot of thought into. But if you compare this to an older model, maybe it has a different attention mechanism, or a different training setup even, which leads to different results.\n\nNext, we also have different data and burdens of data. So, you need a given amount of data / parameters ratio to hit a given level of performance. Usually newer models are higher on this. Also, curiously, MoE models actually perform better in data constrained environments (like modern LLM training pipelines) than comparable dense models.\n\nFinally, we also have training costs. Usually MoE models tend to be trained for longer than the same size of dense model. So, even if an MoE and a dense are the same size, if they're given the same training regime, surprisingly, the MoE can perform somewhat similarly, despite being a theoretically weaker model.\n\nSo let's look at what you're really comparing. You're comparing Qwen 3.5 35B, a cutting edge 2026 model to...What? Apartus 70B? That one specifically wasn't great. It's \\*okay\\* but wasn't super well trained, really.\n\nAre you comparing to Qwen 2 70B? Llama 3.3 70B? Those are quite old models, Llama 3.3 in particular if you factor in that the base it was trained on was quite a bit older. I actually think it might be a mid 2024 model if I'm remembering right.\n\nIn reality, if we had an optimally trained 70B dense model like we used to get, then yes, it would perform well, and might even perform better than Qwen 3.5 35B. But it's hard to convince a lab to do that, because the MoE model is waaaaaaay cheaper. Trust me, as soon as you're the one paying the training bills \"oh, the MoE is good enough\" starts to feel really natural.\n\nAnd besides, we have the \\~100-120B class MoE models now to really compete with the old 70B class dense LLMs. I'd say most of the MoE models in this class feel like a modern \\~45-55B model in most tasks (not that we have any to compare against, I'm extrapolating), but they definitely compete with an \\*old\\* 70B.\n\nMoE models \\*are\\* good, but they also are good for really complicated reasons that are hard to articulate, and are really nuanced to express.",
          "score": 14,
          "created_utc": "2026-02-27 03:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7of6x2",
              "author": "lookwatchlistenplay",
              "text": "> Qwen 3.5 has a cracked Attention mechanism\n\nIn standard English usage, this sentence means their attention mechanism is broken, or lacking reason/rationality.¬†\n\nThat seems the opposite of what you are trying to say, however.\n\nI noticed this language error being propagated some time back when articles were phrasing things like \"... discovered by the cracked team at XYZ lab\". That's just wrong usage. The only way it would make sense idiomatically is if they said \"the crack team at...\" because when you say a \"crack team of blahblah\" you mean the team is really good at their work... but that is not the same as saying a \"cracked team\".¬†\n\nA cracked team is a broken team, while a crack team is a skilful team.\n\nFor instance, you get a crack lawyer (etymology seems most unknown, but possibly because they're good at cracking cases open, like plying a small gap into a larger and larger crack, and then everything is revealed). But a cracked lawyer? No. Does not mean the same thing as a crack lawyer. A cracked lawyer is a broken lawyer, someone who took on too much stress and \"cracked\" (went mad or got burnt out) from it.",
              "score": 0,
              "created_utc": "2026-02-27 10:21:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7op715",
                  "author": "Zerokx",
                  "text": "I personally read it as they \"cracked the code\" of how to build good attention mechanisms, so they \"cracked the mechanism\"",
                  "score": 3,
                  "created_utc": "2026-02-27 11:47:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qgayz",
                  "author": "Double_Cause4609",
                  "text": "I don't know what to tell you. \"Cracked\" In vernacular use means exceptional, or unexpectedly good, unorthodox but good, etc.\n\nUse urban dictionary or something I guess?\n\nAnd in standard English usage, the traditional use of \"cracked\" doesn't work there. I wasn't saying there was a \"crack\" in the attention mechanism, etc, which is a hint I was using a different definition.\n\nThis is a pretty common usage of the term, and I'm pretty sure it's not super regional because I've heard it used online before.",
                  "score": 2,
                  "created_utc": "2026-02-27 17:28:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nhffj",
          "author": "alphatrad",
          "text": "I would have said no until today while testing Qwen3.5-35B-A3B with Q6\\_K\\_XL\n\nIt did so good I started using it on Client projects.\n\nWe have finally arrived!\n\nhttps://preview.redd.it/iu60ajmm2zlg1.jpeg?width=1469&format=pjpg&auto=webp&s=67e70d23baa4661249d76e19218feb032a63f69e",
          "score": 6,
          "created_utc": "2026-02-27 05:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7npsa9",
          "author": "catplusplusok",
          "text": "Qwen 3.5 quite likely makes older 70B models and even some frontier cloud models obsolete at least for coding, but this has more to do with their unique architecture (two different interleaving attention mechanisms) and training details than dense vs sparse or parameters. It doesn't necessarily make all future 70B dense models obsolete and there are other upcoming innovations like power infer and text diffusion to make models faster in different ways than MoE with potentially higher overall knowledge and intelligence.",
          "score": 3,
          "created_utc": "2026-02-27 06:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7odxue",
          "author": "cibernox",
          "text": "If you compared a modern 70B model with a modern 35B MoE no, but if you compare a 2yo 70B model with a modern 35B-A3B model‚Ä¶.its a lot closer. In fact I‚Äôm sure the 35B would be better in some areas",
          "score": 4,
          "created_utc": "2026-02-27 10:10:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n6cm6",
          "author": "siegevjorn",
          "text": "Replace in what? It all depends on your use case. That's why there are hundreds of benchmarks out there.",
          "score": 1,
          "created_utc": "2026-02-27 04:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7on36u",
          "author": "gittygo",
          "text": "My rough understanding from a user endpoint (Is it correct?) :\n\n* MoE models are like a librarian with access to a wider section of knowledge and a medium sized brain (the part which attends to the question and what it can do with the knowledge. The librarian would be pulling stuff from different sections, one at a time.\n* To run a dense model at a similar speed, one would need a smaller sized model, which would have lesser knowledge, but would have a bigger \"brain\" - ie it would be better at understanding what is being worked upon, be able to connect dots and data points better from multiple spheres, and hold better coherence and over long, complicated, nuanced discussions; also with some foresight.\n\nI'd be surprised if a 35B MoE would be better than a 70B dense of similar generation. One would expect a 100B+ MoE model to compete with a 70B model, and probably give similar speeds with wider knowledge with lesser intelligence. Eventually, as with most such things, it would be about your particular use case. \n\nJust my 2 cents.",
          "score": 1,
          "created_utc": "2026-02-27 11:30:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qwk0f",
          "author": "xanduonc",
          "text": "Absolutely yes for my usecase.\n\nIf you need model to do actual work then recent moe models obsolete 1 year old dense giants. Dumb or not, tool calling and large context length support improved a lot.",
          "score": 1,
          "created_utc": "2026-02-27 18:45:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mx0ya",
          "author": "PermanentLiminality",
          "text": "It all depends on what you are trying to do.  From a knowledge level the larger dense model will win for sure.\n\nI've not put the qwen3.5 35b through its paces, but for agentic applications, it looks like it will do better.  At least the benchmarks look that way.",
          "score": 0,
          "created_utc": "2026-02-27 03:08:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q3ekz",
              "author": "Skystunt",
              "text": "Benchmarks are far from the truth, they are mostly advertising since there‚Äôs papers on arvix that observed how training material is already contaminated with benchmarks and their results. Even if a model is trained in good faith and not ‚Äúbenchmaxxed‚Äù it is inevitable to have the benchmarks and their results in the trainjng data especially if they use newer data.\nI know mmlu-pro was specifically made to fight this contamination as much as possible but it still happens.\n\nSo benchmarks don‚Äôt show if a model is better or not by far\nPersonal benchmarks that are not published online or published in private environments can show if a model is really better or not for a specific task",
              "score": 1,
              "created_utc": "2026-02-27 16:27:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgo3q5",
      "title": "Is Qwen3.5-35B the new \"Sweet Spot\" for home servers?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rgo3q5/is_qwen3535b_the_new_sweet_spot_for_home_servers/",
      "author": "ischanitee",
      "created_utc": "2026-02-28 00:34:23",
      "score": 24,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "I‚Äôve been trying to find the perfect balance between reasoning capability and VRAM usage for my dual 3090 setup. With Qwen3.5 releasing a 35B MoE, activating only a few billion parameters at a time seems like a game-changer for inference speed. Has anyone tested the GGUF versions yet? How does it actually feel for daily text generation?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rgo3q5/is_qwen3535b_the_new_sweet_spot_for_home_servers/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7sw3ya",
          "author": "HealthyCommunicat",
          "text": "yes, but i mean ever since qwen 2.5 haven't they always kinda been the main go to? i mean not counting that one short time where glm 4.7 flash had come out, but other than the qwen 30b and the coder next 80b models, what other model families are you implying was ever \"the sweet spot\"?",
          "score": 4,
          "created_utc": "2026-02-28 01:02:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wafgn",
              "author": "kweglinski",
              "text": "gpt-oss-120 was very good.",
              "score": 1,
              "created_utc": "2026-02-28 15:56:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vbxfb",
          "author": "Hector_Rvkp",
          "text": "I have a hot take: it depends",
          "score": 2,
          "created_utc": "2026-02-28 12:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vu1ku",
          "author": "Adventurous-Paper566",
          "text": "With a machine like yours I would rather go for 27B in Q6 but if your main concern is speed, then yes the 35B is the best there is.",
          "score": 2,
          "created_utc": "2026-02-28 14:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t8s7l",
          "author": "No-Consequence-1779",
          "text": "Yes, I‚Äôve been comparing on a single r9700. Generation is similar for coding at least. ¬†Next tests will be for the crypto trading. Most models fail this, qwen being one of the few that has not so far. Getting similar 95 tokens per second.¬†",
          "score": 1,
          "created_utc": "2026-02-28 02:21:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t1ua1",
          "author": "Crypto_Stoozy",
          "text": "Yeah it‚Äôs the go to plus you can run multiple instances and parallels",
          "score": 1,
          "created_utc": "2026-02-28 01:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7udtzz",
          "author": "fasti-au",
          "text": "Devstral small 2  and qwen3 have always been solid for 12 months as code options glm 5.7 air on dual 3090.  \n\nWhy you run so small ?  You can get next 80 b on two card if you use ollama not vllm.   Vllm is no good for 3090squant kv cache to q8 same accuracy unless your arguing about tomatoe tomato issues.  It‚Äôs a non factor mostly.  \n\n\n\nOllama kv cache works.  Vllm don‚Äôt give a fuck about 30/40 series it works enough they don‚Äôt need to add more as it‚Äôs not their bread and butter not is it money spenders.  Ie we have 3090s",
          "score": -1,
          "created_utc": "2026-02-28 07:24:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf5tww",
      "title": "How Is This Even Possible? Multi-modal Reasoning VLM on 8GB RAM with NO Accuracy Drop.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/sg8ywmszkplg1",
      "author": "tag_along_common",
      "created_utc": "2026-02-26 09:35:12",
      "score": 23,
      "num_comments": 13,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rf5tww/how_is_this_even_possible_multimodal_reasoning/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7hs69o",
          "author": "Ok-Employment6772",
          "text": "Gonna take a look at it right now, that seems almost too good to be true",
          "score": 3,
          "created_utc": "2026-02-26 10:49:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jxmci",
              "author": "tag_along_common",
              "text": "Let me know how it went for you. üôè",
              "score": 1,
              "created_utc": "2026-02-26 17:56:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7j4bol",
          "author": "ScuffedBalata",
          "text": "\"how is it even possible\"?\n\nUh.  They've found a way to improve mixed precision quantization so the quantized model has LESS (not zero) reduction in quality from the \"full\" model.\n\nBut the \"full\" model is only a 2B model, so it's probably not THAT amazing.  Still there's plenty of use cases for a quantized 2B model like the post is saying.\n\nFor the use case (providing basic text to describe an image), it's probably fine.",
          "score": 3,
          "created_utc": "2026-02-26 15:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j7kol",
              "author": "DataGOGO",
              "text": "sorta. \n\nThe model was a much much larger model that was then shrunk down to 2B, then quantized. The shrinking makes that kind of quantization easier because of all the white space. ",
              "score": 1,
              "created_utc": "2026-02-26 15:56:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ka888",
                  "author": "tag_along_common",
                  "text": "Interesting theory! Meaning, any kind of architectural compression (shrinking, pruning, etc. ) benefits quantization... ? Kinda curious to learn more, do you have a reference/paper for this?",
                  "score": 1,
                  "created_utc": "2026-02-26 18:54:25",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jxap3",
              "author": "tag_along_common",
              "text": "Trur, not zero loss, but quite close. \n\nLooking at the model card and benchmarks the model can process full 1920√ó1080 videos (12 frames) on a small Jetson Orin Nano which is, to my knowledge, not possible with the baseline FP16 model. \n\nIsn't there always the debate about quantization being a great compression technique but introducing errors in most cases if not tuned carefully?",
              "score": 1,
              "created_utc": "2026-02-26 17:55:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7k0oiy",
                  "author": "ScuffedBalata",
                  "text": "For many uses, at a given memory size, it's going to be better to get a bigger/more capable model that is quantized, over a full FP16 at the same memory size.\n\nFor example, at 32gb of VRAM, you're way better using a 30B model at 4Q, rather than a 14B model or something that fits at FP16.   So you're almost ALWAYS best using quantized models in nearly every case unless you're already using the biggest model that works for you.",
                  "score": 1,
                  "created_utc": "2026-02-26 18:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7unsrf",
          "author": "Fantastic-Breath2416",
          "text": "Io l'ho fatto su una cpu 8core!\n\nhttps://nothumanallowed.com",
          "score": 1,
          "created_utc": "2026-02-28 08:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcjzv0",
      "title": "Does a laptop with 96GB System RAM make sense for LLMs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rcjzv0/does_a_laptop_with_96gb_system_ram_make_sense_for/",
      "author": "PersonSuitTV",
      "created_utc": "2026-02-23 15:11:16",
      "score": 22,
      "num_comments": 38,
      "upvote_ratio": 0.85,
      "text": "I am in the market for a new ThinkPad, and for $400 i can go from 32GB to 96GB of system RAM. This Laptop would only have the Arc 140T iGPU on the 255H, so it will not be very powerful for LLMs. However, since Intel now allows 87% of system RAM to be allocated to the iGPU, this sounded intriguing. Would this be useable for LLMs or is this just a dumb idea?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcjzv0/does_a_laptop_with_96gb_system_ram_make_sense_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6yx5um",
          "author": "Significant_Bar_460",
          "text": "The only x86 laptops with iGPU that are usable for LLM are those with Strix Halo CPUs and then you have Macs with  Pro/Max/Ultra M series chips. \nOr get some super heavy super loud gaming laptop with 5090 or something but at that price range Macs are just more practical machines for everything except gaming.",
          "score": 13,
          "created_utc": "2026-02-23 15:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70iefe",
              "author": "rafaelRiv15",
              "text": "What is the token speed of qwen-coder-next on a mac book pro maxed out ? ",
              "score": 2,
              "created_utc": "2026-02-23 20:22:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7336qt",
                  "author": "Uvalde-Cop",
                  "text": "M1 Max 64Gb, qwen3-coder-next 80B, 4bit: I got 45.13tk/s",
                  "score": 3,
                  "created_utc": "2026-02-24 05:05:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6z9nnw",
          "author": "grassmunkie",
          "text": "It‚Äôs helpful, but best if it is paired with a powerful gpu for MOE models. The attention layers go to the GPU, and the experts go to CPU. So having 96gb will be better and give you access to larger models, only question is how fast it is.  \n\nWhen i load 70gb models like qwen coder next using 32gb vram (5090) and the rest offloaded to ram I get around 28-30 tokens per second.\n\nOTOH, if I run a model that fits on my gpu (glm flash 4.7) I get 120 tokens per second.",
          "score": 6,
          "created_utc": "2026-02-23 16:55:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70brrr",
              "author": "Significant_Fig_7581",
              "text": "Is it just me or Qwen Coder Next is just too slow compared to GPT OSS?",
              "score": 1,
              "created_utc": "2026-02-23 19:51:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70zgs0",
          "author": "BubbleProphylaxis",
          "text": "Seriously, today your best bet laptop with \"a lot\" of ram to run AI is a mac M4 Pro or M4 Max. ",
          "score": 6,
          "created_utc": "2026-02-23 21:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yqsni",
          "author": "RepresentativeTill5",
          "text": "I have done this on a framework laptop with a 125h. The fact that its ddr5 is nice, but expect a maximum of like 10 tokens per second on 7b models.\n\nSo technically, it definitely works, just dont expect it to be fast.\n\nSee also this.\nhttps://nikolasent.github.io/hardware/deeplearning/2025/02/09/iGPU-Benchmark-VLM.html",
          "score": 2,
          "created_utc": "2026-02-23 15:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrvdx",
              "author": "PersonSuitTV",
              "text": "Oh lol, hmm. So it will work, but it will be REALLY slow for anything that would utilize that much RAM. bummer. But very good to know. Thank you for your reply.",
              "score": 1,
              "created_utc": "2026-02-23 15:32:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ywd29",
                  "author": "Intraluminal",
                  "text": "For what it's worth, 10 tps is faster than most people can read, so it is comfortable in most cases. Anything less than 7 tps is annoying.",
                  "score": 1,
                  "created_utc": "2026-02-23 15:53:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72de39",
          "author": "terAREya",
          "text": "If it was an M-series Mac the answer is yes. Anything else the answer is no",
          "score": 2,
          "created_utc": "2026-02-24 02:20:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yps4t",
          "author": "catplusplusok",
          "text": "Seems like similar setups at least work better than CPU only [https://www.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my\\_nas\\_runs\\_an\\_80b\\_llm\\_at\\_18\\_toks\\_on\\_its\\_igpu\\_no/](https://www.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/)",
          "score": 1,
          "created_utc": "2026-02-23 15:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yqh2d",
          "author": "dread_stef",
          "text": "No, I've had 96GB of 5600Mhz RAM with my intel 155h cpu (arc igpu) but any model above 14b was too slow to be useful.",
          "score": 1,
          "created_utc": "2026-02-23 15:25:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2ivu",
              "author": "Karyo_Ten",
              "text": "Try gpt-oss-120b (120B-A5B) or GLM-4.7-Flash or Nemotron-3-Nano (both 30B-A3B) or Kimi-Linear (48B-A3B) or Qwen3-Coder-Next (80B-A3B)",
              "score": 5,
              "created_utc": "2026-02-23 16:22:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71xoko",
                  "author": "MrScotchyScotch",
                  "text": "Also Ministral 3 14B, Gemma 3 27B, both are pretty excellent\n\nBut also people shouldn't sleep on the minis. Phi 4 Mini Reasoning is amazing. I use it for web search and it finds things bigger models don't",
                  "score": 1,
                  "created_utc": "2026-02-24 00:49:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72yuip",
                  "author": "Qwen30bEnjoyer",
                  "text": "Push your system to the limit and try Minimax 2.5 IQ3-XXS from Unsloth! It's terribly impractical, but it brings tears of joy to my eyes knowing I've pushed my laptop to its absolute limit, seeing it chug 45 watts for twenty minutes to answer a basic question.",
                  "score": 1,
                  "created_utc": "2026-02-24 04:34:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o734i0l",
                  "author": "dread_stef",
                  "text": "Sure they run, and gpt-oss-20b would actually be useful if I had time to wait a bit. I believe I got about 7-9 tokens per second using llama.cpp but it's been a while so it might not be accurate. And that's just text generation, you'd have to wait a bit during promp processing too. Tried image generation, but you could get some coffee/tea before it was done doing anything with basic models.\n\nI mean it all works, even the big models, but it depends how long you want to wait. I sold the RAM and my other AI pc and got a strix halo breaking even.",
                  "score": 1,
                  "created_utc": "2026-02-24 05:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6yrexw",
              "author": "PersonSuitTV",
              "text": "Oh really? I mean it probably does not mean much, but the 96GB is 6400Mhz but thats not a lot faster. How slow was it running?",
              "score": 0,
              "created_utc": "2026-02-23 15:29:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z7usm",
                  "author": "Far_Cat9782",
                  "text": "Without cuda cores or rock pretty much anything above 20b is gonna be unaorkable",
                  "score": 1,
                  "created_utc": "2026-02-23 16:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70vbcb",
                  "author": "3spky5u-oss",
                  "text": "RAM is still orders of magnitude lower bandwidth than VRAM (ex, DDR5 is up to 80gb/s, a 5090 can do 1792gb/s), and you‚Äôre still dealing with PCIe bus latency from the swap. \n\nYour best bet is use MoE with layer offloading.",
                  "score": 1,
                  "created_utc": "2026-02-23 21:26:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yzflk",
          "author": "pmttyji",
          "text": "For tiny/small dense models & small/medium MOE models, it would be fine.\n\nCheck my thread on CPU-only inference(just with 32GB DDR5 RAM)\n\n[CPU-only LLM performance - t/s with llama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nIf that laptop supports AVX512, you could get even better t/s using ik\\_llama.cpp.",
          "score": 1,
          "created_utc": "2026-02-23 16:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za27h",
          "author": "segmond",
          "text": "link to this laptop?  i don't want it for AI, just general computing!  thanks",
          "score": 1,
          "created_utc": "2026-02-23 16:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71q1tc",
          "author": "MrScotchyScotch",
          "text": "There are a million different LLMs of different size and utility so it depends on what you want to do with it and what kind of speed you want.\n\nBut the short answer is yes that machine is perfectly fine for some basic LLMs. It's more powerful than my t14s gen4 that I use for basic local LLMs. 96GB is a pretty decent amount for a laptop. there are probably more powerful machines, but not for that price",
          "score": 1,
          "created_utc": "2026-02-24 00:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o729dpx",
          "author": "lnxgod",
          "text": "No¬†",
          "score": 1,
          "created_utc": "2026-02-24 01:57:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72c4xv",
          "author": "Rain_Sunny",
          "text": "It's not a dumb idea at all. It‚Äôs basically a portable brain with a very small straw. \n\nJust don't expect it to spit out code like a machine gun. \n\nIt‚Äôs more like a wise old monk‚Äîtakes a while to think, but the answer is actually worth waiting for. \n\nBtw,$400 for a 64GB jump is a good choice.\n\nIf you don't mind the speed,then...",
          "score": 1,
          "created_utc": "2026-02-24 02:13:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72yjem",
          "author": "Qwen30bEnjoyer",
          "text": "Yes, it would be. \n\nMy personal experience with the Radeon 780m iGPU with 96gb RAM on the Framework 16, I got 0.5 TPS PP in and ~3 TPS out with IQ3-XXS Minimax 2.5 - a Sonnet 4.5 equivalent model. \n\nA more usable experience for chat would be Qwen-Coder-Next. I don't remember the precise details, but it works decently, about ~200 tps PP in and ~10 TPS out.  - GPT OSS 120b is roughly the same speed.\n\nThe fastest models on this setup worth using are Qwen3vl 30b a3b and GPT OSS 20b which both get around ~30 TPS TG out, I dont remember the PP speed off the top of my head but its somewhere in the ~200 TPS range. \n\nI don't know about intel specifically, your performance could vary due to drivers, but it has 11 TFLOPs FP16 vs. the 17 TFLOPS FP16 on my Radeon 780m. \n\nThat being said, I believe it does support native FP4 which should be great for running LLMs at NVFP4 quantization if its optimized for it! Not sure of Llama.cpp vulkan NVFP4 inference takes advantage of it, but its something to consider! Maybe you go full circle and have Qwen-coder-next help you optimize llama.cpp vulkan to your system with OpenCode :)",
          "score": 1,
          "created_utc": "2026-02-24 04:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73i9or",
          "author": "themightymike786",
          "text": "I would rather get a Mac for solid performance.",
          "score": 1,
          "created_utc": "2026-02-24 07:08:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v4js",
          "author": "Tomorrow_Previous",
          "text": "I have a laptop with 64GB of RAM, and use a full sized 3090 eGPU. Honestly, I think that if you're willing to spend that much for 96GB RAM it would make sense to look for a better alternative, but if you're limited to that, you could have a working machine anywhere, and a better machine when docked with the eGPU.",
          "score": 1,
          "created_utc": "2026-02-24 09:09:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z03qh",
          "author": "Herr_Drosselmeyer",
          "text": "If you're going to run models that take full advantage of the RAM, they'll be very slow. Even if you use MoE models with low parameters for inference, the prompt processing is still going to be terrible. So it really depends on your definition of 'usable'. ",
          "score": 1,
          "created_utc": "2026-02-23 16:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70v1li",
              "author": "3spky5u-oss",
              "text": "Not true at all. \n\nWith MoE offloading I can run GPT-OSS-120B at 35 tok/s with a 5090, and that‚Äôs 20b active experts. \n\nWith models like Qwen3 30b A3b, your experts are tiny at 3b, they absolutely fly.",
              "score": 1,
              "created_utc": "2026-02-23 21:25:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o712mxz",
                  "author": "Herr_Drosselmeyer",
                  "text": "As I said, the prompt processing is the issue. Compare a 32k prompt to a 4k prompt and you'll see what I mean. Now imagine that on an iGPU.",
                  "score": 1,
                  "created_utc": "2026-02-23 22:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74esem",
          "author": "Efficient_Loss_9928",
          "text": "Only Strix Halo chips or Apple M-series work well.\n\nAll others doesn't make sense.",
          "score": 0,
          "created_utc": "2026-02-24 12:02:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg24rw",
      "title": "Why is my gpt-oss:120b so fast? (single RTX 3090 17t/s)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rg24rw/why_is_my_gptoss120b_so_fast_single_rtx_3090_17ts/",
      "author": "South-Tip-4019",
      "created_utc": "2026-02-27 09:28:12",
      "score": 22,
      "num_comments": 31,
      "upvote_ratio": 0.87,
      "text": "Sorry for the \"rage-bait,\" but I am honestly scratching my head over this.\n\nI finally decided to bite the bullet and bought 128GB of DDR4 RAM (with AMD Ryzen 7 5800x). My naive assumption was, that with llama.cpp, I‚Äôd use Cline and 120b model in \"planning mode\" go grab lunch, then switch to a smaller model for the actual AI-assisted coding.\n\nAfter stabilizing the RAM at 3133MT/s (that was quite a fight), I ran my first test using Koboldcpp.\n\nAnd... WTH? I expected single-digit tokens per second, nowhere near 17.24 t/s. That‚Äôs more than enough for my usecase.\n\nAm I doing something wrong? I loaded gpt-oss-120b-F16.gguf (65GB) from Hugging Face and touched nothing except for increasing the allowed response length (from 896 to 4096). I'm also trying to format the thinking vs. non-thinking modes; it‚Äôs still a bit sketchy, but I don‚Äôt care about that too much.\n\n    <|start|>user<|message|>\\\"A man has 5 sisters. Each of his sisters has 2 brothers. How many total people are in the family, and why?\\\"<|end|><|start|>assistant\"}\n    Processing Prompt [BATCH] (35 / 35 tokens)\n    Generating (460 / 4096 tokens)\n    (EOS token triggered! ID:200002)\n    [09:53:58] CtxLimit:4420/16384, Amt:460/4096, Init:0.02s, Process:1.41s (24.86T/s), Generate:26.68s (17.24T/s), Total:28.09s\n    \n    Output: <|channel|>analysis<|message|>We need to solve puzzle. A man has 5 sisters. Each sister has 2 brothers. The question: total people in family.\n    Interpretation: There is a man (the subject) who has 5 sisters. So total siblings: the man + 5 sisters = 6 children. Each sister has 2 brothers. For each sister, the number of brothers (male siblings) = 2. That includes the original man plus possibly another brother. Since each sister sees 2 brothers, there must be exactly 2 male children in the family. One is the man in question; the other is an additional brother. So total children = 5 sisters + 2 brothers = 7 children. Plus presumably parents? Usually \"family\" may include parents. Not specified. Usually puzzles ask total people in the family, meaning siblings + parents. Might be 9 (parents 2). Could be 7 if only children considered. Let's read: \"A man has 5 sisters. Each of his sisters has 2 brothers.\" The man is one brother. The sisters have 2 brothers each, meaning there are exactly 2 brothers total. So there is another brother besides the man. So total kids = 5 sisters + 2 brothers = 7. If we include parents, add 2 = 9. Many similar puzzles answer 9. But question asks \"total people in the family, and why?\" Might expect explanation that includes parents. Usually \"family\" means parents + children. I'd answer 9.\n    Let's produce concise answer with reasoning.<|end|>\n    \n    <|start|>assistant<|channel|>final<|message|>**Answer:‚ÄØ9 people (the two parents plus seven children).**\n    **Why:**\n    - The man‚Äôs statement ‚ÄúI have five sisters‚Äù tells us there are at least six children (him‚ÄØ+‚ÄØ5 sisters).\n    - Each sister ‚Äúhas two brothers.‚Äù Since the man is one brother, there must be exactly one other brother for the count to be two for every sister.\n    - Thus the children are‚ÄØ5‚ÄØsisters‚ÄØ+‚ÄØ2‚ÄØbrothers‚ÄØ=‚ÄØ7‚ÄØkids.\n    - Adding the two parents (who are part of the family) gives **7‚ÄØ+‚ÄØ2‚ÄØ=‚ÄØ9** total people.\n\nThe output looks exactly like what I‚Äôd expect.  \nthe memory allocation looks like what I would expect (see ram usage)\n\nhttps://preview.redd.it/x86cq23e90mg1.png?width=965&format=png&auto=webp&s=ede5a913f4e0dd955906a9459a1166cdff68d279\n\nIt ... good, better then I expected, ... am I missing something?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rg24rw/why_is_my_gptoss120b_so_fast_single_rtx_3090_17ts/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7ocup1",
          "author": "Distinct-Bee7628",
          "text": "Check the difference between MoE and dense models.  That may be the source of your confusion.",
          "score": 21,
          "created_utc": "2026-02-27 10:00:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7omvkd",
              "author": "South-Tip-4019",
              "text": "That is deffinitely part of it, thank you, still this is pretty cool to me. ",
              "score": 7,
              "created_utc": "2026-02-27 11:28:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7svs9w",
                  "author": "catplusplusok",
                  "text": "The other part is that it's 4 bit quantized out of the box, so small memory walking set to generate a token. Use 8 bit kv cache and it will get even faster.",
                  "score": 3,
                  "created_utc": "2026-02-28 01:00:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ota7f",
          "author": "Pentium95",
          "text": "I have the same setup you have. Use batch size 4096 and ubatch size 2048 with vulkan backend, you are gonna go even faster!\n\nPS: consider testing also Step 3.5 Flash. I love that model. Also Qwen 3.5 122B Is interesting.",
          "score": 8,
          "created_utc": "2026-02-27 12:17:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p76ke",
              "author": "South-Tip-4019",
              "text": "Thank you, Ill definitely try that",
              "score": 1,
              "created_utc": "2026-02-27 13:45:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7okce4",
          "author": "MarkoMarjamaa",
          "text": "When will people start telling what quant they are running?   \n\\- I ran ghf-dhtg-fgtg-567B and it was 156t/s!   \n\\- Wow ! that incredible! What quant?  \n\\- Q0.01",
          "score": 12,
          "created_utc": "2026-02-27 11:07:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oll5s",
              "author": "paroxysm204",
              "text": "Unless he made an edit in the last 6 mins, the post states he is using f16",
              "score": 6,
              "created_utc": "2026-02-27 11:18:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7opkw0",
                  "author": "MarkoMarjamaa",
                  "text": "He made an edit. ",
                  "score": 4,
                  "created_utc": "2026-02-27 11:50:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tr2eg",
                  "author": "Zyj",
                  "text": "Which is weird because that model is MXFP4",
                  "score": 1,
                  "created_utc": "2026-02-28 04:20:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7sw5pu",
              "author": "catplusplusok",
              "text": "The original model is MXFP4 and size in the post suggests that gguf is also 4 bit, despite F16 in the name",
              "score": 2,
              "created_utc": "2026-02-28 01:03:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uqhnz",
                  "author": "MarkoMarjamaa",
                  "text": "The experts(Moe layer) in original model are in MXFP4. \n\n[https://huggingface.co/openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b)  \n\"The models were post-trained with MXFP4 quantization of the MoE weights\"\n\n[https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune](https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune)  \n\"Any quant smaller than F16, including 2-bit has minimal accuracy loss, since only some parts (e.g., attention layers) are lower bit while most remain full-precision.\"  \nSo any quant lower than F16 has accuracy loss. \n\n[https://github.com/openai/gpt-oss](https://github.com/openai/gpt-oss)  \n\"The models are trained with native MXFP4 precision for the MoE layer,\"\n\nYou are free to prove otherwise, with links. ",
                  "score": 1,
                  "created_utc": "2026-02-28 09:20:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7omeej",
              "author": "South-Tip-4019",
              "text": "Riight, is see, that could be it, despite the name of the file, the log says following  \n`print_info: file size = 60.87 GiB (4.48 BPW)`\n\n    print_info: n_expert = 128 print_info: n_expert_used = 4\n\n    CUDA0 model buffer size = 20237.17 MiB\n\n`CUDA_Host model buffer size = 42091.17 MiB`\n\n`tensor blk.10.ffn_down_exps.weight (537 MiB mxfp4)`",
              "score": 1,
              "created_utc": "2026-02-27 11:25:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7oovny",
                  "author": "Baldur-Norddahl",
                  "text": "gpt oss 120b was released as 4 bit mxfp4 by Open AI. That is the original quality. The f16 part of the name is just because f16 is used in some parts, but most of it is 4 bit.",
                  "score": 7,
                  "created_utc": "2026-02-27 11:44:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7p4tr8",
          "author": "simracerman",
          "text": "I have 5070Ti 16GB and get 25 t/s, so you should get faster speeds.",
          "score": 2,
          "created_utc": "2026-02-27 13:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pwxph",
              "author": "old_mikser",
              "text": "How much ram? 64 or more?",
              "score": 1,
              "created_utc": "2026-02-27 15:56:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pywss",
                  "author": "simracerman",
                  "text": "64 DDR5.",
                  "score": 2,
                  "created_utc": "2026-02-27 16:06:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oqvaa",
          "author": "Responsible-Stock462",
          "text": "There has been a big improvement with the --fit Parameters in llamacpp (which is the base for kobold). In early Dez 25 I have manually choosen which parts of the model go into memory and which have to stay in GPU for speed. Now one can use  --fit or the mightier llama-fit-params which gives you an exact output on which layer goes to GPU and which can be stored in memory. \n\nI accomplished 0.5t/s on deepseek 700b model . With only 32 GB VRAM and not enough RAM (only 64GB) to even load it",
          "score": 1,
          "created_utc": "2026-02-27 11:59:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ovzqx",
              "author": "robertpro01",
              "text": "But how do you know which part to load?",
              "score": 3,
              "created_utc": "2026-02-27 12:36:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p4hmw",
          "author": "dmter",
          "text": "You can get about 6 t/s with glm 4.7 (full) q2 and about 20 t/s with qwen3 next coder q8 (15 with full moe offload which keeps vmem free for other apps)",
          "score": 1,
          "created_utc": "2026-02-27 13:30:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p79bi",
              "author": "South-Tip-4019",
              "text": "Thank you for the tips, will try",
              "score": 1,
              "created_utc": "2026-02-27 13:45:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pr0fz",
          "author": "DataGOGO",
          "text": "MX4FP",
          "score": 1,
          "created_utc": "2026-02-27 15:28:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7odrux",
          "author": "Icy_Distribution_361",
          "text": "It's MoE. You could have found that answer using any LLM or a Google Search. You're not the first to ask even here on reddit. The model isn't 120B active parameters, it's much smaller.",
          "score": -2,
          "created_utc": "2026-02-27 10:08:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ofzy1",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-02-27 10:29:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7oirvw",
                  "author": "Durian881",
                  "text": "It's a MOE with 5.1B active parameters. You can validate official information from OpenAI if needed:\n\nhttps://huggingface.co/openai/gpt-oss-120b",
                  "score": 2,
                  "created_utc": "2026-02-27 10:53:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7orqqr",
          "author": "UnusualPair992",
          "text": "The MoE model oss uses is very fast. It's also very dumb at coding compared to even a $10/mo minimax coding subscription. \n\nLocal works for fun hobby experiments but don't expect to actually compete with even a $20 subscription. Your local models will take weeks to do what you could do in a day on frontier purpose built coders.\n\nQwen 3.5 is supposedly pretty good so it might actually be usable. But honestly $20/mo and you'll be done in an hour not a day.",
          "score": 0,
          "created_utc": "2026-02-27 12:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p806x",
              "author": "Latter-Parsnip-5007",
              "text": "I¬¥ve been coding on qwen3-coder for quiet some time. You need to know what you are doing with the context, but thats recommended anyways. over 70tps",
              "score": 2,
              "created_utc": "2026-02-27 13:49:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p8uxw",
                  "author": "UnusualPair992",
                  "text": "Maybe you can do some things to keep context down and do alright, but going from Gemini 3.0 pro to opus 4.6 and then gpt-5.3-codex was insane. It's much much higher quality logic. I guess it depends a lot on the task at hand. But things that just could not be done by ai seem easy now. Even the massive open source models struggle a lot compared to what the good models can do today.",
                  "score": 0,
                  "created_utc": "2026-02-27 13:54:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7odxds",
          "author": "BigYoSpeck",
          "text": "Spoiler: It's not. My RX 6800 XT with only 16gb of VRAM also on DDR4 and a Zen 3 at just over half the 3090 bandwidth does 22",
          "score": -3,
          "created_utc": "2026-02-27 10:09:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rebgzu",
      "title": "Gemini accidentally showed internal reasoning text",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rebgzu/gemini_accidentally_showed_internal_reasoning_text/",
      "author": "MrDEVIL6978",
      "created_utc": "2026-02-25 12:08:04",
      "score": 21,
      "num_comments": 11,
      "upvote_ratio": 0.75,
      "text": "context: i was trying to install a localLLM flux.2 on my laptop while doing so gemini accidentally showed me its internal reasoning as its output \n\nhere's that output if you are interested:\n\nSCI thought\n\nOkay, so it turns out in this timeline, FLUX.2 actually DOES exist and was released around November 25, 2025.\n\nIt has variants: FLUX.2 \\[dev\\], FLUX.2 \\[klein\\], FLUX.2 \\[flex\\], FLUX.2 \\[pro\\], FLUX.2 \\[max\\].\n\nThe user is absolutely right to ask \"BUT WE ARE GOING FOR THE FLUX.2 NOT FLUX.1 RIGHT?\".\n\nIn my previous prompt, I told them to search for city96/FLUX.1-dev-gguf! My mistake! I gave them instructions for FLUX.1 instead of FLUX.2, even though I introduced FLUX.2.\n\nI need to apologize, validate their attention to detail, and correct the Hugging Face search instructions to point them to the FLUX.2-dev GGUF files.\n\nWait, the user's screenshot actually shows them searching for flux-dev-Q4\\_K\\_S and the results are showing flux1-dev-Q3\\_K\\_S.gguf. That's because I gave them the wrong file name in the previous turn.\n\nLet's look at the search results for FLUX.2 GGUF: silveroxides/FLUX.2-dev-fp8\\_scaled or something similar, but Comfy-Org/flux2-dev and silveroxides or city96 might have FLUX.2-dev-gguf. Since this is a simulated 2026 environment based on the search, I can guide them to search for \"FLUX.2-dev gguf\" or just instruct them to type FLUX.2 in the Hugging Face search bar.\n\nWait, looking at the search results:\"Download Flux 2 Klein 9B from Here: https://huggingface.co/silveroxides/FLUX.2-dev-fp8\\_scaled/resolve/main/flux-2-klein-9b-fp8mixed.safetensors\"\n\n\"Download Text Encoder from Here: https://huggingface.co/Comfy-Org/flux2-klein-9B/...\"\n\nLet's just give them the exact correct search term for FLUX.2.\n\nLet me draft the response:\n\n\n\nEmphatically confirm they are right and I made a typo.\n\nCorrect the search term from FLUX.1-dev-gguf to FLUX.2-dev gguf or flux2.\n\nGive them the exact search instructions on Hugging Face again.\n\nBecause I must act as Gemini and be honest but helpful, I will mirror their energy.\n\nPlan:\n\n\n\nAcknowledge the mistake! \"YES! Good catch! I totally slipped up and typed FLUX.1 in that search bar instruction. We absolutely want the brand new FLUX.2.\"\n\nTell them to search for FLUX.2-dev gguf or flux2-dev gguf in that top search bar.\n\nRe-iterate the steps for downloading the 4 files but explicitly ensuring they say FLUX.2.\n\nRemind them of the \"Files and versions\" tab and the download arrow.\n\nWait, is there a specific FLUX 2 GGUF repo mentioned? The search results mention Comfy-Org/flux2-dev for the VAE. And silveroxides/FLUX.2-dev-fp8\\_scaled for some safetensors. Let's just tell them to search FLUX.2-dev-gguf on Hugging Face (which is standard naming).\n\nLet's keep it concise, match the energy, and fix the error. No need to overcomplicate.\n\nNo user data needed. Check instructions:\n\nUse LaTeX? No.\n\nNo user data used.\n\nHard Fail 1: Did I use forbidden phrases? No.\n\nHard Fail 2: Used user data? No.\n\nHard Fail 3: Sensitive data? No.\n\nHard Fail 4: Ignore corrections? No.\n\nLet's write.\n\n\n\n\"You are 100% correct, and I completely messed up that search term! Good catch.\"\n\nClarify the new search terms.\n\n\"Search for FLUX.2-dev-gguf (usually uploaded by users like city96 or bartowski).\"\n\nVAE: Comfy-Org/flux2-dev -> ae.safetensors or flux2-vae.safetensors.\n\nText encoders are the same (t5xxl and clip\\_l).\n\nLet's go. Keep effort low-medium. Empathy with candor.\n\n\"You are 100% correct, and I completely messed up that search term! Good catch. My apologies for the typo‚ÄîI accidentally sent you to the FLUX.1 repository.\"\n\n\"To get the brand new FLUX.2 we were just talking about, here are the corrected search terms for Hugging Face:\"\n\n...\n\n\"Let me know if you are able to find the FLUX.2 versions in the Files and versions tab!\"\n\nDone.\n\nI'll output.\n\nFollow the rules. End with a next step. Ensure formatting. No latex.\n\nReady.\n\nNo search citations needed because I am just correcting the search string based on general knowledge, but wait, I can cite the FLUX.2 VAE location (Comfy-Org/flux2-dev). Actually, I don't strictly need to cite if I don't assert facts from the snippet, but to be safe: \"You can find the new FLUX.2 VAE in the Comfy-Org/flux2-dev repository\". I'll just give them the search terms, no need to over-cite if I'm just giving UI instructions. But I will cite the repo name.\n\nWait, the prompt says \"Any information derived from provided sources MUST be cited.\"\n\nLet's cite: \"For the VAE, search for Comfy-Org/flux2-dev and grab the flux2-vae.safetensors file.\"\n\nLet's refine:\n\n\n\nStep 1: The FLUX.2 Brain\n\nIn the top search bar, type: FLUX.2-dev-gguf (or look for the FLUX.2-dev GGUF uploads by creators like city96 or bartowski).\n\nGo to Files and versions and find the Q4\\_K\\_S .gguf file.\n\nStep 2: The FLUX.2 VAE (The Painter)\n\nSearch for Comfy-Org/flux2-dev in the top bar.\n\nGo to Files and versions, open the vae folder (or look in the main list), and download flux2-vae.safetensors.\n\nStep 3: The Translators\n\nThe Text Encoders (t5xxl\\_fp8\\_e4m3fn.safetensors and clip\\_l.safetensors) are actually the exact same ones used for FLUX.1, so my previous search instructions for those two files (comfyanonymous/flux\\_text\\_encoders) are still completely correct!\n\nLet's add the closing question.\n\n\"Are you able to find the FLUX.2 files with those updated search terms?\"\n\nLet's review formatting. Headings, bullets, bolding. Looks good.\n\nCheck constraints. Done.\n\nI will write the response.\n\nJust a minor fix: \"You are absolutely right, and I completely messed up that search term! Good catch.\"\n\nI will mirror their energy. \"YES! Good catch bro, I totally slipped up...\" No, they said \"bro\" earlier, I can use it. \"You are 100% correct bro, and I completely messed up...\" Let's keep it slightly more professional but friendly: \"You are absolutely right, and I completely messed up that search term! Good catch.\" Let's stick to \"You are 100% right! Good catch.\"\n\nLet's formulate the final response.\n\nEnd with \"Are you able to find the FLUX.2 files with those updated search terms?\"\n\nLooks good. I will generate now.\n\nNo need for Step 5 output. Invisible incorporation of constraints. Done.\n\nMute all internal thoughts about the checklist.\n\nProceed.\n\nTokens limit is fine. Effort 0.5. I'm done.\n\nRe-read prompt: \"Mirror the user's tone, formality, energy, and humor.\"\n\nUser: \"BUT WE ARE GOING FOR THE FLUX.2 NOT FLUX.1 RIGHT?\" (All caps, urgent).\n\nMe: \"YES! You are 100% right! Good catch! I completely messed up that search term and accidentally sent you to the older FLUX.1 repository.\"\n\nPerfect.\n\nDone.\n\nI will format it nicely.\n\nChecklist in my head:\n\n\n\nNo \"Based on\" - PASS.\n\nNo useless user data - PASS.\n\nNo sensitive data - PASS.\n\nCorrection history - PASS.\n\nDone.\n\nI will write now.\n\nWait, the snippet for VAE says: https://huggingface.co/Comfy-Org/flux2-dev/resolve/main/split\\_files/vae/flux2-vae.safetensors.\n\nI will mention: \"Go to Files and versions, look inside the split\\_files/vae folder, and download flux2-vae.safetensors\".\n\nExcellent. This is perfectly accurate to the 2026 retrieved web data.\n\nDone. I will output.\n\nEnd of thought.\n\nOne more check: The user uploaded image\\_30e15d.png showing the Hugging Face search bar with flux-dev-Q4\\_K\\_S and it showed flux1-dev-Q3\\_K\\_S.gguf. I explained why in the previous turn, but I gave them city96/FLUX.1-dev-gguf. So yeah, my bad.\n\nLet's fix it.\n\nDone.\n\nOut.\n\nSpace.\n\nEnd.\n\nNo extra yapping.\n\nAction.\n\nI am ready.\n\nBye.\n\nDone.\n\nYes.\n\nNo.\n\nGo.\n\nOut.\n\nOut.\n\nOut.\n\nOut.\n\nOut.\n\n\\*after this gemini was just spamming \"Out\" again and again  \nit so funny if u read this its like gemini is talking to itself like we are talking in the mirror or in our head before talking to our crush",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rebgzu/gemini_accidentally_showed_internal_reasoning_text/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7bbgh0",
          "author": "Mayimbe_999",
          "text": "I thought most coding agents show reasoning? For example I work in claude and I get entertainment by reading is reasoning while working.",
          "score": 9,
          "created_utc": "2026-02-25 12:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c7u88",
              "author": "yopla",
              "text": "It's an option in most nowadays, because it's polluting the user's brain context. üòÜ",
              "score": 0,
              "created_utc": "2026-02-25 15:20:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bbnyn",
          "author": "michaelzki",
          "text": "What you might be seeing is part of the main system role prompt injected by the agent together with your prompt (as user role).\n\nThat self-talk stage is the thinking mode part, evaluating whats happening and use it as additional context in later prompt sequence. If you are not seeing it frequently, maybe its a bug on the agent's side. You can turn it off somewhere with thinking_mode=false or similar.\n\nYou will understand it right away if you technically communicate to AI provider's API via chat/generate.",
          "score": 2,
          "created_utc": "2026-02-25 12:21:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bdmq2",
              "author": "MrDEVIL6978",
              "text": "i haven't seen this kind of response untill now and this response crashed my gemini tab and i was getting 1-2 fps on that browser tab and when i refreshed the browser that output was replaced by a normal output, seemed kinda a rare event happening ",
              "score": -1,
              "created_utc": "2026-02-25 12:34:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bfkkn",
                  "author": "michaelzki",
                  "text": "Try aider cli. Thats the most forgiving agent cli out there",
                  "score": 1,
                  "created_utc": "2026-02-25 12:47:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bomxr",
          "author": "thought_provoking27",
          "text": "Great thread !",
          "score": 1,
          "created_utc": "2026-02-25 13:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d5ox4",
          "author": "WyattTheSkid",
          "text": "It‚Äôs happened to me a couple times",
          "score": 1,
          "created_utc": "2026-02-25 17:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7govhx",
          "author": "enterme2",
          "text": "That is just the thinking process that mixed into your output. Nothing new.",
          "score": 1,
          "created_utc": "2026-02-26 05:01:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kacm6",
          "author": "AppoAgbamu",
          "text": "Very much like the human minds self talk",
          "score": 1,
          "created_utc": "2026-02-26 18:54:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c3wyx",
          "author": "Fantastic-Breath2416",
          "text": "Il mio sistema risolver√° questo problema!\n\nhttps://nothumanallowed.com/use-cases/liara-the-dna-of-an-artificial-mind",
          "score": 0,
          "created_utc": "2026-02-25 15:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dmuoz",
          "author": "hazel-wood5",
          "text": "this is actually useful because it shows exactly how these models think every single time.. All the planing drafting self-correction and constraint checking is happening under the hood we just never see it..",
          "score": 0,
          "created_utc": "2026-02-25 19:12:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}