{
  "metadata": {
    "last_updated": "2026-02-11 09:06:59",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 311,
    "file_size_bytes": 381853
  },
  "items": [
    {
      "id": "1qwjgj4",
      "title": "Anyone here actually using AI fully offline?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qwjgj4/anyone_here_actually_using_ai_fully_offline/",
      "author": "Head-Stable5929",
      "created_utc": "2026-02-05 12:02:24",
      "score": 164,
      "num_comments": 136,
      "upvote_ratio": 0.96,
      "text": "I keep coming back to the idea of running AI locally you know, like a GPT-style assistant that just works on your own device without the internet or Wifi connection?\n\nNot to build anything serious or commercial. I just like the idea of being able to read my own files, understand things or think stuff through without relying on cloud services all the time. Especially when there is no connection, internet services change or when things gets locked behind paywalls.\n\nEvery time I try local setups though, it feels more complicated than it should be. The models work, but the tools feel rough and itâ€™s easy to get lost tweaking things when you just want something usable.\n\nI'm just curious if anyone here actually uses offline AI day to day or if most people try it once and move on. I would really be interesting to hear what worked and what didnâ€™t.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qwjgj4/anyone_here_actually_using_ai_fully_offline/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3pg7d0",
          "author": "Neun36",
          "text": "You have different options, easist is LM Studio as already mentioned, search for the Model on huggingface which suits your GPU (fast response) or your RAM (slow response). Then there is Ollama which also runs locally and you can search for Models on ollama Web Page and how to. Then there is openwebUI which is Local Web UI which runs locally and Access to UI via Browser and it Looks more Like ChatGPT but you have the control, you can combine this with ComfyUI to generate images too but itâ€™s more complicated. There are many other options available and above one are just a tiny bit of Tipps and easy ways.",
          "score": 48,
          "created_utc": "2026-02-05 12:36:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qhnyk",
              "author": "Head-Stable5929",
              "text": "Thanks a lot that helps!",
              "score": 3,
              "created_utc": "2026-02-05 15:58:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wqoqf",
                  "author": "FlatImpact4554",
                  "text": "The guy above me nailed it . Listen to him.  You'll have the same exact web page and results like online . But private.",
                  "score": 1,
                  "created_utc": "2026-02-06 14:44:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3tp7st",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-06 01:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uhbfp",
                  "author": "Neun36",
                  "text": "Depends on, Most Common is ComfyUI and there you have all the Image, Video, Audio, 3d Models which you can Play with, they integrated also already a UI which reminds of swarmUI and else, so more user friendly but generating stuff locally depends on your GPU and RAM, Not only GPU is crucial for ComfyUI especially for Video Generation, to combine that with openwebUI to have the look of ChatGPT is possible with openwebUI But I think there is no Video Generation implemented in openwebUI (didnâ€™t checked the latest Version, but last i checked 3 weeks ago there was none.) But this also can be solved.\n\nFor Video + Audio Like Sora there is LTX2 which runs locally but as said depends on gpu and ram. There are also other models like wan 2.2, scail, Move and many more depends on your use case.",
                  "score": 2,
                  "created_utc": "2026-02-06 04:24:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pfpk8",
          "author": "dsartori",
          "text": "Yeah Iâ€™m all offline. My use cases are coding.  consulting, and community organizing. \n\nOnly the coding workflows really require a beefy setup. My teammate who is a pure consultant uses gpt-oss-20b in LMStudio pretty heavily but not exclusively.",
          "score": 18,
          "created_utc": "2026-02-05 12:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3q3rxv",
              "author": "Recent_Double_3514",
              "text": "What type of consulting?",
              "score": 4,
              "created_utc": "2026-02-05 14:51:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q8oyp",
                  "author": "dsartori",
                  "text": "We do data engineering work, software development, organizational development (strategic planning and so on), education and training in a variety of fields. I run my document authoring workflows through Cline these days.",
                  "score": 3,
                  "created_utc": "2026-02-05 15:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3q8unt",
              "author": "woundedkarma",
              "text": "He just chatting with it? Or does he run agents?",
              "score": 1,
              "created_utc": "2026-02-05 15:17:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q9fnw",
                  "author": "dsartori",
                  "text": "She's mostly chatting with it in LMStudio. We tried it with Cline but the results were pretty poor.",
                  "score": 6,
                  "created_utc": "2026-02-05 15:19:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3q3j7z",
              "author": "HumanThing1233",
              "text": "What are you using for coding? I can't figure out what kind of hardware I need to do so.",
              "score": 1,
              "created_utc": "2026-02-05 14:50:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q93xy",
                  "author": "dsartori",
                  "text": "Strix Halo. I got the GMKTec one. It's a cheap path to good performance from midsized MoEs.",
                  "score": 7,
                  "created_utc": "2026-02-05 15:18:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3q8tko",
                  "author": "Rand_o",
                  "text": "Strix halo 395+ 128 GB is pretty versatile",
                  "score": 4,
                  "created_utc": "2026-02-05 15:16:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pexse",
          "author": "DAlmighty",
          "text": "Iâ€™m 100% offline and loving it.",
          "score": 54,
          "created_utc": "2026-02-05 12:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3q84yp",
              "author": "p_235615",
              "text": "I use beside many apps like maubot, vscode cline, n8n, homeassistant also running open-webui in docker as a simple interface for chats or voice interaction with AI.",
              "score": 6,
              "created_utc": "2026-02-05 15:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3pfj4h",
              "author": "Head-Stable5929",
              "text": "can you tell me more about what you did and how you did it? im collecting info, so your input will be useful!",
              "score": 4,
              "created_utc": "2026-02-05 12:31:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3qpc6q",
                  "author": "DrummerHead",
                  "text": "On Mac:\n\n\n    brew install lm-studio\n    brew install draw-things\n\n\nThey both give you the ability to download models from within the app. Find the documentation for both and get experience over time.\n\nIf you want two good starting models for each:\n\n* https://huggingface.co/openai/gpt-oss-20b\n* https://huggingface.co/Tongyi-MAI/Z-Image-Turbo\n\nYou don't really need the links, you can just choose them from the menus within each program.\n\nAnd that's it! From there, the more independent research you do, the more value you'll be able to extract from these programs and the many models you'll find and get experience with. Enjoy!",
                  "score": 11,
                  "created_utc": "2026-02-05 16:33:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pg5ee",
                  "author": "DAlmighty",
                  "text": "I run vLLM in docker for inference and use the API to connect various services. Which is what everyone does. Youâ€™re not going to get anything different from anyone if youâ€™re speaking only about inference.",
                  "score": 16,
                  "created_utc": "2026-02-05 12:35:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pmhbb",
                  "author": "Karyo_Ten",
                  "text": "I rotate between vLLM, SGLang, ik-llama.cpp, TabbyAPI / ExLlamav3 in docker depending on model support, quantization, etc. I just connect to them through \"OpenAI-compatible\"",
                  "score": 4,
                  "created_utc": "2026-02-05 13:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pieit",
          "author": "DatBass612",
          "text": "I bought the most expensive M3 Ultra and worried a ton about getting to positive ROI running OSS 120B. I ended up hitting that positive number in 5 months. \n\nI use about 200$ish in tokens a day by my estimates. Now with stuff like OpenClaw itâ€™s only going to be more tokens and Iâ€™m glad I have the extra unified memory to have virtualization run sub agents.",
          "score": 13,
          "created_utc": "2026-02-05 12:50:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pm0yj",
              "author": "Front-Relief473",
              "text": "However, m3 ultra's computing power is so low, its prompt prefill speed is too slow, and the amount of contexts you carry in handling complex tasks with agent is too large, which leads to a very long time for you to receive the first word in each round. How do you handle this situation?",
              "score": 3,
              "created_utc": "2026-02-05 13:13:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pupmp",
                  "author": "beachguy82",
                  "text": "Define â€œlong timeâ€. Iâ€™ve been considering a Mac Studio myself.",
                  "score": 3,
                  "created_utc": "2026-02-05 14:02:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3q6dtl",
                  "author": "Crazyfucker73",
                  "text": "Incorrect",
                  "score": 1,
                  "created_utc": "2026-02-05 15:04:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3qxlin",
                  "author": "andreabarbato",
                  "text": "I have a topk implementation for llama.cpp that can make prefill faster sometime. wanna try? hmu (it's free on github but I never tested it on apple, might take some tweaking)",
                  "score": 1,
                  "created_utc": "2026-02-05 17:12:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3rdyf0",
                  "author": "Fair-Cookie9962",
                  "text": "Why long time is an issue? Parallel agents cut the time. If you have reliable output, time does not matter that much, and if you adjust tasks to utilise smaller models, can get nice speedups. Also using llama.cpp directly also often cuts wait in half.",
                  "score": 1,
                  "created_utc": "2026-02-05 18:27:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xj3l4",
                  "author": "DatBass612",
                  "text": "I mean slow is relative. Sure my Nvidia GPU is faster, but I have the 70GB model loaded with overhead to spawn sub models. But itâ€™s more than incredibly fast for what I need it for. Itâ€™s queue based on openclaw and I can schedule things out for night time when there is low usage. My token context is 128000 and batching at 10000. Itâ€™s absolutely prosumer grade. People who want the utmost speed should just pay for Anthropic or Gemini. Unless you have massive budget backed by enterprise of some sort these are the trade offs you have to make. As a consumer i say anything over 5 tokens a second is usable and i cant really read responses faster than that. Plus the scheduling and batching has me come back to read things after long tool call process chains. \n\nWhats important to remember is to ask yourself, how much AI garbage data are you actually going to read, and is what it is producing even valuable that you need blazing fast speeds.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:00:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3q8jgu",
              "author": "_hephaestus",
              "text": "I have a similar setup, is OSS 120B still the best in class?  Also just surprised at the cost, I didn't think you _could_ burn through that many a day, but the billing details at work are hidden from me.",
              "score": 1,
              "created_utc": "2026-02-05 15:15:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xjiir",
                  "author": "DatBass612",
                  "text": "Best in class, maybe. Itâ€™s great for what Iâ€™m using it for. You can get way more complicated with specialist smaller models. But Iâ€™m not willing to dedicate more time to this project which already is like 40+ hours of setup",
                  "score": 1,
                  "created_utc": "2026-02-06 17:02:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xkmpk",
                  "author": "DatBass612",
                  "text": "I evaluate solely based on how the agent responds. So with GLM and Mistral and the others they just felt â€œoffâ€ completely personal preference.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3v9k23",
              "author": "holdmymandana",
              "text": "What do you use open claw for?",
              "score": 1,
              "created_utc": "2026-02-06 08:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xhzd7",
                  "author": "DatBass612",
                  "text": "I use it for outreach automation, a newsletter for my customers, and a help chatbot. More plans Iâ€™m still formulating but itâ€™s a great scheduled research assistant. I also have it moderate some chats.",
                  "score": 1,
                  "created_utc": "2026-02-06 16:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pnpyi",
          "author": "michael_p",
          "text": "I run qwen 3 32b on a Mac Studio m3 ultra with 96 gb ram. I run confidential information through it and get incredible results (Claude code wrote the prompts). Itâ€™s an invaluable tool for me.",
          "score": 10,
          "created_utc": "2026-02-05 13:23:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vnq4b",
              "author": "tuxmaniac",
              "text": "What do you mean by \"Claude code wrote the prompts\"? Can you explain your setup a bit more? Thank you!",
              "score": 1,
              "created_utc": "2026-02-06 10:28:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o431yq4",
                  "author": "michael_p",
                  "text": "Yes!! Iâ€™m so excited to geek out over how good qwen3 is. I had Claude code build me a platform to find businesses for sale. When I reach out to the seller, theyâ€™ll send me confidential documents like bank statements and tax returns etc. I can drop those into a documents page on the system Claude code built and have it run a local analysis with qwen3. I worked with Claude code to explain what the analysis should do and it made them leaps and bounds more detailed than what I would ask for. So Claude code built out all the prompts of how the documents get loaded in and then what it asks for from qwen. It was having issues with context windows (eg loading in 12 bank statements or more) so Claude code built a way to chunk those out, get a response and then feed that response back in with the next set to get a full and accurate analysis. Qwen makes minor mistakes but Claude has been able to understand them and tweak them.",
                  "score": 3,
                  "created_utc": "2026-02-07 14:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fn7e1",
              "author": "LinkAmbitious8931",
              "text": "I am thinking of doing the same for contract analysis, but have yet to find a model around 32b capable enough compared to OpenAI API using 5.1. If you can recommend a solid model, please share.\n\n",
              "score": 1,
              "created_utc": "2026-02-09 14:05:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3q6xeu",
          "author": "FaceDeer",
          "text": "I've been using a system of custom scripts to process and summarize transcripts of audio files containing a lot of personal information, so strictly local AIs for that. I recently tried playing around with [AnythingLLM](https://anythingllm.com/) and found it had a pretty good \"NotebookLM\"-style system that was easy to use, I dumped a bunch of data into it and that worked nicely.",
          "score": 8,
          "created_utc": "2026-02-05 15:07:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q7eda",
          "author": "ionizing",
          "text": "I spent several months building my own tool enabled Ai chat interface that uses llama.cpp and I love it.",
          "score": 7,
          "created_utc": "2026-02-05 15:09:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qoit2",
          "author": "henk717",
          "text": "Of course, I am one of the people involved with KoboldAI and KoboldCpp so its natural that I run our own stuff and enjoy local AI. If I wasn't passionate about running local I wouldn't be involved that heavily in a community that focus on running it yourself (or hosting it in the cloud with the same software).",
          "score": 5,
          "created_utc": "2026-02-05 16:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qa12g",
          "author": "woundedkarma",
          "text": "I've been thinking setting up local AI for people will become a business. Just not sure how to find less technical people who feel like you guys and want it local. ðŸ¤”",
          "score": 5,
          "created_utc": "2026-02-05 15:22:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xmye",
              "author": "Head-Stable5929",
              "text": "I know right! I want to make one that most likely runs offline, I'm kinda stuck and I want to know the other options",
              "score": 1,
              "created_utc": "2026-02-07 04:02:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pt9b6",
          "author": "segmond",
          "text": "Yes, for 2 years now.",
          "score": 3,
          "created_utc": "2026-02-05 13:54:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q5974",
          "author": "Craygen9",
          "text": "I batch process documents to create summaries as part of an automated workflow, I want to keep everything local for privacy reasons. I've used the API of a local install LM Studio and it was easy to set up and worked well, but now I use llama.cpp with my own code as I have more control over it.",
          "score": 4,
          "created_utc": "2026-02-05 14:58:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qbwmu",
          "author": "GravitationalGrapple",
          "text": "I use kobold.cpp for LLMs, comfyui for image/video generation, and indextts2 with its own custom gui for TTS. I only have 16gb VRAM, 64 RAM, so I am somewhat limited in the models I use.",
          "score": 4,
          "created_utc": "2026-02-05 15:31:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pcwx5",
          "author": "ptear",
          "text": "If you haven't used LM Studio, try it.",
          "score": 8,
          "created_utc": "2026-02-05 12:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pdw1p",
              "author": "Head-Stable5929",
              "text": "i see, ill check it out! thanks for the suggestionnnn, could you also tell me if you have come across ollama, it seemed a bit similar and what could possibly be the difference between them?",
              "score": 2,
              "created_utc": "2026-02-05 12:19:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ppyx9",
                  "author": "ThatHavenGuy",
                  "text": "As a beginner, LMStudio has a sleek interface that makes setting this stuff up pretty easy and can help you learn more about the different ways to run local models and mess with the parameters. Ollama gets out of your way and runs as a service behind the scenes. Both are good stepping stones to better, more complicated setups like using llamacpp's router function or vLLM. I'd say give LMStudio a go and see how it treats you.",
                  "score": 8,
                  "created_utc": "2026-02-05 13:36:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3qat8j",
                  "author": "GravitationalGrapple",
                  "text": "Llama.cpp > ollama. LM studio uses llama.cpp, just has a beginner friendly GUI.",
                  "score": 2,
                  "created_utc": "2026-02-05 15:26:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3qe8gu",
          "author": "Virtual_Actuary8217",
          "text": "Gptoss 120b for tasks like planning,qwen coder 3 next for heavy python coding , I have a custom c api that exposed to python, I just need to feed the api.md , qwencoder3 next works surprisingly well, one shot, but it is slow like 6/t per seconds, it needs 80g of memory to work, still it's free , local ,that's important for my use cases",
          "score": 3,
          "created_utc": "2026-02-05 15:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qj1pu",
          "author": "Resonant_Jones",
          "text": "im almost there, I had to build my own system because like you said, the tools available are a little rough. I love having full control over the system though and nothing unexpected changes ever. \n\nYou can literally run the computer all day long coding or other tasks and it just doesn't cost anymore than electricity. I love it. Highly encourage anyone interested to get into it. \n\n",
          "score": 3,
          "created_utc": "2026-02-05 16:04:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ta2m8",
              "author": "rolleicord",
              "text": "What did you build it in? Details please :)",
              "score": 3,
              "created_utc": "2026-02-06 00:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3tgc5k",
                  "author": "Resonant_Jones",
                  "text": "Mostly Python and used Codex to build most of it with sprinkles of Claude and Gemini in between. What do you want to know?",
                  "score": 2,
                  "created_utc": "2026-02-06 00:39:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rrobl",
          "author": "fallingdowndizzyvr",
          "text": "I only use AI offline.",
          "score": 3,
          "created_utc": "2026-02-05 19:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xqn8",
              "author": "Head-Stable5929",
              "text": "Can you tell me more please",
              "score": 1,
              "created_utc": "2026-02-07 04:03:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o413x06",
                  "author": "fallingdowndizzyvr",
                  "text": "You download AI. You run AI. It's offline.",
                  "score": 2,
                  "created_utc": "2026-02-07 04:47:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rz9w1",
          "author": "X3r0byte",
          "text": "The problem is you can make this as complicated as you want it to be, and so without a specific use case youâ€™re going to try and boil the ocean.\n\nMainly I draw the line with anything privacy focused, like working through personal problems, interpreting docs I do not want sent to a server, things of that nature. OWUI + name_your_model is more than enough for that.\n\nI also have a totally custom assistant with speech to text/text to speech that generates and runs code inside a container arbitrarily. Thatâ€™s for â€œfunâ€.\n\nI also use cloud models for code if itâ€™s unreasonable to expect quality out of my hardware.\n\nI guess the point is - â€œit depends.â€",
          "score": 3,
          "created_utc": "2026-02-05 20:06:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s461v",
          "author": "SweetHomeAbalama0",
          "text": "Yes, we have a 768Gb Ai server for this exact thing (a local \"chatGPT\"-like replacement).\n\nCombine something like GLM 4.6v (vision support) with ComfyUI Flux/ZIT/QwenEdit/etc. and you can get something that closely resembles GPT without any token restrictions. It might not be a 1:1 exact, but if you have patience to set up the moving parts, front end, back end, tool calling, etc., the result can accomplish 95-98% of what chatGPT can do. Feedback I've received has actually been that the primary users \"prefer\" the new local alternative over chatGPT, citing that chatGPT is slower for things like image gen/edits with not enough variation in art style, and can be more finicky understanding prompts if the user is not a strong English speaker.\n\nIt comes with an initial startup cost and probably some technical aptitude, but it can absolutely be done.",
          "score": 3,
          "created_utc": "2026-02-05 20:30:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s5d1m",
              "author": "Clear-Astronaut-8006",
              "text": "Curious on how you made the server? Approx. Cost and put in context on build date. Thanks.",
              "score": 3,
              "created_utc": "2026-02-05 20:35:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3yt8hy",
                  "author": "SweetHomeAbalama0",
                  "text": "I have a deep dive post on localllama about the server with the details you may be looking for. Total est cost \\~17k usd but these are prices from before the component price spike. \n\n[The post](https://www.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/)\n\nThere's a link in the post body to the full youtube video if you want better quality, it is a little rough around the edges since it was my first time doing a show/demo like this but it touches on what I thought most people would be interested to know. I will say, the case is what made it all work, so I dedicated a good chunk of time just talking about the case. Feel free to skip to whatever part you want to see tho, time stamps are in the YT description.",
                  "score": 3,
                  "created_utc": "2026-02-06 20:44:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3sxv8q",
          "author": "Bino5150",
          "text": "I run completely local. I have a few different ways to set it up. Lately Iâ€™ve been running LM Studio with AnythingLLM as my agent. Been playing around with Agent Zero, OpenClaw, and n8n as well, but still using LM Studio or Ollama. Whisper for STT, and Piper for TTS.",
          "score": 3,
          "created_utc": "2026-02-05 22:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tkqgc",
          "author": "ScuffedBalata",
          "text": "The tools ARE rough and require setup.  There's no \"plug and play assistant\".\n\nThey require intense amounts of hardware and some willingness to set up complex workflows.  It's still \"techie coder\" territory.",
          "score": 3,
          "created_utc": "2026-02-06 01:04:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pekjm",
          "author": "custodiam99",
          "text": "It is f\\*cking slow, but yeah, you can do serious stuff with it.",
          "score": 6,
          "created_utc": "2026-02-05 12:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pfcnd",
              "author": "DAlmighty",
              "text": "Sounds like you need more VRAM. \n\nI personally will NEVER advise anyone to use system RAM for inference. It stops being worth it to me if you use it.",
              "score": 10,
              "created_utc": "2026-02-05 12:30:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q31me",
                  "author": "custodiam99",
                  "text": "I have 96GB DDR5 and 24GB VRAM, and I think that's all the RAM I will have for the foreseeable future lol.",
                  "score": 5,
                  "created_utc": "2026-02-05 14:47:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pfmj7",
                  "author": "Head-Stable5929",
                  "text": "why is that? can you explain it for me please?",
                  "score": 2,
                  "created_utc": "2026-02-05 12:32:06",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o3pgn67",
                  "author": "Electrical_Hat_680",
                  "text": "Or get a AI GPU like Modivius by Intel off eBay.",
                  "score": 2,
                  "created_utc": "2026-02-05 12:39:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pbnft",
          "author": "codeforgeai",
          "text": "Yes i'm running ai fully offline and it's actually worth it",
          "score": 11,
          "created_utc": "2026-02-05 12:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pbs5j",
              "author": "Head-Stable5929",
              "text": "can you tell more on what do you think about it, please? it would be really helpful",
              "score": 4,
              "created_utc": "2026-02-05 12:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pkjqm",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": -14,
                  "created_utc": "2026-02-05 13:04:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ph7tb",
          "author": "LondonTownGeeza",
          "text": "Run mine on a i5 with a 6600, slightly slower than online OpenAI, but very useable and private.",
          "score": 2,
          "created_utc": "2026-02-05 12:42:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pi4mr",
          "author": "Large_Election_2640",
          "text": "Iâ€™m running stable diffusion on comfyui fully\nLocal. Playing with image models. It works fine on my 3060 12gb gpu with 24gb ram.",
          "score": 2,
          "created_utc": "2026-02-05 12:48:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pvn1o",
          "author": "Klutzy_Ad_1157",
          "text": "Yes only offline. I use Gemma 3 27b in 5 bit quant on a RTX 3090 because it speaks German very well. On my main rig I use a RTX 4090 and a RTX 5090 for image and video generation with ComfyUI.",
          "score": 2,
          "created_utc": "2026-02-05 14:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qfnqe",
          "author": "Ok-Dog-4",
          "text": "Try Localai.io. Youâ€™re able to run basically any model and the docker configuration isnâ€™t the most complicated. Works offline in your terminal. I sometimes will finetune with a LoRA in a CoLab notebook and use my new model with localai. Then also you can API to practically anything. Cheers",
          "score": 2,
          "created_utc": "2026-02-05 15:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qhww9",
          "author": "gptlocalhost",
          "text": "\\> document authoring\n\nCurious if the following hybrid approach might be useful:\n\n  [https://youtu.be/RkxbCAaZ7Dw](https://youtu.be/RkxbCAaZ7Dw)\n\n",
          "score": 2,
          "created_utc": "2026-02-05 15:59:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qi9g3",
              "author": "Head-Stable5929",
              "text": "Omg thank you very much!",
              "score": 1,
              "created_utc": "2026-02-05 16:01:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qoewy",
          "author": "aniketmaurya",
          "text": "Ollama and chill!",
          "score": 2,
          "created_utc": "2026-02-05 16:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r391e",
          "author": "shibe5",
          "text": "My AI uses DNS, so it won't work without internet connection.\n\nAs for the tools, I mostly use my own.",
          "score": 2,
          "created_utc": "2026-02-05 17:38:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ri8nz",
              "author": "Fair-Cookie9962",
              "text": "You can have local DNS, just like your router.",
              "score": 2,
              "created_utc": "2026-02-05 18:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ruogu",
                  "author": "shibe5",
                  "text": "Yes, I can have the router resolve relevant names without using the internet. But currently, it's not configured that way.",
                  "score": 2,
                  "created_utc": "2026-02-05 19:44:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rd8gr",
          "author": "Fair-Cookie9962",
          "text": "Yes, why? All is in the prompt.",
          "score": 2,
          "created_utc": "2026-02-05 18:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rezec",
          "author": "maximebodereau",
          "text": "Yes ! Check my ux tool, I need feedbacks on it ! \nYou have to have a llamacpp installed on your machine \n\nhttps://ux-research-production.up.railway.app/",
          "score": 2,
          "created_utc": "2026-02-05 18:32:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rt3t2",
          "author": "Eupolemos",
          "text": "Sure.\n\nI have a 5090. I use Gemma 3 for stuff not related to development and Devstral 2 for my programming needs.\n\nAt work, we use Claude Code, but Devstral 2 works fine for my personal needs.",
          "score": 2,
          "created_utc": "2026-02-05 19:37:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s2ui6",
          "author": "Macdaddy4sure",
          "text": "I wrote a program for offline ai extensibility.\nhttp://macdaddy4sure.ai/",
          "score": 2,
          "created_utc": "2026-02-05 20:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sap2k",
          "author": "GlassAd7618",
          "text": "Iâ€™m experimenting with Ollama and the latest models.",
          "score": 2,
          "created_utc": "2026-02-05 21:01:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t4a3o",
          "author": "Express_Quail_1493",
          "text": "i use openwebui with python and connect it to my localhost models running on lmstudio server. i expose it through a cloudflare tunnel and talk to my localhost AI on my phone anywhere via voice ",
          "score": 2,
          "created_utc": "2026-02-05 23:30:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tfiln",
          "author": "Limebird02",
          "text": "Main bottleneck is having the memor, GPU and overall horsepower to run a worthy model that's significant enough to provide useful intelligence to you. I'd say it will cost at least $1400 in computing hardware. Most people can't justify that right now when for free or for $20/month they get access to USA frontier models.",
          "score": 2,
          "created_utc": "2026-02-06 00:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tgdek",
          "author": "Weary_Long3409",
          "text": "I run only qwen3-vl-8b-instruct to automate daily activity reports processing from 25 remote legal teams across the country. Something that commercial endpoints are irrelevant for such tasks. Have a really good complex prompt from ChatGPT, and this small qwen is does the job very well.",
          "score": 2,
          "created_utc": "2026-02-06 00:39:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tjfxo",
          "author": "KooperGuy",
          "text": "Yes",
          "score": 2,
          "created_utc": "2026-02-06 00:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tr2lu",
          "author": "ANTIVNTIANTI",
          "text": "almost exclusively",
          "score": 2,
          "created_utc": "2026-02-06 01:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ul0sp",
          "author": "ccalabro",
          "text": "Yeah I used ollama deepseek. It worked better than I expected.",
          "score": 2,
          "created_utc": "2026-02-06 04:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xw8v",
              "author": "Head-Stable5929",
              "text": "Oh wow! I'll give it a try",
              "score": 1,
              "created_utc": "2026-02-07 04:04:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3uvfqq",
          "author": "minitoxin",
          "text": "i use it everyday openwebui with searxng and LMstudio as back-end server , When i need deep searches i use perplexica with mistral14b  running on a headless  m4 mini with 16GB , it gives very detailed responses although its slow ",
          "score": 2,
          "created_utc": "2026-02-06 06:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xy7f",
              "author": "Head-Stable5929",
              "text": "Still that sounds like it can work just fine",
              "score": 1,
              "created_utc": "2026-02-07 04:04:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3uzkie",
          "author": "my_story_bot",
          "text": "yeah LMStudio or Ollama is great. whats nice with LMStudio is that you can simply search for a model in their UI and download and run it in a couple clicks. \n\nWhats REALLY cool is that you can  run it as a web server locally through localhost and then use it for stuff like openclaw",
          "score": 2,
          "created_utc": "2026-02-06 06:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v1a2g",
          "author": "djtubig-malicex",
          "text": "Yes. Never trust cloud.",
          "score": 2,
          "created_utc": "2026-02-06 06:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v6bx5",
          "author": "El_Danger_Badger",
          "text": " I built a local chat/LangBoard/RAG platform on my stock M1 Mac Mini. MLX native, on my stock M1 Mini.  I use it daily. Not flashy, but it does its job. Has some bells and whistles, plus an agent constitution.    >cough< OpenClaw >cough< . I co-deved with ChatGPT.  Not one shot. Typed one module at a time, asked an endless flow of questions over months because I wanted to learn how to build something like this.  Now I know. If anyone is hiring... ",
          "score": 2,
          "created_utc": "2026-02-06 07:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v6lvo",
          "author": "Efficient_Bug_0",
          "text": "For running the actual models offline, Ollama works pretty well on most modern Macbooks. For vibe-coding offline, I had the same need, and ended up building something called Codistry for exactly that purpose: [https://codistry.ai/docs/guides/ollama](https://codistry.ai/docs/guides/ollama)",
          "score": 2,
          "created_utc": "2026-02-06 07:46:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x6pph",
          "author": "Polysulfide-75",
          "text": "Yes but youâ€™re right it can be complicated.  I write my own interfaces and such though.\n\nYou can use LMstudio for something simple to install and run.  Or there are plenty of ChatGPT clones you can run locally.",
          "score": 2,
          "created_utc": "2026-02-06 16:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xi5yj",
          "author": "notbullshittingatall",
          "text": "I do. My setup is Ollama running on my gaming rig using Tailscale to connect to it from my MacBook or iPhone when I'm away from the house. I use an app called Reins on my phone. Works well for me.",
          "score": 2,
          "created_utc": "2026-02-06 16:56:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xkq3r",
          "author": "Decent-Freedom5374",
          "text": "I run 5 off line all day continuously",
          "score": 2,
          "created_utc": "2026-02-06 17:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xwv5c",
          "author": "Johnsmith2k18",
          "text": "Im an idiot and pinokio was easy enough to use to set up an offline chatgpt style thing, an image editor, image generator, voice cloning tts, and it's really shoddy due to my computer specs but I can even do some video generation, also have a charcter Ai type thing through silly tavern but that uses ollama.\n\nBut yeah pinokio makes it really easy for image and video stuff just use pinokio to get comfyui and use some template workflows to learn how it works, which i promise is not as complicated as itll seem when you first look at it.",
          "score": 2,
          "created_utc": "2026-02-06 18:06:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40y68r",
              "author": "Head-Stable5929",
              "text": "Thank you so much that sounds like a juicy insight! I'll give it a try",
              "score": 1,
              "created_utc": "2026-02-07 04:06:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42syq6",
          "author": "Mac_NCheez_TW",
          "text": "I only use offline for my personal projects. I use Gemini for front end crap to build basic websites.Â ",
          "score": 2,
          "created_utc": "2026-02-07 13:37:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jq5ge",
          "author": "Tough_Frame4022",
          "text": "The variety and endless creative options presented by these local AI models is priceless. Good times to be living in!",
          "score": 2,
          "created_utc": "2026-02-10 02:40:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qs61p",
          "author": "mystery_biscotti",
          "text": "What's the concern? Are you worried it's just a toy, or do you feel like it's not \"as good\"?",
          "score": 1,
          "created_utc": "2026-02-05 16:46:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xh0o",
              "author": "Head-Stable5929",
              "text": "Well I'm not worried about anything, i want to explore that's all and I just wanted to know what all the options there are out there :)",
              "score": 1,
              "created_utc": "2026-02-07 04:01:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41iu89",
                  "author": "mystery_biscotti",
                  "text": "Ah, okay, thanks for clarifying!",
                  "score": 1,
                  "created_utc": "2026-02-07 06:49:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43vq0z",
          "author": "Ok_Membership3521",
          "text": "I am running completely offline.. Claude wrote most of my set up as well.  I have a custom front end feels like a corporate model easy to use.  Iâ€™m completely local.. works well for me.",
          "score": 1,
          "created_utc": "2026-02-07 17:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44juuf",
          "author": "greymatter_ac3",
          "text": "I use it everyday offline and only go to the big models online when I am forced to. Also do the same in my work. Large model are overkill for most tasks. The real advantage is SLM (Small Language Models) and fine running those on specific use cases. This is the real unlock. A small custom model for your use case that runs on low power inexpensive hardware.",
          "score": 1,
          "created_utc": "2026-02-07 18:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fmq0d",
          "author": "LinkAmbitious8931",
          "text": "Yes and having pretty good success with it. I am also having some solid success in fine-tuning. Mind you, I do not have some super-expensive setup, just two old P40s each with 24GBs.   \nI wrote about it here: [https://www.reddit.com/r/Qwen\\_AI/comments/1qxyxx4/guardrailcentric\\_finetuning/](https://www.reddit.com/r/Qwen_AI/comments/1qxyxx4/guardrailcentric_finetuning/)\n\n",
          "score": 1,
          "created_utc": "2026-02-09 14:02:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pjryz",
          "author": "atkr",
          "text": "skill issue",
          "score": -3,
          "created_utc": "2026-02-05 12:59:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40x7x3",
              "author": "Head-Stable5929",
              "text": "No need to be like that my friend",
              "score": 1,
              "created_utc": "2026-02-07 03:59:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0swmh",
      "title": "Is Local LLM the next trend in the AI wave?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r0swmh/is_local_llm_the_next_trend_in_the_ai_wave/",
      "author": "G3grip",
      "created_utc": "2026-02-10 06:02:01",
      "score": 84,
      "num_comments": 130,
      "upvote_ratio": 0.89,
      "text": "Suddenly I've been seeing a lot of content and videos centred around the cost of running LLMs vs paying subscriptions.\n\nCouple of months back it was all about Claude Code, very recently it is OpenClaw, now I feel, that by the coming week, everyone would be talking hardware and local LLM setups.\n\nIt will start with people raving about \"how low is the cost of local AI over time\", \"privacy\", \"freedom\", only to be followed by gurus saying \"why did I not do this earlier?\" and dropping crazy money into hardware setups. Then there will be an influx of 1-click setup tools and guides.\n\nHonestly, I've been loving all the exploration and learning with the past couple of trends, but I'll admit, it's a bit much to keep up with. I don't know, maybe I'm just crazy at this point.\n\nThoughts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r0swmh/is_local_llm_the_next_trend_in_the_ai_wave/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4kqfeh",
          "author": "jpinbn",
          "text": "I can't afford to sell an arm and a leg for RAM, so no LLM until prices come back to earth.",
          "score": 19,
          "created_utc": "2026-02-10 07:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kqxdw",
              "author": "corelabjoe",
              "text": "Uses Vram mostly not normal ram. I've run adorable little LLMs on even an old GTX1660 but presently run them on an RTX3060 12gb.",
              "score": 6,
              "created_utc": "2026-02-10 07:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mfjlw",
                  "author": "Prestigious_Ad572",
                  "text": "Running LLMs on RAM with CPU for inference is surprisingly cheap and with good enough performance that it becomes possible to use big models locally.",
                  "score": 3,
                  "created_utc": "2026-02-10 14:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ksm01",
              "author": "G3grip",
              "text": "Don't even get me started on the hardware cost these days. The market has constantly cursed ever since the crypto boom.\n\nFrom there, I've never seen \"normal\" pricing across the right components. It's always something or the other that jacks up the price of memory, GPUs, and whatnot. Then they rarely get down to the regular price.",
              "score": 3,
              "created_utc": "2026-02-10 07:20:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kqdhs",
          "author": "civil_politics",
          "text": "I mean local is always going to be limited by your gear - to have a â€˜goodâ€™ local LLM youâ€™re talking 5k minimum to have something reasonable and really 10k+ to get something that is significant value add - and at these prices you can get access to good online models for a long period of time. \n\nFor some usecases, and I think something like OpenClaw is one, there will be people building good local setups, but writ large I donâ€™t see this trend taking off. I mean even with OpenClaw there are a ton of reviews basically saying it is near useless without hooking it up to one of the large models.",
          "score": 31,
          "created_utc": "2026-02-10 07:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kqxxn",
              "author": "G3grip",
              "text": "Cost is a big barrier to entry. Agree.\n\nUnfortunately, this type of first principle thinking only comes to sense after the wave settles.\n\nHaving that said, I think there's real value in budget, small and specialised LLMs. Use cases that we are yet to see. \n\nThere are always going to trade offs and advantages on both sides.",
              "score": 11,
              "created_utc": "2026-02-10 07:05:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4l0a45",
                  "author": "pot_sniffer",
                  "text": "For coding I've had good results with qwen2.5-coder 32b q4, using a rag with chromadb that contains all the info about the project.\n\nI've also had good results. Maybe better actually using Qwen3-Coder-Next 80b q6 without the rag as it caused it to be too slow. \n\nI'm running a 7950x, 64gb ddr5 and a 9060XT 16gb. Using llama.cpp. the 7950x isn't essential but it definitely helps, a 9070XT would've been better if I had the money at the time but this is working well enough I don't regret getting the 9060. Cost me Â£1200 just before the ram crisis. Would be a few hundred more now but not out of reach",
                  "score": 8,
                  "created_utc": "2026-02-10 08:33:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n6im1",
              "author": "PeanutButterApricotS",
              "text": "Local ai isnâ€™t to far from being able to be a quality agent. A lot of it is the process.\n\nI was able to build my own personal Ai agent app. It has worked better then openclaw because openclaw restricts quality items/features behind paywalls.\n\nI was able to make a agent\n\nI told the agent to have a way to check my weather locally\n\nThe agent made a python script after scraping the web and finding an api. It took the script and provided the weather.\n\nIt provided the wrong time zone (defaults to ust)\n\nI told it I needed CST\n\nIt went and fixed the time zone and updated the script.\n\n\n\nI couldnâ€™t get openclaw to do that because itâ€™s restricted to paying users.",
              "score": 2,
              "created_utc": "2026-02-10 16:59:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n71n8",
                  "author": "civil_politics",
                  "text": "I mean OpenClaw is just a wrapper - what models are you running locally? \n\nGimmicky things like single page websites or small little single API integration tasks are fun and neat, but they hardly move the needle on being practically viable. That being said getting that built on a local setup that costs less than 5k would be pretty impressive",
                  "score": 1,
                  "created_utc": "2026-02-10 17:01:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nqk7y",
                  "author": "cheechw",
                  "text": "What exactly are you talking about that's behind a paywall? I'm using Openclaw and there's nothing to even pay for right now. Am I missing something huge?\n\nAlso, Openclaw is open source. You can just fork it and change it to do what you want. Frankly, I don't even know if we're talking about the same thing here.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mjsry",
              "author": "danny_094",
              "text": "The situation will change once RAM prices recover. Current development efforts are also focused on allocating RAM to VRAM. AI is not even five years old in this form. Anything concerning the future is speculation.",
              "score": 1,
              "created_utc": "2026-02-10 15:12:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o44et",
                  "author": "stuffitystuff",
                  "text": "RAM manufacturers are selling unbuilt manufacturing capacity for a couple years from now. It's going to be a bit.",
                  "score": 3,
                  "created_utc": "2026-02-10 19:33:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mpnzs",
                  "author": "quantgorithm",
                  "text": "Itâ€™s not estimated to recover until 27 or 28.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:41:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4p02u9",
              "author": "nntb",
              "text": "The thing is I'm quite surprised how many things were able to overcome even though we have small gear or something like that. I never thought I'd be able to run you know one of the super big models without having an entire farm of gpus but then to see that I can not only run it but run it pretty decently using hard drive space on a solid state that's kind of amazing.",
              "score": 1,
              "created_utc": "2026-02-10 22:02:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4p1c24",
                  "author": "civil_politics",
                  "text": "Oh I completely agree itâ€™s pretty incredible and we will move more and more towards a world where local is accessible, but Iâ€™m not sure if we get to a point where local can run truly impressive models that arenâ€™t hyper specialized and even then for a decent chunk of change. How often do you need to leverage something that costs 10k to build?",
                  "score": 1,
                  "created_utc": "2026-02-10 22:08:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4pwr9q",
              "author": "alias454",
              "text": "I think using local models can solve the good enough problem very well though. A lot of things don't need SOTA models to perform decently well. The quality of relatively small local models is also getting better.",
              "score": 1,
              "created_utc": "2026-02-11 01:00:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l5pb5",
          "author": "truthputer",
          "text": "I think weâ€™re coming up on a tipping point where free small local models make more sense than paid cloud models.\n\nThe big cloud models will always be better than the local models, but the gap between them is closing as the big models have diminishing returns on their improvements.\n\nIe: Local models are getting better faster than the cloud models are improving.\n\nLocal models will still be taking big jumps forward in the next 12 months - especially with the next generation of AI trained small LLMs giving similar result to models 10x bigger.\n\nWhereas cloud models might be a bit better? Like how Opus 4.6 is a bit better than 4.5 but some people have decided to stick with 4.5?\n\nWeâ€™re already at the point where if you care about image generation, youâ€™re probably running it locally. While cloud stuff is still â€œbetterâ€ and usually faster, itâ€™s not as flexible with usage limits and content restrictions.\n\nLike youâ€™re crazy to pay for online image generation with usage restrictions when you can just download Ollama or llmstudio and make as many images as you want, even tho it might take a few minutes for a high quality result.\n\nI can see in the next 12 months it might make more financial sense to have a $2000 mini PC with 128GB RAM running a local model that you connect Claude Code or KiloCode to vs paying $200 per month for a Claude Code subscription.",
          "score": 9,
          "created_utc": "2026-02-10 09:26:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8ijh",
              "author": "G3grip",
              "text": "Solid perspective. The way you placed it, I think it really makes sense.\n\nI just hope that the hardware pricing comes to its senses soon.",
              "score": 2,
              "created_utc": "2026-02-10 09:54:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4no0uu",
              "author": "chimph",
              "text": "Can local image gen really compare to nano banana pro tho?\nIâ€™ve gone deep into comfy ui in the past and tbf havenâ€™t revisited for a while but recently have been using nano banana for help designing and renovating a house and itâ€™s been superb. Nails the images. I feel like I would have to do a lot of tweaking for local and still get mixed results. But yes, not having limitations is nice.",
              "score": 2,
              "created_utc": "2026-02-10 18:19:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4owfhg",
              "author": "NaiveAccess8821",
              "text": "Very interesting take, I wondered what Local LMs are on top of your useage list",
              "score": 1,
              "created_utc": "2026-02-10 21:44:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kv114",
          "author": "BeatTheMarket30",
          "text": "Local LLMs are much less capable with 16-24GB GPU memory limit. We need cheap GPUs with like 128GB memory.",
          "score": 6,
          "created_utc": "2026-02-10 07:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l7y8h",
              "author": "Look_0ver_There",
              "text": "There's already machines we can buy today that have dedicated AI inference acceleration with that much memory for less than the price of a 5090.\n\nI would say that 256 or 512GB may be a better memory target for a very strong capable local machine that would get within spitting distance of the \"big boys\".\n\nThe thing is, that would democratize strong local AI compute to the masses, and that would break this artificial monopolistic bubble that has formed.\n\nI suspect that is the real reason why the tech companies have aligned to push prices sky high. To me, this is remiscent of the 80's to 90's, when vast compute power was in the hands of the few while the masses got scraps. The strong personal PCs of the time cost tens of thousands of dollars in today's value.\n\nThen PC\"s started to actually get good, and now everyone has on their desktop machines that cost a few thousand dollars that outperform machines that used to cost milliona.\n\nTo my mind, it's inevitable that this artificial bubble will break and access to wide scale high quality personal and private AI will start to become available to all. The big boys know it, and they're doing all the can to gate keep it and make money while the sun is shining.",
              "score": 3,
              "created_utc": "2026-02-10 09:48:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ozvul",
                  "author": "NaiveAccess8821",
                  "text": "If we, as a community, (or someone who built a company) can train a collection of agents by using a shared pool of individual data (with consent first), or experiences sampled from such small agents. \n\nThen with such agent swarm work coordinatively, don't we have a path beating those big guys? ",
                  "score": 1,
                  "created_utc": "2026-02-10 22:01:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4l1qvr",
              "author": "G3grip",
              "text": "One can only hope. RAM is expensive as is, V-RAM more so, plus we're at the mercy of OEMs to bundle it within GPUs. So you only get a decent amount alongside beefy, expensive GPUs.",
              "score": 2,
              "created_utc": "2026-02-10 08:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4klbmh",
          "author": "oldboi",
          "text": "I think the maturity of self-hosted LLM's is starting to reach a point of actual usefulness to those with user-end hardware. Tiny, effective MoE models, models under 20gb that match or outperform SOTA models from 12 months ago, more apps to use them with, and then the odd viral use-case like OpenClaw... \n\nI have been tinkering with them for a while but they weren't really that useful or effective until recently. Now I actually *use* them instead of *testing* them.",
          "score": 5,
          "created_utc": "2026-02-10 06:16:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l43l7",
              "author": "RnRau",
              "text": "Yup models have been getting better over the last year. \n\nThere is a paper out there that reckons that the capability per parameter doubles every 3.5 months.\n\nhttps://arxiv.org/abs/2412.04315",
              "score": 4,
              "created_utc": "2026-02-10 09:11:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lbvj4",
                  "author": "G3grip",
                  "text": "2x every 3.5 months is insane growth.",
                  "score": 1,
                  "created_utc": "2026-02-10 10:25:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4klwn0",
              "author": "G3grip",
              "text": "The most useful things that I hope we get out of this is simplified self hosting LLMs and practically skilled local LLMs under budget.",
              "score": 2,
              "created_utc": "2026-02-10 06:21:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kqrla",
                  "author": "corelabjoe",
                  "text": "Simplified? [Oh it's super simplified!](https://corelab.tech/private-chatgpt-local-llm/)\n\nYou can just fire a docker up with access to your gpu, load an LLM module and BAM! Start chatting with it...\n\nIt's been like this for AWHILE already ;)",
                  "score": 2,
                  "created_utc": "2026-02-10 07:03:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mbj9n",
          "author": "Hector_Rvkp",
          "text": "A Strix halo for 2100$ (128) starts to be capable with large MoE models. It's slower and dumber than a frontier model, but it doesn't have to be slow or dumb. Moe models will obviously keep getting better. Meanwhile, models are running out of data to train on, synthetic data is often but helpful, and model improvement curve looks logarithmic, not linear, let alone exponential. Also, killing a fly with a bazooka isn't a sign of intelligence, so most people don't need a 1tb dense model to find a recipe. \nPrivacy concern is real. \nSkilling/ reskilling concern is real too. \nIt would be intelligent to develop skills and work flows to know when to use local compute, and maybe, when you really do need speed or compute, use the cloud. But running a dense pro model for every prompt sounds dumb, wasteful, and only sensible from the perspective of the cloud provider that's charging you per token irrespective of the slop you make. \nOne fear I have: letting a model running on the cloud overnight working on some multi step task to wake up to a large bill with garbage to show for. If that ran locally, all I've done is heat my place a little bit.",
          "score": 3,
          "created_utc": "2026-02-10 14:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kkk5r",
          "author": "hchen25",
          "text": "Yes, looking forward to build a machine to run local LLM.",
          "score": 3,
          "created_utc": "2026-02-10 06:09:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4krnlc",
              "author": "goobervision",
              "text": "Mac M1 Max Pro 64gb second hand isn't a bad shout on a budget.",
              "score": 3,
              "created_utc": "2026-02-10 07:11:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l4pft",
                  "author": "ScuffedBalata",
                  "text": "I found one of those, busted up Macbook with bad screen (i wasn't using it as a laptop anyway) for $700 recently.",
                  "score": 1,
                  "created_utc": "2026-02-10 09:16:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4klndk",
              "author": "G3grip",
              "text": "Me too! But I'm terrified of being so excited about every new thing all the time.",
              "score": 3,
              "created_utc": "2026-02-10 06:18:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kqpaj",
                  "author": "MrScotchyScotch",
                  "text": "I don't think you need to be scared of being excited",
                  "score": 1,
                  "created_utc": "2026-02-10 07:03:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kukao",
          "author": "SebastianOpp",
          "text": "Remember to factor in electricity too âš¡ï¸",
          "score": 3,
          "created_utc": "2026-02-10 07:38:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l25fx",
              "author": "G3grip",
              "text": "And fire safety.\n\nPeople underestimate the effort that goes into uptime. Entire businesses run on maintaining hardware and redundancy.",
              "score": 2,
              "created_utc": "2026-02-10 08:51:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4nozcc",
              "author": "chimph",
              "text": "Again why Mac Minis are great. 5w idle",
              "score": 1,
              "created_utc": "2026-02-10 18:24:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l699a",
          "author": "hydropix",
          "text": "Local LLM are actually quite expensive.\n\nIn data centers, the hardware is used almost 100% of the time and is much better optimized. If you calculate precisely what it costs to amortize a configuration capable of running AI and its obsolescence after three years. You need to perform inference for at least 6 hours continuously per day (7 days a week) for the local option to be profitable. I advise you to ask an AI to give you the details of the calculations and depreciation costs; it's quite easy to compare different scenarios.\n\nThe difference with GPUs for gaming, where latency is critical, does not apply to the usual use of AI, so again, this does not argue in favor of local processing.\n\nOn the other hand, for those who want to work with AI, having the hardware to perform all the tests is very valuable. For others, unless you have very intensive use or data protection needs, the cloud is a better option.",
          "score": 3,
          "created_utc": "2026-02-10 09:32:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfn1k",
              "author": "profcuck",
              "text": "You're right about all that of course but despite the current blip in RAM prices (and SSDs) a broad version of Moore's Law still very much holds.\n\nIn 2016 \"Intel Core i7 Extreme Edition is the Most Powerful Desktop PC Processor Ever\" - https://www.thurrott.com/hardware/67574/computex-2016-intel-core-i7-extreme-edition-powerful-desktop-pc-processor-ever\n\nIt is slower than an Intel N150 which is now considered appropriate for a very basic mini pc.\n\nWe could dig a bit deeper:\nhttps://medium.com/@cli_87015/the-evolution-of-gpu-pricing-a-deep-dive-into-cost-per-fp32-flop-for-hyperscalers-cbf072b85bb5\n\nMy overall point?  While it is always going to be true that near 100% usage of datacenter hardware means that it will be cheaper to go that way, it won't be long until it's all plenty cheap enough to run good stuff locally.",
              "score": 2,
              "created_utc": "2026-02-10 10:59:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o5pus",
                  "author": "stuffitystuff",
                  "text": "Even now you can get an Nvidia A100 80GB on eBay for \\~$7k and they were around twice that a year ago, IIRC.",
                  "score": 2,
                  "created_utc": "2026-02-10 19:40:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l2g1j",
          "author": "NoobMLDude",
          "text": "LocalLLM is not â€œthe next trendâ€ , itâ€™s already â€œthe CURRENT TRENDâ€ practiced by many (including me) for few years .\n\nIâ€™ve been[trying to inform and educate](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) the general public that they do not need to give their money and personal data to big AI companies. We can get Local LLMs running and it might be enough for 90% or users. \n\nWhy would you share your Personal information with AI companies who will use it against you to show you/ manipulate you with Ads and other products !??\n\n\nNot just data, the offerings of AI APIs and services are very questionable:\n1. Customers of Perplexity paid for Pro account (advertised to have limit of 2000 research questions) noticed they have reduced the limit now tor 20. How a company can change the limits of a product after customers paid for  it is still unclear, but itâ€™s happening.\n\n2. companies change the underlying model running behind the Chat Apps to lower quality model without telling us to save costs. Customers notice degradation in models few weeks after the model has generated buzz and captured attention.\n\nAll of this is happening and  the only way to have some control over your AI experience/access is to run your own model locally.\n\n\nHereâ€™s some ideas for Private and Local AI uses:\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 5,
          "created_utc": "2026-02-10 08:54:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l5t0q",
              "author": "G3grip",
              "text": "Of course, I know many are doing it and even swear by it. \n\nWhen I say 'next trend,' Iâ€™m talking about a total takeover. I mean the kind of buzz where my phone is literally drowning in notifications from Google and X every single day. The same way everyone (myself included) is obsessed with OpenClaw right now.",
              "score": 2,
              "created_utc": "2026-02-10 09:27:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lolim",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-10 12:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m2dv8",
              "author": "G3grip",
              "text": "Hey, this looks great. \n\nCan I ask, what sort of a model / use case would I be able to run on a simple macbook (if any at all)?",
              "score": 2,
              "created_utc": "2026-02-10 13:40:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4mhm53",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-02-10 15:01:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lqs6l",
          "author": "techlatest_net",
          "text": "Totally feel you on the hype cycle overloadâ€”Claude coding buzz to OpenClaw blowing up overnight, now everyone's eyeing local setups. Local LLMs are def picking up steam for privacy and no-sub fees long-term (like $1/day amortized vs $20 cloud), but that hardware drop (GPUs, rigs hitting $1k-3k) is gonna suck people in hard. I'm all in on the freedom angle, but yeah, it's exhausting keeping paceâ€”stick to what works for your workflow and ignore the FOMO gurus!",
          "score": 2,
          "created_utc": "2026-02-10 12:27:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3aug",
              "author": "G3grip",
              "text": "Finally, someone shares my pain!",
              "score": 1,
              "created_utc": "2026-02-10 13:45:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lzay2",
          "author": "pot4scotty20",
          "text": "next trend? if getting squeezed by market to pay the maximum price is something that interests you? the time to build was 12 months ago, best to wait for prices to stabilize after these artificially inflated hype rates come crashing down",
          "score": 2,
          "created_utc": "2026-02-10 13:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3i6z",
              "author": "G3grip",
              "text": "ðŸ˜­",
              "score": 1,
              "created_utc": "2026-02-10 13:46:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mirj9",
          "author": "danny_094",
          "text": "I don't see locale execution as a trend. It's more like the logical next step. Locale LLM has existed since GPT. It's just becoming more user-friendly. It will eventually be possible to run large models on less powerful hardware. Not because magic happens, but because AI is still quite new and its development toward greater efficiency is still in its infancy. Tools like Clawbot simply make AI more appealing to the local market, which in turn generates more interest. OpenAI's first AI model was open source and available to everyone back then. So, locally.",
          "score": 2,
          "created_utc": "2026-02-10 15:07:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mk3fl",
          "author": "beedunc",
          "text": "It would have been, but not with these ram prices. \n\nMy $180 96GB sticks (needed to run big local models) are now almost  $1200. Thatâ€™s more than I spent on everything else.",
          "score": 2,
          "created_utc": "2026-02-10 15:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mnbld",
          "author": "mr__smooth",
          "text": "The issue is memory. If there wasnt a supply constraint created by that company last year it would have been much more realizable. But now there is a shortage of memory that will impact what the average consumer can buy on their own",
          "score": 2,
          "created_utc": "2026-02-10 15:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n97f2",
              "author": "G3grip",
              "text": "This RAM pricing thing resonates with everyone.",
              "score": 1,
              "created_utc": "2026-02-10 17:11:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4oqs2p",
              "author": "VaporwaveUtopia",
              "text": "The RAM shortage is shitty, but it won't last forever. There's demand and money to be made, so it won't be long before existing manufacturers scale up, or new players get into the space.",
              "score": 1,
              "created_utc": "2026-02-10 21:18:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4msbgv",
          "author": "robbro9",
          "text": "Nah, it's quantum blockchain ai..... With COVID.  Seriously i how your right.  I get the feeling the ai cos are booking up all the hardware needed to run ai locally but they can't sustain that.  Imagine memory prices from a year ago, ai would be so much more accessible with that locally.",
          "score": 2,
          "created_utc": "2026-02-10 15:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4na20n",
              "author": "G3grip",
              "text": "I feel like I should have bulk-ordered RAM before the price hike; I would have been rich. The return on these things has turned out to be better than gold... lol...",
              "score": 1,
              "created_utc": "2026-02-10 17:15:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4msq8t",
          "author": "AppointmentAway3164",
          "text": "Yes. Privacy is important. But also I feel like the cloud based models have hit a wall and arenâ€™t advancing sufficiently to make their position a requirement for ai. I can get similar results locally, and so the opportunity is present for local models to gain popularity.\n\nAs a dev I really am not interested in constantly improving Anthropicâ€™s models, which is what youâ€™re doing when using them. Youâ€™re paying them to reinforcement train their models. And at the end of the day, I want my source code private. \n\nThe nvidia DGX Spark is really nice, and apple would be smart to position themselves as local LLM hosts. We will see if Tim Apple can see the forest for the trees.",
          "score": 2,
          "created_utc": "2026-02-10 15:55:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nbxlz",
              "author": "G3grip",
              "text": "Good points. And with models like Kimi 2.5 available for free, the hardware is the only limitation. However, if you want to run something like that locally and do not already own the hardware to support it, then the monthly subscriptions start to look good.\n\nOnly if you are actually dropping 1000s of dollars on the subs every month, you should then get a machine so powerful (and expensive).\n\n  \nBut my hope is that in time we will have small, specialised and efficient models that most people can run decent personal machines. Leaving the subscription-based ones on for heavy-duty work that not everyone needs all the time.",
              "score": 1,
              "created_utc": "2026-02-10 17:24:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mt8nj",
          "author": "locai_al-ibadi",
          "text": "Local LLMs (and AI in general) has been improving massively with lower computing resources/specs. It is costly to have a \"good\" local LLM but not as much as it used to be 6+ month ago. Honestly, I think for a lot of the daily average use cases of LLMs, people do not need cloud servers running those massive models. ",
          "score": 2,
          "created_utc": "2026-02-10 15:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc367",
              "author": "G3grip",
              "text": "I second this.",
              "score": 2,
              "created_utc": "2026-02-10 17:24:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtaes",
          "author": "mpw-linux",
          "text": "The question is do we really want to have a local super computer to run high-end LLM's? Cost of hardware, cost of power to run the machines. For price of buying all the hardware one can just pay the 30-50 USD a. month to let the cloud computers do all the work for us. Local LLM's are a great for experimentation, learning models, training, etc. \n\nI think that the future will be more powerful quantized versiions of powerful LLM's for local usage like what they are doing at Liquid AI. ",
          "score": 2,
          "created_utc": "2026-02-10 15:58:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ncj27",
              "author": "G3grip",
              "text": "Intrusting stuff, this Liquid AI. First time I got to know. Thanks.\n\nI agree.",
              "score": 1,
              "created_utc": "2026-02-10 17:26:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nf0xi",
          "author": "One-Distribution-376",
          "text": "its not the future they want, you'll own nothing and you will be happy!",
          "score": 2,
          "created_utc": "2026-02-10 17:38:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nh713",
              "author": "G3grip",
              "text": "I heard Linus say the same thing today.",
              "score": 1,
              "created_utc": "2026-02-10 17:48:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nmv6a",
          "author": "chimph",
          "text": "Local LLM will become a bigger part of peopleâ€™s toolsets for sure. And I think the whole Clawdbot + Mac Mini has made people more aware of local LLM. This years Mac M5 will be known for driving good local models",
          "score": 2,
          "created_utc": "2026-02-10 18:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l1fsp",
          "author": "beryugyo619",
          "text": "Of course it would be great if you could just run Claude Opus(any) locally for free, if that could be done at all. It's not the matter of demand but feasibility.",
          "score": 1,
          "created_utc": "2026-02-10 08:44:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1yzb",
              "author": "G3grip",
              "text": "What if we don't need Opus? What if there's real use in much smaller but specialised and practical models that may run on much cheaper hardware?\n\nI mean ya, it's all speculation but the possibilities are exciting.",
              "score": 1,
              "created_utc": "2026-02-10 08:50:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l4f46",
          "author": "ScuffedBalata",
          "text": "No, unless there's a breakthrough that makes the ever-larger scaling of models breakdown somehow.\n\nThe cloud-based \"frontier\" models will always be (at least in the short/medium term) SIGNIFICANTLY more capable than the local ones.",
          "score": 1,
          "created_utc": "2026-02-10 09:14:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l6pg4",
              "author": "G3grip",
              "text": "Yes, but do we always need the best model at all times? \n\nI mean, think of it this way, I can rent a ridiculously powerful machine on AWS. They will always have servers more powerful than any personal computer that I can go out and buy. \n\nI still own a very low-powered laptop for my personal day-to-day tasks, as I don't need a 128-core CPU for running chrome and checking email (RAM is a different story for chrome... lol...).",
              "score": 2,
              "created_utc": "2026-02-10 09:36:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4mpqk5",
                  "author": "ScuffedBalata",
                  "text": "For Coding?  I feel like \"often\".\n\nI mean even when you have a better model, i find it makes more elegant code, it plans better and architects better for future upgrades and cleaner code.  \n\nA lower quality model might make it work, but it may do it with spaghetti code, or choose a suboptimal library that doesn't account for something.\n\nJust my feeling doing a lot of AI-backed development.\n\nIf you're just chatting or asking about muffin recipies, then no, no you don't.  But even if you're asking about research, the actual concreteness and reduction of \"AI buzz\" (aka mild hallucination and exageration) is better with larger models.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:41:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4li35j",
          "author": "Soft-Dragonfruit6447",
          "text": "Local LLMs feel inevitable long-term, but hardware cost is still the real gate. Once RAM/VRAM prices drop and setup gets easier, adoption will explode. Right now itâ€™s a power-user game, not mainstream yet.",
          "score": 1,
          "created_utc": "2026-02-10 11:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lmx4e",
          "author": "tcoder7",
          "text": "I do not thunk so. The cartels colluded to price gouge [RAM.So](http://RAM.So) the next waves of PC will have lower RAM. Also the best LLM models are still not available to download locally. There is still some low proba chance of local private LLM winning the race if there is an innovation to dramatically reduce RAM and VRAM needs.  ",
          "score": 1,
          "created_utc": "2026-02-10 11:59:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m1guj",
              "author": "G3grip",
              "text": "Wait, doesn't the RAM pricing impact the \"cartel\" too?",
              "score": 1,
              "created_utc": "2026-02-10 13:35:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m2o3f",
                  "author": "tcoder7",
                  "text": "Not if you are the first buyer.",
                  "score": 1,
                  "created_utc": "2026-02-10 13:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lsne4",
          "author": "LeaderBriefs-com",
          "text": "The hardware you would need to run a competent LOCAL LLM is really cost prohibitive for the majority. \n\nYou will always trade quality for time and money. \nThat doesnâ€™t really scale in my mind.",
          "score": 1,
          "created_utc": "2026-02-10 12:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m3408",
              "author": "G3grip",
              "text": "I think for now, yes.\n\nIn time, smaller models will get better and small setups would be viable.",
              "score": 1,
              "created_utc": "2026-02-10 13:44:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nrdcz",
          "author": "No_Vehicle7826",
          "text": "If they ever make local LLMs efficient enough to run good models of basic hardware, closed ai will just be government ai \n\nUntil then, there's Venice for us broke peeps without a rig lol\n\nLeChat is pretty cool too. There's too much \"safety\" on the ChatGPT, Claude and Gemini... then Grok just seems dumb to me. Hoping Grok 4.20 is cool though, supposed to be 6 trillion parameters",
          "score": 1,
          "created_utc": "2026-02-10 18:34:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nwmyp",
          "author": "Budget-Length2666",
          "text": "Not right now - it will cost you 20k+ to run a similar high end model locally in some cluster just to be sort of on equal playing field with the frontier foundational models. No guarantees your hardware you buy today will be able to compete with the frontier models of 2 years from now. And tps is generally slower locally, even with latency considered. inference is just hard and expensive.",
          "score": 1,
          "created_utc": "2026-02-10 18:58:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o7dqm",
          "author": "Zestyclose_Paint3922",
          "text": "Local will always be behind. I think this is more for corporate \"limited\" applications.",
          "score": 1,
          "created_utc": "2026-02-10 19:48:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o9vdq",
          "author": "darkestvice",
          "text": "Given that said AI buildup has skyrocketed the cost of all PC parts required to run not-so-garbage local LLMs, I highly doubt Local LLMs will become trendy any time soon. It's still squarely in an up front cost range that's out of reach for the majority of people.",
          "score": 1,
          "created_utc": "2026-02-10 20:00:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4och8n",
          "author": "abe-azam",
          "text": "If you use high end models, local wonâ€™t do.\n\nIf you want something basic the it will pass.",
          "score": 1,
          "created_utc": "2026-02-10 20:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ozqni",
          "author": "nntb",
          "text": "You're going to find a lot of videos comparing the cost of local versus software as a solution because there's a lot of money being poured into software as a solution right now. And a lot of companies have to recoup that cost so they may be fighting each other and they may be offering stuff for inexpensive for now until the user base gets big enough it can warrant them increasing costs. I've been on this subreddit for a really long time and I've been involved in locally running AI for years I would say that it's not a new thing that local is a popular option. And I would say it's the best option to be 100% honest. Now I know those who don't have hardware of their own will go and use things like Google collab or other services to host theirs and that's fine but really the heart of this entire suburb has been locally running AI",
          "score": 1,
          "created_utc": "2026-02-10 22:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p3i2y",
          "author": "billytryer",
          "text": "I was thinking of buying a M4 Mac mini Â£900new new with 24gb of ram is that good for ollama and Some of the smaller LLMs , I saw here that the M1 Max has a better performance ? I just don't know",
          "score": 1,
          "created_utc": "2026-02-10 22:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pcf1d",
          "author": "epSos-DE",
          "text": "Yes. Logical Bit Operators on the CPU make it 100% possible.\n\nGPU vector calualtions can assist.\n\nIT is possible to have local LLMs that reduce the decision space with binery trees and run logical operators , instead of vector search. LLM with logical bit operators and some GPU offload can run on one CPU core , if it can prune the decision space correctly.\n\nTraining the Ai is still hard tho ! Needs server farms or p2p bot swarms that share computational load.\n\n\n\nBit Logical LLMs will need to be trained for logical bit operators or the vector space LLM traiing data will need to be converted to bit operators and binary decision trees  for look ups of data.\n\n\n\nTechnically it is possible, but NVIDIA wants to sell more GPUs !!! SO they train vectors all day long, because Bit operations would harm their profits !",
          "score": 1,
          "created_utc": "2026-02-10 23:05:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pl2sj",
          "author": "Apprehensive_Gap3673",
          "text": "I don't think it will honestly ever be a trend.Â  Local LLM can't compete with frontier models in terms of performance and most people wouldn't be able to set them up or be willing to pay the upfront costs",
          "score": 1,
          "created_utc": "2026-02-10 23:54:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pqcjk",
          "author": "HealthyCommunicat",
          "text": "i was pretty dissapointed because even with 350+ gb of VRAM clustered, it felt hard making any LLM have a fast enough pp/tgs to feel really useful, (20 token/s is not realistically useful for someone who uses a agentic cli tool literally 8+ hrs a day) - but the most recent qwen 3 coder next gives me hope, i really wish companies would focus more on speed, minimax and stepfun are also two that come to mind, but the 200b models seem to be the perfect golden spot for those that want to spend 5-10k to run models from home.",
          "score": 1,
          "created_utc": "2026-02-11 00:23:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qtpvx",
          "author": "Shoddy_Bed3240",
          "text": "Two Mac Ultra M3 machines with 512GB of memory are probably the best option if you want to run very large local models at a quality level comparable to paid subscriptions â€” but the total cost is around $30,000.",
          "score": 1,
          "created_utc": "2026-02-11 04:26:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4knwms",
          "author": "woundedkarma",
          "text": "Yeah... I think it could be a business for a while. I just don't know how to find clients. LOL  There's bound to be people who want local llms but don't know how to do it. \n\nI posted in a local sub and got zero interest. I need more outreach but lol I'm an introvert.",
          "score": 0,
          "created_utc": "2026-02-10 06:38:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kt9o3",
              "author": "Low_Amplitude_Worlds",
              "text": "My advice:\n1) Use LLMs to help with your marketing and outreach strategy, find leads, etc. and maybe even generate copywriting for you, though that can turn many people off.\n2) Avoid that toxic sub you posted to at all costs. Not only did they reject your post, but made up misinformation about why it wouldnâ€™t work. Thatâ€™s not your target market or audience.",
              "score": 2,
              "created_utc": "2026-02-10 07:26:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kv9ys",
              "author": "Wide_Brief3025",
              "text": "Finding clients for niche tech like local LLMs can be tough, especially if cold outreach is not your thing. Try looking for communities where small businesses or devs vent about AI challenges and jump in to share your knowledge. Tools like ParseStream can help by tracking those discussions across multiple platforms so you do not miss any real opportunities to help.",
              "score": 2,
              "created_utc": "2026-02-10 07:45:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ko9jn",
              "author": "G3grip",
              "text": "These days you can get outreach white sitting in your room. Try posting at multiple places maybe? Also, care to share that post? I'm intrigued to see.",
              "score": 2,
              "created_utc": "2026-02-10 06:41:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4kp59l",
                  "author": "woundedkarma",
                  "text": "[https://www.reddit.com/r/kzoo/comments/1qvavf9/fully\\_local\\_ai/](https://www.reddit.com/r/kzoo/comments/1qvavf9/fully_local_ai/) ",
                  "score": 1,
                  "created_utc": "2026-02-10 06:49:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxrvq9",
      "title": "My $250 24gb of VRAM setup (still in 2026)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qxrvq9/my_250_24gb_of_vram_setup_still_in_2026/",
      "author": "Jack_at_BrewLedger",
      "created_utc": "2026-02-06 19:49:08",
      "score": 78,
      "num_comments": 50,
      "upvote_ratio": 0.95,
      "text": "What I'm running is a nvidia Tesla p40, a server compute accelerator card from 2016 which just so happens to have 24 gigs of VRAM on the highest end version. They can be found on ebay for about $250 bucks right now.\n\nThe card is passively cooled and designed for a server rack, so I made a custom cooling shroud to force air into the back and through it like it would work in a server race. On the back is a PWM high pressure fan, controlled by my motherboard, and the speed is directly bound to the tesla's temperature through nvidia-smi and FanControl on Windows.\n\nBought a big ass PC case, cut a big chunk out of the back. Got myself an 8 pin server card adapter to dual 6 pin GPU power outputs from a PSU, and got myself a nice big ass PSU. Fired the whole thing up as a Frankenstein design.\n\nI wouldn't call it fast by any means, but in 4bit quant I can fit gpt-oss 20b in there with 32k+ context length, all on the GPU. The speeds are fast enough to be used as a local chatbot, so works well as my AI assistant. Also, works with CUDA 12 if you pick the right driver.\n\n  \nOh, I forgot to mention, this thing has no video output, as it's a server accelerator card. I have a Ryzen 5700G as my processor, with integrated graphics. The Tesla is driver hacked into registering as a nVidia Quadro in workstation mode, and so I can run games on the Tesla using the windows settings for high performance graphics (meant to be used on gaming laptops with GPUs) and it gets relayed through my integrated GPU. The actual die on this card is a clone of the 1080ti, so I get 1080ti performance gaming too, just with 24 gigs of VRAM, and it'll run anything as long as I put the game's exe in a list. I'm most proud of that part of the setup.\n\n[The TESLA running in my rig](https://preview.redd.it/xodlgmmphxhg1.png?width=4624&format=png&auto=webp&s=7adaaf3f9eaed0dee176343d18e12a534af33bd8)\n\n[Underside View](https://preview.redd.it/u2ftaa3shxhg1.png?width=4624&format=png&auto=webp&s=642d7b83a028dd6b268a16c6a6241f2dcd4feb05)\n\n[closer look at the cooling solution and power adapter](https://preview.redd.it/up5colythxhg1.png?width=3472&format=png&auto=webp&s=0d15f0ddcf217b939ca2f806dad6d97043afd36e)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxrvq9/my_250_24gb_of_vram_setup_still_in_2026/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3yjnbi",
          "author": "Used_Chipmunk1512",
          "text": "Kudos to you, seriously this looks great, do post more of your adventures here",
          "score": 14,
          "created_utc": "2026-02-06 19:56:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxga0",
              "author": "Jack_at_BrewLedger",
              "text": "thanks man :)",
              "score": 1,
              "created_utc": "2026-02-06 21:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yni06",
          "author": "According_Study_162",
          "text": "How many tps?",
          "score": 7,
          "created_utc": "2026-02-06 20:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yoy6u",
              "author": "Jack_at_BrewLedger",
              "text": "30ish",
              "score": 6,
              "created_utc": "2026-02-06 20:22:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zi62r",
                  "author": "Themash360",
                  "text": "At 15GB MoE I'd be expecting a lot more. How much context is that?",
                  "score": 2,
                  "created_utc": "2026-02-06 22:50:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41lajy",
                  "author": "MrScotchyScotch",
                  "text": "that is wild... i have a thinkpad from 2023 and i get 30 t/s with qwen3 coder (q4_k_m). the newer chips really make a difference!",
                  "score": 1,
                  "created_utc": "2026-02-07 07:11:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3zi4o4",
                  "author": "ClimateBoss",
                  "text": "bruh can it run vLLM ?",
                  "score": 1,
                  "created_utc": "2026-02-06 22:50:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zi3tv",
              "author": "Themash360",
              "text": "https://www.techpowerup.com/gpu-specs/tesla-p40.c2878\n\n350GB/s vram, so uh at 15GB MoE I'd be expecting a lot more.",
              "score": 1,
              "created_utc": "2026-02-06 22:50:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zj3k7",
                  "author": "Jack_at_BrewLedger",
                  "text": "It's the pascal architecture that's the limitation, not the ram speed",
                  "score": 5,
                  "created_utc": "2026-02-06 22:55:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ynel2",
          "author": "TMuel1123",
          "text": "Small Tipp. Try a radial fan with an adapter shroud. They can produce more pressure and higher airflow.Â ",
          "score": 4,
          "created_utc": "2026-02-06 20:14:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ypbvg",
              "author": "Jack_at_BrewLedger",
              "text": "heard. I considered that originally, but this was cheaper to make. That's definitely better, but this with the high pressure fan will keep the thing under 70C at full load constantly, which is all I need,",
              "score": 1,
              "created_utc": "2026-02-06 20:24:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z272p",
          "author": "Klutzy_Ad_1157",
          "text": "Good old P40 never disappoints for LLM :)",
          "score": 3,
          "created_utc": "2026-02-06 21:28:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ykz2z",
          "author": "Purrsonifiedfip",
          "text": "Question because I'm new to the hardware stuff. Just built my first pc. Put a 5070ti in it and finding out its enough to chat, but if I want heavy tasks, I'll need more. Was looking at adding an older 3060 or something of the sort.\n\nDo older gpus need aggressive cooling for LLMs? or would high powered fans on the psu shroud be adequate?",
          "score": 2,
          "created_utc": "2026-02-06 20:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yp57n",
              "author": "Jack_at_BrewLedger",
              "text": "The fans built into the GPUs should work fine, I only have this set up because the Tesla has 0 cooling setup on it. It was designed to be used in a data center with massive fans so it just has a heatsink, you have to design a solution.",
              "score": 2,
              "created_utc": "2026-02-06 20:23:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ypgwo",
                  "author": "Purrsonifiedfip",
                  "text": "ahhh, thanks",
                  "score": 1,
                  "created_utc": "2026-02-06 20:25:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yska7",
          "author": "Decent_Solution5000",
          "text": "Come to my house and make us one. We'll feed you and everything! Seriously, that thing looks like it seriously rocks! Congrats on your genius innovation!",
          "score": 2,
          "created_utc": "2026-02-06 20:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxfk7",
              "author": "Jack_at_BrewLedger",
              "text": "made me smile",
              "score": 1,
              "created_utc": "2026-02-06 21:05:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zmxqt",
                  "author": "Decent_Solution5000",
                  "text": "Mad me a little jelly, ngl, but also made me happy for you. :)",
                  "score": 2,
                  "created_utc": "2026-02-06 23:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yuxac",
          "author": "UnlikelyPotato",
          "text": "I believe P100s are faster with their HBM2 bandwidth. $80 for 16GB cards. 32GB for $160.",
          "score": 2,
          "created_utc": "2026-02-06 20:52:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxd99",
              "author": "Jack_at_BrewLedger",
              "text": "im pretty sure it doesnt come in 32, maybe you're thinking of something else?\n\n",
              "score": 2,
              "created_utc": "2026-02-06 21:04:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yyjg5",
                  "author": "UnlikelyPotato",
                  "text": "Sorry. Two for $80 each is $160. P100s have twice the memory bandwidth of the P40. If you can fit multiple GPUs in your system then P100s are hands down better.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:10:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3zhe2v",
          "author": "No-Leopard7644",
          "text": "Kudos to innovation, great idea and execution",
          "score": 2,
          "created_utc": "2026-02-06 22:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ztvmy",
          "author": "apVoyocpt",
          "text": "I have the p40 too. Under linux (with enough ram) the oss 120b ran surprisingly well!",
          "score": 2,
          "created_utc": "2026-02-06 23:57:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zzthc",
              "author": "Jack_at_BrewLedger",
              "text": "do you mean 20b? or did you get the 120b to run? Iv'e got 48gb of ram to work with\n\n",
              "score": 1,
              "created_utc": "2026-02-07 00:31:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41vdf8",
                  "author": "apVoyocpt",
                  "text": "the 120b. I have 64GB of ddr4 and the p40. It worked really well!",
                  "score": 1,
                  "created_utc": "2026-02-07 08:47:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o419h35",
          "author": "hw999",
          "text": "Put a brace under than card or the fan, gravity is not your friend.",
          "score": 2,
          "created_utc": "2026-02-07 05:29:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45qs1y",
              "author": "MyTwitterID",
              "text": "This.",
              "score": 1,
              "created_utc": "2026-02-07 22:50:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o46ytot",
          "author": "singlebit",
          "text": "very nice! why dont I think the same!",
          "score": 2,
          "created_utc": "2026-02-08 03:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o479x4i",
          "author": "Fit-Rub3325",
          "text": "I am ashamed to call myself as engineer after reading things here on reddit :) ",
          "score": 2,
          "created_utc": "2026-02-08 04:39:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43a6ra",
          "author": "milkipedia",
          "text": "this is worthy of r/homelab",
          "score": 1,
          "created_utc": "2026-02-07 15:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43d4wq",
          "author": "AlexGSquadron",
          "text": "How does this compare to Intel b60 pro?",
          "score": 1,
          "created_utc": "2026-02-07 15:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48nvtp",
          "author": "LaunchAgentHQ",
          "text": "This is the kind of setup that makes local inference practical. The P40s are a steal for the VRAM you get - my only suggestion would be to look into vLLM or llama.cpp with tensor parallelism to actually use both cards together. Single card 24GB is great for 13B-30B models quantized, but if you can get both cards working in tandem you could potentially run 70B models at decent quality. The passive cooling is going to be your main challenge in a standard case though - those cards were designed for server airflow.",
          "score": 1,
          "created_utc": "2026-02-08 12:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dnxgm",
              "author": "Jack_at_BrewLedger",
              "text": "This setup works fine, but id get a sideways blower style fan next time.",
              "score": 1,
              "created_utc": "2026-02-09 04:19:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4eusj5",
          "author": "harktron",
          "text": "That's an awesome use for old hardware. Might have to steal this idea.",
          "score": 1,
          "created_utc": "2026-02-09 10:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rgpan",
              "author": "Jack_at_BrewLedger",
              "text": "DM me if you want and I can give you information about how I did it",
              "score": 1,
              "created_utc": "2026-02-11 07:34:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvktbl",
      "title": "Is running a local LLM for coding actually cheaper (and practical) vs Cursor / Copilot / JetBrains AI?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qvktbl/is_running_a_local_llm_for_coding_actually/",
      "author": "vMawk",
      "created_utc": "2026-02-04 10:14:05",
      "score": 77,
      "num_comments": 92,
      "upvote_ratio": 0.95,
      "text": "Hey all,\n\nI work a lot with Cursor, VS Code and JetBrains IDEs, and I currently have multiple Pro subscriptions (Cursor, ChatGPT, etc.).\n\nThe problem:  \nI still hit token limits fairly often. When that happens I notice I subconsciously start â€œusing the AI more carefullyâ€, shorter prompts, fewer refactors, less exploration. It kind of kills the flow.\n\nSo I started wondering:\n\nWould it actually be cheaper (or at least more comfortable) to just build a beefy local machine, run a local LLM, and hook that directly into my IDE?\n\nIn theory that sounds very appealing:\n\n* One-time hardware cost\n* No token anxiety\n* Unlimited usage\n* Fully private codebase\n* IDE â†’ LLM loop as tight as possible\n\nBut Iâ€™m unsure how realistic this is *today*.\n\nSome concrete questions:\n\n* Does this actually work well in practice for real coding tasks (refactoring, understanding large codebases, generating tests, etc.)?\n* Which local models are â€œnot too dumbâ€ for serious dev work? (Code Llama? DeepSeek-Coder? Qwen-Coder? Mixtral variants?)\n* What kind of hardware are we realistically talking about? (Single high-end GPU? Dual GPU? 64â€“128GB RAM?)\n* How painful is the IDE integration compared to Cursor / Copilot? (Latency, context handling, indexing, etc.)\n* In the long run: is this actually cheaper, or just a fun rabbit hole?\n\nI donâ€™t expect local models to beat GPT-4o / Claude at everything, but if I could get to â€œgood enoughâ€ with zero limits and full privacy, that feels like a big win.\n\nCurious to hear from people whoâ€™ve actually tried this setup in daily work.  \nWorth it, or not there yet?\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qvktbl/is_running_a_local_llm_for_coding_actually/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3ib3zd",
          "author": "TheAussieWatchGuy",
          "text": "Currently yes sort of.. Qwen Coder and GLM 4.7 are very good and be run on somewhat affordable consumer machines. Roughly comparable results to Claude Sonnet.\n\n\nIf I was a betting man looking at say the recent Kimi 2.5 requiring ungodly VRAM, like 96gb+ then it won't stay affordable for long.\n\n\nThe state of the art will move fast and the cloud models will be cheaper and better my miles.",
          "score": 31,
          "created_utc": "2026-02-04 10:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k3h29",
              "author": "FaceDeer",
              "text": "What I'm ultimately hoping for is a situation where my coding agent is using a local LLM for much of its work, and occasionally outsources \"hard stuff\" to bigger non-local models when it finds it needs to. Feels like that'll be a true best-of-all-worlds scenario.",
              "score": 9,
              "created_utc": "2026-02-04 16:49:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3litad",
                  "author": "GlassAd7618",
                  "text": "Yes, absolutely! IMHO that's the most pragmatic and most likely scenario. Technically, the only question is how to decide if something is \"hard stuff\". Actually, this is also an interesting question from the computer science perspective. I don't have an answer, but several naive alternatives come to mind:  \n\\- You could replicated the code (just make multiple copies on the disk) and run Ralph loop for n iterations. If none of the replicas can pass the test (assuming you would have specified/generated one for a certain feature), you could decide that this is indeed \"hard stuff\"  \n\\- You could use the local model itself (or another local model as a judge), give it the code base and the prompt/task, and ask it to decide whether it's simple enough to solve it locally or whether a bigger, non-local model should be invoked  \n\\- You could use a heuristic. For example, you could create some kind of a \"balanced score card\" with different categories such as \"the number of files that would need to be changed to complete the task\", \"the number of functions that would need to be changed\", \"total cyclomatic complexity of these functions or files\", etc. Based on the values in the individual categories of that \"balanced score card\" (or a lumped sum) you could decide whether the change is \"hard stuff\" or not",
                  "score": 4,
                  "created_utc": "2026-02-04 20:47:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4dz2sr",
                  "author": "Sourav_Anand",
                  "text": "IMO you should try to write and save different sets of prompts. Let's take a very simple example. You are trying to replicate a code block in another class but having different objects in place of original one. \n\nLet's break the steps. If you ask an agent for this\n\nStep 1 - look for reference of objects in original code block and then find their declaration.\n\nStep 2 - look for required reference in your new class and find the dependencies.\n\nStep 3 - Send the referenced code to LLM and get back your expected code block.\n\nNow I think most of the tokens are consumed during step 1 and 2. While that can be done with help of your local LLM as well. So you can use it for reading code finding actual references then generate a proper prompt for actual GPT 5 or Gemini or whatever you prefer.\n\nJust like these you can prepare multiple prompts for you to use them as needed.\n\nNow just to be clear this is my opinion and there might be other options as well. Let us know whatever you settle with.",
                  "score": 1,
                  "created_utc": "2026-02-09 05:40:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3iilxf",
              "author": "StaysAwakeAllWeek",
              "text": ">If I was a betting man looking at say the recent Kimi 2.5 requiring ungodly VRAM, like 96gb+ then it won't stay affordable for long.\n\n\nWe've had huge unattainable open source models ever since the original deepseek 671b. Kimi is actually relatively easy to run in comparison\n\nEdit: even earlier actually, llama 3 took hundreds of GB to load in 2024",
              "score": 8,
              "created_utc": "2026-02-04 11:41:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lokvo",
              "author": "Logiteck77",
              "text": "How much VRAM do Qwen Coder and GLM take?",
              "score": 1,
              "created_utc": "2026-02-04 21:14:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o9q6z",
                  "author": "SKirby00",
                  "text": "That's not as easy to answer as you'd think. Assuming he's referring to `Qwen3-Coder-30B-A3B-Instruct` and `GLM-4.7-Flash`, the models themselves take about ~18-19GB of VRAM (assuming 4-bit quantization), but that's before you factor in room for context. Context windows is where memory gets tricky.\n\nI have 24GB of VRAM (5060Ti 16GB + 3060Ti 8GB) and have been able to get about ~20K tokens of context on each of these in LM Studio (basically easy mode). According to Gemini, it estimates ~40GB of VRAM to fit 128k context but that might assume some (not necessarily easy) optimizations. Worth noting that GLM's context is a bit more memory-efficient than Qwen, but also it's a thinking model and Qwen Coder isn't so Qwen is more token-efficient.\n\nNot much can be done about the model size itself without sacrificing quality, but there are some optimizations that can help you squeeze in a bit more context if you're willing to stray away from the \"easy mode\" setup.",
                  "score": 2,
                  "created_utc": "2026-02-05 06:19:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mhvrh",
              "author": "ForsookComparison",
              "text": "> Roughly comparable results to Claude Sonnet.\n\nI get that these models feel different between different people but I still feel confident in saying that this isn't true anywhere but a bar chart.",
              "score": 1,
              "created_utc": "2026-02-04 23:43:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jsy1f",
              "author": "saltyghoul",
              "text": "Are Qwen Coder and GLM 4.7 14B or 32B?",
              "score": 0,
              "created_utc": "2026-02-04 16:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o2s77",
                  "author": "SKirby00",
                  "text": "I'm gonna go out on a limb and assume that he's referring to `Qwen3-Coder-30B-A3B-Instruct` (30B) and `GLM-4.7-Flash` (31B â€” I think).\n\nThat being said, there's a small chance that he's referring to `Qwen3-Coder-Next` which just came out like 2 days ago and is 80B.",
                  "score": 3,
                  "created_utc": "2026-02-05 05:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i8rbh",
          "author": "No_Clock2390",
          "text": "My 128GB RAM AMD AI MAX machine seems to be just as good as Gemini at writing code. It's great being able to get my computer to write code for me. Until it takes my job.",
          "score": 19,
          "created_utc": "2026-02-04 10:16:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iml08",
              "author": "ZenEngineer",
              "text": "What model so you run",
              "score": 5,
              "created_utc": "2026-02-04 12:11:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3irc2a",
                  "author": "No_Clock2390",
                  "text": "GPT-OSS-120B\n\nQwen3-Coder-Next",
                  "score": 10,
                  "created_utc": "2026-02-04 12:44:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3i9vhy",
              "author": "Elegant_Jellyfish_96",
              "text": "how much did the entire setup cost if I may ask?",
              "score": 1,
              "created_utc": "2026-02-04 10:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3i9z7v",
                  "author": "No_Clock2390",
                  "text": "I bought it for $2200 but now it's almost $3000. Minisforum MS-S1 Max.",
                  "score": 7,
                  "created_utc": "2026-02-04 10:27:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3isq5g",
                  "author": "Proper_Taste_6778",
                  "text": "2k usd bosgame m5",
                  "score": 2,
                  "created_utc": "2026-02-04 12:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jf5ed",
              "author": "tomByrer",
              "text": "Did you buy that machine ONLY to run AI coding assistants?  \nOr were you like, \"I need a new laptop anyway, so might as well spend an extra $1000 to make it run AI.\"?\n\nI made my desktop machine partly to run AI, partly to run games.",
              "score": 1,
              "created_utc": "2026-02-04 14:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jg60e",
                  "author": "No_Clock2390",
                  "text": "That's part of it. I just liked the overall package you get with the MS-S1 Max. It has great I/O. USB4v2 80Gbps (Thunderbolt 5 equivalent). It's like a Mac Studio but way less expensive for what you get. You can carry it in a backpack like a laptop but it's way more powerful than a laptop. You can game on it at up to 4K60FPS with frame gen on. But I also have a separate gaming computer.",
                  "score": 5,
                  "created_utc": "2026-02-04 15:00:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3nhbjc",
              "author": "Chance-Map-6039",
              "text": "How are you running these models? I have the gmktec evo x2 128 gb amd ai max 395. Iâ€™ve been learning on the go and I donâ€™t have a background in coding or anything but Iâ€™m curious what your workflow is or if you have any preferred UI. Thanks.",
              "score": 1,
              "created_utc": "2026-02-05 03:03:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nhy5d",
                  "author": "No_Clock2390",
                  "text": "I use LM Studio in Windows 11. Be sure to set the VRAM to 96GB in the AMD Adrenalin app so you can load the larger models. With 96GB VRAM, you still have 32GB left over to serve as the system RAM. In LM Studio, look for the GPU Offload option and enable it if it's not already enabled. Also, make sure your Windows 11 is up-to-date.",
                  "score": 3,
                  "created_utc": "2026-02-05 03:07:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ihss2",
          "author": "Big_River_",
          "text": "you can do much better with a hybrid approach - local plus cloud - use case makes all the difference - for coding with a large established repo you are not optimizing or rewriting - local is fantastic with fine tuned GLM 4.7 in my experience - imho if you were to spend ~20k you would outperform with fine tuned models to your specific use case - but then that is a great deal of tokens / but then you own your own dependencies which is where I landed due to all the garbage factors of uncertain geopolitical/economic climate",
          "score": 7,
          "created_utc": "2026-02-04 11:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j20ax",
          "author": "Look_0ver_There",
          "text": "Just my 2c.  Privacy wise yes.  Budget wise, it depends on how much you're spending monthly on your subscriptions and what your expected time period for return on investment is.  Local models have gotten pretty good lately with a handful certainly being capable enough to assist with the development of most anything.  Newer models get released fairly frequently that keep on raising the bar.\n\nIt's a difficult question to answer though, as I'm sure you know, one model may suck at one task while another is hitting hole runs on the same task.  The advantage of a local machine is that you can quickly flip between models without needing to have a half dozen subscriptions.\n\nSo, privacy wise, always worth it. Budget wise, it depends. Access to the best and latest models wise, you'll have to accept that you're always maybe six months behind the capabilities of the online models (very rough hand wavy guess), but this can be mitigated somewhat with model flexibility. Having said that though, some of the latest local models are surprisingly good.",
          "score": 5,
          "created_utc": "2026-02-04 13:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3igcru",
          "author": "tillybowman",
          "text": "i'd say no. \n\nwhile there are open weight models that come close to opus 4.5, running those models without quants needs a machine above 3k. \n\nyou can do your math now. it would take years for me to spent that in tokens online. and when i have, the tech will have changed anyway.\n\nthis answer is tailored to coding where you need high context, max tokens, fast tps",
          "score": 4,
          "created_utc": "2026-02-04 11:23:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ii7aw",
              "author": "Careful_Breath_1108",
              "text": "Did you mean 3k or 30k? What type of set up did you mean for 3k",
              "score": 5,
              "created_utc": "2026-02-04 11:38:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iuzca",
          "author": "Silentparty1999",
          "text": "No.  The primary argument for local LLM, IMO, is privacy.\n\nLLM as a service is continually upgraded and available at scale on-demand.",
          "score": 4,
          "created_utc": "2026-02-04 13:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0vbr",
          "author": "2020jones",
          "text": "Opus 4.5, Gemini 3, and Gpt 5.2 only worked well at launch; afterwards, they reduced the quality. So if a local model has 50% performance in problem situations, it actually has 50%. The results of these standard AIs were achieved with them at maximum performance; what you buy today is garbage.",
          "score": 4,
          "created_utc": "2026-02-04 16:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kf2kv",
              "author": "killerkongfu",
              "text": "Really?? I thought it stayed the same?",
              "score": 1,
              "created_utc": "2026-02-04 17:43:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3snlcw",
                  "author": "2020jones",
                  "text": "NÃ£o mesmo! Veja o Opus 4.6 lanÃ§ado hoje, experimente ele daqui a 5 meses e vai ver muitos erros. Eu sei disso pois eu uso para as MESMAS tarefas e o desempenho cai bastante. Gemini Ã© de longe o pior de todos nesse sentido por que ele sustenta seus erros.",
                  "score": 1,
                  "created_utc": "2026-02-05 22:03:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3inodn",
          "author": "0Bitz",
          "text": "Do it for the privacy",
          "score": 3,
          "created_utc": "2026-02-04 12:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jubjz",
          "author": "andrew-ooo",
          "text": "The \"token anxiety\" you describe is real and honestly one of the best reasons to consider local. That mental friction of rationing API calls genuinely hurts productivity.\n\nFor practical coding work today: Qwen3-Coder (the new one just dropped) and DeepSeek-Coder-V2 are legitimately usable for real tasks - refactoring, test generation, explaining code. They're not Claude/GPT-4 level but they're \"good enough\" for 80% of daily coding tasks.\n\nHardware math that worked for me: A used 3090 (24GB VRAM, ~$700-800) can run 32B models quantized at reasonable speed. For pure coding assistance you don't need 405B params - a well-tuned 32B coder model handles most things.\n\nThe hybrid approach mentioned above is honestly the sweet spot: local for unlimited \"thinking out loud\" iterations and exploration, cloud API for the complex stuff that needs frontier model capability. Kills the anxiety without the $3k+ upfront.",
          "score": 3,
          "created_utc": "2026-02-04 16:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vsc6u",
          "author": "singh_taranjeet",
          "text": "Qwen2.5-Coder 32B and DeepSeek-Coder-V2 are solid for real dev work. I've been running them locally and they handle refactoring and test generation surprisingly well. The token anxiety disappearing alone may already make the hardware investment worth it if you code daily",
          "score": 3,
          "created_utc": "2026-02-06 11:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vui0v",
              "author": "Sandzaun",
              "text": "I'm still struggling to find a plugin for PyCharm that gives me autocomplete with an local LLM. This is all I need. No logins or accounts, just plain and simple autocomplete with my local model running via koboldcpp. Any suggestions?",
              "score": 1,
              "created_utc": "2026-02-06 11:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iudbe",
          "author": "techlatest_net",
          "text": "Yes, practical now. Qwen2.5-Coder 7B crushes benchmarks (88% HumanEval, beats GPT-4 in spots) on single RTX 4090 or A100. Use Ollama + Continue.dev â€“ latency ~1-2s, context solid up to 128k. Subs add up to $30+/mo, hardware pays off in 6-12 mos.",
          "score": 2,
          "created_utc": "2026-02-04 13:03:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jfsh9",
              "author": "tomByrer",
              "text": "Might be longer with RAM & SSD prices going up....",
              "score": 1,
              "created_utc": "2026-02-04 14:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iyccb",
          "author": "MehImages",
          "text": "depends on whether you have to buy the hardware for this specific reason and how much you benefit from no token limits vs speed.",
          "score": 2,
          "created_utc": "2026-02-04 13:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j3yle",
          "author": "bakawolf123",
          "text": "gpt-oss models are pretty good, setup wise you can run them with llama.cpp or ollama and use in a lot of agents (like Claude code, Copilot).\n\nI'm on 32GB unified RAM so for me it's not there yet (aside from mentioned gpt-oss in 20B I can only use quantized 30B models), but I think with 128GB VRAM or unified RAM machine you can feel pretty good as there're some existing ones (gpt-oss 120B, full GLM4.7 Flash, ful Qwen3 30B Coder, fp8 Qwen3 Next 80B Coder) and more decent model options keep popping up, inference engines keep getting optimizations.\n\nOne additional bonus of being local is having full control of the whole processing stack (model, inference engine, agent harness), so there might be more undiscovered integrations between those levels which could better exploit that fact.",
          "score": 2,
          "created_utc": "2026-02-04 13:57:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0g2w",
          "author": "unity100",
          "text": "Deepseek paid API is extremely cheap.",
          "score": 2,
          "created_utc": "2026-02-04 16:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pqq75",
          "author": "Euphoric_Emotion5397",
          "text": "I'm using Anti-gravity, and it's definitely cheaper and faster than anything i can run on my own machine. (a rtx 5080 16gb plus a rtx 5060TI 16gb  with 64gb ddr5).\n\nI'm using that for coding the apps . But I am using the local machine for inference for the app purpose. This type of use case will be more than adequate for the gpt-oss 20b or qwen 3 VL 30b models.\n\nso you actually can saved a lot versus sending even your local app data for online inference. Tokens are \"foc\" in local LLM. I can test and retest and scrape and rescrape daata without concern for cost..  Not too sure abt the electricity bill though. lol",
          "score": 2,
          "created_utc": "2026-02-05 13:40:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icbbb",
          "author": "Logisar",
          "text": "Which local LLM setup has the quality of Claude Sonnet 4.5? That doesnâ€™t exist.",
          "score": 2,
          "created_utc": "2026-02-04 10:48:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jhu0n",
              "author": "huzbum",
              "text": "Uh, I would argue that GLM 4.7 is equivalent to Sonnet.  I've heard good things about MiniMax M2.1, Kimi K2.5, and Qwen 3.5 is just around the corner.  \n\nThe hardware to run these large models is expensive, but it CAN be done locally.  \n\nI doubt they are equivalent to Sonnet, (maybe Haiku?) but I look forward to taking some time to try GLM 4.7 Flash and Qwen3 Coder Next.  I've been using Qwen3 30b Coder for some stuff for a while, but rely on GLM via [z.ai](http://z.ai) cloud subscription for my main workhorse.  I don't have the equipment, but it's feasible to run it locally.",
              "score": 3,
              "created_utc": "2026-02-04 15:09:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kfvq8",
                  "author": "ihackportals",
                  "text": "I'm running Qwen3-Coder-Next on a GB10 with Claude and I would say it's comparable.",
                  "score": 3,
                  "created_utc": "2026-02-04 17:46:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jizzo",
          "author": "Vittorio792",
          "text": "This is an interesting question. Running a local LLM could potentially be cheaper than relying on cloud-based AI services, especially if you have the hardware and expertise to set it up. The tradeoff is the additional overhead and maintenance required. Ultimately, it depends on your specific needs, usage patterns, and the performance requirements of your codebase. I'd suggest doing some cost analysis and benchmarking to see which option makes the most sense for your workflow.",
          "score": 1,
          "created_utc": "2026-02-04 15:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kf95g",
          "author": "beryugyo619",
          "text": "**NO**. Will it stay that way over the next 3, 5, 10 years? Everyone fears it won't stay that way, no it won't. Is it likely rational to be scared like that? Also no lol but we tend to be lol",
          "score": 1,
          "created_utc": "2026-02-04 17:44:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l1x72",
          "author": "lan-devo",
          "text": "Depends for light work, just having a 10-20 dollar sub for  more complex tasks and a local llm for simple stuff is much better, now for the simple stuff I do as a hobby-semipro is enough. One thing people undervalue is with local LLM I can go step by step, with API is a waster of money and tokens because you want to do it but you don't because it will cost you.",
          "score": 1,
          "created_utc": "2026-02-04 19:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3leeus",
          "author": "squachek",
          "text": "No",
          "score": 1,
          "created_utc": "2026-02-04 20:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mtbs1",
          "author": "No-Consequence-1779",
          "text": "I use copilot mostly for work. I code normally so itâ€™s usually method by method, through the stack.Â \n\nFor large tasks like orm mapping 150 properties or similar tedious tasks, I use a local qwen3-coder-30b. Â Â \n\nLocal can handle more for that situation. Â Â \n\nFor vibe coding type stuff, I read itâ€™s possible but I have never found a real example of a prd or whatever that gets done by agents. Â My opinion is itâ€™s highly complex and most is just marketing or people thinking they are making complex things.Â \n\nCustom business rules or complex workflows will probably be faster to just do, than iteratively describe until it gets close enough.Â \n\nThat is also the question: is it faster to just do and know itâ€™s complete or faster to write it in detail to instruct the LLM? Â \n\nIf anyone wants to msg me a real prd that actually works once, please do so. I would be a convert if itâ€™s true.Â ",
          "score": 1,
          "created_utc": "2026-02-05 00:46:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n43th",
          "author": "mpw-linux",
          "text": "What type of coding are you doing? Do you know programming well to know if the model is accurate ? If need so help with code i just use Google gemini which gives pretty good examples. Look into Liquid AI models for local llm's",
          "score": 1,
          "created_utc": "2026-02-05 01:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ohdm0",
          "author": "Proof_Scene_9281",
          "text": "Claude code in the â€˜bestâ€™ plan is gonna be hard to beat.Â \n\nIâ€™m thinking local bots will be fun for making home automation stuff without adsÂ ",
          "score": 1,
          "created_utc": "2026-02-05 07:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vp3c9",
          "author": "natanloterio",
          "text": "What's your local LLM Setup? Do you use any IDE plugin or a generic agentic  \"LLM CLI\" ? This is my current bottle neck. I have tried some \"Claude CLI Clone\" open source projects but they are usually broken or full of bugs. ",
          "score": 1,
          "created_utc": "2026-02-06 10:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icbfd",
          "author": "former_farmer",
          "text": "You can use composer 1 for free once you run out of Cursor tokens. I do.",
          "score": 1,
          "created_utc": "2026-02-04 10:48:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icu80",
          "author": "Taserface_ow",
          "text": "Donâ€™t forget to factor in power costs. And the fact that youâ€™ll need to replace your hardware frequently when youâ€™ve worn it out from running at 100% all the time.",
          "score": -2,
          "created_utc": "2026-02-04 10:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jfksw",
              "author": "tomByrer",
              "text": "Depends on where you live.  \nSome places still have cheaper electricity.  \nIn cold climates, a hot computer can be a bonus! ;)",
              "score": 2,
              "created_utc": "2026-02-04 14:57:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3l3uyu",
              "author": "GCoderDCoder",
              "text": "Cloud providers run all the time and only are saying they're replacing every 5 years or more. There are freak issues but besides x3d CPUs on ASRock boards I dont personally know anybody whose CPU or GPU died on them without them doing something weird. Hard drives are a different story... I'm actually trying to convince a certain friend to upgrade from a GTX1080 lol",
              "score": 1,
              "created_utc": "2026-02-04 19:35:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mo1n6",
                  "author": "Taserface_ow",
                  "text": "Itâ€™s the constant higj load on the machines that wear them out quickly, not that theyâ€™re running all the time.\n\nwhen i run a local llm my gpu is flat out at 100%. i already have an implementation plan prepped for my projects so iâ€™m just constantly feeding my llm input as soon as it finishes work. \n\ncloud servers distribute the load and theyâ€™re not necessary running compute intensive processes all the time.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:17:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ptlw3",
                  "author": "happytodrinkmore",
                  "text": "Nvidia is lying about a 5 year lifespan. Some chips are only lasting less than 54 days. \n\"Google architect assessed that GPUs running at 60-70% utilizationâ€”standard for AI workloadsâ€”survive one to two years, with three years as a maximum. The reason: thermal and electrical stress is simply too high.\"\n\nhttps://blog.citp.princeton.edu/2025/10/15/lifespan-of-ai-chips-the-300-billion-question/\n\nMeta found unexpected failures in a 16,000 chip farm after 54 days:\nhttps://techblog.comsoc.org/2024/11/25/superclusters-of-nvidia-gpu-ai-chips-combined-with-end-to-end-network-platforms-to-create-next-generation-data-centers/",
                  "score": 1,
                  "created_utc": "2026-02-11 00:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mj3ul",
              "author": "Internal_Werewolf_48",
              "text": "Power costs can be negligible for Macs and the Strix Halo. \n\nOr they could be crazy if you're running 6x4090's on server hardware.",
              "score": 1,
              "created_utc": "2026-02-04 23:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3t06ch",
                  "author": "Total_Engineering_51",
                  "text": "Even a big server can be managed if youâ€™re the only one using itâ€¦ 100% up time at full tilt is nonsense unless the only thing youâ€™re doing is endless model training or hosting a service. Sleep and Wake on LAN go a long way to manage a power hungry system if itâ€™s just you using it most/all of the time.",
                  "score": 1,
                  "created_utc": "2026-02-05 23:08:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qw0ofn",
      "title": "Help me find the biggest and best model!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/7oahin01kjhg1.jpeg",
      "author": "NHooked",
      "created_utc": "2026-02-04 20:55:51",
      "score": 68,
      "num_comments": 21,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qw0ofn/help_me_find_the_biggest_and_best_model/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lzc6n",
          "author": "Historical-Internal3",
          "text": "Monitor the GB10 Nvidia Dev forums, also study this repo:\n\n[https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker)\n\nYou'll get the most out of your spark that way. \n\nTake a peep at the networking guide.",
          "score": 21,
          "created_utc": "2026-02-04 22:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nxi1l",
              "author": "ftwEsk",
              "text": "Thanks for this,getting my QSFP56 cables tomorrow, will test this!",
              "score": 1,
              "created_utc": "2026-02-05 04:46:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3mgxwh",
              "author": "ifheartsweregold",
              "text": "This is the way.Â ",
              "score": 1,
              "created_utc": "2026-02-04 23:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lor7h",
          "author": "alin_im",
          "text": "kimi k2.5",
          "score": 7,
          "created_utc": "2026-02-04 21:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lps68",
              "author": "NHooked",
              "text": "On the Ollama site Kimi k2.5 shows as a cloud model, I do see other models mentioning the name. Could you show me which one I need?",
              "score": 2,
              "created_utc": "2026-02-04 21:20:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lqs34",
                  "author": "rslarson147",
                  "text": "Use vllm",
                  "score": 4,
                  "created_utc": "2026-02-04 21:25:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lrify",
                  "author": "alin_im",
                  "text": "check out huggingface\n\nhttps://huggingface.co/moonshotai/Kimi-K2.5/tree/main",
                  "score": 3,
                  "created_utc": "2026-02-04 21:28:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m0jvs",
                  "author": "twack3r",
                  "text": "The first thing is stop using Ollama. Itâ€™s a way slower, unflexible wrapper/botchjob on top of llama.cpp. Itâ€™s a complete waste of time and in many ways the worst way to start running LLMs locally.\n\nItâ€™s very easy to get the same model running locally via llama.cpp, if in doubt event ChatGPT instant will churn out a standard launch command respecting your hardware constraints.\n\nThat way you actually learn about ctx sizes, quant differences, FA specifics, different types of layer splits etc. and some out of the experience with an actual added value to your personal skillset.",
                  "score": 11,
                  "created_utc": "2026-02-04 22:11:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lpaqk",
          "author": "Mountain_Station3682",
          "text": "For open claw probably minimax m2.1 or qwen-3-coder-next (80b). You can try glm 4.7 flash but at 30b parameters it won't be amazing.\n\nYou could fit glm 4.7 (non-flash) with 2 sparks, but glm 4.7 isn't getting great reviews for open claw and it would be < 20 tokens per second.\n\n[https://forums.developer.nvidia.com/t/how-to-run-glm-4-7-on-dual-dgx-sparks-with-vllm-mods-support-in-spark-vllm-docker/355603](https://forums.developer.nvidia.com/t/how-to-run-glm-4-7-on-dual-dgx-sparks-with-vllm-mods-support-in-spark-vllm-docker/355603)",
          "score": 3,
          "created_utc": "2026-02-04 21:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lyj78",
              "author": "FishIndividual2208",
              "text": "I noticed instantly inference improvement when i moved from ollama to lama.cpp\n\nWorth a try.",
              "score": 2,
              "created_utc": "2026-02-04 22:02:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o4m0j",
                  "author": "p_235615",
                  "text": "I dont understand, why would be llama.cpp faster than ollama, when ollama is the same llama.cpp underneath with a wrapper around it. I mean sure, llama.cpp could have better defaults and expose more options, but most of those you can tune also through ollama...",
                  "score": 1,
                  "created_utc": "2026-02-05 05:38:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3nlmfx",
          "author": "pmttyji",
          "text": "Just try multiple models & share benchmarks.\n\nFrequently mentioned coding models in LLM subs(sorted by size):\n\n* GPT-OSS-20B\n* Devstral-Small-2-24B-Instruct-2512\n* Qwen3-30B-A3B\n* Qwen3-30B-Coder\n* Nemotron-3-Nano-30B-A3B\n* Qwen3-32B\n* GLM-4.7-Flash\n* Seed-OSS-36B\n* Qwen3-Next-80B\n* Qwen3-Coder-Next\n* GLM-4.5-Air\n* GPT-OSS-120B\n* Devstral-2-123B-Instruct-2512\n* MiniMax-M2.1\n* Qwen3-235B-A22B\n* GLM-4.5, 4.6, 4.7\n* Qwen3-480B-Coder\n* Deepseek-Vx, R1\n* Kimi-K2, K2.5",
          "score": 3,
          "created_utc": "2026-02-05 03:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mayjy",
          "author": "DerFreudster",
          "text": "You bought two Sparks and are using Ollama? ",
          "score": 8,
          "created_utc": "2026-02-04 23:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nfsrt",
              "author": "NotLogrui",
              "text": "Just because he has money doesn't mean they've dived extremely deep into local model infrastructure",
              "score": 5,
              "created_utc": "2026-02-05 02:54:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3oie9n",
                  "author": "NHooked",
                  "text": "Itâ€™s all a matter of time and money. Canâ€™t run any services in the cloud, everything must be on-prem.\n\nNow finally got some budget for some machines, now need to invest the time to get the most out of it.\n\nI am here to learn ðŸ˜",
                  "score": 3,
                  "created_utc": "2026-02-05 07:35:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m34c4",
          "author": "Everlier",
          "text": "Kimi 2.5 w/ vllm or ik_llamacpp is the only answer for this setup, it's far outside a segment optimised for consumers, so some setup will be needed",
          "score": 1,
          "created_utc": "2026-02-04 22:24:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mntbj",
          "author": "KooperGuy",
          "text": "Nothing too good",
          "score": 1,
          "created_utc": "2026-02-05 00:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pxfb5",
          "author": "Clear-Astronaut-8006",
          "text": "Same boat here dual DGX, currently running seperate Qwen2.5-Coder-32B-Instruct\nDeepSeek-R1-Distill-Qwen-32B\nDuring the day to vibe code and then Eagle3 training at night.\n\nCurious to see other uses.",
          "score": 1,
          "created_utc": "2026-02-05 14:17:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxuwh7",
      "title": "Super-light, 90ms latency, runs locally on Apple Silicon. More expressive and prosodic than Elevenlabs.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/gbb6ro2d2yhg1",
      "author": "EmbarrassedAsk2887",
      "created_utc": "2026-02-06 21:43:54",
      "score": 54,
      "num_comments": 18,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxuwh7/superlight_90ms_latency_runs_locally_on_apple/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3zg0hd",
          "author": "LSU_Tiger",
          "text": "Underwhelming technology and that may have been the most boring video I've ever seen. No one wants to watch someone click around a screen for 6 minutes.",
          "score": 10,
          "created_utc": "2026-02-06 22:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zi4gw",
              "author": "EmbarrassedAsk2887",
              "text": "sure! we are not here to please but show the exciting possibilities of a silicon. ",
              "score": -2,
              "created_utc": "2026-02-06 22:50:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zgrco",
          "author": "ProfMooreiarty",
          "text": "The onboarding seems like quite a bit to do.",
          "score": 6,
          "created_utc": "2026-02-06 22:42:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhuys",
              "author": "EmbarrassedAsk2887",
              "text": "its a few mins setup if you choose the hardcore mode! you can reach me out if theres any other issue!\n\nthanks :)",
              "score": 1,
              "created_utc": "2026-02-06 22:48:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o408wmt",
          "author": "TopTippityTop",
          "text": "Very cool! Wish it supported more languages (Portuguese, at least), but I get it. I take that it's open source, right? Any way to fine tune it to other languages? Can it clone voices?",
          "score": 4,
          "created_utc": "2026-02-07 01:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d9rmn",
              "author": "EmbarrassedAsk2887",
              "text": "yes it can clone voices as well. in the upcoming days, we gonna push it to production as well and before easter we will release the model weights as well. \n\nyes we did train it on 14+ languages but specifically for english and french, then spanish. \n\nbut yeah!! here you go, you can be the judge here :) \n\nhereâ€™s one example i generated for you. \n\nhttps://youtu.be/Kyd3TL5p-lQ?si=TvTUUAjsyZhPEYL9",
              "score": 1,
              "created_utc": "2026-02-09 02:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dh6rg",
                  "author": "TopTippityTop",
                  "text": "Very cool! I think the Portuguese needs more training, but it's a start!",
                  "score": 1,
                  "created_utc": "2026-02-09 03:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43zifx",
          "author": "Maximum-Wishbone5616",
          "text": "what ???\n\n# Desktop Only Experience\n\nHey there! BodegaOS is currently only available on desktop. Please visit this downloads page on your desktop computer to experience BodegaOS and all its features.\n\nWTF  \non 2x 5090, 1x 7950x3d, 5x 4k monitors and you still using some crap that cannot recognize desktop? \n\nThat is 2001 knowledge...",
          "score": 1,
          "created_utc": "2026-02-07 17:19:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d64vr",
              "author": "EmbarrassedAsk2887",
              "text": "hey there! we relied on the browser user agent for detecting the device type. apologies for that, now you can visit againâ€” you wonâ€™t have a problem now.\n\napologies for any inconvenience. please dm or hit me up anytime with any query you might have.",
              "score": 1,
              "created_utc": "2026-02-09 02:36:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40w9en",
          "author": "somethingdangerzone",
          "text": "More competition in the voice space is great! I look forward to seeing how this develops.\n\nI tried going to your srswti.com/download site above, but I keep getting a black, blank page. It could be me, or maybe there is a site issue.",
          "score": 1,
          "created_utc": "2026-02-07 03:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41kaxl",
              "author": "EmbarrassedAsk2887",
              "text": "hey there, thanks! the post mentions https://www.srswti.com/downloads\n\nyou can try visiting it again? please hit me up or dm if any other queries.",
              "score": 0,
              "created_utc": "2026-02-07 07:03:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o432knh",
                  "author": "somethingdangerzone",
                  "text": "I tried it again on both LibreWolf and Brave, and I got a black blank page both times.if anything else, i will reach out, thanks",
                  "score": 1,
                  "created_utc": "2026-02-07 14:33:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qyl7ac",
      "title": "What 's a realistic comparison betwee what you can run on a 512GB ram M3 Ultra vs frontier models",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qyl7ac/what_s_a_realistic_comparison_betwee_what_you_can/",
      "author": "rwvyf",
      "created_utc": "2026-02-07 18:22:06",
      "score": 51,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m looking for real-world impressions from the \"high-RAM\" club (256GB/512GB M3 Ultra owners). If you've been running the heavyweights locally, how do they actually stack up against the latest frontier models (Opus 4.5, Sonnet 4.5, Geimini 3 pro etc)\n\n* Coding in a relative large codebase, python backend, javascript front end \n* Best-quality outputs (not speed) for RAG over financial research/report + trading idea generation, where I focus on:\n   * (1) data quality + retrieval, \n   * (2) verification/compute\n   * (3) a multi-pass reasoning pipeline.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qyl7ac/what_s_a_realistic_comparison_betwee_what_you_can/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o44qdz3",
          "author": "TokenRingAI",
          "text": "Dual M3 Ultra 512GB will allow you to run Kimi 2.5 at a reasonable speed, which is 100% a frontier model.\n\nIf M5 ultra shows up with a bump in compute, memory bandwidth, 768G ram, and a 15K price tag, which seem like reasonable possibilities, then we will have frontier level performance on a single mac",
          "score": 42,
          "created_utc": "2026-02-07 19:33:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o455aeg",
              "author": "SpicyWangz",
              "text": "768 M5 ultra would go so hard. That could start seriously eating into nvidiaâ€™s prosumer market share.Â ",
              "score": 16,
              "created_utc": "2026-02-07 20:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45tpnn",
                  "author": "TokenRingAI",
                  "text": "If the M5 Ultra utilizes the memory speeds of the M5 generation while maintaining the 1024-bit bus width of the M3 Ultra, it would achieve a theoretical memory bandwidth of approximately 1,224 GB/s",
                  "score": 12,
                  "created_utc": "2026-02-07 23:07:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45gipz",
                  "author": "segmond",
                  "text": "NOT could, but WOULD",
                  "score": 6,
                  "created_utc": "2026-02-07 21:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45zeep",
              "author": "mewnor",
              "text": "Donâ€™t get me excited like that.",
              "score": 4,
              "created_utc": "2026-02-07 23:43:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ytow",
          "author": "false79",
          "text": "I would not allow any LLM to do backtesting. You're just asking for trouble when it's real-time and the results are so far apart.",
          "score": 5,
          "created_utc": "2026-02-07 20:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44z57p",
              "author": "rwvyf",
              "text": "Thatâ€™s for sure. Thats what the codebase is for.",
              "score": 3,
              "created_utc": "2026-02-07 20:19:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o478wqg",
          "author": "Technical_Buy_9063",
          "text": "I run a single m3 ultra v3 512gb - I run Qwen-3-coder-next as a virtual assistant - clearly some delay, but for now I'm getting all of my LLM use out of it. I use ChatGPT 5.3 to craft some prompts and such for it, but this is more of a expediting measure than anything. Not exactly what you're asking but also I'm not fully optimized at all - this is just a raw setup for now. ",
          "score": 3,
          "created_utc": "2026-02-08 04:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45ggcx",
          "author": "segmond",
          "text": "Kimi K2.5 & DeepSeekv3.2 are just as good as SOTA and can 100% serve as substitutes.\n\ngpt-oss-120b, GLM4.7, MiniMax2.1 will handle 90% of SOTA needs.",
          "score": 7,
          "created_utc": "2026-02-07 21:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46olbb",
          "author": "desexmachina",
          "text": "Do you guys trust it without web context?",
          "score": 2,
          "created_utc": "2026-02-08 02:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49cjz6",
              "author": "AnxietyPrudent1425",
              "text": "Thatâ€™s why you add web context in LM Studio",
              "score": 2,
              "created_utc": "2026-02-08 14:49:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48gmqo",
          "author": "Hector_Rvkp",
          "text": "I've been comparing hardware options from Strix halo to stuff costing 15k$. I don't think 256+gb apple vram makes sense currently because the bandwidth is too low, even at c.1000gb/s. Token speed is bandwidth/model size, so you can't run a dense model, obviously, but even MoE models that take 128gb+ call layers so big that token speed is still unusable. Afaik, sweet spot is moe model fitting on 128gb ram. The penalty seems to be 5-25pc performance, and what that means in practice is anybody's guess, there's no standard. But comparing quality without factoring in speed / usability is somewhat irrelevant.",
          "score": 2,
          "created_utc": "2026-02-08 11:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ael0o",
              "author": "JonasTecs",
              "text": "So what is best from your comparison?",
              "score": 2,
              "created_utc": "2026-02-08 17:58:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l84je",
                  "author": "Hector_Rvkp",
                  "text": "\"best\" depends on budget. Strix halo 128gb @2k$ is cheapest entry point for legit LLM work. \"Legit\" isn't without caveats. From 3-4k$ upwards, it gets less clear. Apple is great, Nvidia dgx spark CAN make a lot of sense, and Nvidia GPUs can destroy everything in some use cases. There's also a time function (N1X might come out, new M5 configs will come out).",
                  "score": 1,
                  "created_utc": "2026-02-10 09:50:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44gz5w",
          "author": "Powerful-Street",
          "text": "Itâ€™s shit compared to Claude or Gemini. Context is limited so much, that a large codebase gets hallucinated. Nothing local comes close.",
          "score": -1,
          "created_utc": "2026-02-07 18:45:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44vnh9",
              "author": "redditor_420_69_lol",
              "text": "Claude and Gemini are just doing context compression and pretending to have super long context, which you can totally do locally",
              "score": 11,
              "created_utc": "2026-02-07 20:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o454vhq",
                  "author": "XccesSv2",
                  "text": "How can i Do this locally?",
                  "score": 1,
                  "created_utc": "2026-02-07 20:50:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o49vapy",
                  "author": "Powerful-Street",
                  "text": " When you have a program with 10k lines of spec, local rag, doesnâ€™t have near enough information for the model to find the correct context. I have tried other things and all will get lost if you donâ€™t know what you are pointing them to. Also, Gemini Ultra has much more context, and it still gets it wrong, most times. I now use one instance of Gemini or Claude to plan a debug session and will use another instance, to follow in the past of the lastâ€”if the knowledge base is clean. None are perfect in large codebase, which is what the OP asked about. I have found ways to work with Claude code or Gemini on its ultra plan. Local with RAG doesnâ€™t even touch them yet.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:25:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44i403",
              "author": "rwvyf",
              "text": "Thatâ€™s sad to hear",
              "score": 0,
              "created_utc": "2026-02-07 18:51:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44d18i",
          "author": "ComfyUser48",
          "text": "Following",
          "score": 0,
          "created_utc": "2026-02-07 18:26:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzsynx",
      "title": "Qwen3 Coder Next on M3 Ultra v.s. GX10",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "author": "Imaginary_Ask8207",
      "created_utc": "2026-02-09 03:11:13",
      "score": 50,
      "num_comments": 22,
      "upvote_ratio": 0.96,
      "text": "https://preview.redd.it/rg9mxsm7zdig1.png?width=2294&format=png&auto=webp&s=7400c7c54428910158a160e5e407022cbba24947\n\n[Qwen3-Coder-Next served on GX10](https://reddit.com/link/1qzsynx/video/cfad1dooxdig1/player)\n\n[Qwen3-Coder-Next served on M3 Ultra 512GB](https://reddit.com/link/1qzsynx/video/sawsb3kpxdig1/player)\n\nI'm currently exploring CLI-based coding tools as an alternative to GitHub Copilot.\n\nRight now I'm testing **opencode** (seems to be the most popular open-source option at the moment), and I paired it with the new **Qwen3-Coder-Next** model. It's decent!\n\nThe tasks I tested were pretty simple, just some small refactoring in my toy project and fixing one bug. But that's exactly the point, for these kind of everyday coding tasks, you ain't gonna need Opus 4.6 level of intelligence, a \"small and sweet\" open source model is usually good enough.\n\nI feel like 80b model is a sweet spot for GX10 with 128GB of GPU memory, 8-bit quantized models could be comfortably loaded into the GPU. For comparison between the two devices, you can clearly see M3 ultra gives a higher throughput, but it's 3x the price of GX10.\n\nDo you think going local will be the coming trend?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4df22x",
          "author": "Thump604",
          "text": "This is my agent pick of the day with Step as the planner",
          "score": 8,
          "created_utc": "2026-02-09 03:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4diz60",
              "author": "Imaginary_Ask8207",
              "text": "Great suggestion! I don't think GX10 will be able to load Step-3.5-Flash, I'll try on M3 ultra.",
              "score": 2,
              "created_utc": "2026-02-09 03:47:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dr3e2",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-02-09 04:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dmfxb",
          "author": "NoobMLDude",
          "text": "Going local is already the current trend !!\nYou are in the right subreddit for it. ðŸ˜‰\n\n\nIâ€™ve gone Local since a long time for most of my tasks that are Private.\n\nIâ€™m also trying to educate the community that you donâ€™t need to give your money and personal data to big AI companies. You can download Free and OpenSource models and they are good enough for 90% of users. \n\n\nHere are few examples of Local AI workflows that I use:\n- [Local Meeting Assistant](https://youtu.be/cveV7I7ewTA)\n- [Local Talking Assistant](https://youtu.be/2VHzYy45kPw)\n- [Terminal with AI Context support](https://youtu.be/_sDJBosDznI)\n\n\nI added a few more in the [LocalAI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)\n\nOwen3-Coder-Next looks like a solid model for Coding if you can run a 80B model on your hardware.",
          "score": 4,
          "created_utc": "2026-02-09 04:10:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4e5mlg",
          "author": "HumanDrone8721",
          "text": "As I know someone using a similar DGX Spark setup, they use a specialized replacement for the otherwise excellent nvtop that is not really optimized for Sparks that is called dgxtop:\n\nhttps://github.com/GigCoder-ai/dgxtop\n\nIs vibe coded, but one of the good ones IMHO:",
          "score": 3,
          "created_utc": "2026-02-09 06:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e7gw8",
              "author": "Imaginary_Ask8207",
              "text": "Thanks for sharing! This definitely looks better than nvtop. Is it stable and well-tested?",
              "score": 1,
              "created_utc": "2026-02-09 06:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e884x",
                  "author": "HumanDrone8721",
                  "text": "AFAIK they have terminals open running it on both of their Spark cluster nodes as it shows also the network traffic over the high-speed interfaces (these are indeed FAST), along with the CPU and GPU temps, for me is fascinating how they correlate while doing complex inference. So far I've heard no complains, but I'm a poor so I don't own a Spark or two ;).",
                  "score": 2,
                  "created_utc": "2026-02-09 06:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4eorv5",
              "author": "eleqtriq",
              "text": "Nvtop has been updated and works now",
              "score": 1,
              "created_utc": "2026-02-09 09:38:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h70nx",
                  "author": "HumanDrone8721",
                  "text": "Super, thanks, I will let the \"sparkers\" know about it-",
                  "score": 1,
                  "created_utc": "2026-02-09 18:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gqa1r",
          "author": "kingcodpiece",
          "text": "I can say that it's running a fair bit faster than that Mac example on my GX10 using an FP4MX quant which the Grace Blackwell machines love (I don't have the exact numbers but it was like 65 t/s outout and 370ish prefill) . That's using llama.cpp BTW. It's the first model that's had an acceptable speed vs quality tradeoff on this machine for me.",
          "score": 2,
          "created_utc": "2026-02-09 17:21:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jskzh",
              "author": "Imaginary_Ask8207",
              "text": "How does llama.cpp compare with vLLM on GX10?   \nI am used to serving models through vLLM, but nvfp4 MoE models will crash intermittently, which is quite annoying.",
              "score": 1,
              "created_utc": "2026-02-10 02:54:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k9i9t",
                  "author": "kingcodpiece",
                  "text": "vLLM was a pain to get running. Llama.cpp was much easier to get running and it's faster and so far very stable.\n\nEdit: For anybody interested, the architecture isn't properly implemented in stable pytorch yet (CUDA version is too high, plus ARM CPU is a pain to workaround).",
                  "score": 1,
                  "created_utc": "2026-02-10 04:44:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m0a9i",
          "author": "techlatest_net",
          "text": "M3 Ultra 512GB outpacing GX10 on Qwen3-Coder-Next throughput is no surpriseâ€”Apple's unified memory crushes fragmented GPU loads, especially for 80B 8-bit quant that saturates 128GB cleanly.\n\n**Hardware showdown**:\n- **M3 Ultra wins**: Higher t/s across refactors/bug fixes (your vids prove it). Unified 512GB = zero swapping, CLI tools like opencode scream.\n- **GX10 value king**: 1/3 price for \"good enough\" 80B coding. nVidia ecosystem + CUDA upgrades future-proof it over Apple's walled garden.\n\n**Local coding trend locked in**:\n- Opencode + Qwen3-Coder-Next already beats Copilot for toy projects/real workâ€”your \"small and sweet\" thesis is spot on.\n- 80B sweet spot confirmed: Everyday tasks don't need Opus-level IQ, just reliable context handling.\n\n**Stick with GX10** if budget > peak perf. M3 Ultra's premium is for Mac diehards who refuse Docker. Local CLI stacks (opencode/aider) are obliterating SaaS subscriptionsâ€”your benchmarks prove the economics work today.",
          "score": 2,
          "created_utc": "2026-02-10 13:28:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qqxcg",
          "author": "catplusplusok",
          "text": "Just saying, NVIDIA Thor Dev Kit is slightly cheaper than DGX Spark and seems to be faster both on paper (twice the compute speed) and practice (less crusty NVFP4 support now after a rough start). Now idea about vs SOTA Mac Studio, but those are expensive.",
          "score": 2,
          "created_utc": "2026-02-11 04:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4el8of",
          "author": "HallTraditional2356",
          "text": "Amazing! How did you host it on the Apple Silicon M3 Ultra to Connect opencode to it ?",
          "score": 1,
          "created_utc": "2026-02-09 09:02:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4es3zv",
              "author": "Imaginary_Ask8207",
              "text": "I server the model with LM Studio, which gives OpenAI compatible endpoints.",
              "score": 3,
              "created_utc": "2026-02-09 10:11:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jk42v",
                  "author": "nickollie_",
                  "text": "How are you finding the latest version of LM Studioâ€™s server?",
                  "score": 1,
                  "created_utc": "2026-02-10 02:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fnvqr",
          "author": "bakawolf123",
          "text": "You are comparing GX10 to maxed M3 Ultra though, you can get one with 256GB (alas there's no 128GB option, only 96GB which seems just shy of decent space for context), it's more like 2x price then and you get an actual working environment on top of being able to run a model in background.\n\nOr you can go even cheaper with AMD AI Max+ 395 (TG is about same as gx10 according to llama.cpp benchmarks, but prefill is slower)",
          "score": 1,
          "created_utc": "2026-02-09 14:09:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g5a24",
          "author": "Vozer_bros",
          "text": "I actually just need a good architecture, and a local model with good tool calling and SWE bench a bit better than 60%.",
          "score": 1,
          "created_utc": "2026-02-09 15:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kj5uo",
          "author": "taiphamd",
          "text": "Should try to run the DGX spark with flashinfer as the attention backend using vLLM. Furthermore vLLM also has MoE tuning which might be able to squeeze out some more tps. See this issue : https://github.com/vllm-project/vllm/issues/17619 but you basically launch benchmark_moe.py and remember to specify max batch_size or else it will try to tune up to 4096 and that might take a week to run.",
          "score": 1,
          "created_utc": "2026-02-10 05:58:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx51zc",
      "title": "OpenClaw with local LLMs - has anyone actually made it work well?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qx51zc/openclaw_with_local_llms_has_anyone_actually_made/",
      "author": "FriendshipRadiant874",
      "created_utc": "2026-02-06 02:14:48",
      "score": 47,
      "num_comments": 128,
      "upvote_ratio": 0.79,
      "text": "Iâ€™m honestly done with the Claude API bills. OpenClaw is amazing for that personal agent vibe, but the token burn is just unsustainable. Has anyone here successfully moved their setup to a local backend using Ollama or LM Studio?\n\nI'm curious if Llama 3.1 or something like Qwen2.5-Coder is actually smart enough for the tool-calling without getting stuck in loops. Iâ€™d much rather put that API money toward more VRAM than keep sending it to Anthropic. Any tips on getting this running smoothly without the insane latency?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qx51zc/openclaw_with_local_llms_has_anyone_actually_made/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3uapf9",
          "author": "DataGOGO",
          "text": "Yes, but I wouldnâ€™t run it until they fix the code / massive security holes.\n\nVibecoded slop.",
          "score": 40,
          "created_utc": "2026-02-06 03:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3urjku",
              "author": "onethousandmonkey",
              "text": "This is the correct answer.",
              "score": 10,
              "created_utc": "2026-02-06 05:38:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4plg7t",
              "author": "FindingAwake",
              "text": "What LLM did you use?",
              "score": 1,
              "created_utc": "2026-02-10 23:56:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4qq38v",
                  "author": "DataGOGO",
                  "text": "Hu?",
                  "score": 1,
                  "created_utc": "2026-02-11 04:01:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3yp2b6",
              "author": "Decent-Freedom5374",
              "text": "Why donâ€™t you just build a model that circulates and isolates, protecting your code only but still allowing quantum gain and make it derive any malicious intent for subsequent gain",
              "score": -2,
              "created_utc": "2026-02-06 20:23:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3yvv87",
                  "author": "DataGOGO",
                  "text": "What?\n\nOhâ€¦. Sorry:\n\nGLM-47-flash NVFP4 and qwen3-30b instruct-2507-NVFP4Â ",
                  "score": 2,
                  "created_utc": "2026-02-06 20:57:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zzjxa",
              "author": "HealthyCommunicat",
              "text": "the security flaws that they have are the same exact security flaws we've always had. if you aren't doing ur due dilligence to check every layer of the application to make sure that it just technologically and physically cannot do xyz then you shouldn't really have any security concerns.\n\n  \nin short, you can address the security concerns yourself.",
              "score": -3,
              "created_utc": "2026-02-07 00:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o407pfo",
                  "author": "DataGOGO",
                  "text": "You can address the issues yourself, but that involves a rewriting a LOT of the source codeâ€¦. Most of it in fact.\n\nThere is no way to fix them without recoding.Â ",
                  "score": 2,
                  "created_utc": "2026-02-07 01:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3v4y4r",
              "author": "Boring-Attorney1992",
              "text": "why do you think it's vibecoded? or is that just your way of criticizing the app?",
              "score": -21,
              "created_utc": "2026-02-06 07:31:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3v6fj7",
                  "author": "Thomdesle",
                  "text": "Look up statements of the creator and check out the  source code.",
                  "score": 14,
                  "created_utc": "2026-02-06 07:44:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3vlpfq",
                  "author": "andrewfenn",
                  "text": "The creator literally says on social media that he doesn't even look at the code. When someone demanded he fixed the security issues he just put his hands up ðŸ¤·â€â™‚ï¸ and said he doesn't know what to do. So yes, it's 100% slop and you can't trust that guy at all.",
                  "score": 9,
                  "created_utc": "2026-02-06 10:10:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wo1wf",
                  "author": "noctrex",
                  "text": "Here you go: [https://www.youtube.com/watch?v=8lF7HmQ\\_RgY](https://www.youtube.com/watch?v=8lF7HmQ_RgY)\n\n\"I ship code I don't read\"",
                  "score": 3,
                  "created_utc": "2026-02-06 14:30:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3weldn",
                  "author": "DataGOGO",
                  "text": "â€¦. Did you look at the code?",
                  "score": 1,
                  "created_utc": "2026-02-06 13:40:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u6x0y",
          "author": "regjoe13",
          "text": "I just played with it,  taking over Signal to my local gpt-oss-120b in lmstudio\nI installed openclaw under nologin user on my Linux, locking permissions to a particular folder.  \nIt was fun to play with it, but nothing it does is really worth the risk of having it for me.",
          "score": 7,
          "created_utc": "2026-02-06 03:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vgthg",
          "author": "dragonbornamdguy",
          "text": "Using it with qwen3 coder 30b, its awesome. Setup was undocumented hell. Works very well. He can create own skills only by telling him.",
          "score": 11,
          "created_utc": "2026-02-06 09:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhrso",
              "author": "Technical_Buy_9063",
              "text": "can you share your setup? is it LM Studio?",
              "score": 1,
              "created_utc": "2026-02-06 22:48:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41icxd",
                  "author": "GreaseMonkey888",
                  "text": "I actually told OpenClaw to configure local LMstudio and Ollama by testing the endpoints of the providers. After some iterations it worked and I could switch over to local providers. At some point I tried to use the working configuration in another VM with OpenClaw, but it had to almost start over configuring it self, although I gave it the config snippets of the previous working oneâ€¦ However, I have a Mac Studio M4 with 64GB, but prefill phase is slow, OpenClaw seems to push so much context into the LLM that it takes very long for every response, no matter how small the model is.",
                  "score": 1,
                  "created_utc": "2026-02-07 06:45:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mq2ez",
              "author": "PaulShroom",
              "text": "I got it working on a 5090 laptop 24gb vr but it was slow, but when I run the same llm on lm studio it super nippy think I need to do a different setup.Â ",
              "score": 1,
              "created_utc": "2026-02-10 15:43:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0qbq",
                  "author": "dragonbornamdguy",
                  "text": "I run mainly vllm because of nvlinked 3090s. But when new models are released best out of box experience is indeed with LM Studio.",
                  "score": 1,
                  "created_utc": "2026-02-10 16:32:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3tywva",
          "author": "NoobMLDude",
          "text": "> Iâ€™d much rather put that API money toward more VRAM than keep sending it to Anthropic.\n\nThis is the right way !! ðŸ«¡ \nIâ€™m trying to educate more users to realize this and run their own models for free than pay some company that is going to use your data against you in few months.\n\nQwen3Coder or Qwen3Coder-Next is decent for tool calling and agentic uses.\n\n[https://qwen3lm.com/coder-next/](https://qwen3lm.com/coder-next/)\n\nIâ€™ve not used OpenClaw due to the security loopholes discovered.\n\nHowever if you wish to try other more secure uses for Local LLMs, here are a few simple examples\n- Private Meeting Assistant \n- Private Talking Assistant \n- The usual Coding Assistants\n- terminal with AI support \n\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 17,
          "created_utc": "2026-02-06 02:29:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uicyy",
              "author": "Electronic_Muffin218",
              "text": "Alright, I'll bite - what's the best way to get adequate hardware for these things? Is there some sort of good - better - best (with ballpark prices or not) for nominally consumer-available GPUs (and whatever else matters)? I'm wondering specifically if 48GB is a useful sweet spot, and if so, is there a meaningful performance difference between buying two 24GB cards and just one 48GB card.\n\nIs there a guide to these things that folks keep up to date a la the NUC buyer's guide/spreadsheet? I could of course ask (and have asked) the commercial LLMs themselves, but I'm never sure what they're wrong about or leaving out.",
              "score": 4,
              "created_utc": "2026-02-06 04:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uut8l",
                  "author": "NoobMLDude",
                  "text": "TLDR; You can run with whatever device you have available to try it out. \n\nDisclaimer: Iâ€™ve not tried OpenClaw, all comments below is for agent workflows that do similar things locally.\n\nAll of the above tools currently run on my MacBook M2 Max 32GB laptop without any additional GPUs.\n\nI was considering upgrading to bigger GPUs but the rate at which open Source models are improving, I think i might not even need to upgrade. \n\nThe smaller models are already decent enough for those tasks. Of course the huge models would perform better for tool-calling, but for me the marginal improvements does not justify the huge costs of hardware.\n\n- 2x24GB VRAM can run the same models as single 48GB VRAM.\n- generally higher the VRAM the larger models you can run\n\nPrices are skyrocketing. So donâ€™t buy before you have tried cheaper alternatives. You might not even notice huge differences.",
                  "score": 2,
                  "created_utc": "2026-02-06 06:03:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wrg90",
                  "author": "HuckSauce",
                  "text": "Get a AMD Strix Halo mini pc or laptop (Ryzen AI Max) - 128 GB VRAM for 2-3k",
                  "score": 2,
                  "created_utc": "2026-02-06 14:48:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xb72u",
                  "author": "unique-moi",
                  "text": "One thing to keep in mind is that PCs running one GPU are a commodity, while PCs with two high speed PCI slots and a powerful power supply are specialist.",
                  "score": 2,
                  "created_utc": "2026-02-06 16:24:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45rqrz",
              "author": "Dispo96",
              "text": "When you say security loopholes what do you mean?",
              "score": 1,
              "created_utc": "2026-02-07 22:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o477sqy",
                  "author": "NoobMLDude",
                  "text": "A lot of issues have been published.  \nTLDR; Giving access to all your accounts (telegram, email, etc) to anyone human or AI is not a good idea. Main issues: Private Data access + exposure to Unsecured content + Ability to externally communicate  \n  \n [https://www.paloaltonetworks.com/blog/network-security/why-moltbot-may-signal-ai-crisis/](https://www.paloaltonetworks.com/blog/network-security/why-moltbot-may-signal-ai-crisis/)",
                  "score": 1,
                  "created_utc": "2026-02-08 04:24:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o401q80",
              "author": "HealthyCommunicat",
              "text": "the security loopholes in openclaw are the same security loopholes in any agentic bot that has access to a bunch of tools and high autonomy. if you lack the knowledge to see that then you needa go all the way back to the basics and make sure u are fully capable of going through the code and files and making sure that there just isnt anything that can physically be taken advantage of.\n\n  \nexample, if you make sure that it is physically not possible for ur model to run the command \"rm\" or \"rm -rf\" then you wouldnt be worrying about it being able to delete things. if you dont have ur bot able to be reached whatsoever to public internet, then u truly dont have to worry about anything.\n\n  \nlets stop talking about security flaws like they cant be fixed with really easy steps.",
              "score": 1,
              "created_utc": "2026-02-07 00:42:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o47w0xh",
                  "author": "NoobMLDude",
                  "text": "Of course they can be fixed. \n\nItâ€™s just that people are running these agents without understanding the risks they present.\n\nSo it is the responsibility of the people familiar with the tech to educate and inform those who are not familiar with it, and the responsibility of people running it to understand the risks and apply the fixes.",
                  "score": 1,
                  "created_utc": "2026-02-08 07:47:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u152z",
          "author": "Antique_Juggernaut_7",
          "text": "I got glm4.6v to behave relatively well so far -- been trying it for the past 24h on a dual DGX Spark setup and vLLM. It weirds out at times, but is generally helpful and functional.\n\nI chose this particular model for its image processing capabilities and overall model size. It works for openclaw with a slight change on its chat template.",
          "score": 3,
          "created_utc": "2026-02-06 02:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub6d9",
              "author": "DataGOGO",
              "text": "Try with my 4.6V-NVFP4 quant, it works really well\n\nhttps://huggingface.co/GadflyII/GLM-4.6V-NVFP4",
              "score": 2,
              "created_utc": "2026-02-06 03:43:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uk2bs",
                  "author": "Antique_Juggernaut_7",
                  "text": "Great stuff! Can you share your vllm serve command? I've been having trouble getting NVFP4 to run well in my cluster due to some GB10 shenanigans...\n\nEDIT: wrote before actually checking the HF page. Thanks for adding it there. Are you running this in a DGX Spark?",
                  "score": 1,
                  "created_utc": "2026-02-06 04:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3vcgo7",
              "author": "edmerf",
              "text": "I work on DGX Spark with vLLM and made it work with LLama4-Scout-17b-16e-instruct-NVFP4. However I still couldn't manage to find a perfect chat template. Chat flow is really digsusting. What kind of template do you use and how do you derive it to make it work with OpenClaw?",
              "score": 2,
              "created_utc": "2026-02-06 08:41:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vm01u",
                  "author": "Antique_Juggernaut_7",
                  "text": "The issue with running GLM4.6 is that OpenClaw expects a \"developer\" role, but GLM4.6's chat template only accepts \"system\". So you just need to change that particular line in the chat template to make it run.",
                  "score": 1,
                  "created_utc": "2026-02-06 10:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3x9w9v",
              "author": "unique-moi",
              "text": "How about Claude code self-hosted (I mean pointing at a self-hosted LLM)?",
              "score": 1,
              "created_utc": "2026-02-06 16:18:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xo6yq",
                  "author": "Antique_Juggernaut_7",
                  "text": "Haven't tried yet. Will try and see how it behaves.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44n7dp",
              "author": "scottybowl",
              "text": "Can I ask how you got vllm working? Struggling with the configuration (also using a DGX Spark)",
              "score": 1,
              "created_utc": "2026-02-07 19:16:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o44qz1d",
                  "author": "Antique_Juggernaut_7",
                  "text": "Do you know if you are you having trouble strictly with vLLM, or with getting openclaw to work nice with vLLM?\n\nI'm running vLLM using the community Docker container from eugr: [https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker)  \nIf you only wish to run GLM4.5/4.6V on vLLM (ie not running with openclaw), you can try the following command -- note that the vllm serve command is baked in, so you can also try that directly if you already have vllm installed somewhere (note also: if you're not running a cluster of Sparks, adapt accordingly):  \n\\`\\`\\`  \n./launch-cluster.sh  \\\\\n\nexec vllm serve zai-org/GLM-4.6V-FP8 \\\\\n\n\\--port 8000 \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\\\\n\n\\--gpu-memory-utilization 0.75 \\\\\n\n\\-tp 2  \\\\\n\n\\--distributed-executor-backend ray  \\\\\n\n\\--max-model-len 128000  \\\\\n\n\\--load-format fastsafetensors  \\\\\n\n\\--enable-auto-tool-choice \\\\\n\n\\--tool-call-parser glm45  \\\\\n\n\\--reasoning-parser glm45  \\\\\n\n\\--trust-remote-code \\\\\n\n\\--allowed-local-media-path / \\\\\n\n\\--mm-encoder-tp-mode data  \n\\`\\`\\`\n\nBUT!\n\nOpenclaw at this point requires a chat template that accepts the \"developer\" role. GLM4.5 and 4.6 follow a chat template that only accepts the \"system\" role. So you will need to provide vLLM with a custom chat template for this to work.\n\nEDIT: went ahead and pushed to Github the instructions on how to make GLM4.5/4.6 on vLLM to talk to OpenClaw: [https://github.com/fidecastro/fix\\_glm46v/](https://github.com/fidecastro/fix_glm46v/)",
                  "score": 1,
                  "created_utc": "2026-02-07 19:36:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u1kyv",
          "author": "SillyLilBear",
          "text": "I run it with M2.1 4 bit locally, works well.",
          "score": 3,
          "created_utc": "2026-02-06 02:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u32xg",
          "author": "shigeru777",
          "text": "Try qwen3-coder-next, better inference speed than GLM-4.7-FLASH,  but still too hard to use tool/skill calling. I only use openclaw for chat and weather information / brave api search.",
          "score": 4,
          "created_utc": "2026-02-06 02:53:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o451aki",
              "author": "chimph",
              "text": "why not just run open webui with web search then?\n\n",
              "score": 2,
              "created_utc": "2026-02-07 20:31:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xnv6k",
              "author": "cashedbets",
              "text": "What do you have qwen3-coder-next running on?",
              "score": 1,
              "created_utc": "2026-02-06 17:23:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4026ou",
                  "author": "shigeru777",
                  "text": "Mac Studio M3 ultra 256GB",
                  "score": 1,
                  "created_utc": "2026-02-07 00:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3tzbru",
          "author": "piddlefaffle12",
          "text": "Spent a few days on this with my 5090 and M4 Max 128GB.\n\nOnly model that kinda worked is glm-4.7-flash. Prompt pre-processing is going to be the performance killer for self hosted agentic in my experience.  \n",
          "score": 3,
          "created_utc": "2026-02-06 02:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uayjz",
              "author": "DataGOGO",
              "text": "Depends on your hardware and hosting configuration.Â ",
              "score": 1,
              "created_utc": "2026-02-06 03:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3u3n9g",
          "author": "FinancialMoney6969",
          "text": "I keep fucking mine up. Iâ€™ve tried everything even LM studioâ€¦",
          "score": 3,
          "created_utc": "2026-02-06 02:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub9q7",
              "author": "DataGOGO",
              "text": "LMstudio is pretty trash dude.Â ",
              "score": 0,
              "created_utc": "2026-02-06 03:44:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ufq1z",
                  "author": "FinancialMoney6969",
                  "text": "Whatâ€™re you using?",
                  "score": 2,
                  "created_utc": "2026-02-06 04:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3xg80c",
              "author": "DarkZ3r0o",
              "text": "Try with ollama the difference is huge",
              "score": 0,
              "created_utc": "2026-02-06 16:47:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4001jm",
                  "author": "FinancialMoney6969",
                  "text": "Thatâ€™s where Iâ€™m having the problemâ€¦ I am trying on windows tho",
                  "score": 1,
                  "created_utc": "2026-02-07 00:32:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ud8xk",
          "author": "kdd123456789",
          "text": "If we setup kimi on hetzner vps running openclaw locally, what kind of costs would be involved, as the cost of the hardware to run a descent llm locally is pretty expensive.",
          "score": 3,
          "created_utc": "2026-02-06 03:57:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v7h9x",
          "author": "Gargle-Loaf-Spunk",
          "text": "You mean it's supposed to work at all? ",
          "score": 3,
          "created_utc": "2026-02-06 07:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y9ame",
          "author": "Professional_Owl5603",
          "text": "I have a question, I know Claw is a security nightare but I dont need it to do half the things people say it could. I essentaill want a bot that can help me to research on thing. Example: I'll talk to Grok (yea I know, but if I need spicy, I go there, everythign else is Gemini for anythign serious) and will discuss something I saw on youtube, like a new LLM or API or whatever. Like the new Nvidia Personoplex. I woudl like to have the bot go and research it for me, check the gitgub and see if it can be intergrated into itself. Obviously, this is an extreme situation, but along these lines. \n\n  \nThe reason why I thought this was possible was becasue I was tryign to get it to work with discord so I can talk to it that way, and when I was testing it via Claude Opus, I asked it to help me configure it so it would work the way I wanted to. It just did it. And when it hit problems, it kept trying things, which is GREAT, however, the openwebui credits I have for over a year of 4.35c that I've been using that lasted me forever, was drained in minutes to .35c apparently soakign though hundreds of thousands of tokens. Which is nuts.\n\n  \nSo my take is, claude is great and works as advertised, at the cost of a liver and partial kidney per hour. I realize there isnt a comparable model that's open source, but I'm wondering if I can get close? With those abilities? My rig is pretty basic, I have an older Gigabyte X99P-sli Motherbaord with 225gb of ram, that has pci 3x slots and dual rtx 5090's that I use for minecraft, with ollama, so I have 64gb of pooled vram using ollama. I get about 30tps usign a 70b model. Which im guessign inhundreds if times slower than the cloud API.\n\n  \nAm I just dreaming here? Would a machien like DGX spark make a better machine? I'm guess it probably wouldnt as it just has x2 the vram and nothing would change other than the model and maybe a lower tps even. And yes I knwo giving it access to this machine is dangerous, Ive installed it closed wsl enviorment. I dont plan to give it access to anything and stricly want to use it as a chat bot springboard research assistant. I manage my own calendar.\n\nAm I wasting my time? Thanks for the advice in advance.\n\n![gif](giphy|1D7ryE8SDYuq8kGGGQ)",
          "score": 3,
          "created_utc": "2026-02-06 19:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xfpom",
          "author": "DarkZ3r0o",
          "text": "I tested it with glm-4.7-flash with 35k context and gpt-oss-20b with 120k context and am really satisfied with results. I have 3090ti",
          "score": 2,
          "created_utc": "2026-02-06 16:45:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40he1v",
          "author": "Long_Complex_4395",
          "text": "I used LMStudio with Qwen2.5 instruct. I wrote on how to set it up\n\nhttps://medium.com/@nwosunneoma/how-to-setup-openclaw-with-lmstudio-1960a8046f6b",
          "score": 2,
          "created_utc": "2026-02-07 02:17:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u0w0b",
          "author": "Battle-Chimp",
          "text": "All these OpenClaw posts just prove that smart people still do really, really dumb things.\n\nDon't install OpenClaw. ",
          "score": 5,
          "created_utc": "2026-02-06 02:41:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u4hcc",
              "author": "actadgplus",
              "text": "All these OpenClaw posts just prove that smart people still post really, really dumb things.\n\nDo your research and install OpenClaw.",
              "score": -2,
              "created_utc": "2026-02-06 03:02:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3u5dpr",
                  "author": "Momo--Sama",
                  "text": "I have it running on a separate mini pc with a kimi sub, and its definitely fun to mess around with, but there's not a lot I can actually do with it while refusing to give it access to any of my personal accounts. Maybe I'm just not being creative enough, idk",
                  "score": 2,
                  "created_utc": "2026-02-06 03:07:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3uavlu",
                  "author": "DataGOGO",
                  "text": "I wouldnâ€™t.Â ",
                  "score": 2,
                  "created_utc": "2026-02-06 03:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3uy3lz",
          "author": "grumpycylon",
          "text": "I tried OpenClaw with Llama 3.1 and it was spewing nonsense. I typed hi in the chat and it kept typing giant paragraphs of garbage.",
          "score": 2,
          "created_utc": "2026-02-06 06:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3up97b",
          "author": "IngwiePhoenix",
          "text": "Really want to try it myself to see how far it can go - but I fear my singular 4090 is not going to go that far... x)\n\nI hear Qwen3-Coder (and it's -Next variant) are really good. In general, tool-call optimized models like the recent GLMs and such should do well.\n\nIn theory, anyway.",
          "score": 1,
          "created_utc": "2026-02-06 05:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v0oia",
          "author": "SEND_ME_YOUR_ASSPICS",
          "text": "Any recommendations for 32 RAM and 16 VRAM?",
          "score": 1,
          "created_utc": "2026-02-06 06:53:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wm2rq",
              "author": "mzinz",
              "text": "Gpt-oss:20b. One of the recommended models straight from the ollama docs",
              "score": 1,
              "created_utc": "2026-02-06 14:20:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vjo6c",
          "author": "ifheartsweregold",
          "text": "Yeah working really well I just got it set up with dual DGX Sparks running Minimax 2.1",
          "score": 1,
          "created_utc": "2026-02-06 09:51:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vm0il",
          "author": "prusswan",
          "text": "I wanted a tool like this but only as a guidance rather than having broad executive powers - it is too much of a security burden (can't give it free reign, and whatever it does needs to have audit trail). Open to suggestions",
          "score": 1,
          "created_utc": "2026-02-06 10:13:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vndrj",
          "author": "Zevatronn",
          "text": "I run qwen 8b with openclaw and qwebcoder 30b local models are used ny sub agent while the 'conductor' runs on a chatgpt sub it works fine",
          "score": 1,
          "created_utc": "2026-02-06 10:25:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yasxu",
          "author": "w3rti",
          "text": "I made it work once, it was perfect, write codes, write apps, adjust setup, performance,. Clawd just did everything for me. After graphic card update and some changes it went garbage. I hat 5 days of fun, still keeping those .mds and sessesion, when he will work with the llm like this again, we can continue",
          "score": 1,
          "created_utc": "2026-02-06 19:12:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yoeod",
          "author": "Decent-Freedom5374",
          "text": "I use 8 ram and use the new release ollama gave of awencoder inside for free. Works great same project multiple terminals. Why",
          "score": 1,
          "created_utc": "2026-02-06 20:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zz2qe",
          "author": "HealthyCommunicat",
          "text": "yes it is. and its easy.\n\nif ur looking at qwen 2.5 and llama 3.1 you do not have the required level of information throughput. this is a space that is ever changing at a pace no other field has moved in before. the models we had a year ago (qwen 2.5 as you say) are leagues leagues less capable than the model that just came out, qwen 3 coder next 80b (when comparing to 72b or 70b qwen 2.5) literally feels like an entirely different kind of tech. one can write files and access emails and search the web, one cant even run a simple find command.\n\nif u put in the work and learn ground up instead of trying to rush in and expect results, then you'd come to see very easily that this field requires really high levels of information intake on a daily basis. to top it off, this is a niche that requires u to have a minimum of 5-10k to even touch a model that feels somewhat capable.\n\n  \nwhat makes it even worse is reading that u do in fact have a claude subscription. if u were dilligent you would've used it in combination with ur own local models to better learn how you can utilize these models. if you cared you would've already asked claude to help you with this setup.",
          "score": 1,
          "created_utc": "2026-02-07 00:27:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43b20q",
          "author": "FrankWanders",
          "text": "Anyone here who has experience with Ministral 3 14B? It supports up to 256K context, but with 128K context it seems to be just small enough into one RTX 5090 without offloading, so this would make it doable.\n\nCompared to Flash 4.7 I think ministral 3 14B is the winner,  or am I wrong? To be fair I don't see any other option to run it on local hardware? Open to suggestions!",
          "score": 1,
          "created_utc": "2026-02-07 15:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4onwyb",
          "author": "playingdrumsonmars",
          "text": "couldn't get it to run in any reasonable way until I found lfm2.5-thinking today.  \nTry out this model  \n  \n\"ollama pull lfm2.5-thinking:latest\"\n\nIt is the only model I found that would not hallucinate like crazy even with simple prompts",
          "score": 1,
          "created_utc": "2026-02-10 21:05:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pyh8s",
              "author": "InterestingFly9566",
              "text": "I canâ€™t get it to work at all with ollama, have you got a tutorial how to set it up?",
              "score": 1,
              "created_utc": "2026-02-11 01:11:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3uff1h",
          "author": "HenkPoley",
          "text": "Even the gigantic open weights models are 9 months behind the closed source models (\"always have been\"). The models of Anthropic and OpenAI only recently got to the level where they can work autonomously for a bit. Claude Opus 4.5 was released on 24 November 2025, GPT-5.1 on November 12, 2025.\n\nYou'll have to wait till mid august, and have a very beefy machine by then.\n\nBtw, clawdbot(-descendents) are conceptually fun, but in reality not that interesting.",
          "score": 1,
          "created_utc": "2026-02-06 04:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vmrtq",
          "author": "RevealIndividual7567",
          "text": "I would highly recommend not running openclaw, or if you have to then running it in a sandbox with very limited external websites and resources allowed. It is a security nightmare due to things like website prompt injection.",
          "score": 1,
          "created_utc": "2026-02-06 10:20:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ekq5u",
          "author": "Pains_asistant",
          "text": "Yo lo estoy configurando actualmente y llevaba unos dias pegandome porque al parecer cado modelo cambia un poco en el tema del JSON de openclaw (o eso me parece a mi) pero encontre un comando (solo para configurar el LLM local) luego configurar claw ya es otra historia.   \nUna vez tienes configurado e instalado el bot puedes ejecutar este comando en la bash:  \nollama launch openclaw  \nCon ese comando ya te manda seleccionar el modelo de ollama que quieres de los que trienes instalados o de los que te recomeinda y se configura automatico (cosa que me vi negro para hacer manual yo mismo).  \nPor otro lado tengo en local el llama 3.1 8b y no consigo que ese  modelo me responda bien... pero es que por lo menos reponde que el dolphin o el mannix no responden y no los consigo configurar (manualmente todo hasta ahora que acabo de descubir el comando de ollama y de momento solo pude probar ese modelo utilizable).  \nTengo miedo a que sea por la capacidad del modelo lo que me esta pasando porque no me salen logs de error en el dashboard ni en el status de claw\n\nhttps://preview.redd.it/9g4kctlvnfig1.png?width=962&format=png&auto=webp&s=27edfb69558ff2ad7191172b45f2527fb45fed5c\n\nY con este modelo probando a que me respondiese mal (se desinstalo completamente del equipo a si mismo osea que imaginate el poder que tiene y lo loco que esta jaja) recomiendo hacerlo en un pc aislado y con sus propios recursos de redes y correo.  \nPor cierto acabo de comprobar que despues se te va a lanzar automaticamente con el modelo que selecciones la primera vez que hagas el lauch, estoy mirando como cambiarlo porque esta config de modelo auto es mil veces mas comoda, peleandome con el json no di hecho ",
          "score": 0,
          "created_utc": "2026-02-09 08:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u581c",
          "author": "actadgplus",
          "text": "Iâ€™m have a really powerful Mac Studio M3 Ultra with 256GB RAM so testing out various models via LM Studio.  I havenâ€™t leaned on anything yet.\n\nIn parallel I have been exploring also leveraging Synthetic.  Has anyone given it a try?  Thoughts?\n\nhttps://synthetic.new",
          "score": -3,
          "created_utc": "2026-02-06 03:06:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvu651",
      "title": "Fine-tuned Gemma 3 270M to detect \"AI slop\" - runs in a browser extension",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qvu651/finetuned_gemma_3_270m_to_detect_ai_slop_runs_in/",
      "author": "maciejgryka",
      "created_utc": "2026-02-04 17:04:16",
      "score": 46,
      "num_comments": 14,
      "upvote_ratio": 0.94,
      "text": "https://preview.redd.it/7vhtcf7ydihg1.png?width=1024&format=png&auto=webp&s=3da874322690b25ae002dbbbcd37cd6edf47a181\n\nBeen working on a small project to detect AI-generated \"slop\" text. The goal was simple: make something that runs locally, fits in a browser extension, and doesn't require sending your text anywhere.\n\n\\*\\*The approach:\\*\\*\n\nWe used knowledge distillation to compress a 120B teacher model into Gemma 3 270M. The base Gemma model scores \\~40% on our test set (random guessing on binary classification). After fine-tuning with \\~10k examples distilled from the teacher, the student matches teacher performance on held-out data.\n\nTraining data was based on the \\[Kaggle AI-generated essays dataset\\](https://www.kaggle.com/datasets/denvermagtibay/ai-generated-essays-dataset), expanded via distillation. So the model is specifically tuned for that style of content, though it generalizes reasonably well to other formats.\n\nFor browser deployment, we quantized to Q4\\_K\\_M (\\~242 MB). Accuracy drops from 100% to \\~95% on our test set - that's the tradeoff for fitting in a Chrome extension.\n\n\\*\\*Results:\\*\\*\n\n| Model | Size | Test Accuracy |  \n|-------|------|---------------|  \n| GPT OSS 120B (teacher) | \\~120B | 100% (20/20) |  \n| Gemma 3 270M (fine-tuned) | 270M | 100% (20/20) |  \n| Gemma 3 270M Q4\\_K\\_M | \\~242 MB | 95% (19/20) |  \n| Gemma 3 270M (base) | 270M | \\~40% |\n\nReal-world testing on content outside the training domain:\n\n| Content Type | Accuracy |  \n|--------------|----------|  \n| ChatGPT outputs (n=50) | 98% |  \n| Human tweets (n=50) | 94% |  \n| Reddit comments (n=100+) | \\~92% |  \n| Formal emails (n=30) | 88% |\n\nFormal human writing (business emails, academic text) is where it struggles most - too much stylistic overlap with AI output.\n\n\\*\\*Limitations:\\*\\*\n\n\\- \\~1 in 20 predictions wrong after quantization  \n\\- Trained on essays, so may miss AI patterns in other domains  \n\\- Formal human writing triggers false positives  \n\\- First load downloads \\~253 MB (cached after)  \n\\- Inference \\~0.5-2s on CPU\n\n\\*\\*Tech stack:\\*\\*\n\nThe extension runs via \\[Wllama\\](https://github.com/niconielsen32/wllama) (llama.cpp in WebAssembly). No API calls, works offline after initial model download. Fine-tuning was done with LoRA.\n\n\\*\\*Links:\\*\\*\n\n\\- Repo: [https://github.com/distil-labs/distil-ai-slop-detector](https://github.com/distil-labs/distil-ai-slop-detector)  \n\\- Model weights: [https://huggingface.co/distil-labs/distil-ai-slop-detector-gemma](https://huggingface.co/distil-labs/distil-ai-slop-detector-gemma)\n\nHappy to answer questions about the training setup, LoRA config, or browser deployment.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qvu651/finetuned_gemma_3_270m_to_detect_ai_slop_runs_in/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3kavyz",
          "author": "NoobMLDude",
          "text": "Cool use case for a tiny model. \n\nI think finetuned tiny models like these are the future. Can be used to solve many such problems that require running models locally in constrained hardware /  edge.",
          "score": 11,
          "created_utc": "2026-02-04 17:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l3mjd",
              "author": "maciejgryka",
              "text": "Couldn't agree more, this is basically the thesis behind our platform!",
              "score": 5,
              "created_utc": "2026-02-04 19:34:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3krqyi",
          "author": "ImOutOfIceCream",
          "text": "â€œFormal human writing triggers false positivesâ€ are you sure you didnâ€™t train a model to detect formal writing",
          "score": 5,
          "created_utc": "2026-02-04 18:40:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l3j6p",
              "author": "maciejgryka",
              "text": "I think this is a legit worry :) FWIW we just used the existing dataset and trained a low-param-count classifier to work on that data set, which was the main point we wanted to show. We should probably de-emphasize some claims of the usefulness of the demo tool itself and focus more on the stack behind it!",
              "score": 1,
              "created_utc": "2026-02-04 19:34:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kpvpk",
          "author": "former_farmer",
          "text": "Thanks, good idea. Should come by default in Reddit, Twitter, etc.",
          "score": 2,
          "created_utc": "2026-02-04 18:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lfnye",
          "author": "GlassAd7618",
          "text": "Wow, nice!  \n  \n\\> After fine-tuning with \\~10k examples distilled from the teacher, the student matches teacher performance on held-out data.\n\nIt always amazes me how well this approach works. If you think of it, 10K examples is nothing compared to the vast amount of data used to train the teacher. It is somehow always a wow-moment to see how well the distilled models perform.",
          "score": 2,
          "created_utc": "2026-02-04 20:32:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lnlav",
              "author": "maciejgryka",
              "text": "agreed, it feels pretty magical",
              "score": 2,
              "created_utc": "2026-02-04 21:09:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3klzlb",
          "author": "SourceCodeplz",
          "text": "Hey great project! I don't see a License file in your Github repo.",
          "score": 1,
          "created_utc": "2026-02-04 18:14:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l5osk",
              "author": "maciejgryka",
              "text": "Thanks, added now! (Apache 2)",
              "score": 1,
              "created_utc": "2026-02-04 19:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kp9h4",
          "author": "v01dm4n",
          "text": "What does it say about this post? /s\n\nOn a serious note, did you evaluate your model on any of the benchmark datasets: M4, MAGE, RAID, etc?",
          "score": 1,
          "created_utc": "2026-02-04 18:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l5u0i",
              "author": "maciejgryka",
              "text": "I did ran it before posting and the result did make me chuckle :)\n\nWe didn't, but that's a good idea!",
              "score": 2,
              "created_utc": "2026-02-04 19:45:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3krh4z",
          "author": "former_farmer",
          "text": "Do I need to install anything else in order to make it work? or only install the browser addon? or install also the platform to host the model.\n\nPS: Fix the link in readme, should go to [https://github.com/ngxson/wllama](https://github.com/ngxson/wllama), it's currently giving 404.",
          "score": 1,
          "created_utc": "2026-02-04 18:39:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3krs4p",
              "author": "maciejgryka",
              "text": "You just need the extension, which will download the ~300MB model when started",
              "score": 1,
              "created_utc": "2026-02-04 18:40:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kgsiy",
          "author": "jschw217",
          "text": "Cool idea ðŸ‘",
          "score": 1,
          "created_utc": "2026-02-04 17:51:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyjztp",
      "title": "I built a localâ€‘first RPG engine for LLMs (beta) â€” UPF (Unlimited Possibilities Framework)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qyjztp/i_built_a_localfirst_rpg_engine_for_llms_beta_upf/",
      "author": "Gohzio",
      "created_utc": "2026-02-07 17:36:54",
      "score": 42,
      "num_comments": 45,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nI want to share a hobby project Iâ€™ve been building:Â **Unlimited Possibilities Framework (UPF)**Â â€” a localâ€‘first, stateful RPG engine driven by LLMs.\n\nIâ€™mÂ **not a programmer by trade**. This started as a personal project toÂ **help me learn how to program**, and it slowly grew into something I felt worth sharing. Itâ€™s still aÂ **beta**, but itâ€™s already playable and surprisingly stable.\n\n# What it is\n\nUPF isnâ€™t a chat UI. Itâ€™s anÂ **RPG engine**Â with actualÂ **game state**Â that the LLM canâ€™t directly mutate. The LLM proposes changes; the engine applies them via structured events. That means:\n\n* **Party members, quests, inventory, NPCs, factions, etc.**Â are tracked in state.\n* Changes are applied through JSON events, so the game doesnâ€™t â€œforgetâ€ the world.\n* Itâ€™s localâ€‘first, inspectable, and designed to stay coherent as the story grows.\n\n# Why you might want it\n\nIf you love emergent storytelling but hate losing context, this is the point:\n\n* The engineÂ **removes reliance on context**Â by keeping the world in a structured state.\n* You canÂ **lock fields**Â you donâ€™t want the LLM to overwrite.\n* Itâ€™s built for longâ€‘form campaigns, not just short chats.\n* You get RPGâ€‘like continuity without writing a full game.\n\n# Backends\n\nMyÂ **favourite backend is LM Studio**, and thatâ€™s why itâ€™s the priority in the app, but you can also use:\n\n* **text-generation-webui**\n* **Ollama**\n\n# Model guidance (important)\n\nIâ€™ve tested with modelsÂ **under 12B**Â and IÂ **strongly recommend not using them**. The whole point of UPF is to reduce reliance on context, not to force tiny models to hallucinate their way through a story. Youâ€™ll get the best results if you use yourÂ **favorite 12B+ model**.\n\n# Why Iâ€™m sharing\n\nThis has been a learning project for me and Iâ€™d love to see other people build worlds with it, break it, and improve it. If you try it, Iâ€™d love feedback â€” especially around model setup and story quality.\n\nIf this sounds interesting, this is my repo  \n[https://github.com/Gohzio/Unlimited\\_possibilies\\_framework](https://github.com/Gohzio/Unlimited_possibilies_framework)\n\nThanks for reading.\n\nEdit: I've made a significant update to the consistency of RPG output rules. I strongly recommend you use the JSON schema in LM studio. I know Ollama has this functionality to but I have not tested it.\n\nModels, I have found instruction models ironically fail to follow instructions and actively try to fight the instructions from my framework. Thinking models are also pretty unreliable.\n\nThe best models are usually compound models made for roleplay 12-14b parameters with massive single message context lengths. I recommend uncensored models not because of it's ability to create lewd stories but because they have fewer refusals (none mostly). You can happily play a Lich and suck the souls out of villagers without the model having a conniption.  \nI am hesitant to post a link to a NSFW model because it's not actual the reason I made the app. Feel free to message me for some recommendations.\n\nhttps://preview.redd.it/8817b4fqwnig1.png?width=1784&format=png&auto=webp&s=1e83f62b646baff084265731b8bc2b926b993f48\n\n",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qyjztp/i_built_a_localfirst_rpg_engine_for_llms_beta_upf/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o446o91",
          "author": "ComprehensiveFun3233",
          "text": "It always make me laugh at myself, but even when I'm in a LLM sub, and the explicit interesting tool is using LLM, I just glaze over and stop reading trhw second I realize the explanation is made/highly tuned by a LLM itself.",
          "score": 4,
          "created_utc": "2026-02-07 17:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o447bwc",
              "author": "Gohzio",
              "text": "Or perhaps I'm not the most eloquent and grammatically correct person out there so, to make it comprehensible I took my notes and put it into LM studio and asked for a comprehensible post ðŸ¤·ðŸ¿â€â™‚ï¸",
              "score": 5,
              "created_utc": "2026-02-07 17:58:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45tosj",
                  "author": "nachohk",
                  "text": "If you couldn't be bothered to write it, why should you expect anyone else to be bothered to read it? Using an LLM to help improve and refine your own words is one thing. Using them to inflict slop upon the rest of us is another.",
                  "score": 4,
                  "created_utc": "2026-02-07 23:06:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o448m1a",
                  "author": "ComprehensiveFun3233",
                  "text": "Exactly, I know you did that.  I do it as well.  But it still just makes my eyes glaze over. Interesting.",
                  "score": 0,
                  "created_utc": "2026-02-07 18:04:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o44rts6",
                  "author": "FirstEvolutionist",
                  "text": "I fault people for using AI to summarize or structure text only when the content is already bad: promotion, a disguised ad, AI psychosis or non sense.\n\nFor pruposeful use of AI formatting, summarizing and structure I've actually began to appreciate it: I no longer need to tune in to the style of the person writing to make sure I don't kisunderstand anything or having to deal with unclear text. The same has started happening for audio (podcasts) and video.\n\nWhen I want info, I rather have it done by AI. When I want connection, or thinking, I'm ok with old fashioned content as long as it is well done.\n\nI have a feeling that a lot of people will quickly adopt a similar preference over time.",
                  "score": -1,
                  "created_utc": "2026-02-07 19:40:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o444m8w",
          "author": "dmitryplyaskin",
          "text": "https://preview.redd.it/vvx2zxhn04ig1.png?width=1256&format=png&auto=webp&s=9826e5995f16fa5328ca3db135866a66049c260d\n\nThe repository is not available at this link.",
          "score": 1,
          "created_utc": "2026-02-07 17:44:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o444qiq",
              "author": "Gohzio",
              "text": "It would be a good idea for me to open it. thanks!",
              "score": 3,
              "created_utc": "2026-02-07 17:45:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o445dap",
              "author": "Gohzio",
              "text": "fixed",
              "score": 3,
              "created_utc": "2026-02-07 17:48:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o445jok",
              "author": "drumzalot_guitar",
              "text": "Agreed, same result.  Any amount of searching doesnâ€™t turn anything up.",
              "score": 1,
              "created_utc": "2026-02-07 17:49:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o447mz7",
                  "author": "Gohzio",
                  "text": "fixed",
                  "score": 2,
                  "created_utc": "2026-02-07 17:59:44",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o444q0i",
          "author": "ghostd93",
          "text": "The link does not work",
          "score": 1,
          "created_utc": "2026-02-07 17:45:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o445cos",
              "author": "Gohzio",
              "text": "fixed",
              "score": 3,
              "created_utc": "2026-02-07 17:48:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o445r25",
                  "author": "ghostd93",
                  "text": "Thanks. Looks promising. I will definitely check it out",
                  "score": 1,
                  "created_utc": "2026-02-07 17:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44axih",
          "author": "Oshden",
          "text": "Awesome work man!!!",
          "score": 1,
          "created_utc": "2026-02-07 18:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44n7vj",
          "author": "Aionion",
          "text": "curious, does it allow api's for other services such as OpenAi, Anthropic, NateGPT, Moonshot, etc.?\n\n",
          "score": 1,
          "created_utc": "2026-02-07 19:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44nd65",
              "author": "Gohzio",
              "text": "It's fully OpenAI compatible.",
              "score": 2,
              "created_utc": "2026-02-07 19:17:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45hsvv",
          "author": "DHFranklin",
          "text": "Cool. I did something similar, I've been experimenting in AI Studio since April doing a lot of this.\n\nI have Custom Instructions for the game play and found that systems it's really familiar with like d20 work better than bespoke ones. I then drag and drop RAG files as plain text and JSON and haven't had much trouble with one more than the other. I tell it to rely on python for the math and then relate it narratively for \"critical failure, failure, success, critical success\" However it doesn't really work out. It won't let a character lose.\n\nI have had tones of problem with things like tone and consistent character \"voice\". As it plays out it changes format for game state. It has problems with context bleed and context rot especially at token lengths over 150-200k tokens or so.\n\nHow does this compare?",
          "score": 1,
          "created_utc": "2026-02-07 22:00:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45qfwa",
              "author": "Gohzio",
              "text": "Yes, I know this well. My first 2 tries ended similarly. My first try just shuffled things in and out of context but the LLM just went insane. Second I tried splitting the processing to a back end server type thing but the processing power needed started to ramp up and basically only would work if you connected it to chat gpt or something and had massive context lengths (literally the opposite to what I wanted)\n\nWith this I started with 100% Rust and tried to make the LLM output tags for everything so the engine could read them and process them accordingly. This was the best yet but my keywords list went straight over 300 words and still missed stuff due to how conversations work.\n\nThat's when I read about context requests  in the open ai format. I pivoted to that and it works amazingly. The LLM doesn't need to remember a damn thing it just requests the information it needs to build an output. So then the world data is basically the system prompt and the engine combines the players input and relevant data from the player tab and if the LLM needs to know something like how loot drops work it will just request that data adding delay but removing context needs and so far the LLM has stayed sane.\n\nI've gotten as far as I can in solo testing and as I neither know anyone on Linux or with a GPU as powerful as mine (5080) I decided to let Reddit destroy me ðŸ˜‚.",
              "score": 1,
              "created_utc": "2026-02-07 22:48:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45ymob",
                  "author": "DHFranklin",
                  "text": "Oh geez. Yeah that sounds like a hell of a work around. No I don't have a GPU that hefty. I wanted to ask the speed of turn around on prompts, but I guess that answer wouldn't be terribly useful.\n\nDo you do anything that needs a RNG? How are you handling that? What I'm getting out of Gemini 2.5 and 3 (admittedly all I've tried) is that it will see the instruction to run python for random numbers, do math sometimes and sometimes not, then just auto-win anyway.",
                  "score": 1,
                  "created_utc": "2026-02-07 23:38:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47k6lx",
          "author": "TeamDman",
          "text": "Would be good to add some screenshots to the post and readme",
          "score": 1,
          "created_utc": "2026-02-08 06:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lspnx",
              "author": "Gohzio",
              "text": "Screenshot added. Readme in the link :)",
              "score": 1,
              "created_utc": "2026-02-10 12:41:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48cb8x",
          "author": "AnotherFuckingSheep",
          "text": "I don't get it. Do you have a demo?",
          "score": 1,
          "created_utc": "2026-02-08 10:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48gqeg",
              "author": "Gohzio",
              "text": "It's fully usable in the git.",
              "score": 1,
              "created_utc": "2026-02-08 11:03:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48zmb7",
                  "author": "AnotherFuckingSheep",
                  "text": "This is totally not my domain. From the post you did it sounds like this is something that can actually run and give users results. \n\nFor me running this would probably take a couple of days.\n\nMaybe you can post a chat, a game text, or something else to just show off what it actually does.\n\nAgain not my domain so if this is actually impossible you can just tell me",
                  "score": 1,
                  "created_utc": "2026-02-08 13:33:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aco5t",
          "author": "Narrow_Operation7252",
          "text": "Tried it out, but I keep getting a completion error when I try to send a prompt to my local LLM (with both LM Studio and Ollama). It says â€œLLM error: error sending request for urlâ€. Any idea what that might be?",
          "score": 1,
          "created_utc": "2026-02-08 17:49:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ao0u7",
              "author": "Gohzio",
              "text": "Did you have a model loaded? And was it connected. I've only seen this error when I've forgotten to load a model or press the connect button",
              "score": 1,
              "created_utc": "2026-02-08 18:42:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4apc1n",
                  "author": "Narrow_Operation7252",
                  "text": "Yeah, itâ€™s kind of strange. The UPF program can connect and see the model. And occasionally Iâ€™ll even get a response or two back if I set it to 127.0.0.1 instead of Localhost. But then it will fail on the next prompt I send, and Iâ€™ll keep getting the error messages again after that. \n\nIt might be something weird with my setup, but just wanted to see if it was happening to anyone else.",
                  "score": 1,
                  "created_utc": "2026-02-08 18:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hubjv",
          "author": "fungnoth",
          "text": "It's so hard to get into. I've set it up and struggle to think of that much info to put in.\n\nCan anyone share their world building?",
          "score": 1,
          "created_utc": "2026-02-09 20:33:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hw5vp",
              "author": "Gohzio",
              "text": "Everything is filled out for you in world tab except the description and world name. The only things I would select is the quest rules I keep them all selected. ChatGPT can give you an easy description but here's one I use.\n\n>!Elysium Prime is an immense world, approximately 139,820 times the mass of Earth, with a diameter roughly equal to Jupiter's. Its surface area spans over 65 billion square kilometers, consisting of five major continents and countless smaller islands scattered across vast oceans that cover nearly 75% of its surface.!<\n\nFor player tab I just usually keep the same one that I used from before I made the app, the same I try to play every first run of a new RPG. A melee tank with a shield and warhammer",
              "score": 2,
              "created_utc": "2026-02-09 20:42:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4icb8j",
          "author": "PeaceLoveorKnife",
          "text": "Does it work with tabbyapi?",
          "score": 1,
          "created_utc": "2026-02-09 22:03:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4il8qr",
              "author": "Gohzio",
              "text": "I've literally never heard of it. Until I made this app I actually didn't know so many backends existed. If it supports openai connections it will work otherwise I doubt it.   \nI strongly recommend LM studio or Ollama as both support JSON schema which significantly improves reliability. I work for neither of these companies I just love Local LLM.",
              "score": 2,
              "created_utc": "2026-02-09 22:48:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ko25",
          "author": "ghostd93",
          "text": "Did you consider adding an option for generating images (sending a request to comfyui api)? For imagine last scene etc?",
          "score": 0,
          "created_utc": "2026-02-07 19:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lsmt2",
              "author": "Gohzio",
              "text": "I did think about it but decided not to. It really was just about a way to circumvent the context restrictions in LLM Roleplay. I am looking into adding a 'create prompt' for NPC's that character cards so you can upload images for immersion like the main character can.",
              "score": 1,
              "created_utc": "2026-02-10 12:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0x3kn",
      "title": "What would a good local LLM setup cost in 2026?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r0x3kn/what_would_a_good_local_llm_setup_cost_in_2026/",
      "author": "Lenz993",
      "created_utc": "2026-02-10 10:20:12",
      "score": 42,
      "num_comments": 60,
      "upvote_ratio": 0.96,
      "text": "If you had a budget of around $5,000 and wanted to set up a local LLM, what hardware would you go for? Would you go for a good PC with a powerful graphics card like an RTX 5090 or a Mac like the M3 Ultra? And why?\n\nI would like to connect and use my own local LMM with OpenClaw in the course of the year (hopefully more powerful local models will come onto the market in 2026).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r0x3kn/what_would_a_good_local_llm_setup_cost_in_2026/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4mil5k",
          "author": "PraxisOG",
          "text": "Honestly 4x 3090 is a solid way to go as a balance of memory capacity and speed, and run something like Qwen coder next super fast. You could also get more vram with an obscure option like 7x AMD V620 for full gpu offload using GLM4.7 or a lot of context with minimax 2.1 and soon 2.2. Though if you donâ€™t want a super loud mining rig type thing get a strix halo box for 2k and call it a day, same vram capacity as 4x3090 but slower and whisper quiet in comparison.Â ",
          "score": 12,
          "created_utc": "2026-02-10 15:06:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndwvb",
              "author": "Endhaltestelle",
              "text": "Where can you buy 4 x 3090 for $5000 ?",
              "score": 1,
              "created_utc": "2026-02-10 17:33:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ni0oh",
                  "author": "Doug_Fripon",
                  "text": "How much is a 3090 nowadays? A few weeks ago in Europe you could easily get one for 400â‚¬. Now 600.",
                  "score": 5,
                  "created_utc": "2026-02-10 17:52:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nf8gt",
                  "author": "AnonymousCrayonEater",
                  "text": "Ebay?",
                  "score": 2,
                  "created_utc": "2026-02-10 17:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4qtw1d",
              "author": "WonderfulEagle7096",
              "text": "Depending on where you live, 4x 3090 could end up being very costly operationally. Assuming 8 hours per day operation (average work day), 4 Ã— 350 W = 1.40 kW or roughly 5000 kWh annually.\n\nWhere I live this will translate to \\~$1800/year in electricity alone, then you'll have another chunk in deprecation. An M3/M4 chip (Macbook Pro/Mac Studio) only consumes <100W and depreciates a lot more gracefully.",
              "score": 1,
              "created_utc": "2026-02-11 04:27:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ozxjv",
              "author": "Citizen_Edz",
              "text": "I donâ€™t think 4x 3090s is a very realistic setup for 5k? Thatâ€™s atleast 3k in gpus. Then you probably need a thredripper, or other workstation/server ship to support that many pcie devices.",
              "score": 1,
              "created_utc": "2026-02-10 22:01:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pl382",
                  "author": "PraxisOG",
                  "text": "You can throw it in a 1k X299 platform and mining frame and save a grand if you wantÂ ",
                  "score": 3,
                  "created_utc": "2026-02-10 23:54:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4msiyb",
          "author": "SimplyRemainUnseen",
          "text": "I'd probably cluster 2 128gb ryzen ai max+ systems. 256gb of unified memory is crazy good. Really good 4bit performance on those systems for LLMs and image generation.\n\nYou can finetune a QAT LoRA with unsloth's workflows to shrink the performance gap for int4 quants. For reference you can get usable speeds on models like GLM 4.7 with int4 quantization.\n\nA friend of mine runs a ComfyUI API on one system and GPT OSS 120B on the other as a house AI system that can do image & video gen. \n\nThat amount of unified memory really opens up a lot of possibilities.",
          "score": 16,
          "created_utc": "2026-02-10 15:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n2dak",
              "author": "Zyj",
              "text": "If you add RDMA capable network cards\nyou speed things up with tensor parallelism",
              "score": 4,
              "created_utc": "2026-02-10 16:40:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o0ken",
                  "author": "jstormes",
                  "text": "Do you know of any posts about this?  I am looking into it for my setup?",
                  "score": 1,
                  "created_utc": "2026-02-10 19:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4q0fs7",
              "author": "flyingbanana1234",
              "text": "i would totally get 2 ryzens if they just had higher bandwidth as i want to mess around with video generation, i might just wait for the m5 256 gb mac studio but im so impatient",
              "score": 2,
              "created_utc": "2026-02-11 01:22:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n21rh",
          "author": "Zyj",
          "text": "2x Strix Halo (2x $2000) + 2 used infiniband network cards (2x $200) + 2x Oculink and a small PSU. Probably around $4500-4600 in\ntotal. Gives you the ability to run 470b models at q4 etc.\nSpeed wonâ€™t be great but good enough probably.",
          "score": 8,
          "created_utc": "2026-02-10 16:38:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q0nzy",
              "author": "flyingbanana1234",
              "text": "i could be wrong but waiting for the 256 gb m5 mac studio for 5500 seems to be a better move\nhigher bandwidth ðŸ« ðŸ« ðŸ« ",
              "score": 2,
              "created_utc": "2026-02-11 01:24:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4opwh4",
              "author": "Lenz993",
              "text": "That's a great idea too! Thank you very much.",
              "score": 1,
              "created_utc": "2026-02-10 21:14:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4njtm0",
          "author": "Own_Atmosphere9534",
          "text": "Macbook M4 PRO MAX 128 gb comes in around $5K.  Best models for this config:\n\n**Llama 3.3 70B Instruct (MLX / GGUF 4-bit/8-bit**  \nstrong reasoning, writing, and instruction following. Supposedly GPT 4 Class inference on local machine\n\n**Qwen3** coder variants (\\~80B MLX)  \n**Qwen3-235B-A22B (MLX 3-bit or 4-bit)**  \nGood coding\n\nand others Iâ€™m sure. \n\nRTX 5090 32 GB VRAM  come in new around this price point ($5000).\n\n As others have said here you could stack multiple RTX 3090s and get a pretty good system with large VRAM for \\~5K probably more.\n\niâ€™ve heard that the new Mac chips perform in 3090 class RTX range (irrespective of  ram size).  Maybe someone can confirm or give a better indication of  performance is \n\nThe real question is how large of model  do you want to  run and how comfortable are you with screwing around with hardware. \n\nUntil recently, I was looking at Nvidia based solutions but did some testing on my 24 gig M4  MacBook,  and found that things like GPT-OSS-20B run extremely well for my usage. So I went ahead and bought a M4  PRO MAX 128 gig.\n\n\n\n\n\n\n\n",
          "score": 8,
          "created_utc": "2026-02-10 18:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qgy12",
              "author": "Boring-Attorney1992",
              "text": "what kind of tokens/s are we ranging here with your Mac 128GB?",
              "score": 3,
              "created_utc": "2026-02-11 03:02:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r5b1d",
                  "author": "Own_Atmosphere9534",
                  "text": "Just ordered it. The MAX MacBooks are on backorder. Itâ€™s going to be mid March. There is pretty strong likelyhook that the M5 MAX MacBooks are going to be announced early March. Something to consider if you are looking in this direction. Iâ€™m considering canceling but in all likely hood the M5 MAX will be shipping a month later once they are announced so that would be at least April..",
                  "score": 2,
                  "created_utc": "2026-02-11 05:54:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4o15np",
          "author": "NoobMLDude",
          "text": "Start FREE FIRST.\n\nIf you have not tried it yet, get a taste of Local LLM on whichever device you have.\n\nOnly if you notice issues in the model you run should you put money for hardware. For most users they wonâ€™t notice a huge difference. \n\nFor OpenClaw you might need a big model.\nHowever there are some Local LLM pipelines you could try first which have some assistant features like OpenClaw:\n- [Personal Meeting Assistant](https://youtu.be/cveV7I7ewTA)\n- [Personal Talking Assistant](https://youtu.be/2VHzYy45kPw)\n- Tools for coders- Code Assistant, Terminal, Local API, etc\n\nTry out some of the above setups first as a way to check if you could utilize the expensive hardware.",
          "score": 5,
          "created_utc": "2026-02-10 19:19:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q9mgv",
          "author": "BetaOp9",
          "text": "People don't always realize clustering is good for parallel tasks, not single-stream inference. Splitting one model across multiple nodes means every layer boundary ships activations over the network. On 10GbE that's about 1.25 GB/s. Your nodes spend more time waiting on each other than doing math. You end up slower than a single fat node. \n\nThat said someone in this thread mentioned InfiniBand between two Strix Halo boxes and that actually offloads this bottleneck significantly. Used ConnectX-4/5 EDR cards run about $150-200 each and give you 100 Gbps, roughly 11 GB/s actual throughput with RDMA. That's 9x faster than 10GbE and makes tensor parallelism viable. 256GB unified memory across two nodes, 470B models at Q4, about $4,500 total. Interesting build. \n\nBut for a single $5K box? Mac Studio M4 Max. 128GB. $3,499.\n\n400 GB/s memory bandwidth, 128GB unified so no VRAM ceiling, silent, low power. Q3CN (80B MoE) would do roughly 40-60 tok/s. Most open models through 2026 will fit in 128GB quantized.\n\nThe RTX 5090 has insane bandwidth (about 1,800 GB/s) but only 32GB VRAM. Anything that fits in 32GB will scream. Anything that doesn't, and that's most of the interesting models, won't run at all without spilling to system RAM and tanking performance.\n\nThe M3 Ultra is last gen. Wait for the M4 Ultra if you want 256GB+ unified but that'll blow past $5K.\n\nFor context I'm running Q3CN on a Minisforum N5 Pro (Ryzen AI 9 HX PRO 370, 96GB DDR5, 16 RDNA 3.5 CUs) and hitting 18 tok/s on the iGPU via Vulkan at about 95 GB/s bandwidth. No discrete GPU. Token gen is bandwidth-bound so it scales roughly linear. A Strix Halo box at 212 GB/s would be around 40 tok/s. The M4 Max at 400 GB/s would push 75+.\n\nSave the $1,500 left over for when the M4 Ultra drops. Sell the Max and upgrade to 256GB at 800 GB/s. That's the real next step for local inference in a tight little box.",
          "score": 4,
          "created_utc": "2026-02-11 02:18:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mb83w",
          "author": "I_like_fragrances",
          "text": "At the moment the rtx 5090 is around 4-4.5k and a decent amount of ram bumps it up higher. For around $7k-$8k id do that. With $4-$5k id do a mac studio or dgx spark.",
          "score": 4,
          "created_utc": "2026-02-10 14:28:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mc2ox",
              "author": "eribob",
              "text": "I would do 4xRTX 3090 in an AM4 system",
              "score": 5,
              "created_utc": "2026-02-10 14:32:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4pen8q",
              "author": "Kaokien",
              "text": "I got a pre-built from cyber power for 4.2k so definitely doable, within the 5k range 64gb ram",
              "score": 1,
              "created_utc": "2026-02-10 23:17:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nyhya",
          "author": "Hector_Rvkp",
          "text": "A very clean option would be a mac studio with 128gb ram, 1 or 2tb SSD, and you max out the chip / bandwidth to fit your budget. I don't think 256gb ram makes sense on apple because the bandwidth is 1000gb/s max, which makes models 128+gb too slow to be useful. (Bandwidth/model size). But a good MoE model fitting inside 128gb would be very usable I believe.",
          "score": 3,
          "created_utc": "2026-02-10 19:07:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ojcmq",
              "author": "Lenz993",
              "text": "So you mean the Apple M4 Max (16-core CPU), 128 GB RAM, 1024 GB SSD?",
              "score": 2,
              "created_utc": "2026-02-10 20:44:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4p8f7j",
                  "author": "mikestanley",
                  "text": "Thatâ€™s what I bought last week. Setup changedetection.io and pointed it at the Apple Certified Refurb store. Bought that config 19 hours later. With the additional Veteran discount I paid $2825 before tax.",
                  "score": 3,
                  "created_utc": "2026-02-10 22:44:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4o6ckj",
          "author": "Lenz993",
          "text": "Most AI systems recommended the Mac M4 Max with 128 GB because it fits perfectly into the budget and can run large LLMs with 128 GB. That sounds logical to me. In addition, Apple's RAM is supposed to be particularly fast.",
          "score": 5,
          "created_utc": "2026-02-10 19:43:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4mkoka",
          "author": "mpw-linux",
          "text": "# what about a NVIDIA DGX Spark, 3,999.00 ? There was a review of it I saw on YT it looking pretty impressive, runs a version of LInux",
          "score": 9,
          "created_utc": "2026-02-10 15:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mtwsb",
              "author": "InfraScaler",
              "text": "You can get AMD AI MAX+ 395 with 128GB of unified RAM for half that... But also, someone else mentioned 4x3090 and that's probably the way to go to maximise your 5k",
              "score": 8,
              "created_utc": "2026-02-10 16:01:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4njthm",
                  "author": "Look_0ver_There",
                  "text": "Adding to your comment, there's some benchmarks here: [https://github.com/lhl/strix-halo-testing/](https://github.com/lhl/strix-halo-testing/)  \nThis guy compares the DGX Spark to the Strix Halo.  For Prompt Processing the Spark runs at about twice the speed of the Strix Halo, but for token generation the Spark falls back to just \\~10% faster.  This closely reflects both the compute speed differences (ARM vs AMD64) and the memory speed differences between the two machine.   At $3999 vs $2000, you'd want to mostly be doing prompt processing full-time to justify paying twice as much.",
                  "score": 3,
                  "created_utc": "2026-02-10 18:00:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4n8adp",
                  "author": "Expert_Mulberry9719",
                  "text": "be aware that the AMD ones can only use 50% of the memory for the GPU, Apple ones can use 75% of the memory for the GPU.",
                  "score": -7,
                  "created_utc": "2026-02-10 17:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nflsy",
              "author": "SpecialistNumerous17",
              "text": "Asus Ascent GX10 is a clone of the DGX Spark that is $1000 cheaper. I bought this and am very happy with it. The internals are mostly identical to the Spark, except for smaller disk, and so I just attached an external 2TB disk that I had already and it's been great so far.",
              "score": 3,
              "created_utc": "2026-02-10 17:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q0y75",
                  "author": "flyingbanana1234",
                  "text": "have you tried any video generation ?",
                  "score": 1,
                  "created_utc": "2026-02-11 01:26:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n5rtg",
              "author": "AppointmentAway3164",
              "text": "Yep Iâ€™m shopping these. Really great solution from nvidia.",
              "score": 2,
              "created_utc": "2026-02-10 16:55:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4no0e0",
              "author": "Financial-Source7453",
              "text": "$3k only for Asus Gx10. 1TB drive instead of 4TB. Copper radiator and plastic body instead of vapor chamber and metal foam case, but totally worth it.",
              "score": 2,
              "created_utc": "2026-02-10 18:19:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4r27hc",
              "author": "J673hdudg",
              "text": "I love my DGX Spark!! I do training runs (e.g. nanochat) as well as inference. Power usage is 90W max. The GB10 GPU does about \\~55 tok/s (over 1000 tok/s batched) for Qwen3-VL-30B and is my workhorse. Same Spark is running 3 other models and Flux for image gen. I still have a 2x3090 rig that can only do a fraction of that and uses 10x as much power.   \n  \nI also have a Strix Halo, but it came pre-installed with Windows that took a few hours to set up w/ a monitor (puke!) and haven't tried to re-image it with Linux yet. The Spark is Linux native and will setup headless out of the box. Amazing.",
              "score": 1,
              "created_utc": "2026-02-11 05:29:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n91h9",
          "author": "LSU_Tiger",
          "text": "Apple M4 Mac Studio Max with 128GB of ram.",
          "score": 5,
          "created_utc": "2026-02-10 17:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q138a",
              "author": "flyingbanana1234",
              "text": "this maybe the 256 gb option. I have an m4 128 gb ready for pickup but seriously debating waiting for the mac studio m5 for the higher bandwidth",
              "score": 1,
              "created_utc": "2026-02-11 01:26:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4p9a01",
          "author": "mikestanley",
          "text": "This is what I bought last week. Paid $2825 from Apple as certified refurb with veteran discount.\n\nhttps://preview.redd.it/4pd30mfixqig1.jpeg?width=1219&format=pjpg&auto=webp&s=7e094fd32183651e064d0b82aa1062d570fd2091\n\nIâ€™m still getting started but it can run my large models than my M1 Max 32GB and faster. And I wanted a new Mac anyway.",
          "score": 2,
          "created_utc": "2026-02-10 22:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q1kvb",
              "author": "flyingbanana1234",
              "text": "dang anytime i check it out they only have m1 and m2 in stock !!!",
              "score": 1,
              "created_utc": "2026-02-11 01:29:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q8qor",
                  "author": "mikestanley",
                  "text": "That's what happened to me for a week. The I setup [changedetection.io](http://changedetection.io) in a Docker container on my NAS and pointed it at the Apple Certified Refurb store. Bought that config 19 hours later. I left the query running for a few days - that config shows up regularly, it is just bought within 1-2 hours.",
                  "score": 2,
                  "created_utc": "2026-02-11 02:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n5kk5",
          "author": "AppointmentAway3164",
          "text": "Nvidia DGX Spark.",
          "score": 2,
          "created_utc": "2026-02-10 16:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mspx8",
          "author": "MimosaTen",
          "text": "I would think about 5/6.000 â‚¬",
          "score": 1,
          "created_utc": "2026-02-10 15:55:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n6k5u",
          "author": "esmurf",
          "text": "Gfx card alone 1-2000 usd. More if you have the money.",
          "score": 1,
          "created_utc": "2026-02-10 16:59:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n4m5x",
          "author": "fasti-au",
          "text": "Depend on uostres.  Naked you want qwen 30b with decent cintext as a bench but it was different when you had uostreamnipen ai etc not allowing agentvaccess  with subs",
          "score": 0,
          "created_utc": "2026-02-10 16:50:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4naja7",
          "author": "That-Cost-9483",
          "text": "No, the best thing for LLM solely would be to have the GPU/CPU/systemRAM/vRAM all tied together. Just one big soldered heap. This is kind of the direction apple has been going for awhile but obviously for AI you want cuda, so the spark is the best option. Other options would be to build a somethingâ€¦ you have quite a large budget. Iâ€™ve seen wild 3090 builds, these things are zip tied to racks and feeding into servers to utilize the enterprise ram and power supplies.\n\n5090 in a gaming PC is way more practical. We will have to see what bolt graphics (Zeus) brings usâ€¦ being able to add memory to a graphics processor is either the greatest thing that will ever happen or those guys are stealing everyoneâ€™s money. Then we will have to wait for nvida to steal their research and give us the RTX7095 vRAM 64gb extendable to 256 ðŸ™",
          "score": 0,
          "created_utc": "2026-02-10 17:17:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r05z64",
      "title": "Claude Code vs OpenCode vs Cline vs Qwen Coder",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r05z64/claude_code_vs_opencode_vs_cline_vs_qwen_coder/",
      "author": "ShowTimezz",
      "created_utc": "2026-02-09 14:42:28",
      "score": 28,
      "num_comments": 25,
      "upvote_ratio": 0.94,
      "text": "Hi all, \n\n  \nI'm curious if anyone knows of or if anyone here did a deep dive with different agentic AI tooling based on concrete, measurable metrics? \n\nFor example, let's say someone gave the same prompt to the same coding model, across several different tools. \"Create a flappy bird clone\" to GLM-4.7-Flash running in Claude Code, GLM running in OpenCode, GLM running in Cline, etc. Some more important prompts and requests could be: \"Here's an attached CSV file with unstructured data. Extrapolate meaningful entries in the file, and create a dashboard using NextJS that displays that data in a meaningful format\".\n\nI did try to find something along these lines, but the vast majority of the results have just been slop, comparing features of the tooling without actually going into a deep dive on how the tooling performs for concrete, measurable instructions, or just displaying the \n\nSomething like this could allow us to actually have a tooling comparison instead of a model comparison per se. \n\nMy gut is telling me that this analysis will have a surprising conclusion, with clear winners on what model compared with what tool provides the best results. i.e GLM4.7-flash works best when paired with Claude Code and worst when paired with Cline based on <metric that was used to evaluate all model/tooling combos>. Or, gpt-oss works best when paired with OpenCode and worst with Qwen Coder.\n\n  \nI'm open to doing the research myself, but was wondering if anyone before bothered doing something like this before I spend the time. \n\n  \nThank you",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r05z64/claude_code_vs_opencode_vs_cline_vs_qwen_coder/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4g44g8",
          "author": "see_spot_ruminate",
          "text": "Don't forget our baguette shaped, cheese dealing partners with mistral-vibe. Maybe not what most people use, but tool calls work well with all the models I put with it.\n\nedit (a joke), can it really be called \"vibe\" coding if it is not from the vibe region in france?",
          "score": 10,
          "created_utc": "2026-02-09 15:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gifub",
              "author": "cmndr_spanky",
              "text": "And Cursor (which is pretty great in my experience).",
              "score": 1,
              "created_utc": "2026-02-09 16:43:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gcxyw",
          "author": "qubridInc",
          "text": "Thereâ€™s no serious, metric-driven comparison of **agentic tools** yet and most write-ups compare features, not outcomes. Tooling behavior (planning loops, retries, permissions, context handling) often matters more than the model itself, so the same model can perform very differently across tools. Your intuition is likely right, but this gap is still largely unexplored.",
          "score": 4,
          "created_utc": "2026-02-09 16:17:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gi17m",
              "author": "cmndr_spanky",
              "text": "If thatâ€™s true, what a missed opportunity. Are you telling me with all the hapless and desperate content creators scrambling to remain relevant, with all the noise they are vomiting out to social mediaâ€¦ nobody has through to compare coding agent wrappers for the same model??\n\nWhat an idiotic missed opportunity. \n\nFWIW I agree with op that this is a critical thing. Itâ€™s literally the only reason Cursor (to pick my go-to tool) would even hope to compete or have any relevance.  They should (in theory) have some secret sauce that manages code base and context better than opus being used inside something free like VSCode + Roocode for example.",
              "score": 1,
              "created_utc": "2026-02-09 16:41:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fvpnm",
          "author": "calben99",
          "text": "This is a really interesting research idea! I haven't seen anyone do a systematic comparison like this across the same model with different tooling. I'd be curious to see the results if you end up running the tests. You might also want to track things like token usage and execution time as secondary metrics. Good luck with the research!",
          "score": 2,
          "created_utc": "2026-02-09 14:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g984e",
          "author": "Deep_Traffic_7873",
          "text": "I have a private benchmark like this, but it don't do it on full projects, just questions i can measure",
          "score": 2,
          "created_utc": "2026-02-09 15:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gz0q2",
          "author": "i-eat-kittens",
          "text": "Qwen Coder is their model. The tool name doesn't have an 'r'.",
          "score": 2,
          "created_utc": "2026-02-09 18:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i5iub",
          "author": "Crafty-Diver-6948",
          "text": "you are forgetting pi.\n\npi is better than all of them.",
          "score": 2,
          "created_utc": "2026-02-09 21:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fw0sy",
          "author": "Total-Context64",
          "text": "I'd love to have [CLIO](https://github.com/SyntheticAutonomicMind/CLIO) grouped into this if that's interesting to you, it's a small project with very few users but I'd still like to know how it stacks up.  No worries if it isn't interesting enough, figured it never hurts to try.  :D",
          "score": 1,
          "created_utc": "2026-02-09 14:54:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fy7kd",
          "author": "xLRGx",
          "text": "Probably not. Most everyone prefers the slight intelligence boost of the frontier models to power their coding agents and then muscle memory + the UX just brings us to pair them with each other. Most people using Claude code are just using opus. \n\nItâ€™d be valuable research but Iâ€™m not sure how useful it would really be in the AI landscape. It would deprecate quickly due to the pace of change the big 3 are shipping new models. By the time you finish your first rigorous benchmark openAI, Google, and Anthropic will probably have a new model ready to be launched. \n\nI would add a caveat, I think youâ€™re burying the lead a little bit or maybe you havenâ€™t considered it - the memory costs of AI and AI token generation of enterprise and consumers is going to likely 100x in 2026 compared to just last year. Thatâ€™s a real problem and finding efficiency gains like this might matter more than you think. Right itâ€™s not just humans increasing the cost of AI now itâ€™s Agents who can out produce humans in terms of pure token generation with workflows. It could honestly be 1000x in the aggregate. The real question becomes how do we train the Agents to operate in the â€œefficient frontierâ€ of 2026 and in the future. And it seems like the big 3 are aware thatâ€™s what needs to happen itâ€™s just that they CANT STOP SCALING even if itâ€™s looking flat now.",
          "score": 1,
          "created_utc": "2026-02-09 15:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g9gxm",
          "author": "Corosus",
          "text": "I'm confused why this isn't already being done by other people publicly, thats essentially what I do for when I'm trying to figure out good agent/model combos that aren't broken out of the box or just unusably slow.\n\n> \"Here's an attached CSV file with unstructured data. Extrapolate meaningful entries in the file, and create a dashboard using NextJS that displays that data in a meaningful format\".\n\nLiterally what I've been doing too but with a dump of discord message payload json objects.\n\nIf we could have a matrix comparison with agents on the top, and models on the left, where you can get a gist of the thuroughness of their solution + being able to quickly browse the results it'd be amazing.\n\nI guess where it gets a bit more complicated is what quants do you use, I guess try popular ones, ones at various VRAM/RAM tiers. And then on top of that the params you feed into llama.cpp etc. I guess again cherry picking common sensible usage might help there.\n\nI guess like others have said the info would deprecate quickly, the space moves so fast theres not much time for long standing data, so maybe people don't find it worth doing.",
          "score": 1,
          "created_utc": "2026-02-09 16:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gjusi",
              "author": "cmndr_spanky",
              "text": "Slapping a dashboard ontop of a data structure is such a pedestrian task these days, I donâ€™t think this will be an affective test. \n\nThe secret sauce of these different agentic wrappers is in how efficiently they use context through multiple loops of reasoning, and they index and manage / read a large code base, and the â€œbag of tricksâ€ / bonus features like rules, skills, MCP access, built-in AI controllable browser etc.\n\nYouâ€™d be better off finding a sizable code base on GitHub, prompt the agent to understand it well enough to implement feature x and validate its working correctly.  And again, avoid simple â€œdashboard on dataâ€ use cases.. itâ€™s too common, too small and too easy",
              "score": 2,
              "created_utc": "2026-02-09 16:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h1f78",
                  "author": "Corosus",
                  "text": "Agreed yeah, I guess a couple reasons why I am using the test case I mentioned is because \n\n1. It was part of something I already had claude setup for me (viewing a feed of as many discord channels I want on a single page you can look at without having to navigate around to each channel)\n\n2. It doesn't take a very long time to find out if the results of the test case I am trying is total garbage or not, even that simple task it can range from 5 minutes to 30 minutes depending on context size/model distribution in ram/vram. So in a way its main purpose is an initial filter to see if it's worth testing further at all\n\nMy next step after that was basically what you said, got a lot of work that involves making use of a specific API that isn't used widely and seeing if it can implement it correctly.",
                  "score": 1,
                  "created_utc": "2026-02-09 18:13:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gfovi",
          "author": "MrMisterShin",
          "text": "Qwen3-coder-next did something similar in there tech report (look for the scaffolding table).  https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf?spm=a2ty_o06.30285417.0.0.3bdec921sAzKJK&file=qwen3_coder_next_tech_report.pdf",
          "score": 1,
          "created_utc": "2026-02-09 16:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gj0zz",
              "author": "cmndr_spanky",
              "text": "You completely missed the point. He wants to compare the quality of different agentic wrappers for the same model.  Nobody needs another dumb benchmark comparing the LLMs",
              "score": 1,
              "created_utc": "2026-02-09 16:46:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gkv6m",
                  "author": "MrMisterShin",
                  "text": "Thatâ€™s in the paper if you READ it. It literally measures the same model across various scaffolding. It also measure many models against them also.\n\nThe only thing it doesnâ€™t do is explicitly detail which scaffolding achieved the highest marks for the models.\n\nhttps://preview.redd.it/tjhxjiv81iig1.jpeg?width=1027&format=pjpg&auto=webp&s=7b2a4e50dff2f2821fa86fad92f34946fa8c603e",
                  "score": 3,
                  "created_utc": "2026-02-09 16:55:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l5tlh",
          "author": "Charming_Support726",
          "text": "My Opinion is: (There are rarely real benchmarks out there)\n\n1 . Overall it simply doesn't matter at large. They are human interfaces.\n\n2. Harness and Scaffold don't have large impact on the model - But depending on the infrastructure they can make models perform only worse, never better.\n\n3. Many of these scaffolds have got terrible prompts (T - E - R - R - I - B - L - E). E.g look at the discussion in Opencode about the codex prompt - and their new codex prompt when OpenAI entered the game. To many contradictionary hints, restriction, example and so on. And far to long.\n\n4. Either prompts or tools will degrade performance.\n\nIf that is more or less o.k. you could switch scaffold without notice. I am using multiple of them mostly Zed, Opencode and Codex ( sometimes Antigravity ). I manage the access to the models using CLIProxy - so every harness gets the same provider chain. I feel no difference .",
          "score": 1,
          "created_utc": "2026-02-10 09:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lyscw",
          "author": "techlatest_net",
          "text": "No comprehensive head-to-head benchmarks exist yet comparing those exact tool+model combos on concrete tasks like Flappy Bird or CSVâ†’NextJS dashboard. \n\n**Gut ranking from scattered tests**:\n- **Claude Code + Sonnet 4.5/Opus**: Still king for complex reasoning+autonomy. Best at \"think â†’ plan â†’ code â†’ test â†’ fix\" loops on unfamiliar stacks. SWE-bench leader.\n- **OpenCode + GLM-4.7**: Closest open-source contender. 73% SWE-bench (vs Claude's 72%) + zero lock-in. Great for multi-model routing.\n- **Cline**: Solid but tool-heavyâ€”shines on file ops, weaker on pure generation. Agent scaffolding overhead kills simple prompts.\n- **Qwen Coder**: Fastest raw speed, surprisingly good UI/dashboard code, but needs tight scaffolding for full autonomy.\n\n**Surprise winner**: OpenCode+GLM-4.7 often beats Claude Code on price/performance for production pipelines. The tooling matters more than raw model for agentic flows.\n\n**Your test methodology is perfect**â€”same prompt/model across tools reveals the orchestration gap. I'd run:\n1. Flappy Bird (game logic + Canvas)\n2. CSVâ†’Recharts dashboard (data parsing + React)\n3. GitHub issue reproduction (multi-file edit)\n\nTrack: **task completion rate, edit iterations, token usage, wall-clock time**. \n\nDo itâ€”community needs this data desperately. My bet: Claude Code edges out, but OpenCode closes gap 80%.",
          "score": 1,
          "created_utc": "2026-02-10 13:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nl1gq",
          "author": "Lonely-Ad-3123",
          "text": "this is a really interesting question and I've been wondering the same thing about agentic tooling benchmarks. The reality is most comparisons are just feature checklists without actually measuring output quality or success rates on real tasks. If you're going to do this research yourself, here's how I'd approach it: 1.\n\n\nPick 3-5 concrete tasks with different complexity levels (the flappy bird clone is good for basic generation, the CSV dashboard is good for multi-file coordination, maybe add something like refactoring a legacy codebase or fixing a subtle bug across multiple repos) 2. Define your success metrics upfront. Not just does it run but things like: lines of code changed, build errors introduced, how many iterations needed, time to completion, whether it actually solves the stated problem vs going off track 3.\n\n\nFor the validation step, you might want to look at something like Zencoder Zenflow since it has spec-driven workflows with built-in verification loops that can measure drift between requirements and implementation. That could give you actual quantifiable data on how far each tool/model combo strays from the original prompt 4. Document everything in a spreadsheet with timestamps and iteration counts.\n\n\nScreenshots help too for when things go sideways 5. Run each combo at least 3 times since LLMs can be inconsistent The hardest part is gonna be keeping variables controlled. Same prompt wording, same model temp settings, same time of day even (API performance varies).\n\n\nBut yeah if you do this please share the results because the community desperately needs this kinda analysis instead of just vibes-based recommendations.",
          "score": 1,
          "created_utc": "2026-02-10 18:06:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gw0te",
          "author": "Ok_Rough5794",
          "text": "There are evals and benchmarks and youtubers doing this.. all over the place",
          "score": 0,
          "created_utc": "2026-02-09 17:48:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxun8z",
      "title": "How many mac mini are equal to 1 mac studio?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/snkxawxo0yhg1.jpeg",
      "author": "Far-Stretch5237",
      "created_utc": "2026-02-06 21:33:56",
      "score": 27,
      "num_comments": 21,
      "upvote_ratio": 0.7,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxun8z/how_many_mac_mini_are_equal_to_1_mac_studio/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3z5yvs",
          "author": "xXprayerwarrior69Xx",
          "text": "https://preview.redd.it/r7fnmpf33yhg1.jpeg?width=1290&format=pjpg&auto=webp&s=10b9e1e12ba7a91d88e5197b1c86094034d5b8c3\n\nHope this helps",
          "score": 39,
          "created_utc": "2026-02-06 21:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z644z",
              "author": "Far-Stretch5237",
              "text": "It doesn't work like that ðŸ¥´",
              "score": -32,
              "created_utc": "2026-02-06 21:48:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zjp7g",
                  "author": "yunarivay",
                  "text": "![gif](giphy|10hfegXGKVRVNm)",
                  "score": 11,
                  "created_utc": "2026-02-06 22:58:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3z7pzk",
          "author": "AnonymousCrayonEater",
          "text": "Telling us what you are trying to accomplish might help. At a basic level you can divide the ram of the studio over the mini, but i donâ€™t think that answers your question",
          "score": 12,
          "created_utc": "2026-02-06 21:56:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z81rp",
              "author": "Far-Stretch5237",
              "text": "I AM just curious \nA guy on twitter said he will get mac studio for his openclaw bot\n\nSo i thought....\n\nHow many mac mini are equal to 1 mac studio",
              "score": -11,
              "created_utc": "2026-02-06 21:57:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zaoyp",
                  "author": "AnonymousCrayonEater",
                  "text": "Based on your numbers you are comparing a maxed out studio to a base mini so:\n\n512 / 16 = 32 mac minis",
                  "score": 6,
                  "created_utc": "2026-02-06 22:11:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o48exgv",
                  "author": "TBT_TBT",
                  "text": "If this â€žguy on twitterâ€œ does not use local LLMs but API keys, it is completely irrelevant if this OpenClaw instance runs on a Studio or Mini. If used with local LLMs, no Mini setup will ever reach a Studio with a lot of ram.",
                  "score": 1,
                  "created_utc": "2026-02-08 10:46:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4174m0",
          "author": "Such_Advantage_6949",
          "text": "How many cars is equal to one plane",
          "score": 9,
          "created_utc": "2026-02-07 05:11:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o442wu5",
              "author": "nanotothemoon",
              "text": "Mostly depends on the turn signals, fuselage, and whether they have full size spares or not",
              "score": 2,
              "created_utc": "2026-02-07 17:36:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zf8vt",
          "author": "Momo--Sama",
          "text": "You'd also have to price in the equipment to connect all of these Minis together to work in tandem (that's an option, right?). as opposed to an independent plug and play Mac Studio but I'm not knowledgeable enough to expand on that.",
          "score": 8,
          "created_utc": "2026-02-06 22:34:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zibie",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-02-06 22:51:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zk600",
                  "author": "Buddhabelli",
                  "text": "thunderbolt5. exo.",
                  "score": 3,
                  "created_utc": "2026-02-06 23:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o407vpt",
          "author": "deeddy",
          "text": "42.",
          "score": 4,
          "created_utc": "2026-02-07 01:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40n0hk",
          "author": "SebastianOpp",
          "text": "At least tree fiddy",
          "score": 4,
          "created_utc": "2026-02-07 02:52:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40u5zm",
          "author": "jwrunge",
          "text": "About 3 and a half to 5, depending on if you are keeping the rigid structure or allowing for redistribution of volume (eg melting one down)",
          "score": 3,
          "created_utc": "2026-02-07 03:39:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o406pd1",
          "author": "TimLikesAI",
          "text": "About five by volume from the looks of it.",
          "score": 2,
          "created_utc": "2026-02-07 01:12:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvtxno",
      "title": "Qwen3-Coder-Next GGUFs updated with major bug fixes.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/yoeghkey7ihg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-04 16:56:06",
      "score": 27,
      "num_comments": 1,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qvtxno/qwen3codernext_ggufs_updated_with_major_bug_fixes/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lmkyx",
          "author": "Egoz3ntrum",
          "text": "Has anybody calculated the GGUF perplexity for this model?",
          "score": 2,
          "created_utc": "2026-02-04 21:05:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwg2uk",
      "title": "Which local model to use on MacBook with M4 pro and 48GB RAM",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qwg2uk/which_local_model_to_use_on_macbook_with_m4_pro/",
      "author": "aniozen",
      "created_utc": "2026-02-05 08:43:19",
      "score": 25,
      "num_comments": 33,
      "upvote_ratio": 0.93,
      "text": "Hey! \n\nI just got this MacBook Pro and I'm looking for a decent model to run on it.\n\nI'm a student so I would prefer a non-coding optimized model.\n\nQwen3 30b ran very very slow, almost unusable.\n\nSo I might be aiming to high. Or maybe there's a way to optimize it.\n\nIf you have any suggestions I would love to hear!\n\nThanks in advance",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qwg2uk/which_local_model_to_use_on_macbook_with_m4_pro/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3p9q2j",
          "author": "grandnoliv",
          "text": "I have a Mac mini M4 pro with 48GB so exact same capabilities. I use Ollama (I did not try MLX optimised models yet, I think it needs LMStudio).\n\nMy favorite models are :  \nâ€¢ Qwen3-coder:30b-a3b (I get around 60 tokens/s)  \nâ€¢ Qwen3:30b-a3b (\\~60 t/s with a thinking step)  \nâ€¢ GML-4.7-flash (\\~40 t/s with a long thinking step)  \nâ€¢ GPT-OSS:20b (\\~45 t/s, with thinking step)  \nThese are fast because they are MoE with only a few active parameters.  \nFor comparison, I like Mistral dense models but it's slow and the context window needs more RAM:  \nâ€¢ Magistral:24b (\\~16 t/s)\n\nT/s is not everything if the model can waste its time thinking. But thinking gives smarter results, especially if you configure a larger context size (I use around 30k context size if possible).  \n  \nAsking to build a one page html/js to roll dices as a test:  \nâ€¢ Qwen3-coder:30b-a3b â€” total duration: 21.2s (No thinking)  \nâ€¢ Qwen3:30b-a3b â€” total duration: 1m7s (Thinking step before the code)  \nâ€¢ GML-4.7-flash â€” total duration: 2m27s (Long thinking, usually smart results)  \nâ€¢ GPT-OSS:20b â€” total duration: 28s (With thinking step)  \nâ€¢ Magistral:24b â€” total duration: 5m0s (Very long thinking)\n\nJust a quick opinion on the quality of output (code execution + description output)  \nâ€¢ Qwen3-coder:30b-a3b â€” Works great, pretty and 2D animated (it output only the code)  \nâ€¢ Qwen3:30b-a3b â€” Works but with UI problems needing fixes, no animation (+ features description and details about the choices made)  \nâ€¢ GML-4.7-flash â€” Most ambitious result (3D animated) but it doesn't work out of the boxâ€¦ clean UI (+ one line description)  \nâ€¢ GPT-OSS:20b â€” Very minimalistic and functional, works great, does not try to be beautiful (+ quick \"how it works\" description)  \nâ€¢ Magistral:24b â€” Very minimalistic and functional, works great, ugly (+ quick description)",
          "score": 25,
          "created_utc": "2026-02-05 11:48:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rn4fh",
              "author": "GlassAd7618",
              "text": "Nice comparison and data (tokens per second). Very helpful!",
              "score": 3,
              "created_utc": "2026-02-05 19:09:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3v8nh5",
              "author": "cmndr_spanky",
              "text": "I take it you run everything at q4 ? Some you might be fine at q8 and see a noticeable quality increase",
              "score": 1,
              "created_utc": "2026-02-06 08:05:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vg0pu",
                  "author": "grandnoliv",
                  "text": "I do use q4 for the models, I'm not sure q8+large context window would fit in the VRAMâ€¦ I'll test q8 but for now I mainly use local LLMs for quick chats and did not manage to get it to work consistently with a coding agent (my main goal) probably because of the limited context window",
                  "score": 1,
                  "created_utc": "2026-02-06 09:15:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oq8yq",
          "author": "former_farmer",
          "text": "Are you running MLK 4bit quant models?Â \nEDIT: Yeaaah I meant MLX.",
          "score": 3,
          "created_utc": "2026-02-05 08:50:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p8zam",
              "author": "Additional-Sun-6083",
              "text": "Pretty sure you mean MLX.Â ",
              "score": 5,
              "created_utc": "2026-02-05 11:43:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3tvq11",
              "author": "joe__n",
              "text": "I have a dream",
              "score": 2,
              "created_utc": "2026-02-06 02:10:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3tw9d3",
                  "author": "former_farmer",
                  "text": "![gif](giphy|dtGIRL0FDp6nnOPGb5)",
                  "score": 2,
                  "created_utc": "2026-02-06 02:13:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3oqeei",
              "author": "aniozen",
              "text": "Nope, I just tried qwen and had no luck. What MLK?",
              "score": 1,
              "created_utc": "2026-02-05 08:51:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3oqlmp",
                  "author": "former_farmer",
                  "text": "Mlk is a format optimized for apple chip. And 4 bits version means it's an optimized version for smaller computers. A bit less smart but higher speed.",
                  "score": 3,
                  "created_utc": "2026-02-05 08:53:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oqbyn",
          "author": "former_farmer",
          "text": "Btw this might helpÂ https://www.reddit.com/r/LocalLLaMA/comments/1l6v9eu/whats_the_best_local_llm_for_coding_i_can_run_on/",
          "score": 2,
          "created_utc": "2026-02-05 08:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3swvbm",
              "author": "pauljdavis",
              "text": "But 8 mos old",
              "score": 1,
              "created_utc": "2026-02-05 22:50:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3q8wmr",
          "author": "mpw-linux",
          "text": "What about: \"mlx-community/LFM2.5-1.2B-Thinking-8bit ?",
          "score": 2,
          "created_utc": "2026-02-05 15:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3tnkhd",
              "author": "logic-paradox",
              "text": "Itâ€™s ok for a small model. But if you are running apple silicon you can go with the 8B models. ministral 3 8B is my fav right now. Running on a M4 Mini 16GB",
              "score": 2,
              "created_utc": "2026-02-06 01:21:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uktwv",
                  "author": "mpw-linux",
                  "text": "sounds good. Are those models giving good responses? What  type of information are you getting from them via prompts? I could run those Models as well but my internet speed is slow for larger models to download. I am impressed by the LFM2.5.1 models, good information  on my prompts and takes little memory even though I have 32g of Ram on my M1 pro. on your minstral 3 8B are you using quantized versions ?",
                  "score": 1,
                  "created_utc": "2026-02-06 04:48:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3twi2n",
          "author": "joe__n",
          "text": "I assume you mean for chat/RAG? Get LM Studio and try these:\n\n\\- [https://lmstudio.ai/models/allenai/olmo-3-32b-think](https://lmstudio.ai/models/allenai/olmo-3-32b-think)\n\n\\- [https://lmstudio.ai/models/essentialai/rnj-1](https://lmstudio.ai/models/essentialai/rnj-1)\n\n\\- [https://lmstudio.ai/models/bytedance/seed-oss-36b](https://lmstudio.ai/models/bytedance/seed-oss-36b)\n\nAlso agree with the others suggesting Qwen3:30b-a3b and GPT-OSS:20b\n\nYou need to play with context settings as well.",
          "score": 2,
          "created_utc": "2026-02-06 02:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u3p7w",
          "author": "TheCh0rt",
          "text": "Try LM Studio. Is free, easier to use than Ollama and has an LLM browser/downloader/loader/server/chatbot inside it. I even connected it to VSCode and did some vibe coding on my M1 with 64GB of RAM and it was lightning fast. Iâ€™m sure yours will be faster.",
          "score": 2,
          "created_utc": "2026-02-06 02:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wrox4",
              "author": "Mysterious-Power-571",
              "text": "Which LLM did you use for your vibe coding and how was the code quality compared to the big name subscription LLMs?",
              "score": 1,
              "created_utc": "2026-02-06 14:49:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wy6oo",
                  "author": "TheCh0rt",
                  "text": "Nowhere near successful. You gotta do it with the big ones. They also arenâ€™t good at accessing the internet because theyâ€™re not so much built for it",
                  "score": 1,
                  "created_utc": "2026-02-06 15:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3peb46",
          "author": "Lux_mirawy_3904",
          "text": "He visto que amigos con MacBook y 48GB de RAM, emplean modelos mÃ¡s pequeÃ±os como LLaMA 2 7B o Mistral 7B. Son eficientes, rÃ¡pidos y perfectos para tareas generales como chat o redacciÃ³n. Como consejo, evita modelos grandes como Qwen 30B, que son lentos sin optimizaciÃ³n. Para optimizar el rendimiento, puedes cuantizar el modelo (usar 4-bit o 8-bit) y ejecutar frameworks como llama.cpp o GPT4All con soporte para MPS en Apple Silicon.",
          "score": 3,
          "created_utc": "2026-02-05 12:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3p8vbg",
          "author": "dsartori",
          "text": "That model you selected should be good for your hardware. Something isnâ€™t right.",
          "score": 1,
          "created_utc": "2026-02-05 11:42:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3prqhm",
          "author": "abubakkar_s",
          "text": "Checkout gpt-oss:20b",
          "score": 1,
          "created_utc": "2026-02-05 13:46:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ru8qy",
              "author": "ConspicuousSomething",
              "text": "Yep. I try other models, but keep coming back to this one.",
              "score": 2,
              "created_utc": "2026-02-05 19:42:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3v8p4k",
                  "author": "cmndr_spanky",
                  "text": "Why ? For what ?",
                  "score": 1,
                  "created_utc": "2026-02-06 08:05:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3qvfha",
          "author": "plscallmebyname",
          "text": "Use MLX based arch, 20-40 billion parameters, 8 bit or 16 bit quant whatever fits.",
          "score": 1,
          "created_utc": "2026-02-05 17:02:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1jdq3",
      "title": "My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r1jdq3/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/",
      "author": "BetaOp9",
      "created_utc": "2026-02-11 01:14:08",
      "score": 24,
      "num_comments": 19,
      "upvote_ratio": 0.81,
      "text": "I didn't want to buy two systems. That was the whole thing.\n\nI needed a NAS. I also wanted to mess around with local LLMs. And I really didn't want to explain to my wife why I needed a second box just to talk to a chatbot that sometimes hallucinates, I have my father-in-law for that. So when I was specing out my NAS build, I went a little heavier than most people would and crossed my fingers that the system could pull double duty down the road.\n\nHonestly? I was prepared to be wrong. Worst case I'd have an overpowered NAS that never breaks a sweat. I could live with that.\n\nBut it actually worked. And way better than I expected.\n\n**The Build**\n\n* Minisforum N5 Pro\n* AMD Ryzen AI 9 HX PRO 370 (12c/24t, 16 RDNA 3.5 CUs)\n* 96GB DDR5-5600 (2x 48GB SO-DIMMs)\n* 5x 26TB Seagate Exos in RAIDZ2 (\\~70TB usable)\n* 2x 1.92TB Samsung PM983 NVMe (ZFS metadata mirror)\n* TrueNAS SCALE\n\nDay to day it runs Jellyfin with VAAPI hardware transcoding, Sonarr, Radarr, Prowlarr, qBittorrent, FlareSolverr, Tailscale, and Dockge. It was already earning its keep before I ever touched LLM inference.\n\n**The Experiment**\n\nThe model is Qwen3-Coder-Next, 80 billion parameters, Mixture of Experts architecture with 3B active per token. I'm running the Q4\\_K\\_M quantization through llama.cpp with the Vulkan backend. Here's how it actually went:\n\n**3 tok/s** \\- First successful run. Vanilla llama.cpp and Qwen3-Coder-Next Q8 quantization, CPU-only inference. Technically working. Almost physically painful to watch. But it proved the model could run.\n\n**5 tok/s** \\- Moved to Q4\\_K\\_M quantization and started tuning. Okay. Nearly double the speed and still slow as hell...but maybe usable for an overnight code review job. Started to think maybe this hardware just won't cut it.\n\n**10 tok/s** \\- Ran across a note in a subreddit that someone got Vulkan offloading and doing 11 tok/s on similar hardware but when I tried it...I couldn't load the full model into VRAM despite having plenty of RAM. Interesting. I tried partial offload, 30 out of 49 layers to the iGPU. It worked. Now it actually felt usable but it didn't make sense that I had all this RAM and it wouldn't load all of the expert layers.\n\n**15 tok/s** \\- Then the dumb breakthrough. I discovered that `--no-mmap` was quietly destroying everything. On UMA architecture, where the CPU and GPU share the same physical RAM, that flag forces the model to be allocated twice into the same space. Once for the CPU, once for GPU-mapped memory, both pulling from the same DDR5 pool. I couldn't even load all 49 layers without OOM errors with that flag set. Dropped it. All 49 layers loaded cleanly. 46GB Vulkan buffer. No discrete GPU.\n\n**18 tok/s** \\- Still I wanted more. I enabled flash attention. An extra 3 tok/s, cut KV cache memory in half, and significantly boosted the context window.\n\n3 â†’ 5 â†’ 10 â†’ 15 â†’ 18. Each step was one discovery away from quitting. Glad I didn't.\n\n**Results (Flash Attention Enabled)**\n\n* Up to 18 tok/s text generation\n* 53.8 tok/s prompt processing\n* 50% less KV cache memory\n* Fully coherent output at any context length\n* All while Jellyfin was streaming to the living room for the kids\n\nCouldn't I just have bought a box purpose built for this? Yep. For reference, a Mac Mini M4 Pro with 64GB runs $2,299 and gets roughly 20-25 tok/s on the same model. Apple's soldered LPDDR5x gives it a real bandwidth advantage. But then it wouldn't run my media stack, store 70TB of data in RAIDZ2. I'm not trying to dunk on the Mac at all. Just saying I didn't have to buy one AND a NAS.\n\nWhich was the whole point.\n\nNo exotic kernel flags. No custom drivers. No ritual sacrifices. Vulkan just works on RDNA 3.5 under TrueNAS.\n\n**Still On the Table**\n\nI've barely scratched the surface on optimization, which is either exciting or dangerous depending on your relationship with optimizing. Speculative decoding could 2-3x effective speed. EXPO memory profiles might not even be enabled, meaning I could be leaving free bandwidth sitting at JEDEC defaults. Thread tuning, KV cache quantization, newer Vulkan backends with RDNA 3.5 optimizations landing regularly, UMA buffer experimentation, different quant formats.\n\nOn top of all that, the model wasn't even designed to run on standard transformer attention. It was built for DeltaNet, a linear attention mechanism that scales way better at long context. There's an active PR implementing it and we've been helping test and debug it. The fused kernel already hits 16 tok/s on a single CPU thread with perfect output, but there's a threading bug that breaks it at multiple cores. When that gets fixed and it can use all 12 cores plus Vulkan offloading, the headroom is significant. Especially for longer conversations where standard attention starts to choke.\n\n18 tok/s is where I am but I'm hopeful it's not where this tops out.\n\n**The Takeaway**\n\nI'm not saying everyone should overbuild their NAS for an LLM machine or that this was even a good idea. But if you're like me, enjoy tinkering and learning, and are already shopping for a NAS and you're curious about local LLMs, it might be worth considering specing a little higher if you can afford it and giving yourself the option. I didn't know if this would work when I bought the hardware, a lot of people said it wasn't worth the effort. I just didn't want to buy two systems if I didn't have to.\n\nTurns out I didn't have to. If you enjoyed the journey with me, leave a comment. If you think I'm an idiot, leave a comment. If you've already figured out what I'm doing wrong to get more tokens, definitely leave a comment.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r1jdq3/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4qdcjw",
          "author": "qwen_next_gguf_when",
          "text": "96GB DDR5 5600.",
          "score": 6,
          "created_utc": "2026-02-11 02:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r4kwt",
          "author": "Firestorm1820",
          "text": "Could you post the exact flags youâ€™re using with llama.cpp? Curious to see what I can get on my setup.",
          "score": 4,
          "created_utc": "2026-02-11 05:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qmaa3",
          "author": "HealthyCommunicat",
          "text": "Its cool to see that the HX chips are decently affordable and are being used everywhere. Weâ€™re boutta all be able to inference on our phones in a few years.",
          "score": 2,
          "created_utc": "2026-02-11 03:36:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rdh1y",
              "author": "cramyzarc",
              "text": "Try MNN Chat, works already quite nicely with small models",
              "score": 1,
              "created_utc": "2026-02-11 07:05:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qzx2h",
          "author": "leonbollerup",
          "text": "Try running gpt-oss-20b, set up a server.dev mcp search.. you will find that itâ€™s properly 2x as fast and with search it becomes quite smart",
          "score": 2,
          "created_utc": "2026-02-11 05:11:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r25f9",
              "author": "Icy_Distribution_361",
              "text": "Or possibly even the 120b with this amount of memory. From what I've read the token generation isn't much slower compared to 20b.",
              "score": 1,
              "created_utc": "2026-02-11 05:29:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4q435h",
          "author": "timbo2m",
          "text": "Good stuff! I did something similar yesterday. I have different hardware but maybe this will get a few extra tokens per second:\n\n\n--threads 16 --batch-size 512\n\n\nThat gave me 2 tokens per second extra. Small, but it means a lot when it's sub 20 tokens.  Of course, fix threads to 1 or 2 less cores than you have available for llama server. In my case flash attention didn't help so I left it off.\n\nAlso, I wonder if the 2 bit XL quant is accurate enough, it will probably give you 10 tokens per second extra over the 4. Play with your context size too, I find 64k is good middle ground.",
          "score": 1,
          "created_utc": "2026-02-11 01:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs7ou",
              "author": "Shoddy_Bed3240",
              "text": "Try running it with 8 threads and pin the process to the performance cores only using taskset.\n\n\n\nOn Intel CPUs, this makes a huge difference when you restrict the workload to P-cores instead of mixing P-cores and E-cores. I suspect it should work similarly on AMD systems with Zen 5 performance cores as well.\n\n\n\nItâ€™s definitely worth testing and comparing the results.",
              "score": 1,
              "created_utc": "2026-02-11 04:16:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4qvbu7",
                  "author": "timbo2m",
                  "text": "I'm actually running 2 combos with the same qwen coder next models - an old amd server with a couple of nvidia T4 Tesla cards, comparing with my own gaming machine which is intel i9/4090/32GB, my gaming machine is actually 10 tokens per second faster but it's nice to offload agents to the server. Ideally I can get some Openclaw thing happening when  I get some firewalls in place.",
                  "score": 1,
                  "created_utc": "2026-02-11 04:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4r4303",
              "author": "colin_colout",
              "text": " 768 batch size was the sweet spot on my 780m when i was maining it. \n\nit matches the number of shader cores and improves prompt processing significantly. \n\ni think 16 threads shouldn't be needed since it uses vulkan amd no cpu offload if you set GTT correctly in Linux",
              "score": 1,
              "created_utc": "2026-02-11 05:44:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qqttc",
          "author": "Shoddy_Bed3240",
          "text": "If you want to maximize performance, memory bandwidth is critical. Improving effective memory bandwidth can have a significant impact on overall speed, especially for large models.\n\nThe best tool Iâ€™ve found for measuring memory bandwidth is STREAM:  \n[https://github.com/jeffhammond/STREAM](https://github.com/jeffhammond/STREAM)\n\nYou can compile it with:\n\n    gcc -O3 -fopenmp -mcmodel=large -DSTREAM_ARRAY_SIZE=200000000 stream.c -o stream\n    \n\nThen run it to determine your maximum memory bandwidth:\n\n    export OMP_NUM_THREADS=<max_threads>\n    taskset -c <list_of_cores> ./stream\n    \n\nOn my system, I was able to reach around 96â€“97 MB/s, which allowed me to run a Q8 model at about 10 tokens per second.\n\nIf you're trying to optimize inference speed, itâ€™s definitely worth benchmarking your memory bandwidth first.",
          "score": 1,
          "created_utc": "2026-02-11 04:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r20gd",
              "author": "Icy_Distribution_361",
              "text": "And then you find your memory bandwidth, and then what?",
              "score": 1,
              "created_utc": "2026-02-11 05:28:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4r2f5v",
                  "author": "Shoddy_Bed3240",
                  "text": "Next, find a way to maximize your memory bandwidth. LLM is sensitive to memory bandwidthâ€”the more you have, the faster it can generate tokens.",
                  "score": 1,
                  "created_utc": "2026-02-11 05:31:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r3he1",
          "author": "EbbNorth7735",
          "text": "It's a 3B model. Speculative decoding likely wont get you much.",
          "score": 1,
          "created_utc": "2026-02-11 05:39:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qfs5y",
          "author": "UpbeatDraw2098",
          "text": "your a smart dude",
          "score": 1,
          "created_utc": "2026-02-11 02:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q50pi",
          "author": "greatwilt",
          "text": "Nice! This model works well with open claw too, if you can get the context size up above  200 K. Good luck in your optimization journey.",
          "score": 0,
          "created_utc": "2026-02-11 01:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qsixu",
              "author": "timbo2m",
              "text": "I was going to use Kimi K2.5 as the brain and outsource coding and heartbeats to my local qwen 3 coder next 80B. Are you suggesting the brain could be just the local LLM too?!",
              "score": 1,
              "created_utc": "2026-02-11 04:18:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4r6z5d",
          "author": "RiskyBizz216",
          "text": "Absolute beast of a setup, but I agree - a Mac Studio would have been a better value - higher throughput and you would have gotten access to MLX models and 0-day releases.",
          "score": 0,
          "created_utc": "2026-02-11 06:08:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvx0ra",
      "title": "Fully offline coding agent",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qvx0ra/fully_offline_coding_agent/",
      "author": "lichoniespi",
      "created_utc": "2026-02-04 18:44:41",
      "score": 22,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "Hey guys,\n\nI am using Ollama with OpenWebUI and it works just fine, i am able to load even big models, so no problems with that. The problem is that i got spoiled by paid coding agents and i would like to set up one of my own. \n\nMany IDE's support local coding agents but they kinda require you to authenticate anyway and have some subscription.\n\nWhat i need is a fully offline coding agent that will be able to read the repo, create files, modify files etc.\n\nI have tried Continue plugin but it kinda sucked. Aider was \"okish\", the plugin was pretty horrible, CLI is more or less ok, but thats not exactly what i am looking for. I have also tried to integrate tools like mcpo with ripgrep and server-filesystem but thats also does not work very well.\n\nSo, i wanted to ask what works for you guys in a fully offline env?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qvx0ra/fully_offline_coding_agent/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3lxea4",
          "author": "crunchyrawr",
          "text": "In VS Code, you can add ollama as a model provider (you need to open the chat, click on the model name, \"Mange Models...\" at the bottom, \"+ Add Models...\", select ollama, it'll pull the model list and let you enable them in the Model picker in the chat.\n\nPersonally, depending on your expectations, local models work great for back and forth chat, but in an agentic loop they tend to be slow compared to a cloud provider.\n\nMost CLI harnesses support ollama pretty well (and are configurable for most any local LLM runner):\n\n- [Claude Code](https://docs.ollama.com/integrations/claude-code) `ollama launch claude --model model-id`\n- [pi](https://shittycodingagent.ai) can be configured for local models, and I find local models tend to run well in it (Claude Code, they tend to run very... slow for me).\n- [opencode](https://opencode.ai) also can be configured for local models and tends to run well.\n- [crush](https://github.com/charmbracelet/crush) also is good for local, but I feel wastes a lot of space\n- [codex cli](https://docs.ollama.com/integrations/codex) supports ollama `codex --oss -m id`\n\nPersonally, I like pi and opencode best so far.",
          "score": 5,
          "created_utc": "2026-02-04 21:56:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ktyew",
          "author": "former_farmer",
          "text": "If you find the solution let me know... I'm trying everything.\n\nOpenCode. Cline with Cursor. Koo with Cursor. I get pretty bad results because there are always unexpected failures in the communication between these tools and the model.\n\nI have a regular MBP 2021 with 32 gb of ram so that might affect. But it should work well with models smaller than 20B. I don't care about speed. It just sometimes never works. These tools fail to communicate properly with the model. Or it might be LMStudio but I'm not sure that's the case. When I communicate directly it works well.",
          "score": 3,
          "created_utc": "2026-02-04 18:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3m4lmm",
              "author": "DenizOkcu",
              "text": "Try nemotron 3 nano. It is great at coding and can use tools no problem. If you are looking for a cli tool that truly embraces local models, try Nanocoder (disclaimer: I am a contributor)",
              "score": 4,
              "created_utc": "2026-02-04 22:32:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lmmeq",
              "author": "noctrex",
              "text": "Try the new Qwen3-Coder-Next that was released the other day. Use with the latest llama.cpp. It's quite more capable than the older ones.",
              "score": 3,
              "created_utc": "2026-02-04 21:05:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lprud",
                  "author": "StaysAwakeAllWeek",
                  "text": "It also won't fit in his RAM",
                  "score": 0,
                  "created_utc": "2026-02-04 21:20:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kxb2b",
              "author": "dsartori",
              "text": "A 32GB MBP should be OK. Strong tool calling capabilities are not easy to find in smaller models but Qwen3-Coder-30b should fit your hardware, run well, and work perfectly with coding tools like Cline (thatâ€™s the one I use).",
              "score": 1,
              "created_utc": "2026-02-04 19:05:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kxppk",
                  "author": "former_farmer",
                  "text": "Do you use compact prompt or something like that? I swear cline sucks in my computer. Do you use LMStudio or Ollama?",
                  "score": 1,
                  "created_utc": "2026-02-04 19:07:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kxkj7",
          "author": "dsartori",
          "text": "Cline offers [info](https://docs.cline.bot/running-models-locally/overview) on succeeding with local models on their tool.",
          "score": 3,
          "created_utc": "2026-02-04 19:06:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kyj41",
              "author": "former_farmer",
              "text": "Ohh thanks man. Why don't they show this after installing the addon? I'm finding out thanks to you. This explains a lot why my setup was failing.",
              "score": 3,
              "created_utc": "2026-02-04 19:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3q5evz",
          "author": "arv3do",
          "text": "I use Roo and Opencode but feel free to try a bit, i think the choice of the Model ist more important.\n\nThis is for unified memory so your vram+RAM, i can not recommend to use the Hard Drive because it tends to suffer.\n\nThis will age so bad so fast. Chinese new jear is comming.\nBut here are local coding model recomendations:\n\n12GB+ gpt-oss 20b\n\n20GB+ GLM-Flash\n\n36GB+ Hyper Nova 60b\n\n48GB+ Qwen3-Coder-NEXT\n\n68GB+ GPT-OSS 120b\n\n120GB+ Step-3.5\n\nEdit: Formating and:\nDon't use Ollama, use ik_llama for new Nvidia + RAM, llama.cpp for everthing that does share vram and ram and for vram only vllm/sglang.",
          "score": 3,
          "created_utc": "2026-02-05 14:59:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qudp5",
              "author": "lichoniespi",
              "text": "I have noticed that pretty much anything other than [https://ollama.com/mychen76/qwen3\\_cline\\_roocode:32b](https://ollama.com/mychen76/qwen3_cline_roocode:32b) struggle a lot with tools usage. GLM-4.7 flash, Qwen3-Coder-NEXT, even gpt-oss-120b all suffered from tools halucination and they were unable to read/modify any files at all, some of them even got stuck at creating their own todo list. Having system prompt did not help. I have tried opencode, mistral vibe, claude code, cline and aider. Does anyone else have a similar experience?",
              "score": 1,
              "created_utc": "2026-02-05 16:57:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3y1a6f",
                  "author": "arv3do",
                  "text": "Thats devinitiv a thing. I think OpenCode and the CLIs are a bit better in that. I copied the upper part of the Roo Documentation for write_to_file in the AGENT.MD, that helps for me. I think there are some better Tools out there but i like the management of the context via Orchestrator in Roo.",
                  "score": 1,
                  "created_utc": "2026-02-06 18:27:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pgilg",
          "author": "grandnoliv",
          "text": "I'm looking for the same thing!\n\nYesterday I tried VSCode's extension \"Kilo Code\" (because it's shown as a very used app on OpenRouter's statistics) connected to local Ollama, running on a 48GB RAM M4 Pro.\n\nMy tests a few months ago showed me the context window is key for these agents, so I configured Ollama with flash\\_attention and KV Cache Q8 so that I can configure my models to use a 32k tokens context window.\n\nI prompt it to build a simple web app for managing code-snippets (simple enough to build and compare different models, complex enough to inspect how it structures the code).\n\nFirst try: with GML-4.7-flash with 32k context: It built many files autonomously during 2 hours then stuck with an errorâ€¦ I could not find any solution (error msg: \"Ollama stream processing error: terminated\")\n\nSecond try: with Qwen3coder:30b-a3b: It built even more, I thought I finally got a solution, but then after some time I noticed it was stuck when condensing the contextâ€¦ There was no error but it was abnormally long. I quit all apps, adjusted the RAM/VRAM reserved space with Siliv (https://github.com/Sub-Soft/Siliv) and tried a 28k context instead of 32k (I suspected that with a full 32 context window, something was swapping and getting too slow). I seemed to work better but with Kilo Code having to condense context after every actionâ€¦ so not optimal again. After a while, it was stuck again condensing the context (instead of doing it in 1-2 minutes, it was still stuck after 20 minutes).\n\nI gave up.\n\nI'll try with GPT-OSS20b when I have the available time.\n\nBut Claude Code with Sonnet4.5 does the work in a few minutes soâ€¦ it's just for my curiosity, probably not worth itâ€¦ I still want to do it!",
          "score": 2,
          "created_utc": "2026-02-05 12:38:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lti4y",
          "author": "Professional_Mix2418",
          "text": "I havenâ€™t tried it yet but I am pretty sure that you can link Claude code to a local llm. I have tried this for Gitkraken to automate my git descriptions and pr etc. That works great, Iâ€™m exposing my Nvidia DGX via Tailscale and I have several models in ollama. So even when I am on the move I can still do that with my local AI across the tailscale network.",
          "score": 2,
          "created_utc": "2026-02-04 21:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3odb1m",
          "author": "elephanter",
          "text": "I use Claude Code via Claude Code Router. You can connect local models. Codex has the oss flag, so you can run it with ollama without additional tools.",
          "score": 1,
          "created_utc": "2026-02-05 06:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m0wde",
          "author": "ewaldbenes",
          "text": "I cannot recommend Pi enough:Â [https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent](https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent)\n\nYou can switch between models/providers in-sessionâ€“local models too.\n\nI'm working with it for about 2 weeks now and absolutely love it. I can't think of going back to Claude Code CLI, Codex or similar tools.\n\nHere's my experience in a nutshell:Â [https://ewaldbenes.com/en/blog/the-only-coding-agent-you-ll-ever-need](https://ewaldbenes.com/en/blog/the-only-coding-agent-you-ll-ever-need)",
          "score": 1,
          "created_utc": "2026-02-10 13:31:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qy9kg1",
      "title": "Best local model for Apple Silicon through MLX",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qy9kg1/best_local_model_for_apple_silicon_through_mlx/",
      "author": "PerpetualLicense",
      "created_utc": "2026-02-07 09:32:32",
      "score": 21,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "I am only a user, I am not an expert in AI. I use mostly Claude and I pay now for the Claude Max plan. Now that is a large amount of money in a year (>1000 USD) and I want to cancel that subscription. For this purpose I would like to use my MacBook Pro M4 Max/128 GB for running a good enough local LLM for Swift and Python coding and optionally learning German. Ideally it should also have web searching capabilities and it should store the context long term, but I don't know if that is possible. I have experimented with mlx and it seems that mlx supports only dense models, but again I am not sure. What would be the best current LLM for my setup and use case. Basically I am looking at an assistant which will help me in day to day activities which runs 100% locally.\n\nSorry if my post does not fit here, but I just could not find a better forum to ask, it seems reddit is the best when it comes to AI discussions  \nThanks!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qy9kg1/best_local_model_for_apple_silicon_through_mlx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o424645",
          "author": "BABA_yaaGa",
          "text": "For 128gb UM, qwen 3 coder next 80B",
          "score": 14,
          "created_utc": "2026-02-07 10:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45prnh",
              "author": "GreaseMonkey888",
              "text": "Even for 64GB!",
              "score": 2,
              "created_utc": "2026-02-07 22:44:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4288tp",
          "author": "bakawolf123",
          "text": "gptoss120b - via llama.cpp, it's not correctly supported on MLX, but is lightning fast on llama.cpp  \nfor running on MLX stack I guess latest Qwen3 Coder Next or GLM4.7 Flash/Air is worth to try  \n  \noverall having 2 engines is a good idea, as lately MLX was mostly focusing on distributed performance, which is for clustering macs over TB5 while llama recently added ngram drafting that further improves inference performance",
          "score": 7,
          "created_utc": "2026-02-07 10:54:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o446ly3",
              "author": "GCoderDCoder",
              "text": "I agree with nearly everything but my mlx gpt-oss120b seemed fine when I last tested. I used the lm studio mlx mxfp4 version with no issues in the 75-80t/s range on my m3 ultra. What do you mean about not correctly supported on mac? Genuinely curious since I usually use larger models on my mac studio and 3090s for my gpt-oss-120b so q4kxl for that...",
              "score": 3,
              "created_utc": "2026-02-07 17:54:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48glxa",
                  "author": "bakawolf123",
                  "text": "I've meant base mlx-lm library, LM studio has at least proper support for harmony format and tool parsing I guess.  \nHowever there's also big performance issue - at least on my M1Pro I'm getting up to 400 tps prefill and 37 tps token generation on llama.cpp, and only 245/33 on mlx running 4 bit gptoss20b. The difference of TG is kinda small, I wouldn't even notice it, but PP difference is huge for large agentic prompts.",
                  "score": 1,
                  "created_utc": "2026-02-08 11:02:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o424db5",
          "author": "East-Suggestion-8249",
          "text": "you can try all types of models by installing lm studio, I have an M4 pro with 48GB the model speed and context size is important to me so I am just settling with GPT OSS",
          "score": 1,
          "created_utc": "2026-02-07 10:17:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45s277",
          "author": "Useful_University574",
          "text": "Local modals are nowhere near claude",
          "score": 1,
          "created_utc": "2026-02-07 22:57:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47dtno",
              "author": "catplusplusok",
              "text": "There are also a lot of good cloud models cheaper than Claude, I never seem to run out of personal Gemini plan although I am using Google Antigravity quite heavily.",
              "score": 1,
              "created_utc": "2026-02-08 05:09:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o47orx4",
              "author": "PerpetualLicense",
              "text": "of course, I am looking for a good enough only local model",
              "score": 1,
              "created_utc": "2026-02-08 06:41:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qwg8an",
      "title": "Clawdbot / Moltbot â†’ Misguided Hype?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qwg8an/clawdbot_moltbot_misguided_hype/",
      "author": "chodtoo",
      "created_utc": "2026-02-05 08:53:08",
      "score": 18,
      "num_comments": 36,
      "upvote_ratio": 0.59,
      "text": "It's not the \"free personal AI assistant\" the hype suggests. You need multiple paid subscriptions before it does anything useful.\n\nI got excited about Moltbot (OpenClaw) after seeing it promoted as this amazing personal AI assistant you can run locally. \"Your own AI, your data, your servers!\" Sounds great, right?\n\nThen I actually tried to set it up. Here's what youÂ **actually need**Â to make it semi-functional:\n\n# 1.Â AI Model SubscriptionÂ ðŸ’°\n\nThe bot itself is just a shell. You need your own:\n\n* Anthropic API key (Claude)\n* OpenAI API key (GPT)\n* Google AI API key (Gemini)\n\nNone of these are free for real usage.\n\n# 2.Â Web Search APIÂ ðŸ’°\n\nWant your bot to search the web? You need aÂ **Brave Search API**Â subscription. Google Custom Search API is another option, also paid.\n\n# 3.Â Browser AutomationÂ ðŸ’°\n\nThe browser control feature requires Playwright setup, and if you want it to work reliably, you're looking at potential cloud hosting costs.\n\n# 4.Â Voice/TTS FeaturesÂ ðŸ’°\n\nWant it to talk back? You need anÂ **ElevenLabs API**Â subscription or OpenAI TTS credits.\n\n# The Reality\n\nBy the time you've subscribed to all these services, you're spending $50-100+/month for something that... does what exactly? Sends messages between Discord and an AI?\n\n# My Recommendation\n\nJust stick with what works:\n\n* **Coding:**Â GitHub Copilot in VS Code ($10/mo) or Cursor\n* **AI Research:**Â ChatGPT Plus ($20/mo) or Gemini Advanced ($20/mo)\n* **Image Creation:**Â Midjourney or DALL-E directly\n* **Automation:**Â n8n or [Make.com](http://Make.com) with proper integrations\n\nThese tools are polished, well-documented, and just work.\n\nMoltbot feels like a cool hobby project for developers who want to tinker, but the \"personal AI assistant\" marketing is misleading. You're essentially paying for 5+ services to build something worse than what already exists.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qwg8an/clawdbot_moltbot_misguided_hype/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3os7iy",
          "author": "No_Heron_8757",
          "text": "Iâ€™m just using the ChatGPT Plus plan Iâ€™d be paying for anyway as the main LLM and offload simpler cron jobs to my LM Studio end point running small local LLMs. It supports web search already, so donâ€™t need a discrete Web Search API. Browser automation just runs in the same VM I have OpenClaw in, and I just run a Kokoro endpoint on my PC for TTS, which runs good enough on any semi-modern GPU. STT with faster-whisper. ComfyUI for image gen.\n\nNot saying these are the absolute SOTA each but paying $0 extra for my OpenClaw bot to run. I do wish it worked better with a local LLM as the primary model but pretty happy with it so far.\n\nWorking on setting up a local only OpenClaw instance, but speed just isnâ€™t there for primary usage without very expensive hardware",
          "score": 12,
          "created_utc": "2026-02-05 09:09:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pavk0",
              "author": "Uninterested_Viewer",
              "text": "I'm also just using my existing Gemini subscription for now so no added costs to tinker with it. I have an RTX 6000 pro that I have on of my instances set up with and trying to find a model that can balance speed, tool calling accuracy, and overall smarts to act at that main openclaw orchestrator.. let's just say I haven't found it. \n\nThat's not a dig at what mid-sized (fit in 96gb vram) can do: openclaw is really designed around a frontier model that can pick up the slack for a lack of \"optimization\".. which is also not a dig at the project itself as a lot of the point is to push the boundaries of what bringing together a lot of what frontier models are only recently getting good at doing.",
              "score": 2,
              "created_utc": "2026-02-05 11:57:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3qmidr",
                  "author": "Fristender",
                  "text": "Have you tried GLM 4.7 Flash for the main model? I heard it's good for agentic stuff.",
                  "score": 1,
                  "created_utc": "2026-02-05 16:20:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3resqf",
              "author": "twack3r",
              "text": "How do you tie in your ChatGPT subscription? There is no API key that comes with it. What am I missing?",
              "score": 1,
              "created_utc": "2026-02-05 18:31:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uphif",
                  "author": "No_Heron_8757",
                  "text": "You just select OAuth in the OpenClaw configuration, it launches a browser window of a ChatGPT login and youâ€™re connected",
                  "score": 1,
                  "created_utc": "2026-02-06 05:22:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3sikja",
              "author": "ethereal_intellect",
              "text": "I'm using codex in termux on phone and I feel that gets me like 70% there at least (and ye I have the chatgpt sub anyway for the web app and search) . For the \"let it actually interact with people\" I honestly haven't been brave enough yet. Gboard glide typing is at like 80wpm for me after years on a phone and I prefer reading the output to tts anyway\n\nIs there anything else I'm missing? Except the let it interact with people/leads thing?",
              "score": 1,
              "created_utc": "2026-02-05 21:39:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pgjwc",
          "author": "KnifeFed",
          "text": "lol the AI that wrote this post recommends using DALL-E.",
          "score": 11,
          "created_utc": "2026-02-05 12:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ouqx2",
          "author": "Valuable-Fondant-241",
          "text": "The bot just calls some endpoints for LLM integration, it's simply false that you have to pay for a service because you definitely can self host your LLM, your TTS and so on (maybe the web search makes an exception).\n\nYMMV, of course, and a self hosted LLM doesn't have the power/speed of a datacenter, but saying that you MUST pay a subscription is not true.\n\nBut this is something that here, in this sub, is well known and accepted.\n\nIf you thought that this bot would have run on a potato, and especially integrates all the LLM inferences tasks, you missed some knowledge about the bot or AI.",
          "score": 9,
          "created_utc": "2026-02-05 09:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3tnpsy",
              "author": "Coachgazza",
              "text": "I have llama3.1 running locally. I installed moltbot in a docker container but i can't get it to talk to the local LLM, no matter what i do it try's to talk to claude-opus-4-5 AI like its hard wired.",
              "score": 1,
              "created_utc": "2026-02-06 01:22:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oz8o3",
              "author": "chodtoo",
              "text": "sure you can host your own LLM :) - I was just stating that if you want something useful ...",
              "score": -12,
              "created_utc": "2026-02-05 10:17:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oraju",
          "author": "blamestross",
          "text": "The first thing that stands out is that it isn't \"Local\" at all. Not more or less than any coding agent. It makes sense to talk about using local llms with it here, otherwise it seems off topic.",
          "score": 9,
          "created_utc": "2026-02-05 09:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3otojl",
          "author": "clayingmore",
          "text": "Honestly, I think you've missed out on all of the real sources of hype.\n\n1. We're in 'local' LLM, solving for cost-to-quality-and-quantity is basically the mission of this community. Running a low cost local model is free, especially if you use the zero marginal cost for the routine calls and big frontier models for development, reasoning, or high value communication. Also, the recommendation in the documentation is Claude Max at $100/month so that is realistically the benchmark for all-cloud use.\n\n2 and 3. You do not need Brave search API or playwright they are just one of the options. \n\n4. Mine implemented a free edge tts on its own within like 3-4 prompts. There are many other free TTS you can run locally and I probably will implement something real later. It is functional but there are much better out there.\n\nOverall you have missed the actual 'magic', it isn't the 'assistant' that makes OpenClaw feel new, it is ripping out the guardrails and watching it solve problems autonomously. The 'heartbeat' pattern which forms a cron/while loop in which the LLM essentially routinely strategizes to solve its own problems and takes initiative to solve things feels very different. I think we all could have imagined it, but now can sort of see in action. Agentic solutions through autonomous strategizing, long sequences of reasoning-act loops, followed by layers of verification and continuous improvement on itself. \n\nAnyways, I'd totally flip the conclusion to be honest. Coding should be a frontier model through CLI, in which OpenClaw can actually manifest this but I'd stick with either OpenCode or Claude Code. Definitely never a within IDE code assistant again after going to CLI systems, I don't need that ball and chain.",
          "score": 4,
          "created_utc": "2026-02-05 09:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ozitp",
              "author": "chodtoo",
              "text": "I clearly missed the \"magic\". perhaps it's realy days and we just need to wait and see how it evolves.",
              "score": -1,
              "created_utc": "2026-02-05 10:19:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3orwwb",
          "author": "Intrepid-Struggle964",
          "text": "Its a waste of energy !! Thats all it is talk about wasted resources",
          "score": 2,
          "created_utc": "2026-02-05 09:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ppnbo",
          "author": "Weird-Consequence366",
          "text": "A 3 page AI dissertation on why someoneâ€™s open source project they made available for free isnâ€™t for you, is peak Redditor",
          "score": 2,
          "created_utc": "2026-02-05 13:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rjt2t",
          "author": "blurredphotos",
          "text": "I believe this was the first of the new 'viral' ai-manipulated advertising campaigns. This is the first of many.",
          "score": 2,
          "created_utc": "2026-02-05 18:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ozkkf",
          "author": "neotorama",
          "text": "Itâ€™s a good product for security guys. Goldmine for keys, wallets and cards",
          "score": 3,
          "created_utc": "2026-02-05 10:20:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pw8h4",
          "author": "Direct_Major_1393",
          "text": "I can definetely tell that you asked ChatGPT to write this",
          "score": 1,
          "created_utc": "2026-02-05 14:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qe1kd",
              "author": "One_Conscious_Future",
              "text": "Yeah it helps fix spelling mistakes, pretty useful!",
              "score": 1,
              "created_utc": "2026-02-05 15:41:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3q32n2",
          "author": "AffectionateHoney992",
          "text": "Ya, build your own with this (https://github.com/systempromptio/systemprompt-template, disclaimer I'm the author).\n\nStill needs an API key but never spent more than 5$ a month, integrations can all be vibecoded in minutes.\n\nClawdbot is wrong because it puts expensive intielligence in the execution layer, what you need is a good local control plane.\n\nIf your technical check it out, happy to help...",
          "score": 1,
          "created_utc": "2026-02-05 14:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q38az",
          "author": "desexmachina",
          "text": "It seems like a waste, until it starts to do things you canâ€™t do otherwise, like model switch on the fly when you ask. Look up LM proxy for VS Code and use your copilot w/ it.",
          "score": 1,
          "created_utc": "2026-02-05 14:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qyytb",
          "author": "lukewhale",
          "text": "I have been using it with a local GPT-OSS-120B on strict halo and honestly itâ€™s not too bad so far but Iâ€™m only 24 hours in. Itâ€™s one shot everything I asked it to do.",
          "score": 1,
          "created_utc": "2026-02-05 17:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r6a1r",
          "author": "InfamousAd2011",
          "text": "Im just going to selfhost my own llm",
          "score": 1,
          "created_utc": "2026-02-05 17:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rpzne",
          "author": "lol-its-funny",
          "text": "I have this setup with Telegram, so I can message openclaw thru it. But it feels very chaotic? All chatter about say 3-4 open daily topics all go into the same single chat thread. How are you all keeping topics in their own lanes?",
          "score": 1,
          "created_utc": "2026-02-05 19:22:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sq06h",
          "author": "Evening-Order6343",
          "text": "Ah an AI slop post telling us an AI tool is badÂ ",
          "score": 1,
          "created_utc": "2026-02-05 22:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u4vqq",
          "author": "BradenRiggins",
          "text": "You can just say youâ€™re an idiot and save everyone time.",
          "score": 1,
          "created_utc": "2026-02-06 03:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40mh3v",
              "author": "chodtoo",
              "text": "No need to get personal. It was a legitimate question and a personal observation.",
              "score": 2,
              "created_utc": "2026-02-07 02:49:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o40nt61",
                  "author": "BradenRiggins",
                  "text": "If you donâ€™t make ignorant and sensationalist statements maybe people wonâ€™t think youâ€™re ignorant.",
                  "score": 1,
                  "created_utc": "2026-02-07 02:57:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3v5pkt",
          "author": "Woopidoodoo",
          "text": "It's about being agenic not a chatbot or code auto completer",
          "score": 1,
          "created_utc": "2026-02-06 07:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ozjyp",
          "author": "TBT_TBT",
          "text": "OpenClaw is the reason why people buy Mac minis now. These devices can be used with their shared (V)Ram to run larger and more powerful local models (>20 or 30B at least) to use them with OpenClaw.\n\nBy using these local models, you won't have most of the API costs you mentioned.",
          "score": 1,
          "created_utc": "2026-02-05 10:20:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p9onn",
              "author": "Uninterested_Viewer",
              "text": "My understanding is people are buying mac mini's for this because it's a simple, isolated environment for it- not because they're running local models on it. The tool calling that openclaw relies on to be useful is far too complex for anything in the 30b range to be accurate enough to be useful for. Not that people aren't experimenting in that direction and/or using local agents to hand off small tasks, but your \"main\" chats in openclaw really need a frontier model or close to it.",
              "score": 2,
              "created_utc": "2026-02-05 11:48:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p16t3",
          "author": "mattv8",
          "text": "I agree with all of OP's sentiments...\n\n\nFWIW I built a [self hostable RAG/MCP server](https://github.com/mattv8/ragtime)Â that lets you set up connections to servers via SSH and is fully compatible with VSCode/Copilot. So you can bring your copilot subscription to do just about everything Moltbot can do and save a ton in API costs... Albeit it's more developer focused though.",
          "score": 1,
          "created_utc": "2026-02-05 10:35:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pl0mh",
          "author": "atkr",
          "text": "skills issue",
          "score": 1,
          "created_utc": "2026-02-05 13:07:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}