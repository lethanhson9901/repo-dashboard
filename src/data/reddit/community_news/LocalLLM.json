{
  "metadata": {
    "last_updated": "2026-01-29 02:49:02",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 226,
    "file_size_bytes": 264699
  },
  "items": [
    {
      "id": "1qp880l",
      "title": "Finally We have the best agentic AI at home",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/4qklburem2gg1.jpeg",
      "author": "moks4tda",
      "created_utc": "2026-01-28 10:54:31",
      "score": 222,
      "num_comments": 74,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qp880l/finally_we_have_the_best_agentic_ai_at_home/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o270k87",
          "author": "Recent-Success-1520",
          "text": "If you can host Kimi 2.5 1T+ model at home then it tells me you have a really big home",
          "score": 139,
          "created_utc": "2026-01-28 11:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27dftb",
              "author": "HenkPoley",
              "text": "Apparently it‚Äôs a native 4 bit weights. So ‚Äúonly‚Äù 640 GB needed (according to Apple MLX dev).\n\nIt‚Äôs about $10k for 1 TB RAM. And then a whole server around that.",
              "score": 31,
              "created_utc": "2026-01-28 12:42:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27i5x0",
                  "author": "TechnicalGeologist99",
                  "text": "Sorry...you're going to run that model on RAM? You'll get approximately 0.00000005 tokens per second....also wouldn't the kv cache be like 2.5gb per 1000 tokens?",
                  "score": 20,
                  "created_utc": "2026-01-28 13:11:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27zrqj",
                  "author": "Conscious-Lobster60",
                  "text": "Page out to tape drives, it‚Äôll be fine!",
                  "score": 2,
                  "created_utc": "2026-01-28 14:44:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o271ff0",
          "author": "No_Conversation9561",
          "text": "not in my home",
          "score": 63,
          "created_utc": "2026-01-28 11:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27id97",
              "author": "gonxot",
              "text": "https://preview.redd.it/smbj92s3b3gg1.png?width=864&format=png&auto=webp&s=20f7fa29368ce98043f29374d70b032abee97ff3\n\nMaybe it's the same guy lol",
              "score": 29,
              "created_utc": "2026-01-28 13:12:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o272n51",
          "author": "rookan",
          "text": "yeah, my 16GB VRAM card can easily handle it /s",
          "score": 44,
          "created_utc": "2026-01-28 11:25:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c359c",
              "author": "ihatebadpe0ple",
              "text": "Bro, I have a Ryzen 5 5600G üò≠",
              "score": 1,
              "created_utc": "2026-01-29 02:08:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271uts",
          "author": "keypa_",
          "text": "\"at home\" we probably don't have the same home...and the same budget for hardware and electricity üí∏",
          "score": 25,
          "created_utc": "2026-01-28 11:18:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28pcqt",
              "author": "Proof_Scene_9281",
              "text": "Home with dedicated custom circuit¬†",
              "score": 4,
              "created_utc": "2026-01-28 16:38:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2784wj",
          "author": "RiskyBizz216",
          "text": "ragebait? or a troll post?",
          "score": 19,
          "created_utc": "2026-01-28 12:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27l2ep",
              "author": "shaolinmaru",
              "text": "Yes",
              "score": 6,
              "created_utc": "2026-01-28 13:28:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2728aq",
          "author": "macumazana",
          "text": "\"at home\"? who dafuq calls data center home?",
          "score": 12,
          "created_utc": "2026-01-28 11:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27ejbv",
              "author": "DasBlueEyedDevil",
              "text": "Data",
              "score": 18,
              "created_utc": "2026-01-28 12:49:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27gxem",
                  "author": "macumazana",
                  "text": "true",
                  "score": 5,
                  "created_utc": "2026-01-28 13:04:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29the1",
              "author": "Maleficent-Ad5999",
              "text": "Shouldn‚Äôt these technically be called compute-center as they don‚Äôt hoard or serve any data",
              "score": 1,
              "created_utc": "2026-01-28 19:32:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2797g6",
          "author": "butterninja",
          "text": "Go home, Jensen. You're drunk...",
          "score": 12,
          "created_utc": "2026-01-28 12:14:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275n7q",
          "author": "Soft_Examination1158",
          "text": "In my opinion, spending money unnecessarily is like\nbuying your\nfirst electric car or your first photovoltaic system.\nSystems that cost and operate for 10,000 euros today will run on 1,000 euros systems tomorrow.\nIn another 2-3 years, everything will change.",
          "score": 9,
          "created_utc": "2026-01-28 11:48:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27ku5d",
              "author": "jay3686",
              "text": "my solar system i bought 5 years ago would cost something like 50% more today in the US.  And that's not even taking into acccount loss of federal subsidy.  but I guess US is an outlier on solar cost.  \nSame with EVs.  US is weird.",
              "score": 5,
              "created_utc": "2026-01-28 13:26:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28g76m",
                  "author": "AndersonBlackstar",
                  "text": "Im American and can confirm this",
                  "score": 2,
                  "created_utc": "2026-01-28 15:59:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29p7e2",
                  "author": "Soft_Examination1158",
                  "text": "I'm Italian, but in general, technology changes over time in favor of costs.\nTelevisions, tablets, PCs, and so on, cannot compare to the performance/costs of 10\nyears ago.",
                  "score": 1,
                  "created_utc": "2026-01-28 19:13:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bhujn",
                  "author": "duplicati83",
                  "text": "I wonder if that orange skidmark and his fucken tariffs might have something to do with that?",
                  "score": 1,
                  "created_utc": "2026-01-29 00:13:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o271nwn",
          "author": "Visible_Football_852",
          "text": "\"at home\"",
          "score": 5,
          "created_utc": "2026-01-28 11:17:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272b0m",
          "author": "IntroductionSouth513",
          "text": "like how much memory u need for this",
          "score": 4,
          "created_utc": "2026-01-28 11:22:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272waz",
              "author": "LavishnessCautious37",
              "text": "ideally at least 512gb RAM and then depending on the generation speed you'd like an amount of VRAM approaching the same figure.",
              "score": 3,
              "created_utc": "2026-01-28 11:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2735xo",
                  "author": "IntroductionSouth513",
                  "text": "well fuck me then who has 512gb geez",
                  "score": 2,
                  "created_utc": "2026-01-28 11:29:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o274hgq",
          "author": "Tema_Art_7777",
          "text": "OP lives in a datacenter‚Ä¶",
          "score": 3,
          "created_utc": "2026-01-28 11:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27lmzj",
          "author": "pgrijpink",
          "text": "To be fair, Unsloth did show that larger models can withstand more aggressive quantisation better due to their redundancy. Their dynamic 1.58 bit version of R1 performed very well given its size was reduced by 80%. A similar setup for K2.5 would take roughly 200gb which is expensive but not unheard of. \n\nhttps://unsloth.ai/blog/deepseekr1-dynamic",
          "score": 3,
          "created_utc": "2026-01-28 13:31:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2728iz",
          "author": "ptear",
          "text": "Does it need a SMR to power at home?",
          "score": 2,
          "created_utc": "2026-01-28 11:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2744yt",
          "author": "Birdinhandandbush",
          "text": "Who's home is this",
          "score": 2,
          "created_utc": "2026-01-28 11:37:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o276ege",
          "author": "IngwiePhoenix",
          "text": "\"At home\"? Are you sure? XD The VRAM you'd need for that will destroy your wallet...",
          "score": 2,
          "created_utc": "2026-01-28 11:54:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o288v6l",
          "author": "ab2377",
          "text": "any mention of clawdbot should be banned. absolutely pathetic advertisement is going on of it here.",
          "score": 2,
          "created_utc": "2026-01-28 15:27:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28qcn9",
          "author": "P3rpetuallyC0nfused",
          "text": "Would the only viable option for running this on consumer gpu be 2x m3ultra 512gb clustered with exo?",
          "score": 2,
          "created_utc": "2026-01-28 16:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o273zli",
          "author": "tiffanytrashcan",
          "text": "When your home is simply an extension of RunPod. üòÇ",
          "score": 1,
          "created_utc": "2026-01-28 11:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27av42",
          "author": "trmnl_cmdr",
          "text": "We‚Äôve seen this show before, it doesn‚Äôt end the way you think it does.",
          "score": 1,
          "created_utc": "2026-01-28 12:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27fqu6",
          "author": "_VirtualCosmos_",
          "text": "52 fucking percent on Humanity Last Exam? is that really true?",
          "score": 1,
          "created_utc": "2026-01-28 12:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aw8r7",
              "author": "PerformanceRound7913",
              "text": "Benchmaxed",
              "score": 1,
              "created_utc": "2026-01-28 22:23:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2axliy",
                  "author": "_VirtualCosmos_",
                  "text": "Even if so, do you know how hard those tests are? Also only resolving only 50% of them would be pretty lame if trained directly on the set.",
                  "score": 1,
                  "created_utc": "2026-01-28 22:30:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27g59v",
          "author": "Atomzwieback",
          "text": "Must be ragebait",
          "score": 1,
          "created_utc": "2026-01-28 12:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27rzth",
          "author": "Available-Craft-5795",
          "text": "What kinda GPU do you own!?",
          "score": 1,
          "created_utc": "2026-01-28 14:04:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28j5kz",
          "author": "Separ0",
          "text": "How does Claude continue to cook so hard on coding no matter what. How did they train that thing.",
          "score": 1,
          "created_utc": "2026-01-28 16:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28j92i",
          "author": "Separ0",
          "text": "1 TPS",
          "score": 1,
          "created_utc": "2026-01-28 16:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kks4",
          "author": "hocuspocus4201",
          "text": "At home aka a datacenter",
          "score": 1,
          "created_utc": "2026-01-28 16:18:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rj8e",
          "author": "Witty_Mycologist_995",
          "text": "1T, you must have a really big home",
          "score": 1,
          "created_utc": "2026-01-28 16:48:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a21ov",
          "author": "-PM_ME_UR_SECRETS-",
          "text": "Whose home exactly",
          "score": 1,
          "created_utc": "2026-01-28 20:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aw3w8",
          "author": "PerformanceRound7913",
          "text": "Artificial Analysis is the most useless, for-profit and never aligned with real world",
          "score": 1,
          "created_utc": "2026-01-28 22:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bwi0x",
          "author": "abmateen",
          "text": "Is it good at OpenAI style tool calling? I tried it in Ollama cloud it didn't call any tool I attached to it.",
          "score": 1,
          "created_utc": "2026-01-29 01:31:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmrwxl",
      "title": "Clawdbot: the AI assistant that actually messages you first",
      "subreddit": "LocalLLM",
      "url": "https://jpcaparas.medium.com/clawdbot-the-5-month-ai-assistant-that-actually-messages-you-first-8b247ac850b8?sk=0d521efdf2ceafe37973887b57b168ba",
      "author": "jpcaparas",
      "created_utc": "2026-01-25 18:59:25",
      "score": 151,
      "num_comments": 133,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qmrwxl/clawdbot_the_ai_assistant_that_actually_messages/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1u9m94",
          "author": "inigid",
          "text": "Clawdbot opens up the potential for a massive supply-chain attack that can steal or destroy everything on your machine and network, harvest and exfiltrate emails, crypto wallets, bank account and credit card details, SSN numbers, personal information, ssh keys.\n\nThe Solana meme coin and hype pumps by countless influencers is also a massive red flag.\n\nStay TF away from it if you have any sense.",
          "score": 17,
          "created_utc": "2026-01-26 16:10:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ud0b5",
              "author": "kublaikhaann",
              "text": "stop making too much sense",
              "score": 6,
              "created_utc": "2026-01-26 16:25:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ul578",
                  "author": "inigid",
                  "text": "Ikr, the thing literally has auto-update for itself, and then there are the downloadable skills on top!\n\nIt's like you are asking for trouble!\n\nOh, I'm just going to install this with sudo privileges and connect it up to all my infrastructure and personal accounts.\n\nWhat could possibly go wrong!  Smh.\n\nhttps://preview.redd.it/0h7v5nrr5qfg1.jpeg?width=1581&format=pjpg&auto=webp&s=2566f1d6f9e2d37b127da3ee58bb1740e6990adc",
                  "score": 9,
                  "created_utc": "2026-01-26 16:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yi54i",
              "author": "ab2377",
              "text": "üíØ",
              "score": 2,
              "created_utc": "2026-01-27 04:20:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x3t7z",
              "author": "ParanoidBlueLobster",
              "text": "Bit of a conspiracy theorist are we?\n\nIt's an open-source tool https://github.com/clawdbot/clawdbot\n\nAnd you update it manually https://docs.clawd.bot/install/updating",
              "score": 3,
              "created_utc": "2026-01-26 23:44:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xbf37",
                  "author": "inigid",
                  "text": "[https://cybersecuritynews.com/clawdbot-chats-exposed/](https://cybersecuritynews.com/clawdbot-chats-exposed/)\n\n[https://socradar.io/blog/clawdbot-is-it-safe/](https://socradar.io/blog/clawdbot-is-it-safe/)\n\nhttps://preview.redd.it/4ls6ihradsfg1.jpeg?width=1280&format=pjpg&auto=webp&s=905eda3538cc276b330924c33d14e18022a95d40",
                  "score": 6,
                  "created_utc": "2026-01-27 00:23:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24um1r",
                  "author": "BernardoOne",
                  "text": "no conspiracy theory required, if you have an AI agent with full permissions on your PC and connected to the internet you're 100% vulnerable to prompt injection.",
                  "score": 1,
                  "created_utc": "2026-01-28 01:47:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o23yhf4",
              "author": "Brumotti",
              "text": "Blackbox AI fixes this.  \n  \n[https://x.com/mhtua/status/2015943960867340356](https://x.com/mhtua/status/2015943960867340356)",
              "score": 1,
              "created_utc": "2026-01-27 23:02:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24dp6w",
              "author": "PeakBrave8235",
              "text": "But but but John Gruber told me it was cool!!!!!! And everything he says is amazing so obviously you're wrong¬†\n\nSarcasm.¬†",
              "score": 1,
              "created_utc": "2026-01-28 00:19:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21bvsw",
              "author": "Jasmine_Demir",
              "text": "Probably depends on how much you have to lose as well, and how disruptive it would be for you to stop your entire life for a day or two while you perform triage. \n\nIf you're 22 with a $50,000 net worth it's a very different consequence than begin 55 with 3 children, $5,000,000 in available debt/assets, and a convoluted chain of responsibilities that would come to a halt once you incorporated an assistant at this layer of your life flow and it failed.\n\nThe ones jumping into this don't have much to lose, and/or have the time and experience to stay on top of security issues.",
              "score": 0,
              "created_utc": "2026-01-27 16:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pwp1d",
          "author": "mike7seven",
          "text": "Spent most of the day figuring out that you need to get your Claude Code key (Oauth) on another machine and paste it for use on whatever machine you are setting up Clawdbot on. \n\nFrom Ops tutorial: \n‚ÄúYou have two options. Claude OAuth (the easiest) or API keys (more control over costs). For beginners, OAuth with a Claude Pro subscription is the simplest path.‚Äù\n\nNow I‚Äôm over even playing around with this thing for the rest of the day. \n\nMy two cents: The MacOS app is nice but you have to build it from source. Also good luck getting app auth setup properly if you are using a remote machine.",
          "score": 11,
          "created_utc": "2026-01-25 23:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pxaxs",
              "author": "DependentNew4290",
              "text": "Is it that easy to set up one as a non-technical person???",
              "score": 2,
              "created_utc": "2026-01-25 23:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q1s1j",
                  "author": "mike7seven",
                  "text": "Depends on how you plan on setting it up. Using ChatGPT and Codex it worked immediately and beautifully. Claude was not that easy. For local follow OPs guide.",
                  "score": 2,
                  "created_utc": "2026-01-26 00:08:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20gkav",
                  "author": "brianlmerritt",
                  "text": "If you want to set it up on your personal or work computer, provide unlimited access to said computer and all of your email and messaging accounts plus whatever else is on there like online banking or company information, it is still not that easy.\n\nBut you will be putting your personal computer or company at risk - there are a lot of little add-ons that have \"extra\" code and abilities to log into your computer, access everything plus use your AI tokens and accounts.\n\nUnless you are happy setting this up securely (see the link at the end of the post) there is a very good chance it doesn't go well.    \n  \nAlso note that one email or message can come in and hijack the AI within the system even if you didn't load any dodgy utilities.\n\n",
                  "score": 1,
                  "created_utc": "2026-01-27 13:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20sk57",
              "author": "Manarj789",
              "text": "It took all of 5 minutes to do, they literally tell you the steps to take",
              "score": 0,
              "created_utc": "2026-01-27 14:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ux2ag",
              "author": "[deleted]",
              "text": "[removed]",
              "score": -4,
              "created_utc": "2026-01-26 17:52:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yqrif",
                  "author": "Winter-Editor-9230",
                  "text": "r/LocalLLM follows platform-wide Reddit Rules; crypto currency and self promotion.",
                  "score": 1,
                  "created_utc": "2026-01-27 05:17:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1s2o5c",
          "author": "Ashamed_Promise7726",
          "text": "Has anyone successfully connected a local llm on their machine to Clawdbot? I have a few different models already downloaded to my PC, but I cannot get Clawdbot to work with them, and it keeps wanting an api key for a usage based model.\n\nIs it possible to run Clawdbot 100% locally?",
          "score": 11,
          "created_utc": "2026-01-26 07:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s41bu",
              "author": "kinesivan",
              "text": "If you are hosting with LM Studio or Ollama, does not matter what API key you pass, any should work.",
              "score": 3,
              "created_utc": "2026-01-26 07:33:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s7zwo",
                  "author": "Yorn2",
                  "text": "No, I'm having the same issue, Local OpenAPI-compatible isn't even an option in the choices. Nor is Ollama.",
                  "score": 2,
                  "created_utc": "2026-01-26 08:07:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29mmhl",
                  "author": "ReaverKS",
                  "text": "Curious how successful/useful your local models are for clawdbot as opposed to openai or claude? I'm sure the cost savings is huge.",
                  "score": 1,
                  "created_utc": "2026-01-28 19:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1slfam",
              "author": "Everlier",
              "text": "I spent my entire weekend integrating Clawdbot into Harbor so that it's possible to set it up with a local LLM in a few commands (granted you have Harbor installed):\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot",
              "score": 3,
              "created_utc": "2026-01-26 10:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xkbhh",
                  "author": "Yorn2",
                  "text": "Harbor seems cool but I'm using MacOS and mlx_lm.server for running a local instance of Deepseek 3.2, not llama.cpp or ollama or vllm. Though I do have a non-mac AI box where I might use Harbor instead someday. Do you plan on adding MacOS support sometime?",
                  "score": 2,
                  "created_utc": "2026-01-27 01:09:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25kt3u",
                  "author": "yogabackhand",
                  "text": "First I‚Äôve heard of Harbor. Looks cool! I‚Äôll give it a try. It would be cool if there was a MacOS desktop app (for configuration) and iOS client app like Pigeon (would be nice to have a MacOS client app too).",
                  "score": 1,
                  "created_utc": "2026-01-28 04:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yju3k",
              "author": "Loud-Layer3329",
              "text": "Yes I have using Ollama. Haven't had much luck with the models, though. So far Qwen2.5:14b has been the best to use via the web interface, but it gives a lot of nonsense via Telegram.\n\nNote that you need to use a model that supports tools, and Clawdbot only permits models that have a context window of >16k. Clawdbot supports the openai endpoint so you can use the baseurl [http://x.x.x.x:11434/v1](http://x.x.x.x:11434/v1)",
              "score": 2,
              "created_utc": "2026-01-27 04:31:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1z9r9b",
              "author": "No_Box1288",
              "text": "Running it with MiniMax M2.1 works well. I set it up following this tutorial, pretty straightforward:\nhttps://x.com/MiniMax_AI/status/2014380057301811685\n\nAlso saw a coding plan link in the MiniMax Discord. There‚Äôs a discount + cashback if you use it:\nhttps://platform.minimax.io/subscribe/coding-plan?code=Cp84x9ex1L&source=link",
              "score": 2,
              "created_utc": "2026-01-27 07:50:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22ymy0",
                  "author": "JimmyDub010",
                  "text": "Lucky. you must have some kind of super computer",
                  "score": 1,
                  "created_utc": "2026-01-27 20:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25pgma",
              "author": "Frenchplay57",
              "text": "I tried all day without success with the help of gpt, gemini and Claude. Claude managed to get it connected but there is no response in the interface.¬†",
              "score": 2,
              "created_utc": "2026-01-28 04:39:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28gu3v",
                  "author": "Ashamed_Promise7726",
                  "text": "Same here....I cant get anything to work.",
                  "score": 1,
                  "created_utc": "2026-01-28 16:01:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1pa4kx",
          "author": "Acceptable_Home_",
          "text": "Might try soon, sounds good!",
          "score": 4,
          "created_utc": "2026-01-25 22:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pax6h",
              "author": "jpcaparas",
              "text": "thank you. apologies in advance if the guide isnt too technical. i just want people to get into it. pete the maintainer is also a cool follow on Twitter",
              "score": -2,
              "created_utc": "2026-01-25 22:05:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rogty",
          "author": "threathunter369",
          "text": "your take on this guys?\n\n[https://x.com/theonejvo/status/2015401219746128322](https://x.com/theonejvo/status/2015401219746128322)",
          "score": 5,
          "created_utc": "2026-01-26 05:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27l6b0",
              "author": "samuel79s",
              "text": "That's pretty concerning. I like the \"tailscale funnel\" trick that allows to put a service on the Internet so easily, but the reality is that as soon as you setup it you start getting \"knocks on the door\".\n\nI'm sure a lot of people think that url's like macmini.ts43343.ts.net are hardly discoverable but it's actually the opposite. TLS certificate logs shout their existence pretty openly with the added bias for unprofessionally run services in home devices.\n\nIt's going to be a slaughter.",
              "score": 1,
              "created_utc": "2026-01-28 13:28:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ryqsv",
              "author": "xak47d",
              "text": "That's for noobs who just discovered cloud hosting following the hype and don't properly secure their instances. That's very unrelated to the tool and its capabilities",
              "score": 1,
              "created_utc": "2026-01-26 06:50:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1whdbz",
              "author": "Double-Lavishness870",
              "text": "LLms are insecure per default. Any browsing, email reading, tweet reading can insert bad behavior. \n\nBest knowledge and advice see here: https://media.ccc.de/v/39c3-agentic-probllms-exploiting-ai-computer-use-and-coding-agents#t=1218\n\nIsolation is needed.",
              "score": 1,
              "created_utc": "2026-01-26 21:55:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1slmv3",
          "author": "Everlier",
          "text": "If you're like me and wanted to setup Clawdbot with a local LLM, check out this integration in Harbor:\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot\n\nIt also runs Clawdbot containerized, so that it won't break your host system if something will go wrong",
          "score": 6,
          "created_utc": "2026-01-26 10:12:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1umj0n",
          "author": "cmndr_spanky",
          "text": "From a security perspective, I have zero interest in this right now (LLM autonomy with access to my calendar etc). And the supposed benefits are minimal (I already get calendar reminders, I don‚Äôt need an LLM bot to ‚Äúextra‚Äù remind me). Beyond that‚Ä¶ all the other use cases are fine with direct access to normal chatGPT / Gemini etc ..\n\nAlso Apple with soon replace Siri with Gemini and I imagine that‚Äôs going to be a game changer in terms of iOS integration and proactive AI",
          "score": 5,
          "created_utc": "2026-01-26 17:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcz9x",
          "author": "TendiesOnlyPls",
          "text": "Ok... so has anyone given Clawdbot access to their dating apps and asked it to go wrangle up some consenting ass? Really want to see how well these LLMs are able to spit game.",
          "score": 3,
          "created_utc": "2026-01-26 21:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ug8e0",
          "author": "Weird-Consequence366",
          "text": "Astroturf campaign",
          "score": 4,
          "created_utc": "2026-01-26 16:39:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sdrta",
          "author": "tomByrer",
          "text": "Is your article some where else than Medium please?",
          "score": 3,
          "created_utc": "2026-01-26 08:59:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ri8h1",
          "author": "No_Conversation9561",
          "text": "Entire X feed has been this lately",
          "score": 2,
          "created_utc": "2026-01-26 04:49:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o261w6z",
              "author": "Prestigious_Pace_108",
              "text": "That and crypto bros pushing along with Brave search integration gave me the message. I cancelled setup.",
              "score": 1,
              "created_utc": "2026-01-28 06:06:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rqpdf",
          "author": "Fast_Back_4332",
          "text": "How safe is it?",
          "score": 2,
          "created_utc": "2026-01-26 05:48:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ryvys",
              "author": "xak47d",
              "text": "It might delete all your files if you make it mad",
              "score": 6,
              "created_utc": "2026-01-26 06:51:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s9o8e",
                  "author": "Cressio",
                  "text": "Is that literally true? It seems like it would be but I haven't looked too deep into it. I don't think I *want* something with that deep of access to my stuff lol",
                  "score": 1,
                  "created_utc": "2026-01-26 08:22:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20w5zw",
                  "author": "jeremyckahn",
                  "text": "A hallmark of any great software",
                  "score": 1,
                  "created_utc": "2026-01-27 14:51:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o268zk4",
              "author": "leo-k7v",
              "text": "as safe as sudo anything in your terminal. OS does matter - you start by giving it admin privileges‚Ä¶ good luck",
              "score": 1,
              "created_utc": "2026-01-28 07:02:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1q8d1z",
          "author": "snam13",
          "text": "For once, I‚Äôm early! Set this up last week. \nDon‚Äôt expect miracles. If you‚Äôve used claude code or similar, it will feel like those but in your chat app. Be careful of burning lots of API tokens.",
          "score": 2,
          "created_utc": "2026-01-26 00:40:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qh1fv",
              "author": "ThenExtension9196",
              "text": "You may not be squeezing the lemon yet. There‚Äôs such insane amount of functionality you can get out of it by wiring it to gitlab/github and api services. I legit think this thing will be able to automated nearly my entire workday.",
              "score": 1,
              "created_utc": "2026-01-26 01:24:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rvyp3",
                  "author": "ketaminesuppository",
                  "text": "curious; what's your job?",
                  "score": 1,
                  "created_utc": "2026-01-26 06:28:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qzgz1",
          "author": "Artistic-Read-1097",
          "text": "I'm having a problem when I send a message in the GUI. Nothing happens I get no response. can anyone help if been at it all day",
          "score": 1,
          "created_utc": "2026-01-26 02:57:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25qxux",
              "author": "Frenchplay57",
              "text": "Are you on a local network? I have the same problem.¬†",
              "score": 1,
              "created_utc": "2026-01-28 04:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26vkkr",
                  "author": "No-Mess-8224",
                  "text": "same problem, its just showing \"typing...\" but no response",
                  "score": 1,
                  "created_utc": "2026-01-28 10:25:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rswkr",
          "author": "albany_shithole",
          "text": "How would it work for vLLM ?",
          "score": 1,
          "created_utc": "2026-01-26 06:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sat1j",
          "author": "redblood252",
          "text": "If I use this with local llama-cpp. Which model is best for it? Gpt-oss? Is 14Gb of vram enough for it?",
          "score": 1,
          "created_utc": "2026-01-26 08:32:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1slhhv",
              "author": "Everlier",
              "text": "GLM-4.7-Flash is the best right now, it really wants very strong agentic models",
              "score": 2,
              "created_utc": "2026-01-26 10:10:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sm5vs",
                  "author": "redblood252",
                  "text": "Even at IQ4_XS quantization it takes 16gb of vram. I need to test some cpu offloading and see if it is fast enough. And if the quantization isn‚Äôt too much.",
                  "score": 1,
                  "created_utc": "2026-01-26 10:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sfx42",
          "author": "Necessary_Function_3",
          "text": "Spent hours (and burned all my anthropic tokens for the day and I am paying $200 a month, but API tokens are extra it seems) and cannot get it to work with ollama, extremely frustrating",
          "score": 1,
          "created_utc": "2026-01-26 09:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1skfux",
              "author": "jamesftf",
              "text": "but what this does and what claude code cannot do?!",
              "score": 1,
              "created_utc": "2026-01-26 10:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u3ade",
                  "author": "MaaDoTaa",
                  "text": "It can message you (unsolicited)",
                  "score": 1,
                  "created_utc": "2026-01-26 15:43:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sms05",
          "author": "Separ0",
          "text": "Installed last night on an Ubuntu machine on hetznsr on my tailnet. Crons are not working",
          "score": 1,
          "created_utc": "2026-01-26 10:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vihfg",
          "author": "p_235615",
          "text": "Not sure why this is such a hype, but from what I played around with it in a VM and local llm (its quite PITA to setup), and one thing is for sure - its basically a security nightmare. \n\nYou have a single point rouge AI which sending all your data to cloud if not set up with local LLM and even with local LLM you never know when even accidentally it will send your private files somewhere to the web. Or if it gets somehow compromised by accessing some webpage or something, its like throwing all your files and accounts around. There are basically no security guard rails, and would never feed it some private or corporate accounts.",
          "score": 1,
          "created_utc": "2026-01-26 19:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vq2td",
          "author": "apola",
          "text": "I'm interested in playing around with clawdbot, but my understanding is that it chews through tokens like nobody's business. How useful can it be with the $20/month Claude Pro subscription? Would that give it enough tokens to be remotely useful?",
          "score": 1,
          "created_utc": "2026-01-26 19:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vqy8f",
              "author": "jpcaparas",
              "text": "I've given a bit of guidance on token allowances between Pro and Max plans here:\n\nhttps://jpcaparas.medium.com/why-your-expensive-claude-subscription-is-actually-a-steal-02f10893940c?sk=65a39127cbd10532ba642181ba41fb8a\n\nYou might find it useful.",
              "score": 1,
              "created_utc": "2026-01-26 19:58:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vycbq",
                  "author": "apola",
                  "text": "Interesting, thanks for the link. So, the $20/mo plan gives you \\~$259 worth of compute. My real question is: Is $259 of compute enough to make Clawdbot useful? I've seen videos where people seem to spend $150/day using clawdbot to do a few extremely basic things. Maybe my impression of what they're doing is wrong, but that would seem to suggest that the $20/mo subscription would get me \\~2 days of clawdbot usage per month.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1w32ib",
          "author": "Lonely-Elephant2130",
          "text": "It's impressive for sure, but man the setup barrier is real. Spent hours trying to get it running and couldn't figure it out (not a dev background). Currently using Super Intern (https://www.superintern.ai/ ) which is similar but actually simple - just chat interface, no setup. Way more my speed.",
          "score": 1,
          "created_utc": "2026-01-26 20:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yq4t3",
          "author": "BiggestSkrilla",
          "text": "need to do something about api cost.",
          "score": 1,
          "created_utc": "2026-01-27 05:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yvaaj",
          "author": "FutureboyWavo",
          "text": "Currently using clawdbot running on a VPs with minimax m2 model and I‚Äôm having so many issues.",
          "score": 1,
          "created_utc": "2026-01-27 05:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zcs6h",
          "author": "Different-Pizza-7591",
          "text": "I found an App to connect to Clawdbot with iOS ! Not perfect but it works and it is free!   \n[https://apps.apple.com/us/app/nuvik/id6747774937](https://apps.apple.com/us/app/nuvik/id6747774937)",
          "score": 1,
          "created_utc": "2026-01-27 08:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ziga3",
          "author": "Technical_Self_9996",
          "text": "I've created a a step-by-step guide for non-developers to get going with Clawdbot. No assumed knowledge. Just copy, paste, and follow along. Hope people find it useful: [https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide](https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide)",
          "score": 1,
          "created_utc": "2026-01-27 09:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o208zsm",
          "author": "NineOneOne119",
          "text": "I'm attempting to use the Quen3 models from the [Featherless.ai](http://Featherless.ai) API in Clawdbot, but it's unable to locate the model Quen3/Quen3-14B. Do you have any suggestions?",
          "score": 1,
          "created_utc": "2026-01-27 12:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22blg9",
          "author": "Thrombas",
          "text": "This is the \"DeepSeek 2024\" hype all over again lol",
          "score": 1,
          "created_utc": "2026-01-27 18:36:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23ylca",
          "author": "toyssamurai",
          "text": "This kind of tool has one problem. I would never trust it enough to connect my primary accounts to it. Ask any honest architects behind LLMs if they can guarantee their models will never hallucinate. They will all say no, at least not yet. If they can't make such a promise, why should I trust them with my entire online life?",
          "score": 1,
          "created_utc": "2026-01-27 23:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2455zg",
          "author": "wheredoesitsaythat",
          "text": "Has anyone built or suggest security guardrails.  If its open source, couldn't I make the code more secure?",
          "score": 1,
          "created_utc": "2026-01-27 23:36:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24i090",
          "author": "hungarianhc",
          "text": "So if I want to play with this, should I buy a Mac Mini? Or should I just install it on an LXC on my Proxmox server?",
          "score": 1,
          "created_utc": "2026-01-28 00:41:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o278ue8",
              "author": "kjelan",
              "text": "I am setting up Ubuntu desktop 24 as a VM on proxmox. And I am making sure the Proxmox firewall (outside of that VM) is allowing out traffic to the intern, but it is never allowed traffic into the local LAN. \n\nSo I can reach that machine, but it can not touch my local LAN. But I do give it internet access... Then see what happens. \n\nBut I think keeping local systems / door bell / whatever out of reach by default makes sense.",
              "score": 1,
              "created_utc": "2026-01-28 12:11:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25thtg",
          "author": "MaintenanceQueasy457",
          "text": "Thats really cool. I would love to hear more¬†",
          "score": 1,
          "created_utc": "2026-01-28 05:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o268ls4",
          "author": "leo-k7v",
          "text": "what was that day in The Terminator movie called when SkyNet became self aware? And broke free?",
          "score": 1,
          "created_utc": "2026-01-28 06:59:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a7580",
          "author": "georgekokorikos",
          "text": "It‚Äôs scary and super powerful !",
          "score": 1,
          "created_utc": "2026-01-28 20:33:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qgt9k",
          "author": "ThenExtension9196",
          "text": "I just got it running last night. \n\nIt‚Äôs literally another chatgpt4 moment. Tools like this are going to be huge this year.",
          "score": 1,
          "created_utc": "2026-01-26 01:23:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qi3wx",
              "author": "jpcaparas",
              "text": "yeah man personal ai is gonna explode this year. Tools like Poke and Clawdbot are gonna be big",
              "score": 2,
              "created_utc": "2026-01-26 01:30:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ux0vs",
          "author": "Professional-Cut7836",
          "text": "looking to get into the clawdbot hype? \n\nselling 1 time payment, 2 year gurantee #claude and #OpenAI api access. 0.017 Eth/50 USDT \n\nDm me!:)",
          "score": 0,
          "created_utc": "2026-01-26 17:51:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp0jhl",
      "title": "I used Clawdbot (now Moltbot) and here are some inconvenient truths",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qp0jhl/i_used_clawdbot_now_moltbot_and_here_are_some/",
      "author": "Andy18650",
      "created_utc": "2026-01-28 03:49:18",
      "score": 97,
      "num_comments": 67,
      "upvote_ratio": 0.92,
      "text": "Text wall warning :)\n\nI tried Clawdbot (before the name switch so I am going to keep using it) on a dedicated VPS and then a Raspberry Pi, both considered disposable instances with zero sensitive data. So I can say as a real user: The experience is awesome, but the project is terrible. The entire thing is very \\*very\\* vibe-coded and you can smell the code without even looking at it... \n\nI don't know how to describe it, but several giveaways are multiple instances of the same information (for example, model information is stored in both \\~/.clawdbot/clawdbot.json and \\~/.clawdbot/agents/main/agent/models.json. Same for authentication profiles), the /model command will allow you to select a invalid model (for example, I once entered anthropic/kimi-k2-0905-preview by accident and it just added that to the available model list and selected it. For those who don't know, Anthropic has their own Claude models and certainly doesn't host Moonshot's Kimi), and unless you run a good model (aka Claude Opus or Sonnet), it's going to break from time to time. \n\nI would not be surprised if this thing has 1000 CVEs in it. Yet judging by the speed of development, by the time those CVEs are discovered, the code base would have been refactored twice over, so that's security, I guess? (For reddit purposes this is a joke and security doesn't work that way and asking AI to refactor the code base doesn't magically remove vulnerabilities.)\n\nBy the way, did I mention it also burns tokens like a jet engine? I set up the thing and let it run for a while, and it cost me 8 MILLION TOKENS, on Claude-4.5-OPUS, the most expensive model I have ever paid for! But, on the flip side: I had NEVER set up any agentic workflow before. No LangChain, no MCP, nothing. Remember those 8 million tokens? With those tokens Claude \\*set itself up\\* and only asked for minimal information (such as API Keys) when necessary. Clawdbot is like an Apple product: when it runs it's like MAGIC, until it doesn't (for example, when you try to hook it up to kimi-k2-0905-preview non thinking, not even 1T parameters can handle this, thinking is a requirement).\n\nAlso, I am sure part of why smaller models don't work so well is probably due to how convoluted the command-line UI is, and how much it focuses on eyecandy instead of detailed information. So when it's the AI's turn to use it... Well it requires a big brain. I'm honestly shocked after looking at the architecture (which it seems to have none) that Claude Opus is able to set itself up.\n\nFinally, jokes and criticisms aside, using Clawdbot is the first time since the beginning of LLM that I genuinly feel like I'm talking to J.A.R.V.I.S. from Iron Man.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qp0jhl/i_used_clawdbot_now_moltbot_and_here_are_some/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o25voxs",
          "author": "Bananadite",
          "text": "My biggest issue is idk what to really use it for.",
          "score": 28,
          "created_utc": "2026-01-28 05:20:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26nopb",
              "author": "nickk024",
              "text": "I have the same ‚Äúissue‚Äù for a lot of projects in this space to be honest.",
              "score": 18,
              "created_utc": "2026-01-28 09:13:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27447b",
              "author": "Endflux",
              "text": "I think this is generally the biggest issue of AI altogether, in business environments too. \n\nInstead of having a problem and asking ‚Äòis an AI powered system is the most effective solution for this‚Äô, more often people are fascinated by AI and look for problems.\n\n(Which is fine when learning/exploring ofc.)",
              "score": 6,
              "created_utc": "2026-01-28 11:37:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27fmaq",
                  "author": "UpBeat2020",
                  "text": "The problem is both ways no creativity to use it and its not good enough to use it for the things we actually want to use it. \n\nofc everyone wants a proactive robot that can do stuff but nobody trust it to give it the keys to everything because its not safe and it hallucinates. \n\nThe problem is actually very simple because you know it can fuck up not intentionaly just because of its design you dont trust it.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:56:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o290vb7",
              "author": "BernardoOne",
              "text": "it's amazing how even the biggest glazers of this tool don't know either. Half the usecases they cite are basically \"hey generate me some worthless filler slop with no actua practical uses\", and the other half is automating shit that you could have done yourself in the time that took you to write the prompt, without wasting a small fortune in tokens in the process.",
              "score": 5,
              "created_utc": "2026-01-28 17:29:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a13hk",
                  "author": "Dadud300",
                  "text": "Staymad",
                  "score": -1,
                  "created_utc": "2026-01-28 20:06:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26zghf",
              "author": "Aggravating_Fun_7692",
              "text": "If it was a physical robot you could ask it to go grocery shopping for you",
              "score": 1,
              "created_utc": "2026-01-28 10:58:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2c3pnj",
                  "author": "sleepingthom",
                  "text": "And if my grandmother had wheels she‚Äôd be a bicycle üò¨",
                  "score": 1,
                  "created_utc": "2026-01-29 02:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o280fbw",
              "author": "dumbass_random",
              "text": "That's the story for most of the AI applications today",
              "score": 1,
              "created_utc": "2026-01-28 14:47:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o285og9",
              "author": "NotReallyJohnDoe",
              "text": "Also see: 3d printing",
              "score": 1,
              "created_utc": "2026-01-28 15:12:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28grhj",
              "author": "psychofanPLAYS",
              "text": "im planning / building (slowly üò©) a custom home ai assistant that will live on my pi 24/7 - use my clamshell razer laptop with 6gb vram or pc with 4090 depending what needs done on demand, either wake from lan or ill figure out a way to actually give it the ability to turn my pc on, and ill have it print my schedule every morning, send signed paperwork to my boss, track my daily miles I do at work, track trends, gather my store receipts - and go through my spam email to collect coupons or shit like that. its mostly though so it works as my accountant in a way, doing my invoices etc lol so I dont have to do the manual pc labor\n\nbut I think to achieve this level of customization, id need to develop something on my own‚Ä¶ im still mostly in the planning phases / testing open source projects that I could use etc",
              "score": 1,
              "created_utc": "2026-01-28 16:01:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bh4hr",
              "author": "Onotadaki2",
              "text": "I have been getting my money's worth with giving it instructions to set up cronjobs on custom scripts it makes. A simple example, I gave it my daughter's school's website and said to tell me every morning at a specific time if there are alerts like school's closed or bus delays, etc... It put together a script that pulls the alerts list and put a cronjob in to run it every morning and then message me with an update. It's stuff I could have done, but this implementation with messaging me the alerts is really polished and it took about fifteen seconds to type out the instruction.\n\nYou could do tons with just the web searching and cron alone.  Some dumb ideas: Tell it to check a site every hour until a product is in stock and then alert you.  Dump your Netflix watch history and hand it over, ask it to check the theaters every Thursday and warn you if a movie you might like is releasing in theaters the next weekend.",
              "score": 1,
              "created_utc": "2026-01-29 00:09:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25iz5o",
          "author": "No_Conversation9561",
          "text": "I‚Äôm glad that it exists. Now it (or something else) can only get better from here.",
          "score": 37,
          "created_utc": "2026-01-28 03:59:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26bgcs",
              "author": "Dry_Natural_3617",
              "text": "Valid point",
              "score": 3,
              "created_utc": "2026-01-28 07:23:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28iyi2",
              "author": "FirstEvolutionist",
              "text": "I never looked at it like it was a holy grail. I just got excited because since it is open source, there will forks and copy cats salivating to improve it, if it is useful.",
              "score": 2,
              "created_utc": "2026-01-28 16:11:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2anyj7",
              "author": "Normal-End1169",
              "text": "This is actually very untrue lmao.\n\nIt can actually get quite worse believe it or not.",
              "score": 0,
              "created_utc": "2026-01-28 21:47:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25sj55",
          "author": "alphatrad",
          "text": "It's a fricking wrapper with a pipe to Whatsapp and Cron jobs. I don't get the hype. Have been able to do most of this stuff, with N8N",
          "score": 19,
          "created_utc": "2026-01-28 04:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26yeba",
              "author": "InfraScaler",
              "text": "Everything is a wrapper with pipes.",
              "score": 8,
              "created_utc": "2026-01-28 10:49:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27gcym",
                  "author": "DasBlueEyedDevil",
                  "text": "The Internet is a series of tubes",
                  "score": 11,
                  "created_utc": "2026-01-28 13:00:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27l17j",
                  "author": "LynkSpyder",
                  "text": "https://preview.redd.it/9p5x9vnrd3gg1.png?width=657&format=png&auto=webp&s=08dac67300d9fe7827d5ae89f521a60fad3d1666",
                  "score": 8,
                  "created_utc": "2026-01-28 13:27:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27d1hp",
              "author": "Uninterested_Viewer",
              "text": "The usefulness of this seems to be more in the category of good implementation of existing functionality rather than brand new tech/functionality.\n\nI don't think I could point to an individual thing in moltbot that I couldn't accomplish in a different, existing way, but it's the way that it's all packaged together in an extremely easy to use way that is self-extensible which makes it interesting. \n\nI woke up this morning and was slightly annoyed at the volume that I have my Sonos speakers set up to play as part of a morning routine automation in home assistant. Usually I just turn it down manually and forget about it until the next morning. This morning, I shot moltbot a message from my phone while drinking coffee and asked it to update that automation to have lower volumes.\n\nIt cloned the repo, found the right automation, made the code change, opened a PR, and let me know. I reviewed the PR and merged, triggering a deploy to my home assistant instance.\n\nThe above isn't anything crazy: I could have claude code via tmux on my phone and accomplish the same thing or use n8n to hook through a messaging platform in a similar way. It's more the lack of having to set any of this up and the ability to ask it to set up its own cron jobs or heartbeat tasks. It's a fun package. I don't think it's the end all be all of *anything*, but something worth understanding and getting the feel of because I *do* think this is the direction we'll be trending.",
              "score": 3,
              "created_utc": "2026-01-28 12:40:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o265dc0",
              "author": "SharpKaleidoscope182",
              "text": ">a fricking wrapper with a pipe to Whatsapp and Cron jobs\n\nVery big deal to ppl who dont know what cron or n8n are. It's reaching a new segment of the market. No engineer can comprehend these things.",
              "score": 10,
              "created_utc": "2026-01-28 06:33:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ao12y",
              "author": "Normal-End1169",
              "text": "Exactly my though process",
              "score": 2,
              "created_utc": "2026-01-28 21:47:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o263b23",
              "author": "kal_0008",
              "text": "Super hyped, agree. we need somebody to build these 2 pipes ASAP. I starred Claudegram and runClauderun and suggested improvements to them as they will bring us closer to a remote agent.",
              "score": 0,
              "created_utc": "2026-01-28 06:17:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ppt8",
                  "author": "Soft_Possible1862",
                  "text": "I can‚Äôt tell if you‚Äôre being ironic lol",
                  "score": 1,
                  "created_utc": "2026-01-28 13:52:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26cwh3",
              "author": "Double-Lavishness870",
              "text": "The hype is justified. This will kill half of the current app ecosystem. Commodity apps like food delivery, health support, sport, family organization, meetup planning will be done silently in the background by my own assistant. Stupid apps like will disappear.\n\nIt is simple and not surprising, but closed a obvious gap. Big player will publish similar products",
              "score": -6,
              "created_utc": "2026-01-28 07:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28a95a",
                  "author": "alphatrad",
                  "text": "Right.... maybe if someone can do it with less token consumption and at cheaper costs.\n\nNot this one.",
                  "score": 1,
                  "created_utc": "2026-01-28 15:33:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26omqb",
          "author": "ridablellama",
          "text": "honestly it seems like people are finally just realizing opus is basically good enough to be an amazing agent under any conditions. you can throw literally anything at it and it will make it work. but it‚Äôs just damn expensive. too expensive for most use cases unless your coding. it was cost efficient when they let you use claude max plans but those days are over. the reality is full time opus agent is minimum 500 a month up to 5k. that‚Äôs real human cost territory. sonnet is a lot cheaper but i am still burning 5 dollars a day just on heartbeat and from corn job updates. clawdsbot floor is probably 150 a month.",
          "score": 4,
          "created_utc": "2026-01-28 09:22:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27fvwf",
              "author": "UpBeat2020",
              "text": "Nobody tried it with Deepseek, gemini or any 10x cheaper model yet ?",
              "score": 1,
              "created_utc": "2026-01-28 12:58:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27iexx",
                  "author": "ridablellama",
                  "text": "i will later‚Ä¶.i have asked in discord but it‚Äôs just completely overrun right now with support discussions. i want to try minmax and deepseek and the new kimi k2. some one did say they had success with local 30b model so that is really good sign and i will try that too. local models are the best bet for 24/7 agent.",
                  "score": 3,
                  "created_utc": "2026-01-28 13:13:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28y48m",
                  "author": "leonguyen52",
                  "text": "It just burned my 2M token on claude sonnet in the afternoon to set everything up as my expectation and tonight i just switched to deepseek üôà",
                  "score": 1,
                  "created_utc": "2026-01-28 17:17:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2aqbd2",
                  "author": "Old_Cup3392",
                  "text": "I'm using Gemini-3-Flash and it works quite well for me. For tasks that require more intelligence, like investigation, I use Claude, but Gemini's Flesh handles most tasks just fine.",
                  "score": 1,
                  "created_utc": "2026-01-28 21:57:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2c60fj",
                  "author": "scamiran",
                  "text": "It's... not great with gemini. Haven't used grok with it much, but I'm not sure i expect awesome.\n\nClaude opus is dramatically better than gemini for moltbot. It turns it into a really useful tool. Basically a personal assistant or intern.\n\nGemini doesn't come close.",
                  "score": 1,
                  "created_utc": "2026-01-29 02:23:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25jgwq",
          "author": "macromind",
          "text": "Yep, this matches my experience with a lot of agentic CLIs, when it works it feels like magic, but the config and state management can get messy fast. Token burn is also real once you let an agent loop on tool calls.\n\nOne thing thats helped me is adding hard budgets (max steps, max tool calls, max tokens) plus logging every tool invocation so you can spot where it starts thrashing. Also, forcing the agent to write a short plan before execution cuts down on the random wandering.\n\nIf youre collecting notes on what patterns actually make these setups stable, I bookmarked a few practical breakdowns here: https://www.agentixlabs.com/blog/",
          "score": 6,
          "created_utc": "2026-01-28 04:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27f9j4",
              "author": "MicMastro",
              "text": "I wonder why open source software doesn't allow to set multiple LLMs. Planning of actions could be done using a small LLM, coding with more complex models...",
              "score": 1,
              "created_utc": "2026-01-28 12:54:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2aqsdc",
                  "author": "Old_Cup3392",
                  "text": "It allows it; my agent has a Gemini Flash model, and if he needs more brainpower, he assigns Claude as a sub-agent.",
                  "score": 1,
                  "created_utc": "2026-01-28 21:59:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26f7kt",
          "author": "HealthyCommunicat",
          "text": "The model id‚Äôs when using /model command has fallback behavior that you can setup when whatever model you select isnt available or invalid. I highly recommend hooking it up to opus and then asking it to walk you through the config or do ‚Äúclawdbot configure‚Äù and select the model tab and select ur provider and hook it up, the final tab will show a giant list of model id‚Äôs thst u can select and confirm u want to use.\n\nYou should ask it how it works and then setup proper skills and tools, those are the only two things needed as the entire thing loads up TOOLS.md to all models, and walk through setting up a skill/tool for each thing u wish to use the automation for. For example i even have a ssh tool that‚Äôs used just to ssh into stuff and only investigate, its made me be able to copy paste my clients email and just have it investigate.\n\nThe command line UI is fine. I hookd up mirothinker v1.5 30b a3b and because its just simple tool calls, once its setup literally a frequent gen 30b model can handle it. The .md has OR SHOULD HAVE all usage steps for all ur models to be able to have proper syntax to use tools etc. if you‚Äôre not setting this up properly to work, it is on you for not being resourceful enough to think things through.",
          "score": 2,
          "created_utc": "2026-01-28 07:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26wmy1",
              "author": "ridablellama",
              "text": "wow Mirothinker benchmarks are looking very high for its weight. is a 30b model really reliable with clawdbot? i will have to try later. thats promising news. I need to try alot of alternative models still.",
              "score": 2,
              "created_utc": "2026-01-28 10:34:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a0e8y",
                  "author": "S4L7Y",
                  "text": "If it's true that Mirothinker is pretty good with clawdbot I might have to try it.",
                  "score": 1,
                  "created_utc": "2026-01-28 20:03:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26hyq1",
              "author": "explustee",
              "text": "Dumb question. But do I understand you correctly that you set it up to route to different models depending on the task/prompt input.",
              "score": 1,
              "created_utc": "2026-01-28 08:20:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26ipfp",
                  "author": "HealthyCommunicat",
                  "text": "I meant that I got my models setup that way yes, but no the /model does not get sent to the model, the clawdbot session catches that and responds with pre-set text. Am i understanding ur question correctly",
                  "score": 1,
                  "created_utc": "2026-01-28 08:27:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25q3yf",
          "author": "PK_Wins",
          "text": "Can we not change the api key to a different model ? which is cheaper or free ?",
          "score": 1,
          "created_utc": "2026-01-28 04:43:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25ra4b",
              "author": "DarkXanthos",
              "text": "Myself and others are trying to use local models but the implementation is basically broken. Bugs have been submitted.",
              "score": 6,
              "created_utc": "2026-01-28 04:51:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270tay",
                  "author": "Ill_Grab6967",
                  "text": "Spent so long yesterday trying to figure it out.. thought it was user error",
                  "score": 1,
                  "created_utc": "2026-01-28 11:10:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2753pb",
              "author": "Endflux",
              "text": "Well It‚Äôs only going to work as well as the agents powering the system. I imagine that option is a gateway to lots of issues and complaints. Especially if setting it up with local agents is easy enough for those having no idea what they‚Äôre doing.",
              "score": 1,
              "created_utc": "2026-01-28 11:44:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27bjt5",
              "author": "mister2d",
              "text": "Using a free model id on OpenRouter would be too easy.",
              "score": 1,
              "created_utc": "2026-01-28 12:30:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o274sx5",
          "author": "vibesurf",
          "text": "8 million tokens on Opus is painful‚Äîthat's the price of lazy architecture masquerading as 'magic.' Relying on the most expensive model to brute-force through bad state management isn't sustainable. The real unlock is hybrid workflows: let optimized local LLMs handle the context and grunt work, and only call in the big guns like Opus for complex reasoning. Otherwise, we're just building expensive toys.",
          "score": 1,
          "created_utc": "2026-01-28 11:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28fr8b",
          "author": "psychofanPLAYS",
          "text": "did anyone by any chance have had any sort of positive experience with the clawdbot and local models?   \nIm into local models and planning to set it up on my 4090",
          "score": 1,
          "created_utc": "2026-01-28 15:57:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29lbqk",
          "author": "md-rathik",
          "text": "Not sure why i am feeling this things is really overrated",
          "score": 1,
          "created_utc": "2026-01-28 18:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29ubfe",
          "author": "tvmaly",
          "text": "Saw this post today https://x.com/aakashgupta/status/2016366016155222426 that explained it was essentially 43 projects that were vibe coded and that sort of became Clawbot",
          "score": 1,
          "created_utc": "2026-01-28 19:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29w22n",
          "author": "wuu73",
          "text": "I was trying to set it up last night on a spare Linux VM, didn‚Äôt finish, cuz it doesn‚Äôt work and I am already kinda just tired of spending too much time on it. Like why tf is it making me do so much manual labor or and typing etc but it has a lot of useful things in the repo I can use and it did give me some good ideas.\n\nI like making my own tools, because I tend to understand all the details so I am already fully aware of security issues and things that can go wrong. But when it is someone else and it‚Äôs a trending tool and it‚Äôs a big project that does a lot of stuff, I just don‚Äôt know for sure whether to trust it, usually I just default to no trust and analyze it myself or make my own tool that just does the things I want.",
          "score": 1,
          "created_utc": "2026-01-28 19:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b78td",
          "author": "FunnyRocker",
          "text": "I feel the same. I think people are hyping it because it means they will get views, and thus followers, and thus when they launch something they will have an audience.\n\nI wanted to like it, but honestly all you really need is a Claude code SDK instance with streaming JSON input and output. Then hook it up to a Cron job manager, and a few CLI tools. I much prefer just spinning up Claude code in the terminal. \n\nFor memory files, you can use markdown or SQLite, take your pick. \n\nThe fact that people are praising it for downloading a voice agent software and calling a restaurant all on its own without being asked is not a feature, it's just plain scary. I don't want it to go off and do stuff on its own, that's a security and privacy nightmare.",
          "score": 1,
          "created_utc": "2026-01-28 23:17:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bbixu",
          "author": "zenmagnets",
          "text": "No need to use Opus all the time! Sonnet works for most cases.",
          "score": 1,
          "created_utc": "2026-01-28 23:40:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2btoe9",
          "author": "Late_Seat_299",
          "text": "Totally agree with this, the whole community seems to be marketing it hard, but the product is super under polished, I couldn‚Äôt even find how to configure the models for local llm. Max tokens doesn‚Äôt get sent more than 32000 on lm studio, no matter how I seem to configure it, so it never seems to work for me. User experience is poor and sends massive amounts of context in looking at the logs. Everyone saying infinite memory..don‚Äôt understand this at all. Always going to be limited by disk and the more data you have the more it will become unmanageable",
          "score": 1,
          "created_utc": "2026-01-29 01:15:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o267lew",
          "author": "bamboofighter",
          "text": "You‚Äôre right on the $ about the security flaws :) just have it evolve and make your source code a moving target that makes it not economically viable to go after. Polymorphic software is the future!",
          "score": -1,
          "created_utc": "2026-01-28 06:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2704p3",
              "author": "eli_pizza",
              "text": "This in no way improves security",
              "score": 1,
              "created_utc": "2026-01-28 11:04:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26vwti",
          "author": "Gumbi_Digital",
          "text": "It‚Äôs pretty gated.\n\nWon‚Äôt talk about anything black hat related or anything against Google TOS (I asked it to a local Google account) and it said it‚Äôs against TOS.\n\nThis was using both Gemeni and then Grok.z",
          "score": -1,
          "created_utc": "2026-01-28 10:28:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql4hwj",
      "title": "RTX Pro 6000 $7999.99",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "author": "I_like_fragrances",
      "created_utc": "2026-01-23 22:05:23",
      "score": 74,
      "num_comments": 65,
      "upvote_ratio": 0.96,
      "text": "Price of RTX Pro 6000 Max-Q edition is going for $7999.99 at Microcenter.\n\n[https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card](https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card)\n\nDoes it seem like a good time to buy?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1c5v68",
          "author": "Green-Dress-113",
          "text": "If you want to run 2 or 4 GPUs, Max-Q is the way to go for cooling. I have dual blackwell 6000 pro workstations and one heats the other with the fans blowing sideways. While the power max is 600W I'm only seeing avg 300W during inference with peaks to 350W, but not full 600W consumption So Max-Q being 300W is the sweet spot for performance and cooling.",
          "score": 16,
          "created_utc": "2026-01-24 00:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d5r3r",
              "author": "SillyLilBear",
              "text": "I run two workstation cards I have them power limited to 300W and get 96% of the performance of 600W.",
              "score": 6,
              "created_utc": "2026-01-24 03:35:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1j8410",
                  "author": "m2845",
                  "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
                  "score": 1,
                  "created_utc": "2026-01-25 01:05:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j83rm",
              "author": "m2845",
              "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
              "score": 1,
              "created_utc": "2026-01-25 01:05:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bnzd7",
          "author": "Big_River_",
          "text": "the performance difference is more like 10% and yes 96gb gddr7 vram at 300w stable is best perf per watt there is to stack up a wrx90 threadripper build with - best card for home lab by far",
          "score": 18,
          "created_utc": "2026-01-23 22:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1db5zv",
              "author": "Sufficient-Past-9722",
              "text": "Just need a good waterblock for it.",
              "score": 1,
              "created_utc": "2026-01-24 04:09:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1dla2o",
                  "author": "Paliknight",
                  "text": "For a 300w card?",
                  "score": 2,
                  "created_utc": "2026-01-24 05:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1by4af",
          "author": "morriscl81",
          "text": "You can get it cheaper than that by at least $600-700 from companies like Exxact Corp",
          "score": 4,
          "created_utc": "2026-01-23 23:26:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bzwly",
              "author": "ilarp",
              "text": "how to order from them?",
              "score": 2,
              "created_utc": "2026-01-23 23:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c0iyo",
                  "author": "morriscl81",
                  "text": "Go to their website and make a request for a quote. They will send you an invoice.  That‚Äôs how I got mine",
                  "score": 3,
                  "created_utc": "2026-01-23 23:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c08qr",
          "author": "MierinLanfear",
          "text": "Max q is lower power for if you want to run more than 2 cards on your workstation.   If your only running 1 or 2 cards go for the full version unless you plan on adding more later.\n\nThere are education discounts too.   Prices are likely to go up so now is a good time to buy.",
          "score": 3,
          "created_utc": "2026-01-23 23:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cep80",
          "author": "queerintech",
          "text": "I just bought a 5000 to pair with my 5070ti I considered the 6000 but whew.  üòÖ",
          "score": 3,
          "created_utc": "2026-01-24 00:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cgwrf",
          "author": "No-Leopard7644",
          "text": "Do you monetize this investment or it‚Äôs for fun?",
          "score": 3,
          "created_utc": "2026-01-24 01:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dre0b",
          "author": "gaidzak",
          "text": "Cheapest I found so far is from provantage for non education $7261 dollars. I hope this is real or I am understanding this pricing. \n\n[https://www.provantage.com/nvidia-9005g153220000001\\~7NVID0M1.htm](https://www.provantage.com/nvidia-9005g153220000001~7NVID0M1.htm)\n\nFor education purchases, any NVidia Partner can get them down to $6000;",
          "score": 3,
          "created_utc": "2026-01-24 06:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bx3g3",
          "author": "separatelyrepeatedly",
          "text": "Get workstation and power limit it",
          "score": 4,
          "created_utc": "2026-01-23 23:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cecci",
              "author": "Big_River_",
              "text": "the max-q variant is also a blower card therefore well suited by design to stacking in a box - multiple workstation variants even power limited require powered ddr5 risers or they will thermal throttle each other - so if you only get one sure get a workstation otherwise max-q for sure",
              "score": 4,
              "created_utc": "2026-01-24 00:54:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1cg56k",
          "author": "Qs9bxNKZ",
          "text": "Naw.  Full power version.\n\nThen afterburner if you want to reduce power.  \n\nThen undervolt if you want to reduce heat.  \n\nMost people are not going to be running more than two of them in a case.  And I mean more than two because bifurcation along with chipset performance unless you‚Äôre on a threadripper or Xeon \n\nHave two in one box and I put out about 1200W at 70% along with 950mv.  \n\n$8099 for the black box version.  $7999 for the bulk nvidia version.  Minus $2-400 for education and bulk discounts.",
          "score": 5,
          "created_utc": "2026-01-24 01:04:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dr26a",
              "author": "gaidzak",
              "text": "Education pricing for a RTX 6000 Pro Server is 6k.. I'm about to hit the BUY button.",
              "score": 1,
              "created_utc": "2026-01-24 06:02:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1egntb",
                  "author": "t3rmina1",
                  "text": "Where are you getting 6k? I'm getting edu quotes that are a bit higher for WS",
                  "score": 1,
                  "created_utc": "2026-01-24 09:49:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bij91",
          "author": "snamuh",
          "text": "What‚Äôs the deal with the max-a ed?",
          "score": 4,
          "created_utc": "2026-01-23 22:06:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biody",
              "author": "I_like_fragrances",
              "text": "It is 300W power versus 600W. Typically gets a 20% reduced performance but same VRAM and cores.",
              "score": 7,
              "created_utc": "2026-01-23 22:07:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1bjn31",
                  "author": "hornynnerdy69",
                  "text": "20% reduced performance isn‚Äôt nothing, especially when you consider that might put its performance below a 5090 for models that can fit in 32GB VRAM. And you could get a standard 6000 Pro for under 20% more money (seeing them at like $8750 recently)",
                  "score": 5,
                  "created_utc": "2026-01-23 22:12:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1bk6hr",
                  "author": "nero519",
                  "text": "I don't get it, what's the point of it? Do they just sell a normal card with artificial limitations to make it cheaper or is there something actually missing, like slower memories or less cores",
                  "score": 3,
                  "created_utc": "2026-01-23 22:14:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bzbor",
              "author": "getfitdotus",
              "text": "Better card i run 4 of these. All air cooled and work great max 60c",
              "score": 2,
              "created_utc": "2026-01-23 23:32:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dr57r",
          "author": "Foreign_Presence7344",
          "text": "Could you use the workstation version and a max q in the same box?",
          "score": 2,
          "created_utc": "2026-01-24 06:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dvdor",
          "author": "AlexGSquadron",
          "text": "They were going for $7500??",
          "score": 2,
          "created_utc": "2026-01-24 06:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f8zog",
          "author": "gweilojoe",
          "text": "Get from Computer Central - They don‚Äôt charge tax for purchases outside of California. I bought mine from them (regular version not Qmax) and it‚Äôs worked great.",
          "score": 2,
          "created_utc": "2026-01-24 13:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbkhb",
          "author": "Phaelon74",
          "text": "You can get them for 5800 ish if you spend the time to find the vendor, do inception program, etc.",
          "score": 2,
          "created_utc": "2026-01-24 13:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cfxrs",
          "author": "Cold_Hard_Sausage",
          "text": "Is this one of those gadgets that‚Äôs going to be 10 bucks in 5 years?",
          "score": 6,
          "created_utc": "2026-01-24 01:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dbxs7",
              "author": "Sufficient-Past-9722",
              "text": "If you had bought an Ampere A6000 back in January 2021, you could still sell it for something close to the original price. Similar for the 3090.",
              "score": 8,
              "created_utc": "2026-01-24 04:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1cjk1m",
              "author": "Br4ne",
              "text": "more like 20k in 2 months if you ask me",
              "score": 4,
              "created_utc": "2026-01-24 01:24:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fa25c",
              "author": "Mysterious-String420",
              "text": "Share a link to any current 10$ gadget with butt loads of vram",
              "score": 1,
              "created_utc": "2026-01-24 13:43:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1g1tep",
                  "author": "Cold_Hard_Sausage",
                  "text": "Bro, have someone teach you the concept of sarcasm",
                  "score": 1,
                  "created_utc": "2026-01-24 16:08:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r7bdf",
          "author": "Rain_Sunny",
          "text": "When we use this Graphic card to build a 4-cards Ai-workstation:\n\nTotal VRAM: 384 GB.\n\nBandwidth: 7.2 TB.\n\nComput PowerÔºàFP4Ôºâ: 16 PFLOPs.\n\nTokens Output: 50 tokens/s(70B). 15 Tokens/s(405B).\n\nLLMs Support: DeepSeek V3,Llama 405B...\n\nPrice will be acound 60000 USD.\n\nBy the way, graphic card's price seems very good!",
          "score": 1,
          "created_utc": "2026-01-26 03:41:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21ecgn",
          "author": "kaisersolo",
          "text": "Nvidia would make more bucks with 3 5090's instead of this even though we all want it",
          "score": 1,
          "created_utc": "2026-01-27 16:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21xpqj",
          "author": "PhotographerUSA",
          "text": "What a deal ! I need two more for the kids. Don't want their imagination to be dulled out lacking VRAM on their AI!",
          "score": 1,
          "created_utc": "2026-01-27 17:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29mvo8",
          "author": "PeakBrave8235",
          "text": "lol buy a Mac at that point",
          "score": 1,
          "created_utc": "2026-01-28 19:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ck7rm",
          "author": "Accomplished-Grade78",
          "text": "Anyone compare dual Max-q vs DGXA Spark? \n\n192GB vs 128GB\n\nDDR7 vs LPDDR5X unified \n\nWhat does this mean for real world performance, in your experience?",
          "score": 1,
          "created_utc": "2026-01-24 01:28:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ndmwh",
              "author": "One-Macaron6752",
              "text": "LMGTFY",
              "score": 2,
              "created_utc": "2026-01-25 17:07:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1otzko",
                  "author": "Accomplished-Grade78",
                  "text": "Thanks, it‚Äôs nice to hear recent experiences",
                  "score": 1,
                  "created_utc": "2026-01-25 20:51:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d0fbd",
          "author": "TheRiddler79",
          "text": "If I'm being fair, that thing looks bad to the fucking bone, but if I what's going to spend eight grand right now, I'd probably look for server box that would run eight V100 32 GB. Like I understand the difference in the technology and I suppose it just depends on your ultimate goal, but you could run twice as large of an AI and your inference speed would still be lightning fast. But again everybody has their own motivations. For me I'm kind of like looking at where can I get the largest amount of vram for the minimal amount of money which I'm sure most people also think but at the end of the day I also will trade 2000 tokens a second on GPT OSS 120b for 500 tokens a second on Minimax 2.1",
          "score": 1,
          "created_utc": "2026-01-24 03:02:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkudvz",
      "title": "I gave my local LLM pipeline a brain - now it thinks before it speaks",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 15:50:14",
      "score": 74,
      "num_comments": 26,
      "upvote_ratio": 0.92,
      "text": "[Video from sequential retrieval](https://reddit.com/link/1qkudvz/video/9mel0vq9d4fg1/player)\n\nIn the video you can see how and that it works.\n\nJarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience.¬†[u/frank\\_brsrk](/user/frank_brsrk/)¬†Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nüß† Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions ‚Üí AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions ‚Üí instant answer.\n\nComplex questions ‚Üí step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout to¬†[u/frank\\_brsrk](/user/frank_brsrk/)¬†for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build üòÖ\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\nsimple graphic:\n\n[Simple visualization of MCP retrieval](https://preview.redd.it/f9tm59rkd4fg1.png?width=863&format=png&auto=webp&s=6a65eda552b3b846863f75c59ee04018eb6d6c41)\n\n[Simple visualization pipeline](https://preview.redd.it/6h40kd1sd4fg1.png?width=1147&format=png&auto=webp&s=770dd510d8ebf0fe8b68f6a81bb7ab40d34fa862)\n\n@/[frank\\_brsrk](/user/frank_brsrk/) architecture of the causal intelligence module\n\n[architecture of the causal intelligence module](https://preview.redd.it/6x03fioje4fg1.jpg?width=2800&format=pjpg&auto=webp&s=7df7325899fd7dd3f8ca8743ebca4d04845868b5)\n\n\n\nSmall update the next days:\n\nhttps://preview.redd.it/9q7vo4rnbqfg1.png?width=1866&format=png&auto=webp&s=193c2a1adaabd721b0c04a8386dd9acf3b49f5ff\n\nhttps://preview.redd.it/bwcvm4rnbqfg1.png?width=1866&format=png&auto=webp&s=2457293e38992f70ff8290fada20104b19756a16\n\nhttps://preview.redd.it/ej38q5rnbqfg1.png?width=1866&format=png&auto=webp&s=efd4b330ed74bc48e9af713cc0e5568b94c3a5f7\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1amvip",
          "author": "GCoderDCoder",
          "text": "This is great! I love that a whole generation of technologists have spent our lives trying to be Iron Man and the few making the most progress by working together on it are trying to ruin it for the rest of us now that we can finally see the light at the other end of the tunnel. Thanks for helping the rest of us in the struggle to keep up!\n\nI'll be more focused one these types of projects when they eventually fire me because they don't realize how much we have to correct the AI still but I wish I had time to be consistent working on some joint projects like this. I'm trying to figure out how to piece together things like roo code with something like vibe kanban and MCPs in an opinionated way to reduce the manual burden while allowing more than coding using local LLMs and then here goes Anthropic with their cowork thing lol\n\nKeep up the hard work! When the fairytale of benevolent AI providers crumbles people will be looking for local LLMs.",
          "score": 13,
          "created_utc": "2026-01-23 19:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1aoe33",
              "author": "danny_094",
              "text": "That's exactly why I'm building this. Local-first, privacy-first, yours forever.",
              "score": 10,
              "created_utc": "2026-01-23 19:45:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a5160",
          "author": "No-Leopard7644",
          "text": "Congratulations, sounds interesting, thank you for sharing this information. Is it open source?",
          "score": 4,
          "created_utc": "2026-01-23 18:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a6tb4",
              "author": "danny_094",
              "text": "Yes you can Download on GitHub. It will always be free for private users. Everything for local users :)",
              "score": 8,
              "created_utc": "2026-01-23 18:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1c4d05",
          "author": "Endflux",
          "text": "If I can find the time I'll give it a try this weekend and let you know how it goes! Thanks for sharing :)",
          "score": 3,
          "created_utc": "2026-01-24 00:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c4op2",
              "author": "danny_094",
              "text": "I'm asking for it. I really need more user feedback :D",
              "score": 2,
              "created_utc": "2026-01-24 00:01:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aq2xn",
          "author": "burn-n-die",
          "text": "What's the system configuration you are using?",
          "score": 2,
          "created_utc": "2026-01-23 19:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1avqci",
              "author": "danny_094",
              "text": "Everything runs on an RTX 2060 Super.CPU and RAM aren't really being used. You can find my Ollama container file in the wiki.",
              "score": 3,
              "created_utc": "2026-01-23 20:19:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1b1v5b",
                  "author": "burn-n-die",
                  "text": "Thanks. I am not a software engineer and I don't code. Will you guide be straight forward?",
                  "score": 2,
                  "created_utc": "2026-01-23 20:48:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dvx2l",
          "author": "Hot_Rip_4912",
          "text": "Wow ,man that feels good",
          "score": 2,
          "created_utc": "2026-01-24 06:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f58ev",
          "author": "JinkerGaming",
          "text": "Amazing! Thank you good sir. I will certainly be forking this. :)",
          "score": 2,
          "created_utc": "2026-01-24 13:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ixovi",
          "author": "sweetbacon",
          "text": "This looks **very** interesting, thanks for sharing.",
          "score": 2,
          "created_utc": "2026-01-25 00:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j5isr",
          "author": "yeahlloow",
          "text": "Can someone please explain what is the difference between this and the thinking mode of normal LLMs?",
          "score": 2,
          "created_utc": "2026-01-25 00:51:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mpyju",
              "author": "danny_094",
              "text": "LLMs always think and answer based on the prediction of the next word. It's like autocorrect on steroids. In this case, the LLM is given less room to digress. So that even in longer contexts and questions, it doesn't start to digress or make things up. Imagine the LLM is the train, and the pipeline is the track. There are few opportunities to deviate.",
              "score": 1,
              "created_utc": "2026-01-25 15:23:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dk5rd",
          "author": "leonbollerup",
          "text": "Hey, if you have 15 min to spare ‚Ä¶ listen this pod cast, or scroll forward to the part about augmented LLM - maybe that‚Äôs something that improve your solution even more\n\n[https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1](https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1)\n\nI‚Äôm far from done with ArcAI.. but I could share you the concepts.\n\nhttps://preview.redd.it/sgo3krcbd8fg1.jpeg?width=812&format=pjpg&auto=webp&s=57fbd6c153fac073071e46c4985b71fadb7fcaeb\n\nThese are the result of controlled continued tests",
          "score": 1,
          "created_utc": "2026-01-24 05:10:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1usmpu",
          "author": "danny_094",
          "text": "https://preview.redd.it/hpui09pebqfg1.png?width=1866&format=png&auto=webp&s=c463c0af62940e7b2dc348c69405414f3aa7336e\n\nI'm feeling a bit over-motivated right now.The web interface will be slightly improved.",
          "score": 1,
          "created_utc": "2026-01-26 17:32:13",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1nlpqo",
          "author": "Available-Craft-5795",
          "text": "It kinda sounds like COCONUT but with a really long latent space. I dont really see the point right now",
          "score": 0,
          "created_utc": "2026-01-25 17:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nq626",
              "author": "danny_094",
              "text": "Fair comparison! The key difference: Jatvus/TRION isnt about latent reasoning  itss about orchestrating multiple models with explicit validation checkpoints (CIM catches antipatterns, biases, fallacies). Think of it as adding guardrails + quality control between reasoning steps, not just making the reasoning longer. The 3 layer architecture lets you use smaller, specialized models instead of one massive model.",
              "score": 1,
              "created_utc": "2026-01-25 18:00:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1obt0v",
                  "author": "Available-Craft-5795",
                  "text": "Then that sounds like a TRM with COCONUT smashed togeather",
                  "score": 1,
                  "created_utc": "2026-01-25 19:31:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1oc25t",
                  "author": "Available-Craft-5795",
                  "text": "Can you explain what it does that a TRM or COCONUT architecture doesnt do?",
                  "score": 1,
                  "created_utc": "2026-01-25 19:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjqhja",
      "title": "This Week's Hottest Hugging Face Releases: Top Picks by Category!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "author": "techlatest_net",
      "created_utc": "2026-01-22 09:52:51",
      "score": 52,
      "num_comments": 2,
      "upvote_ratio": 0.98,
      "text": "Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.\n\nCheck 'em out and drop your thoughts‚Äîwhich one's getting deployed first?\n\n# Text Generation\n\n* [**zai-org/GLM-4.7-Flash**](https://huggingface.co/zai-org/GLM-4.7-Flash): 31B param model for fast, efficient text gen‚Äîupdated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.\n* [**unsloth/GLM-4.7-Flash-GGUF**](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF): Quantized 30B version for easy local inference‚Äîhot with 112k downloads in hours. Great for low-resource setups.\n\n# Image / Multimodal\n\n* [**zai-org/GLM-Image**](https://huggingface.co/zai-org/GLM-Image): Image-text-to-image powerhouse‚Äî10.8k downloads, 938 likes. Excels in creative edits and generation.\n* [**google/translategemma-4b-it**](https://huggingface.co/google/translategemma-4b-it): 5B vision-language model for multilingual image-text tasks‚Äî45.4k downloads, supports translation + vision.\n\n# Audio / Speech\n\n* [**kyutai/pocket-tts**](https://huggingface.co/kyutai/pocket-tts): Compact TTS for natural voices‚Äî38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.\n* [**microsoft/VibeVoice-ASR**](https://huggingface.co/microsoft/VibeVoice-ASR): 9B ASR for multilingual speech recognition‚Äîultra-low latency, 816 downloads already spiking.\n\n# Other Hot Categories (Video/Agentic)\n\n* [**Lightricks/LTX-2**](https://huggingface.co/Lightricks/LTX-2) (Image-to-Video): 1.96M downloads, 1.25k likes‚Äîpro-level video from images.\n* [**stepfun-ai/Step3-VL-10B**](https://huggingface.co/stepfun-ai/Step3-VL-10B) (Image-Text-to-Text): 10B VL model for advanced reasoning‚Äî28.6k downloads in hours.\n\nThese are dominating trends with massive community traction.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o14xz85",
          "author": "Count_Rugens_Finger",
          "text": "I have found that GLM-4.7 30B-A3B model is actually inferior to Qwen3-Coder 30B-A3B for programming.  Anyone else?",
          "score": 1,
          "created_utc": "2026-01-22 23:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dhymn",
          "author": "Blizado",
          "text": "Why is Qwen3-TTS missing? For me clearly the most exciting new TTS models, especially since they have multi-language support and not only english as still too many new TTS models have.",
          "score": 1,
          "created_utc": "2026-01-24 04:55:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk9ked",
      "title": "Good local LLM for coding?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "author": "Expensive-Time-7209",
      "created_utc": "2026-01-22 22:56:33",
      "score": 35,
      "num_comments": 28,
      "upvote_ratio": 0.95,
      "text": "I'm looking for a a good local LLM for coding that can run on my rx 6750 xt which is old but I believe the 12gb will allow it to run 30b param models but I'm not 100% sure. I think GLM 4.7 flash is currently the best but posts like this [https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular\\_opinion\\_glm\\_47\\_flash\\_is\\_just\\_a/](https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular_opinion_glm_47_flash_is_just_a/) made me hesitant\n\nBefore you say just download and try, my lovely ISP gives me a strict monthly quota so I can't be downloading random LLMS just to try them out",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o163q5w",
          "author": "Javanese1999",
          "text": "[https://huggingface.co/TIGER-Lab/VisCoder2-7B](https://huggingface.co/TIGER-Lab/VisCoder2-7B) = Better version of Qwen2.5-Coder-7B-Instruct\n\n[https://huggingface.co/openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) =Very fast under 20b, even if your model size exceeds the VRAM capacity and goes into ram.\n\n[https://huggingface.co/NousResearch/NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B) = Max picks IQ4\\_XS. This is just an alternative\n\nBut of all of them, my rational choice fell on gpt-oss-20b. It's heavily censored in refusal prompts, but it's quite reliable for light coding.",
          "score": 13,
          "created_utc": "2026-01-23 02:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16qo3q",
          "author": "RnRau",
          "text": "Pick a coding MoE model and then use llama.cpp inference engine to offload some of the model to your system ram.",
          "score": 3,
          "created_utc": "2026-01-23 05:14:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o190rhs",
              "author": "BrewHog",
              "text": "Does llama.cpp have the ability to use both CPU and GPU? Or are you suggesting running one process in CPU and another in GPU?",
              "score": 1,
              "created_utc": "2026-01-23 15:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1bxmro",
                  "author": "RnRau",
                  "text": "It can use both in the same process. Do a google on 'moe offloading'.",
                  "score": 3,
                  "created_utc": "2026-01-23 23:23:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17b5hk",
              "author": "mintybadgerme",
              "text": "Or LMstudio.",
              "score": 1,
              "created_utc": "2026-01-23 07:59:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o17he3f",
          "author": "vivus-ignis",
          "text": "I've had the best results so far with gpt-oss:20b.",
          "score": 3,
          "created_utc": "2026-01-23 08:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18zcxh",
          "author": "DarkXanthos",
          "text": "I run QWEN3 coder 30B on my M1 Max 64GB and it works pretty well. I think I wouldn't go larger though.",
          "score": 3,
          "created_utc": "2026-01-23 15:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o191fwg",
              "author": "BrewHog",
              "text": "How much RAM does it use? Is that quantized?",
              "score": 1,
              "created_utc": "2026-01-23 15:16:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fb7mt",
                  "author": "guigouz",
                  "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally Q3 uses around 20gb here (~14gb on gpu + 6gb on system ram) for a 50k context.\n\nI also tried Q2 but it's too dumb for actual coding, Q3 seems to be the sweet spot for smaller GPUs (Q4 is not **that** better).",
                  "score": 2,
                  "created_utc": "2026-01-24 13:50:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14y7b5",
          "author": "Used_Chipmunk1512",
          "text": "Nope, 30B quantized to q4 will be too much for your gpu, don't download it. Stick with models under 10B",
          "score": 5,
          "created_utc": "2026-01-22 23:08:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14ykru",
              "author": "Expensive-Time-7209",
              "text": "Any recommendations under 10B?",
              "score": 1,
              "created_utc": "2026-01-22 23:10:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14zjyb",
                  "author": "iMrParker",
                  "text": "GLM 4.6v flash is pretty competent for its size. It should fit quantized with an okay context size¬†",
                  "score": 1,
                  "created_utc": "2026-01-22 23:15:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15prnv",
          "author": "Available-Craft-5795",
          "text": "GPT OSS 20B if it fits. Could work just fine in RAM though.  \nIts surprisingly good",
          "score": 2,
          "created_utc": "2026-01-23 01:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1exduu",
              "author": "Virtual_Actuary8217",
              "text": "Not even support agent tool calling no thank you",
              "score": -1,
              "created_utc": "2026-01-24 12:17:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1f9apu",
                  "author": "Available-Craft-5795",
                  "text": "https://preview.redd.it/xhk81ws4wafg1.png?width=965&format=png&auto=webp&s=be4edb166f90d403c3016ec71a66aa288a66b4e2\n\nWhat?  \nYes it does.",
                  "score": 1,
                  "created_utc": "2026-01-24 13:39:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1qpitb",
                  "author": "10F1",
                  "text": "Yes it does?",
                  "score": 1,
                  "created_utc": "2026-01-26 02:07:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rps45",
                  "author": "Virtual_Actuary8217",
                  "text": "It says one thing, but when you pair it with cline ,it basically can't do anything",
                  "score": 1,
                  "created_utc": "2026-01-26 05:41:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gqpua",
          "author": "SnooBunnies8392",
          "text": "I had Nvidia RTX 3060 12GB and I used\n\nQwen3 Coder @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nand\n\nGPT OSS 20B @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nBoth did offload a bit to system ram, but they were both useful anyway.",
          "score": 2,
          "created_utc": "2026-01-24 17:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o160ny9",
          "author": "No-Leopard7644",
          "text": "Try devstral, Qwen 2.5 Coder. You need to choose a quant so that the size of the model fits the vram. Also for coding you need some vram for context. What are using for model inference?",
          "score": 1,
          "created_utc": "2026-01-23 02:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o163jhs",
          "author": "nitinmms1",
          "text": "Anything beyond 8b q4 will be difficult.",
          "score": 1,
          "created_utc": "2026-01-23 02:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17n36t",
          "author": "WishfulAgenda",
          "text": "I‚Äôve found that higher q in smaller models is really helpful. Also don‚Äôt forget your system prompt or agent instructions.",
          "score": 1,
          "created_utc": "2026-01-23 09:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ao98f",
          "author": "Few_Size_4798",
          "text": "There are reviews on YouTube from last week:\n\nThe situation is as follows: even if you don't skimp on the Strix Halo ($2000+ today), all local ones can be shoved in the ass: Claude rules, and Gemini is already pretty good.",
          "score": 1,
          "created_utc": "2026-01-23 19:44:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21m0nc",
              "author": "GeroldM972",
              "text": "And none of the Youtube channels you pull information from receive any sponsorship from those same cloud-LLM providers and/or \"middle-men\" (those that allow you to connect to several of those cloud-LLM providers, via their single monthly subscription)?\n\nI use my own set of test questions and regularly test cloud and local LLMs. Cloud are often better and faster. Not always though.  But even NVidia claimed that the current cloud-LLM structures are not the solution, running local LLMs is. \n\nBesides, When I run local, I choose which model and its specialization, while I don't have any say in what the cloud-LLM  provider will give me. Or when they update their update their model and require me to rewrite/redefine configurations for agents, because of their internal changes.\n\nThere are very good reasons to use local LLMs, there are strong reasons to use cloud-provider LLMs. And it is not an 'either/or'-story, but an 'and' story. As in: use both at the moments in your processes that you need these to.",
              "score": 1,
              "created_utc": "2026-01-27 16:47:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o227mby",
                  "author": "Few_Size_4798",
                  "text": "I agree, but in the long run, cloud-based systems are constantly learning, including from closed data, so to speak, which cannot be said about local systems.\n\nLocal systems are good for texts, perhaps even for translations‚Äînot many idioms are used in everyday speech, but algorithms for specific languages need constant improvement.",
                  "score": 1,
                  "created_utc": "2026-01-27 18:20:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bxos0",
          "author": "Inevitable_Yard_6381",
          "text": "Hi totally new but tired of waiting Gemini on Android studio to answer...I have a MacBook Pro M1 pro 16 GB ram.. Any chance I could use a local LLM? \nAnd if possible how to integrate with my IDE to work like an agents and have access to my project? Could also be possible to send links to learn some new API or dependency? \nThanks in advance!!",
          "score": 1,
          "created_utc": "2026-01-23 23:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbadr",
          "author": "guigouz",
          "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally",
          "score": 1,
          "created_utc": "2026-01-24 13:50:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qojhkj",
      "title": "Local LLM for Coding that compares with Claude",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qojhkj/local_llm_for_coding_that_compares_with_claude/",
      "author": "thecrogmite",
      "created_utc": "2026-01-27 16:59:10",
      "score": 33,
      "num_comments": 69,
      "upvote_ratio": 0.75,
      "text": "Currently I am on the Claude Pro plan paying $20 a month and I have hit my weekly and daily limits very quickly. Am I using it to essentially handle all code generation? Yes. This is the way it has to be as I'm not familiar with the language I'm forced to use. \n\n  \nI was wondering if there was a recommended model that I could use to match Claude's reasoning and code output. I don't need it to be *super fast* like Claude. I need it to be accurate and not completely ruin the project. While most of that I feel like is prompt related, some of that has to be related to the model. \n\n  \nThe model would be ran on a MacBook Pro M3. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qojhkj/local_llm_for_coding_that_compares_with_claude/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o21s08f",
          "author": "Ryanmonroe82",
          "text": "You won't be able to match a cloud model on that setup",
          "score": 48,
          "created_utc": "2026-01-27 17:13:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21tkr5",
              "author": "MostIncrediblee",
              "text": "I have 48GB ram. What one model is best for coding. Probably not on Claude‚Äôs level.¬†",
              "score": 5,
              "created_utc": "2026-01-27 17:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21w9zw",
                  "author": "synth_mania",
                  "text": "Probably Devstral Small 2 (devstral-small-2-2512). It was released by Mistral last month.\n\nExcellent 24B param dense model. I'm running it at Q4\\_K\\_M to good effect, but the largest quant you can fit up to Q8 is probably worth sacrificing RAM for if you can still fit the context you need.\n\nI've had really good success with Aider, but it might trip up sometimes with less hands-on tools like Kilo Code, or other agentic programming applications.",
                  "score": 16,
                  "created_utc": "2026-01-27 17:31:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21xi13",
                  "author": "minaskar",
                  "text": "Devstral Small 2 (2512), GLM 4.7 Flash, and Qwen 3 Coder 30b",
                  "score": 9,
                  "created_utc": "2026-01-27 17:36:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21wm9h",
                  "author": "hoowahman",
                  "text": "People usually mention qwen3coder for this, not sure how local GLM 4.7 fairs though in comparison. Probably pretty good.",
                  "score": 3,
                  "created_utc": "2026-01-27 17:33:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23940v",
                  "author": "RnRau",
                  "text": "ram or vram?\n\nIf ram, try gpt-oss-20b with thinking mode set on 'high'. If vram, try the big brother - gpt-oss-120b with model offloading.",
                  "score": 3,
                  "created_utc": "2026-01-27 21:05:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23lrjt",
                  "author": "Ryanmonroe82",
                  "text": "Qwen coder is pretty good. Use the 32b version if possible",
                  "score": 2,
                  "created_utc": "2026-01-27 22:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21x0zj",
          "author": "son_et_lumiere",
          "text": "Use the claude/larger model for the reasoning capabilities to break down big task into smaller coding tasks. Then take that task list and use a cheap/free model to the actual coding from the well defined task.",
          "score": 30,
          "created_utc": "2026-01-27 17:34:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22p0b9",
              "author": "intertubeluber",
              "text": "Look at the big brains on Brad!",
              "score": 4,
              "created_utc": "2026-01-27 19:34:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o22nwdn",
              "author": "Weird-Consequence366",
              "text": "This is the way",
              "score": 2,
              "created_utc": "2026-01-27 19:29:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29as3e",
              "author": "fourfastfoxes",
              "text": "one other thing to do is that you can install Google Jules cli and assign the smaller tasks to Jules to complete. you get about 15 tasks per day free.",
              "score": 1,
              "created_utc": "2026-01-28 18:11:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o220e9j",
          "author": "pot_sniffer",
          "text": "You won't find a local llm as good as Claude but what i do use my Claude pro to do the planning, within that I plan to break the project up into manageable chunks. Then I get Claude to make structured json prompts for the local llm. I'm currently using qwen2.5-coder but I've had similar results with the other I've done this with",
          "score": 18,
          "created_utc": "2026-01-27 17:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22telk",
              "author": "thumperj",
              "text": "Just curious to learn a bit more about your workflow. Do you have this claude-->local handoff process scripted or do you do it manually?\n\nCurrently, I'm using claude cli for pretty much everything, which includes editing files but I'm also making a nice car payment to the claude gods every month....  One day soon I want to jump to a more efficient methodology BUT my current setup enables me to work like a banshee, produce excellent work and charge $$$$ to my clients so it's been well worth it.",
              "score": 4,
              "created_utc": "2026-01-27 19:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o234gun",
                  "author": "pot_sniffer",
                  "text": "Currently manual. Claude generates a structured JSON prompt, I copy/paste to Qwen2.5-coder, test the result. Haven't scripted it because I'm early in the build and the manual handoff isn't painful yet. Plan is to keep it manual until it gets annoying enough to justify writing automation.\nThe key is STATE.md - single source of truth that both Claude and local model read so they don't suggest things I've already tried/failed. That prevents context waste more than scripting would.",
                  "score": 3,
                  "created_utc": "2026-01-27 20:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o22nun5",
              "author": "thecrogmite",
              "text": "Good to know, maybe this is the move for me. Thank you for that.",
              "score": 4,
              "created_utc": "2026-01-27 19:29:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22dml6",
          "author": "milkipedia",
          "text": "It seems this question is asked every day. Only difference is the user's available hardware, if they even bother to specify.",
          "score": 5,
          "created_utc": "2026-01-27 18:45:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22pptl",
              "author": "l8yters",
              "text": "welcome to reddit.",
              "score": 1,
              "created_utc": "2026-01-27 19:38:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22gsvc",
          "author": "armyknife-tools",
          "text": "Check out open routers leaderboard. I think it‚Äôs pretty accurate. 3 of the top 10 are open weights models.",
          "score": 5,
          "created_utc": "2026-01-27 18:58:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22e1oh",
          "author": "SimplyRemainUnseen",
          "text": "Depending on how much memory your system has GLM-4.7-Flash would be a good local model you can run. It won't be as good as Claude 4 models but it can handle a lot. I suggest giving it a try. I've found local LLMs have been at the performance I need for my programming workflow since 2024.",
          "score": 3,
          "created_utc": "2026-01-27 18:47:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26mbdi",
              "author": "ScoreUnique",
              "text": "This is the right answer, I wouldn't hesitate to claim 4.7 Flash is Sonnet 3.5 but open source. \n\nSo @OP if you can combine Claude sub for writing specifications and GLM 4.7 for writing code, you can go very far with your config. GL",
              "score": 1,
              "created_utc": "2026-01-28 09:00:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o22mwdv",
              "author": "thecrogmite",
              "text": "Thank you! I'll take a peek and give it a try.",
              "score": 1,
              "created_utc": "2026-01-27 19:25:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21ziyw",
          "author": "JordanAtLiumAI",
          "text": "Is your pain mostly code generation, or reasoning across a lot of project context like docs, configs, logs, and past commits?",
          "score": 2,
          "created_utc": "2026-01-27 17:45:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22nsjv",
              "author": "thecrogmite",
              "text": "Great question. For this specific task that I'm trying to overcome is code generation. The project has expanded from a SQL database to a .NET8 middleware to React front end. While I'm familiar with the SQL side of things slightly, the middleware and React are foreign.   \n  \nI've got the project at a solid first pass working state however making changes Claude seems to want to make huge passes across the entire architecture, then make it's decision as to what to change. While I believe that my prompting could improve to essentially add better guardrails, I'm worried I'll burn my availability fairly quickly regardless just based on project size.",
              "score": 1,
              "created_utc": "2026-01-27 19:29:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22ws9w",
                  "author": "JordanAtLiumAI",
                  "text": "Pro usage limits depend on the total length of your conversation, the number of messages, and which model or feature you use, and they may vary with current capacity.\n\n  \nSo as your project grows, it is normal that you hit limits faster, because each prompt tends to require more context and more turns. Their recommended mitigation is to be specific and concise and avoid vague prompts that trigger extra clarification cycles.\n\n  \nA practical workflow pattern that aligns with that  \n‚Ä¢ Convert ‚Äúmake this change‚Äù into a single narrowly defined task  \n‚Ä¢ Provide only the minimal relevant snippets, not the whole repo  \n‚Ä¢ Constrain edits to a file list  \n‚Ä¢ Require diff output  \n‚Ä¢ Plan first, then implement step 1\n\n  \nIt reduces the chance of large refactors and keeps each turn cheaper. \n\nHope this helps! Feel free to DM me if you want. We can dive in more.",
                  "score": 3,
                  "created_utc": "2026-01-27 20:09:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22zbr9",
          "author": "ServiceOver4447",
          "text": "Nopes, you will have to pay more for it to do your job.",
          "score": 2,
          "created_utc": "2026-01-27 20:21:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o232f71",
          "author": "StardockEngineer",
          "text": "We need to start having a flair post requirement for these posts.",
          "score": 2,
          "created_utc": "2026-01-27 20:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23sc20",
          "author": "Educational_Sun_8813",
          "text": "try devstral2 small, and qwen3-coder",
          "score": 2,
          "created_utc": "2026-01-27 22:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23tid2",
          "author": "Torodaddy",
          "text": "Qwen3 coder 30b model works well for me. Running it in llama.cpp",
          "score": 2,
          "created_utc": "2026-01-27 22:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22d1ip",
          "author": "greeny1greeny",
          "text": "nothing... all the latest models i tried are complete buns and dont even touch claude.",
          "score": 3,
          "created_utc": "2026-01-27 18:43:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22oh83",
          "author": "ithkuil",
          "text": "Claude Opus 4.5 is probably running on 8 x H200 clusters anywhere they have capacity for that. It's not the same because they do batching, but your Mac may be as much as 1000 times less powerful.¬†",
          "score": 1,
          "created_utc": "2026-01-27 19:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22p9xa",
          "author": "isleeppeople",
          "text": "Maybe not applicable but I want my machine to be an expert about itself to help me troubleshoot and add its own integrations that match my stack. It also helps to keep track of upgrades if I break something. I have a corpora of everything I use gitingest, readme docs, etc. I have it ingested in qdrant. Seems like you could do the same thing in your situation. Not an expert coder for everything but you might be able to make it as good as Claude in this one particular area.",
          "score": 1,
          "created_utc": "2026-01-27 19:36:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22tm19",
          "author": "passive_interest",
          "text": "I went this route on my M4 - developed a service that plans and applies atomic commits via local models & Ollama, but the round trip was painfully slow compared to having a Claude subagent or Codex skill perform the same task.",
          "score": 1,
          "created_utc": "2026-01-27 19:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22y83z",
          "author": "Stargazer1884",
          "text": "Before you go down the local route...(which I love, but it's not currently comparable to a frontier model on cloud) try using Opus to plan and Sonnet to execute the individual tasks.",
          "score": 1,
          "created_utc": "2026-01-27 20:16:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22zn8z",
          "author": "Terminator857",
          "text": "Comments in this poll might help answer: [https://www.reddit.com/r/LocalLLaMA/comments/1qj935h/poll\\_when\\_will\\_we\\_have\\_a\\_30b\\_open\\_weight\\_model\\_as/](https://www.reddit.com/r/LocalLLaMA/comments/1qj935h/poll_when_will_we_have_a_30b_open_weight_model_as/)",
          "score": 1,
          "created_utc": "2026-01-27 20:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wpgh",
          "author": "Ok_Chef_5858",
          "text": "If you're hitting limits that fast, you might want to just bring your own API keys instead of the $20 plan. Way more control over costs. I work on a project in VS Code, using Kilo Code atm and for local models specifically, Qwen Coder or DeepSeek R1 are solid options, but honestly nothing fully matches Claude yet. Best bet is mixing local for simple stuff and cloud for complex.",
          "score": 1,
          "created_utc": "2026-01-28 10:35:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27spjs",
              "author": "thecrogmite",
              "text": "I wondered if getting an API key was also smarter...",
              "score": 1,
              "created_utc": "2026-01-28 14:08:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27w3qo",
          "author": "XccesSv2",
          "text": "If your are not just looking into local models and a opinion is to switch the cloud provider I would suggest GLM 4.7 Coding plan is a good choice for you. Cheaper, nearly as good as Sonnet 4.5 in most tasks and more usage before hitting rate limits. Also api keys are included in the plan so you can use that GLM models anywhere you want.",
          "score": 1,
          "created_utc": "2026-01-28 14:25:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2804kc",
              "author": "thecrogmite",
              "text": "Good to know, thank you! You're the second person to suggest GLm 4.7 Coding.",
              "score": 1,
              "created_utc": "2026-01-28 14:45:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ai0vi",
          "author": "Remarkable-Jump-6227",
          "text": "Kimi 2.5 from moonshot ai  just came out , I‚Äôm using it with opencode because my Claude code limits have also hit and I have to say it‚Äôs definitely not bad! Also got 5.2 codex model is superior in a coding sense, Claude code works the best in an agentic loop but my god 5.2-codex model is easily just as good if not better when it comes to coding.",
          "score": 1,
          "created_utc": "2026-01-28 21:21:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22vnc1",
          "author": "bakawolf123",
          "text": "I'd suggest antigravity/gemini-cli as an option. Beside Gemini Pro it even has Opus included and while the limit for latter is quite short Gemini itself is good enough.   \n  \nAs for local models, current best small model for agent is GLM4.7 Flash. However Macs are really bad with prefill/prompt processing and afaik M3 PRO also screwed up architecture (lower memory bandwith) so it is worse than M1 Pro https://github.com/ggml-org/llama.cpp/discussions/4167. Current harnesses all start with 10-15k token system prompt so it feels quite garbage.  \nThere's hope for M5 with Pro/Max and possibly even Ultra coming this year though.",
          "score": 1,
          "created_utc": "2026-01-27 20:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23jh1f",
          "author": "Decent-Freedom5374",
          "text": "I train Sensei off Claude and codex, he‚Äôs an intelligent layer that orchestrates my ollama models, with the ability to rag anything that Claude and codex does he works just as good as them! :)",
          "score": 1,
          "created_utc": "2026-01-27 21:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23d2tk",
          "author": "Christosconst",
          "text": "Sonnet 4.5 is good and cheaper than opus",
          "score": 0,
          "created_utc": "2026-01-27 21:22:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2251c6",
          "author": "Jackster22",
          "text": "GLM 4.5 has been great with Claude via their own cloud service.\n$6 a month option, currently 50% off for the first month, gives you more than the Claude $20 subscription.\n\nI had done 200,000,000 tokens in the past 24 hours and it has been solid. No time outs no nothing.\n\nhttps://z.ai/subscribe?ic=SNK0LAU2OF\n\nYou can self hosted but why bother when it is this cheap and so much faster...",
          "score": -7,
          "created_utc": "2026-01-27 18:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22mzol",
              "author": "thecrogmite",
              "text": "I'll take a look, is that some sort of affiliate link?",
              "score": 1,
              "created_utc": "2026-01-27 19:25:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22tru7",
                  "author": "btc_maxi100",
                  "text": "it's a paid shill",
                  "score": 3,
                  "created_utc": "2026-01-27 19:55:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o237xcu",
                  "author": "Jackster22",
                  "text": "It is, we both get a couple pennies each time. I only shill for products I actually use.  [https://i.postimg.cc/9F7xYyLZ/image.png](https://i.postimg.cc/9F7xYyLZ/image.png)",
                  "score": 0,
                  "created_utc": "2026-01-27 20:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qljfuw",
      "title": "AI & ML Weekly ‚Äî Hugging Face Highlights",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "author": "techlatest_net",
      "created_utc": "2026-01-24 10:16:03",
      "score": 27,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "Here are the most notable¬†**AI models released or updated this week on Hugging Face**, categorized for easy scanning üëá\n\n# Text & Reasoning Models\n\n* **GLM-4.7 (358B)**¬†‚Äî Large-scale multilingual reasoning model¬†[https://huggingface.co/zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)\n* **GLM-4.7-Flash (31B)**¬†‚Äî Faster, optimized variant for text generation¬†[https://huggingface.co/zai-org/GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)\n* **Unsloth GLM-4.7-Flash GGUF (30B)**¬†‚Äî Quantized version for local inference¬†[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n* **LiquidAI LFM 2.5 Thinking (1.2B)**¬†‚Äî Lightweight reasoning-focused LLM¬†[https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)\n* **Alibaba DASD-4B-Thinking**¬†‚Äî Compact thinking-style language model¬†[https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking](https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking)\n\n# Agent & Workflow Models\n\n* **AgentCPM-Report (8B)**¬†‚Äî Agent model optimized for report generation¬†[https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n* **AgentCPM-Explore (4B)**¬†‚Äî Exploration-focused agent reasoning model¬†[https://huggingface.co/openbmb/AgentCPM-Explore](https://huggingface.co/openbmb/AgentCPM-Explore)\n* **Sweep Next Edit (1.5B)**¬†‚Äî Code-editing and refactoring assistant¬†[https://huggingface.co/sweepai/sweep-next-edit-1.5B](https://huggingface.co/sweepai/sweep-next-edit-1.5B)\n\n# Audio: Speech, Voice & TTS\n\n* **VibeVoice-ASR (9B)**¬†‚Äî High-quality automatic speech recognition¬†[https://huggingface.co/microsoft/VibeVoice-ASR](https://huggingface.co/microsoft/VibeVoice-ASR)\n* **PersonaPlex 7B**¬†‚Äî Audio-to-audio personality-driven voice model¬†[https://huggingface.co/nvidia/personaplex-7b-v1](https://huggingface.co/nvidia/personaplex-7b-v1)\n* **Qwen3 TTS (1.7B)**¬†‚Äî Custom & base voice text-to-speech models¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign)\n* **Pocket-TTS**¬†‚Äî Lightweight open TTS model¬†[https://huggingface.co/kyutai/pocket-tts](https://huggingface.co/kyutai/pocket-tts)\n* **HeartMuLa OSS (3B)**¬†‚Äî Text-to-audio generation model¬†[https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B](https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B)\n\n# Vision: Image, OCR & Multimodal\n\n* **Step3-VL (10B)**¬†‚Äî Vision-language multimodal model¬†[https://huggingface.co/stepfun-ai/Step3-VL-10B](https://huggingface.co/stepfun-ai/Step3-VL-10B)\n* **LightOnOCR 2 (1B)**¬†‚Äî OCR-focused vision-language model¬†[https://huggingface.co/lightonai/LightOnOCR-2-1B](https://huggingface.co/lightonai/LightOnOCR-2-1B)\n* **TranslateGemma (4B / 12B / 27B)**¬†‚Äî Multimodal translation models¬†[https://huggingface.co/google/translategemma-4b-it](https://huggingface.co/google/translategemma-4b-it)¬†[https://huggingface.co/google/translategemma-12b-it](https://huggingface.co/google/translategemma-12b-it)¬†[https://huggingface.co/google/translategemma-27b-it](https://huggingface.co/google/translategemma-27b-it)\n* **MedGemma 1.5 (4B)**¬†‚Äî Medical-focused multimodal model¬†[https://huggingface.co/google/medgemma-1.5-4b-it](https://huggingface.co/google/medgemma-1.5-4b-it)\n\n# Image Generation & Editing\n\n* **GLM-Image**¬†‚Äî Text-to-image generation model¬†[https://huggingface.co/zai-org/GLM-Image](https://huggingface.co/zai-org/GLM-Image)\n* **FLUX.2 Klein (4B / 9B)**¬†‚Äî High-quality image-to-image models¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-4B](https://huggingface.co/black-forest-labs/FLUX.2-klein-4B)¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n* **Qwen Image Edit (LoRA / AIO)**¬†‚Äî Advanced image editing & multi-angle edits¬†[https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA](https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA)¬†[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO)\n* **Z-Image-Turbo**¬†‚Äî Fast text-to-image generation¬†[https://huggingface.co/Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)\n\n# Video Generation\n\n* **LTX-2**¬†‚Äî Image-to-video generation model¬†[https://huggingface.co/Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)\n\n# Any-to-Any / Multimodal\n\n* **Chroma (6B)**¬†‚Äî Any-to-any multimodal generation¬†[https://huggingface.co/FlashLabs/Chroma-4B](https://huggingface.co/FlashLabs/Chroma-4B)",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qpix0z",
      "title": "clawdbot what am I missing?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qpix0z/clawdbot_what_am_i_missing/",
      "author": "olearyboy",
      "created_utc": "2026-01-28 18:04:46",
      "score": 26,
      "num_comments": 25,
      "upvote_ratio": 0.86,
      "text": "This week my feeds have been over thrown with something called 'clawdbot' / 'moltbot'\n\nHere's the breakdown of what I'm seeing\n\n\\* 80% - here's a 20 minute video on how to install it\n\n\\* 15% - (hype) best thing ever / massive security concern\n\n\\* 5% - here's a thing I did with it\n\n\n\nWithout installing, it just seems like a regular agent the same as we've all been building with the kitchen sink thrown at it for in-out bound communication and agentic skills md's and tooling with a bit of memory.\n\nThat 5% was one dude comparing clawdbot to claude code\n\n\n\nWhat am I missing?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpix0z/clawdbot_what_am_i_missing/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o29at9i",
          "author": "TokenRingAI",
          "text": "There is a point with every new technology, where the uninformed mob learns about it and storms the gates in some kind of massive bandwagon\n\nHere is a summary of how that is going\n\nhttps://preview.redd.it/fbu1phaes4gg1.png?width=1078&format=png&auto=webp&s=715be3f1e1dff93e1fe0ea193bb5fa2aeeed20d1",
          "score": 43,
          "created_utc": "2026-01-28 18:11:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bj7jt",
              "author": "SalemStarburn",
              "text": "Is ChaosGPT still around? Hook it up to Clawdbot + sudo + internet access and a couple thousand dollars and let it cook.",
              "score": 1,
              "created_utc": "2026-01-29 00:20:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2biuik",
              "author": "meva12",
              "text": "Is that true !? Damn",
              "score": -1,
              "created_utc": "2026-01-29 00:18:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o29dk8w",
          "author": "commandedbydemons",
          "text": "It‚Äôs cool, but also destroys tokens like there‚Äôs no tomorrow.\n\nDon‚Äôt let it use your full pc, sandbox it for the love of god",
          "score": 15,
          "created_utc": "2026-01-28 18:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29jkl3",
              "author": "ObsidianNix",
              "text": "Docker with an automatic firewall should be the default install for any AI installation. Open whatever doors are need when they are needed.",
              "score": 7,
              "created_utc": "2026-01-28 18:48:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ad2bz",
                  "author": "Iron-Over",
                  "text": "Docker is not secure enough get VM to stop host kernel vulnerabilities and in multi tenant docker environments. Beyond VM least privileged principal. Do not use untrusted data.",
                  "score": 3,
                  "created_utc": "2026-01-28 20:59:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29vre7",
                  "author": "tillybowman",
                  "text": "clawdbot does not have an official docker yet, right?",
                  "score": 1,
                  "created_utc": "2026-01-28 19:42:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29dve5",
          "author": "blamestross",
          "text": "Its the best off the shelf tooling for self-run agents. I'm running it on gemma. \n\nIts meh? Brittle config, bit overcomplicated, clearly vibecoded and spams the context with too much garbage that isn't task related.\n\nIts horrible. It does hook up multiple chat platforms to an agent with a pile of tools. Thats nice.",
          "score": 5,
          "created_utc": "2026-01-28 18:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29dk7p",
          "author": "noctrex",
          "text": "The letter \"s\" in clawdbot¬†stands for security.\n\nhttps://preview.redd.it/8mobb8ygu4gg1.png?width=857&format=png&auto=webp&s=3537fe9c6708ed9054d2e285356da2b36615ce69",
          "score": 11,
          "created_utc": "2026-01-28 18:23:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a1hii",
          "author": "Scott_Malkinsons",
          "text": "IMO Clawd/Moltbot + [Z.AI](http://Z.AI) = Poor mans Claude Code.\n\nYes, it's basically just an Agent, there's nothing \"ground breaking\" about it from what I can tell using it. It's basically Claude Code but instead of $200/mo for the max plan, you pay [Z.AI](http://Z.AI) $3-6/month, run Clawd/Molt on a $6-10 VPS, and you effectively get Claude Code for under $20/month.\n\nThen there's the 'I'm not sure if Claude Code can do this, it probably can, but I've never done it' things like talking to the LLM through Telegram, both with text back and forth, or voice back and forth with TTS and STT.",
          "score": 4,
          "created_utc": "2026-01-28 20:08:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c73r0",
              "author": "iplaypianoforyou",
              "text": "What is z.ai",
              "score": 1,
              "created_utc": "2026-01-29 02:29:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a4fml",
          "author": "radiofreevanilla",
          "text": "Anyone interested in how it gets built, Pragmatic Engineer just published an interview:  \nThe creator of Clawd: \"I ship code I don't read\"  \n[https://newsletter.pragmaticengineer.com/p/the-creator-of-clawd-i-ship-code](https://newsletter.pragmaticengineer.com/p/the-creator-of-clawd-i-ship-code)",
          "score": 4,
          "created_utc": "2026-01-28 20:21:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aowqg",
          "author": "Necessary-Drummer800",
          "text": "I'm with you.  From the middle it looks like a desperate attempt to push out the long tail of the bust.",
          "score": 2,
          "created_utc": "2026-01-28 21:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rcat",
          "author": "MyGoldfishGotLoose",
          "text": "I would have been curious but the hype has me in ‚Äúwait and see‚Äù mode.",
          "score": 1,
          "created_utc": "2026-01-28 19:22:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29z7e5",
          "author": "epSos-DE",
          "text": "In my OPINION.\n\nIT is a good start, BUT it needs to be p2p gossip chat. Ai to AI !\n\n  \nAlso, it is too broad. Needs to focus , not being distracted. \n\n  \nIT will run out of steam without focused application !",
          "score": 1,
          "created_utc": "2026-01-28 19:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9380",
          "author": "Dense-Map-4613",
          "text": "Installed for 3 hours, get it to works, doesn‚Äôt see any benefit. Delete. Just a bunch hype out of nothing.",
          "score": 1,
          "created_utc": "2026-01-28 20:42:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2an0uj",
          "author": "HatEducational9965",
          "text": "I don't get it either",
          "score": 1,
          "created_utc": "2026-01-28 21:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aout1",
          "author": "pandodev",
          "text": "definitely try it, it is cool but not what people are making it out to be and honestly NOONE should be running this on their home network, best it to securely sandbox it in an ec2 with only able to access via sessionmanager. Maybe I am too paranoid but this uses dependencies if any of those dependencies get infiltrated would you rather them be in a aws secluded server or your home network?",
          "score": 1,
          "created_utc": "2026-01-28 21:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b03a0",
          "author": "Crazy_Patience921",
          "text": "I would say nothing :)\n\n",
          "score": 1,
          "created_utc": "2026-01-28 22:42:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2btsme",
          "author": "EdgardoZar",
          "text": "I found it useful for my minimax subscription which does not have a chatGUI like claude or even Zai, and I found it useful just to automatically create notes out of my memory dialy dump, without having to turn on the PC and use claude code or whatever, I have some n8n workflows but you still need interaction at some point, not only triggering workflows with commands. Nothing fancy but still kind of useful for some tasks",
          "score": 1,
          "created_utc": "2026-01-29 01:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29a3lg",
          "author": "xyzzzzy",
          "text": "Man I don't know either. I am trying to install it to see what I'm missing. I know that's the point of the hype, but here we are.",
          "score": 1,
          "created_utc": "2026-01-28 18:08:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29j0rk",
              "author": "ObsidianNix",
              "text": "Wait until it matures. Thats what im doing too after trying for 5 hours on docker well secured. Its very finicky. \n\nRight now its going through the polishing stage. I guess we‚Äôll have to see if this is just another side project.",
              "score": 3,
              "created_utc": "2026-01-28 18:46:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o29m2d1",
          "author": "_hephaestus",
          "text": "Took a crack at it this morning, seemed like interesting tooling to be able to talk to my llms via signal/whatsapp/imessage but the configuration is not particularly intuitive for your own models.",
          "score": 0,
          "created_utc": "2026-01-28 18:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b7t0b",
          "author": "Objective-Arrival637",
          "text": "I am using kimi-k2.5 using ollama-cloud and added the homeassistant skill. It couldn't turn on the lights in my office. After so much back and forth it finally learnt on how to do that :( Facing the same thing with browser use, and other skills. Maybe it is me, or maybe this is not built for kimi models? But GLM-4.7 was worse.",
          "score": 0,
          "created_utc": "2026-01-28 23:20:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qluto8",
      "title": "HashIndex: No more Vector RAG",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 18:33:54",
      "score": 24,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama and llama cpp . Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha\n\n[https://github.com/JasonHonKL/HashIndex/tree/main](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1ifug5",
          "author": "FaceDeer",
          "text": "I didn't see any documentation there about how the \"guts\" of the system worked, so [I asked Gemini to do a Deep Research run to produce one](https://gemini.google.com/share/2e13e0d1a6fe). Some key bits:\n\n> The documentation for HashIndex identifies it as a \"vectorless\" index system. This characterization is central to its \"under the hood\" operations. Instead of calculating a mathematical hash or a vector embedding, the system invokes an LLM to generate what it terms a \"semantic hash key\".  \n\n> When a document is ingested by HashIndex, it is first split into segments or pages. For each segment, the system initiates a dual-process LLM call. The first process involves generating a highly descriptive, human-readable label that encapsulates the core theme of the content. This label‚Äîfor example, ``revenue_projections_FY2024_Q3``‚Äîserves as the index key in the hash map. The second process generates a concise summary of the page.\n\n> This \"single-pass\" parsing allows the document to be structured for retrieval without the need for pre-computed embedding datasets. However, the cost of this precision is time. While a traditional cryptographic hash function $H(x)$ or an embedding model can process data in milliseconds, the semantic key generation in HashIndex requires significant inference time, typically 2 to 3 seconds per page.\n\n[...]\n\n> In HashIndex, the hash table is implemented in-memory, allowing for rapid access once the indexing phase is complete. The \"hash function\" in this context is the cognitive process performed by the LLM during key generation. This approach eliminates the need for complex tree rebalancing and multi-level traversal required by systems like ChatIndex or PageIndex. However, it places a higher burden on the \"agentic\" side of the retrieval process, as the agent must now navigate a flat list of keys rather than a hierarchical tree. \n\nDoes this look like an accurate summary of how it works? Might be worth calling out that the \"hash\" in this case is not a traditional hash in the way that word is usually meant, but an LLM-generated semantic \"tag\" of sorts.",
          "score": 1,
          "created_utc": "2026-01-24 22:38:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l7wdg",
              "author": "jasonhon2013",
              "text": "Haha generally speaking yea this summary is right. We call it hash is because the data structure we use behind is a hash table hahaha",
              "score": 1,
              "created_utc": "2026-01-25 09:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l7lf4",
          "author": "mister2d",
          "text": "Checking it out now with some pending energy legislation bills.",
          "score": 1,
          "created_utc": "2026-01-25 09:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yp85z",
          "author": "No-Lobster486",
          "text": "it would be much helpful if there is an relatively detailed explanation about the \"hash map to handle data\" part and a chart of the business flow.    \nthere are too many similar things these days, it will definitely help people make decision whether it will suit for their issue or worth trying",
          "score": 1,
          "created_utc": "2026-01-27 05:06:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h4awo",
          "author": "jschw217",
          "text": "Why does it require httpx? Any connections to remote servers?",
          "score": 0,
          "created_utc": "2026-01-24 18:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h4gs2",
              "author": "jasonhon2013",
              "text": "Oh it‚Äôs is for the purpose of fetching APIs like open router not connected to any remote server no worries !",
              "score": 2,
              "created_utc": "2026-01-24 18:57:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnjiym",
      "title": "SHELLper üêö: Multi-Turn Function Calling with a <1B model",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "author": "gabucz",
      "created_utc": "2026-01-26 15:43:32",
      "score": 23,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "We fine-tuned a 0.6B model to translate natural language into bash commands. Since it's tiny, you can run it on your laptop with complete data privacy.\n\nSmall models struggle with multi-turn tool calling - out of the box, Qwen3-0.6B achieves 84% accuracy on single tool calls, which drops to **just 42% over 5 turns.** Our tuning brings this to 100% on the test set, delivering robust multi-turn performance.\n\n|Model|Parameters|Tool call accuracy (test set)|=> 5-turn tool call accuracy|\n|:-|:-|:-|:-|\n|Qwen3 235B Instruct (teacher)|235B|99%|95%|\n|Qwen3 0.6B (base)|0.6B|84%|42%|\n|**Qwen3 0.6B (tuned)**|**0.6B**|**100%**|**100%**|\n\nRepo: [https://github.com/distil-labs/distil-SHELLper](https://github.com/distil-labs/distil-SHELLper)\n\nHuggingface model: [https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper](https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper)\n\n# Quick Start\n\nSet up the environment:\n\n    # Set up environment\n    python -m venv .venv\n    . .venv/bin/activate\n    pip install openai huggingface_hub\n    \n\nDowload the model:\n\n    hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model\n    cd distil_model\n    ollama create distil_model -f Modelfile\n    cd ..\n    \n\nRun the assistant:\n\n    python filesystem_demo.py\n    \n\nThe demo prompts for confirmation before running commands (safety first) and blocks certain dangerous operations (like `rm -r /`), so feel free to try it out!\n\n# How We Trained SHELLper\n\n# The Problem\n\nSmall models really struggle with multi-turn tool calling - performance degrades as tool calls chain together, dropping with each additional turn. If we assume independent errors for each tool call (like incorrect parameter values), a model at 80% accuracy only has a 33% chance of getting through 5 turns error-free.\n\n|Single tool call accuracy|5-turn tool call accuracy|\n|:-|:-|\n|80%|33%|\n|90%|59%|\n|95%|77%|\n|99%|95%|\n\nFor this demo, we wanted to test if we could dramatically improve a small model's multi-turn performance. We started with a task from the [Berkeley function calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) (BFCL) - the [gorilla file system tool calling task](https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json). We adapted it:\n\n* Original task supports multiple tool calls per turn ‚Üí we restrict to one\n* Cap at 5 turns max\n* Map commands to actual bash (instead of gorilla filesystem functions)\n* Skip adding tool outputs to conversation history\n\nBasically, the same tool set, but new, simpler [train/test data.](https://github.com/distil-labs/distil-SHELLper/tree/main/data)\n\n# Training Pipeline\n\n1. **Seed Data:** We built 20 simplified training conversations covering the available tools in realistic scenarios.\n2. **Synthetic Expansion:** Using our [data synthesis pipeline](https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&utm_medium=referral&utm_campaign=shellper), we generated thousands of training examples.\n\nSince we're dealing with variable-length conversations, we broke each conversation into intermediate steps. Example:\n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models\n    \n\n... becomes 2 training points:\n\n    [Input] User: List all files\n    [Output] Model: ls -al\n    \n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models`\n    \n\n1. **Fine-tuning:** We selected **Qwen3-0.6B** as the [most fine-tunable sub-1B](https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning) model with tool calling support on our platform.\n\n# Usage Examples\n\nThe assistant interprets natural language, generates bash commands, and can execute them (with Y/N confirmation).\n\n**Basic filesystem operations**\n\n    > python filesystem_demo.py\n    \n    USER: List all files in the current directory\n    COMMAND: ls\n    USER: Create a new directory called test_folder\n    COMMAND: mkdir test_folder\n    USER: Navigate to test_folder COMMAND: cd test_folder\n    \n\n# Limitations and Next Steps\n\nCurrently, we only support a basic bash tool set:\n\n* no pipes, chained commands, or multiple tool calls per turn\n* no detection of invalid commands/parameters\n* 5-turn conversation limit\n\nWe wanted to focus on the basic case before tackling complexity. Next up: multiple tool calls to enable richer agent workflows, plus benchmarking against [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html).\n\nFor your own bash workflows, you can log failing commands, add them to `data/train.jsonl`, and retrain with the updated data (or try a bigger student model!).\n\n# Discussion\n\nWould love to hear from the community:\n\n* Is anyone else fine-tuning small models for multi-turn tool calling?\n* What other \"focused but practical\" tasks need local, privacy-first models?",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1u3wq5",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 15:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u7b5a",
              "author": "gabucz",
              "text": "Thanks! Qwens tool call format - inputs are translated via transformers chat templating, and for outputs we have a converter to match what qwen does",
              "score": 1,
              "created_utc": "2026-01-26 16:00:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u7m4o",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 0,
                  "created_utc": "2026-01-26 16:02:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uk3mr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 16:55:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1upa98",
              "author": "gabucz",
              "text": "Exactly - we mention it in the post",
              "score": 1,
              "created_utc": "2026-01-26 17:17:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qoyoty",
      "title": "Why don‚Äôt most programmers fine-tune/train their own SLMs (private small models) to build a ‚Äúlibrary-expert‚Äù moat?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qoyoty/why_dont_most_programmers_finetunetrain_their_own/",
      "author": "Outside-Tax-2583",
      "created_utc": "2026-01-28 02:27:44",
      "score": 23,
      "num_comments": 35,
      "upvote_ratio": 0.73,
      "text": "AI coding tools are rapidly boosting development productivity and continually driving ‚Äúcost reduction and efficiency gains,‚Äù reshaping how programmers work. At the same time, programmers are often heavy users of these tools.\n\nHere‚Äôs my observation:\n\n* Most programmers may not be ‚Äúarchitect-level,‚Äù but many are power users of specific libraries/frameworks‚Äîtrue ‚Äúlib experts.‚Äù They know the APIs, best practices, common pitfalls, version differences, and performance/security boundaries inside out.\n* In theory, they could turn that expertise into data assets: for example, curate **1,000‚Äì5,000 high-quality samples** from real projects‚Äî‚Äúbest usage patterns, common mistakes, debugging paths, migration guides, performance optimizations, FAQs, code snippets + explanations.‚Äù\n* Then, by lightly fine-tuning or aligning an open-source base model (an SLM), they could create a ‚Äúlibrary-specialist model‚Äù that serves only that lib‚Äîforming a new moat in the AI era: **better than general LLMs for that library, closer to one‚Äôs engineering habits, more controllable, and more reusable.**\n\nBut in reality, very few developers actually do this.\n\nSo I‚Äôd love to hear from experienced engineers:\n\n1. Is this path **theoretically viable**? With **1,000‚Äì5,000 samples**, can fine-tuning reliably improve a model into a solid ‚Äúlibrary expert assistant‚Äù?\n2. What‚Äôs the main reason people don‚Äôt do it‚Äî**technical barriers** (data curation/training/evaluation/deployment), **ROI** (easier to use existing tools), or **lack of good tooling** (dataset management, evaluation, continuous iteration, private deployment)?\n3. If you think it‚Äôs viable, could you share a more **engineering-oriented, practical path** to make it work?\n\nI‚Äôm especially looking for hands-on, real-world answers‚Äîideally from people who‚Äôve done fine-tuning, private knowledge systems, or enterprise model deployments.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qoyoty/why_dont_most_programmers_finetunetrain_their_own/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o2570zq",
          "author": "CuteLewdFox",
          "text": "Because it's way easier to just use a RAG (or something similar to feed data to the model) instead of fine-tuning models. It's also way faster and more energy efficient. And doesn't need to be re-done on every new model release. Simply switch the model, and you're done (most of the time, not always).\n\nNote: I've built machine learning frameworks and applications for a few years now at my current job, and doing a lot of things with SLMs at home.",
          "score": 29,
          "created_utc": "2026-01-28 02:52:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o275g14",
              "author": "ScoreUnique",
              "text": "Hey, do you have some workflows to share ? Thanks.",
              "score": 3,
              "created_utc": "2026-01-28 11:47:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25bwiw",
          "author": "catplusplusok",
          "text": "RAG is more reliable for most tasks and doesn't risk breaking the model. The problem with fine-tuning is the quality issues are often not obvious until some time after starting to use the model, like it suddenly forgetting the context because you didnt have enough long samples",
          "score": 11,
          "created_utc": "2026-01-28 03:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25d7pd",
              "author": "Outside-Tax-2583",
              "text": "That makes me think: if I could capture the programming process content from Claude Code, and then automatically filter and categorize it, wouldn‚Äôt it be fascinating to fine-tune SLMs automatically?",
              "score": 0,
              "created_utc": "2026-01-28 03:26:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2554zt",
          "author": "Juan_Valadez",
          "text": "Because they're programming, not doing AI. I want to make a sandwich, not sow the tomato.",
          "score": 16,
          "created_utc": "2026-01-28 02:42:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25asj8",
              "author": "Outside-Tax-2583",
              "text": "If everyone ends up relying on the same general-purpose models and toolchains, the outcome is likely **rapid capability convergence**: outputs become increasingly similar and differentiation shrinks. Competition then shifts from ‚Äúwho‚Äôs more expert and more systematic‚Äù to ‚Äúwho‚Äôs cheaper and faster,‚Äù pushing the market toward **price wars** rather than genuine capability-based competition.",
              "score": -5,
              "created_utc": "2026-01-28 03:13:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o271l75",
                  "author": "Limebird02",
                  "text": "Whilst this may be likely, this isn't bad, this lowers cost for everybody else. Software has a very healthy ecosystem and other things evolve to compete, perhaps better than any other area.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:16:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25umo5",
                  "author": "elbiot",
                  "text": "Fine tuning a SLM won't be better than using a huge one with billions of dollars in development behind it",
                  "score": 3,
                  "created_utc": "2026-01-28 05:13:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2576nc",
          "author": "Odhdbdyebsksbx",
          "text": "What are the incentives for the programmers to do this? They're already an expert in the library, so obviously not for their own self use. Charge as a service for other people, seems kinda niche unless there's like a marketplace platform.",
          "score": 10,
          "created_utc": "2026-01-28 02:53:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25agb2",
              "author": "Outside-Tax-2583",
              "text": "I‚Äôve been thinking along the same lines: npm has roughly 3.8 million packages, but the people who truly understand a given library are usually its maintainers and a small group of core contributors. Since their knowledge often far exceeds that of a general-purpose LLM‚Äîcovering design trade-offs, edge cases, version evolution, best practices, and common pitfalls‚Äîwhy don‚Äôt we see them productizing this advantage more proactively in two directions?\n\n\n\n1. **Library-specific SLMs / model plugins**: distilling authoritative usage, migration guides, performance/security constraints, and common fixes into callable capabilities‚Äîso users get more reliable and consistent guidance.\n2. **Library-specific agent services**: delivering an end-to-end ‚Äúgenerate + verify‚Äù loop‚Äîauto-generate examples and validate them, automate cross-version upgrades, run lint/compat/security checks, pre-review PRs, etc.‚Äîsold via subscription or outcome-based pricing.\n\n\n\n\n\nMy intuition is that once these capabilities can be delivered in a standardized way, high-quality library teams would benefit disproportionately. Take a small team like Tailwind CSS: ‚Äúauthoritative knowledge + automated verification‚Äù could become a scalable service‚Äîsmoother monetization, more consistent UX, and lower support costs‚Äîrather than being reduced to a mere upstream data source for model giants.\n\n\n\nThe key question is: what‚Äôs the real friction? Maintainer bandwidth and ROI, heavy engineering and tooling requirements, high distribution/ops costs, or open-source community norms around commercial boundaries? This feels like a direction worth systematically exploring and validating.",
              "score": -7,
              "created_utc": "2026-01-28 03:11:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o25av3e",
                  "author": "SashaUsesReddit",
                  "text": "Is this response from AI?",
                  "score": 11,
                  "created_utc": "2026-01-28 03:13:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25ltka",
          "author": "Diligent-Union-8814",
          "text": "Any handful way to do this? Such as, running a single command produces the fine-tuned model.",
          "score": 3,
          "created_utc": "2026-01-28 04:16:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o259uq3",
          "author": "pinmux",
          "text": "Fine tuning needs a lot more memory than inference. ¬†Most people don‚Äôt own this kind of compute. ¬†Fine tuning takes quite a bit of time and if you‚Äôre renting enough GPU to do it, isn‚Äôt $0 and for small to medium sized models may easily run to hundreds or thousands of dollars (depending on how it‚Äôs done).\n\nThen, generating the thousands of inputs to perform the fine tuning also isn‚Äôt easy, cheap, or seemingly well understood by many people.¬†\n\nIt‚Äôs definitely interesting! ¬†It definitely could be powerful! ¬†But there doesn‚Äôt seem to be much publicity written about people doing it, yet.¬†",
          "score": 4,
          "created_utc": "2026-01-28 03:07:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25lsfb",
              "author": "Vegetable-Score-3915",
              "text": "Unsloth.ai and other approaches ie using quantised models exist to get away with fine-tuning with less resources, ie free tier Google Colab, Kaggle Notebooks etc.\n\nNot saying this as a counter point, what you have written is generally valid. But can get away with 16gb vram for smaller models.\n\nDeeplearning.ai has at least 1 good short course you can take for free showing how to finetune. Doesn't take long, just recommend a coffee and chocolate snacks to get through it in one sitting.\n\nOther startups are trying to make it easier to fine tune slms as well and tend to let you try it out for free. Distil Labs is one thst looks promising.\n\nI think Synalinks has also produced a similiar product very recently. Again making it easier to set things up.\n\nThere is plenty of scope for fine-tuning SLMs to become more of a norm. It will come down to the particular situation, but learning the code base, knowing the intended approach for the architecture etc, I imagine it would make sense as a viable option for what OP is describing, ie code review tasks etc. Fine tuned slms can be used as part of a range of models doing different things, ie could be used concurrently with larger more general models.",
              "score": 4,
              "created_utc": "2026-01-28 04:16:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27omwk",
                  "author": "pinmux",
                  "text": "Definitely all good points.  For small models <10B parameters or if QLoRA gives good results, then definitely Colab/Kaggle/home-lab GPUs could work well.\n\nCurrently, I view the 20-30B parameter models as being as small as I'd want to use for real work.  Things like devstral-small-2 or glm-4.7-flash look to have real promise, so fine tuning from those is quite interesting to me.\n\nI'm still learning a ton about this.  Like the OP, I don't understand why this isn't a more talked about idea.  At the very least, it seems like taking a small model and doing this kind of fine tuning and writing about it would be a great way for a new researcher to start to get noticed.",
                  "score": 2,
                  "created_utc": "2026-01-28 13:47:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25eear",
          "author": "HealthyCommunicat",
          "text": "I‚Äôve literally been doing exactly this. Got hired for a company that handles Oracle stuff. They have over 5000 guides and documents altogether of manageengine ticket solutions, guides, procedures, etc. - I literally think of it as a goldmine of high quality data when it comes to anything Oracle as its the accumulation of 8-10 years of customer service and technical support. Nobody has an Oracle expert model simply because Oracle would not approve of that whatsoever, so I‚Äôm doing the next best thing and just making one. Hopefully goes better than I expect, but we‚Äôre still formatting and organizing the massive library of examples. It‚Äôs been nonstop trial and error trying to figure out what kind of examples should count towards ‚Äúpre-training‚Äù using different groups of data to see what kind of outcome I get. It‚Äôs been insane amounts of trial and error just being the only person working on this in the past 2 months, if I‚Äôm honest I don‚Äôt have any true hope of for sure making something ususable.\n\nWorst case, I‚Äôll just go finetune a qwen moe variant.",
          "score": 4,
          "created_utc": "2026-01-28 03:33:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25sjxh",
              "author": "Torodaddy",
              "text": "Feels like you are building a better seat for a horse and buggy",
              "score": 3,
              "created_utc": "2026-01-28 04:59:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25mafg",
              "author": "Vegetable-Score-3915",
              "text": "Awesome and best of luck!\nFeel free to share when you have progress.",
              "score": 2,
              "created_utc": "2026-01-28 04:19:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ai3t7",
              "author": "ithkuil",
              "text": "Have you tried RAG? And benchmark it against Tavily search on the same questions.¬†",
              "score": 1,
              "created_utc": "2026-01-28 21:21:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o258rkr",
          "author": "Lame_Johnny",
          "text": "Out of the box coding models can usually do it well enough with a little guidance and documentation",
          "score": 2,
          "created_utc": "2026-01-28 03:02:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25rxn7",
          "author": "Torodaddy",
          "text": "Whats the point? You'll never curate more examples than the llm has already seen and what is the incremental value from that, even 5000 examples are small potatoes against something trained on all of github",
          "score": 2,
          "created_utc": "2026-01-28 04:55:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27311s",
              "author": "twjnorth",
              "text": "Not every codebase is on GitHub or uses languages that are a large part of training for foundation models. \n\nFine tuning a SLM on a specific application with examples from its coding standards, existing functions etc.. should prevent things like reinventing the wheel by creating a function that already exists in the codebase.",
              "score": 1,
              "created_utc": "2026-01-28 11:28:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a8jvp",
                  "author": "Torodaddy",
                  "text": "Im just saying you are going to spend time and money for a gain thats negligible. Most likely a negative ROI exercise",
                  "score": 1,
                  "created_utc": "2026-01-28 20:39:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2763bf",
          "author": "radarsat1",
          "text": "Test time training will do this, if it ever becomes a thing.",
          "score": 2,
          "created_utc": "2026-01-28 11:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aihzo",
              "author": "ithkuil",
              "text": "Actually I think this is coming within a few months based on a lot of continual learning work being focused on by high profile groups recently.",
              "score": 2,
              "created_utc": "2026-01-28 21:23:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26fxq6",
          "author": "Crazyfucker73",
          "text": "So you got ChatGPT to write that entire thing?",
          "score": 1,
          "created_utc": "2026-01-28 08:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26oqsm",
              "author": "Outside-Tax-2583",
              "text": "yes Ôºåi will first write content , and then let GPT check it and send it back to me.",
              "score": -2,
              "created_utc": "2026-01-28 09:23:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27wftk",
          "author": "WolfeheartGames",
          "text": "When you fine tune the model doesn't memorize the information. It embeds a compressed representation of some portion of the original information.",
          "score": 1,
          "created_utc": "2026-01-28 14:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o286mgt",
          "author": "East-Muffin-6472",
          "text": "I think it‚Äôs because fine tuning is difficult even with great libraries out there for the same \nSecond is dataset generation \nThird is to create a skill in antigravity to follow for a particular pattern when doing something so yea it‚Äôs a great project learning wise but not so much of a usage daily wise",
          "score": 1,
          "created_utc": "2026-01-28 15:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o298sss",
          "author": "verbose-airman",
          "text": "It‚Äôs just cheaper and easier to provide more context instead of fine-tune a model (and having to fine-tune a new model everytime the model is updated).",
          "score": 1,
          "created_utc": "2026-01-28 18:03:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ah9ud",
          "author": "ithkuil",
          "text": "You can't just provide the raw examples. You have to create a question and answer dataset. So it's a lot less convenient than you think.\n\n\nBut the real reason is that small models are just dumb. Their reasoning, abstraction and just general intelligence is not comparable to very large SOTA models and is generally insufficient for tasks that aren't fairly narrow. They are more brittle.\n\n\nAlso, RAG is much easier and works as well or better as long as you have do it right and have a strong model interpreting the results.",
          "score": 1,
          "created_utc": "2026-01-28 21:18:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpbxkh",
      "title": "You can now run Kimi K2.5 on your local device!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/nwp8ammpf3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 13:52:53",
      "score": 21,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpbxkh/you_can_now_run_kimi_k25_on_your_local_device/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2857sq",
          "author": "Egoz3ntrum",
          "text": "If you own a nuclear plant!",
          "score": 6,
          "created_utc": "2026-01-28 15:10:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o285qc5",
              "author": "yoracale",
              "text": "Runs on a 256ram Mac actually!\n\nAnd you can run it on lower requirements, it'll just be much slower",
              "score": 2,
              "created_utc": "2026-01-28 15:12:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28n6u1",
          "author": "yomohiroyuzuuu",
          "text": "So my old MacBook is definitely inadequate‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-28 16:29:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bd0jo",
              "author": "yeet5566",
              "text": "It isn‚Äôt the MacBook that is inadequate it‚Äôs your patience that is‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-28 23:47:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qlqvdh",
      "title": "Minimum hardware for a voice assistant that isn't dumb",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "author": "JacksterTheV",
      "created_utc": "2026-01-24 16:07:22",
      "score": 18,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "I'm at the I don't know what I don't know stage. I'd like to run a local LLM to control my smart home and I'd like it have a little bit of a personality. From what I've found online that means a 7-13b model which means a graphics card with 12-16gb of vram. Before I started throwing down cash I wanted to ask this group of I'm on the right track and for any recommendations on hardware. I'm looking for the cheapest way to do what I want and run everything locally ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1gbeqd",
          "author": "LowTip9915",
          "text": "I have a 7b on an m2 Mac mini and it‚Äôs acceptable, but I‚Äôm not using voice (just llama and open web ui). The 14b runs, but is painfully slow, however the difference in answers is evident, so I use it occasionally when the 7b answer is overly vague/top level.  (Disclaimer also a noob to this)",
          "score": 5,
          "created_utc": "2026-01-24 16:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h01e9",
          "author": "Boricua-vet",
          "text": "First off, you do not need a model to do what you want to do. Voice assist can do pretty much what you want and in the case that you have unique requests, you can just add those by hand. Even the personality part can be creating using custom answers. The only downside is that it can only do what you program it to do and you will not be able to have conversations with it. If you need conversations, then you will certainly need a model but you don't need a 13b to do what you want. You do not even need a GPU as long as you have a good CPU and fast ram and enough ram to load the model. \n\nSince I do not know your hardware specs, I recommend you starting with [https://huggingface.co/unsloth/Qwen3-4B-GGUF?show\\_file\\_info=Qwen3-4B-Q4\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q4_K_M.gguf)\n\nYou can run that on CPU and get really good speed depending on your hardware. Example on my hardware using only CPU I can get 100+ tokens per second on this model but depends on your hardware.\n\nThat's the cheapest way to do it.\n\nNow if you still want to be cheap and use GPU\n\nGet a P102-100 for 60 bucks.\n\n[https://www.ebay.com/itm/156284588757](https://www.ebay.com/itm/156284588757)\n\nyou can load piper and whisper using hardware acceleration and get responses from HA in milliseconds.  For this purpose , you do not need to buy a stupid expensive card. 60 bucks will do better job than any 3060, 4060 and 5060 for this particular application and use case.\n\nwith one P102-100, you can run Piper, Whisper and Qwen 4b and get everything you wanted to do.\n\nnow, if you really want to go nuts, you can get two and then you can run Qwen30b and get these speeds.\n\n`llamacpp-server-1  | ggml_cuda_init: found 2 CUDA devices:`\n\n`llamacpp-server-1  |   Device 0: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  |   Device 1: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  | load_backend: loaded CUDA backend from /app/libggml-cuda.so`\n\n`llamacpp-server-1  | load_backend: loaded CPU backend from /app/libggml-cpu-piledriver.so`\n\n`llamacpp-server-1  | | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |`\n\n`llamacpp-server-1  | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           pp512 |        968.28 ¬± 4.69 |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           tg128 |         72.10 ¬± 0.23 |`\n\n`llamacpp-server-1  |` \n\n`llamacpp-server-1  | build: 557515be1 (7819)`\n\n`llamacpp-server-1 exited with code 0`\n\n  \nabout 1,000 in prompt processing and 70+ tokens generation.\n\ntrust me, for home assistant, you will not need more than that and at that price, you will not find a better solution.\n\nAsk people that actually own P102-100, don''t take my word for it.\n\nFor this use case.... this is it.",
          "score": 5,
          "created_utc": "2026-01-24 18:38:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h9az6",
              "author": "JacksterTheV",
              "text": "Awesome, I'm going to go the CPU route and see what I can get working. Thanks for the recommendation.¬†",
              "score": 1,
              "created_utc": "2026-01-24 19:18:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1kwfhl",
              "author": "MakerBlock",
              "text": "Fantastic write up!",
              "score": 1,
              "created_utc": "2026-01-25 07:25:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1nfx91",
                  "author": "Boricua-vet",
                  "text": "Thank you. Really appreciate that. Just trying to help.",
                  "score": 3,
                  "created_utc": "2026-01-25 17:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ga8q2",
          "author": "tillemetry",
          "text": "Anyone try this with an M4 Mac mini?  You can run it headless, it doesn‚Äôt take up much space and the noise and power draw is minimal.  LM studio will point you at ai‚Äôs optimized for MLX.",
          "score": 5,
          "created_utc": "2026-01-24 16:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gbm53",
          "author": "PermanentLiminality",
          "text": "There is no single correct answer.\n\nYou left out a few parts.  You need the audio in and out.  A bluetooth speaker can work.  Now you need the Speech to text (STT)  and Text to Speech(TTS) in addition to a LLM.  Both the TTS and STT need VRAM, but there are TTS like Piper than can actually run on the CPU.\n\nOne nice thing here is most voice assistants don't do a lot of context so you only need say 15% more VRAM than the LLM.\n\nYou will need to experiment to find what works for you.  Put a few bucks into OpenRouter and try the models.  A possible example would be mistral 3 8B which fits in about 7GB of VRAM.  They free models too.   Find what will work for what you want.\n\nIf I ever get some time, I plan on doing what you are trying to do.  My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU.  Not sure if I can get a smart enough model STT and TTS , but I want to try.  That setup will idle around 10 watts.  I may consider picking up a 24GB P40 as it also is pretty low idle power.   The P40 is around $200 after the needed fan to cool it.\n\nMy power is crazy expensive so cheap 16 GB GPUs like the P100 or CMP100-210 idle at 40 or 50 watts.  That is like $200/yr for me.\n\nAnyway, before dropping coin on a GPU, test out a solution with available tools like Openrouter and Huggingface.  For a project like this $10 can go a long way for testing.",
          "score": 2,
          "created_utc": "2026-01-24 16:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ht9nr",
              "author": "Boricua-vet",
              "text": "\"My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU. Not sure if I can get a smart enough model STT and TTS\"\n\nyou sure can.. I will go as far as give you the models I am using with hardware accel that works for me.  \nfor piper PIPER\\_VOICE=en\\_US-libritts-high  \nfor whisper  WHISPER\\_MODEL=distil-medium.en  \nthose two will consume 3.5GB of vram for both. 3.5 total for both.\n\nMy only suggestion is to use really good microphone like [https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html](https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html)  this has the xmos chip that will clean all the input even with loud music or background noise. It works fantastic...\n\non the P102-100 with those models and that mic, your response time from HA after your command should be in the milliseconds. Not even a second like half a second or less, extremely responsive, that is unless you are running it in an under powered device.\n\nI run this setup flawlessly.",
              "score": 3,
              "created_utc": "2026-01-24 20:50:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gn24j",
          "author": "JacksterTheV",
          "text": "To add some more context I'm using home assistant and have the voice pipeline worked out. I have a hacked echo show that I can talk to like Alex. The problem is right now it takes about 30 seconds for something as simple as locking the back door. I can connect home assistant to any local LLM. This is all running on an old laptop. Ultimately I want everything to run locally but I really like the idea of spending a few bucks on a cloud service to figure out what I need before dropping cash on hardware.¬†",
          "score": 2,
          "created_utc": "2026-01-24 17:43:19",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1haztq",
              "author": "Blizado",
              "text": "I actually tend to use a Qwen3 30B A3B Instruct model in ~2-4bit. I tried it with Q2_K in GGUF format with KoboldAI, needs with a small KV Cache around 12GB VRAM (Q4 would be around 16-18GB VRAM) and I was surprised how good it still is, but even more surprised how crazy fast it is (at least on my RTX 4090). Thanks to its MoE pipeline, where only 3B of the model is active, so it is mostly as fast as a 3B model at Q2_K would be, but of cause way better, you don't need to try a 3B at Q2_K, it will be very very bad but crazy fast. It's hard for me to use dense models now, they are so slow in comparison.\n\nSo as long you don't want to have a realtime conversation with the LLM, you can also split a some layers of the model into normal RAM. It slows down the model a bit but if you only put ~10% of the model in RAM it should stay fast enough so you can bring down the SmartHome reaction to maybe 5 seconds or lower. The only question is if you need tool calling, that could break on Q2_K, but there are ways to work around that.\n\nBut I would go for a solid 16GB VRAM card (RTX 40xx/50xx series), not too much on the low end, then you can put the LLM completely into the VRAM. So maybe a 5070TI with 16GB VRAM. No clue if a 5060TI is still fast enough. I \"only\" have a 4090 and a 5080. Could be if you get the full model into VRAM, because if not, the 5060TI has only PCIe 5 8x, the 5070TI has PCIe 5 16x, but that is only really relevant if you use offloading (splitting the model into RAM) or if the GPU is in a PCIe 5 x16 slot. If you need offloading, better don't put a PCIe 5 8x GPU into a PCIe 5 motherboard setup.\n\nBut beside that, it really depends what you want to exactly do. If you only want the bar minimum, for example only some different phrases that should react for one SmartHome function, you can do that with much much less hardware and a tiny AI model that only recognize what you want to do. Such an AI didn't need to be that smart at all, only always recognize what you want from it, you can even give it a bit the illusion of personality. But if your plan is to build your SmartHome more and more out, then a LLM is surely not a bad decision, but a more costly one.",
              "score": 1,
              "created_utc": "2026-01-24 19:26:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i0p7u",
          "author": "atlantageek2",
          "text": "currently developing something similar for work using whisper. dev machine at home is my m4 mac mini 16gb. theres like a 3 second delay but other than that it works well. its using whisper",
          "score": 2,
          "created_utc": "2026-01-24 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i0zr0",
              "author": "atlantageek2",
              "text": "forgot to mention whisper cannot use max gpu",
              "score": 1,
              "created_utc": "2026-01-24 21:27:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ihhpy",
          "author": "Powerful-Street",
          "text": "LFM2.5-audio if you can figure out how to get it to work, other than in the demo. It‚Äôs only ~3GB and does a pretty good job. The other thing you could do is, inject personaplex weights into moshi. Takes much more work and ram though.",
          "score": 1,
          "created_utc": "2026-01-24 22:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1izk91",
          "author": "PermanentLiminality",
          "text": "Good to know.   Now I just need some time...",
          "score": 1,
          "created_utc": "2026-01-25 00:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r35e0",
          "author": "Bino5150",
          "text": "I'm running an 8b model locally with LM Studio/AnythingLLM, and using Piper for TTS. I'm running an HP Zbook Studio G7 laptop on Linux Mint, with an i7, 16GB ram, and an Nvidia Quadro T1000 Mobile gpu with 4GB vram. It runs just fine. I can do SST via the laptops mic, but I haven't configured home assistant yet as I've just got this up and running. Thinking of setting up the SST on a Raspberry Pi.",
          "score": 1,
          "created_utc": "2026-01-26 03:17:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qovbrq",
      "title": "Will.i.am is promoting running local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qovbrq/william_is_promoting_running_local_llms/",
      "author": "beefgroin",
      "created_utc": "2026-01-28 00:05:26",
      "score": 18,
      "num_comments": 10,
      "upvote_ratio": 0.82,
      "text": "And fine-tuning for that matter. Or at least that‚Äôs how I understood what he was saying lol. What do you think?\n\n[ https://youtu.be/sSiaB90XpII?t=384 ](https://youtu.be/sSiaB90XpII?t=384)\n\nStarts at 6:25. But the whole interview is worth watching too.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qovbrq/william_is_promoting_running_local_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o24i87j",
          "author": "SocialDinamo",
          "text": "He was always big on encouraging kids to learn to code so this makes sense. Great to see!",
          "score": 8,
          "created_utc": "2026-01-28 00:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24yf78",
              "author": "ForsookComparison",
              "text": "Juniors blaming A.I. for their degree and livelihood being worthless but real ones know that it was that one Bill Gates and Will.I.Am video from like 2011 that saturated their market.",
              "score": 4,
              "created_utc": "2026-01-28 02:07:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28kaf4",
                  "author": "Tired__Dev",
                  "text": "It didn‚Äôt though. I still come across execs, product managers, product owners, project managers, and managers that have no fucking clue what their software company does. The 2010‚Äôs tech companies were absolutely littered with smooth talking bureaucrats that offered nothing to help capture new market share. The problem with software is, and mostly always has been, that it operates on a Ponzi scheme of funding that cycles on interest rates and macro economic conditions. \n\nThere almost no overhead to running a software org comparatively to a traditional brick and mortar. It‚Äôs mostly employment. There‚Äôs nothing stopping people from grouping together and creating a startup. But the big problem is that people viewed tech through the lens of working at a big company and making six figure salaries and not being innovative themselves. Now many of them couldn‚Äôt actually create anything themselves because their coding skills are almost bureaucratic in a sense. \n\nLLMs pose risk to people who believe that innovation stopped in 2021. The stick of innovation just extends with tooling and bigger things can be accomplished. An over exaggeration would be if a few people now can program Facebook that would scale then a Facebook amount of people would be able to program the matrix. (Again heavily over exaggerating to make a point) \n\nSoftware developers aren‚Äôt becoming useless. Bureaucrats are. Keep learning code. It will be everywhere.",
                  "score": 2,
                  "created_utc": "2026-01-28 16:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o260a98",
          "author": "rditorx",
          "text": "It's Wi.llm.ai for you",
          "score": 5,
          "created_utc": "2026-01-28 05:53:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24lf20",
          "author": "sizebzebi",
          "text": "give me the ram for it tried one my rtx 4070 and it's utter trash",
          "score": 2,
          "created_utc": "2026-01-28 00:58:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25dr6q",
              "author": "Count_Rugens_Finger",
              "text": "i7-6700K, 32GB DDR4-3000, RTX 3070 8GB over here.  I can run several interesting models at reasonable speeds.",
              "score": 4,
              "created_utc": "2026-01-28 03:29:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27qdit",
                  "author": "vipx237",
                  "text": "Which models ?",
                  "score": 2,
                  "created_utc": "2026-01-28 13:56:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkmv44",
      "title": "DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog",
      "subreddit": "LocalLLM",
      "url": "https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage",
      "author": "EchoOfOppenheimer",
      "created_utc": "2026-01-23 09:59:43",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.64,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkmv44/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/",
      "domain": "introl.com",
      "is_self": false,
      "comments": [
        {
          "id": "o18oklh",
          "author": "ForsookComparison",
          "text": "> 12 upvotes\n\nFor a sub about LLMs this sub is pretty shitty at spotting bots",
          "score": 3,
          "created_utc": "2026-01-23 14:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iqcip",
          "author": "HealthyCommunicat",
          "text": "I‚Äôve been trying to talk about DS 3.2 and LongCat 2601 more as they genuinely seem to be on a higher level than GLM or MiniMax when it comes to knowing very vast specific niche knowledge, they‚Äôre the only two models I trust for work, as they‚Äôre the only two models so far that get questions about Oracle EBS correct, as most of Oracle‚Äôs software is proprietary and existing documentation is absolutely horrendous. - these two models can answer questions that even GPT 5.2 has trouble answering if I specifically tell it not to use any kind of search.",
          "score": 1,
          "created_utc": "2026-01-24 23:32:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22znhz",
          "author": "JimmyDub010",
          "text": "Only rich people can run it though.........",
          "score": 1,
          "created_utc": "2026-01-27 20:22:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkxpuo",
      "title": "AMD Ryzen AI Software 1.7 released for improved performance on NPUs, new model support",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/AMD-Ryzen-AI-Software-1.7",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-23 17:52:03",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkxpuo/amd_ryzen_ai_software_17_released_for_improved/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qpp820",
      "title": "ClawdBot / MoltBot",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qpp820/clawdbot_moltbot/",
      "author": "Normal-End1169",
      "created_utc": "2026-01-28 21:51:59",
      "score": 16,
      "num_comments": 20,
      "upvote_ratio": 0.79,
      "text": "Just stumbled across this tool today from my Co Founder in one of my startups so being techy I decided to give it a quick peak.\n\nAm I missing understanding the purpose of the tool? We're running a local process that is interacting with external AI APIs to run local tasks that actively interact with your file system????? I mean cool I guess but one doesn't sound to safe, and 2 all your local data is ending up on a server somewhere.\n\nSeriously even tried to create some sort of use case, maybe help me with file sorting on a Linux machine, managing servers but it just feels so wrong personally.\n\nMaybe someone can enlighten me because I don't fully understand why you would want a AI actively interacting with your entire file system.  \n  \n ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qpp820/clawdbot_moltbot/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o2aqsex",
          "author": "Nzkx",
          "text": "I can see 2 easy disasters from such project :\n\n\\- Data send through public network on your behalf, when you want to keep them private.  \n\\- Data erased or altered, when you certainly don't want to erase or modify them.\n\nI guess we can assume the dev ain't that naive, and put railguard against dangerous commands. But nothing can be certain unless you deep dive into the source code and the docs. I would expect something like a notification to allow/disallow dangerous commands when the agent is about to perform weird stuff, but this quickly get annoying if your phone start to ring at 4am because the agent is stuck waiting for your input.\n\nRegular backup would help a lot against the second problem.",
          "score": 5,
          "created_utc": "2026-01-28 21:59:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2as7t1",
              "author": "Normal-End1169",
              "text": "Well even then, it's not like majority of users are using local LLMs ran from like Ollama. There going to be connecting to external API's like OpenAI, so in theory your entire PC is accessible and sent to a OpenAI server somewhere.\n\nIn terms of Data privacy and exactly what you said users data being sent away when they aren't aware or didn't want to is crazy in my mind.\n\nCool tool, but seems like a massive issue just waiting to happen.",
              "score": 1,
              "created_utc": "2026-01-28 22:05:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2b6s7l",
                  "author": "Outdatedm3m3s",
                  "text": "That‚Äôs why alot of people are buying dedicated computers for these with their own accounts for everything. This way they aren‚Äôt using any of your information.",
                  "score": 1,
                  "created_utc": "2026-01-28 23:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2apdrw",
          "author": "Normal-End1169",
          "text": "On top of all this I mean we're in theory letting publicly open chats / AI access interact with local system.\n\nCan't even imagine all the CVE's coming out for this in less than a month time",
          "score": 7,
          "created_utc": "2026-01-28 21:53:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2arq93",
              "author": "colin_colout",
              "text": "yep... not in theory.  The vulnerabilities are VERY practical.  2026 will be \"The year of the prompt injection email phishing\".  \n\nJust watch Low Level's video [https://www.youtube.com/watch?v=kSno1-xOjwI](https://www.youtube.com/watch?v=kSno1-xOjwI) and you'll see why.",
              "score": 8,
              "created_utc": "2026-01-28 22:03:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2aslnc",
                  "author": "Normal-End1169",
                  "text": "Yup, this is insane that we are actively opening any device up to prompt injection.\n\nI actually am in school for cyber and this is just mind blowing.",
                  "score": 3,
                  "created_utc": "2026-01-28 22:07:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2arsc4",
          "author": "pandodev",
          "text": "I think what's cool about it also makes it completely unsafe and not to be used which is it has access to the whole environment machine and tweak itself. so giving it its own GitHub for example and telling it to do projects for you, or set cron jobs to give you news basically like a true assistant but is nothing insane. Also NOONE should be running this on their home network, best it to securely sandbox it in an ec2 with only able to access via sessionmanager. Maybe I am too paranoid but this uses dependencies if any of those dependencies get infiltrated would you rather them be in a aws secluded server or your home network?",
          "score": 2,
          "created_utc": "2026-01-28 22:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2astjo",
              "author": "Normal-End1169",
              "text": "I think maybe if your using Local LLMS at least your removing the external data issue, but even then your giving a chat full control over your PC. This is not safe on a EC2 or any sort of VPS lol.\n\nYou would trust a chatbot to be ran on a publicly accessible machine that has full access to a machine??",
              "score": 2,
              "created_utc": "2026-01-28 22:08:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2az6e0",
                  "author": "pandodev",
                  "text": "if is a secluded EC2 or for the sole purpose not publicly accessible like I said with session manager how? with local llm in your home network will not mean anything if dependencies in the library are infiltrated you just gave the hacker access to your entire home network. that's not the case with a secluded vps all they would be able to access is secrets and API Keys that are stored in that machine and that machine alone so like your Claude api key.",
                  "score": 1,
                  "created_utc": "2026-01-28 22:37:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2bp354",
              "author": "eli_pizza",
              "text": "Not paranoid enough if you‚Äôre giving it access to anything at all that you care about (like your email inbox or text messages!). \n\nYou‚Äôre worried about software supply chain attacks, but the app itself is fundamentally insecure.",
              "score": 1,
              "created_utc": "2026-01-29 00:50:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2axorp",
          "author": "Echo_OS",
          "text": "This is why some people prefer tiny / narrow models.\nNot because they're smarter, but because the responsibility radius is small.\n\nClear \"can't do\" > more capability.\nBounded agents are easier to trust than general ones with full FS access.",
          "score": 1,
          "created_utc": "2026-01-28 22:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b0zmk",
          "author": "brimanguy",
          "text": "That's crazy ... Might as well run a local server and make every directory and sub directory public to the internet ü§£",
          "score": 1,
          "created_utc": "2026-01-28 22:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b5412",
          "author": "Normal-End1169",
          "text": "Going to keep this as a bit of a time piece and edit as I go;\n\nHere's a list of articles / videos regarding security issues so far;\n\n[https://www.youtube.com/watch?v=7GS6Xs4hdvg](https://www.youtube.com/watch?v=7GS6Xs4hdvg)  \n[https://www.aikido.dev/blog/fake-clawdbot-vscode-extension-malware](https://www.aikido.dev/blog/fake-clawdbot-vscode-extension-malware)  \n[https://www.youtube.com/watch?v=kSno1-xOjwI](https://www.youtube.com/watch?v=kSno1-xOjwI) (Thanks to colin\\_colout for sharing)  \n[https://lukasniessen.medium.com/clawdbot-setup-guide-how-to-not-get-hacked-63bc951cbd90](https://lukasniessen.medium.com/clawdbot-setup-guide-how-to-not-get-hacked-63bc951cbd90)",
          "score": 1,
          "created_utc": "2026-01-28 23:06:56",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2b6b4b",
          "author": "Momsbasement13",
          "text": "The catch 22 with this bot is that if you seclude it within a sandbox it kinda loses the majority of the features making it stand out. For clawd/molt to actually shine, it requires to have all this access otherwise it is just another slightly different LLM.  \nI feel like we currently lack proper control and railguards for agents like these to be mainstream. A lot of people will pay in blood for it to be.",
          "score": 1,
          "created_utc": "2026-01-28 23:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6s8a",
              "author": "Normal-End1169",
              "text": "We'll like others mentioned the main issue is prompt injection;\n\nOnce prompt injection exists and is taken more serious it may be a consideration for actual use but now this  such a dumb idea to use. For example in one of the videos linked in this thread, the creator talks about his buddy who had his email connected which then he proceeded to email his email from his wife's account pretty much saying it's him and asking to turn of specific music on spotify which it proceeded to do.\n\nWhat's stopping someone from using the same method to read out sensitive files.\n\nNot to mention all the APIs/Credentials used to connect the external applications are stored in plane text.",
              "score": 1,
              "created_utc": "2026-01-28 23:15:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2be6au",
          "author": "HealthyCommunicat",
          "text": "Your explanation is literally all it is, I can swear and bet everything I own that the only actual reason this got popular is because it has a GUI. There have been so many options to do literally the exact same thing clawdbot is able to do - I say this LITERALLLY because I had something NEAR EXACTLY the same, a slack bot that‚Äôs hooked up to a llm endpoint thats running on the same machine able to act autonomously - in fact, you literally wouldn‚Äôt be able to tell the two apart. I moved over to clawdbot because it has more integration but everyone is hyping it up when they 1.) dont even realize you‚Äôve been able to do this for the past year. 2.) they think its something new and great because it has a GUI.\n\nI love watching people jump into setting up clawdbot thinking its a magic tool that will solve all their answers and needs, only for them to realize you need to setup tools, literally the same exact way you would need to for any other automation. People literally do not want to learn and keep wanting the fastest easiest cheatiest way to have fully autonomous AI that is tailored for their own needs, while not willing to actually learn or even understand what it is they need to do.",
          "score": 1,
          "created_utc": "2026-01-28 23:53:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bekym",
              "author": "Normal-End1169",
              "text": "That was my thought process. Seems like a over glorified MCP lol.\n\nJust AI interacting with data, expect this time it's not just a email, or maybe a CRM. It's a MACHINE!\n\nCouldn't tell you about older stuff like this tho never really went beyond MCP.\n\nWait till ppl find out 95% of these automations can just be coded with simple languages like python.",
              "score": 2,
              "created_utc": "2026-01-28 23:56:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2bfj5p",
                  "author": "HealthyCommunicat",
                  "text": "Its exactly that, just a ton of single file mcp tools made in python. All these new terminology and unnecessary lingo and overhyping is making clawdbot out to be something that hasnt already existed for so long. It just irks me in a wrong way when I keep seeing people claiming out of pure arrogance and ignorance that something is amazing and new when it‚Äôs been around for so long, if they bothered to learn in the first place",
                  "score": 2,
                  "created_utc": "2026-01-29 00:01:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnmc4s",
      "title": "Building my first LLM HomeLab, where to start?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnmc4s/building_my_first_llm_homelab_where_to_start/",
      "author": "rmjcloud",
      "created_utc": "2026-01-26 17:20:29",
      "score": 13,
      "num_comments": 18,
      "upvote_ratio": 0.86,
      "text": "Hey all! üëã\n\nI‚Äôm looking to delve into AI and local LLMs head first with the aim eventually of building some cool AI/LLM apps for self learning.\n\nI wanted to see if anyone had some good recommendations of hardware for a homelab, preferably on the mid-starter end of budget.\n\nSpecifically CPU, GPU and RAM suggestions so i can test the water to see how much i need to spend to build a decent lab for running local LLMs with Ollama to kickstart my AI journey and learning!\n\nGaming orientated GPUs not necessary but a nice compromise for gaming too i guess!\n\nBudget 1-2k GBP¬£.\n\nI have an M3 MBP and an M2 Macbook Pro, both with around 16GB RAM, are these any good for achieving this? Small scale is fine, i just want to get to grips with LLM concepts locally and digging into how they work, tuning and deploying apps in an AIOps approach!\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnmc4s/building_my_first_llm_homelab_where_to_start/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1uu7rb",
          "author": "Kyuiki",
          "text": "I did my research on this and there is a single conclusion I made. Don‚Äôt waste your money on it. Like really. You can never run the amazing models you see like GPT, Claude, DeepSeek, GLM in a way that feels good. You can have 4 video cards costing 10 grand which will LOAD the model but you‚Äôll still be generating text at 15ish/tokens a second. You‚Äôll be running quantized models that adds additional hallucinations to LLM‚Äôs that already hallucinate a bunch.\n\nYou can get some really really cheap cloud provided services that give API access to some amazing models. In most cases what you spend on your own hardware would take 20+ years to spend on a subscription.\n\nNow if you‚Äôre doing it for pure hobby, start with GPU power. Memory will help LOAD a model and can increase max context size but it will not make the model run faster. GPU offloading is the only thing that will increase tokens/sec in a way that matters. The higher the VRAM on the GPU the better.",
          "score": 14,
          "created_utc": "2026-01-26 17:39:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v2knb",
              "author": "catplusplusok",
              "text": "Depends? I have NVIDIA Jetson Thor which is kind of pricey, but not 10K. I can run Qwen3-Next lighting fast, FP4 quantized but seems perfectly capable, or an uncensored GLM-4.5-Air model for storytelling/roleplay which is 15tps as you mentioned, but worth it when cloud models would lecture you instead of writting stories or you do high volume batch data processing. Mac is another good choice for affordable-ish local AI.",
              "score": 3,
              "created_utc": "2026-01-26 18:15:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1usnrb",
          "author": "PermanentLiminality",
          "text": "The range is from a cheap GPU like a P102-100 for $50 in whatever computer you already have, to $50k or more for a serious rig.   You really need to start with some kind of budget and what you are trying to run. \n\nTo get started it is almost always the answer to start with an API provider like Openrouter and only look to buying hardware once you have some idea of what you actually need.  In almost all circumstances it will cost a lot more to buy hardware than pay for an API.",
          "score": 5,
          "created_utc": "2026-01-26 17:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uvn2n",
              "author": "rmjcloud",
              "text": "updated!",
              "score": 1,
              "created_utc": "2026-01-26 17:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uvs5c",
          "author": "GoodRPA",
          "text": "I've tried to build local LLM/SML just in the last few weeks.\n\nMy recommendation would be:\nEither don't worry about GPU, responses will be slow, but very stable on CPU + ram. \n\nIf you do decide to go with GPU, get more vram(12gb, 16gb, 24gb) and a relevantly recent/supported GPU architecture from 2018+.\n\nBets options with balance of vram/price/power consumption/architecture: \n\nNvidia t4 - 16gb, 70watt only!, Turing, requires fan mode/cooling\nNvidia RTX 3090 - 24gb, 350 watt , Ampere\n\nBudget:  \nAgain, either CPU/shared ram (M1 mini, 16gb)\nEven hp thin clients (t520, t730) can manage CPU models, slowly but these work and very stable.\n\nNvidia RTX A2000 - 12gb, 70watt, ampere\n\nInstead of getting 6gb and below, you can use CPU only or even use remote LLMs, these are good quality, whether we like/can run them locally or not )\n\nBuild everything else around this. 16gb ram is a must.",
          "score": 4,
          "created_utc": "2026-01-26 17:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ursk3",
          "author": "gittb",
          "text": "Budget numbers would help, ranges from a couple grand to a down payments on houses for the starter to mid range as far performance goes.",
          "score": 2,
          "created_utc": "2026-01-26 17:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uvmmz",
              "author": "rmjcloud",
              "text": "updated!",
              "score": 1,
              "created_utc": "2026-01-26 17:45:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wqb72",
          "author": "its_a_llama_drama",
          "text": "This depends on a lot of variables.\n\nHow much faff and fiddling are you realistically willing to tollerate?\n\nDo you want to be able to scale up in the future without upgrading everything? \n\nWhat exactly do you want to do with LLMs? Use them and if so how? Fine tune them? Try and train a model yourself?\n\nAre you only bothered about LLMs? Or does image generation interest you? This changes things considerably\n\nI started with your budget in mind, and before i knew it, i spent quite a bit more. \n\nIt depends what you are expecting this machine to do. That budget is tight. You could get a consumer board, cpu and single card running smaller models with high throughput, with ddr5 and pcie 5.0 for that price too.\n\nOr you could go for the old server route and go for xeon dual socket, ddr4 ecc ram and pcie 3.0. ddr4 ram is still expensive, just not as expensive as ddr5. But this option (if you choose the right board), gives you nore optionality later and slower but bigger capacity and future expandability.\n\nThey both serve different purposes. \n\nAlso, some gpus are cheaper for a reason. You can get cheap cards, but they don't have all the bells and whistles a newer card might have. At this budget, you have to choose between throughput and vram. At 2k, you might squeeze two 3090s into the build budget and maybe 4x32GB ECC RDIMMs as well, if you go with a reasonably priced lga 2011 board and xeon v4 cpus.  But factoring in psu(s), storage, cooling, a case or open frame for it, it will be tight. \n\nIf you go for an older card than a 3090 (like the P40) you might find performance disappointing, they lack fp16 performance. but again, it depends what you expect. Do you need it to be pushing 50+ tok/s. You might, if you get the cards and then realise you do, you will feel like it is a let down.  \n\nIf you go for older MI50 AMD cards, you get a lot of VRAM for your money (if you can find 32GB variants), but you pay with set up time and compatability instead. \n\nI think if I were you. I would start off with one of the better e5 v4 xeons on a dual socket lga2011-3 board, probably look for 2699 or if too expensive, 2698 v4. buy 4x32GB RAM, and buy one 3090. Use it, push the limits and then you will know if you need more throughput or more vram. Once you know what you actually need, you can choose what to change\n\n you can probably sell the 3090 for what you bought it for if you decide you want to build around a different card.\n\nYou can add more ram, most dual socket boards have 12 or 16 dimm slots.\n\nyou can add more 3090s if one was good but you want more vram (try and get a board with plenty of x16 pcie slots)\n\nOr if the cpu/platforn is the limitation, you didn't lose too much after you sell the board and cpus, and you can weigh up costs for a newer platform (considerably more)\n\nThis leaves as many options open whilst allowing you to play with a fast gpu with 24GB VRAM and good compatability, and 128GB of RAM. \n\nYou should be able to build that for 1k (just about) and use the other 1k if and when you know what you need.",
          "score": 2,
          "created_utc": "2026-01-26 22:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21bs8f",
              "author": "Akimotoh",
              "text": "Damn, what is your spending budget?",
              "score": 1,
              "created_utc": "2026-01-27 16:02:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22zeiz",
                  "author": "its_a_llama_drama",
                  "text": "All in i probbably spent closer to 3.5k, but i already had 4TB of 990 pro storage from an old build.\n\nlooking back, i regret choosing lga3647, i do not need ghe more powerful cpus it unlocks and it adds complexity for cooling if you aren't actually building a server. There are very few quiet cooling solutions for this platform and the ones that are quiet are expensive. I made ghe mistake if buying a mother board which has two very tightly spaced x16 slots, so if i want more gpus, i shoukd probably get a new board, rather ghan cramming gpus into x8 slots with adapters.\n\nIt did not gain me anything significant. I have 2x xeon 8276 which are significantly better than any cpu on lga 2011-3, but the platform is still ddr4 and it is still pcie 3.0. i should have gone with amd epyc at this price point, but i didn't realise this was my budget untill i finished adding bits",
                  "score": 1,
                  "created_utc": "2026-01-27 20:21:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uw7xt",
          "author": "catplusplusok",
          "text": "Best used Apple Silicon Mac you can find (in terms of maxing out RAM). This will give you much better bang for the buck then PC GPUs and you get a computer useful in many other ways. \n\nThere is also a [64GB NVIDIA dev kit](https://www.amazon.com/NVIDIA-Jetson-Orin-64GB-Developer/dp/B0BYGB3WV4?th=1) within your budget, but beware of limited memory speed (so you can run bigger models but slowly) and need to build lots of things from source for custom compute.",
          "score": 2,
          "created_utc": "2026-01-26 17:48:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxjk4",
              "author": "rmjcloud",
              "text": "I have an M3 Macbook Pro, would this be enough to play around with small models to learn fundamentals and things such as RAG and how to expose LLMs for app usage etc?",
              "score": 2,
              "created_utc": "2026-01-26 17:54:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v0omr",
                  "author": "catplusplusok",
                  "text": "Absolutely! I would say with 16GB you can play with a lot of task specific workflows (machine vision, image generation, processing structured data). With 32GB you can run models that can replace cloud chat AI for most cases, roleplay and write production code for you (look at QWEN3 or other mixture of experts models with around 30B total parameters). Look for mlx 4 bit quantized ones for decent quality / efficiency balance.",
                  "score": 3,
                  "created_utc": "2026-01-26 18:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v09xh",
          "author": "teachersecret",
          "text": "Start by getting your hands on a decent computer. At the cash you have, you're probably looking used. I assume you already have a PC laying around that was built in the last 5 or so years with a modern processor and 32gb-64gb ddr3/ddr4/ddr5 in it. If so, that's the EASIEST way to get going. Just go buy a 3090/4090 or a 5090, shove it in, and you're done.\n\nIf not, you can buy one fairly cheap prebuilt computer secondhand. Go after something from the more modern intel or AMD lineups. Aim for a HIGHER END cpu built some time this decade (2020 or newer if possible) with as many cores/highest speed you can find. You can go on places like facebook marketplace and find people selling entire used rigs that would work fine. Aim for the best processor/most ram you can find, and try to stick DDR4 or DDR5 if you can (as an example I recently purchased a whole 5900x rig with a 165hz monitor, 3080ti, and 64gb of ddr4 for less than $500 all-in off FB marketplace).\n\nAfter you get the machine, the focus needs to be on the GPU. Hopefully the rig already has one that is useful, but if it DOESN'T have a 3090/4090/5090, thats where you need to dump the rest of your budget. Grab a used 3090/4090, or save up for a 5090.\n\nThat's about as far as you can reasonably go on a budget, while still allowing you to play with some of the best performing models. 24gb vram gets you high speed 30b moe models, lightning-fast smaller models like oss 20b or the smaller 8b and 4b options, high speed image and video generation (ltx-2 fits in 24gb), and plenty of support for your explorations (3090 and newer are gold standards of AI right now, and are likely to maintain strong support for years). Anything cheaper is going to be significantly slower, older, barely supported, and really not a great option. You'll see people in here strapping together impressive server rigs using old cast off server parts that can run big boy MoE models, and that's cool, but unless you have a damn good reason to do that and an exhaustive understanding of server hardware, I'd avoid that space. Older server rigs end up too slow to be worth it for most uses, and newer server rigs are too expensive for your budget.\n\nAnything more capable than what I mentioned is going to be significantly more expensive. Obscenely more expensive in some instances. At that point, you're just flat out better off using API inference and paying pennies for tokens.\n\nOne dark horse here is a AI Max 395x 128gb rig. They're small, sip power, and have unified 128gb ram which allows you to run hefty models like 200b style models at usable speed. That said, 'usable' is still SIGNIFICANTLY SLOWER than what you'll get off a good GPU, and those AI max rigs are over 2k USD now. Not a terrible option if you want a quiet little box to experiment with though. Same goes for the newer macbook pro line. They're pricey, but if you grab one with enough unified memory you can run something as big as deepseek at a speed that could still be considered usable. These might not be the cheapest options today, but as time passes and more of these kinds of rigs hit the used market, they're going to end up being pretty solid AI hardware to snatch up.",
          "score": 1,
          "created_utc": "2026-01-26 18:05:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yc3s6",
          "author": "WishfulAgenda",
          "text": "Alright, awesome questions. I‚Äôll start with what I think then go into why. What I think you should do is on the MBP m3 download lm studio, vs code with continue dev. On the MBP m2 download vm fusion install Linux (mint, tumbleweed etc) and on the Linux machine install clickhouse and grafana) connect the lm studio llm to the clickhouse vm via an mcp server and your laughing. You could probably even use docker instead of the Linux vm as well. Also have a look at librechat . All of those applications are open source and free. I‚Äôd then start saving my money as the next jump is likely expensive and my guess would be an MBP m5 max, a tricked out Mac mini or a Mac Studio  when they come out or a substantially more expensive desktop. \n\nThe above isn‚Äôt as much fun as building a new machine but for your budget I‚Äôm not sure that the new machine would have much on the MBP m3, the apple silicon is very good. Especially with the current ram and gpu prices.\n\nAlso don‚Äôt let it put you off as the small models with the right configuration and actually be really good. You can set it up so you have specialist agents with system prompts that focuses them and the set up MoA architectures on the same model.\n\nGood luck :-)",
          "score": 1,
          "created_utc": "2026-01-27 03:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29muhr",
          "author": "chancechants",
          "text": "Some decent comparison data here, this is what I would get.\nhttps://www.kickstarter.com/projects/167544890/olares-one-the-local-al-powerhouse-on-your-desk",
          "score": 1,
          "created_utc": "2026-01-28 19:02:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a96ri",
          "author": "see_spot_ruminate",
          "text": "Sell one of the macbooks. Use the money to get a strix halo. Set up strix halo headless. Use remaining macbook to remote in. \n\nI have been trying several different models over the last several months, but keep coming back to gpt-oss-120b. MOE is very good these days and the strix halo does well with it. With trying to buy a hodge podge of stuff, you can prob run the same pp and tg as me with a strix halo.",
          "score": 1,
          "created_utc": "2026-01-28 20:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uunh6",
          "author": "Limebird02",
          "text": "OP said around $1000 GBP which is $1370. \n\nDoes a local 30B model off Ollama do ok or can we get by with less? Id like to try the same with $500 mini pc. \n\nHow does open code help? \n\nHow does clawdbot work well with smaller local models? \n\nLike OP want to dabble without large outlay.",
          "score": 0,
          "created_utc": "2026-01-26 17:41:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}