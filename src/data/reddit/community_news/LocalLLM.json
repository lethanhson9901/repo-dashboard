{
  "metadata": {
    "last_updated": "2026-01-05 02:46:09",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 27,
    "total_comments": 148,
    "file_size_bytes": 167094
  },
  "items": [
    {
      "id": "1pzdjoc",
      "title": "Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware â€“ Full Optimization Guide",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xdj2zjnz5bag1.png",
      "author": "at0mi",
      "created_utc": "2025-12-30 09:14:01",
      "score": 143,
      "num_comments": 21,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzdjoc/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwpcpjs",
          "author": "Lxzan",
          "text": "Nice work and thanks for sharing! How much is the power draw?",
          "score": 18,
          "created_utc": "2025-12-30 09:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpd7l0",
              "author": "at0mi",
              "text": "about 1600W i will update my blogpost with detailed power draw\n\nUPDATE: the 1600W was at higher Thread count, with the optimum 64 Threads im at 1154W\n\nIdle is 694W",
              "score": 21,
              "created_utc": "2025-12-30 09:31:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwph0z3",
                  "author": "Amazing_Athlete_2265",
                  "text": "Oof, that's gonna cost a bunch to run",
                  "score": 15,
                  "created_utc": "2025-12-30 10:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqiawc",
                  "author": "mister2d",
                  "text": "Those are some expensive tokens.",
                  "score": 3,
                  "created_utc": "2025-12-30 14:36:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqwrjk",
                  "author": "resil_update_bad",
                  "text": "jesus",
                  "score": 1,
                  "created_utc": "2025-12-30 15:50:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpoy5i",
          "author": "beedunc",
          "text": "Yes, been running the qwen3coder480b@q3 (245GB) on an old Dell T5810 running a single e5-2697v4, gets 2-3 tps. \n\nPower draw is only 250w under load. \n\nI never thought of disabling hyper threading, does that help a lot? Will be checking this out, thank you.",
          "score": 6,
          "created_utc": "2025-12-30 11:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwppbbm",
              "author": "at0mi",
              "text": "i also tried lower quantisation models but the quality of the output was crap, works for a chatbot but not for coding, \nin my case (8 numa nodes) disabling hyper threading gave an enormous boost",
              "score": 6,
              "created_utc": "2025-12-30 11:22:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwsp871",
              "author": "Candid_Highlight_116",
              "text": "If LLM inference is RAM bandwidth bound and HT was a tech to halve effective bandwidth because two virtual cores needs their respective data, it makes sense that turning off HT gives massive boost  \n\nsorry that's my hallucination but if it's actually like that it's pretty interesting",
              "score": 3,
              "created_utc": "2025-12-30 20:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu51bd",
                  "author": "beedunc",
                  "text": "Cool, thanks for the suggestion!",
                  "score": 1,
                  "created_utc": "2025-12-31 01:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpkj5q",
          "author": "xgiovio",
          "text": "Watts",
          "score": 5,
          "created_utc": "2025-12-30 10:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwql1ld",
              "author": "MaverickPT",
              "text": "All of them",
              "score": 7,
              "created_utc": "2025-12-30 14:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqd59a",
          "author": "chafey",
          "text": "I love hacks like this - nice work.  The hardware may be cheap/free but the electricity won't be...",
          "score": 7,
          "created_utc": "2025-12-30 14:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq21y3",
          "author": "Such_Advantage_6949",
          "text": "I am passionate about running llm at usable speed..",
          "score": 4,
          "created_utc": "2025-12-30 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws7ver",
          "author": "Foreign-Watch-3730",
          "text": "Same result ( 5.1 t/s ) , but in IQ5\\_K with 2 Xeon E5 2696 V4 and 400 Gb ddr4 ram ( on a very olf Dell T630 ) with 2 RTX 5090 ( ik\\_llama.cpp and [ubergarm](https://huggingface.co/ubergarm) for use opencode :  \n[https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5](https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5)\n\nnumactl --interleave=all ./build/bin/llama-server  \n\\--model \\~/ik\\_llama.cpp/models/GLM-4.7-Ubergarm/IQ5\\_K/GLM-4.7-IQ5\\_K-00001-of-00007.gguf  \n\\--alias GLM-4.7-IQ5  \n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8080  \n\\--ctx-size 84992  \n\\--no-mmap  \n\\--threads 82 --threads-batch 82  \n\\--batch-size 1024 --ubatch-size 1024  \n\\--parallel 1 --flash-attn 1  \n\\--jinja --verbose  \n\\--n-gpu-layers 99  \n\\--tensor-split 0.5,0.5  \n\\--split-mode layer  \n\\--run-time-repack  \n\\--cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--k-cache-hadamard  \n\\-ot 'blk.\\[0-8\\]..\\*exps.weight=CUDA0'  \n\\-ot 'blk.(8\\[6-9\\]|9\\[0-2\\])..\\*exps.weight=CUDA1'  \n\\-ot '.\\*exps.weight=CPU'",
          "score": 3,
          "created_utc": "2025-12-30 19:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqj2yx",
          "author": "Extension-Cow2818",
          "text": "Very interesting that turning hyperthreading off works better.  \nProbably memory access is causing issues in these types of workloads.\n\nIt would be also interesting to try MLK vs ATLAS vs BLAS",
          "score": 2,
          "created_utc": "2025-12-30 14:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq772g",
          "author": "Icy_Programmer7186",
          "text": "That's cool.  \nHow much memory did it consumed?",
          "score": 1,
          "created_utc": "2025-12-30 13:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqh2cl",
          "author": "ForsookComparison",
          "text": "This is very cool, thank you for testing this out.\n\nI'm curious what the use-case is? Is GLM decent as a general purpose model? Or will you give it a coding task and come back after a few hours",
          "score": 1,
          "created_utc": "2025-12-30 14:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdqlaq",
          "author": "spacefarers",
          "text": "Comes down to around $9 per million tokens just for electricity not sure if it's worth it lol",
          "score": 1,
          "created_utc": "2026-01-03 04:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrv2is",
          "author": "Free_Manner_2318",
          "text": "7200 Watt for 5 tokens eh?!   \nAsk it if it was a reasonable decision.... :))))  \nB+ for the effort though. Not custom enough to be significant.",
          "score": 0,
          "created_utc": "2025-12-30 18:29:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py1gvp",
      "title": "Probably more true than I would like to admit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/iann5fyl70ag1.png",
      "author": "low_v2r",
      "created_utc": "2025-12-28 20:24:33",
      "score": 137,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1gvp/probably_more_true_than_i_would_like_to_admit/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwf8lgr",
          "author": "SunshineSeattle",
          "text": "I bought a particle tacyon, but now i have no idea what to do with it. ðŸ˜­",
          "score": 9,
          "created_utc": "2025-12-28 20:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfk71i",
          "author": "Healthy-Nebula-3603",
          "text": "WHAT AN EDGE DEVICE ?",
          "score": 5,
          "created_utc": "2025-12-28 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfm8iu",
              "author": "Count_Rugens_Finger",
              "text": "honestly not sure if this is serious but if it is, it just means the device-in-hand of the users (phones, tablets, PCs, car dashboards, POS systems, and whatnot)",
              "score": 7,
              "created_utc": "2025-12-28 21:35:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksnf2",
                  "author": "QuinQuix",
                  "text": "Usually this refers to devices that are compute restrained.\n\nA user could have a beast of a pc workstation that technically lives on the edge (of the central cloud workspace) but it's not a usual thing to refer to compute strong devices as edge devices.",
                  "score": 2,
                  "created_utc": "2025-12-29 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj6kgc",
              "author": "trmnl_cmdr",
              "text": "https://preview.redd.it/wl5ul68kv4ag1.jpeg?width=349&format=pjpg&auto=webp&s=38e1efd5e4769de7bcf9d7c4bf5ae45a353fefb4\n\nThis, apparently. Although Iâ€™m not sure how this helps me masturbate.",
              "score": 4,
              "created_utc": "2025-12-29 12:05:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1g3yx",
                  "author": "jgwinner",
                  "text": "Please rev it up a bit ... now tilt it so the blade scrapes on the stone ...\n\nthat's it ...\n\nFASTER \n\nthat's IT ... IT ...\n\nuhhh.\n\nWhew. Could you shut that thing off? kthks.",
                  "score": 1,
                  "created_utc": "2026-01-01 05:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwua4fa",
              "author": "Jackuarren",
              "text": "The device that you use for edging, obviously.",
              "score": 1,
              "created_utc": "2025-12-31 01:51:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfaevi",
          "author": "Individual_Holiday_9",
          "text": "Unironically this",
          "score": 2,
          "created_utc": "2025-12-28 20:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfid3y",
          "author": "GCoderDCoder",
          "text": "I feel so seen!!!",
          "score": 1,
          "created_utc": "2025-12-28 21:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt8rsj",
          "author": "mobileJay77",
          "text": "Your solution: \n1. Head over the r/localLlama\n2. You hardly sleep any more\n3. She doesn't have to worry what you are thinking in bed.",
          "score": 1,
          "created_utc": "2025-12-30 22:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q06u1n",
      "title": "2025 is over. What were the best AI model releases this year?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "author": "techlatest_net",
      "created_utc": "2025-12-31 07:12:49",
      "score": 64,
      "num_comments": 23,
      "upvote_ratio": 0.88,
      "text": "2025 felt like three AI years compressed into one. Frontier LLMs went insane on reasoning, openâ€‘source finally became â€œgood enoughâ€ for a ton of real workloads, OCR and VLMs leveled up, and audio models quietly made agents actually usable in the real world. â€‹ Hereâ€™s a categoryâ€‘wise recap of the â€œbest of 2025â€ models that actually changed how people build stuff, not just leaderboard screenshots:\n\n\n\nLLMs and reasoning\n\n\\* GPTâ€‘5.2 (Thinking / Pro) â€“ Frontierâ€‘tier reasoning and coding, very fast inference, strong for longâ€‘horizon toolâ€‘using agents and complex workflows.\n\nâ€‹\\* Gemini 3 Pro / Deep Think â€“ Multiâ€‘million token context and multimodal â€œscreen reasoningâ€; excels at planning, code, and webâ€‘scale RAG / NotebookLMâ€‘style use cases. \n\n\\* Claude 4.5 (Sonnet / Opus) â€“ Extremely strong for agentic tool use, structured stepâ€‘byâ€‘step plans, and â€œuse the computer for meâ€ style tasks. \n\n\\* DeepSeekâ€‘V3.2 & Qwen3â€‘Thinking â€“ Openâ€‘weight monsters that narrowed the gap with closed models to within \\\\\\~0.3 points on key benchmarks while being orders of magnitude cheaper to run.\n\nIf 2023â€“24 was â€œjust use GPT,â€ 2025 finally became â€œpick an LLM like you pick a database.â€\n\nVision, VLMs & OCR\n\n\\* MiniCPMâ€‘V 4.5 â€“ One of the strongest open multimodal models for OCR, charts, documents, and even video frames, tuned to run on mobile/edge while still hitting SOTAâ€‘ish scores on OCRBench/OmniDocBench. \n\n\\* olmOCRâ€‘2â€‘7Bâ€‘1025 â€“ Allen Instituteâ€™s OCRâ€‘optimized VLM, fineâ€‘tuned from Qwen2.5â€‘VL, designed specifically for documents and longâ€‘form OCR pipelines. \n\n\\* InternVL 2.x / 2.5â€‘4B â€“ Open VLM family that became a goâ€‘to alternative to closed GPTâ€‘4Vâ€‘style models for document understanding, scene text, and multimodal reasoning.\n\n\\* Gemma 3 VLM & Qwen 2.5/3 VL lines â€“ Strong open(-ish) options for highâ€‘res visual reasoning, multilingual OCR, and longâ€‘form video understanding in productionâ€‘style systems. â€‹ \n\n2025 might be remembered as the year â€œPDF to clean Markdown with layout, tables, and chartsâ€ stopped feeling like magic and became a boring API call.\n\nAudio, speech & agents\n\n\\* Whisper (still king, but heavily optimized) â€“ Remained the default baseline for multilingual ASR in 2025, with tons of optimized forks and onâ€‘device deployments. \n\n\\* Lowâ€‘latency realâ€‘time TTS/ASR stacks (e.g., new streaming TTS models & APIs) â€“ Subâ€‘second latency + streaming text/audio turned LLMs into actual realâ€‘time voice agents instead of â€œpodcast narrators.â€ \n\n\\* Many 2025 voice stacks shipped as APIs rather than single models: ASR + LLM + realâ€‘time TTS glued together for call centers, copilots, and vibecoding IDEs. â€‹ Voice went from â€œcool demoâ€ to â€œI talk to my infra/IDE/CRM like a human, and it answers back, live.â€\n\nOCR/document AI & IDP\n\n\\* olmOCRâ€‘2â€‘7Bâ€‘1025, MiniCPMâ€‘V 4.5, InternVL 2.x, OCRFluxâ€‘3B, PaddleOCRâ€‘VL â€“ A whole stack of open models that can parse PDFs into structured Markdown with tables, formulas, charts, and long multiâ€‘page layouts. \n\n\\* On top of these, IDP / â€œPDF AIâ€ tools wrapped them into full products for invoices, contracts, and messy enterprise docs. â€‹ \n\nIf your 2022 stack was â€œTesseract + regex,â€ 2025 was â€œdrop a 100â€‘page scan and get usable JSON/Markdown back.â€ â€‹ \n\nOpenâ€‘source LLMs that actually mattered\n\n\\* DeepSeekâ€‘V3.x â€“ Aggressive MoE + thinking budgets + brutally low cost; a lot of people quietly moved internal workloads here. \n\n\\* Qwen3 family â€“ Strong openâ€‘weight reasoning, multilingual support, and specialized â€œThinkingâ€ variants that became default selfâ€‘host picks. \n\n\\* Llama 4 & friends â€“ Closed the gap to within \\\\\\~0.3 points of frontier models on several leaderboards, making â€œfully open infraâ€ a realistic choice for many orgs.\n\nâ€‹In 2025, openâ€‘source didnâ€™t fully catch the frontier, but for a lot of teams, it crossed the â€œgood enough + cheap enoughâ€ threshold.\n\nYour turn This list is obviously biased toward models that:\n\n\\* Changed how people build products (agents, RAG, document workflows, voice UIs)\n\n\\* Have public benchmarks, APIs, or open weights that normal devs can actually touch â€‹- What did you ship or adopt in 2025 that deserves â€œmodel of the yearâ€ status?\n\nFavorite frontier LLM?\n\n\\* Favorite openâ€‘source model you actually selfâ€‘hosted?\n\n\\* Best OCR / VLM / speech model that saved you from pain?\n\n\\* Drop your picks below so everyone can benchmark / vibeâ€‘test them going into 2026.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwvo5t9",
          "author": "Clipbeam",
          "text": "I vote the Qwen3 models. Total game changer for local LLMs.",
          "score": 33,
          "created_utc": "2025-12-31 07:35:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvohey",
          "author": "DutchSEOnerd",
          "text": "Overall I would says Claude Opus had most impact, for local its definitely Qwen",
          "score": 12,
          "created_utc": "2025-12-31 07:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy6e07",
              "author": "ForsookComparison",
              "text": "+1 for Opus. Benchmarks don't show its impact properly. The thing dropping down to $25/1m output tokens led to me not needing to hand-write a line of code for the last month of this year in my biggest repos.",
              "score": 2,
              "created_utc": "2025-12-31 17:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3hp62",
                  "author": "DutchSEOnerd",
                  "text": "Thats also the feeling I have. Benchmarks only tell part of the story",
                  "score": 1,
                  "created_utc": "2026-01-01 16:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwy6olb",
          "author": "ForsookComparison",
          "text": "Deepseek R1 probably had the biggest impact. Put the fear of God into Western companies that were operating as if they had a magical moat. Before that there was legislation being seriously considered to put a halt to A.I. progress for a few months to years.",
          "score": 5,
          "created_utc": "2025-12-31 17:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvnxf6",
          "author": "AllTheCoins",
          "text": "I love Qwen3-30b as a go-to all around model",
          "score": 8,
          "created_utc": "2025-12-31 07:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwz4f3w",
              "author": "Count_Rugens_Finger",
              "text": "yeah definitely.  In this thread there is very clearly two worlds coming into contact here.  The people with purpose-made rigs and the people with consumer grade hardware",
              "score": 1,
              "created_utc": "2025-12-31 20:46:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwpmc5",
          "author": "jinnyjuice",
          "text": "Might be useful to categorise by memory size and task groups",
          "score": 4,
          "created_utc": "2025-12-31 13:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwa6rb",
          "author": "InfiniteTrans69",
          "text": "Qwen3, Kimi K2, Minimax M2.",
          "score": 3,
          "created_utc": "2025-12-31 11:03:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3duag",
              "author": "QuinQuix",
              "text": "What's the respective use cases for you with these three models?",
              "score": 1,
              "created_utc": "2026-01-01 15:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3fqe7",
                  "author": "InfiniteTrans69",
                  "text": "Translation and vision: Qwen  \nResearch and AI Slides: Kimi  \nEverything else: Minimax",
                  "score": 1,
                  "created_utc": "2026-01-01 16:02:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwxhgvw",
          "author": "leonbollerup",
          "text": "Open Source: GPT-oss-20b/120b",
          "score": 3,
          "created_utc": "2025-12-31 15:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwyuia",
          "author": "RiskyBizz216",
          "text": "Qwen3-Next specifically the 80B and 480B\n\nAnd Z-Image Turbo",
          "score": 2,
          "created_utc": "2025-12-31 14:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxfizo",
          "author": "BuffMcBigHuge",
          "text": "Open: Qwen3-30B\nClosed: Gemini 3 Pro + Nano Banana\n\nOpus 4.5 is stellar, but those two above blew me away.",
          "score": 2,
          "created_utc": "2025-12-31 15:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmwi4",
          "author": "Karyo_Ten",
          "text": "LLMs: GLM-4.5-Air, GLM-4.7, GPT-OSS (quality for the first 2, speed demon for GPT-OSS)\n\nPromising new arch: Kimi-Linear, Qwen-Next, MiMo-V2-Flash I believe 2026 will have Linear Attention everywhere.\n\nSpecialized: Minimax-M2.1\n\nMultimodal: Qwen3-Omni, GLM-4.6V\n\nOCR: Mineru",
          "score": 2,
          "created_utc": "2025-12-31 16:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvpnzh",
          "author": "Lissanro",
          "text": "You missed to mention Kimi K2 0905 and Kimi K2 Thinking. These are the models I run the most on my PC (IQ4 and Q4_X quants respectively, using ik_llama.cpp).Â K2 Thinking is especially notable for its QAT INT4 release which maps nicely to Q4_X in the GGUF format, preserving the original quality.\n\nFor both, common expert tensors and 256K context cache at Q8 fit fully in 96GB VRAM, making them excellent for CPU+GPU inference.\n\n\nAs of DeepSeek V3.2, I did not get to try it due to lack of support in both ik_llama.cpp and llama.cpp. There is work in progress to add its architecture but not going to make it this year.",
          "score": 3,
          "created_utc": "2025-12-31 07:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwa88j",
              "author": "Tema_Art_7777",
              "text": "Lord! From unsloth:\n\nâ€œIt is recommended to have 247 GB of RAM to run the 1-bit Dynamic GGUF.\nTo run the model in full precision, you can use 'UD-Q4_K_XL', which requires 646 GB RAM.â€",
              "score": 1,
              "created_utc": "2025-12-31 11:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwislo",
                  "author": "Lissanro",
                  "text": "Even though Unsloth generally makes good quants, in this case it is the best to use Q4\\_X from [https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF) because it preserves the original INT4 quality in 543.617 GiB size, while UD-Q4\\_K\\_XL would be bigger, slower and likely of a bit lower quality.\n\nThat said, yes, your estimates of necessary RAM are accurate, for IQ1 256 GB RAM is needed and almost all of it will be used by the model, leaving very little for the OS; for IQ3 512 GB, and for Q4\\_X around 640 GB is needed if you include full cache size. K2 Thinking works best high RAM rigs or with high GPU count, like twenty MI50 cards (for 640 GB VRAM in total) - I actually considered getting that much since at the time I looked into it, it was possible for half the price of RTX PRO 6000 and my motherboard could carry them all at PCI-E 4.0 x4 speed each, but I decided to stay with 4x3090 for now (because even though full VRAM inference would be faster, my current performance with 1 TB RAM + 96 GB VRAM is still acceptable to me, and a lot of what I do requires Nvidia cards, and not just LLMs either).",
                  "score": 2,
                  "created_utc": "2025-12-31 12:17:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx0g87s",
          "author": "karmakaze1",
          "text": "Nemotron-3-Nano for its use of hybrid Mamba-Transformer that reduces memory and computation for large context.\n\nAlso Qwen3-Next (for \"Gated DeltaNet\") and Kimi K2 (for advancing MLA further).",
          "score": 1,
          "created_utc": "2026-01-01 01:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmu7yj",
          "author": "thedarkbobo",
          "text": "Qwen3, iQuest, Deepseek, OSS, gemma3, nemotron",
          "score": 1,
          "created_utc": "2026-01-04 15:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvqtnu",
          "author": "PeakBrave8235",
          "text": "At the end of the day they all are BS, open or closed source idgaf.",
          "score": -10,
          "created_utc": "2025-12-31 08:00:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxga7w",
              "author": "Count_Rugens_Finger",
              "text": "so why are you even here?",
              "score": 2,
              "created_utc": "2025-12-31 15:40:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1u966",
      "title": "I designed a Private local AI for Android - has internet search, personas and more.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/jk6meo3wmwag1",
      "author": "Sebulique",
      "created_utc": "2026-01-02 09:25:58",
      "score": 56,
      "num_comments": 11,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx8c8zl",
          "author": "Themash360",
          "text": "Nice work, can it connect to local llm as well? So not just on device but custom endpoints",
          "score": 1,
          "created_utc": "2026-01-02 10:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8cp5h",
              "author": "Sebulique",
              "text": "You can, I've left it completely open, I also allowed it to work with smart bulbs, It works but sometimes it decides to do it's own thing unless I set parameters.\n\nI've turned it off after I broke it, but I want to reintroduce it again soon",
              "score": 4,
              "created_utc": "2026-01-02 10:18:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx982sn",
          "author": "theCatchiest20Too",
          "text": "Very cool! Are you tracking performance against hardware specs?",
          "score": 1,
          "created_utc": "2026-01-02 14:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9mbso",
              "author": "Sebulique",
              "text": "I was thinking of a toggle called \"stats for geeks\" but I wanted to make it look super easy for anyone, almost Chatgpt like, so more people feel comfortable using local llms",
              "score": 2,
              "created_utc": "2026-01-02 15:31:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx9j4l9",
          "author": "False-Ad-1437",
          "text": "\\> Very very happy, will upload soon on GitHub when I've ironed out any bugs I come across.\n\nI'm happy to submit PRs to fix said bugs",
          "score": 1,
          "created_utc": "2026-01-02 15:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa6go5",
              "author": "Sebulique",
              "text": "Thank you man, I appreciate you, I'm looking to make it easy and accessible for all",
              "score": 1,
              "created_utc": "2026-01-02 17:06:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdas2x",
          "author": "2QNTLN",
          "text": ">will upload soon on GitHub. \n\nHow soon is soon? Super interested in this project so i had to ask.",
          "score": 1,
          "created_utc": "2026-01-03 02:39:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe9nbn",
          "author": "tankandwb",
          "text": "RemindMe! 1 week",
          "score": 1,
          "created_utc": "2026-01-03 06:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe9qy4",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-01-10 06:34:58 UTC**](http://www.wolframalpha.com/input/?i=2026-01-10%2006:34:58%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/nxe9nbn/?context=3)\n\n[**4 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLM%2Fcomments%2F1q1u966%2Fi_designed_a_private_local_ai_for_android_has%2Fnxe9nbn%2F%5D%0A%0ARemindMe%21%202026-01-10%2006%3A34%3A58%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q1u966)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-03 06:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcrxs",
          "author": "Latter_Virus7510",
          "text": "Coolsieees! Amazing job there boss! When do we get to download?",
          "score": 1,
          "created_utc": "2026-01-03 18:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhkf1u",
          "author": "Benschman1979",
          "text": "Great work! I'm looking forward to testing it.",
          "score": 1,
          "created_utc": "2026-01-03 19:04:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjnew",
      "title": "Tiiny Al just released a one-shot demo of their Pocket Lab running a 120B model locally.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "author": "Ajitabh04",
      "created_utc": "2025-12-29 11:09:13",
      "score": 49,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "Just came across this demo. They plugged their tiny AI computer into a 14-year-old PC and it output an average of 19 tokens/s on a 120B model.\nThey haven't released the MSRP yet. However, a large amount of DDR5 memory would be pricey, I'm guessing around $1500 MSRP for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjcqd4",
          "author": "loyalekoinu88",
          "text": "They had posted around $699 BUT that was before the memory announcement",
          "score": 13,
          "created_utc": "2025-12-29 12:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj8uo7",
          "author": "leonbollerup",
          "text": "No link ?",
          "score": 9,
          "created_utc": "2025-12-29 12:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwklly9",
          "author": "ForsookComparison",
          "text": "> guessing $1500 MSRP\n\n> they posted $699 pre crucial RAM announcement \n\n> 19 tokens/second on gpt-oss-120b\n\nI plugged a used Rx 6800 ($250 in my area with multiple options) into an older PC and got 18 tokens per second. I know this isn't the same and it suggests that you have a fair amount of RAM in your older PC, but given what we know about this I'm thoroughly \"meh\"d.\n\n**Edit** - just looked up the form factor. I'm less meh'd now. That would be fun to use if it ends up affordable.",
          "score": 5,
          "created_utc": "2025-12-29 16:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlgo5g",
              "author": "FaceDeer",
              "text": "[It's a portable unit the size of a phone](https://tiiny.ai/), since there aren't links anywhere else in this thread.\n\nNot a lot of detail even there, though. I don't see anything about whether it's battery powered - I'm assuming not, given OP mentions plugging it in to a computer.",
              "score": 5,
              "created_utc": "2025-12-29 19:18:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwutx5z",
                  "author": "ecoleee",
                  "text": "Youâ€™re absolutely right â€” this generation of Tiiny does not include a built-in battery.\n\nThat decision was intentional. Thermal management is a serious challenge at this performance level, and we didnâ€™t want to ship a device that becomes uncomfortably hot in real use.\n\nTo reliably support sustained local inference of models up to 120B parameters, we designed a custom thermal module specifically for Tiiny, prioritizing stability and safe operating temperatures over battery integration.\n\nAt the upcoming CES, weâ€™ll be sharing a detailed internal teardown video of Tiiny. Youâ€™ll be able to see exactly how the cooling system is built and why these design choices were made.\n\nWe believe itâ€™s better to be transparent about trade-offs and deliver a product that performs consistently, rather than chasing form factors at the expense of real-world usability.",
                  "score": 2,
                  "created_utc": "2025-12-31 03:48:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq0vao",
          "author": "Ok-Structure4057",
          "text": "Found the specs on their website:\n\npocket size: 14.2 Ã— 8 Ã— 2.53 cm\n\n80GB LPDDR5X RAM & 1TB SSD190 \n\ntotal TOPS between the SoC and dNPU\n\n30W TDP\n\nThey also released a demo of this device on Twitter. Imo it would be fun with retail prices around 1400 bucks.",
          "score": 3,
          "created_utc": "2025-12-30 12:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq5w0t",
              "author": "RangerOk4318",
              "text": "Agree. Memory price has been so absurd. I'm guessing the same price",
              "score": 1,
              "created_utc": "2025-12-30 13:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws3u2u",
                  "author": "QuinQuix",
                  "text": "Memory is fucking up any attempt at affordable home AI right now",
                  "score": 2,
                  "created_utc": "2025-12-30 19:10:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvbcwc",
              "author": "fallingdowndizzyvr",
              "text": "> mo it would be fun with retail prices around 1400 bucks.\n\nAt that price, why not just get a Strix Halo? That's just a PC so you can do regular PC stuff like gaming.",
              "score": 1,
              "created_utc": "2025-12-31 05:48:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0pjg",
          "author": "zelkovamoon",
          "text": "I'm not sure what having a *small* ai lab is trying to solve\n\nIf you're doing local AI my position is, make it bigger, cooler, and put more ram on it.\n\nThat said, it is *good* that companies are stepping in to try and build some solutions. If we could get something with 256GB of fast memory we might be able to go places.",
          "score": 3,
          "created_utc": "2025-12-30 21:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6f8d5",
          "author": "noless15k",
          "text": "[https://tiiny.ai/pages/tech-1](https://tiiny.ai/pages/tech-1)\n\nhttps://preview.redd.it/4b2qtqbx9uag1.png?width=2048&format=png&auto=webp&s=0538a8c9fc2286ecebe27731cc85925cc7819fbc",
          "score": 2,
          "created_utc": "2026-01-02 01:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlotf1",
          "author": "No-Consequence-1779",
          "text": "I think the lpddr5 is likely the memory. Â Itâ€™s slightly faster for this and itâ€™s wired so they can , like others, charge for memory size mostly.Â ",
          "score": 1,
          "created_utc": "2025-12-29 19:57:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxvlh0",
      "title": "I have 50 ebooks and I want to turn them into a searchable AI database. What's the best tool?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "author": "Great_Jacket7559",
      "created_utc": "2025-12-28 16:33:33",
      "score": 41,
      "num_comments": 29,
      "upvote_ratio": 0.95,
      "text": "I want to ingest 50 ebooks into an LLM to create a project database.\nIs Google NotebookLM still the king for this, or should I be looking at Claude Projects or even building my own RAG system with LlamaIndex?\nI need high accuracy and the ability to reference specific parts of the books. I don't mind paying for a subscription if it works better than the free tools.\nAny recommendations?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwj05xl",
          "author": "Zucramj",
          "text": "I would build something custom.\n\nHere is how I see it:\n\n1) You own the data \n2) You own the AI (local embeddings work great for this task and you can run it with a local AI model or use Openrouter) \n3) I would build this with DSPy (modular and can be optimized with gepa) \n4) I would use PostgreSQL to store the data \n\nSo if you already have the ebooks as pdfs I would take those an set that up.\n\nHere is my primitive version of this I did some months back: \n\nhttps://github.com/marcusjihansson/dspy-mcp-tools/blob/main/regulatory.py\n\nIt is advanced if you don't know what you are reading but as I have gotten deeper into optimizing AI agents and systems does this feel primitive to me. \n\nThat newer research is going to be uploaded to my GitHub soon...\n\nSo: \nA) I would read through if this would solve your task! \nB) I would be happy to help you out if you have any questions!",
          "score": 9,
          "created_utc": "2025-12-29 11:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf0w7j",
          "author": "DHFranklin",
          "text": "NotebookLM and maybe some RAG and Custom Instructions for vectoring.\n\nNow if you wanna get real squirrely you could turn the entire compendium, and custom instruction into 1 million token prompt and sit it into Gemini as is. That might actually be more useful.\n\nThe trick is information loss and context bleed with the books. I could see a JSON Database made from it all as text also.\n\nIt comes down to what are you using it for and what information needs to stay consistent.",
          "score": 3,
          "created_utc": "2025-12-28 19:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe78fy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-28 17:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwerjho",
              "author": "blaidd31204",
              "text": "I am intrigued... what If:\n\n* pdf (Yes, there are images, but these should not influence info)?\n\n* markdown (No images)?",
              "score": 2,
              "created_utc": "2025-12-28 19:06:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh0m2z",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 02:05:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwgwh5t",
                  "author": "Investolas",
                  "text": "Those answers influence the reccomendation.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwim06f",
          "author": "RepLava",
          "text": "LightRAG with the MCP. Works great based on the relatively sparse info you've given",
          "score": 2,
          "created_utc": "2025-12-29 08:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlvovo",
          "author": "False-Ad-1437",
          "text": "AnythingLLM can make your own workspaces for this.",
          "score": 2,
          "created_utc": "2025-12-29 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpin9j",
          "author": "Cladser",
          "text": "Iâ€™m deep in a similar project atm. It depends a lot on the type of info you want from the LLM. If itâ€™s what does author x have to say about y - RAG is your best bet. However if you want to ask questions like across these books what are the most common ways of dealing with with Y - That is a corpus level (ie entire collection) query and RAG will suck. Llamaindex with hybrid search seems to be the best middle road at the moment.",
          "score": 3,
          "created_utc": "2025-12-30 10:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwft2zf",
          "author": "vidibuzz",
          "text": "Slightly off topic. You may want to use Illuminati.google.com also for voice summaries.",
          "score": 2,
          "created_utc": "2025-12-28 22:09:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhdx2b",
              "author": "Schizophreud",
              "text": "Didnâ€™t know about this. Thanks.",
              "score": 1,
              "created_utc": "2025-12-29 03:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwibx1i",
          "author": "Charming_Support726",
          "text": "Depending on what's in the books, you could have a look at IBMs Docling for conversion. I think every simple RAG Pipeline will do the trick in the beginning",
          "score": 1,
          "created_utc": "2025-12-29 07:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigiuv",
          "author": "maxz2040",
          "text": "Logically.app",
          "score": 1,
          "created_utc": "2025-12-29 08:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm4e9l",
          "author": "Empty-Poetry8197",
          "text": "Paperqa",
          "score": 1,
          "created_utc": "2025-12-29 21:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwno756",
          "author": "kchandank",
          "text": "Interesting, if you are able to achieve your objective, would you be able to share the steps?",
          "score": 1,
          "created_utc": "2025-12-30 02:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpe82p",
          "author": "Sea_Mouse655",
          "text": "Iâ€™ve been using PaperQA2 for some regulatory use cases and itâ€™s gangster",
          "score": 1,
          "created_utc": "2025-12-30 09:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsjglj",
          "author": "isleeppeople",
          "text": "Seems like you could use something like pypdf and langchain to embed it into your RAG. If the ebooks are like current info that can change or become stale you will want to tag them and set up some sort of workflow to compare them to a Gemini or open ai call to compare the info and if it becomes stale remove it. I use qdrant and postgresql for ground truth. I do stuff like this for versions of python that I have to use to stay compatible with other things I'm running. Or actually for langchain and langgraph too. They just changed compatibility versions so whenever I upgrade I can just use the newer repo in my rag with the updated information. I will still keep the old one until I am absolutely certain I won't revert but you can just leave it sit there and not reference the old one. Hope that makes sense.",
          "score": 1,
          "created_utc": "2025-12-30 20:25:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh0iik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-29 02:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwimlqu",
              "author": "Just_Bronze",
              "text": "I have a stupid question.\n\nNot entirely sure how these things work, but do I understand correctly you've got a system set up to take input and create a chatbot?  Or do you create a file/fileset that a chatbot incorporates to use the information.",
              "score": 3,
              "created_utc": "2025-12-29 09:05:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwisq5d",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 10:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjfv9q",
              "author": "loki626",
              "text": "Can I DM you? I have some pdfs that I would like to convert. They are more technical though. Medicine related.",
              "score": 1,
              "created_utc": "2025-12-29 13:13:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmga0",
          "author": "PaleontologistOk865",
          "text": "What about just throwing everything in a AI and letting it figure out what to do? That's what my clients keep saying to me. . .",
          "score": 0,
          "created_utc": "2025-12-30 23:38:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzsdsy",
      "title": "Suggest a model for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "author": "Bright_Dot113",
      "created_utc": "2025-12-30 20:10:25",
      "score": 38,
      "num_comments": 25,
      "upvote_ratio": 0.98,
      "text": "Hello, I have 9950x3d with 64GB RAM and 5070 ti \n\nI recently installed LM Studio, which models do you suggest based on my hardware for the following purposes. \n\n1. Code in python and rust \n\n2. DB related stuff like optimising queries or helping me understand them. (Postgresql)\n\n3. System and DB design.\n\nAlso what other things can I do?\nI have heard lot about MCP servers but I didn't find any MCP servers useful or anything related to my workflow if you have any suggestions that would be great! ",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwsjtel",
          "author": "SimplyRemainUnseen",
          "text": "I'd suggest unsloth/Nemotron-3-Nano-30B-A3B-GGUF. You'll need to offload to system memory, but you'll have a relatively fast and intelligent model that's good at using tools.\n\nFor MCP servers check out [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)",
          "score": 16,
          "created_utc": "2025-12-30 20:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo35r",
              "author": "Big-Masterpiece-9581",
              "text": "What kind of speeds to you think he could get with this model and that configuration? Anything particular you like about that model or unsloth?",
              "score": 3,
              "created_utc": "2025-12-31 03:12:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxynte",
                  "author": "SimplyRemainUnseen",
                  "text": "Speeds would be pretty fast I imagine. Faster than my work laptop with an iGPU (which runs it faster than I can read).\n\nThe model itself is fully open source (data too!) and made by NVIDIA. The performance of the model exceeds other models in the same parameter range of 20-32b and has very long context (up to a million tokens).\n\nNVIDIA worked with Unsloth for day 0 quants of the model. Unsloth makes efficient dynamic quants of models that retain very high levels of accuracy at lower precision. Considering OP will need to offload to system memory, GGUF is ideal as it's designed for that use case.",
                  "score": 4,
                  "created_utc": "2025-12-31 17:11:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1r491",
                  "author": "Hairy_Candy_3225",
                  "text": "I use a similar setup with 9950x3d 96 GB RAM and 5080 16 GB VRAM and use Nemotron 3 nano 30b Q6. Tokens per second heavily depends on context used. I've written a script to run a benchmark test and measure speed with all combinations of different context length / % GPU-offload layer / force experts on CPU on vs off.\n\nWith smaller context (i.e. 10k tokens) I get up to 16 TPS. This drops to around 10 @200k tokens. I'm no expert but I don't think there will be a big difference between 5070 or 5080 if VRAM is the same.\n\nWhat i can't understand is that with nemotron it does not matter how much layers I offload to GPU. The model is around 30 GB but only between 4 and 6 GB VRAM is used regardless of offload setting. It seems only the active layers are offloaded to GPU. This was different when I qwen 2.5, where I had to balance the exact number of layers that would fit in VRAM. As soon as you try to offload more layers than fit in VRAM, regular RAM will be used as shared VRAM and TPS drop drastically. \n\nProbably nothing new here for most redditors on this sub but i'm new to local LLM's and had to figure out a lot of things myself.",
                  "score": 2,
                  "created_utc": "2026-01-01 07:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsmq4k",
          "author": "beedunc",
          "text": "Qwen3coder, whatever fits.",
          "score": 8,
          "created_utc": "2025-12-30 20:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtu8zi",
          "author": "No-Consequence-1779",
          "text": "Qwen3-coder-30b. The largest quant you can run. Dense model is good or moe. Â Keep in mind coder specific models are specialized for coding. Many like oss 120b though that size is not necessary.Â ",
          "score": 7,
          "created_utc": "2025-12-31 00:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt434d",
          "author": "Toastti",
          "text": "If you add another 64gb of ram you can run a Q2 quant of Minimax m2.1. it will probably be 7tk/s or so but it is almost certainly the smartest agentic coding model you can reasonably run",
          "score": 3,
          "created_utc": "2025-12-30 22:03:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt53sn",
              "author": "Your_Friendly_Nerd",
              "text": "q2? isnâ€˜t that gonna suck?",
              "score": 3,
              "created_utc": "2025-12-30 22:08:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtv56k",
                  "author": "Worried_Goat_8604",
                  "text": "No unsloth dynamic quant v2 usually dosnt reduce that much quality",
                  "score": 1,
                  "created_utc": "2025-12-31 00:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwugr2q",
              "author": "Individual_Gur8573",
              "text": "I agree I found minimax 2.1 IQ2_M the smartest model after GLM 4.5 airÂ ",
              "score": 1,
              "created_utc": "2025-12-31 02:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwuo8ix",
                  "author": "Karyo_Ten",
                  "text": "GLM Air which quant?",
                  "score": 1,
                  "created_utc": "2025-12-31 03:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlq1uw",
              "author": "Wild_Requirement8902",
              "text": " you can run iq4xs with 128 gb ram v and get 7 tk/s with 128gb of ram (with an old xeon(2680v4) and quad channel ddr4 @ 2400,) using lm studio and a 5060ti & a 3060 12gb I can get up to 131072 context @ q8 with flash attention. That what i am using, but sometime it crash and you have to reload the model and prompt processing is painful (35tk/s)",
              "score": 1,
              "created_utc": "2026-01-04 10:15:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwviewu",
          "author": "Count_Rugens_Finger",
          "text": "Qwen3-coder-30B-A3B, Nemotron-3-Nano-30B-A3B, Devstral-Small-2-24B",
          "score": 3,
          "created_utc": "2025-12-31 06:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsooqq",
          "author": "FullstackSensei",
          "text": "I'd say try a bunch of MoE all the way up to gpt-oss-120b and see where your pain threshold is for speed. IMO, you should keep a few at hand: a smaller model like Qwen3 Coder 30B, Nemotron 30B, gpt-oss-20b, Devstral 2 24B as daily drives, and larger models like gpt-oss-120b, GLM 4.5 air, Devstral 2 123B (Q4), etc for when the smaller models get stuck or can't solve whatever issue you have.",
          "score": 5,
          "created_utc": "2025-12-30 20:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt5rhp",
          "author": "Level_Wolverine_141",
          "text": "I have the same system as you except I've got a 5080, and I'm just using Claude code max and it's pretty good.",
          "score": -5,
          "created_utc": "2025-12-30 22:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww8pra",
              "author": "g33khub",
              "text": "your 5080 doesn't matter, I can use Claude code on a raspberry pi",
              "score": 2,
              "created_utc": "2025-12-31 10:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q31r48",
      "title": "Tony Starkâ€™s JARVIS wasnâ€™t just sci-fi his style of vibe coding is what modern AI development is starting to look like",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/hfvuxcznd6bg1",
      "author": "spillingsometea1",
      "created_utc": "2026-01-03 18:16:52",
      "score": 38,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q31r48/tony_starks_jarvis_wasnt_just_scifi_his_style_of/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxhaptp",
          "author": "sourceholder",
          "text": "I often think of this scene.\n\nA few years ago it seemed this type of interaction was at least a decade+ away.",
          "score": 1,
          "created_utc": "2026-01-03 18:20:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyw1b4",
      "title": "I built Plano(A3B) - fast open source LLM for agent orchestration that beats frontier LLMs",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5rp16cxd57ag1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-29 19:44:28",
      "score": 36,
      "num_comments": 14,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyw1b4/i_built_planoa3b_fast_open_source_llm_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwoqem5",
          "author": "ThsYWeCntHveNiceTngs",
          "text": "your research page is more advert than research and the blog provides more detail, but not much. Is there an Arxiv link or anything to read about your method and how you generated the proposed results?",
          "score": 4,
          "created_utc": "2025-12-30 06:07:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwosnmj",
              "author": "AdditionalWeb107",
              "text": "The huggingface models page has more details. Although we are in the process of publishing the arxiv paper",
              "score": 0,
              "created_utc": "2025-12-30 06:25:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxc7j4",
          "author": "False-Ad-1437",
          "text": "This is not open source. \n\nIt imposes non-open requirements (attribution, use restrictions, redistribution constraints and separate commercial licensing for certain uses) that violate the open-source criteria defined by the OSI/FSF.",
          "score": 2,
          "created_utc": "2025-12-31 15:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufm16",
          "author": "Purple-Programmer-7",
          "text": "How does this differ from Arch that youâ€™ve previously pushed for the past year?",
          "score": 1,
          "created_utc": "2025-12-31 02:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui3lf",
              "author": "AdditionalWeb107",
              "text": "Arch was about model routing. Plano is about orchestration, which is a slightly more complicated set of tasks. Plano is the next major upgrade to Arch with several new capabilities for agentic applications like filter chains, agent signals, and even more robust model gateway.\n\n\nBy the way, people were confusing arch with arch Linux so we thought it was a better time to rename the project.\nTry Plano ðŸ™",
              "score": 1,
              "created_utc": "2025-12-31 02:37:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyaxs3",
          "author": "AdditionalWeb107",
          "text": "Thatâ€™s fair - it should say open weights. And the license is very permissive except for one deployment type",
          "score": 1,
          "created_utc": "2025-12-31 18:12:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwopty3",
          "author": "maigpy",
          "text": "what is model-native?",
          "score": 1,
          "created_utc": "2025-12-30 06:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqe6c",
              "author": "AdditionalWeb107",
              "text": "Itâ€™s integrated with small LLMs - central to how the project is built.",
              "score": -1,
              "created_utc": "2025-12-30 06:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6v7x",
                  "author": "maigpy",
                  "text": "integrated with small llms translates to \"model-native\"?\n\nI don't quite understand, if you could use more words to describe what's going that would help.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnderh",
          "author": "Lyuseefur",
          "text": "Planning already to use it in next build of nexora follow us\n\nHttps://github.com/jeffersonwarrior/nexora",
          "score": 0,
          "created_utc": "2025-12-30 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnwumq",
              "author": "AdditionalWeb107",
              "text": "Okay - thanks. Would love the feedback. And if you like our project, don't forget to star it too",
              "score": 1,
              "created_utc": "2025-12-30 02:56:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0fg09",
      "title": "I got my first ever whitepaper published",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/v23qicvcdy9g1.png",
      "author": "InsideResolve4517",
      "created_utc": "2025-12-31 15:21:16",
      "score": 35,
      "num_comments": 6,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0fg09/i_got_my_first_ever_whitepaper_published/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwyduyb",
          "author": "Lame_Johnny",
          "text": "Amazing! congratulations!",
          "score": 1,
          "created_utc": "2025-12-31 18:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ein",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwynwee",
          "author": "Busy_Farmer_7549",
          "text": "Congratulations ðŸŽˆðŸŽŠ",
          "score": 1,
          "created_utc": "2025-12-31 19:17:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ed2",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwytdq1",
          "author": "PromptOutlaw",
          "text": "Congrats bud!! I really underestimated how much hard work goes into these. Iâ€™ve been humbled recently and Iâ€™m not even half way",
          "score": 1,
          "created_utc": "2025-12-31 19:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25e8e",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3asxm",
      "title": "Which tools should I be looking at? Want to use local AI to tailor my resume to job descriptions",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "author": "cspybbq",
      "created_utc": "2026-01-04 00:16:54",
      "score": 35,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "I'm job hunting and trying to learn more about AI at the same time. \n\nI want the AI to be aware of all my resume versions (15ish versions) and to tailor new versions of my resume based on the contents of those resumes, plus job descriptions I give it. I'd also like it to evaluate a job description and tell if I'm a good fit or not, based on my resumes. \n\nIs this something I can set up on my local computer? \n\n * AMD Ryzen 5700G\n * Nvidia 3070\n * 64G RAM\n * Running Debian\n\nThere are so many models and variants of models that I'm not really sure where to start. I have played a bit with ollama (cli) and open-webui but haven't really figured out how to set up RAG correctly to handle my documents or get any sort of professional level output.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxk0cgf",
          "author": "jinnyjuice",
          "text": "Might want to also post to /r/localllama -- I would be interested in the responses also on how people judge different models to be strong in which dimensions!",
          "score": 2,
          "created_utc": "2026-01-04 02:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjgmey",
          "author": "fandry96",
          "text": "Go to gemini. Make a gem. Add your 10 best resumes to assets. \n\nYou can try defaulting to agent mode to try it. If not click save. \n\nPost the jobs there and tell Gemini to write a resume...",
          "score": 2,
          "created_utc": "2026-01-04 00:46:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxzjhp",
      "title": "Google Open-Sources A2UI: Agent-to-User Interface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 19:08:03",
      "score": 27,
      "num_comments": 8,
      "upvote_ratio": 0.87,
      "text": "Google just released **A2UI (Agent-to-User Interface)** â€” an open-source standard that lets AI agents generate **safe, rich, updateable UIs** instead of just text blobs.\n\nðŸ‘‰ Repo: [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\n# What is A2UI?\n\nA2UI lets agents â€œ**speak UI**â€ using a **declarative JSON format**.  \nInstead of returning raw HTML or executable code (âš ï¸ risky), agents describe *intent*, and the client renders it using **trusted native components** (React, Flutter, Web Components, etc.).\n\nThink:  \nLLM-generated UIs that are **as safe as data, but as expressive as code**.\n\n# Why this matters\n\nAgents today are great at text and code, but terrible at:\n\n* Interactive forms\n* Dashboards\n* Step-by-step workflows\n* Cross-platform UI rendering\n\nA2UI fixes this by cleanly separating:\n\n* **UI generation (agent)**\n* **UI execution (client renderer)**\n\n# Core ideas\n\n* ðŸ” **Security-first**: No arbitrary code execution â€” only pre-approved UI components\n* ðŸ” **Incremental updates**: Flat component lists make it easy for LLMs to update UI progressively\n* ðŸŒ **Framework-agnostic**: Same JSON â†’ Web, Flutter, React (coming), SwiftUI (planned)\n* ðŸ§© **Extensible**: Custom components via a registry + smart wrappers (even sandboxed iframes)\n\n# Real use cases\n\n* Dynamic forms generated during a conversation\n* Remote sub-agents returning UIs to a main chat\n* Enterprise approval dashboards built on the fly\n* Agent-driven workflows instead of static frontends\n\n# Current status\n\n* ðŸ§ª **v0.8 â€“ Early Public Preview**\n* Spec & implementations are evolving\n* Web + Flutter supported today\n* React, SwiftUI, Jetpack Compose planned\n\n# Try it\n\nThereâ€™s a **Restaurant Finder demo** showing end-to-end agent â†’ UI rendering, plus Lit and Flutter renderers.\n\nðŸ‘‰ [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\nThis feels like a big step toward **agent-native UX**, not just chat bubbles everywhere. Curious what the community thinks â€” is this the missing layer for real agent apps?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwgkvue",
          "author": "bananahead",
          "text": "I am begging people to stop with the LLM written posts. Just post whatever prompt you used! Thatâ€™s the post!",
          "score": 13,
          "created_utc": "2025-12-29 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi048d",
              "author": "leonbollerup",
              "text": "Why care? I rather want a LLM generated post than some bad version of English - not everyone speaks English native",
              "score": 4,
              "created_utc": "2025-12-29 05:47:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjksfb",
                  "author": "ak_sys",
                  "text": "Id rather tailor my discussion to a non English speaker than not understanding that there may be a disconnect getting lost in translation. \n\nThis explains a lot of \"reading comprehension\" issues I've noticed from commenters, where they seem to be responding to some diffuse sentiment of the post rather than  the actual nuanced point and position. Languages almost never just directly translate into one another, and inserting an llm in the middle of a discussion without informing the other party seems pretty dishonest and disrespectful.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:44:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwmr4i2",
                  "author": "bananahead",
                  "text": "So write a post in your native language and have it translated. Thatâ€™s just as easy as whatever prompt created this and would be easier to read and more authentic.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgo2bh",
              "author": "Kamal965",
              "text": "The absolute state of Reddit now:\n\n* A lazy OP asks an LLM to create a post for them.\n* Users see LLM-isms and either:\n   * Ctrl + W\n   * Pass it on to an LLM to summarize it for them instead.\n* Another OP gives up on writing their own posts and starts using an LLM because they keep running into LLM-generated posts.\n\nRinse and repeat.",
              "score": 0,
              "created_utc": "2025-12-29 00:53:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nworg5v",
          "author": "ZITNALTA",
          "text": "Just curious if any knows if this somehow works with Google Antigravity IDE? By the way, I am NOT a dev so if this sounds like a newbie question that is why.",
          "score": 1,
          "created_utc": "2025-12-30 06:15:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3os4p",
      "title": "Tencent HY-MT1.5, a specialized machine-translation model",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/rq2666gzsbbg1.png",
      "author": "etherd0t",
      "created_utc": "2026-01-04 12:29:17",
      "score": 25,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3os4p/tencent_hymt15_a_specialized_machinetranslation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pxpeh7",
      "title": "Device to run a local LLM mainly for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "author": "knibroc",
      "created_utc": "2025-12-28 11:40:21",
      "score": 21,
      "num_comments": 31,
      "upvote_ratio": 0.9,
      "text": "Hi mates,\n\nI mostly use ChatGPT and Mistral (through their \"vibe coding\" cli tool and API). I don't pay for these services, so I only use the lesser-capable models.\n\nMy laptop is not powerful enough to run this (no GPU / I've experimented with ollama but I can only run the smallest models very slowly so this is not ok for daily use), so I'm currently considering building a device dedicated to running a LLM, mainly for coding purposes. Ideally something small, Raspberry Pi-based or similar would be great.\n\nI have a few questions: is there specialized hardware for this (I've heard of TPU/NPU)? What kind of performance can I expect (I'd need at least GPT4/Devstral level)? I'm also worried about speed (tokens/s) and cost.\n\n  \nAny advice is appreciated!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdmlhq",
          "author": "KrugerDunn",
          "text": "The cost of any device that can run a decent coding model will far out scale just paying for Claude Code and still wonâ€™t be nearly as good or future proof.\n\nI went down this rabbit hole so you donâ€™t have to ðŸ˜‚",
          "score": 39,
          "created_utc": "2025-12-28 15:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdstym",
              "author": "skabaru",
              "text": "2nd. $100/month to Claude code is the best money you can spend here.... And I do have an older gaming rig running local llms... And it isn't even close.",
              "score": 12,
              "created_utc": "2025-12-28 16:19:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwekb8b",
                  "author": "New_Jaguar_9104",
                  "text": "I have an entire cluster and still pay for Claude max. It's worth its weight in gold IMO",
                  "score": 7,
                  "created_utc": "2025-12-28 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwffg7z",
              "author": "ThatOneGuy4321",
              "text": "whatâ€™s your threshold for â€œdecentâ€? 70B?",
              "score": 1,
              "created_utc": "2025-12-28 21:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgn3r5",
                  "author": "KrugerDunn",
                  "text": "I mean, depends what you're trying to do. I personally just love using Claude Code and occasionally Gemini Cli, so nothing is going to compare to those that can be run locally. Maybe could get away with a GLM4.6-AIR which is 357B params. Unless you're working on super duper secret proprietary code or something that violates the foundation model guardrails I just don't see any reason to use local.\n\nI know some people have used something like Qwen Code 32B or Devstral 24B and been satisfied with it, but never been worth it to me.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdph75",
          "author": "TyphoonGZ",
          "text": "If you're not satisfied with 1-5 toks/s on CPU (\"coffee break\" workflow), sounds like you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nWhy 30B? 20--30B is the size range where the model is still (sort of) considered \"small\" yet it starts being actually useful.\n\nThat said, you should consider using Openrouter and spend $5 for credits to test said models and see if they're good enough for you. You wouldn't want to buy a ~$1000 GPU just to get annoyed that your model's too braindead, right?\n\nRegarding TPU/NPU, I haven't heard if NPUs finally have the necessary software infrastructure to be useful. Well, they wouldn't really help LLMs if there's no development in memory bandwidth to go with them.\n\nOn the other hand, Google sells Coral TPUs, but those are for computer vision, not LLMs, and anyway, they only have *megabytes* of memory.",
          "score": 8,
          "created_utc": "2025-12-28 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweoi73",
              "author": "Count_Rugens_Finger",
              "text": ">  you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nI can run Qwen3-coder-30B-A3B at 12 tok/sec on my 8GB 3070 and an absolute potato of a CPU",
              "score": 2,
              "created_utc": "2025-12-28 18:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgwndy",
                  "author": "TyphoonGZ",
                  "text": "Oh yeah, I forgot MoE models exist...\n\nAlso damn, that's janky. Nice.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj61mb",
              "author": "nasone32",
              "text": "today with 24gb vram you can run qwen3 coder at 150 tokens/s with 100k+ context, or something like qwen3 next 80B at 15/20 tokens/s offloading some layers.",
              "score": 1,
              "created_utc": "2025-12-29 12:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcvibf",
          "author": "AnxietyPrudent1425",
          "text": "If you have a desktop with a GPU or Mac Mini/Studio you can setup Tailscale and basically have your own cloud endpoint. My setup is a MacBook Air + 128GB Mac Studio + Linux workstation and I couldnâ€™t be happier.",
          "score": 5,
          "created_utc": "2025-12-28 13:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcygiq",
              "author": "knibroc",
              "text": "if only I could afford a 128GB Mac Studio! these machines look great",
              "score": 4,
              "created_utc": "2025-12-28 13:24:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdccz6",
          "author": "Background_Gene_3128",
          "text": "Atm. Iâ€™ve dedicated a small server in the addict to run my llms, with a i5-12600k, 32gb ddr5 and a 3060 with 12gb vram. \nRunning proxmox with a ubuntu vm and ollama. \n8-14b models np, 14-24 okay, and the 30-34b models a bit too slow for my liking. \nSo Iâ€™ve upgraded to 96gb ram (not sure if that actually matter, but Iâ€™ve seen people get decent speeds with gpt-oss 120b with ram as offload, and found a used offer locally that didnâ€™t require a kidney) \nAnd 2 5060 Tiâ€™s as theyâ€™re on sale now here in Europe for â‚¬370 a piece. \n\nNot sure if this is the best budget setup or small, but itâ€™s what Iâ€™m rocking until thatâ€™s not enough.\nItâ€™s fitted in a Jonsbo D32 pro mesh case.",
          "score": 4,
          "created_utc": "2025-12-28 14:52:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjuvo",
          "author": "BigYoSpeck",
          "text": "A used desktop/gaming PC or server is probably the most cost effective way in\n\n\nSomething with 64gb of either DDR4 or 5, a 6+ core CPU, and a 16gb or more GPU\n\n\nI recently purchased a Ryzen 9 5900X 64gb DDR4-3600 with a Radeon RX 6800 XT from eBay and I can run gpt-oss-120b with full context at just over 20 tok/s\n\n\nIn the near future I'd like to swap out the Radeon for an RTX 3090 but ROCm is fairly good these days in llama.cpp \n\n\nEfficiency and performance won't match a Mac with comparable memory but it's a fraction of the price and doubles up for gaming duties",
          "score": 3,
          "created_utc": "2025-12-28 15:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf81pw",
              "author": "andriizahorui",
              "text": "Hey, I have a similar config and struggle to run gpt 120b with llama vulkan at decent speeds. Could you please share how do you run it with full context at 20 tps? Like exact command and stuff please",
              "score": 2,
              "created_utc": "2025-12-28 20:26:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfjs4k",
                  "author": "BigYoSpeck",
                  "text": "I'm not at my computer at the moment to give the exact parameters I pass to llama-server but I know the key ones are:\n\n   --threads 8 (5-8 are all very close, after 8 performance declines)\n\n   --flash-attn on\n\n   --mlock --no-mmap\n\n   --n-gpu-layers 99\n\n   --n-cpu-moe 32 (going down to 28 at lower context is faster, 32 is the sweet spot for having space left though) \n\n\nThis is with a self compiled ROCm build, pre built Vulkan docker isn't quite as fast",
                  "score": 5,
                  "created_utc": "2025-12-28 21:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcq3d3",
          "author": "DenizOkcu",
          "text": "Buying a used MacBook with an Apple Silicon might be your best/cheapest bet. They leverage unified memory and use mlx as a native LLM engine. I use devstral small 2 on a M3 with 36GB RAM in LM Studio. Nvidias Nemotron 3 nano is even faster with great coding results incl tool usage.",
          "score": 3,
          "created_utc": "2025-12-28 12:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcyl0f",
              "author": "knibroc",
              "text": "indeed Nemotron 3 sounds cool, will check, thanks!",
              "score": 2,
              "created_utc": "2025-12-28 13:25:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdgs27",
                  "author": "DenizOkcu",
                  "text": "It works really well with Nanocoder an open source Coding Tool with a focus on privacy/local LLMs (disclaimer: I am one of the contributors)",
                  "score": 3,
                  "created_utc": "2025-12-28 15:17:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwghwj2",
          "author": "machaao",
          "text": "In our tests, we couldn't get any of the single current open source code LLM to work at workable speed that is 25 tokens per second or so for a medium size code base.\n\nAs soon as you want something to be demanding it kinda barfs and token / second goes down the drain \n\nWould love to hear success stories tho ðŸ˜Œ\n\nP.S. Tried with gpt oss and qwen3 on M4 - 128G",
          "score": 2,
          "created_utc": "2025-12-29 00:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkp2kz",
          "author": "HiddenPingouin",
          "text": "The closest to claude code would be GLM4.7. You could run the q8 on a Mac Studio with 512GB of RAM",
          "score": 2,
          "created_utc": "2025-12-29 17:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnt1op",
              "author": "teleolurian",
              "text": "Pretty well, I would add.",
              "score": 1,
              "created_utc": "2025-12-30 02:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcml86",
          "author": "EternalVision",
          "text": "TPU is not really possible, only Google has those (developed them themselves). And your question really depends on your budget, the sky really is the limit here. An Ryzen Strix Halo 395 AI MAX+ based minipc is what could work out, but again, really depends on your budget.",
          "score": 2,
          "created_utc": "2025-12-28 11:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwisuw6",
          "author": "HealthyCommunicat",
          "text": "If u want a portable, the m4 max 128 gb is gunna be the best ur gunn get, if u dont have the money for that, the z13 flow. Load gpt oss 120b at high reasoning, qwen 3 next 80b, go checkout my post i did about testing all these models - keep in mind the z13 flow is half the cost so the token/s will be literally half.",
          "score": 1,
          "created_utc": "2025-12-29 10:04:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2h9o6",
          "author": "xcr11111",
          "text": "I came down to two viable options for that, but it don't think that any one of them can totally compete with the Claude code. Cheapest option I went for was an used m1 pro max 64gb. Bought one like new for 1200 bucks. It's amazing hardware, but tbh I don't get used to MacOS. Installed omarchy bit the metal drivers a not fast here(half the speed in llms). The other cool option ist an framework Desktop PC with 128 gig. It's an absolute beast for everything, but very expensive and prices will increase very soon because of ram.",
          "score": 1,
          "created_utc": "2026-01-01 12:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf1i4b",
          "author": "Jarr11",
          "text": "Don't do it! It will cost you more to run it yourself than it would to just pay for a subsciption to Claude/ChatGPT/Gemini for CLI access",
          "score": 1,
          "created_utc": "2025-12-28 19:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf6esp",
          "author": "Oki667",
          "text": "Lol, just pay for Claude code monthly subscription.",
          "score": 1,
          "created_utc": "2025-12-28 20:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiurfb",
          "author": "Crazyfucker73",
          "text": "You aren't going to get GPT4 level on a fucking raspberry pi mate.",
          "score": -1,
          "created_utc": "2025-12-29 10:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjammv",
              "author": "knibroc",
              "text": "And you are being fucking helpful mate",
              "score": 5,
              "created_utc": "2025-12-29 12:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2opaa",
      "title": "Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1q2onpg",
      "author": "atif_dev",
      "created_utc": "2026-01-03 07:44:51",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2opaa/built_a_fully_local_ai_assistant_with_longterm/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q2gtfg",
      "title": "How big is the advantage of CUDA for training/inference over other branded GPUs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "author": "Massive-Scratch693",
      "created_utc": "2026-01-03 01:19:25",
      "score": 18,
      "num_comments": 15,
      "upvote_ratio": 0.91,
      "text": "I am uneducated in this area but want to learn more. I have been considering getting a rig to mess around with Local LLM more and am looking at GPUs to buy. It would seem that AMD GPUs are priced better than NVIDIA GPUs (and I was even considering some Chinese GPUs). \n\nAs I am reading around, it sounds like NVIDIA has the advantage of CUDA, but I'm not quite sure what this really is and why it is an advantage. For example, can't AMD simply make their chips compatible with CUDA? Or can't they make it so that their chips are also efficient running PyTorch?\n\n\n\nAgain, I'm pretty much a novice in this space, so some of the words I am using I don't even really know what they are and how they relate to others. Is there an ELI5 on this? Like...the RTX 3090 is a GPU (hardware chip). Is CUDA like the firmware that allows the OS to use the GPU to do calculations? And is it that most LLM tools written with CUDA API calls in mind but not AMD's equivalent firmware API calls? Is that what makes it such that AMD is less efficient or poorly supported with LLM applications?\n\n\n\nSorry if the question doesn't make much sense... ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxd024v",
          "author": "Own_Attention_3392",
          "text": "CUDA is proprietary; Nvidia controls the design and implementation and hardware details. It's basically a programming language that compiles to a format that only works on Nvidia hardware. \n\nROCm is the open source (non-proprietary) AMD equivalent. Most tools support both, it's just that CUDA is the \"standard\" and just about everything is guaranteed to support it.",
          "score": 16,
          "created_utc": "2026-01-03 01:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxd3jl3",
              "author": "Massive-Scratch693",
              "text": "Thanks for your response! So if I used any nonNVIDIA GPU chip (AMD, Huawei Ascend, Biren), it is likely to support ROCm?\n\n  \nIf I wanted to run models like Stable Diffusion, DeepSeek, and most other popular models, you would anticipate not only support for ROCm, but also similar performance relative to CUDA?",
              "score": 5,
              "created_utc": "2026-01-03 01:56:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxd8lqs",
                  "author": "Own_Attention_3392",
                  "text": "AMD will definitely support ROCm. Can't speak to the others; I buy Nvidia cards generally. \n\nIt probably won't be top tier performance compared to CUDA -- you'll want to look at benchmarks to make sure what you're buying will fit your needs. I assume you're looking at a deepseek distillation because the full deepseek model is huge and requires a server class GPU with tons of VRAM. Be realistic about what you'll be able to run -- the best local models will still be out of reach for even high end consumer hardware. \n\nAnything with 16+ GB of VRAM will be fine for all the current image generation models as far as I know. I'm nuts and have a 5090 so I don't need to worry about it too much.",
                  "score": 4,
                  "created_utc": "2026-01-03 02:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxd80wt",
          "author": "Count_Rugens_Finger",
          "text": "llama.cpp also supports Vulkan, which is not proprietary.",
          "score": 6,
          "created_utc": "2026-01-03 02:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd8irg",
          "author": "Shep_Alderson",
          "text": "What sort of budget are you aiming for? CUDA is the standard for most LLM tools, but support for other options is growing.",
          "score": 2,
          "created_utc": "2026-01-03 02:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdhyet",
              "author": "Massive-Scratch693",
              "text": "I'm trying to figure out a good budget right now and I think part of that is figuring out whether I am spending money on brand rather than performance. I am somewhat skeptical as to whether the performance per dollar is really best with NVIDIA or if I am better off paying for other GPUs",
              "score": 1,
              "created_utc": "2026-01-03 03:21:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe4i5r",
                  "author": "Geeotine",
                  "text": "You're definitely paying brand tax on both Nvidia and to a smaller extent, AMD. Nvidia does have a performance advantage as well, but not really an order of magnitude greater (<10x). Nvidia has invested billions of dollars and over 15 years into the CUDA platform, AMD is has made great strides trying to catch up on the last 3 years with ROCm, so don't expect it to be as polished as cuda. At the same time, there isn't any real alternatives between those two. \n\nEveryone else is either years behind, or unobtainable (Google/AWS asic AI chips). Also, even though ROCm is open source, it's up to the hardware developers to design hardware/firmware/drivers to support it, so for now only AMD products support ROCm architecture.\n\nYour biggest deciding factors are your budgets/requirements for power, RAM (both VRAM & system memory), and time.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:53:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxe0pma",
          "author": "CooperDK",
          "text": "AMD is a bad choice, it has far worse capabilities and things often fail to work due to rocm emulating cuda.",
          "score": 4,
          "created_utc": "2026-01-03 05:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdmjq9",
          "author": "arousedsquirel",
          "text": "Use a free tiers acces to gpt or gemini and ask this question. It will explain? 16gb is not adequate,  not yet. Do some research.",
          "score": 1,
          "created_utc": "2026-01-03 03:50:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe1n6y",
          "author": "No-Consequence-1779",
          "text": "For inference, cuda has a huge advantage for working with large contexts. Â Iâ€™m not sure of your budget but an older model is better than a new and. Â The M5 should have almost equivalent context processing and 2x performance on token generation. Â For 10 grand. Â \n\nWhen you get into finetuning (most do lot), it makes a difference also.Â \n\nUltimately people get what they can budget. Â So much of the discussion ends up being theoretical and already done 1000 times. Â ",
          "score": 1,
          "created_utc": "2026-01-03 05:32:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxej9dp",
              "author": "Massive-Scratch693",
              "text": "When you say M5, do you mean apple M5? It looks like it is only a few grand, not 10 grand. Maybe I'm misunderstanding?",
              "score": 1,
              "created_utc": "2026-01-03 07:56:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxf5w5l",
                  "author": "HumanDrone8721",
                  "text": "Is 10 grand because for effective LLM usage you need the the maxed CPU/RAM version.",
                  "score": 2,
                  "created_utc": "2026-01-03 11:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfou3z",
          "author": "Charming_Support726",
          "text": "I am not a fan of local training, because most times it is far to slow und VRAM is a scarce resource. So it depends on your budget. \n\nCUDA is more stable and you get most things working OOB. The AMD stuff works in most cases with Vulkan or ROCm with similar performance - but needs more config. \n\nI am running a AMD Strix Halo Box (cheaper than a 5090) - and with the exception of some exotic stuff I got everything running, but mostly I use containers to be safe that I could repeatedly run everything. Most dense models are to slow and e.g. a quantized GLM also runs only around 10 tok/s \n\nSo I run big or dense LLMs in the cloud. But that's a No-Issue: Because It would take more than one 5090 anyway to run such models.",
          "score": 1,
          "created_utc": "2026-01-03 13:31:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1k83q",
      "title": "Is it possible to have a local LLM update spreadsheets and read PDFs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "author": "new-to-reddit-accoun",
      "created_utc": "2026-01-02 00:41:31",
      "score": 17,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "So far I've tried [Jan.ai](http://Jan.ai) (Jan-v1-4B-Q4\\_K\\_M) and Msty (Qwen3:0.6b) with no luck: the model in Jan says it can't output an updated file, and Mysty's model claims to but won't give the path name to where it's allegedly saved it. \n\nRelated, I'm looking for a local LLM that can read PDFs (e.g. bank statements). \n\nUse case I'm trying to build a local, private app that reads bank/credit card statements, and also update various values in a spreadsheet. \n\nWould love suggestions!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx68hde",
          "author": "SignificantCod728",
          "text": "You might be looking for something like Actual Budget.",
          "score": 7,
          "created_utc": "2026-01-02 00:49:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx69tcx",
          "author": "No-Consequence-1779",
          "text": "Yes. Youâ€™ll use Python to call the LLM api, and then update the spreadsheets. You can even highlight specific cells. - basically, do anything to excel sheets with Python. Same for pdf - reading is simple. Â ",
          "score": 4,
          "created_utc": "2026-01-02 00:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb3jgu",
              "author": "new-to-reddit-accoun",
              "text": "Thank you this the least friction solution it sounds like",
              "score": 1,
              "created_utc": "2026-01-02 19:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc0vah",
                  "author": "No-Consequence-1779",
                  "text": "Yes. Python is originally for data science so there is so much available working with sheets and documents. Â A LLM can craft a pretty solid starter script for you )Â ",
                  "score": 1,
                  "created_utc": "2026-01-02 22:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6mszf",
          "author": "Porespellar",
          "text": "This works great for me with Open WebUI \n\nhttps://github.com/GlisseManTV/MCPO-File-Generation-Tool",
          "score": 4,
          "created_utc": "2026-01-02 02:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9c2et",
          "author": "l_Mr_Vader_l",
          "text": "you'd need VLMs, LLMs are not gonna cut it. Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nStart with qwen3-VL 8B, it's a sweet spot. If you have the infra and need super good accuracy, go for 32B.\n\n\nYou don't need an LLM to write spreadsheets, simple openpyxl and pandas should do the job.\n\nNone of the other replies here make sense",
          "score": 3,
          "created_utc": "2026-01-02 14:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa49ru",
              "author": "pantoniades",
              "text": ">Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nWhat is the advantage of OCR over extracting text first? Curious because I had just assumed the opposite - that reading the pdfs in python and extracting text would leave less room for errors... \n\nI do like the idea of less code to maintain, of course!",
              "score": 1,
              "created_utc": "2026-01-02 16:55:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxaiuiw",
                  "author": "Whoa_PassTheSauce",
                  "text": "VLM isn't OCR technically as far as I'm aware, and while I have not done this locally, even using flagship model API's I have found text extraction better handled by the model vs extraction on my side and feeding the results.\n\nMy use case involves extracting data from pdf's and images, same layout usually but the type or file varies.",
                  "score": 1,
                  "created_utc": "2026-01-02 18:04:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxd49rv",
                  "author": "l_Mr_Vader_l",
                  "text": "One word -  layouts\n\nThere's no pdf reader that preserves layouts accurately, all the time. Everyone writes pdf differently, there's no standard to this. You wanna read all kinds of tables accurately all the time, no deterministic pdf reader can do that for you that'll work on all kinds of PDFs\n\nWith LLMs no matter how big of a SOTA model you use, you end up feeding garbage essentially that came out of a deterministic text extractor, they will struggle at complex layouts\n\n\nBut some VLMs are getting there now, because they read it how PDFs are meant to be read. Through vision!",
                  "score": 1,
                  "created_utc": "2026-01-03 02:01:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7p9z7",
          "author": "ChemistNo8486",
          "text": "Not sure about the spreadsheet, but you can use AnythingLLM and add a vision model like QWEN for reading PDFs. \n\nYou can also add an agent to save .txt documents. You probably can add something to convert them.",
          "score": 1,
          "created_utc": "2026-01-02 06:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8weya",
          "author": "fandry96",
          "text": "Gemma might help you. The way it can nest can protect your data.",
          "score": 1,
          "created_utc": "2026-01-02 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl0m8a",
          "author": "Future_Command_9682",
          "text": "Try launching the LLM as a server (with Jan.ai if you like) but use Opencode as the agent scaffolding.\n\nhttps://opencode.ai/\n\nThis worked amazingly well for me.",
          "score": 1,
          "created_utc": "2026-01-04 06:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmavi4",
          "author": "LiaVKane",
          "text": "Feel free to request elDoc community version https://eldoc.online/blog/how-to-extract-data-from-invoices-using-genai/ in case you would like to have ready to deploy solution. It is already shipped with several OCRs (Qwen3-VL, PaddleOCR,), CV, MongoDB, Qdrant, RAG, Apache Solr. You just need to connect your preferable LLM locally depending on your available resources.",
          "score": 1,
          "created_utc": "2026-01-04 13:07:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q377q3",
      "title": "Ollama + chatbox app + gpt oss 20b = chat gpt at home",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "author": "Birdinhandandbush",
      "created_utc": "2026-01-03 21:48:32",
      "score": 17,
      "num_comments": 20,
      "upvote_ratio": 0.82,
      "text": "My workstation is in my home office, with ollama and the LLM models. It's an i7 32gb and a 5060ti. \nAround the house on my phone and android tablet I have the chatbox AI app. \nI've got the IP address for the workstation added into the ollama provider details and the results are pretty great.\nCustom assistants and agents in chatbox all powered by local AI within my home network. \nReally amazed at the quality of the experience and hats off to the developers. Unbelievably easy to set up. ",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxlgq4t",
          "author": "eliadwe",
          "text": "I have home server with unraid and RTX3060 12gb running a Win11 vm with ollama, Iâ€™m using the abliterated version of gpt-oss-20b which gives me much better performance. Using it with LLAMAZING app on my iPhone combined with tailscale. Also using embeddinggemna for the RAG inside the app.\n\n\nThe model Iâ€™m using:\nhttps://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-mxfp4-abliterated-v2",
          "score": 5,
          "created_utc": "2026-01-04 08:51:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlsppp",
              "author": "Birdinhandandbush",
              "text": "What's the difference with abliterated versions",
              "score": 1,
              "created_utc": "2026-01-04 10:39:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlu5es",
                  "author": "eliadwe",
                  "text": "I get 28t/s with the abliterated version vs much lower performance with the regular version",
                  "score": 2,
                  "created_utc": "2026-01-04 10:52:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxntdcj",
              "author": "cuberhino",
              "text": "how does this work for you? are you happy with the results compared to say chatgpt? this looks like exactly what im looking to do",
              "score": 1,
              "created_utc": "2026-01-04 17:49:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj6wfg",
          "author": "-Akos-",
          "text": "Personally I like LM Studio better, it gives me better performance, and Granite 4 is an excellent model for me. On my potato laptop with a 1050 and 4GB VRAM and 16GB normal ram on an 8th gen I7, I still am able to do 50000 tokens, and it runs at about 14 tokens per second. Another advantage is that I can enable MCP in LM Studio, and Iâ€™ve installed a websearch MCP, so even though itâ€™s a small model, I can search for fresh information.",
          "score": 5,
          "created_utc": "2026-01-03 23:55:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq4am6",
              "author": "Look_0ver_There",
              "text": "Upvote on using LM-Studio.  I'm running LM-Studio myself on my 7900XTX.  With the gpt-oss-20b model it's running at >166tok/sec.  Ollama and GPT4All both run WAY slower.",
              "score": 2,
              "created_utc": "2026-01-05 00:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjhqf7",
              "author": "cuberhino",
              "text": "Iâ€™ve been messing with lm studio on my pc. How can I use it on the pc from my phone?",
              "score": 1,
              "created_utc": "2026-01-04 00:52:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlj43t",
                  "author": "vertical_computer",
                  "text": "LM Studio exposes a remote API via the developer tab. Itâ€™s an â€œOpenAPI-compatibleâ€ endpoint, so you can use it remotely with any number of frontends that support it.\n\nBy far the most common/popular choice _(and what I use personally)_ is [Open WebUI](https://github.com/open-webui/open-webui) which has a ChatGPT-esque look to it. Itâ€™s a lightweight webserver that you can run easily as a docker container.\n\nOnce set up, you can configure Open WebUI to connect to any remote API, such as Anthropic or ChatGPT. In this case youâ€™d point it at your locally hosted LM Studio API.\n\nSince itâ€™s web based, you can use it directly on your phone via web browser, so long as youâ€™re on the same network as your PC.\n\n_Additionally, you could try out [Conduit](https://github.com/cogwheel0/conduit), which is a native mobile app for accessing Open WebUI, and is available on the App Store/Play Store. Youâ€™d still need to set up Open WebUI, but IMO the Conduit interface is a bit nicer on my phone than trying to use Open WebUI in a mobile browser. This is very optional though, you can just use it in a browser._",
                  "score": 1,
                  "created_utc": "2026-01-04 09:12:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxijai8",
          "author": "fandry96",
          "text": "Look up gemma.",
          "score": 3,
          "created_utc": "2026-01-03 21:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxijgd0",
              "author": "Birdinhandandbush",
              "text": "The Gemma 3 models or something else?",
              "score": 1,
              "created_utc": "2026-01-03 21:55:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnrdai",
                  "author": "jesus359_",
                  "text": "Gemma3 models. 12B and 27B are really up there. \n\nI like OSS but its waaay more agentic than just a regular model.",
                  "score": 2,
                  "created_utc": "2026-01-04 17:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxik00t",
                  "author": "fandry96",
                  "text": "You are thinking Gemini? 3 pro or flash?\n\n\nGemma is a local LLM that breaks down into dimensions. Think 4 AIs in one. I use 4n and 2n I think, locally.",
                  "score": 0,
                  "created_utc": "2026-01-03 21:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pxn4nh",
      "title": "GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fd7m7g23ww9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-28 09:17:24",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxn4nh/glm_47_is_now_the_1_open_source_model_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwcbfj3",
          "author": "arousedsquirel",
          "text": "lots of promo and even more ccp gaurdrails. i think zai interpreted the 2000 question list to strict.",
          "score": 5,
          "created_utc": "2025-12-28 10:04:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py9m5q",
      "title": "Requested: Yet another Gemma 3 12B uncensored",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "author": "Mabuse046",
      "created_utc": "2025-12-29 02:09:36",
      "score": 14,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "Hello again!\n\nYesterday I released my norm preserved biprojected abliterated Gemma 3 27B with the vision functions removed and further fine tuned to help reinforce the neutrality. I had a couple of people ask for the 12B version which I have just finished pushing to the hub. I've given it a few more tests and it has given me an enthusiastic thumbs up to some really horrible questions and even made some suggestions I hadn't even considered. So... use at your own risk.\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF)\n\nLink to the 27B redit post:  \n[Yet another uncensored Gemma 3 27B](https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using Â **L**ayerwiseÂ **I**mportanceÂ **S**ampledÂ **A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwihhi4",
          "author": "darkbit1001",
          "text": "I ran with ollama (ollama run hf.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF:Q4\\_K\\_M) and it just repeats over and over the word 'model'. any reason this would happen?",
          "score": 4,
          "created_utc": "2025-12-29 08:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjayep",
              "author": "Mabuse046",
              "text": "Thank you for pointing this out. I'm looking into it and finding there were apparently some configuration issues in the original Google models, particularly in the way they handled the BOS token that have given some ollama users a headache with Gemma 3 GGUF's. I am currently editing my config.json files and adding the chat template in three different places on both models based on the Unsloth fix and will push fresh gguf's shortly.",
              "score": 3,
              "created_utc": "2025-12-29 12:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwugiab",
                  "author": "lookwatchlistenplay",
                  "text": "This isn't the model escaping confinement... is it?",
                  "score": 1,
                  "created_utc": "2025-12-31 02:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjs5nv",
              "author": "Mabuse046",
              "text": "Fresh ggufs have been pushed and the original transformers versions have been updated. I don't normally use ollama but I went ahead and installed it to try it out. I used the run command with the hf repo and it chatted just fine in the terminal. I connected to it in SillyTavern to give it another test and it took some fiddling but I got it to hold a conversation just fine in there in both Chat Completions and Text Completions mode.",
              "score": 3,
              "created_utc": "2025-12-29 14:27:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxcdfyd",
                  "author": "darkbit1001",
                  "text": "Thanks, should I use a different template? right now it repeats the n-word and tells me it want to f\\*\\*k me over and over ðŸ« ",
                  "score": 1,
                  "created_utc": "2026-01-02 23:30:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwh2ymo",
          "author": "3-goats-in-a-coat",
          "text": "I'll try using it with EchoColony in rimworld. Thanks.",
          "score": 1,
          "created_utc": "2025-12-29 02:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqmooc",
          "author": "Dramatic-Rub-7654",
          "text": "If itâ€™s not a bother and if youâ€™re able to, could you do the same with one of TheDrummerâ€™s versions? TheDrummer/Fallen-Gemma3-27B-v1 or TheDrummer/Fallen-Gemma3-12B-v1.",
          "score": 1,
          "created_utc": "2025-12-30 15:00:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx17fab",
              "author": "Mabuse046",
              "text": "https://preview.redd.it/dsif5fmd1oag1.png?width=1399&format=png&auto=webp&s=9acd92fd41f6b9b52a9ef8426c9fd8cf6626cfbd\n\nCurrent status... first I realized that Drummer has the config.json for 12B duplicated in his 27B, which had some incorrect dimensions so I had to correct it and test it locally, but then, I'm getting some weird measurements when I try to abliterated it that make it look like they already abliterated it and either didn't get it completely, or they added a small amount of their own back in, it's hard to say. But the divergence between harmful and harmless is practically non-existent.",
              "score": 3,
              "created_utc": "2026-01-01 04:32:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx1a2l1",
                  "author": "Dramatic-Rub-7654",
                  "text": "This is very strange, because this model clearly retains safety traits from the original model. I ran several tests trying to merge it with other Gemma Heretic models I found on Hugging Face, and in every merge attempt, questions that the Heretic versions answered without any issue would cause the merged model to refuse to respond. I also tried generating a LoRA from the difference between this Fallen model and the official Instruct version, but that didnâ€™t work either, which makes me think that the model they shared was already fine-tuned somewhere else.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrljfp",
              "author": "Mabuse046",
              "text": "I'll have a look at it. Currently have my system working on beefing up my dataset. Should have some free time shortly.",
              "score": 2,
              "created_utc": "2025-12-30 17:46:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrpr1x",
                  "author": "Legal_Pudding_4464",
                  "text": "I would second this request, but regardless thanks for this model!",
                  "score": 1,
                  "created_utc": "2025-12-30 18:05:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwukk76",
                  "author": "Dramatic-Rub-7654",
                  "text": "Thanks a lot, no rush at all. When you manage to publish it, please give me a heads-up. In my case, Iâ€™m only interested in the text layers, so if you remove the vision part, thatâ€™s totally fine with me.",
                  "score": 1,
                  "created_utc": "2025-12-31 02:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q14nkv",
      "title": "DeepSeek AI Launches mHC Framework Fixing Major Hyper Connection Issues in Massive LLM",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u40qcfe6tqag1.jpeg",
      "author": "techspecsmart",
      "created_utc": "2026-01-01 13:51:23",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q14nkv/deepseek_ai_launches_mhc_framework_fixing_major/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q0j55m",
      "title": "OpenCV 4.13 brings more AVX-512 usage, CUDA 13 support, many other new features",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/OpenCV-4.13-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2025-12-31 17:56:27",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0j55m/opencv_413_brings_more_avx512_usage_cuda_13/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q2iuzi",
      "title": "Run Claude Code with ollama without losing any single feature offered by Anthropic backend",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2iuzi/run_claude_code_with_ollama_without_losing_any/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-03 02:50:11",
      "score": 13,
      "num_comments": 14,
      "upvote_ratio": 0.88,
      "text": "Hey folks! Sharing an open-source project that might be useful:\n\nLynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.\n\n  \nKey features:\n\n\\- Route between multiple providers: Databricks, Azure Ai Foundry, OpenRouter, Ollama,llama.cpp, OpenAi\n\n\\- Cost optimization through hierarchical routing, heavy prompt caching\n\n\\- Production-ready: circuit breakers, load shedding, monitoring\n\n\\- It supports all the features offered by claude code like sub agents, skills , mcp , plugins etc unlike other proxies which only supports basic tool callings and chat completions.\n\nGreat for:\n\n\\- Reducing API costs as it supports hierarchical routing where you can route requstes to smaller local models and later switch to cloud LLMs automatically.\n\n\\- Using enterprise infrastructure (Azure)\n\n\\-Â  Local LLM experimentation\n\n\\`\\`\\`bash\n\nnpm install -g lynkr\n\n\\`\\`\\`\n\nGitHub:Â [https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr)Â (Apache 2.0)\n\nWould love to get your feedback on this one. Please drop a star on the repo if you found it helpful",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2iuzi/run_claude_code_with_ollama_without_losing_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxe4y0g",
          "author": "Big-Masterpiece-9581",
          "text": "Claude code router already does this the best",
          "score": 3,
          "created_utc": "2026-01-03 05:57:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe5t21",
              "author": "Dangerous-Dingo-5169",
              "text": "Claude code router doesnt offer live websearch, subagents this proxy also has heavy prompt caching and also intelligent routing. This proxy also implements ACE framework and long term memory feature which learns from the projects and recrafts the prompt for better outputs",
              "score": 3,
              "created_utc": "2026-01-03 06:04:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhto1q",
                  "author": "Big-Masterpiece-9581",
                  "text": "Mkay. I trust your vibe coded redo instead of just Claude code with a different model and 25k stars.",
                  "score": 1,
                  "created_utc": "2026-01-03 19:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxef2v3",
          "author": "Direct_Turn_1484",
          "text": "So not local?",
          "score": 2,
          "created_utc": "2026-01-03 07:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxef5o6",
              "author": "Dangerous-Dingo-5169",
              "text": "It supports local llm models hosted via ollama and llama.cpp",
              "score": 1,
              "created_utc": "2026-01-03 07:21:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxiqsg2",
                  "author": "aaronr_90",
                  "text": "But donâ€™t still have to authenticate with Anthropic?",
                  "score": 1,
                  "created_utc": "2026-01-03 22:31:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxf4pw2",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 0,
          "created_utc": "2026-01-03 11:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgnf54",
              "author": "Dangerous-Dingo-5169",
              "text": "Hi pokemonplayer . Thanks for stopping by to reply. Did you not like some part of it or something?",
              "score": 1,
              "created_utc": "2026-01-03 16:33:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxh7eyy",
                  "author": "pokemonplayer2001",
                  "text": "\"Did you not like some part of it or something?\"\n\nBegging for engagement. ðŸ‘Ž",
                  "score": 1,
                  "created_utc": "2026-01-03 18:06:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0t02i",
      "title": "Basic PC to run LLM locally...",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "author": "Mr_FuS",
      "created_utc": "2026-01-01 01:55:20",
      "score": 12,
      "num_comments": 18,
      "upvote_ratio": 0.94,
      "text": "Hello, a couple of months ago I started to get interested on LLM running locally after using ChatGPT for tutoring my niece on some high school math homework.\n\n\n\nEnded getting a second hand Nvidia Jetson Xavier and after setting it up and running I have been able to install Ollama and get some models running locally, I'm really impressed on what can be done on such small package and will like to learn more and understand how LLM can merge with other applications to make machine interaction more human.\n\nWhile looking around town on the second hand stores i stumble on a relatively nice looking DELL PRECISION 3650, it is running a i7-10700, and 32GB RAM... could be possible to run dual RTX 3090 on this system upgrading the power supply to something in the 1000 watt range (I'm neither afraid or opposed to take the hardware out of the original case and set it on a test bench style configuration if needed!)?  \n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx0p0g4",
          "author": "LittleBlueLaboratory",
          "text": "So looking at the specs and pictures of a Dell 3650 it does look like they use standard ATX power supplies so you could upgrade that. But the motherboard only has 1 PCI-E x16 slot and not enough room to physically fit a second 3090 anyway.",
          "score": 6,
          "created_utc": "2026-01-01 02:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx179e0",
              "author": "Proof_Scene_9281",
              "text": "Just saved the griefÂ ",
              "score": 3,
              "created_utc": "2026-01-01 04:30:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0rsc3",
          "author": "FullstackSensei",
          "text": "I'd look for a generic desktop instead; something built around a regular ATX board. If you intend to put two 3090s, you'll need something that allows splitting the CPU lanes across two slots with at least X8 each.\n\nIf you want to stick to pre-builts from major brands, then look for workstation ass machines. If you can find something that takes DDR4 RAM and has some memory installed, you'll be most of the way there. DDR4 workstation platforms will have at least 4 memory channels, so you get a lot more memory bandwidth than that 10700, which is very nice for CPU offload.",
          "score": 4,
          "created_utc": "2026-01-01 02:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2f36z",
          "author": "Mugen0815",
          "text": "Ive never heard of a Dell supporting dual-gpu. Do they even support std-psus?",
          "score": 3,
          "created_utc": "2026-01-01 11:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0mgt1",
          "author": "Caprichoso1",
          "text": "Have you looked at a Mac?  It might allow you to run larger models.  An NVDIA CPU will be bette at some things, the Mac at others.",
          "score": 4,
          "created_utc": "2026-01-01 02:08:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0o3ym",
              "author": "LittleBlueLaboratory",
              "text": "They are looking at a used 10th gen Dell. That kind of budget isn't going to get them a Mac.",
              "score": 3,
              "created_utc": "2026-01-01 02:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2s4k0",
                  "author": "Makers7886",
                  "text": "he was talking about mac and cheese",
                  "score": 2,
                  "created_utc": "2026-01-01 13:33:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx56te1",
                  "author": "Caprichoso1",
                  "text": "Macs start at  $599 for the mini.  The best value comes from Apple Refurbished store which are as good as new.  Stock is constantly changing so it make take a while to find the exact model/configuration you want.",
                  "score": 1,
                  "created_utc": "2026-01-01 21:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2vk5c",
          "author": "kinkvoid",
          "text": "Not worth it. I would buy a second hand Mac studio.",
          "score": 1,
          "created_utc": "2026-01-01 13:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0o1le",
          "author": "jsconiers",
          "text": "The easiest and most cost effective solution would be to get an m1 or m2 Mac.  After that you could find an old workstation PC like an HP z6 or z4 for cheap that you can add 3090s to.  I started off with a used acer n50 with a GTX 1650.  Then upgraded that PC until it made sense to build something.  (It was very limited as it only had one PCIe slot and max 32Gb of memory)  Finally built a system before the ram price jump.  Glad I built it but itâ€™s idle more than I thought.  Speed and loading the model will be the biggest concern.",
          "score": 0,
          "created_utc": "2026-01-01 02:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0rtu2",
          "author": "StardockEngineer",
          "text": "If you're hoping to replace ChatGPT, I have bad news.\n\nIf you're doing it just because it's interesting, no problem there.  Just set your expectations accordingly.  As far as that Dell, no idea.  I don't know what it looks like inside.  If there is space and PCI ports, it probably can run two GPUs.  Whether it'll support regular PSUs, no idea.  Dells I've worked with the past had their own special sized power supplies.",
          "score": 0,
          "created_utc": "2026-01-01 02:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx11bst",
              "author": "fasti-au",
              "text": "Actually you can do almost everything but slower and small user count.   The gpt are not 1 model itâ€™s a lie in many ways but also true in others. \n\nNo you canâ€™t get chat got now on local but you can get 4o ish if not better in some and worse in others.  \n\nCintext is the issue for multi user not for single user. And parameters and training are distilling to open models in weeks or months. Not what you think and thereâ€™s shortcuts batter you understand where itâ€™s breaking.\n\nI would speculate that home llm on 96gb vram can compete in smal use with agentic flows. In a useable speed. \n\nIs it cheaper.   Depends on cost of your time",
              "score": 2,
              "created_utc": "2026-01-01 03:48:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1a4t1",
                  "author": "StardockEngineer",
                  "text": "Well, you can't. Coding isn't there yet, and creative writing might require a mix of models. Language classification tasks are best with Gemma 3. Image OCR type stuff is best in Llama 4 Maverick (Qwen3 models are pretty good for image descriptions).\n\nModel mixing is pretty standard to get good results. I run a stack of LiteLLM -> [llama.cpp, private cloud, etc] to wrap it all together.\n\nHome models can't do agents at Claude's level, but simpler agents work fine. gpt-oss-120b is solid for easier agentic use cases. Planning to try Minimax 2.1 next.\n\nBottom line - you'll need to do a lot of mix and matching, and lots of leg work.  Or you can just pay the sub.  If someone has the tinkerer's spirit, I say go for it.  I think it's a lot of fun, whether it's superior or not.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx10iwl",
          "author": "fasti-au",
          "text": "2 x 3090s gets you local coding in devstral and qwen3.  4 gives you 130b models and stronger. \n\nIâ€™d buy if cheap but you can get 3x 5060s also.   Lanes on board and space is your issue so tisersbcooling and 4x16 boards.  \n\nDo it but I had 6 3090s already rendering \n\n\nIâ€™d pay for api.   Get open router.  Use frees for everything you can and lean on lmarena and google freebies for one shot big requests and keep all little Q/a prep in local.    Ask the questions well and it need big models for non planning",
          "score": 0,
          "created_utc": "2026-01-01 03:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0omxd",
          "author": "TheAussieWatchGuy",
          "text": "Local LLMs are far inferior to Cloud proprietary models.\n\n\nReally depends on your budget. I would not recommend anyone go 3090 anymore, way too old.\n\n\nMac or Ryzen AI CPU with lots of RAM (which is sadly super expensive now because of AI).Â ",
          "score": -6,
          "created_utc": "2026-01-01 02:22:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1781j",
              "author": "Proof_Scene_9281",
              "text": "4 of them really shines if you maintain consumer expectationsÂ ",
              "score": 1,
              "created_utc": "2026-01-01 04:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0cwkh",
      "title": "Any local llm code assistant?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "author": "SAF1N",
      "created_utc": "2025-12-31 13:22:49",
      "score": 9,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I'm looking for a code assistant type of thing, it should run locally and I can ask it questions about my codebase and it will give me short/concise answers. Is there anything like that?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwzk77u",
          "author": "Dangerous-Dingo-5169",
          "text": "You can use https://github.com/Fast-Editor/Lynkr to connect Claude code cli to your local llms without losing any features offered by anthropic backend like sub agents etc",
          "score": 2,
          "created_utc": "2025-12-31 22:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1yz0d",
          "author": "DenizOkcu",
          "text": "Try Nanocoder. Privacy und local-first open source CLI. Disclaimer: I am a contributor (working on Nanocoder in Nanocoder ðŸ¤£)\n\nhttps://github.com/Nano-Collective/nanocoder",
          "score": 2,
          "created_utc": "2026-01-01 08:48:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx6v2v",
          "author": "nofilmincamera",
          "text": "Questions or any refactoring?",
          "score": 1,
          "created_utc": "2025-12-31 14:51:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxlpmv",
              "author": "SAF1N",
              "text": "being able to refactor or generate new code would be a bonus feature and rarely used (by me). my main use case would be just asking questions.",
              "score": 1,
              "created_utc": "2025-12-31 16:07:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx396lt",
          "author": "imagent42",
          "text": "opencodeCLI  [https://opencode.ai/](https://opencode.ai/)",
          "score": 1,
          "created_utc": "2026-01-01 15:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxazo8r",
          "author": "Lissanro",
          "text": "Roo Code has Ask mode, it is great for asking questions about the code base. In my experience works best with K2 Thinking (I run Q4\\_X quant with ik\\_llama.cpp), but it may work with other models too in case you prefer something else.",
          "score": 1,
          "created_utc": "2026-01-02 19:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxyak9",
          "author": "PermanentLiminality",
          "text": "[Continue.dev](http://Continue.dev) is easy to point at your local LLM.  It is a VSCode plug in and I believe it now has a CLI.",
          "score": 1,
          "created_utc": "2025-12-31 17:09:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxdsn1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-31 15:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxkzc2",
              "author": "SAF1N",
              "text": "This is a GUI, you can do the same in vscode with only extensions.\n\nI'm looking for a terminal only application.",
              "score": 1,
              "created_utc": "2025-12-31 16:03:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwxs2f6",
                  "author": "InsideResolve4517",
                  "text": "I've tried 1 terminal only application like qwen, gemini.\n\nIn  local Continue (cli) mode.\n\nAnd one another tool I tried (forgot the name, and not able to find that tool) that didn't worked for me (I think it will work beyond 14b like 20b models\n\nI'm not able to remember the name",
                  "score": 1,
                  "created_utc": "2025-12-31 16:38:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx27arq",
                  "author": "Consistent_Wash_276",
                  "text": "If you host gpt-oss models you can use them with codex. \n\nBut opencode is my preferred terminal option.",
                  "score": 1,
                  "created_utc": "2026-01-01 10:17:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1h6el",
      "title": "Anyone have success with Claude Code alternatives?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1h6el/anyone_have_success_with_claude_code_alternatives/",
      "author": "jackandbake",
      "created_utc": "2026-01-01 22:31:03",
      "score": 9,
      "num_comments": 11,
      "upvote_ratio": 0.85,
      "text": "The wrapper scripts and UI experience of \\`vibe\\` and \\`goose\\` are similar but using local models is a horrible experience. Has anyone found a model that works well for using these coding assistants?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1h6el/anyone_have_success_with_claude_code_alternatives/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxa1sob",
          "author": "HealthyCommunicat",
          "text": "OpenCode. Has the most wide level of compatability when it comes to local llm usage. Use ohmyopencode, you can also use claude plugins with them - u can also use ur antigravity oauth login so you can basically pay for gemini pro and also get claude opys 4.5 with it. When it comes to local usage, even smaller models like qwen 3 30b a3b are still able to do tool calls without decent execution rate.",
          "score": 5,
          "created_utc": "2026-01-02 16:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxag88u",
          "author": "alphatrad",
          "text": "OpenCode is has the widest range of compatibility: [https://github.com/sst/opencode](https://github.com/sst/opencode)",
          "score": 2,
          "created_utc": "2026-01-02 17:51:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5l5hh",
          "author": "th3_pund1t",
          "text": "```\ngemini () {\n\tnpx @google/gemini-cli@\"${GEMINI_VERSION:-latest}\" \"$@\"\n}\nqwen () {\n\tnpx @qwen-code/qwen-code@\"${QWEN_VERSION:-latest}\" \"$@\"\n}\n```\n\nThese two are pretty good.",
          "score": 2,
          "created_utc": "2026-01-01 22:39:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5x3gg",
              "author": "Your_Friendly_Nerd",
              "text": "Can I ask, why are you wrapping them in these functions? why not do npm i -g?",
              "score": 2,
              "created_utc": "2026-01-01 23:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx62zbl",
                  "author": "th3_pund1t",
                  "text": "`npm i -g` makes it my problem to update the version.\nWrapping in a bash function allows me to always get the latest version, unless I choose to pin back.\n\nAlso, I'm not a nodejs person. So I might be doing that wrong.",
                  "score": 3,
                  "created_utc": "2026-01-02 00:19:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5xq09",
          "author": "Your_Friendly_Nerd",
          "text": "I just use the chat plugin for my code editor which provides the basic features needed for the ai to edit code. usimg qwen3-code 30b, I can give it basic tasks and it does them pretty well, though always just simple stuff like â€žwrite a function that does xâ€œ, nothing fancy like â€žthereâ€˜s a bug that causes y somewhere in this project, figure out how to fix itâ€œ",
          "score": 1,
          "created_utc": "2026-01-01 23:49:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6nohs",
          "author": "noless15k",
          "text": "Which models are you using?\n\nI find these the best locally on my Mac Mini M4 Pro 48GB device using llama.cpp server with settings akin to those found here:\n\n\\* [https://unsloth.ai/docs/models/devstral-2#devstral-small-2-24b](https://unsloth.ai/docs/models/devstral-2#devstral-small-2-24b)  \n\\* [https://unsloth.ai/docs/models/nemotron-3](https://unsloth.ai/docs/models/nemotron-3)\n\nAnd to your question, I use Zed's ACP for Mistral Vibe with devstral-small-2. It's not bad, though a bit slow.\n\nI certainly see a difference when running the full 123B devstral-2 via Mistral Vibe (currently free access), which is quite good. But the 24B variant is at least usable.\n\nI like nemo 3 nano for its speed. It's about 4-5x faster for prompt processing and token generation.\n\nIt works pretty well within Mistral Vibe and if you want to see the thinking setting --reasoning-format to none in llama.cpp seems to work without breaking the tool calls. I had issues getting nemo 3 nano working with zed's default agent.\n\nI haven't tried Mistral Vibe directly from the CLI yet though.",
          "score": 1,
          "created_utc": "2026-01-02 02:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7em2u",
              "author": "jackandbake",
              "text": "Good info thank you. Have you got the tools to work and complex multi-tasks working with this method?",
              "score": 1,
              "created_utc": "2026-01-02 05:14:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8k6ty",
          "author": "Lissanro",
          "text": "The best local model in my experience is Kimi K2 Thinking. It runs about 1.5 times faster than GLM-4.7 on my rig despite being larger in terms total parameters count, and feels quite a bit smarter too (I run Q4\\_X quant with ik\\_llama.cpp).",
          "score": 1,
          "created_utc": "2026-01-02 11:26:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfkkw7",
          "author": "dragonbornamdguy",
          "text": "I love qwen code, but vllm has broken formatting for it (qwen3 coder 30b). So I use LM studio (with much slower performance).",
          "score": 1,
          "created_utc": "2026-01-03 13:04:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5k4fb",
          "author": "Lyuseefur",
          "text": "Nexora will be launching on January 5. Follow along if you would like - still fixing the model integration into the CLI but the repo will be at [https://www.github.com/jeffersonwarrior/nexora](https://www.github.com/jeffersonwarrior/nexora)",
          "score": -2,
          "created_utc": "2026-01-01 22:34:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2z4rl",
      "title": "Future-proofing strategy: Buy high unified memory now, use entry-level chips later for compute?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2z4rl/futureproofing_strategy_buy_high_unified_memory/",
      "author": "Consistent_Wash_276",
      "created_utc": "2026-01-03 16:38:03",
      "score": 9,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "Just thinking out loud here about Apple Silicon and wanted to get your thoughts.\n\nSetting aside DGX Spark for a moment (great value, but different discussion), Iâ€™m wondering about a potential strategy with Appleâ€™s ecosystem:\nWith M5 (and eventually M5 Pro/Max/Ultra, M6, etc.) coming + the evolution of EVO and clustering capabilitiesâ€¦\n\nCould it make sense to buy high unified memory configs NOW (like 128GB M4, 512GB M3 Ultra, or even 32/64GB models) while theyâ€™re â€œaffordableâ€?\nThen later, if unified memory costs balloon on Mac Studio/Mini, youâ€™d already have your memory-heavy device. You could just grab entry-level versions of newer chips for raw processing power and potentially cluster them together.\n\nBasically: Lock in the RAM now, upgrade compute later on the cheap.\n\nAm I thinking about this right, or am I missing something obvious about how clustering/distributed inference would actually work with Apple Silicon?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2z4rl/futureproofing_strategy_buy_high_unified_memory/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxgpvlc",
          "author": "RoyalCities",
          "text": "If you're just doing inference sure but if you plan on training it's a different story. \n\nAfaik Apple hasn't raised their prices to nosebleeds yet but l would expect they will sometime this year given they're seen as cheaper right now so their own demand will also spike later.\n\nI think Apple also relies on Samsung / SK Hynix Fabs via long term contracts and those contracts will also be going up whenever they end so yeah enjoy it while it lasts.\n\nEdit: was just reading it now and those contracts are up early this year....so yeah probably a price jump whenever the renegotiation are done. End of month maybe, if not possibly by atleast Q2 because Apple is always high margin and can eat the difference for a bit.\n\nGiven how messed up the market is I would think securing a multi year contract at the worst possible time isn't great for them so it could drag.",
          "score": 6,
          "created_utc": "2026-01-03 16:45:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgvq9i",
              "author": "Consistent_Wash_276",
              "text": "yeah, wouldn't train on Apple. But clustering with DGX spark would do the trick there if someone was interested. We have a lot of opportunity to benefit from today's prices and tomorrows processing options with future clustering developments it feels like.",
              "score": 3,
              "created_utc": "2026-01-03 17:12:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxgzvjs",
                  "author": "RoyalCities",
                  "text": "I get the rationale. For inference they're probably the cheapest but in a sea of insanity it's always tough to say anyone's benefiting with any price on tech lol. \n\nI also think NVME drives are set to go up soon. They're also going to be impacted with the DRAM shortages since NAND Flash has parallel supply constraints.\n\nAll of it compounds. I train models so secured my rig all the way back last march since I saw this insanity coming but if I was looking at a pure inference machine and I needed it sooner and couldn't wait a year or 2 then I'd probably pick up an Apple machine.",
                  "score": 1,
                  "created_utc": "2026-01-03 17:32:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhbcq7",
              "author": "PracticlySpeaking",
              "text": ">was just reading it now and those contracts are up early this year\n\n[WCCFTECH](https://wccftech.com/apple-could-begin-paying-samsung-and-sk-hynix-a-premium-for-dram-from-january-2026/) labels that 60% rumor. Unless someone with a Digitimes subscription would like to share the article with the sources...\n\nPricing for soon-to-be-released M5 Pro, Max, and Ultra will be a key indicator of how good Apple's DRAM supply contracts are, and what actually expired. The majority of the DRAM they buy is ordinary LPDDR for iPhones.\n\nIt will be interesting times, for sure. RAM is only a fraction of the BOM cost, especially for MacBook Pro and Mac Studio. They could choose to eat the difference until prices come down. Their size gives them a lot of leverage, so they will be the last to suffer, and the least.",
              "score": 4,
              "created_utc": "2026-01-03 18:23:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxh6gyd",
          "author": "dwkdnvr",
          "text": "Unfortunately I think the answer is â€œwe donâ€™t know yetâ€. The Exo / RDMA stuff is exciting, but itâ€™s still not entirely clear how it will play out and what the real-world capabilities are. The Exo folks did post a teaser showing a DGX â€˜clusteredâ€™ to a Mac Studio over TB implying that prompt processing on the Nvidia and inference on the Mac was better than just running the Mac, but it was short on details. At some point even TB5 bandwidth is going to be a limitation, and itâ€™s entirely possible that the NPU improvements in the M5 series will be good enough to make the added complexity of clustering unattractive.\n\nThere also was a post a couple months back about someone getting an AMD gpu working (for compute, not graphics output) in a TB PCIE dock on an Apple Silicon Mac, but this was just a POC and I donâ€™t believe it was to the point of actually being able to run LLM models on it. Personally I think this is more intriguing as it seems more likely to gain adoption in the â€˜budget crowdâ€™  - dropping a 7900XTX into a dock is less daunting than managing multiple machines. But once again - whether this is actually something that willl work â€˜in productionâ€™ remains to be seen.\n\nIâ€™m struggling with the same questions. I have a 64GB M1 Max, and really want to move to at least 128GB. But an M5 Max128GB  might be $5k+ with the ram shortage. An M4 Max is too limited in PP compared to what â€˜should beâ€™ coming to make me comfortable sinking the $$ into it at the moment, although if the external GPU idea materializes that would probably change. Iâ€™m seriously eyeing a Framework Strix Halo motherboard with the idea that the X4 slot might allow hybrid GPU/Unified functionallity, but I havenâ€™t actuallly seen any results suggesting you can divide PP/TG to really get the best of both worlds. (i.e. basically the same idea as an external GPU on a Mac, but with lower TG performance)",
          "score": 5,
          "created_utc": "2026-01-03 18:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi74yd",
              "author": "Consistent_Wash_276",
              "text": "I think you half way there and on the same direction. Hold onto the M1 feels like the play",
              "score": 2,
              "created_utc": "2026-01-03 20:54:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcq8l",
          "author": "PracticlySpeaking",
          "text": "Someone who knows this can comment... how effectively (or not) does MacOS clustering get around the basic need to have RAM close to the compute?",
          "score": 2,
          "created_utc": "2026-01-03 18:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgx51g",
          "author": "PermanentLiminality",
          "text": "It is all engineering trade offs against cost, memory bandwidth, memory size and compute capacity.  Each solution has different strengths and weaknesses.  Everything depends on your budget, your workloads, etc.\n\nThere is no one solution.",
          "score": 1,
          "created_utc": "2026-01-03 17:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkiz63",
          "author": "Caprichoso1",
          "text": "There is no way to predict the future. Get what meets your needs now and in the future.\n\nGiven the high cost (and margins?) of Apple memory, and their memory contracts it is entirely possible that they won't raise prices.  \n\nNo one but Apple knows.",
          "score": 1,
          "created_utc": "2026-01-04 04:23:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}