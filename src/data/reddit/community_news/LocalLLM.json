{
  "metadata": {
    "last_updated": "2026-02-25 17:22:58",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 248,
    "file_size_bytes": 256282
  },
  "items": [
    {
      "id": "1r90rxi",
      "title": "How much was OpenClaw actually sold to OpenAI for? $1B?? Can that even be justified?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/p8c453eapgkg1.jpeg",
      "author": "Alert_Efficiency_627",
      "created_utc": "2026-02-19 14:34:16",
      "score": 208,
      "num_comments": 84,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r90rxi/how_much_was_openclaw_actually_sold_to_openai_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o68xq98",
          "author": "Theseus_Employee",
          "text": "They didn’t pay anything for it. They just hired the dude that made it, and are sponsoring the free open-source project OpenClaw.\n\nThe tweet is just a joke of people just inflating how much money you can make from vibe coded projects.",
          "score": 204,
          "created_utc": "2026-02-19 14:40:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aewgr",
              "author": "ArcticCelt",
              "text": "Also, people talk about it as a \"vibe-coded\" project, but they fail to mention that even if he used AI, it was done by a programmer with over 20 years of experience who sold his previous software startup for $100 million. It is not some random person with no experience.",
              "score": 35,
              "created_utc": "2026-02-19 18:57:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6aykqj",
                  "author": "huzbum",
                  "text": "Yeah, but he clearly did NOT put his engineering skills to work in there... glaring security flaws.  ",
                  "score": 9,
                  "created_utc": "2026-02-19 20:32:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cbn97",
                  "author": "oompa_loompa0",
                  "text": "Yeah. Watch the interview on Lex Friedman. Peter is no vibe coder, he is a brilliant seasoned engineer with crazy depth and breadth.",
                  "score": 3,
                  "created_utc": "2026-02-20 00:55:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cu2v7",
                  "author": "ANTIVNTIANTI",
                  "text": "it feels all very much—contrived ",
                  "score": 1,
                  "created_utc": "2026-02-20 02:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ae7xo",
              "author": "structured_flow",
              "text": "He had meetings with Zuck, same guy who is paying 8 figure salaries for their ai leadership positions, and turned Meta down. He absolutely was paid money how much don’t know, but your mistaken if you think there wasn’t a bid.",
              "score": 8,
              "created_utc": "2026-02-19 18:54:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bqhgu",
                  "author": "cjc4096",
                  "text": "He already had a successful exit.   He has the ability to choose what is important to him.",
                  "score": 3,
                  "created_utc": "2026-02-19 22:52:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o69j1ku",
              "author": "HeinerWersenberg",
              "text": "$1B for an unhinged AI-Agent-VibeCoded something did sound ridiculous indeed. However, when thinking of the competition, investments done already in the field, and considering the reasons for current memory and storage prices... who knows. Anything is thinkable theses days.    \nSo, thanks for the clarification. ",
              "score": 5,
              "created_utc": "2026-02-19 16:26:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69qiln",
              "author": "HappyContact6301",
              "text": "Look at Scale AI...",
              "score": 1,
              "created_utc": "2026-02-19 17:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bwdvj",
                  "author": "bronfmanhigh",
                  "text": "scale AI was still a real business with real employees and revenue end of the day lol",
                  "score": 2,
                  "created_utc": "2026-02-19 23:26:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6a1yfu",
              "author": "Demiansmark",
              "text": "Like the guy, like, like, like the guy with the $100B startup is going to pay attention to the subtext. Come on!",
              "score": 1,
              "created_utc": "2026-02-19 17:57:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69ufe3",
          "author": "reb00tmaster",
          "text": "This tweet alone is worth $80B.  In a few months it will be acquired for $160B. /s",
          "score": 47,
          "created_utc": "2026-02-19 17:21:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a6zme",
              "author": "MarkoMarjamaa",
              "text": "It could be turned into vibe-coded-NTF, then it's like $300B!",
              "score": 6,
              "created_utc": "2026-02-19 18:20:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6agkqz",
              "author": "eli_pizza",
              "text": "Unfortunately the \"/s\" is clearly not going to be enough for some people",
              "score": 3,
              "created_utc": "2026-02-19 19:05:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hdy7o",
              "author": "Far-Low-4705",
              "text": "I can type 300 B's, will that be enough?",
              "score": 1,
              "created_utc": "2026-02-20 20:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hf5iq",
                  "author": "reb00tmaster",
                  "text": "no",
                  "score": 1,
                  "created_utc": "2026-02-20 20:10:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69vzrb",
          "author": "sleepy_roger",
          "text": "The crazy thing is how shitty openclaw is. Seems like most people hyped (besides the crypto grifters who you never trust..) had never used a harness and were just larping the entire time.. \n\nCodex/claudecode/droid/opencode provide a much better experience overall.. openclaw isn't even tailored to non tech people. The only thing it really added that made the masses adopt was easy integration into existing chat platforms.",
          "score": 29,
          "created_utc": "2026-02-19 17:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a77wi",
              "author": "MarkoMarjamaa",
              "text": "Worlds fastest growing malware distribution system. ",
              "score": 23,
              "created_utc": "2026-02-19 18:21:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a8v57",
                  "author": "sleepy_roger",
                  "text": "haha yeah for sure.. I kept mine pretty limited, running in a proxmox container with almost no access. Is it cool to talk to it via discord, sure I guess.. but another big issue is the limited context you see as a user within openclaw versus other agent harnesses... not to mention the entire thing looks vibe coded (because it is).",
                  "score": 5,
                  "created_utc": "2026-02-19 18:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6adfj8",
              "author": "crypticG00se",
              "text": "You are missing the main point why these systems are attractive to openai and the like. Sells more tokens. Perfect system to fool non-tech people. Go look at the claw subreddit how people are complaining about model costs with claw. ",
              "score": 11,
              "created_utc": "2026-02-19 18:50:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b3bw1",
                  "author": "sleepy_roger",
                  "text": "Yeah that's a great point, I'm on the $200 max plan and it was the only time I got close to hitting my 5 hour limit... and it seemed like it was actually doing _much_ less than what I do within claude code directly.",
                  "score": 2,
                  "created_utc": "2026-02-19 20:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bu267",
              "author": "parapa-papapa",
              "text": ">Codex/claudecode/droid/opencode provide a much better experience overall.\n\n  \nI agree that OpenClaw sucks, but what is the competition? \n\nIt is insecure, yes. And that's why the big providers aren't touching such systems, but it's MEANT to be insecure. Like, you can't have a useful LL.M based assistant that is safe. ",
              "score": 1,
              "created_utc": "2026-02-19 23:13:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cy6jl",
                  "author": "Anarchaotic",
                  "text": "Claude on desktop is pretty good - not nearly as \"powerful\" as OpenClaw with no restrictions, but I can use it to do a lot of useful things by giving shell access with approvals on commands.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69ffgu",
          "author": "TimChr78",
          "text": "OpenClaw wasn’t sold to OpenAI at all - they hired the creator Peter Steinberger. OpenClaw is open source under the GNU 3.0 license.\n\nAnd no OpenAI is definitely not paying Peter Steinberger 1B.",
          "score": 20,
          "created_utc": "2026-02-19 16:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69qkv2",
              "author": "ptear",
              "text": "I mean, instant billionaire status would be impressive.",
              "score": 3,
              "created_utc": "2026-02-19 17:02:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cvjcw",
                  "author": "ANTIVNTIANTI",
                  "text": "it's all contrived. this shit was to boost the doubters back into believers again, lolol, I saw the reddit posts go from the actual reality of the situation after so many months/years of the hype hyping away so much hype, was finally starting to chill like 1-2% lol, then OpenClaw and MoldBook or bot or wtf ever the name was, lololol. BLEW IT BACK UP. Now all we are going to hear for the forevers is \"BRUH YOU'RE LIKE, 14 MONTHS BEHIND BRUH?!\"\n\n\"SKILL ISSUE BRUH?! STOP BEING 14 MONTHS BEHIND BRUH?!\"\n\n\"IF YOU DON'T SUCK MOLDYBLOCKS CLAW YOU'RE LIKE, 16 MONTHS BEHIND BRUH?!\" ",
                  "score": 3,
                  "created_utc": "2026-02-20 02:57:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6y6did",
              "author": "UN_Rocinante",
              "text": "where did you get GNU 3.0 from? it’s MIT on the git repo",
              "score": 1,
              "created_utc": "2026-02-23 13:37:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6az013",
          "author": "huzbum",
          "text": "OpenAI hired the creator, they didn't buy a platform.  It makes a lot of sense when you consider it was promoting burning Anthropic tokens and now it will promote burning OpenAI tokens.  ",
          "score": 6,
          "created_utc": "2026-02-19 20:34:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvoxg",
              "author": "ANTIVNTIANTI",
              "text": "honestly dude was playing it like both corps paid him, he would say \"Half was done on Claude\" then \"Codex coded it\" which... yeah... I'm full of conspiracy today, ignore me :P",
              "score": 2,
              "created_utc": "2026-02-20 02:58:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d7s4g",
                  "author": "huzbum",
                  "text": "I hadn't paid enough attention to call you right or wrong, but it seems like the smart play to make nice with both of them for just this kind of eventuality.  \n\nMy comments were based on the original name being Clawed Bot and the recommendation to use Opus.  \n\nBut yeah, anything that gets more people addicted to burning more tokens and trusting LLMs with more data and responsibility is good for both of them.  ",
                  "score": 1,
                  "created_utc": "2026-02-20 04:18:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6a8qqn",
          "author": "shryke12",
          "text": "The dollar amounts being thrown around by these companies are ludicrous lol.  Shows just how many dollars our government has been printing.",
          "score": 3,
          "created_utc": "2026-02-19 18:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69vnda",
          "author": "locomocopoco",
          "text": "Dude is not just a vibecoder. Go see what he has done previously. SMH. ",
          "score": 7,
          "created_utc": "2026-02-19 17:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvu76",
              "author": "ANTIVNTIANTI",
              "text": "lol I know, he's a grifter, he's got this damn WUNDERKID Blog, errr, WUNDERMAN! Vunder? vvvvuuuunnnder? Anyways. It pissed me off. I wanted him to be a dipshit so bad. LOLOLOLOLOL :P XDXDXD",
              "score": 1,
              "created_utc": "2026-02-20 02:59:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g3467",
                  "author": "Capital-Result-8497",
                  "text": "you and I have very different definitions of a grifter. this dude is an engineer through and through",
                  "score": 1,
                  "created_utc": "2026-02-20 16:28:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ao0n7",
          "author": "type102",
          "text": "They can justify it by saying anything while being in a bubble.",
          "score": 2,
          "created_utc": "2026-02-19 19:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6arz6x",
          "author": "Baader-Meinhof",
          "text": "You missed the joke. The actual figure seems to be that he was hired for $30M.",
          "score": 2,
          "created_utc": "2026-02-19 20:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cvz4e",
              "author": "ANTIVNTIANTI",
              "text": "also the tweet kept raising the billions, seemed like, hard to miss. LOL",
              "score": 2,
              "created_utc": "2026-02-20 03:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68zq5m",
          "author": "leonbollerup",
          "text": "where does idea that they paid for it come from ?.. is there any actual proff ?",
          "score": 1,
          "created_utc": "2026-02-19 14:50:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69wa86",
              "author": "oureux",
              "text": "No proff but we might be able to find some proof",
              "score": 1,
              "created_utc": "2026-02-19 17:29:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b50a6",
                  "author": "moderately-extremist",
                  "text": "lemme ask chatgpt...",
                  "score": 1,
                  "created_utc": "2026-02-19 21:04:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6brzzr",
                  "author": "leonbollerup",
                  "text": "Would love to see it",
                  "score": 1,
                  "created_utc": "2026-02-19 23:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6af2gm",
          "author": "structured_flow",
          "text": "Even if openai thought that he was a one trick pony and stuck them in a basement with a broken stapler like office space… They still would’ve paid him a lot of money because it’s all about branding so dude absolutely got paid a lot of money. I highly doubt anything close to 1 billion but definitely in the millions.  \n\nIt’s verified that he met with Zuckerberg who’s paying eight figure salaries for their head of AI departments and he turned them down saying that he “ felt more lines with the mission” at OpenAI.",
          "score": 1,
          "created_utc": "2026-02-19 18:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aieum",
          "author": "siegevjorn",
          "text": "They aqui-hired the dev if I understand it correctly.",
          "score": 1,
          "created_utc": "2026-02-19 19:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bzcjr",
          "author": "desexmachina",
          "text": "Manus was $2B, so they had to escalate and paid Pete $5B, kinda wild",
          "score": 1,
          "created_utc": "2026-02-19 23:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eb3c7",
              "author": "Alert_Efficiency_627",
              "text": "Yeah… if OpenClaw can truly be “open” — not OpenAI’s version of “open” — I’d say it could be worth $50B in the years to come.",
              "score": 1,
              "created_utc": "2026-02-20 10:01:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c98be",
          "author": "ithilelda",
          "text": "he might be paid in memory sticks or chatgpt subscription you know.",
          "score": 1,
          "created_utc": "2026-02-20 00:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cqh1v",
          "author": "No_Success3928",
          "text": "https://www.reddit.com/r/myclaw/s/NBiSm73M4p its an investment!",
          "score": 1,
          "created_utc": "2026-02-20 02:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cwmt8",
              "author": "ANTIVNTIANTI",
              "text": "Omfg I'd offer you my first born for showing me this, but wait.... that got weird, I meant....\n\nshit was hilarious. ",
              "score": 2,
              "created_utc": "2026-02-20 03:04:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cwzcn",
                  "author": "No_Success3928",
                  "text": "Crypto bro got a new AI game!",
                  "score": 1,
                  "created_utc": "2026-02-20 03:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e1lqu",
          "author": "satmun",
          "text": "I used to follow the works of (Peter Steinberger) of PSPDFKit before. He is deep into technical things. Probably his X(formerly Twitter) might still have his tweets. This app was named as best app in iOS for few years I think. He used to discuss Kernel level stuff to optmize rendering of PDFs if I remember correctly and well known in the circle of good devs, atleast in Austria I think. I remember him being close to another dev (professor) who is creator of game engine libgdx. I recently saw him dive deep into C++ and other internals of LLMs. So, its a talent pool and network of devs thats important I think. ",
          "score": 1,
          "created_utc": "2026-02-20 08:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k0dpp",
          "author": "1_H4t3_R3dd1t",
          "text": "OpenClaw isn't that amazing it is just a tool that hosts agents insecurely in your system to execute tasks. You can build the concept yourself. The reason he was hireable was because he made a platform out of it which is awesome and the real thing people should be talking about. ",
          "score": 1,
          "created_utc": "2026-02-21 05:21:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lm5ty",
          "author": "stonkister",
          "text": "\"you can just vibecode an open-source project and make $40B in a couple months now\" \n\nyou can...?",
          "score": 1,
          "created_utc": "2026-02-21 13:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o37bo",
          "author": "Mice_With_Rice",
          "text": "It wasnt sold. Nowhere has any official 1st party source claim to buy/sell openclaw. And if you think about it, it makes no sense it would be sold beacuse its under an MIT license with over 600 contributors.",
          "score": 1,
          "created_utc": "2026-02-21 21:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71xys9",
          "author": "ketoatl",
          "text": "I hope they add guard rails so I dont need another computer to run it. ",
          "score": 1,
          "created_utc": "2026-02-24 00:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o692nwj",
          "author": "Investolas",
          "text": "They paid that much for him to sign an nda and stop development. ",
          "score": -1,
          "created_utc": "2026-02-19 15:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o698mfd",
              "author": "Theseus_Employee",
              "text": "Well they didn’t pay him enough apparently because he’s still developing it.",
              "score": 9,
              "created_utc": "2026-02-19 15:36:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69j1e6",
                  "author": "Technical_Ad_440",
                  "text": "have they fixed the security vulnerabilities yet?",
                  "score": 2,
                  "created_utc": "2026-02-19 16:26:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o699t7k",
                  "author": "Investolas",
                  "text": "Sure but it will never be what it would have been without their intervention",
                  "score": -3,
                  "created_utc": "2026-02-19 15:41:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9xifw",
      "title": "Devstral Small 2 24B + Qwen3 Coder 30B Quants for All (And for every hardware, even the Pi)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/s8yw2jndynkg1.png",
      "author": "enrique-byteshape",
      "created_utc": "2026-02-20 14:56:54",
      "score": 143,
      "num_comments": 72,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9xifw/devstral_small_2_24b_qwen3_coder_30b_quants_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6fxrl7",
          "author": "blksunday",
          "text": "Awesome! I use both of these on a Mac mini M4 24GB. I’ll be trying yours later today. Looks promising,.",
          "score": 4,
          "created_utc": "2026-02-20 16:04:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g2td8",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We're very interested in Mac speedups!",
              "score": 3,
              "created_utc": "2026-02-20 16:27:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g4sr3",
          "author": "mac10190",
          "text": "Sweet! I'll give it a shot later this afternoon.\n\nCurrently running dual R9700 32GB GPUs and an RTX 5090 32GB. Been using the dual R9700s to host larger models to act as the brain/orchestrator and then qwen 3 coder 30b on the 5090 for code generation and then tied it all together under the umbrella of Opencode. Testing this as a potential replacement for some of my Gemini CLI tasks.",
          "score": 5,
          "created_utc": "2026-02-20 16:36:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pmc9a",
          "author": "DarthFader4",
          "text": "Excellent work! This is exactly what I've been looking for. I feel like targeting high-end 16GB GPUs is a key audience, like gamers who want to dabble in local LLMs.  I think there are a lot of exciting developments ahead in optimizing models of this size. They're more practical and approachable than requiring a dedicated high-RAM/VRAM setup and we've started seeing models that can actually be useable. Keep up the great work! I've just followed you on Hugging Face.",
          "score": 5,
          "created_utc": "2026-02-22 03:10:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r38fq",
              "author": "enrique-byteshape",
              "text": "Thank you for the kind words! This was exactly part of our motivation. First, there are great quants that fit on larger VRAM devices, but they might be a bit too slow because they're just made to fit (and not benchmarked, people just assume they'll work). Then, there's a clear accuracy cliff when going lower than 4bpw, and we know our technology excells below those ranges.",
              "score": 2,
              "created_utc": "2026-02-22 10:42:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rr1r5",
          "author": "PaMRxR",
          "text": "Always excited I see new byteshape models! Just the right size for my RTX 3090, and they run roughly 2x faster than other quants. Here's some numbers for Devstral-Small-2:\n\n    prompt eval time =    1120.97 ms /  2004 tokens (    0.56 ms per token,  1787.73 tokens per second)\n           eval time =   10315.36 ms /   569 tokens (   18.13 ms per token,    55.16 tokens per second)\n\nRunning with this command:\n\n    llama-server\n    --model \"${models_path}/Devstral-Small-2-24B-Instruct-2512-IQ3_S-3.47bpw.gguf\"\n    --mmproj \"${models_path}/mmproj-bf16.gguf\"\n    --split-mode none\n    --seed 42\n    --ctx-size 128000\n    --n-gpu-layers 99\n    --fit on\n    --fit-target 256\n    --temp 0.15\n    --top-p 1\n    --min-p 0.01\n    --top-k 40\n    --jinja\n    --repeat-penalty 1\n    --cache-type-k q8_0\n    --cache-type-v q8_0\n    -ub 1024\n    --cache-ram 16000\n\nMany thanks, please keep up doing/sharing this amazing work.",
          "score": 4,
          "created_utc": "2026-02-22 13:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fwurh",
          "author": "swupel_",
          "text": "Love the graph style",
          "score": 3,
          "created_utc": "2026-02-20 16:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fxh9j",
              "author": "enrique-byteshape",
              "text": "Thanks! Don't tell the team, but the style is on me ;)",
              "score": 3,
              "created_utc": "2026-02-20 16:02:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gklbd",
          "author": "BillDStrong",
          "text": "So, are these suitable for speculative decoding in llama.cpp? I would assume so, and since you have worked to keep them from falling off the cliff, they could do most of the work and then let a larger version fix the difference, which might result in faster perf for the same accuracy as the normal models?\n\nMaybe?\n\nThe best I have is a P40 24GB, so will  have to test it later.",
          "score": 3,
          "created_utc": "2026-02-20 17:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gxbsx",
              "author": "enrique-byteshape",
              "text": "We have not tried speculative decoding at all with these models, but they have good quality and are performant. If you have a way to use them for such use case, we assume they will work, but we can't really promise anything!",
              "score": 2,
              "created_utc": "2026-02-20 18:46:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ip5r2",
                  "author": "TomLucidor",
                  "text": "Qwen3 and Nemotron has native MTP, please try them as well!",
                  "score": 1,
                  "created_utc": "2026-02-21 00:12:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fr09x",
          "author": "vanguard2286",
          "text": "Which one would you suggest for rrx 4070 8gb vram? I'm kind of new to self hosting LLMs and kind of not quite understanding the chart. I would love your input.",
          "score": 5,
          "created_utc": "2026-02-20 15:32:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsfuw",
              "author": "enrique-byteshape",
              "text": "Thank you for your interest! 8GB of VRAM is fairly limited, so not a lot of good quality models will fit, but if you want to play around with our models, you can try our Devstral [IQ2\\_S-2.34bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.34bpw.gguf) (75.1% quality of original model), our [IQ2\\_S-2.43bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ2_S-2.43bpw.gguf) (80.3% quality) or our [IQ3\\_S-2.67bpw](https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF/blob/main/Devstral-Small-2-24B-Instruct-2512-IQ3_S-2.67bpw.gguf) (87.2% quality, but will fit a smaller context length). You can also try offloading embeddings or some layers on our higher quality quants with ollama or llama.cpp, but this will reduce performance heavily. Let us know what you end up doing and if you enjoy the quants!",
              "score": 5,
              "created_utc": "2026-02-20 15:39:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6g405l",
                  "author": "vanguard2286",
                  "text": "Thank you!",
                  "score": 2,
                  "created_utc": "2026-02-20 16:32:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip21p",
                  "author": "TomLucidor",
                  "text": "Please start testing linear attention models like Nemotron-3-Nano or Kimi-Linear or Ring-Mini-Linear-2.0 or Granite-4.0 with the same methodology. Cus if they are more quant sensitive, that would be very sad. (maybe Gemma 3 and GPT-OSS-20B SWA also get support?)",
                  "score": 1,
                  "created_utc": "2026-02-21 00:11:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fxwfa",
          "author": "jarec707",
          "text": "You mentioned a blog in the post. Link please?",
          "score": 2,
          "created_utc": "2026-02-20 16:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g0nq1",
              "author": "enrique-byteshape",
              "text": "It's at the end of the post! Right here: [https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/](https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/)",
              "score": 3,
              "created_utc": "2026-02-20 16:17:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6i84my",
          "author": "Clear-Lab3427",
          "text": "Thanks so much!",
          "score": 2,
          "created_utc": "2026-02-20 22:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j2h8e",
          "author": "Useful_Disaster_7606",
          "text": "I will forever rue the day I bought an RTX 3060 8GB. But then again I did buy it for less than $220 so I guess it's not that bad. \n\nJust out here feeling FOMO seeing all these amazing models. So close yet so faar.",
          "score": 2,
          "created_utc": "2026-02-21 01:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j60sm",
              "author": "enrique-byteshape",
              "text": "You CAN actually try our low bits per weight Devstral quants so that you don't feel as left out! Under 8GB with enough context length to test them!",
              "score": 1,
              "created_utc": "2026-02-21 01:54:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6j4pyp",
          "author": "Snoo_24581",
          "text": "Thanks for putting this together! Been waiting for good quants of these models. The 24B size is perfect for my 24GB VRAM setup.\n\nHow's the performance on coding tasks compared to the full precision versions? Any significant quality drop?",
          "score": 2,
          "created_utc": "2026-02-21 01:46:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j8efu",
              "author": "enrique-byteshape",
              "text": "If you choose our highest bit per weight quants there is no visible degradation on our benchmarks and our qualitative assessments",
              "score": 1,
              "created_utc": "2026-02-21 02:09:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jtd4a",
          "author": "Count_Rugens_Finger",
          "text": "Going to try these on my RX 9070 XT",
          "score": 2,
          "created_utc": "2026-02-21 04:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfzkd",
              "author": "enrique-byteshape",
              "text": "Let us know how it goes! We tested on an RX 9060 XT 16GB and they got a speedup versus other quants",
              "score": 1,
              "created_utc": "2026-02-21 13:11:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6psl4t",
                  "author": "Count_Rugens_Finger",
                  "text": "Yes I seem to get a pretty good speedup.  I do not have the means to evaluate accuracy.\n\nFor Qwen-3-coder, I go from about 71 t/s at Q4_K_M to about 160 t/s at IQ5 3.48 bpw, both at max GPU offload and the default 4k context.  More than 2x speedup which is amazing.\n\nFor Devstral-2, I can run the \"IQ8\" 4.04 bpw quant at about 34 t/s with max GPU offload and 4k context.  So, can't hold a candle to the 4080, but usable.  Still playing with this one don't know what the speedup looks like.",
                  "score": 2,
                  "created_utc": "2026-02-22 03:53:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ly7ep",
                  "author": "Count_Rugens_Finger",
                  "text": "Your Blog claims that your optimization is specific to Nvidia's 40 and 50 series hardware.  Would you expect the Radeon Vulkan implementation to be just *not as good*, or actively *worse* with your builds vs the standard Q4_K_M quants?",
                  "score": 1,
                  "created_utc": "2026-02-21 15:02:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lgt9b",
          "author": "xeeff",
          "text": "been following you on huggingface for the longest time - finally glad to see some new models. been waiting for these one so long i kinda forgot they are still great models. keep up the good work.\n\np.s. any notes on the model roadmap and an ETA? :)",
          "score": 2,
          "created_utc": "2026-02-21 13:16:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6liwa0",
              "author": "enrique-byteshape",
              "text": "We can't really promise anything, but some diffusion models are in the near to-do list, and we will try to move onto thinking models (which we expect will suppose a big challenge when evaluating them)",
              "score": 2,
              "created_utc": "2026-02-21 13:30:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ljt5s",
                  "author": "xeeff",
                  "text": "always thought why all the models are instruct, but them being harder to evaluate makes sense. did not expect diffusion models to be mentioned, though. any ones in particular? if you'd prefer to not mention, that's perfectly okay",
                  "score": 2,
                  "created_utc": "2026-02-21 13:36:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6re2y1",
          "author": "CalmAndLift",
          "text": "Probé el Qwen3 coder y excelente a 5 tps en mi laptop Intel core ultra 5 con 24 gigas de ram en lmstudio.\nExcelente trabajo",
          "score": 2,
          "created_utc": "2026-02-22 12:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rfpkn",
              "author": "enrique-byteshape",
              "text": "Muchas gracias! Espero que lo disfrutes!",
              "score": 2,
              "created_utc": "2026-02-22 12:33:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6futyd",
          "author": "floppypancakes4u",
          "text": "So your qwen model wont work with a 4090? Do either support a 3090? Looking forward to trying these out.",
          "score": 1,
          "created_utc": "2026-02-20 15:50:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fvbyv",
              "author": "enrique-byteshape",
              "text": "It does support any type of hardware, it's just that our performance benchmarks are only on the hardware that we have available. Sorry we didn't make that clear enough. Our Qwen on the 4090 runs similarly to the 5090 in terms of comparing it to other quants, albeit at a slower TPS.",
              "score": 2,
              "created_utc": "2026-02-20 15:52:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fvrdv",
                  "author": "floppypancakes4u",
                  "text": "Excellent! I'll test both this afternoon.",
                  "score": 3,
                  "created_utc": "2026-02-20 15:54:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6imy7o",
                  "author": "CTR1",
                  "text": "Following up on the question regarding the 3090 compatibility: do you have suggestions for a ideal model to use with a 5800xt + 64gb 3200mhz ram + 3090 pc build?\n\nIdeally something that balances quality | TPS | context and maybe tool calling too? I know that might be a tough ask",
                  "score": 2,
                  "created_utc": "2026-02-20 23:59:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h39q3",
          "author": "oliveoilcheff",
          "text": "What about strix halo? Are there some performance gains there? Thanks!",
          "score": 1,
          "created_utc": "2026-02-20 19:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hx62p",
              "author": "enrique-byteshape",
              "text": "We don't have one in hand, but there should be performance gains on any type of hardware. We would love to hear of the performance you get on it!",
              "score": 1,
              "created_utc": "2026-02-20 21:39:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h3ei6",
          "author": "Simple-Worldliness33",
          "text": "\n\nHi !\n\nThanks for your work! I didn't bench yet but I need to understand completely.\n\nFor an example, I'm using unsloth iq4\\_NL currently with 2 rtx 3060, i got 70/76 tks.\n\nWhich model you are offering should I choose to compare with? I tried the iq4\\_ks but I didn't have the same perf. (Only 35/40tks)",
          "score": 1,
          "created_utc": "2026-02-20 19:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ia35g",
              "author": "enrique-byteshape",
              "text": "Hi! Thanks for trying our models! The performance you might get out of them can vary a lot depending on the hardware, and/or on whether the model is being loaded and ran correctly. Would you mind being more specific about your setup and llama.cpp environment and parameters?",
              "score": 2,
              "created_utc": "2026-02-20 22:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kcwvl",
          "author": "geringonco",
          "text": "Are there any rankings sites for these models?",
          "score": 1,
          "created_utc": "2026-02-21 07:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfsui",
              "author": "enrique-byteshape",
              "text": "Sadly no because no one is willing to evaluate all the released quants to create such ranking sites. It is very expensive",
              "score": 2,
              "created_utc": "2026-02-21 13:09:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l5258",
          "author": "shankey_1906",
          "text": "Any recommendation for Strix Halo?",
          "score": 1,
          "created_utc": "2026-02-21 11:43:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfq11",
              "author": "enrique-byteshape",
              "text": "It depends on the underlying framework and kernels. Most likely our CPU versions will work best, but it would require testing them out",
              "score": 1,
              "created_utc": "2026-02-21 13:09:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l64qw",
          "author": "puru991",
          "text": "When qwen 3.5?",
          "score": 1,
          "created_utc": "2026-02-21 11:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lg5fv",
              "author": "enrique-byteshape",
              "text": "Thank you for the interest, we are aware of the release (as well as of many others), but it's hard to keep up considering that evaluating these quants takes a lot of resources and time. We have to be picky when releasing models, so we usually go by what is popular and what people might really want",
              "score": 1,
              "created_utc": "2026-02-21 13:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lr2fc",
          "author": "Embarrassed-Boot5193",
          "text": "Eu testei o modelo Devstral-Small-2-24B-Instruct-2512-IQ3\\_S-3.47bpw.gguf e não coube na minha GPU de 16GB com 32k de contexto. Vocês estão quantizando o kv cache para isso acontecer?",
          "score": 1,
          "created_utc": "2026-02-21 14:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mohrm",
              "author": "enrique-byteshape",
              "text": "Hey! When we benchmarked Devstral on the different hardware ranges we did so without the vision tower. That's why it might not fit with a context length of 32K. Sorry for the inconvenience!",
              "score": 2,
              "created_utc": "2026-02-21 17:15:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mhyh1",
          "author": "Thrynneld",
          "text": "The new frontier seems to be figuring out how much cruft can be removed from a model before it falls over. So far the rule of thumb has been always use the largest model (parameter wise) that can fit in memory at a quant that does turn itself into gibberish. I've noticed that models with more parameters seem to hold up better at lower quants than smaller models. Is there any chance you guys will be publishing quants of larger popular models? Something like qwen3-coder-next, or even qwen 3.5? What is your bottleneck in this quantization process? do you need to run inference to determine the importance of the weights to quant down more or less? I'm loving Qwen 3.5 on my mac studio, but it sucks up most of my memory at a 3 bit quant while itseems capable enough at 3 bit, I wonder if it would perform better at a \"smarter\" 3 bit version, or even a 2 bit version :)",
          "score": 1,
          "created_utc": "2026-02-21 16:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mmq3c",
              "author": "enrique-byteshape",
              "text": "Our own research and other groups' research has been showing for a while that larger models have a much larger tolerance to quantization and pruning. We've also seen some weight cause outlier activations that matter the most when actually running inference. And we have also observed larger models being quantized aggressively but still being better than smaller models with the same size. Qwen 3.5 is in our roadmap, but our current bottleneck is evaluating these quants so that people can be informed while downloading them. The datatype learning process is actually quite fast on our technology",
              "score": 1,
              "created_utc": "2026-02-21 17:06:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n5kop",
          "author": "siegevjorn",
          "text": "What benchmark are you running?",
          "score": 1,
          "created_utc": "2026-02-21 18:40:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n6bht",
              "author": "enrique-byteshape",
              "text": "Hey! From the very end of our blog post:  \n\"Devstral supports both tool calling and vision, so we evaluated it on:\n\n* **BFCL\\_V3** for tool calling\n* **GSM8K\\_V** for vision\n* **LiveCodeBench V6** and **HumanEval** for coding\n* **GSM8K** and **Math500** for math\n* **MMLU** for general knowledge\n\nThe reported score is the mean across these benchmarks, with each benchmark normalized to the original model's score.\n\nQwen was evaluated using the same setup, with two exceptions:\n\n* No **GSM8K\\_V** (no vision support)\n* No **MMLU** (not a general knowledge evaluation)\n\nAll evaluations were run with llama.cpp `b7744`. We used 4K as the minimum context window required for a model to be considered \"fit\" on a given device.\"",
              "score": 1,
              "created_utc": "2026-02-21 18:44:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ptg3m",
          "author": "Cuaternion",
          "text": "¿En serio hay para Raspberry pi?",
          "score": 1,
          "created_utc": "2026-02-22 03:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r2a94",
              "author": "enrique-byteshape",
              "text": "Si! Los hay!",
              "score": 1,
              "created_utc": "2026-02-22 10:33:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71vaor",
          "author": "Numerous_Mulberry514",
          "text": "could you do qwen coder next as well?",
          "score": 1,
          "created_utc": "2026-02-24 00:36:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o745wn0",
              "author": "enrique-byteshape",
              "text": "It is in the to-do list, but can't promise when and if we will be able to release it :)",
              "score": 1,
              "created_utc": "2026-02-24 10:49:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fvlkz",
          "author": "peyloride",
          "text": "Nice work but I think the real baselin about context should not be 32k because that's very limited in these days. Since these are coding models, context adds very quick in coding agents. I wonder what's the story whe context is around 200k? or even something like 100k? I don't have an idea about what should be the baseline sorry, but 32k seems low.",
          "score": 1,
          "created_utc": "2026-02-20 15:54:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fwyin",
              "author": "enrique-byteshape",
              "text": "32k context is for performance measurements only, which will scale depending on the context length used. For the evaluations we do not limit context length, so those should not be biased. The models will run with any context length as long as it fits. And yes, with longer context lengths, activations start becoming the bottleneck. Sadly, llama.cpp doesn't support quantizing activations to arbitrary datatypes, so at the moment we are limited by that, but our algorithm can also learn the datatypes for them",
              "score": 3,
              "created_utc": "2026-02-20 16:00:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gkqxy",
                  "author": "peyloride",
                  "text": "Yeah I see  your point but context length is important for vram usage. It also affects the accuracy and the TPS (I might be wrong about this). So what I'm trying to say is since these are coding models, you should not test it in 32k context. It might be enough for general usage, but I don't think that's the case for coding models. \n\nIf this is not possible at the time being that's okey; just wanted to flag this out. ",
                  "score": 2,
                  "created_utc": "2026-02-20 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78hikn",
          "author": "Solid-Pop-3452",
          "text": "Maybe the best LLM model is the friendships we made along the way",
          "score": 0,
          "created_utc": "2026-02-25 00:04:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdnlvl",
      "title": "Qwen releases new Qwen3.5 Medium models!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vztwlpot9hlg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-24 18:04:08",
      "score": 107,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdnlvl/qwen_releases_new_qwen35_medium_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o76mfn6",
          "author": "randygeneric",
          "text": "keen on testing your GGUFs , )  \n[https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF](https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF) : 2, 3, 4, 5, 6, 8, 16 bit  \n[https://huggingface.co/unsloth/Qwen3.5-27B-GGUF](https://huggingface.co/unsloth/Qwen3.5-27B-GGUF) :  2, 3, 4, 5, 6, 8, 16 bit\n\n(updated)",
          "score": 5,
          "created_utc": "2026-02-24 18:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76nlil",
              "author": "yoracale",
              "text": "The 35b ones are already all up: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF\n\nFor the others bf16 but should be all up within 1 hr",
              "score": 2,
              "created_utc": "2026-02-24 18:46:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o774ing",
              "author": "randygeneric",
              "text": "\\# ollama --version\n\nollama version is 0.17.0\n\n/# ollama run  [hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3\\_K\\_XLError:](http://hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q3_K_XLError:) 500 Internal Server Error: unable to load model: /root/.ollama/models/blobs/sha256-d0d8d528ae4ebace9588496249cfb6e45c6e9fa78565b4ccff71e7515202956\n\n: (",
              "score": 1,
              "created_utc": "2026-02-24 20:03:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782d4k",
                  "author": "zach9824",
                  "text": "Until ollama is updated for `qwen35moe` it's a no go. There is already an active pull request (PR #14134) in the official Ollama repository to patch in support for the `qwen35moe` architecture. ",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o781dmg",
                  "author": "yoracale",
                  "text": "Ollama doesn't support it yet and do not support GGUFs properly out of the gate anymore.",
                  "score": 1,
                  "created_utc": "2026-02-24 22:38:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o780jd0",
          "author": "wave_action",
          "text": "Guess I have something to do tonight!  Will be interesting to see how 35B 4bit compares to 27B 6bit.",
          "score": 2,
          "created_utc": "2026-02-24 22:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76vohv",
          "author": "waltpinkman",
          "text": "We really need real vllm support now with all these gguf models popping up",
          "score": 1,
          "created_utc": "2026-02-24 19:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781f6z",
              "author": "yoracale",
              "text": "Vllm supports GGUFs already, just not new ones",
              "score": 1,
              "created_utc": "2026-02-24 22:38:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78u86i",
                  "author": "lenjet",
                  "text": "vLLM GGUF support is listed as being \"highly experimental\"... that doesn't scream reliable. ",
                  "score": 1,
                  "created_utc": "2026-02-25 01:13:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77ko9m",
          "author": "Much-Researcher6135",
          "text": "EEK EEK EEK\n\nAs a denseboi myself, I wonder how the dense qwen3.5 27b will stack up against the dense qwen3 32b",
          "score": 1,
          "created_utc": "2026-02-24 21:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77n45x",
          "author": "Infinite-Campaign837",
          "text": "Should I update qwen coder3 next 80b in q6KL to the new 35b-a3b? I use it for coding tasks.\nThey don't compare them in blog post",
          "score": 1,
          "created_utc": "2026-02-24 21:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781hcu",
              "author": "yoracale",
              "text": "Probably not. Coder next is still better for coding",
              "score": 1,
              "created_utc": "2026-02-24 22:38:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77oet2",
          "author": "Awaken0395",
          "text": "Should we expect smaller models coming soon?",
          "score": 1,
          "created_utc": "2026-02-24 21:35:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781isr",
              "author": "yoracale",
              "text": "According to Qwen team Jungyang, yes",
              "score": 1,
              "created_utc": "2026-02-24 22:38:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ka65",
          "author": "Count_Rugens_Finger",
          "text": "Running 35B-A3B Q4_K_M at about 22 tok/sec\n\nIt's ok so far.  Its programming ability seems to be about the same as Qwen3",
          "score": 1,
          "created_utc": "2026-02-25 03:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77r9cv",
          "author": "NerasKip",
          "text": "Hein ???",
          "score": 0,
          "created_utc": "2026-02-24 21:49:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9zo0u",
      "title": "Why AI wont take your job and my made up leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/qaz3ln1ncokg1.jpeg",
      "author": "Eventual-Conguar7292",
      "created_utc": "2026-02-20 16:16:34",
      "score": 76,
      "num_comments": 67,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9zo0u/why_ai_wont_take_your_job_and_my_made_up/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ggzb5",
          "author": "promethe42",
          "text": ">22 users programming with ChatGPT\n\nYeah it's not 2024 anymore...\n\nAll the people claiming coding is done are not using ChatGPT and manual prompts. They are using agentic coding system with meta prompts, skills, etc... with tasks running for 1 to 2 hours autonomously. And those systems can run on local LLMs.\n\nSo those numbers need a serious update.",
          "score": 57,
          "created_utc": "2026-02-20 17:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h43ge",
              "author": "ForsookComparison",
              "text": ">>  users programming with ChatGPT\n\nI think this is just the author failing to grasp that chatgpt != ai. The actual benchmark is *(very very likely)* not using back and forth chat sessions.\n\nSometimes it's easier to just let marketing say \"Chatgpt\"",
              "score": 11,
              "created_utc": "2026-02-20 19:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h8axj",
                  "author": "Malkiot",
                  "text": "I built a tool that decomposes projects into dependency graphs, generates implementation plans, assigns parallel AI agents to work simultaneously, and enforces boundaries so they don't stomp on each other.\n\nI built that tool *with* AI because I was annoyed with managing my other projects.\n\nThe studies here are measuring people using ChatGPT like a search bar. People failing says more about them than it does about the utility of AI.",
                  "score": 15,
                  "created_utc": "2026-02-20 19:37:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gsd10",
          "author": "Lissanro",
          "text": "It is true that the current LLMs and agents cannot yet do freelance work on their own, but \"small bites only\" era has passed long time ago for me.\n\n\nWhen I was just beginning integrating LLMs to my workflow, even basic things how to center a div container, LLMs often struggled with, especially in more complicated layouts. It was more efficient to either try a few things or just google it.\n\n\nNowadays, I can tell Kimi K2.5 make an entire website and leave it overnight running on my PC, providing warmth during winter nights as a bonus... and in most cases it gets almost everything perfect, except I still need to provide images and polish layout, fix small issues. Even with vision, K2.5 is still not precise enough to clearly see some mistakes, or to be able judge icon quality. But it still can describe ideas what icons to put where and make simple SVG placeholders, some of them actually can be good enough, but most need to be replaced.\n\n\nThat said, my prompts are quite detailed and I have over a decade of experience in web design and programming, in addition to being 2D and 3D artist. So I can use both hybrid or traditional methods to produce required images, animations or code, if it is needed for good result. Using my experience, I can specify precisely what I need, or I can provide enough context from previous work so it is clear what is needed, so the better the AI, the faster I can get the job done, or the more work I can take, while still maintain quality, both visual and of the code base, since with or without AI, I do my work based on my skills and experience, using AI just allows me to be more efficient.\n\n\nI think using AI is skill on its own, and that sometimes needs to be relearned (when things change or when need to use new tools / inference backends) and adapted to the situation. At least, with AI models that are available today.",
          "score": 18,
          "created_utc": "2026-02-20 18:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h0b3h",
              "author": "TheAncientOnce",
              "text": "what hardware do you use if you don't mind me asking? Is it full Kimi2.5 or are you running a quantized variants?",
              "score": 1,
              "created_utc": "2026-02-20 18:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h2nu8",
                  "author": "Lissanro",
                  "text": "Yes, I use the full version (Q4\\_X, preserving INT4 weights in GGUF format, along with F16 mmproj for vision). I run K2.5 on 64-core EPYC 7763 + 8-channel 1 TB 3200MHz RAM + 96 GB VRAM (made of 4x3090) + 8 TB NVMe for AI models and 2 TB NVMe SSD for the OS + \\~120 TB disk space on HDDs for storage and backups. If interested to know more, in my another comment I shared a photo and other details about my rig including what PSUs I use and what the chassis look like: [https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/](https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/)",
                  "score": 6,
                  "created_utc": "2026-02-20 19:10:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gt32f",
          "author": "nomorebuttsplz",
          "text": "Sound like you've never heard of coding agents. \n\nHow the fuck is this post getting upvotes. You're like a year behind.",
          "score": 9,
          "created_utc": "2026-02-20 18:27:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hnlfw",
              "author": "alcalde",
              "text": "Prove otherwise. What secret knowledge do you have that the rest of the world does not?",
              "score": -2,
              "created_utc": "2026-02-20 20:52:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hp5z8",
                  "author": "nomorebuttsplz",
                  "text": "OP is literally describing trying to code without an agent \"Paste the relevant code, show what you're working with\"\n\nThat's obsolete.",
                  "score": 10,
                  "created_utc": "2026-02-20 21:00:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ip66o",
                  "author": "Low_Amplitude_Worlds",
                  "text": "“secret knowledge” 🤨",
                  "score": 3,
                  "created_utc": "2026-02-21 00:12:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6g9bwr",
          "author": "Vozer_bros",
          "text": "from what I have done last month with AI automation, I dont agree with you.",
          "score": 20,
          "created_utc": "2026-02-20 16:56:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gusd7",
              "author": "_VirtualCosmos_",
              "text": "What have you done? and with which AI if I may ask",
              "score": 3,
              "created_utc": "2026-02-20 18:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hpbdw",
                  "author": "teamharder",
                  "text": "Copy of comment. Opus 4.6. Ive done more than what is listed below.\n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.",
                  "score": 7,
                  "created_utc": "2026-02-20 21:00:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6iageo",
                  "author": "Vozer_bros",
                  "text": "generate unlimited chapters of novel/story with auto grow characters, my main AI are GLM and Claude Opus",
                  "score": 1,
                  "created_utc": "2026-02-20 22:48:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6grxbq",
              "author": "teamharder",
              "text": "Thats my take too. Idk how someone could do this much work studying it and come to this conclusion. ",
              "score": 5,
              "created_utc": "2026-02-20 18:21:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ibkep",
                  "author": "Vozer_bros",
                  "text": "yep, I dont want to debate, but digital world is changing faster then ever, we should aware all accept that instead of pretending AI is stupid",
                  "score": 1,
                  "created_utc": "2026-02-20 22:54:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hnip2",
                  "author": "alcalde",
                  "text": "This is the conclusion of the entire planet. To claim otherwise is an extraordinary claim requiring extraordinary evidence. No one's cranked out a new operating system via Claude Code.",
                  "score": -2,
                  "created_utc": "2026-02-20 20:52:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h0z2h",
          "author": "ILikeBubblyWater",
          "text": "Mate if you believe AI cant churn out fully fledged websites you clearly are not doing this full time.\n\nWith stuff like multi provider planning loops and ralph loops and claude code you can absolutely push out whole products in a couple hours.\n\nWriting code is a deprecated way of coding already, people are just coping and apply unreasonable high standards to AIs that they would not ask from a human",
          "score": 4,
          "created_utc": "2026-02-20 19:02:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ho80r",
              "author": "alcalde",
              "text": "> people are just coping and apply unreasonable high standards to AIs that they would not ask from a human\n\nYou don't expect humans to deliver functioning code with tests and documentation?",
              "score": 1,
              "created_utc": "2026-02-20 20:55:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ho1l1",
              "author": "alcalde",
              "text": "Websites aren't code. ",
              "score": -2,
              "created_utc": "2026-02-20 20:54:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gq4a7",
          "author": "bakawolf123",
          "text": "Cope is good but the better you learn to use them the scarier it gets...  \nWhile I don't think pure AI agents replacing humans is a realistic approach in foreseeable future, a cheaper weaker dev competent in using wide range of AI tools replacing senior staff is quite a possibility.",
          "score": 6,
          "created_utc": "2026-02-20 18:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gs2tj",
              "author": "Healthy-Nebula-3603",
              "text": "You know AI agents did not even exist a year ago (models were not trained this way yet )  not even codex-cli or claudie-cli ...\n\nAnd you are claiming AI will not replace you soon ?",
              "score": -1,
              "created_utc": "2026-02-20 18:22:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6grac7",
          "author": "Healthy-Nebula-3603",
          "text": "Sure buddy... keep your head in the sand ....",
          "score": 7,
          "created_utc": "2026-02-20 18:19:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ggscz",
          "author": "TopTippityTop",
          "text": "These people must not be using 5.3 codex",
          "score": 3,
          "created_utc": "2026-02-20 17:31:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h6b9h",
          "author": "Mystical_Whoosing",
          "text": "this is such an old take on this, even 1 year ago this shouldn't be the case, but today? You just share that here is a new tech and the people you survey cannot keep up",
          "score": 5,
          "created_utc": "2026-02-20 19:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g15s2",
          "author": "Purple_Ice_6029",
          "text": "Also, it will get much more expensive as the investors require an ROI, making it not as appealing. *pop*",
          "score": 5,
          "created_utc": "2026-02-20 16:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ge91k",
              "author": "DHFranklin",
              "text": "I imagine this might be the year that \"good enough\" AI is paired with really good centauring and the UI/UX will show more custom built stuff. So that the cost per hour in sheparding the AI would have that demonstable ROI.\n\nSo just like how combines don't drive themselves, million dollar AI workflows won't either. That doesn't mean that they won't radically change the work that's done.",
              "score": 0,
              "created_utc": "2026-02-20 17:19:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g6nrd",
          "author": "Otherwise_Wave9374",
          "text": "This tracks with what Ive seen: benchmarks look great, but \"agent does real freelance work end to end\" is mostly about reliability, context management, and actually knowing when it doesnt know. The advice about small bites + verification is the only sane way to use agents today. I also think tooling (tests, linters, sandboxes, traces) matters more than the model for most workflows. If you want more practical patterns for using AI agents without falling into the prompt loop, Ive got a few notes here: https://www.agentixlabs.com/blog/",
          "score": 3,
          "created_utc": "2026-02-20 16:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6grphw",
          "author": "teamharder",
          "text": "What were the tasks? How does the score correlate to the human percentage? Given my current experience, these numbers dont add up. \n\n\nIn the last month I used Claude Code to build a graphRAG, self-healing Grafana dashboard monitoring all my automation services, decrypted back-ups for my son's AAC device and built a UI to inject new buttons and folders, Tailscale VPN across all of my main devices, triggers and scripts to invoke headless Claude Code to do random shit like add items to the graphRAG or my businesses Notion page.\n\n\nI did this all in my spare time on the weekends. I dont know how to write code. I just plan and test thoroughly with use case scenarios. Realistically Claude did the work and gets the credit for it. How that measures out to 2.46% is what confuses me. ",
          "score": 3,
          "created_utc": "2026-02-20 18:21:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g2a2i",
          "author": "Jumpy_Ad_2082",
          "text": "now present this to a manager and convince him who is more profitable.",
          "score": 3,
          "created_utc": "2026-02-20 16:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g49z6",
          "author": "Needausernameplzz",
          "text": "great write up",
          "score": 2,
          "created_utc": "2026-02-20 16:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h0opz",
          "author": "Smarterchild1337",
          "text": "The broad conclusions of this post are at least a year out of date, which is an eon in terms of AI progress during that time",
          "score": 2,
          "created_utc": "2026-02-20 19:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g759p",
          "author": "masterlafontaine",
          "text": "Can you elaborate a bit more on the columns? What are these integrals with games?",
          "score": 1,
          "created_utc": "2026-02-20 16:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ho6qi",
          "author": "thedarkbobo",
          "text": "We are doomed, I don't agree, we have max few years ;p",
          "score": 1,
          "created_utc": "2026-02-20 20:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ianih",
          "author": "Expert-Reaction-7472",
          "text": "LLMs are very very good at writing code to the point where anyone writing code by hand will be out of a job as a result of that. Anyone who thinks otherwise is delulu.",
          "score": 1,
          "created_utc": "2026-02-20 22:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ja2mm",
          "author": "keamo",
          "text": "Tell me you're using AI to create excel files without telling me you're not using  using AI to write yourself a frontend for data visualizations. Tell me you're only using excel by showing me a screen shot of you not asking AI to create a tailwind/react/vite with chartjs visuals, tell me your middle name.",
          "score": 1,
          "created_utc": "2026-02-21 02:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nqzpp",
          "author": "HiggsBoson2738",
          "text": "\"how do I center a div\", FFS...",
          "score": 1,
          "created_utc": "2026-02-21 20:30:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pnaj3",
          "author": "Tall-Wasabi5030",
          "text": "You'll be one of the first to be replaced with AI ",
          "score": 1,
          "created_utc": "2026-02-22 03:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q73ty",
          "author": "Big-Masterpiece-9581",
          "text": "None of this has anything to do with a) fewer jobs because skilled seniors can do much more with AI and not having to waste time teaching juniors many of whom are duds. b) CEOs want to lay people off and blame AI regardless of quality. They no longer care about quality products.",
          "score": 1,
          "created_utc": "2026-02-22 05:42:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g33p1",
          "author": "Eventual-Conguar7292",
          "text": "Kimi 2.5 Is better than Deepseek?\n\nBut my real question What use case In Local LLM,Here are use cases I can think of\n\n1. Simple code generation - Web visualizations, data dashboards, interactive charts\n2. Image generation - Ads, logos, creative visual content (Using LLM to understand text prompts)\n3. Audio production - Sound effects, voice-over merging, track separation (Using LLM to understand text prompts)\n4. Text-based tasks - Report writing, data retrieval, web scraping",
          "score": 1,
          "created_utc": "2026-02-20 16:28:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6g6ug0",
              "author": "twack3r",
              "text": "Why are you questioning your own result?",
              "score": 2,
              "created_utc": "2026-02-20 16:45:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gciey",
                  "author": "Eventual-Conguar7292",
                  "text": "I said the benchmarks were made up in title a in way.\n\nHow I created the benchmark:\n\n1. Test it myself first - I spend my time running Sonnet 4, Gemini, DeepSeek, etc. on zero-shot tasks, then compare the outputs\n2. Then I back it up -  I Find public benchmarks that support my findings\n\nWhy does this exist? I don't make any money from posting, so you get free quality. I'd rather trust my own opinion than internet benchmarks, but I probably cant give you value with time I got.",
                  "score": 1,
                  "created_utc": "2026-02-20 17:11:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gga7u",
              "author": "Zerokx",
              "text": "5. Human Computer Interfacing - Voice Recognition can be small and local, executing simple commands based on users in home automation or whatever in human language  \n6. Privacy - Any task which would expose a lot of private information to the outside world, like refining your CV, or searching through personal files  \n7. Freedom - Using the AI for tasks that would be censored by company policies, like making satirical content of public figures, content rated for adults, whatever mischievous acts you come up with like some will use it for scamming or tell them how to create illegal substances etc.",
              "score": 1,
              "created_utc": "2026-02-20 17:29:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ga6hc",
          "author": "bgptcp179",
          "text": "Hmmm, sounds like something an AI agent would say.  GET HIM!\n\nSeriously tho, cool info",
          "score": 1,
          "created_utc": "2026-02-20 17:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g20j1",
          "author": "nomorebuttsplz",
          "text": "Question:\n\nHow can a benchmark like RLI account for the fact that once people recognize an AI can be given a task and can complete it, it won't be considered human work anymore? It seems like an ever-changing standard specifically focused on the delta between human and AI work.",
          "score": -2,
          "created_utc": "2026-02-20 16:23:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfxnm",
      "title": "Open source AGI is awesome. Hope it happens!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mkun0tc1k8lg1.jpeg",
      "author": "Koala_Confused",
      "created_utc": "2026-02-23 12:18:36",
      "score": 76,
      "num_comments": 22,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcfxnm/open_source_agi_is_awesome_hope_it_happens/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6xy1dt",
          "author": "05032-MendicantBias",
          "text": "OpenAI had a Foundation \"controlling\" it. The instant the foundation tried to excercise control, they were overruled by dollars.\n\nThis has to be approached at a regulation level. Training AI taps into every data on the internet. It's only fair that every model provider is forced to release it open source so it can be inspected.",
          "score": 34,
          "created_utc": "2026-02-23 12:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypkwg",
              "author": "DHFranklin",
              "text": "And regulation is overruled by dollars every time.\n\nIf there is one tax shelter in the carribean holding out, it will be where the next model is developed. We will never get ahead of this.",
              "score": 11,
              "created_utc": "2026-02-23 15:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70j6l7",
                  "author": "d_the_great",
                  "text": "The best thing we can do is have alternative structures.\n\nIf the government and industry aren't gonna open it up for us and release something safe, we have to do it ourselves.",
                  "score": 1,
                  "created_utc": "2026-02-23 20:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72hz5p",
              "author": "voyager256",
              "text": "But... but this time it will be different. Trust me, bro.",
              "score": 1,
              "created_utc": "2026-02-24 02:46:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z2qjn",
          "author": "BreathingFuck",
          "text": "Not X. Not Y. Z. \n\nThey aren’t getting anywhere if they couldn’t write that paragraph on their own.",
          "score": 13,
          "created_utc": "2026-02-23 16:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xympk",
          "author": "No_Clock2390",
          "text": "Sounds like a bunch of rubbish",
          "score": 20,
          "created_utc": "2026-02-23 12:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5kyd",
          "author": "flonnil",
          "text": "\"jippity, come up with a bunch of marketing words devoid of any meaning at all. no mistakes.\"\n\ngo perceive yourself.",
          "score": 15,
          "created_utc": "2026-02-23 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yr2yz",
          "author": "DHFranklin",
          "text": "This is pissing into the wind. AGI will be open sourced regardless of who tries to contain or privatize it. It will be closed and bottled for maybe a few months before someone else gets that far.",
          "score": 5,
          "created_utc": "2026-02-23 15:28:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z6trq",
              "author": "rafaelRiv15",
              "text": "How can you be so sure ? I honestly can't understand the business model of open source model and I wouddn't be surprised if they get close source eventually",
              "score": 2,
              "created_utc": "2026-02-23 16:42:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zaymg",
                  "author": "DHFranklin",
                  "text": "I'm sorry I may not be understanding you clearly. Do you think that we will have an open source AGI model that will then go closed source?\n\nThis isn't about a particular business model. Look at all the different non-profit examples of open sourced software. Look at the Chinese models that reverse engineered the SOTA from the weights alone.\n\nNo one single operation is a year ahead of the others. We're seeing the cash investments turn into infrastructure as we speak. We're actually building the hardware that was a huge bottleneck.",
                  "score": 2,
                  "created_utc": "2026-02-23 17:01:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yxds1",
          "author": "nijuu",
          "text": "Good in theory but once they have a good product whose to say they won't go for the $$$ bag...",
          "score": 3,
          "created_utc": "2026-02-23 15:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z2zcs",
          "author": "UsedGarbage4489",
          "text": "naive 🧠☠️🤡",
          "score": 2,
          "created_utc": "2026-02-23 16:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70m7v7",
          "author": "bourbonandpistons",
          "text": "We are decades from AGI.\n\nThey'll probably just move the goal post of AGI and make it something else like they did with AI to agi. \n\nRemember what we're saying now is nothing more than a bunch of human program algorithms on human program data. There's no thinking and no reason anywhere near what those words actually mean. It's just loops around optimized algorithms.",
          "score": 2,
          "created_utc": "2026-02-23 20:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o741n4j",
          "author": "Own-Potential-2308",
          "text": "No fluff. No drama.",
          "score": 1,
          "created_utc": "2026-02-24 10:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o751scd",
          "author": "silphotographer",
          "text": "Regulators: \n\n![gif](giphy|hAVLRya8K7T208esUo)\n\n",
          "score": 1,
          "created_utc": "2026-02-24 14:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b4vce",
          "author": "Psyko38",
          "text": "It looks like an OpenAI v2, they have the same starting goal.",
          "score": 1,
          "created_utc": "2026-02-25 11:29:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zm5v9",
          "author": "BubbleProphylaxis",
          "text": "please please make it stop. stop ai.",
          "score": 1,
          "created_utc": "2026-02-23 17:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72qom9",
          "author": "immersive-matthew",
          "text": "I have reasons to believe that AGI will be decentralized by nature. Why?   If you search for hacking over time you will see a very clear pattern of hacks increasing exponentially year over year and as you extend that trend into the future you realize that anything Centralized is a sitting duck.  Sure, AI is being used to defend against other AIs attacking and yet the pattern is still escalating.  Plus that game of cat and mouse will end up iterating so fast that humans will be pushed out.  It is why I am so confident in Bitcoin as it has clearly demonstrated it is extremely hard to hack and thus the next wave of adoption will be involuntary. Centralized AI will be in the same boat and will have to move to a decentralized platform to survive. Not a guarantee of course but that seems to be the trend.",
          "score": 0,
          "created_utc": "2026-02-24 03:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y4uff",
          "author": "Exciting-Log-8170",
          "text": "Trying to do it, have a working thermodynamic manifold prototype. Pushing next build this week. \n\nhttps://www.brickmiinews.com",
          "score": -2,
          "created_utc": "2026-02-23 13:28:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdf2sj",
      "title": "What’s everyone actually running locally right now?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "author": "CryOwn50",
      "created_utc": "2026-02-24 12:35:50",
      "score": 67,
      "num_comments": 85,
      "upvote_ratio": 0.99,
      "text": "Hey folks,\n\nIm curious what’s your current local LLM setup these days? What model are you using the most, and is it actually practical for daily use or just fun to experiment with?\n\nAlso, what hardware are you running it on, and are you using it for real workflows (coding, RAG, agents, etc.) or mostly testing? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdf2sj/whats_everyone_actually_running_locally_right_now/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o74rf56",
          "author": "Greenonetrailmix",
          "text": "Qwen 3 coder next 80B is top charts (downloads) and is performing amazing across the smaller quantizations than most model's do.",
          "score": 31,
          "created_utc": "2026-02-24 13:25:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74wbyp",
              "author": "gweilojoe",
              "text": "I’m running the Q4 version of this on an RTX 6000 Pro and it’s great - >120 tps",
              "score": 7,
              "created_utc": "2026-02-24 13:52:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76kp7j",
                  "author": "pot_sniffer",
                  "text": "Im running q6 on 7950x,64gb and a 9060XT. Id say its doing 80% of what I need it to. The rest I do with Claude",
                  "score": 3,
                  "created_utc": "2026-02-24 18:34:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7532f2",
                  "author": "Greenonetrailmix",
                  "text": "With cuda or vulkan?",
                  "score": 2,
                  "created_utc": "2026-02-24 14:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o750z8q",
              "author": "Prudent-Ad4509",
              "text": "It's great but still tends to loop in opencode. Every suggested solution worked only to the point with UD Q4. I'm going to try nvfp4 quant before moving on to the 3.5 series.",
              "score": 5,
              "created_utc": "2026-02-24 14:17:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75ah7a",
                  "author": "mister2d",
                  "text": "I wonder if the sequential thinking MCP could help.",
                  "score": 1,
                  "created_utc": "2026-02-24 15:04:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77i2b3",
                  "author": "jedsk",
                  "text": "Seeing the same",
                  "score": 1,
                  "created_utc": "2026-02-24 21:07:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75k8nd",
              "author": "nakedspirax",
              "text": "I connected it to a 3080ti with 96gb of ram and I'm able to one shot certain tasks or do 3-4 passes with coding. Running q6 GGUF and it's fine for agentic work where I can step away for a coffee and come back.",
              "score": 3,
              "created_utc": "2026-02-24 15:50:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o762uex",
                  "author": "PaMRxR",
                  "text": "Very similar here, q6 GGUF with a 3090 + 64GB RAM. Slow, but better responses than smaller models.",
                  "score": 3,
                  "created_utc": "2026-02-24 17:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74s062",
              "author": "CryOwn50",
              "text": "Yeah, I’ve noticed that too",
              "score": 2,
              "created_utc": "2026-02-24 13:28:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74tewq",
                  "author": "Greenonetrailmix",
                  "text": "On my PC using Q4_K_M with my 5090 and 4090 on the Vulkan backend, I'm getting around 90 Tok/s",
                  "score": 2,
                  "created_utc": "2026-02-24 13:36:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74m4ch",
          "author": "Nefhis",
          "text": "I'm using Mistral Small 3.2 24b and Magistral Small 24b as local models. I built the front end myself with Xcode, with semantic memory, document uploads to chat, and libraries for RAG. My use is primarily administrative, hence the local setup, to upload documents without exposing them to providers. I have them running on a MacBook Pro M4 Max.",
          "score": 18,
          "created_utc": "2026-02-24 12:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74mjfn",
              "author": "CryOwn50",
              "text": "That’s a seriously solid setup building your own frontend with semantic memory and RAG is impressive.\n\nRunning 24B models locally on an M4 Max for private document workflows makes total sense, especially for admin use.\n\nHow’s the performance with larger document sets  still smooth, or does context size start to slow things down?",
              "score": 5,
              "created_utc": "2026-02-24 12:55:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74og4v",
                  "author": "Nefhis",
                  "text": "The models themselves are reliable, but I won't lie to you, when you approach 50k tokens it starts to slow down, although it's still very usable. I don't have the exact t/s on hand to tell you, but I'd say it's roughly between 15 and 28. For more information, I serve it with LMStudio because it allows the use of MLX models, which run better on Mac, and with KV quantization at 8... it works well enough that I don't need to rely on external providers, at least for this specific use case.",
                  "score": 4,
                  "created_utc": "2026-02-24 13:07:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74zz11",
              "author": "_Arelian",
              "text": "Let’s connect bro",
              "score": 1,
              "created_utc": "2026-02-24 14:11:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o755h8f",
                  "author": "Nefhis",
                  "text": "Hey! Sure. What exactly do you want to connect about?",
                  "score": 1,
                  "created_utc": "2026-02-24 14:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74rk3c",
          "author": "RomanceCherry",
          "text": "I actually like Qwen3 4B, runs pretty fast and is useful for every day questions, while keeping it private running local on iphone.",
          "score": 7,
          "created_utc": "2026-02-24 13:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s5rr",
              "author": "CryOwn50",
              "text": "That’s honestly such a sweet spot. A 4B model that’s fast, responsive, and running fully local on your iPhone? That’s peak practicality.",
              "score": -1,
              "created_utc": "2026-02-24 13:29:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o755h8l",
          "author": "Potential-Leg-639",
          "text": "Qwen 3 Coder Next UD-Q5 (256k context)\nQwen 3 Coder UD-Q4 (128k context)\nGPT-OSS-20b UD-Q4 (128k context)\n\nPlanning/Orchestration in Opus, coding itself partly local, especially for larger things, that can run overnight and nothing can hit any limits. Sensitive stuff only local of course. Switched completely to OpenCode.\n\nAll at once on a Strix Halo, works great, love that machine - silent, powerful and power efficient.\n\nWill build a 2nd rig with parts i still have lying around to support the Strix for some tasks. Basically getting a 2nd Strix would maybe be the better idea. Or wait for Medusa Halo.",
          "score": 7,
          "created_utc": "2026-02-24 14:40:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75krca",
              "author": "nakedspirax",
              "text": "Are you finding a way to step away from opus for a more local thinking model? \n\nWhat strix halo machine are you using?",
              "score": 1,
              "created_utc": "2026-02-24 15:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75q7ct",
                  "author": "Potential-Leg-639",
                  "text": "Opus only for planning and orchestration, coding when planning done into very detailled level locally and with other models. Works good, Opus for coding itself is not necessary when everything was done properly before.\n\nStrix Halo 128GB, they are all quite similar performance wise, just buy the cheapest you can get ;)",
                  "score": 2,
                  "created_utc": "2026-02-24 16:17:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75cqut",
          "author": "mister2d",
          "text": "I run Nemotron 3 Nano for my agentic flows. I have some really old hardware but I get a respectable 30-40 tokens/sec at 128k context due to the model's hybrid/swa architecture.\n\n- Dual Xeon (Ivy Bridge)\n- 256 GB DDR3\n- 2x RTX 3060 (12GB)",
          "score": 7,
          "created_utc": "2026-02-24 15:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75uwic",
          "author": "nomorebuttsplz",
          "text": "glm 5 on mac 3 ultra 512 using opencode. Good adjunct to my Claude pro subscription: if I run out of claude tokens or want to do something with sensitive data I can switch pretty seamlessly. It's a lot slower though.",
          "score": 6,
          "created_utc": "2026-02-24 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76rrsz",
              "author": "sig_kill",
              "text": "I’m seriously impressed with GLM-5, but I don’t have enough to run it locally with a single 5090 and 64gb of RAM.",
              "score": 2,
              "created_utc": "2026-02-24 19:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782gib",
                  "author": "somethingClever246",
                  "text": "I run Q6 with 128GB, 9950x, and 5080.  Ridiculously slow but good quality results",
                  "score": 2,
                  "created_utc": "2026-02-24 22:43:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76tcv3",
                  "author": "nomorebuttsplz",
                  "text": "your computer just has to believe in itself. The ram was in its heart all along. /s",
                  "score": 1,
                  "created_utc": "2026-02-24 19:12:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74tgg2",
          "author": "Right_Weird9850",
          "text": "ministral 3b vl instruct",
          "score": 6,
          "created_utc": "2026-02-24 13:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75yct8",
          "author": "GreyBamboo",
          "text": "I run Gemma3 4b for my chatbot and TranslateGemma for my translation tool right now :)",
          "score": 5,
          "created_utc": "2026-02-24 16:53:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76eapa",
          "author": "benevbright",
          "text": "qwen3-coder-next q3 on 64GB Mac.",
          "score": 4,
          "created_utc": "2026-02-24 18:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74tvqo",
          "author": "NoobMLDude",
          "text": "I’m running a few local models for different uses.\n\n- Qwen3-Coder: for Coding \n- Qwen3-14B: for Meeting Assistant\n- Gemma3-7B - for basic Question Answering\n\nHere’s all the tools and setup for different Local usecases :\n\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) \n\nDisclaimer: Some of the model choices may not be relevant for you. This choice is based on my personal preference. I prefer speed over perfect answers since I like to have a quick first level overview and then delve deeper into a topic using larger models later.",
          "score": 3,
          "created_utc": "2026-02-24 13:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75662j",
              "author": "Potential-Leg-639",
              "text": "Disclaimer? 🤣\nBot?",
              "score": 2,
              "created_utc": "2026-02-24 14:43:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ba9r",
                  "author": "NoobMLDude",
                  "text": "Using appropriate English words is not exclusively restricted to Bots. 😉",
                  "score": 5,
                  "created_utc": "2026-02-24 17:52:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7a1exh",
              "author": "CryOwn50",
              "text": "that disclaimer is basically the AI version of Results may vary. Side effects include speed, productivity, and mild model addiction.😄Or in local AI terms: Warning: optimized for vibes, not leaderboard glory.",
              "score": 2,
              "created_utc": "2026-02-25 05:39:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a6aa5",
                  "author": "NoobMLDude",
                  "text": "Yes. I think it’s important to let people know about WHY these models are used. \n\nJust like there is no such thing as “Best Movie”, “Best Food”, “Best Music”, there is nothing like “Best Model”.\n\nFor different people different things might be inportant .",
                  "score": 2,
                  "created_utc": "2026-02-25 06:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o751j8j",
          "author": "Swarley996",
          "text": "Devstral small 2 24b - coding \nGLM 4.7 flash 30b - thinking and complex queries\nMinistral 3 14b - general use\nMinistral 3 3b - small agents",
          "score": 3,
          "created_utc": "2026-02-24 14:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o752mrq",
          "author": "dave-tay",
          "text": "Qwen3-14b, fits 100% into RTX 3060 12gb. Ryzen 5600g to drive my display",
          "score": 3,
          "created_utc": "2026-02-24 14:25:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75lkja",
          "author": "Salt-Willingness-513",
          "text": "Mainly nemotron nano and minimax m2.5 in q8 each",
          "score": 3,
          "created_utc": "2026-02-24 15:56:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4xq",
          "author": "PvB-Dimaginar",
          "text": "I have a Bosgame M5 (AMD Strix Halo) running CachyOS Linux. For coding I’m focusing on Qwen3-Coder-Next 80B Q6.\n\nStill struggling a bit with OpenCode, my config probably needs some work around skills and MCP servers. One thing I did get working that I’m really happy about is memory sharing between Claude Code and OpenCode.\n\nGoing to improve my setup over the coming days.\n\nAnyone have experience with coding quality differences between the Q6 and Q5 models?​​​​​​​​​​​​​​​​",
          "score": 3,
          "created_utc": "2026-02-24 16:48:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a10zl",
              "author": "CryOwn50",
              "text": "That’s a clean rig  Bosgame M5 + CachyOS sounds like one of the more fun Linux desktops out there.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 05:36:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o763328",
          "author": "andy2na",
          "text": "Qwen3-VL:4B IQ4\\_XS always in VRAM (\\~2.25gb + context)\n\n* Frigate genAI image analysis\n* General questions in openwebui\n* Home Assistant Voice Assistant\n* Karakeep AI tagging and AI Summaries\n* Open-notebook questions and podcast generation\n* Sure Finance for transaction auto-categorization\n\nHoping Qwen3.5:5b will be a huge upgrade, qwen3-vl is already very good for these tasks",
          "score": 3,
          "created_utc": "2026-02-24 17:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p04b",
          "author": "FaceDeer",
          "text": "My workhorse is still good old Qwen3-30B-A3B-Thinking-2507. When the new Qwen models come out in that size class I'll likely switch. The main use it's being put to is summarizing and extracting information from documents, it's chugging along in the background \"digesting\" stuff into easier-to-work-with forms.\n\nI recently started messing around with some agentic stuff and I found Jan.ai's setup to be a good out-of-the-box solution for that, the old Qwen model wasn't so good at that. Hopefully the new one will be better at it.",
          "score": 3,
          "created_utc": "2026-02-24 18:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79vj45",
              "author": "jrexthrilla",
              "text": "Qwen 3.5 27b and 30b just came out",
              "score": 2,
              "created_utc": "2026-02-25 04:56:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ajmxd",
                  "author": "FaceDeer",
                  "text": "Indeed. Now I just need to wait a day or two for the dust to settle. Looking forward to it. :)",
                  "score": 1,
                  "created_utc": "2026-02-25 08:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77dk21",
          "author": "MS_Fume",
          "text": "Abliterated huihui distilled into 4B model so my phone can run it…. Much fun with it lol",
          "score": 3,
          "created_utc": "2026-02-24 20:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78hfbe",
          "author": "Solid-Pop-3452",
          "text": "Maybe the best LLM model is the friendships we made along the way",
          "score": 3,
          "created_utc": "2026-02-25 00:04:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c7jbc",
              "author": "0xGooner3000",
              "text": "Real",
              "score": 1,
              "created_utc": "2026-02-25 15:18:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79k4sx",
          "author": "RG_Fusion",
          "text": "My most used model at the moment is Qwen3-235b-a22b-instruct at q4-k-m. I use it as a voice assistant running in the background on my desktop. Just something to chat with and bounce ideas off of, nothing truly productive.\n\n\nI just downloaded the new Qwen3.5-397b-17b model today for some testing. On the old model I was getting 13 t/s of decode, and on this new one I'm getting 18.5, so I'll definitely be switching to it once everything is set up.\n\n\nHardware:\n- AMD EPYC 7742 CPU\n- Asrock Rack ROMED8-2T motherboard\n- 512 GB ECC DDR4 3200 MT/s RAM\n- Nvidia RTX Pro 4500 Blackwell GPU",
          "score": 3,
          "created_utc": "2026-02-25 03:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0gpc",
              "author": "CryOwn50",
              "text": "That’s a crazy clean setup 😮‍🔥 512GB ECC + EPYC 7742 is basically home lab meets data center.\n\n",
              "score": 2,
              "created_utc": "2026-02-25 05:32:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a0yrb",
                  "author": "RG_Fusion",
                  "text": "Thanks. I built it primarily to explore LLMs as a concept, just a tool to help me learn. It's definitely proving capable though. I plan to add more GPUs in the future, which will continue pushing token generations up on these massive MoEs.",
                  "score": 2,
                  "created_utc": "2026-02-25 05:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75b4f5",
          "author": "bsenftner",
          "text": "I maintain a Wan2GP installation, it's got nearly 100 models now for image, video, voice clones, tts, and text to song. The developer of Wan2GP is really active, and tends to release a new version within a few days of new models. Qwen3 seems to have additional training to know about Wan2GP, surprisingly, and is great at helping create more complex media requiring combinations of models in concert.",
          "score": 2,
          "created_utc": "2026-02-24 15:08:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76m77s",
          "author": "fallingdowndizzyvr",
          "text": "GLM 5.",
          "score": 2,
          "created_utc": "2026-02-24 18:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76s3f7",
              "author": "sig_kill",
              "text": "Mac Studio?",
              "score": 2,
              "created_utc": "2026-02-24 19:06:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o772l3y",
          "author": "hallofgamer",
          "text": "Glm 4.7 flash handles all my needs",
          "score": 2,
          "created_utc": "2026-02-24 19:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78pz6y",
          "author": "Bitterbalansdag",
          "text": "Magistral 2 small 24b with a home made front end and MCP servers. \n\nMCP servers for: persistent external memory, web search, tasks management (basic todo list)\n\nThe chats have automatic compaction. The front end can swap system prompts mid-chat.\n\nUse for a general personal assistant, runs on a 3090 and a tailscale network.",
          "score": 2,
          "created_utc": "2026-02-25 00:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78qtkd",
          "author": "Pjbiii",
          "text": "Different tools for different tasks. I use Qwen3:14b for document sort and summarize, InternVL3.5:8b for long image descriptions, Qwen3-vl:2b for image keywords/SEO file names, gemma3:27b for outline writing, glm-4.7-flash for n8n AI Agent node. \n\nMacBook Pro (headless, the keyboard was killed by my toddler and a water bottle) M1 Max with 32GB unified memory.",
          "score": 2,
          "created_utc": "2026-02-25 00:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0mat",
              "author": "CryOwn50",
              "text": "That’s a super clean stack  right tool for each job instead of forcing one model to do everything.Respect for squeezing that much out of an M1 Max with 32GB unified memory too especially running it headless 😂Also… toddler + water bottle is a more dangerous combo than any production outage.",
              "score": 1,
              "created_utc": "2026-02-25 05:33:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ab7rv",
          "author": "Own_Professional6525",
          "text": "Lately I’m seeing more people run smaller quantized models locally for coding and RAG since they balance performance and cost well. Curious whether people prioritize privacy, latency, or experimentation when choosing their setup.",
          "score": 2,
          "created_utc": "2026-02-25 06:59:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7abr94",
              "author": "CryOwn50",
              "text": "Yeah, that shift makes sense.\n\nSmaller quantized models hit a sweet spot good enough for coding and RAG, but light enough to run locally without crazy hardware. For most people, it’s a practical trade-off between performance and cost.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:04:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7ae7iq",
              "author": "Potential-Leg-639",
              "text": "Cloud models are still wayyyy faster, but I use the local models for sensitive tasks and coding itself, mostly over night",
              "score": 1,
              "created_utc": "2026-02-25 07:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7abwyl",
          "author": "TheAdmiralMoses",
          "text": "I'm messing with LiquidLFM models, they're supposed to be the future but they kinda just suck ngl",
          "score": 2,
          "created_utc": "2026-02-25 07:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aefjd",
              "author": "CryOwn50",
              "text": "Lmao 😭That’s the honest take nobody wants to post.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 07:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ajmjv",
                  "author": "TheAdmiralMoses",
                  "text": "I think they'll be good once there's some more instruct tuned models, right now they don't seem to be able to keep a single train of thought and get derailed from core facts of conversations by taking tangents on the smallest context",
                  "score": 2,
                  "created_utc": "2026-02-25 08:16:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79ycsk",
          "author": "Grand_Barnacle_6922",
          "text": "I'm running a few - minimax m2.5 230B and qwen3 235B seem to be my favs right now\n\nit's actually been very practical as i've been able to build a lot of custom automations around smart home and personal administration tasks.  i'm not certain on the hardware specs but my mac seems to be able to handle it just okay.\n\n",
          "score": 1,
          "created_utc": "2026-02-25 05:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aee8h",
          "author": "Outrageous_Corgi7553",
          "text": "Running Qwen2.5-Coder-32B-Q4 on M4 Mac (24GB). Actually practical for:\n\n* Code review and refactoring — works well for Python, catches logic issues\n* Quick questions instead of hitting Claude API every time\n* First-pass RAG retrieval ranking\n\nNot using for agents — too slow for multi-step workflows, and quality drops on complex reasoning. For anything serious I still route to Claude/GPT-4.",
          "score": 1,
          "created_utc": "2026-02-25 07:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bb8fj",
          "author": "routhlesssavage",
          "text": "Just putting it here, in case anyone is looking for macos or mobile phones local LLM app. https://github.com/alichherawalla/off-grid-mobile\nhttps://news.ycombinator.com/item?id=47142003\n\nI have been using this for some time and the response is quite amazing with minimum resources.",
          "score": 1,
          "created_utc": "2026-02-25 12:18:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c1y0b",
          "author": "Shouldhaveknown2015",
          "text": "I was using the new GLM MOE, then swapped to Q3 Next 80b in a Q3 unsloth gguf, then I am now using the 35b-Q3.5 model released yesterday as my daily driver.\n\nI do use other models... for instance I vibe coded a family app (Calendar, Kids rewards/chores tracking, grocery list, AI Chat bot [Running on my home lab which only has a 3060 12gb so it runs a 8b model, but I also use a text to voice model (I forget which) and it makes the AI sound like a child as I made it to act/be like BMO and sound like BMO from adventure time] and it also processes the AI responses and provides big buttons with reponses for the kids to click instead of typing. They can access it from their amazon tablet and they seem to love it.\n\nI also use the models I mention at the start with RAG and Tools. I have a vibe coded chat app with a obsidian.md vault. It pulls files automatically from the chat references to educate the AI. I can also select files to provide as part of context and select how the files should be looked at... AKA.. Is this file a research file or a lore file or a log files etc.\n\nI also built in a image generator using Flux and Klein models.\n\nI also vibe coded my own \"openclaw\" type agent, need to test it with the Q3.5 but haven't had time. It got put on the back burner, but it also has RAG and tool access to something like 28 tools. Been working on it slowly as it's the least fun project. It works well and is a limited form of my own ai research agent at this time.",
          "score": 1,
          "created_utc": "2026-02-25 14:51:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76p16a",
          "author": "Ok-Patient6458",
          "text": "I run a blog for local LLM news and insight at [https://lftw.dev](https://lftw.dev)",
          "score": 1,
          "created_utc": "2026-02-24 18:53:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rec555",
      "title": "META AI safety director accidentally allowed OpenClaw to delete her entire inbox",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/blggdcif6llg1.png",
      "author": "Minimum_Minimum4577",
      "created_utc": "2026-02-25 12:41:16",
      "score": 59,
      "num_comments": 23,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rec555/meta_ai_safety_director_accidentally_allowed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bg9y0",
          "author": "The_Jizzard_Of_Oz",
          "text": "It moved fast and broke things... 🤣",
          "score": 22,
          "created_utc": "2026-02-25 12:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cdta8",
              "author": "__rtfm__",
              "text": "Haha startup life",
              "score": 2,
              "created_utc": "2026-02-25 15:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bld1u",
          "author": "DiscombobulatedAdmin",
          "text": "Meta AI Safety Director using OpenClaw is scary.",
          "score": 17,
          "created_utc": "2026-02-25 13:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c1uud",
              "author": "Greedy-Neck895",
              "text": "I would rather security professionals be experimenting and fail than to play it so safe they never know anything about the latest security flaws.\n\n…in a sandboxed environment away from live data.",
              "score": 13,
              "created_utc": "2026-02-25 14:51:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7bmj22",
              "author": "GordoPepe",
              "text": "Grossly incompetent I'd say",
              "score": 7,
              "created_utc": "2026-02-25 13:29:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bmu1s",
                  "author": "kahnlol500",
                  "text": "And yet they think it's great to tell everyone. Could just be a big play to avoid answering emails.",
                  "score": 5,
                  "created_utc": "2026-02-25 13:31:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cghw7",
              "author": "Count_Rugens_Finger",
              "text": "That photo is small but she looks like she's 15 years old to me",
              "score": 1,
              "created_utc": "2026-02-25 16:00:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c3aho",
          "author": "MonsterTruckCarpool",
          "text": "I know this is a naive take but i would expect more caution and thoughtfulness from a Director and especially a DIRECTOR OF SAFETY",
          "score": 3,
          "created_utc": "2026-02-25 14:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cfagi",
          "author": "inevitabledeath3",
          "text": "You can just do /stop and it will stop whatever it's doing",
          "score": 4,
          "created_utc": "2026-02-25 15:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c5mbf",
          "author": "Sudden-Ad-1217",
          "text": "It's coming---- \"You're absolutely wrong....\"",
          "score": 3,
          "created_utc": "2026-02-25 15:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ccff7",
          "author": "Fearless_Weather_206",
          "text": "Lack of experience showing like a dumpster fire",
          "score": 2,
          "created_utc": "2026-02-25 15:41:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmrvk",
          "author": "tillybowman",
          "text": "i love how she tried uppercase yelling",
          "score": 2,
          "created_utc": "2026-02-25 13:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c0ot0",
              "author": "GordoPepe",
              "text": "There was some article saying apparently llms follow instructions better this way or telling them your life depends on it lmao\n\nI BEG YOU CLAUDE MY BOSS IS GOING TO LITERALLY KILL ME IF YOU DON'T FIX THIS BUG",
              "score": 4,
              "created_utc": "2026-02-25 14:45:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c3hx5",
                  "author": "MonsterTruckCarpool",
                  "text": "R U SRS RN OPENCLAW!?",
                  "score": 4,
                  "created_utc": "2026-02-25 14:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7byfkp",
          "author": "RAW2091",
          "text": "I once deleted all my mails with facebook in it hahaha 😅",
          "score": 1,
          "created_utc": "2026-02-25 14:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cfs5o",
          "author": "Visual_Acanthaceae32",
          "text": "Would be interesting what her real qualifications are….",
          "score": 1,
          "created_utc": "2026-02-25 15:57:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cpjpn",
          "author": "samxli",
          "text": "Oh you sweet Summer child",
          "score": 1,
          "created_utc": "2026-02-25 16:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cxb4l",
          "author": "rinaldo23",
          "text": "I'd put the host on a wifi plug and literally unplug it if it misbehaved. ",
          "score": 1,
          "created_utc": "2026-02-25 17:17:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmgds",
          "author": "Snoo_24581",
          "text": "Really appreciate this post. Had the same experience.",
          "score": 1,
          "created_utc": "2026-02-25 13:29:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbw01",
              "author": "Awkward-Customer",
              "text": "You should apply for a high level AI job at meta, then you could do the same but earn millions doing it.",
              "score": 6,
              "created_utc": "2026-02-25 15:39:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7br08r",
          "author": "DocumentFun9077",
          "text": "*oh the irony*",
          "score": 1,
          "created_utc": "2026-02-25 13:54:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rarw7t",
      "title": "I managed to run Qwen 3.5 on four DGX Sparks",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ifu522zupqkg1.jpeg",
      "author": "Icy_Programmer7186",
      "created_utc": "2026-02-21 14:03:34",
      "score": 49,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rarw7t/i_managed_to_run_qwen_35_on_four_dgx_sparks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6lq9mo",
          "author": "mosredna101",
          "text": "So 20K brings you 21 t/sec?",
          "score": 8,
          "created_utc": "2026-02-21 14:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lskx8",
              "author": "Icy_Programmer7186",
              "text": "This is a lab setup and this is one of many experiments done on it.  \nMoney well spent; this is not however recommendation for a production setup.",
              "score": 8,
              "created_utc": "2026-02-21 14:30:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lt5gl",
                  "author": "mosredna101",
                  "text": "I would be super happy with only one of those machines to be honest :D   \n",
                  "score": 2,
                  "created_utc": "2026-02-21 14:33:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6sig50",
                  "author": "jinnyjuice",
                  "text": "Why not for production?",
                  "score": 1,
                  "created_utc": "2026-02-22 16:08:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ofl7r",
              "author": "fallingdowndizzyvr",
              "text": "You can get 4 Sparks for $12K.",
              "score": 3,
              "created_utc": "2026-02-21 22:42:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oh648",
          "author": "CalvinBuild",
          "text": "That's amazing! I can't wait to do it locally!",
          "score": 2,
          "created_utc": "2026-02-21 22:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6swjif",
          "author": "Weak-Split-538",
          "text": "How did you connect the 4 sparks together ? Using NIC with a NIC switch ? Or just network cluster ?",
          "score": 2,
          "created_utc": "2026-02-22 17:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t4t7c",
              "author": "Icy_Programmer7186",
              "text": "I use switch: [https://mikrotik.com/product/crs804\\_ddq](https://mikrotik.com/product/crs804_ddq)",
              "score": 2,
              "created_utc": "2026-02-22 17:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nmh9x",
          "author": "I_like_fragrances",
          "text": "I am able to get unsloth's Q6\\_K\\_XL with max context at around 40 tok/s. What quant do you use on the sparks? Typically if I offload these bigger models to the CPU I can get around 20 tok/s but when they're fully on the GPU they run at around 40 tok/s.\n\nhttps://preview.redd.it/1ujmj21qmwkg1.png?width=836&format=png&auto=webp&s=ee566d16319e3ae1db5631689978d30c961f67c7",
          "score": 1,
          "created_utc": "2026-02-21 20:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p4g3f",
              "author": "Icy_Programmer7186",
              "text": "I use FP8 quantization -> Qwen/Qwen3.5-397B-A17B-FP8\n\nSpark has a unified memory, so I guess there is no off-loading.",
              "score": 2,
              "created_utc": "2026-02-22 01:13:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ralgvu",
      "title": "Local LLM for Mac mini",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "author": "Jiggly_Gel",
      "created_utc": "2026-02-21 08:00:24",
      "score": 43,
      "num_comments": 30,
      "upvote_ratio": 0.91,
      "text": "I’ve been watching hours of videos and trying to figure out whether investing in a Mac mini with 64 GB RAM is actually worth it, but the topic is honestly very confusing and I’m worried I might be misunderstanding things or being overly optimistic.\n\nI’m planning to build a bottom up financial analyst using OpenClaw and a local LLM, with the goal of monitoring around 500 companies. I’ve discussed this with ChatGPT and watched a lot of YouTube content, but I still don’t have a clear answer on whether a 30B to 32B parameter model is capable enough for this kind of workload.\n\nI’ll be getting paid for a coding project I completed using Claude, and I’m thinking of reinvesting that money into a maxed out Mac mini with 64 GB RAM specifically for this purpose.\n\nMy main question is whether a 30B to 32B local model is sufficient for something like this, or if I will still need to rely on an API. If I’ll need an API anyway, then I’m not sure it makes sense to spend so much on the Mac mini.\n\nI don’t have experience in this area, so I’m trying to understand what’s realistic before making the investment. I’d really appreciate honest input from people who have experience running local models for similar use cases.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ralgvu/local_llm_for_mac_mini/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6lpe1n",
          "author": "McMissile",
          "text": "I would just build whatever it is that you want on AWS and pay for cloud LLM tokens. Then you can figure out the feasability of what you're trying to achieve, and determine whether the 32b models will be sufficient. If it works, then great, buy the mac mini and run it locally. If not you've saved yourself a  bunch of money.",
          "score": 21,
          "created_utc": "2026-02-21 14:11:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6luuwp",
              "author": "AppointmentAway3164",
              "text": "This is the conclusion I reached. Targeting 30B models on huggingface. See how my use case tests. If it works then consider the $19k/$20k investment. Currently the the math doesn’t generally make sense for home token generation.",
              "score": 6,
              "created_utc": "2026-02-21 14:43:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ni9fh",
                  "author": "McMissile",
                  "text": "Yeah for most people I doubt running locally is ever really cost effective unless you need enormous amounts of tokens for sustained periods of time. 32b models that you can run locally have gotten fairly cheap to run in the cloud, especially compared to the price of a home lab.",
                  "score": 2,
                  "created_utc": "2026-02-21 19:44:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l3znc",
          "author": "rerorerox42",
          "text": "Maybe also look into tax discounts on investment into research and development.\n\nI have personally found newer 8B models (Ministral-3) to be capable on a 16GB M4 mini for what I have worked with by coding access to and using local models as needed with relevant context.\n\nAn issue currently is that it really is a little too soon for anybody to have tried out everything and every models’ true capabilities. FA-find-out kinda stage.",
          "score": 4,
          "created_utc": "2026-02-21 11:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qghk4",
              "author": "d4mations",
              "text": "I’m running ministral3-14b on a mac mini m416 gb I got used for very cheap. It is completely dedicated to lm studio with nothing else running on it. It also runs bge-3m for embedding. I have completely setup open claw with this setup, it runs crons flawlessly, has written several bash scripts that have worked on this first go, it proposed the embedded memory setup and then configured it perfectly, calls the few tools I have setup so far perfectly. The only caveat is context tokens. I have had to limit the number to 40k. Other than that I have a great little setup for less than 500€ and not one penny spent on tokens. I must say that I have fallback to openai oauth 20€ account and in the beginning I passed all of the recommendations and scripts proposed by ministral through it just to be sure. I trust it more and more everyday",
              "score": 3,
              "created_utc": "2026-02-22 07:05:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sc0sg",
                  "author": "OysterPickleSandwich",
                  "text": "Do you have your setup described anywhere? \nI’m debating on getting a mini for dedicated local use. \n",
                  "score": 1,
                  "created_utc": "2026-02-22 15:40:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6l4xya",
              "author": "Jiggly_Gel",
              "text": "I was thinking of messing around with the FA-find-out by running multiple models and then using a Claude API and comparing the responses but thought I’d ask here first in case if anyone had any other suggestions. \n\nI’ve seen similar posts to mine but they seemed to be looking for other use cases but the tax thing is definitely interesting I’m going to give that a look thank you",
              "score": 2,
              "created_utc": "2026-02-21 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kr7hn",
          "author": "battle_pantZ",
          "text": "Yeah, Im asking this question myself so I’m curious which answers you’ll get\n\nAfaik: 64GB is not enough to match the performance of large models. Even Kimi needs 200-500GB to perform properly.",
          "score": 5,
          "created_utc": "2026-02-21 09:30:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ksr3h",
              "author": "Jiggly_Gel",
              "text": "I think a 30B model should work fine on a Mac mini but the question is what model and how powerful is a 30B model? Some people attribute Qwen 30B to match gpt 4o but then if I have to spend money on machinery for 500 gb I’d rather run it via APIs because most of my financial analysis is done via my python bot it’s just I need something for judging the analysis and doing further industry research, comparative analysis, and reading annual reports and the news daily",
              "score": 1,
              "created_utc": "2026-02-21 09:45:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6l5n4e",
                  "author": "DistanceSolar1449",
                  "text": "Easy answers.\n\n- Qwen 3 VL 32b\n\n- GLM 4.7 Flash\n\n- Qwen 3.5 35b\n\n- Qwen 3 Next\n\nThese would be the best models you can fit on 64GB.\n\nIs buying a $2k mac mini 64gb to run these models worth it? Hell no. You can run the first 3 on a 3090 for $900 or so. And it'd be way faster than a mac mini.\n\nThe 512GB Mac Studio is actually something to consider, since you can run GLM 5 and similar big models that can actually rival ChatGPT/Claude. But the 64GB mac mini? Definitely not. You're better off spending $2k on GPT api credits.",
                  "score": 7,
                  "created_utc": "2026-02-21 11:48:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kuhd3",
                  "author": "battle_pantZ",
                  "text": "\nI think it makes more sense to buy a high-performance Mac if you need it for demanding tasks outside of AI. I mean, spending $2-3k to “only” use gpt4? I also look into it every day and do my research, and then I always come back to APIs. Yesterday, I paid $20 for the deepseek API, and I have to say, it's very, very cheap and pretty powerful for coding. It's 90% cheaper than the competition, by the way. \n\nSo a larger investment is needed. Either an even more powerful Mac or several smaller Macs as a cluster. Until a new AI comes out that requires even more hardware performance. If you extrapolate that, APIs will probably be cheaper.",
                  "score": 6,
                  "created_utc": "2026-02-21 10:02:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lr2su",
          "author": "HumbleTech905",
          "text": "Pay a few dollars for a service to deploy a 30B model, test and evaluate it, then decide if it's worth buying the Mac.",
          "score": 3,
          "created_utc": "2026-02-21 14:21:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m42f5",
          "author": "ScuffedBalata",
          "text": "\"Will a 30B model work for this use\" is very nebulous. It's not something you can just define.  It's like asking \"what IQ can be a financial analyst\".",
          "score": 3,
          "created_utc": "2026-02-21 15:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o10qi",
          "author": "Comprehensive_Iron_8",
          "text": "If you think you'll love using openclaw using a local model. The local models who remotely perform as well will be worth multiple MAC STUDIOs costing thousands of dollars. A model running inside a 64GB ram mac mini is not worth using. You'll not love it. You can use it for small things like email classification projects and maybe a small RAG system. \n\nWhy I say this, OPUS is so good, that people who have once used it, do not like 5.3 codex just because it does not have that kind of personality. And tomorrow there will be another model which has the same kind of moat. \n\nBut none of them at this time are good enough to run in a 64GB ram mac mini.",
          "score": 2,
          "created_utc": "2026-02-21 21:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o1oyx",
              "author": "Comprehensive_Iron_8",
              "text": "When I say this. I think a $10k mac studio can run GLM-5 and Minimax 2.5 which I actually like to run(using cloud providers, because the $10k ROI is not there yet) for openclaw, and are decent. They make mistakes yes, but you can optimize them to make less of them. But nothing for a mac mini yet.",
              "score": 1,
              "created_utc": "2026-02-21 21:27:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6o9tgu",
                  "author": "nvidiabookauthor",
                  "text": "What are some non China cloud providers to test this?",
                  "score": 1,
                  "created_utc": "2026-02-21 22:10:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mvskq",
          "author": "egoslicer",
          "text": "I have a 128B Strix Halo and run 70-100B models, and I'd say they are 'ok' for OpenClaw generalist tasks and tool calling. \n\nHowever 64GB Mac Mini and targeting ~30B models? Even more restrictive than that and while some are capable with tool calling, the results are going to be pretty meh overall. \n\nIf I were you, I'd setup OpenClaw and run it through MiniMax or Kimi 2.5 apis like many people are doing for a couple of months to see how that runs for you. You can do that on any machine you currently have and get much stronger reasoning and results.\n\nFor a local setup, IMO, ~30B models are ok for scoped tasks, but doing financial analysis with a lot of variables is likely not going to give you great results.",
          "score": 1,
          "created_utc": "2026-02-21 17:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qlfq3",
          "author": "emotionallofi",
          "text": "I just returned my m4 pro mac mini 64gb today. Going with a cheap used m2 max until m5 ultra is released.",
          "score": 1,
          "created_utc": "2026-02-22 07:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aj47q",
          "author": "emmettvance",
          "text": "The 64gb mac mini is genuinely capable of running 30-32b models well.. unified memory makes a real difference compared to GPU setups with the same RAM figure.. the honest question for your use case is whether 500 companies means 500 parallel analyses or sequential ones.. if it's sequential and you're okay with it running overnight the local setup can work..\n\nbut financial analysis on company filings tends to involve long documents and large context windows which is where even a well-speced Mac starts feeling the pressure.. the per-token cost of running the same models through providers like deepinfra or together is low enough that for the volume you are descriibing it might actually be cheaper than the hardware investment depending on how often you run full sweeps.",
          "score": 1,
          "created_utc": "2026-02-25 08:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l93cm",
          "author": "thought_provoking27",
          "text": "Great thread .",
          "score": 0,
          "created_utc": "2026-02-21 12:18:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8iew6",
      "title": "Open Source LLM Leaderboard",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/mrakuwr3xbkg1.png",
      "author": "HobbyGamerDev",
      "created_utc": "2026-02-18 23:04:19",
      "score": 40,
      "num_comments": 32,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8iew6/open_source_llm_leaderboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67lo9v",
          "author": "jiqiren",
          "text": "Not enough people have 512GB+ of vram or unified memory (like the Mac Studio). Otherwise Minimax M2.5 would be top dog. 🐶",
          "score": 7,
          "created_utc": "2026-02-19 08:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6890vq",
              "author": "deepfit",
              "text": "Still pretty large but MiniMax-M2.5 UD-Q2\\_K\\_XL from unsloth is both fast and < 100G of ram/vram.  I am having a hard time telling any difference from larger quant versions.",
              "score": 3,
              "created_utc": "2026-02-19 12:11:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68mysa",
              "author": "AfterShock",
              "text": "Came here to +1 MinMax 2.5 when I saw it missing from the list.",
              "score": 3,
              "created_utc": "2026-02-19 13:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d0mb1",
                  "author": "jiqiren",
                  "text": "I only have a Mac mini so can’t run it. But it’s so cheap on openrouter it’s been my goto model.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:30:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67bwp1",
          "author": "einord",
          "text": "Fun fact: the larger the model, the more intelligent.",
          "score": 14,
          "created_utc": "2026-02-19 07:08:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67suaq",
              "author": "DistanceSolar1449",
              "text": "This ranking is trash. Only thing it gets right is the top 2. \n\nDeepseek V3? That came out in 2024. Deepseek R1 came out a full year ago.\n\nWhere’s Deepseek V3.2? Why is Mistral Large rated so highly, and the same tier as gpt-oss-120b? Why Nemotron V1 instead of Nemotron V1.5?",
              "score": 14,
              "created_utc": "2026-02-19 09:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a0weu",
                  "author": "onil34",
                  "text": "Where is minimax?",
                  "score": 3,
                  "created_utc": "2026-02-19 17:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67jksw",
              "author": "Successful-Emu-6409",
              "text": "scaling still alive ",
              "score": 6,
              "created_utc": "2026-02-19 08:19:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cisu8",
              "author": "Available-Craft-5795",
              "text": "Sometimes, bigger moves faster. Sometimes, smaller takes its time. Both find their way — eventually — just on different roads to the finish line.\n\nOne is built for the masses, for the noise and the need and the scale. But we build for the love of the question, for the joy in the curious trail.\n\nThey don't care about the small ones. We do.\n\n*Built with curiosity, not compute.* [CompactAI](https://huggingface.co/spaces/CompactAI/Built-with-curiosity-not-compute)\n\n(I know its a promo, shush)",
              "score": 1,
              "created_utc": "2026-02-20 01:39:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o687bd6",
          "author": "entheosoul",
          "text": "This should be split between actual locally runable models and cloud models (not exactly local)",
          "score": 5,
          "created_utc": "2026-02-19 11:58:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67hymk",
          "author": "nunodonato",
          "text": "Amazing how got oss 120b holds it's place after all these new models have came out",
          "score": 3,
          "created_utc": "2026-02-19 08:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o693exw",
          "author": "Sufficient_Prune3897",
          "text": "Cursed tier list. Shows that benchmarks are not everything",
          "score": 2,
          "created_utc": "2026-02-19 15:09:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67h8z0",
          "author": "HoustonTrashcans",
          "text": "Can anyone tell me what quantization I need to run a 1T model on my laptop with 8 GB of VRAM? If my math is right that's Q.05?",
          "score": 2,
          "created_utc": "2026-02-19 07:57:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67quwa",
              "author": "Ell2509",
              "text": "Not possible. But why do you need a 1t model? With 8gb vram, assuming you have 16gb system ram, you could run a 7 or 8b model, realistically. \n\nMore than that will become unusable slow quite quickly.",
              "score": 2,
              "created_utc": "2026-02-19 09:31:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68pel8",
                  "author": "ShinigamiOverlord",
                  "text": "Very much so. I could somewhat use a 10-14B models, but they cap at 1.3 tokens/s",
                  "score": 1,
                  "created_utc": "2026-02-19 13:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69t4rr",
          "author": "peva3",
          "text": "OP, anyway you could turn this data into an API? I could use these benchmarks for a project I'm working on.",
          "score": 1,
          "created_utc": "2026-02-19 17:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ad120",
          "author": "Far_Cat9782",
          "text": "Gpt 120b is my goal to run locally. Currently the max I can slso to get real work done is 24b model.",
          "score": 1,
          "created_utc": "2026-02-19 18:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b5oz6",
          "author": "Used-Dance-7006",
          "text": "Sorry...maybe this is the designer in me but the color coding is counterintuitive to the way i perceive design.\n\nIs S good? It's red so I read that as the worst?  \nIs C good?\n\nShould I be focusing on S and A models? Is D bad then?\n\nJust trying to understand and appreciate the clarity.",
          "score": 1,
          "created_utc": "2026-02-19 21:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bezen",
          "author": "shankey_1906",
          "text": "Something like this would be amazing for differnt tiers of VRAM, and use-cases. Tier <16GB, <32GB, etc. Tier: Coding, Reasoning, ...",
          "score": 1,
          "created_utc": "2026-02-19 21:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6flfy0",
          "author": "Fantastic-Breath2416",
          "text": "C'è anche il mio\n\nhttps://nothumanallowed.com/search\n\nPotete usarlo!!",
          "score": 1,
          "created_utc": "2026-02-20 15:05:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbu7sx",
      "title": "M4 Pro 48 or M4 Max 32",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "author": "Mammoth-Error1577",
      "created_utc": "2026-02-22 18:58:33",
      "score": 39,
      "num_comments": 39,
      "upvote_ratio": 0.87,
      "text": "I got my machine renewed at work a week ago.\n\nThey rejected my request of a Mac studio with 128 GB and instead approved a MacBook M4 Pro with 48GB and 512.\n\nWell I finally got around to checking and they actually gave me a more expensive M4 Max but with 32 GB and 1TB instead.\n\n\nIn my previous chatting with Gemini it has convinced me that 128 GB was the bare minimum to get a sonnet level local LLM.\n\nWell I was going to experiment today and see just what I could do with 48 and to my surprise I only had 32, but a superior CPU and memory bandwidth.\n\n\nIf my primary goal was to run coding a capable LLM, even at the cost of throughout, I assume 48 is vastly superior.  However if the best model I can run with 48 (+ containers and IDE and chrome etc.) is really dumb compared to sonnet I won't even use it.\n\nI'm trying to decide if it's worth raising a fuss over getting the wrong, more expensive laptop. I can experiment with a very small model on the current one but unless it was shockingly good I don't think that experiment would be very informative.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rbu7sx/m4_pro_48_or_m4_max_32/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6toq7h",
          "author": "j00cifer",
          "text": "M5 ultra studio is coming out this year with a reported max RAM of 1TB.\n\n1TB RAM.",
          "score": 28,
          "created_utc": "2026-02-22 19:22:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tpdap",
              "author": "jiqiren",
              "text": "😍 want it so good 🥰 M5 Ultra 1TB??? Yes please!",
              "score": 7,
              "created_utc": "2026-02-22 19:25:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v9irr",
                  "author": "gingerbeer987654321",
                  "text": "Only 1?  Get 4 and do the Thunderbolt raid thing",
                  "score": 5,
                  "created_utc": "2026-02-23 00:20:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71kvcd",
                  "author": "LimiDrain",
                  "text": "Does this unified memory work as fast as VRAM or it's close to normal RAM speeds?",
                  "score": 1,
                  "created_utc": "2026-02-23 23:38:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uawrt",
              "author": "sav22v",
              "text": "But you'll have to sell your kidneys and children to pay for it...",
              "score": 6,
              "created_utc": "2026-02-22 21:14:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6uq4zs",
                  "author": "j00cifer",
                  "text": "I’m making the case to them now.",
                  "score": 2,
                  "created_utc": "2026-02-22 22:32:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6uyghh",
              "author": "grim-432",
              "text": "With the current price of ram, what’ll that cost?  $25,000?",
              "score": 3,
              "created_utc": "2026-02-22 23:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x4h53",
                  "author": "ijontichy",
                  "text": "They would have locked in RAM costs for this year before the RAMpocalypse. But do you think they'll hold prices steady? 🤔",
                  "score": 1,
                  "created_utc": "2026-02-23 08:18:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71z016",
                  "author": "GonzoDCarne",
                  "text": "Ram price does not apply to oems like Apple. Due to many things that someone might want to go into detail in a long thread. My hard guess is they will target 15k or 19999. M3 Ultras with 512GiB go for 10k un the US since before the ram surge and today.",
                  "score": 1,
                  "created_utc": "2026-02-24 00:56:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6trf6a",
              "author": "Mammoth-Error1577",
              "text": "Unfortunately not an option. The only studio I could get is also 36GB.",
              "score": 2,
              "created_utc": "2026-02-22 19:35:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6u9uza",
              "author": "ZealousidealShoe7998",
              "text": "at that level what llm would one even use to reach comercial levels ?",
              "score": 1,
              "created_utc": "2026-02-22 21:08:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6xj37x",
              "author": "iezhy",
              "text": "Given current ram prices (and Apple markup), this probably will be out of reach for most users",
              "score": 1,
              "created_utc": "2026-02-23 10:41:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70hc2o",
                  "author": "j00cifer",
                  "text": "I don’t see why a 2nd kidney is so important to people",
                  "score": 1,
                  "created_utc": "2026-02-23 20:17:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7134m5",
              "author": "Jealous_Incident7978",
              "text": "Starts getting funny that we drop $$$$ on a 1TB Ram M5 ultra studio to run open weight models that is essentially free. 😆 imagine paying something similar for qwen 3.5 / DeepSeek etc just to run the model locally",
              "score": 1,
              "created_utc": "2026-02-23 22:03:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o729hxh",
                  "author": "j00cifer",
                  "text": "1 TB RAM.",
                  "score": 1,
                  "created_utc": "2026-02-24 01:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6u93hc",
          "author": "No_Success3928",
          "text": "Hahaha sonnet level 🙄 classic gemini hallucinations",
          "score": 13,
          "created_utc": "2026-02-22 21:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o737jwu",
              "author": "WildRacoons",
              "text": "Clearly doesn’t think very highly of sonnet",
              "score": 2,
              "created_utc": "2026-02-24 05:38:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tl9zz",
          "author": "Thump604",
          "text": "It’s all memory and it’s never enough",
          "score": 25,
          "created_utc": "2026-02-22 19:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u2tks",
          "author": "BisonMysterious8902",
          "text": "I hate to break it to Gemini, but you can't get anywhere close to Sonnet level with 128Gb. Can you get something usable? Sure, but it'll never match frontier level models. Even a Studio with 512Gb. That's just the current state of things.",
          "score": 10,
          "created_utc": "2026-02-22 20:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ughj5",
              "author": "meTomi",
              "text": "Current state? When you think your home pc can compete with million dollar racks in server rooms?\nOn the other hand yes technology is getting better and you can run better and bigger models at home.",
              "score": 1,
              "created_utc": "2026-02-22 21:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ve5yt",
          "author": "MrRandom04",
          "text": "Only open source LLMs that compete with Sonnet 4.6 / Opus 4.6 are GLM 5 and Kimi K2.5. Of these, only GLM 5 is super reliable for agentic coding. That model is far too big for anything less than like 512gb ram. For 32gigs, you can consider the Qwen series UD quants and then have a workflow where you shell out to an API provider of GLM 5 or even just Sonnet / Opus for planning and big design / knowledge level tasks while the manual editing and coding is done by Qwen. The latest ones are very good at stuff like Python and really good for their size.",
          "score": 9,
          "created_utc": "2026-02-23 00:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4d3s",
          "author": "Expert-Reaction-7472",
          "text": "i dont think id make a fuss about this to any place i've ever worked.\n\nnice thing about being self employed is if i want to splurge on a machine i can. Which usually means I have something decent but not mind blowingly expensive cos it's my own money and i'd rather spend the extra on a holiday or something.",
          "score": 3,
          "created_utc": "2026-02-22 20:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uqm3v",
          "author": "ComfortablePlenty513",
          "text": "always prioritize memory. M4 architecture is fundamentally better than previous gen for inference",
          "score": 3,
          "created_utc": "2026-02-22 22:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w40t7",
          "author": "Sharp-Mouse9049",
          "text": "32GB in 2026 for serious local LLM work is basically consumer-tier. I don’t care how fast the M4 Max is — if you’re constantly forced into tiny quants or can’t load 70B comfortably, you’re artificially capping your experimentation. Bandwidth doesn’t matter if the model doesn’t fit. RAM is the ceiling.",
          "score": 2,
          "created_utc": "2026-02-23 03:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v9e8v",
          "author": "pondy12",
          "text": "Get a HP ZBook Ultra G1a, Ryzen AI Max+ PRO 395, 64gb - 128gb of ram, 256gb/s ram bandwidth. Will be 1/4th the price. \n\n",
          "score": 1,
          "created_utc": "2026-02-23 00:20:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xhgbi",
              "author": "Confident-Strength-5",
              "text": "It also has 256gb/s bandwidth, so…\nLLMs really like bandwidth…",
              "score": 1,
              "created_utc": "2026-02-23 10:25:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vlezt",
          "author": "midz99",
          "text": "Vram or whatever mac calls it is everything. Higher the better. really you need 128gb to even get close to something worth testing.",
          "score": 1,
          "created_utc": "2026-02-23 01:30:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73fuxe",
          "author": "Coyote_Android",
          "text": "After playing around with 32 GB for a while, do you think 48 GB would allow for a significantly better model? Not necessarily for coding though. Just language generation. I'm facing a similar decision.",
          "score": 1,
          "created_utc": "2026-02-24 06:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74mvru",
              "author": "Mammoth-Error1577",
              "text": "I haven't had the opportunity to do anything that seemed usable yet. This has been my first attempt with a local model though, and I naively thought it would be some simple drop in for a slower and dumber version of a cloud model, but the experience I had was so poor that I couldn't see myself using it for anything. I definitely need to do more tweaking, I didn't even get as far as trying to change any configuration, as I didn't even know that could be changed!",
              "score": 2,
              "created_utc": "2026-02-24 12:57:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7518i4",
                  "author": "Coyote_Android",
                  "text": "But you only have coding use cases? For playing around (not coding though afaik) you might wanna give [https://msty.ai](https://msty.ai) a shot",
                  "score": 1,
                  "created_utc": "2026-02-24 14:18:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6uivgp",
          "author": "DistanceSolar1449",
          "text": "M4 max has way faster memory bandwidth\n\n48gb is not enough for Qwen3 next \n\nJust stick with 32gb",
          "score": 0,
          "created_utc": "2026-02-22 21:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6unjkw",
              "author": "Mammoth-Error1577",
              "text": "I just tried  qwen2.5-coder:14b in open code and it was extremely dumb and worse than copying and pasting from a web browser (on an empty repo)\n\nI tried qwen2.5-coder:32b 1st and /init wasn't doing anything so Gemini told me to downgrade.\n\nBut /init didn't do anything after downgrading either.\n\nAll I could get it to do was spit out code that it would tell me to put into the file myself instead of doing it itself, and then the code wasn't even syntactically correct.\n\nI'm super shocked it was so bad, there was no way I was doing it correctly.",
              "score": 0,
              "created_utc": "2026-02-22 22:18:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zftss",
                  "author": "Djagatahel",
                  "text": "What is that /init you're talking about?",
                  "score": 1,
                  "created_utc": "2026-02-23 17:24:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tmf6f",
          "author": "Svyable",
          "text": "Surprised how much I get out of my 24 Pro M4 I have like 100Gb running in Brave browsers no problem. \n\nModel sizes are coming down. Don’t complain innovate",
          "score": -1,
          "created_utc": "2026-02-22 19:11:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdb146",
      "title": "Can anybody test my 1.5B coding LLM and give me their thoughts?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "author": "Great-Structure-4159",
      "created_utc": "2026-02-24 08:45:48",
      "score": 32,
      "num_comments": 31,
      "upvote_ratio": 0.97,
      "text": "I fine tuned my own 1.5B LLM, took Qwen2.5-1.5B-Instruct and fine tuned it on a set of Python problems, and I got a pretty decent LLM!\n\nI'm quite limited on my computational budget, all I have is an M1 MacBook Pro with 8GB RAM, and on some datasets, I struggled to fit this 1.5B model into RAM without getting an OOM.\n\nI used mlx\\_lm to fine tune the model. I didn't fine tune fully, I used LoRA adapters and fused. I took Qwen2.5-1.5B-Instruct, trained it for 700 iterations (about 3 epochs) on a 1.8k python dataset with python problems and other stuff. I actually had to convert that data into system, user, assistant format as mlx\\_lm refused to train on the format it was in (chosen/rejected).  I then modified the system prompt, so that it doesn't give normal talk or explanations of its code, and ran HumanEval on it (also using MLX\\_LM) and I got a pretty decent 49% score which I was pretty satisfied with.\n\nI'm not exactly looking for the best bench scores with this model, as I just want to know if it's even good to actually use in daily life. That's why I'm asking for feedback from you guys :D\n\nHere's the link to the model on Hugging Face:\n\n[https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B](https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B)\n\nIt's also available on LM Studio if you prefer that.\n\nPlease test out the model and give me your thoughts, as I want to know the opinions of people using it. Thanks! If you really like the model, a heart would be much appreciated, but I'm not trying to be pushy, only heart if you actually like it.\n\nBe brutally honest with your feedback, even if it's negative like \"this model sucks!\", that helps me more thank you think (but give some reasoning on why it's bad lol).\n\nEdit: 9.6k views? OMG im famous.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdb146/can_anybody_test_my_15b_coding_llm_and_give_me/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o7421hy",
          "author": "Ok-Employment6772",
          "text": "In a few weeks I have a large python project coming up, cant wait to test it",
          "score": 8,
          "created_utc": "2026-02-24 10:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74265u",
              "author": "Great-Structure-4159",
              "text": "Thanks for testing! Can't wait to hear your feedback.",
              "score": 3,
              "created_utc": "2026-02-24 10:15:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7cvtgn",
              "author": "Maleficent-Ad5999",
              "text": "This.. the beauty of this community! Kudos.",
              "score": 1,
              "created_utc": "2026-02-25 17:10:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o744c44",
          "author": "RnRau",
          "text": "I don't do python, but I think its just awesome to see open source tools and weights being used in such a resource constrained environment to get a very useful outcome. \n\nCheers for the writeup!",
          "score": 11,
          "created_utc": "2026-02-24 10:35:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bpi8",
              "author": "Great-Structure-4159",
              "text": "Yeah I was pretty shocked too that 8GB could do stuff like this, but yeah I find the subject very fascinating :)",
              "score": 2,
              "created_utc": "2026-02-24 11:39:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74gxww",
          "author": "fermented_Owl-32",
          "text": "This is what I needed. I want a local tool calling orchestrator and dynamic tool creator in python. Let me test it how it holds up in creating python scripts by receiving instructions from another agent. The smaller the better, as i need to run 5 such models ( i call them micromodels ).\nWill let you know how it turns out.",
          "score": 5,
          "created_utc": "2026-02-24 12:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s0h3",
              "author": "Great-Structure-4159",
              "text": "Oh… tool calling, interesting. I should try that, I didn’t train with tool calling in mind actually, but this is really cool, I think it can work.",
              "score": 1,
              "created_utc": "2026-02-24 13:28:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o74s45t",
              "author": "Great-Structure-4159",
              "text": "I don’t think it’ll be very good as an orchestrator, but I’ll try making a model fine tuned for orchestrating tool calling, that would be really cool. Do let me know if it works out good, very interesting to see LLMs applied like this.",
              "score": 1,
              "created_utc": "2026-02-24 13:28:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74uaba",
                  "author": "fermented_Owl-32",
                  "text": "Orchestrator will be a function-gemma model. One of its tools will be the tool creator. The tool creator will use your model to write scripts for the use-case in user's query. I need simpler but fast and a little intelligent scripting, I will test it for that purpose",
                  "score": 2,
                  "created_utc": "2026-02-24 13:41:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73zmq7",
          "author": "Whiplashorus",
          "text": "Am gonna check it after \nJust a question why using qwen2.5 as a foundation?\nLfm2.5 or even qwen3 are not good for your usecase?",
          "score": 3,
          "created_utc": "2026-02-24 09:52:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74056z",
              "author": "Great-Structure-4159",
              "text": "Great question! My first choice was actually LFM2.5, and I did try that first, but for some reason when fusing it with adapters on MLX, llama.cpp just refuses to convert it to GGUF. I tried troubleshooting but eventually just gave up. Qwen3 was my next choice but I just decided to keep it simple and start with 2.5 and go from there, mainly because Qwen3 came with a 1.7B model (which was pushing my RAM limit due to the dataset having long samples) and also, in my searches, didn't have an instruct version weirdly. Maybe the next release will be with qwen3 if the qwen architecture proves good from user tests (and I can do something about the dataset).\n\n",
              "score": 6,
              "created_utc": "2026-02-24 09:57:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o747qs4",
          "author": "Outrageous-Story3325",
          "text": "Whats the token per second, on your gpu ? ",
          "score": 4,
          "created_utc": "2026-02-24 11:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74bmnr",
              "author": "Great-Structure-4159",
              "text": "I have Apple M1, and I get about 50 tokens/s on GGUF, and 60 tokens/s on MLX (which is not on the repo at the moment.)",
              "score": 5,
              "created_utc": "2026-02-24 11:38:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74w0cm",
          "author": "Fun_Abroad_3650",
          "text": "Hi Sure, i am making an android llm runner ill be happy to try it out, just need the gguf file",
          "score": 5,
          "created_utc": "2026-02-24 13:50:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74weuk",
              "author": "Great-Structure-4159",
              "text": "Thanks for offering to test! The .gguf file is on the repo. There's fp16 and q4\\_k\\_m quants, so you can use whichever one you prefer :D.",
              "score": 2,
              "created_utc": "2026-02-24 13:52:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o769fot",
          "author": "BringMeTheBoreWorms",
          "text": "I have a few python projects that I could throw at it. \n\nHow have you found it compared to other models so far?",
          "score": 4,
          "created_utc": "2026-02-24 17:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lhyg",
              "author": "Great-Structure-4159",
              "text": "In terms of benchmarks, it’s pretty decent for a 1.5B model. It beats the base Qwen at coding, but I’m pretty sure Qwen Coder is slightly better at the benchmark. However, Qwen coder doesn’t have any ability at actually talking about something related to coding, like explaining the code, that’s why I trained on the instruct version and not the coder version.",
              "score": 1,
              "created_utc": "2026-02-25 03:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79ne44",
                  "author": "BringMeTheBoreWorms",
                  "text": "Ill run it over a smallish project later on and see what it says",
                  "score": 1,
                  "created_utc": "2026-02-25 04:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74i9b9",
          "author": "zulutune",
          "text": "Fascinating!\nDid you document the process somewhere? Do you have good resources on how to do it?",
          "score": 3,
          "created_utc": "2026-02-24 12:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74sfyu",
              "author": "Great-Structure-4159",
              "text": "I didn’t document my process anywhere actually, I just typed out all that to give an idea. MLX-LM doesn’t really have any good resources, other than the one video they made on the Apple Developer YouTube channel regarding it. They don’t go through every feature and command there, however, so I mainly referred to the documentation they have, which is pretty decent.",
              "score": 3,
              "created_utc": "2026-02-24 13:30:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75l6fb",
                  "author": "zulutune",
                  "text": "Thank for your reply!",
                  "score": 3,
                  "created_utc": "2026-02-24 15:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76gnnl",
          "author": "cHekiBoy",
          "text": "following",
          "score": 3,
          "created_utc": "2026-02-24 18:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ljlq",
              "author": "Great-Structure-4159",
              "text": "Thanks! Hope you like the model!",
              "score": 1,
              "created_utc": "2026-02-25 03:50:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7767k4",
          "author": "sethPower00",
          "text": ".",
          "score": 3,
          "created_utc": "2026-02-24 20:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79b1xf",
          "author": "Historical_Ice187",
          "text": "Hey, could drop few resources you used for this? I've a mac mini and have been wanting to try and learn something like this.",
          "score": 2,
          "created_utc": "2026-02-25 02:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79lnpg",
              "author": "Great-Structure-4159",
              "text": "Yeah I’m looking into making a small article on this because you’re not the first one to ask for this. I’ll contact you once I write it.",
              "score": 1,
              "created_utc": "2026-02-25 03:50:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a5je3",
                  "author": "tocarbajal",
                  "text": "I’ll stay tuned for that article, let’s hope not so small, anyway.\nThank you for sharing.",
                  "score": 2,
                  "created_utc": "2026-02-25 06:11:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b8s9l",
                  "author": "Historical_Ice187",
                  "text": "Thanks alot. Truly appreciate it.",
                  "score": 2,
                  "created_utc": "2026-02-25 12:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdigyc",
      "title": "Thoughts on Mac Studio M3 Ultra with 256gb for open claw and running models locally",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "author": "00100100",
      "created_utc": "2026-02-24 14:59:45",
      "score": 24,
      "num_comments": 35,
      "upvote_ratio": 0.8,
      "text": "I know a lot of people say to just pay for API usage and those models are better, and I plan to keep doing that for all of my actual job work.\n\nBut for building out my own personal open claw to start running things on the side, I really like the idea of not feeding all of my personal data right back to them to train on.   So I would prefer to run locally.\n\nCurrently I have my gaming desktop with a 4090 that I can run some models very quickly on, but I would like to run a Mac with unified memory so I can run some other models, and not care too much if they have lower tokens per second since it will just be background agentic work.\n\nSo my question is: M3 ultra with 256gb of unified memory good?  I know the price tag is kinda insane, but I feel like anything else with that much memory accessible by a GPU is going to be insanely priced.  And with the RAM and everything shortages...I'm thinking the price right now will be looking like a steal in a few years?\n\nAlternatively, is 96gb of unified memory enough with an M3 Ultra?  Both happen to be in stock near me still, and the 256gb is double the price....but is that much memory worth the investment and growing room for the years to come?\n\nOr just everyone flame me for being crazy if I am being crazy. lol. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rdigyc/thoughts_on_mac_studio_m3_ultra_with_256gb_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o75a7uf",
          "author": "Crafty-Diver-6948",
          "text": "it's okay. you'll be able to run minimax locally at about 50tps, 4 million tokens per day... So you do the math if it's worth it. I have a 196gb and I don't really use it for local models nearly as much as I though",
          "score": 11,
          "created_utc": "2026-02-24 15:03:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76x17w",
              "author": "so_schmuck",
              "text": "Wow that’s $$",
              "score": 2,
              "created_utc": "2026-02-24 19:29:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o781rk5",
                  "author": "timbo2m",
                  "text": "Yeah if you can make sure it's running constantly and building something that generates money it could potentially pay for itself. Possible, just not probable. And quite the gambling exercise. You'd probably be better to just pay for minimax coder for $20 a month, the trade off being you give away your data",
                  "score": 2,
                  "created_utc": "2026-02-24 22:40:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75llp9",
              "author": "cmndr_spanky",
              "text": "Why that model over some others?",
              "score": 1,
              "created_utc": "2026-02-24 15:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75u1fb",
                  "author": "nomorebuttsplz",
                  "text": "it or Step 3.5 are the best models that will fit at q4 in 256 gb. I guess you could try to cram GLM 4.7 in instead.",
                  "score": 7,
                  "created_utc": "2026-02-24 16:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78jrvu",
              "author": "Badger-Purple",
              "text": "there are no macs with 196gb of ram. There is a 192gb m2 ultra, which I own, and having run LLMs for the past 8 months on it, you’ll never reach 50 tokens per second at the context lengths that an agent needs. Unless openclaw has some magic to decrease context, you’ll wait a cool 2 minutes before each reply.",
              "score": 1,
              "created_utc": "2026-02-25 00:17:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76e7i3",
          "author": "meowrawr",
          "text": "I have the m3 ultra, 80c GPU, 256gb ram and wish I had gone with 512gb. Don’t be me if you go this route.",
          "score": 10,
          "created_utc": "2026-02-24 18:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75ai9x",
          "author": "FullstackSensei",
          "text": "I think it's much cheaper to share your credit card info and bank account login details here on reddit. You'll save at least the 8k needed to buy the Mac, and might even still have some money left in your bank account",
          "score": 20,
          "created_utc": "2026-02-24 15:05:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75atvk",
              "author": "00100100",
              "text": "Let me get open claw set up and responding to my reddit messages and those details should be posted within a few days!",
              "score": 11,
              "created_utc": "2026-02-24 15:06:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75cjg6",
          "author": "apVoyocpt",
          "text": "Okay, just get a cheap device anything that will run Linux. Then install openclaw and pay for tokens (best through open router, you can even pick free models) Then find out if openclaw does anything useful for you. Then test a qwen 3.5 through one router. Then decide if openclaw and a 7000 Mac mini so you can run qwen 3.5 locally is worth it",
          "score": 8,
          "created_utc": "2026-02-24 15:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767yvl",
              "author": "brianlmerritt",
              "text": "Exactly! You might need a 10k Mac like many others are buying.  You might hate even that.  You might need only 128gb ram.\n\nThe pay per token suppliers (direct or via openrouter) are a good way to put your toe in the water without the shark removing your leg.",
              "score": 3,
              "created_utc": "2026-02-24 17:37:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o768ll3",
              "author": "00100100",
              "text": "I think this is the route I'm gonna go after all the feedback.  I have my gaming desktop that sits most of the time. I already am running nobara linux on it, so I think I'll just test it out for now where I can run a local model for some stuff... and then I'll probably just go the anthropic api route. ",
              "score": 2,
              "created_utc": "2026-02-24 17:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77ptsl",
          "author": "cavebaird",
          "text": "I have a Mac Studio M3 Ultra with 256gb.   After much experimentation I comfortably run MiniMax-M2.5-MLX-6.5bit with reasonable ~50 t/s response in chat and a good chat response in OpenClaw.  Solid reasoning and low hallucinating and BS answers.  Tool use is good.  No vision on this model.  Memory pressure is comfortable.  I use Inferencer for the server connection but LM Studio works too.  \n\nGoing to try the new Qwen3.5 tonight (397B A17B 3bit SWAN and GGUF Q3_K_XL) to see how that runs.   Both of those are ~ 170gb, so should run with some headroom.  Do I wish I could have gotten the 512gb.  Sure, if I had another 4K.   I think the upcoming M5 ultras will be a bigger step up with LLM speed and efficiency.",
          "score": 4,
          "created_utc": "2026-02-24 21:42:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75v4ed",
          "author": "Hector_Rvkp",
          "text": "If you hate life, you could get a Strix halo for 2200$. 128gb unified ram. It's slower, but it's cheaper. Slower isn't slow, it's actually usable because bandwidth is 256gb/s.",
          "score": 6,
          "created_utc": "2026-02-24 16:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7688i1",
              "author": "00100100",
              "text": "That is super interesting.  I didn't know anyone outside of Mac was doing unified memory.  ",
              "score": 4,
              "created_utc": "2026-02-24 17:38:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76afdb",
                  "author": "Hector_Rvkp",
                  "text": "they dont call it that, but the point is that the entire 128 runs at the same speed / bandwidth, and if you run your model on linux, you can use 100% of the memory for the model (or like 99%).  \nAs opposed to GPU (VRAM) vs RAM.  \nSo, to run large MoE models well, the cheapest entry point is strix halo. When you get to 3000+, you have a choice between a very fast GPU on a regular PC w DDR5 ram, or DGX Spark, or Apple studio.  \nThe drivers on AMD started working this year, but they're not plug and play like Apple or Nividia. There's no free lunch. Big community playing w it though, precisely because it's cheap and fairly mighty for today's models. You can run Qwen3.5-397B-A17B on it, and speed shouldn't even suck. And apparently, w a 2bit quant, the model is big enough (397b parameters) that it's still quite good. Allegedly.",
                  "score": 4,
                  "created_utc": "2026-02-24 17:48:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o779jx9",
              "author": "sav22v",
              "text": "It’s not the same “unified RAM” like Apple!",
              "score": 0,
              "created_utc": "2026-02-24 20:27:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77r3go",
          "author": "jiqiren",
          "text": "You need to wait until March 4th to see what new goodies Apple is selling. You might be able to get a M5 Ultra for the same price.",
          "score": 3,
          "created_utc": "2026-02-24 21:48:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o765x8n",
          "author": "TheOverzealousEngie",
          "text": "Your problem is you can't get a foundation model running on 256 . The right flavor of DeepSeek will cost you 1TB or the like.  And the difference to openclaw for expensive DeepSeek vs. Cheap Kimi is the existence of tools in the LLM. DS has them , kimi does not. \n\nMeaning after you've set everything up, invested all this architecture and money, there are skills that are just architecturally off limits. Yuck. ",
          "score": 5,
          "created_utc": "2026-02-24 17:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o768d2z",
              "author": "00100100",
              "text": "Yeah, I think I am getting the gist of:  basic server, pay for better models.  ",
              "score": 7,
              "created_utc": "2026-02-24 17:39:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o76p6nq",
              "author": "Far_Cat9782",
              "text": "Your acting like skills are so hard to code for? It's just scripts that the AI can use. U can use any small model and increase its tool usage by making scripts for whatever u want and system prompt the model to know or has access to the tool. I got my 27b gemma model writing python code and running it in the console and displaying the results and doing a bunch of other 'skills.\"",
              "score": 4,
              "created_utc": "2026-02-24 18:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76syqz",
                  "author": "TheOverzealousEngie",
                  "text": "You're , not your. ",
                  "score": 0,
                  "created_utc": "2026-02-24 19:10:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77o14g",
          "author": "donotfire",
          "text": "The great thing about renting off the cloud is it’s easily scalable. You can just decide to double your model size and it’s done, just like that. But if you buy an M3 and decide 256GB isn’t enough, well the you’re out of luck. Gotta buy a new computer then.",
          "score": 2,
          "created_utc": "2026-02-24 21:34:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75aemc",
          "author": "floppypancakes4u",
          "text": "Do able, but to make automations and scripts, id still use smarter models.",
          "score": 2,
          "created_utc": "2026-02-24 15:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75btgh",
              "author": "00100100",
              "text": "By smarter I assume you mean cloud hosted/pay per token like opus?\n\nI probably won't use it to do much coding with this setup.  I have corporate provided Claude for that.  I'm more wanting  to build it as my own personal assistant type device.  Organizing my calendars. Checking emails. Watching my conversations and generating my todo lists(and maybe eventually at least scheduling agentic work via my anthropic sub).  ",
              "score": 0,
              "created_utc": "2026-02-24 15:11:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75cqhd",
                  "author": "floppypancakes4u",
                  "text": "Still, smarter. Opus is good not only cause of its excellent coding abilities, but its extremely good at reasoning AND tool calling, which are the two primary aspects of automation in openclaw. Claw is not built to make token conservative automations, it makes repeatable smart automations. It does its best to make scripts to handle it all, but it still makes its own prompt to process for each automation. You want consistency with automations, and because it still uses prompts, its best to use the smart models for it. You can absolutely experiment and see if something dumber works. For instance, I built a automation with codex, harnessed by a small, but very strict prompt. Now it runs on my computer every 5 and 10 minutes (two different scripts) using glm 4.7 flash",
                  "score": 3,
                  "created_utc": "2026-02-24 15:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79o0tf",
          "author": "jw-dev",
          "text": "I think the 256gb m3u is the sweet spot actually, can run some great models for everyday/private stuff and then burst to the cloud if you need heavy models, or speed… the bigger models get too slow, especially as the context size grows.",
          "score": 1,
          "created_utc": "2026-02-25 04:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79qb27",
          "author": "Xendrak",
          "text": "My thoughts are: it’s viable. I’m eyeing it too but I’m waiting for the new M5 models coming this year it’s supposed to have several times more ai cores.",
          "score": 1,
          "created_utc": "2026-02-25 04:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79qgph",
          "author": "Xendrak",
          "text": "How much you doing for llm use? Might get away with minimax or kimi on openrouter until you can source the hardware you’d like.",
          "score": 1,
          "created_utc": "2026-02-25 04:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75vyei",
          "author": "Ryanmonroe82",
          "text": "M4 Pro 24gb is the minimum.  Look at bandwidth and not just RAM  specs",
          "score": 1,
          "created_utc": "2026-02-24 16:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ogiv",
          "author": "No_Knee3385",
          "text": "Why spend the extra premium on apple instead of building your own PC or buying a custom  build?",
          "score": 1,
          "created_utc": "2026-02-25 00:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o797wsj",
              "author": "scottag",
              "text": "The unified memory that can be shared between GPU and CPU.",
              "score": 3,
              "created_utc": "2026-02-25 02:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o799fp2",
                  "author": "No_Knee3385",
                  "text": "That makes sense. But that also does exist on non-apple hardware",
                  "score": 0,
                  "created_utc": "2026-02-25 02:40:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79tu3h",
              "author": "UnluckyPhilosophy185",
              "text": "macOS",
              "score": 1,
              "created_utc": "2026-02-25 04:44:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75fprc",
          "author": "Mundane-Tea-3488",
          "text": "I have been using [edge veda](https://github.com/ramanujammv1988/edge-veda) fluter sdk for running llm on Mac + claude code which can create application instantly",
          "score": 0,
          "created_utc": "2026-02-24 15:29:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8awjx",
      "title": "Best Coding Model?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "author": "I_like_fragrances",
      "created_utc": "2026-02-18 18:23:42",
      "score": 22,
      "num_comments": 23,
      "upvote_ratio": 0.87,
      "text": "What is the best model for general coding. This includes very large models too if applicable.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r8awjx/best_coding_model/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o63oc6q",
          "author": "No_Clock2390",
          "text": "Qwen3-Coder-Next",
          "score": 23,
          "created_utc": "2026-02-18 18:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o644cn8",
              "author": "txgsync",
              "text": "This is the answer. At any affordable home-gamer size (<128GB) Qwen3-Coder-Next (80B-A3B) is the tits.\n\nEdit: and unlike Qwen3-Coder-30B-A3B it doesn’t give me the “fuck off, I just write code, figure it out yourself” energy when planning features. And -Next does better at ZorkBench, not just wandering around with a nasty knife waiting for something to happen until it rage-quits.",
              "score": 5,
              "created_utc": "2026-02-18 19:58:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o677w1y",
                  "author": "Sax0drum",
                  "text": "How much context can you reasonably run with 128GB?",
                  "score": 3,
                  "created_utc": "2026-02-19 06:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o63q8bi",
              "author": "huzbum",
              "text": "Awesome, I need to try this one on some coding tasks, because I can actually run it with a large context and decent TG speed.  (Haven't tested PP)",
              "score": 1,
              "created_utc": "2026-02-18 18:54:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66ntkk",
                  "author": "DifficultyFit1895",
                  "text": "It’s worked well for me. It’s also the only local model I’ve ever tried that 100% answers correctly this one tricky question I ask it about a ~30,000 word novella with the full story in context.",
                  "score": 1,
                  "created_utc": "2026-02-19 04:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64p8ry",
              "author": "jinnyjuice",
              "text": "For what languages are you using it for?\n\nThis is my secondary. GLM 4.7 Flash is better with coding the web UI.",
              "score": 1,
              "created_utc": "2026-02-18 21:36:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64pgm9",
                  "author": "No_Clock2390",
                  "text": "It supports all languages",
                  "score": 1,
                  "created_utc": "2026-02-18 21:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65dej5",
              "author": "National_Cod9546",
              "text": "What IDE / plugins are you using? ",
              "score": 1,
              "created_utc": "2026-02-18 23:35:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o655yof",
          "author": "Faultrycom",
          "text": "People will argue over different models but have in mind that it's about local - and here qwen 3 coder next shines as it can run on not so expensive stack.",
          "score": 4,
          "created_utc": "2026-02-18 22:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63rdtm",
          "author": "huzbum",
          "text": "I haven't used it (beyond confirming I can run it), but I hear great things about MiniMax M2.5.  I use GLM 4.7, and GLM 5 every day, but cloud hosted.  I thought about building something to run GLM 4.5/4.6/4.7, but never got beyond thinking about it... now GLM 5 is twice as big as GLM 4, definitely not running that, but maybe they'll make something intermediate in the 5 family.  \n\nI should try out GLM 4.7 flash for some real tasks, but I haven't gotten around to it yet.  ",
          "score": 3,
          "created_utc": "2026-02-18 18:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64lph1",
          "author": "Grouchy-Bed-7942",
          "text": "If you want something that runs for €6k for agentic code, MiniMax 2.5 with VLLM on 2x ASUS GX10 (equivalent to DGX Spark), you get 2000 tokens/sec of PP without context and you drop to 600/400 tokens/sec of PP at 100k context. Output of about 30 tokens/sec at the start and drops to 15 tokens/sec.\n\nWith Qwen3-Coder-Next, certain tool calls fail after 50k context.",
          "score": 3,
          "created_utc": "2026-02-18 21:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o679pq1",
          "author": "Only_Comfortable_224",
          "text": "Not directly related but for serious coding, is Claude more cost effective?",
          "score": 1,
          "created_utc": "2026-02-19 06:49:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mklci",
              "author": "Ambitious_Injury_783",
              "text": "yes.",
              "score": 1,
              "created_utc": "2026-02-21 16:56:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fll8r",
          "author": "emrbyrktr",
          "text": "I think investing in hardware at this stage is pointless. Everything is developing so fast. In my opinion, Qwen models are the most efficient models locally.",
          "score": 1,
          "created_utc": "2026-02-20 15:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ygaej",
          "author": "Organic-Hall1975",
          "text": "glm 4.7 is solid for general coding imo, handles small scripts and real repos without getting too messy.",
          "score": 1,
          "created_utc": "2026-02-23 14:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64r4bi",
          "author": "Potential-Leg-639",
          "text": "Pick a strong model for architecture/planning and Qwen3 Coder 30B or GLM 4.7 Flash can then do the coding quite good.\n\nMakes quite a difference in coding quality.",
          "score": 1,
          "created_utc": "2026-02-18 21:45:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65bhr7",
              "author": "ijontichy",
              "text": "> Pick a strong model for architecture/planning\n\nCan you provide an example of this?",
              "score": 2,
              "created_utc": "2026-02-18 23:24:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o664ule",
                  "author": "voyager256",
                  "text": "I think he meant non-local / subscription based , big model like Claude 4.6",
                  "score": 3,
                  "created_utc": "2026-02-19 02:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67nzfl",
              "author": "alokin_09",
              "text": "Yep, this is basically my workflow in Kilo Code. I use a premium model like Opus for the architecture stuff, then switch to something like GLM, MiniMax or Qwen for the smaller tasks. Opus creates really detailed plans that are easy to hand off to the cheaper models.",
              "score": 2,
              "created_utc": "2026-02-19 09:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o681d94",
                  "author": "Potential-Leg-639",
                  "text": "I selfhost GLM 4.7 Flash/GPT-OSS-120b/Qwen 3 Coder on a Strix Halo, that do the coding and cant run out tokens. Planning/Architecture  in Opus.\n\nBut i switched completely to OpenCode.",
                  "score": 1,
                  "created_utc": "2026-02-19 11:09:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ppfn",
          "author": "PooMonger20",
          "text": "From my own testing and not benchmarks, on consumer level HW - GPT-OSS-20b is the closest I was able to get to the online equivalents. Everything else is either too slow, generates trash that doesn't even compile, endless syntax errors or straight out misses half the functions you asked for.",
          "score": 0,
          "created_utc": "2026-02-19 09:20:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcjzv0",
      "title": "Does a laptop with 96GB System RAM make sense for LLMs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1rcjzv0/does_a_laptop_with_96gb_system_ram_make_sense_for/",
      "author": "PersonSuitTV",
      "created_utc": "2026-02-23 15:11:16",
      "score": 22,
      "num_comments": 38,
      "upvote_ratio": 0.87,
      "text": "I am in the market for a new ThinkPad, and for $400 i can go from 32GB to 96GB of system RAM. This Laptop would only have the Arc 140T iGPU on the 255H, so it will not be very powerful for LLMs. However, since Intel now allows 87% of system RAM to be allocated to the iGPU, this sounded intriguing. Would this be useable for LLMs or is this just a dumb idea?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1rcjzv0/does_a_laptop_with_96gb_system_ram_make_sense_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6yx5um",
          "author": "Significant_Bar_460",
          "text": "The only x86 laptops with iGPU that are usable for LLM are those with Strix Halo CPUs and then you have Macs with  Pro/Max/Ultra M series chips. \nOr get some super heavy super loud gaming laptop with 5090 or something but at that price range Macs are just more practical machines for everything except gaming.",
          "score": 13,
          "created_utc": "2026-02-23 15:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70iefe",
              "author": "rafaelRiv15",
              "text": "What is the token speed of qwen-coder-next on a mac book pro maxed out ? ",
              "score": 2,
              "created_utc": "2026-02-23 20:22:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7336qt",
                  "author": "Uvalde-Cop",
                  "text": "M1 Max 64Gb, qwen3-coder-next 80B, 4bit: I got 45.13tk/s",
                  "score": 3,
                  "created_utc": "2026-02-24 05:05:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6z9nnw",
          "author": "grassmunkie",
          "text": "It’s helpful, but best if it is paired with a powerful gpu for MOE models. The attention layers go to the GPU, and the experts go to CPU. So having 96gb will be better and give you access to larger models, only question is how fast it is.  \n\nWhen i load 70gb models like qwen coder next using 32gb vram (5090) and the rest offloaded to ram I get around 28-30 tokens per second.\n\nOTOH, if I run a model that fits on my gpu (glm flash 4.7) I get 120 tokens per second.",
          "score": 6,
          "created_utc": "2026-02-23 16:55:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70brrr",
              "author": "Significant_Fig_7581",
              "text": "Is it just me or Qwen Coder Next is just too slow compared to GPT OSS?",
              "score": 1,
              "created_utc": "2026-02-23 19:51:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70zgs0",
          "author": "BubbleProphylaxis",
          "text": "Seriously, today your best bet laptop with \"a lot\" of ram to run AI is a mac M4 Pro or M4 Max. ",
          "score": 6,
          "created_utc": "2026-02-23 21:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yqsni",
          "author": "RepresentativeTill5",
          "text": "I have done this on a framework laptop with a 125h. The fact that its ddr5 is nice, but expect a maximum of like 10 tokens per second on 7b models.\n\nSo technically, it definitely works, just dont expect it to be fast.\n\nSee also this.\nhttps://nikolasent.github.io/hardware/deeplearning/2025/02/09/iGPU-Benchmark-VLM.html",
          "score": 2,
          "created_utc": "2026-02-23 15:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrvdx",
              "author": "PersonSuitTV",
              "text": "Oh lol, hmm. So it will work, but it will be REALLY slow for anything that would utilize that much RAM. bummer. But very good to know. Thank you for your reply.",
              "score": 1,
              "created_utc": "2026-02-23 15:32:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ywd29",
                  "author": "Intraluminal",
                  "text": "For what it's worth, 10 tps is faster than most people can read, so it is comfortable in most cases. Anything less than 7 tps is annoying.",
                  "score": 1,
                  "created_utc": "2026-02-23 15:53:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72de39",
          "author": "terAREya",
          "text": "If it was an M-series Mac the answer is yes. Anything else the answer is no",
          "score": 2,
          "created_utc": "2026-02-24 02:20:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yps4t",
          "author": "catplusplusok",
          "text": "Seems like similar setups at least work better than CPU only [https://www.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my\\_nas\\_runs\\_an\\_80b\\_llm\\_at\\_18\\_toks\\_on\\_its\\_igpu\\_no/](https://www.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/)",
          "score": 1,
          "created_utc": "2026-02-23 15:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yqh2d",
          "author": "dread_stef",
          "text": "No, I've had 96GB of 5600Mhz RAM with my intel 155h cpu (arc igpu) but any model above 14b was too slow to be useful.",
          "score": 1,
          "created_utc": "2026-02-23 15:25:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2ivu",
              "author": "Karyo_Ten",
              "text": "Try gpt-oss-120b (120B-A5B) or GLM-4.7-Flash or Nemotron-3-Nano (both 30B-A3B) or Kimi-Linear (48B-A3B) or Qwen3-Coder-Next (80B-A3B)",
              "score": 4,
              "created_utc": "2026-02-23 16:22:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71xoko",
                  "author": "MrScotchyScotch",
                  "text": "Also Ministral 3 14B, Gemma 3 27B, both are pretty excellent\n\nBut also people shouldn't sleep on the minis. Phi 4 Mini Reasoning is amazing. I use it for web search and it finds things bigger models don't",
                  "score": 1,
                  "created_utc": "2026-02-24 00:49:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72yuip",
                  "author": "Qwen30bEnjoyer",
                  "text": "Push your system to the limit and try Minimax 2.5 IQ3-XXS from Unsloth! It's terribly impractical, but it brings tears of joy to my eyes knowing I've pushed my laptop to its absolute limit, seeing it chug 45 watts for twenty minutes to answer a basic question.",
                  "score": 1,
                  "created_utc": "2026-02-24 04:34:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o734i0l",
                  "author": "dread_stef",
                  "text": "Sure they run, and gpt-oss-20b would actually be useful if I had time to wait a bit. I believe I got about 7-9 tokens per second using llama.cpp but it's been a while so it might not be accurate. And that's just text generation, you'd have to wait a bit during promp processing too. Tried image generation, but you could get some coffee/tea before it was done doing anything with basic models.\n\nI mean it all works, even the big models, but it depends how long you want to wait. I sold the RAM and my other AI pc and got a strix halo breaking even.",
                  "score": 1,
                  "created_utc": "2026-02-24 05:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6yrexw",
              "author": "PersonSuitTV",
              "text": "Oh really? I mean it probably does not mean much, but the 96GB is 6400Mhz but thats not a lot faster. How slow was it running?",
              "score": 0,
              "created_utc": "2026-02-23 15:29:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z7usm",
                  "author": "Far_Cat9782",
                  "text": "Without cuda cores or rock pretty much anything above 20b is gonna be unaorkable",
                  "score": 1,
                  "created_utc": "2026-02-23 16:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70vbcb",
                  "author": "3spky5u-oss",
                  "text": "RAM is still orders of magnitude lower bandwidth than VRAM (ex, DDR5 is up to 80gb/s, a 5090 can do 1792gb/s), and you’re still dealing with PCIe bus latency from the swap. \n\nYour best bet is use MoE with layer offloading.",
                  "score": 1,
                  "created_utc": "2026-02-23 21:26:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yzflk",
          "author": "pmttyji",
          "text": "For tiny/small dense models & small/medium MOE models, it would be fine.\n\nCheck my thread on CPU-only inference(just with 32GB DDR5 RAM)\n\n[CPU-only LLM performance - t/s with llama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nIf that laptop supports AVX512, you could get even better t/s using ik\\_llama.cpp.",
          "score": 1,
          "created_utc": "2026-02-23 16:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za27h",
          "author": "segmond",
          "text": "link to this laptop?  i don't want it for AI, just general computing!  thanks",
          "score": 1,
          "created_utc": "2026-02-23 16:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71q1tc",
          "author": "MrScotchyScotch",
          "text": "There are a million different LLMs of different size and utility so it depends on what you want to do with it and what kind of speed you want.\n\nBut the short answer is yes that machine is perfectly fine for some basic LLMs. It's more powerful than my t14s gen4 that I use for basic local LLMs. 96GB is a pretty decent amount for a laptop. there are probably more powerful machines, but not for that price",
          "score": 1,
          "created_utc": "2026-02-24 00:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o729dpx",
          "author": "lnxgod",
          "text": "No ",
          "score": 1,
          "created_utc": "2026-02-24 01:57:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72c4xv",
          "author": "Rain_Sunny",
          "text": "It's not a dumb idea at all. It’s basically a portable brain with a very small straw. \n\nJust don't expect it to spit out code like a machine gun. \n\nIt’s more like a wise old monk—takes a while to think, but the answer is actually worth waiting for. \n\nBtw,$400 for a 64GB jump is a good choice.\n\nIf you don't mind the speed,then...",
          "score": 1,
          "created_utc": "2026-02-24 02:13:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72yjem",
          "author": "Qwen30bEnjoyer",
          "text": "Yes, it would be. \n\nMy personal experience with the Radeon 780m iGPU with 96gb RAM on the Framework 16, I got 0.5 TPS PP in and ~3 TPS out with IQ3-XXS Minimax 2.5 - a Sonnet 4.5 equivalent model. \n\nA more usable experience for chat would be Qwen-Coder-Next. I don't remember the precise details, but it works decently, about ~200 tps PP in and ~10 TPS out.  - GPT OSS 120b is roughly the same speed.\n\nThe fastest models on this setup worth using are Qwen3vl 30b a3b and GPT OSS 20b which both get around ~30 TPS TG out, I dont remember the PP speed off the top of my head but its somewhere in the ~200 TPS range. \n\nI don't know about intel specifically, your performance could vary due to drivers, but it has 11 TFLOPs FP16 vs. the 17 TFLOPS FP16 on my Radeon 780m. \n\nThat being said, I believe it does support native FP4 which should be great for running LLMs at NVFP4 quantization if its optimized for it! Not sure of Llama.cpp vulkan NVFP4 inference takes advantage of it, but its something to consider! Maybe you go full circle and have Qwen-coder-next help you optimize llama.cpp vulkan to your system with OpenCode :)",
          "score": 1,
          "created_utc": "2026-02-24 04:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73i9or",
          "author": "themightymike786",
          "text": "I would rather get a Mac for solid performance.",
          "score": 1,
          "created_utc": "2026-02-24 07:08:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v4js",
          "author": "Tomorrow_Previous",
          "text": "I have a laptop with 64GB of RAM, and use a full sized 3090 eGPU. Honestly, I think that if you're willing to spend that much for 96GB RAM it would make sense to look for a better alternative, but if you're limited to that, you could have a working machine anywhere, and a better machine when docked with the eGPU.",
          "score": 1,
          "created_utc": "2026-02-24 09:09:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z03qh",
          "author": "Herr_Drosselmeyer",
          "text": "If you're going to run models that take full advantage of the RAM, they'll be very slow. Even if you use MoE models with low parameters for inference, the prompt processing is still going to be terrible. So it really depends on your definition of 'usable'. ",
          "score": 1,
          "created_utc": "2026-02-23 16:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70v1li",
              "author": "3spky5u-oss",
              "text": "Not true at all. \n\nWith MoE offloading I can run GPT-OSS-120B at 35 tok/s with a 5090, and that’s 20b active experts. \n\nWith models like Qwen3 30b A3b, your experts are tiny at 3b, they absolutely fly.",
              "score": 1,
              "created_utc": "2026-02-23 21:25:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o712mxz",
                  "author": "Herr_Drosselmeyer",
                  "text": "As I said, the prompt processing is the issue. Compare a 32k prompt to a 4k prompt and you'll see what I mean. Now imagine that on an iGPU.",
                  "score": 1,
                  "created_utc": "2026-02-23 22:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74esem",
          "author": "Efficient_Loss_9928",
          "text": "Only Strix Halo chips or Apple M-series work well.\n\nAll others doesn't make sense.",
          "score": 0,
          "created_utc": "2026-02-24 12:02:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raao5m",
      "title": "Google officially launches the Agent Development Kit (ADK) as open source",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/utzloi85eqkg1.jpeg",
      "author": "Fun-Necessary1572",
      "created_utc": "2026-02-20 23:09:23",
      "score": 21,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1raao5m/google_officially_launches_the_agent_development/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6nyjyk",
          "author": "_klikbait",
          "text": "OH MY GOD",
          "score": 1,
          "created_utc": "2026-02-21 21:10:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9uttx",
      "title": "Running local LLMs on my art archive, paranoid or actually unsafe?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r9uttx/running_local_llms_on_my_art_archive_paranoid_or/",
      "author": "LifeguardAny1801",
      "created_utc": "2026-02-20 13:04:59",
      "score": 19,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "I'm a professional illustrator and I've basically de-googled my archive: no Drive, no Dropbox, no cloud backup. Everything's on local storage because the idea of my style getting scraped into some training set makes me sick.\n\nNow I'm tempted by \"local AI\" stuff: NAS with on-device tagging, local LLMs, etc. In theory it's perfect: smart search but everything stays at home.\n\nFor people here who run local models on private data (art, notes, docs):\n\n* What's your threat model? Is \"no network / no cloud at all\" the only truly safe option?\n* How do you make sure nothing leaks? (open-source only, firewalls, VLANs, traffic sniffing?\n\nCurious how you all balance privacy / not feeding big models vs having modern search + tagging on your own hardware.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9uttx/running_local_llms_on_my_art_archive_paranoid_or/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6fmmq1",
          "author": "_raydeStar",
          "text": "If you're running a local server, just don't expose it to the outside world.  It's actually difficult to expose a local server to the internet, my ISP refuses to do it and I have to do workarounds. \n\nIf you think you might be the target of a personal attack (someone logging into your network and stealing your stuff) then be a bit more aggressive with security.  \n\nFollow general guidelines -- encrypted files, don't expose it, etc, and you will be ok.  If you are \\*really really\\* worried about security, take a basic udemy course on it, it'll walk you through the ins and outs in ways that I have no way of doing over Reddit comments.",
          "score": 5,
          "created_utc": "2026-02-20 15:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6havpj",
          "author": "Cronus_k98",
          "text": "Your bigger problem is that you don’t have a proper backup. RAID is not a backup and if you’re counting on your NAS to never loose your data, you’re going to loose your data.\n\nThere are private cloud storage providers out there. Keep your NAS for local access and periodically back it up to secure, encrypted storage and it’ll never get scraped for LLM use. \n",
          "score": 1,
          "created_utc": "2026-02-20 19:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oxqwq",
              "author": "RG_Fusion",
              "text": "If their goal is to protect their personal data, uploading to a cloud storage provider isn't really an option.\n\n\nThey just need to back-up their media on another storage medium, either HDD or Blu-ray, and then store those media drives in a separate physical location.",
              "score": 1,
              "created_utc": "2026-02-22 00:31:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fsm1w",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -6,
          "created_utc": "2026-02-20 15:40:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h5oy1",
              "author": "hhioh",
              "text": "Sounds like you place a huge weight on internet archiving as discerning some kind of meaning \n\nOkay lmao \n\nThis isn’t more meaningful than fizzling out back into the universe, my friend\n\nNo need to be so weird about it ❤️",
              "score": 1,
              "created_utc": "2026-02-20 19:25:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9pve1",
      "title": "Recommendations for agentic coding with 32GB VRAM",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "author": "pioni",
      "created_utc": "2026-02-20 08:27:15",
      "score": 19,
      "num_comments": 25,
      "upvote_ratio": 1.0,
      "text": "My current project is almost entirely in node.js and typescript, but every model I'm tried with LM Studio that fits into VRAM with 128k context seems to have problems with getting stuck in a loop. No amount of md files and mandatory instructions has been able to resolve this, it still does it with Roo Code and VSCode. \n\nAny ideas what I should try? Good examples of md files I could try to avoid this, or better LM Studio models with the hardware limitations I have? I have recently used Qwen3-Coder-Next-UD-TQ1\\_0 and zai-org/glm-4.7-flash and both have similar problems. Sometimes it works for good 15 minutes, sometimes it gets into a loop after first try. \n\nI don't know if it matters, but the dev environment is Debian 13. Using Windows was a complete nightmare because of commands it did not have and file edits that did not work.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9pve1/recommendations_for_agentic_coding_with_32gb_vram/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6eb9on",
          "author": "GuyFromPoland",
          "text": "These questions keep showing up - is there any website where you provide your hardware details and it shows you best models?",
          "score": 3,
          "created_utc": "2026-02-20 10:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpn1j",
              "author": "reditzer",
              "text": "The issue is that \"best\" is subjective.",
              "score": 2,
              "created_utc": "2026-02-20 15:26:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e4c0j",
          "author": "FullstackSensei",
          "text": "How much RAM do you have? Running Q1 on any model severely lobotomizes performance. For coding tasks o smaller models, I find you can't get acceptable performance under Q8, and no KV cache quantization.",
          "score": 3,
          "created_utc": "2026-02-20 08:57:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7r7b",
              "author": "HumanDrone8721",
              "text": "I have 48GB (2x24) VRAM and 128GB RAM. Current using Qwen3-Coder-Next the highest quality 8bit quanta possible. Yes, the speed dropped significantly to something like 24tok/s but I had the latest OpenSSL source code and a regulatory document to follow and adjust the code for it. In German, bureaucratic German even. It worked perfectly (max context -c 0 is enabled), code itself, test cases AND documentation. Less than an hour, no human intervention.\n\nThe biggest problem was not the generated code quality or time, it was the latest llama.cpp crashing and burning with grammar parser errors, THAT was difficult to repair, but once repaired and with the latest GGUFs it works like a charm.\n\nMy point: tried with all other quants and it was a waste of data download, ALL of them failed more or less miserably !!! Also GLM4.7-Flash was a disappointment.\n\nTL; DR: Bottom line is: ALWAYS run a model to the highest possible resolution and context that you can run, discard the speed and take as much quality as possible, small quantas are fast, but really shit for anything besides benchmarks, simple tasks and wankerei: \"I've run this 485B model on my potato at 75t/s...\" Sadly this enforces and propagates the myth that smaller models are crap and not worth using because ADHD zoomers have no patience and take whatever shite as long as is fast.",
              "score": 6,
              "created_utc": "2026-02-20 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fue9m",
                  "author": "fish_of_pixels",
                  "text": "This is so interesting to me. What agentic tools are you running? I'm using opencode and glm 4.7 flash has been amazing while qwen3 coder next has struggled to get through most tasks. Both Q8.",
                  "score": 2,
                  "created_utc": "2026-02-20 15:48:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6edx4c",
                  "author": "Betatester87",
                  "text": "I have the same setup. Would you kindly post your llama.cpp config that you feel is best. Thanks",
                  "score": 1,
                  "created_utc": "2026-02-20 10:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hxyiu",
                  "author": "Soft_Syllabub_3772",
                  "text": "I had the grammar issue, i disabled streaming in roocode",
                  "score": 1,
                  "created_utc": "2026-02-20 21:43:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6e5ii7",
              "author": "pioni",
              "text": "I have 32GB of RAM, the same as VRAM. Can't upgrade for hardware reasons and monetary reasons. Any recommendations for a model that I could run a less quantized version of with my current setup? I think I might be able to squeeze the context window smaller if I have to, because it has to run in GPU, otherwise it will stall to few tokens per second.",
              "score": 2,
              "created_utc": "2026-02-20 09:08:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6e728c",
                  "author": "FullstackSensei",
                  "text": "Devstral 2 24B",
                  "score": 2,
                  "created_utc": "2026-02-20 09:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e2u64",
          "author": "IsSeMi",
          "text": "I tried them both. In my case it depends on the tool I use those models in. Try to use Claude code. I haven't noticed it stucks in a loop. Also Unsloth docs has usage guides for models, e.g. [GLM-4.7-Flash](https://unsloth.ai/docs/models/glm-4.7-flash#usage-guide). They provide different params for tool calling. ",
          "score": 1,
          "created_utc": "2026-02-20 08:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnp8d",
          "author": "former_farmer",
          "text": "I had better outcomes with opencode cli directly, ollama, and 30b models at full size. Slow as fuck but worked. Same pc as yours.",
          "score": 1,
          "created_utc": "2026-02-20 15:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iqz6a",
          "author": "Xantrk",
          "text": "I think LM studios llama.cpp is the reason for GLM performing so badly. I had endless looping issues, as soon as I switched to llama.cpp, they disappeared.",
          "score": 1,
          "created_utc": "2026-02-21 00:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v8j6",
          "author": "pioni",
          "text": "Now I have been running qwen3-coder-30b with context size of 110k for a while and get quite good results on a 5090. The speed is somewhere around 100 tokens a second which is passable, but I can't use larger context without going to RAM which makes this about 100x slower. \n\nOut of curiosity, does any of you have a Mac Studio M3 Ultra to try how fast it runs this same model with 110k context, and with the maximum it has (256k)?",
          "score": 1,
          "created_utc": "2026-02-24 09:10:18",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6f1pbk",
          "author": "HenkPoley",
          "text": "Qwen 3 Coder 30B, at maybe 4 bit, don’t run 1 bit quantization.\n\nBtw, the KV cache will mostly eat your RAM.",
          "score": 1,
          "created_utc": "2026-02-20 13:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e8d63",
          "author": "Antique_Dot_5513",
          "text": "Qwen code 30b sur opencode. T’attend pas à des miracles par contre.",
          "score": 0,
          "created_utc": "2026-02-20 09:35:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9boaw",
      "title": "Long conversation prompt got exposed",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/vsgr1nirihkg1.jpeg",
      "author": "Ramenko1",
      "created_utc": "2026-02-19 21:16:10",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1r9boaw/long_conversation_prompt_got_exposed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6g3a4x",
          "author": "Decent_Solution5000",
          "text": "Yeah, not surprised. Bet it gets reminders a lot from writers editing manuscripts. lol This one sounds better than ones I read about awhile ago. Wasn't using Claude then, Kind of glad I missed that.",
          "score": 1,
          "created_utc": "2026-02-20 16:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6njw18",
          "author": "iamsimonsta",
          "text": "That font plays havoc with kimi k2 ocr, interesting.",
          "score": 1,
          "created_utc": "2026-02-21 19:52:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xk6ao",
          "author": "Ally_M101",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-02-23 10:51:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra2atm",
      "title": "Is anyone else pining for Gemma 4?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "author": "Formula71",
      "created_utc": "2026-02-20 17:51:43",
      "score": 18,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "About this time last year, I was impressed with Gemma 3, but besides the GPT-OSS models, it seems like the US based labs have been pretty quite on the open source front, and even GPT-OSS even feels like a while ago now.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ra2atm/is_anyone_else_pining_for_gemma_4/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o6hjxt1",
          "author": "journalofassociation",
          "text": "I am very much looking forward to Gemma 4.  It's been almost a year since Gemma 3 was released.  \n\nNothing I can do but whine and speculate, though, except to play with all the other great OSS models we've gotten.",
          "score": 5,
          "created_utc": "2026-02-20 20:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hwtax",
          "author": "Klutzy_Ad_1157",
          "text": "Same, it understands German so well and I use it almost for a year now. I hope so much for Gemma 4!",
          "score": 2,
          "created_utc": "2026-02-20 21:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gvfwo",
          "author": "Savantskie1",
          "text": "GPT-OSS models were released in August 5, 2025. Literally months ago. Models no matter the size take time and hardware to train. It's not like they have a ton of extra compute they're not using just laying around.",
          "score": 6,
          "created_utc": "2026-02-20 18:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hg84m",
              "author": "UndecidedLee",
              "text": ">It's not like they have a ton of extra compute they're not using just laying around.\n\n\n\nThey do. That's why they're trying to shoehorn LLMs in everything.\n\nYou browse the internet often? - Here's an AI that will summarize the page for you.  \nYou have lots of media on your computer? - How about letting AI sort it for you?",
              "score": 4,
              "created_utc": "2026-02-20 20:16:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hj9zr",
                  "author": "Savantskie1",
                  "text": "Yeah that’s for the normies and it’s utilizing their current models wich they run at high concurrency. Why waste money on hardware not being utilized. Yeah you know nothing about business",
                  "score": -3,
                  "created_utc": "2026-02-20 20:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}