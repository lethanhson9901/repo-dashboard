{
  "metadata": {
    "last_updated": "2026-01-17 16:48:01",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 144,
    "file_size_bytes": 175628
  },
  "items": [
    {
      "id": "1qf5l2n",
      "title": "Local AI Final Boss ‚Äî M3 Ultra v.s. GB10",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ysf36b97qudg1.jpeg",
      "author": "Imaginary_Ask8207",
      "created_utc": "2026-01-17 06:13:15",
      "score": 106,
      "num_comments": 28,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qf5l2n/local_ai_final_boss_m3_ultra_vs_gb10/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0314fh",
          "author": "adspendagency",
          "text": "sheeesh. seeing as we install about 5 of these per day into business to do private on-prem infra I should probably make myself more aware of what the GB10 performs like compared to the M3. we‚Äôve just been shipping M3s to all of our customers. How does the GB10 hold up ?",
          "score": 14,
          "created_utc": "2026-01-17 10:24:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03fyx1",
              "author": "CustomerNo30",
              "text": "Finger in air.. What sort of money are we talking about here for a M3 solution. If you can't say then that's fair enough. I've been trying to push for a dedicated server but resistance is high from the holders of budgets.",
              "score": 3,
              "created_utc": "2026-01-17 12:34:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o03h4r6",
              "author": "Imaginary_Ask8207",
              "text": "I am primarily testing with VLMs serving with vLLM. So far the latency is not that bad with models quantised in NVFP4, but it's acceptable only for medium sized models such as Qwen3-VL-30b-a3b and gemma3-27b (also GLM-4.6V with 106B 12B MoE, but crashes quite often).  \nI've also noticed that NVFP4 isn't very stable on GB10 when serving with vLLM, it crashes randomly. I haven't dived into the issue yet tho.",
              "score": 3,
              "created_utc": "2026-01-17 12:42:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03tleb",
                  "author": "divyanshkul",
                  "text": "How are you quantising the models? Running your own pipeline over the hugging face model? And then serving it via endpoint?",
                  "score": 1,
                  "created_utc": "2026-01-17 14:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03hdkg",
              "author": "Imaginary_Ask8207",
              "text": "5 M3 ultras per day is crazy! Mind sharing which models are you guys deploying?",
              "score": 1,
              "created_utc": "2026-01-17 12:44:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02e00r",
          "author": "No_Conversation9561",
          "text": "Try clustering them together using EXO. They actually posted about this exact same setup.\nThey said it speeds up prompt processing.\n\n\nhttps://exolabs.net\n\nIssues:  https://github.com/exo-explore/exo/issues/1102",
          "score": 25,
          "created_utc": "2026-01-17 06:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03foo0",
              "author": "Imaginary_Ask8207",
              "text": "Yess! I also saw this blog by EXO: https://blog.exolabs.net/nvidia-dgx-spark/. \n\nNot sure if it's stable tho, will give it a try when I got time :)",
              "score": 3,
              "created_utc": "2026-01-17 12:31:58",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o03bpto",
              "author": "cipioxx",
              "text": "Exo isnt working with linux.",
              "score": 0,
              "created_utc": "2026-01-17 11:59:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03jbcu",
          "author": "Ok_Remove3449",
          "text": "PLEASE! LLM inference tps. Are they as bad as I've heard?",
          "score": 4,
          "created_utc": "2026-01-17 12:58:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03zx33",
              "author": "Imaginary_Ask8207",
              "text": "I think it depends on which model and what quantisation strategy. Any particular model in your mind?",
              "score": 2,
              "created_utc": "2026-01-17 14:35:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04kvj4",
                  "author": "Mean-Sprinkles3157",
                  "text": "Qwen3-Next-80B, gpt-oss-120b, can you try these two if  possible?",
                  "score": 1,
                  "created_utc": "2026-01-17 16:19:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02m63w",
          "author": "belgradGoat",
          "text": "I‚Äôd see if Mac Studio is actually stable running 500 gb ram models. I tried with 256 version and it gets messy towards the limit",
          "score": 4,
          "created_utc": "2026-01-17 08:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03f71d",
              "author": "Imaginary_Ask8207",
              "text": "Any model in your mind? 4-bit quantised DeepSeekR1 should only be using \\~300-400GB.",
              "score": 2,
              "created_utc": "2026-01-17 12:28:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03fwie",
                  "author": "belgradGoat",
                  "text": "Maybe something like this? I wouldn‚Äôt even try anything else then mlx mlx-community/DeepSeek-V3-0324-5bit",
                  "score": 4,
                  "created_utc": "2026-01-17 12:33:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o03vnlp",
              "author": "GermanK20",
              "text": "obviously you changed the default RAM limit from 2/3 to something like 9/10, didn't you? But even then, you gotta keep the context small.",
              "score": 2,
              "created_utc": "2026-01-17 14:12:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o045giq",
                  "author": "belgradGoat",
                  "text": "That was not the point. I have 256 gb ram, used model was like 190, context is not 50gb. I use lm studio.\n\nIssue is not that it doesn‚Äôt work, it works, model performs ok. But after a while of holding this model in memory system becomes unstable and started glitching hard, eventually crashing the system",
                  "score": 1,
                  "created_utc": "2026-01-17 15:05:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o02qmku",
              "author": "No_Conversation9561",
              "text": "Same reason I got two 256GB ones instead of one 512GB.",
              "score": 1,
              "created_utc": "2026-01-17 08:45:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03veu5",
          "author": "StrangeMuon",
          "text": "I‚Äôd be interested to see some side by side comparisons between the GX10 and the M3 Ultra for inference with gpt-oss-120b & Stable Diffusion and also (more interestingly for me) a run of this benchmark on both:\n\nhttps://github.com/TristanBilot/mlx-benchmark",
          "score": 2,
          "created_utc": "2026-01-17 14:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03zcq0",
              "author": "Imaginary_Ask8207",
              "text": "I could try on M3 Ultra\\~ but seems the repo does not support GB10 yet.",
              "score": 1,
              "created_utc": "2026-01-17 14:32:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02lczd",
          "author": "SoaokingGross",
          "text": "I'd be asking it to do something to fix our political situation somehow.  It's fine for us to have fun with these things, I bought an M3 for this reason but I feel pretty strongly that more of my time should be spent doing something instead of tuning out.",
          "score": 6,
          "created_utc": "2026-01-17 07:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02se48",
              "author": "thecurrykid",
              "text": "Amen",
              "score": -2,
              "created_utc": "2026-01-17 09:01:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03mazk",
          "author": "CharlesCowan",
          "text": "very cool",
          "score": 1,
          "created_utc": "2026-01-17 13:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04dvx0",
          "author": "HealthyCommunicat",
          "text": "I‚Äôve done an extreme deep dive into this and the m3 ultra will outbeat the spark for pure text inference no matter what, especially as context grows, PP will still go down but the TG is just that much greater that it mathematically never reaches a point where the output is produced faster - with agentic coding cycles MLX allows cache reuse making PP near same as the GB10 anyways. I posted stats of a test done going up to 100k context and the spark just cannot, will not be able to do those speeds EVEN with 2x sparks.\n\nThat being said, exo‚Äôing these two seems like its going to be a new niche class of being able to combine the two to do exactly that and have each other make up for its own weaknesses, amazing.",
          "score": 1,
          "created_utc": "2026-01-17 15:46:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdiwdh",
      "title": "Oh Dear",
      "subreddit": "LocalLLM",
      "url": "https://i.imgur.com/3UY6yoB.png",
      "author": "bamburger",
      "created_utc": "2026-01-15 13:01:56",
      "score": 63,
      "num_comments": 28,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qdiwdh/oh_dear/",
      "domain": "i.imgur.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzpzy10",
          "author": "mp3m4k3r",
          "text": "Might want to check the tuning parameters like temperature match with what the model is recommended to use.",
          "score": 21,
          "created_utc": "2026-01-15 13:10:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw4ljm",
              "author": "mxforest",
              "text": "This should be standard with the model weights. Why second guess. Why not have a config file with all \"best\" settings preapplied?",
              "score": 4,
              "created_utc": "2026-01-16 09:27:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq9664",
          "author": "iamzooook",
          "text": "while the system prompt. \"only reply with continuous\"the\"\"",
          "score": 26,
          "created_utc": "2026-01-15 14:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq1b70",
          "author": "ScoreUnique",
          "text": "I suggest trying pocket pal, allows loading gguf files",
          "score": 5,
          "created_utc": "2026-01-15 13:18:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq67p0",
          "author": "l_Mr_Vader_l",
          "text": "check if the model needs a system prompt?\n\nsome models just don't work without a system prompt",
          "score": 5,
          "created_utc": "2026-01-15 13:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvhfaz",
              "author": "Much-Researcher6135",
              "text": "Is there any big spreadsheet of models' recommended settings somewhere?",
              "score": 1,
              "created_utc": "2026-01-16 06:05:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvhukc",
                  "author": "l_Mr_Vader_l",
                  "text": "I wouldn't think so. mostly you get the info for a given model from huggingface usage snippets and config files in the repo",
                  "score": 2,
                  "created_utc": "2026-01-16 06:08:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqsanu",
          "author": "HealthyCommunicat",
          "text": "Noone‚Äôs recommending the most obvious solution that you should be trying - raise your repeat penalty. Start at 1.1 and go higher. Make sure your model isnt forced to use more experts than regularly recommended. \n\nThese two are usually the top actual real most common reasons as to why local llm‚Äôs do this",
          "score": 4,
          "created_utc": "2026-01-15 15:36:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv4pzz",
              "author": "pieonmyjesutildomine",
              "text": "Was writing this comment, excellent job knowing your knowledge",
              "score": 2,
              "created_utc": "2026-01-16 04:36:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00zfk5",
              "author": "Confident-Ad-3212",
              "text": "Repeat penalty is not going to solve this. Zero chance. It comes from other issues",
              "score": 1,
              "created_utc": "2026-01-17 00:58:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o00zwbp",
                  "author": "HealthyCommunicat",
                  "text": "id appreciate it u told me what u think it might be so that its something i can at least keep in mind next time this happens to me.",
                  "score": 1,
                  "created_utc": "2026-01-17 01:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt3737",
          "author": "rinaldo23",
          "text": "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the 2",
          "score": 2,
          "created_utc": "2026-01-15 21:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt7hih",
              "author": "hugazebra",
              "text": "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the 3/912",
              "score": 2,
              "created_utc": "2026-01-15 22:12:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq2toq",
          "author": "Feztopia",
          "text": "Moreli",
          "score": 2,
          "created_utc": "2026-01-15 13:27:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq4rdl",
              "author": "I1lII1l",
              "text": "Oh dear",
              "score": 1,
              "created_utc": "2026-01-15 13:37:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzs4xkr",
                  "author": "FaceDeer",
                  "text": "1.",
                  "score": 1,
                  "created_utc": "2026-01-15 19:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqfwsl",
          "author": "low_v2r",
          "text": "alias ollama=\"yes the\"",
          "score": 1,
          "created_utc": "2026-01-15 14:36:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr1ld0",
          "author": "Sicarius_The_First",
          "text": "Ah, the classic \"didn't read the instructions, no idea why it won't work\"",
          "score": 1,
          "created_utc": "2026-01-15 16:18:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrepid",
              "author": "Witty_Mycologist_995",
              "text": "the the the the the the the previous the the",
              "score": 1,
              "created_utc": "2026-01-15 17:17:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztqzj9",
          "author": "BowtiedAutist",
          "text": "Stuttering sally",
          "score": 1,
          "created_utc": "2026-01-15 23:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuc9ih",
          "author": "Fit-Medicine-4583",
          "text": "The same thing happened to me on ollama. The issue was fixed by increasing the context length between 16k to 32k.",
          "score": 1,
          "created_utc": "2026-01-16 01:50:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyvpyq",
          "author": "C3H8_Tank",
          "text": "The lack of matrix jokes is sad.",
          "score": 1,
          "created_utc": "2026-01-16 18:40:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzr814",
          "author": "misha1350",
          "text": "Why don't you simply use Qwen3-1.7B in Q4",
          "score": 1,
          "created_utc": "2026-01-16 21:06:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00z04t",
          "author": "Confident-Ad-3212",
          "text": "You are having an issue with the stop in your template. Also, seems like you may have had duplicates or some other issue during training. Maybe too many samples or your LR, rank and or alpha was too high. I just went through this. I have just gotten through this exact issue. Try lowering LR and deduping first. If you ran too many epochs this can happen as well‚Ä¶. Try reducing epochs and LR after making sure you have removed all duplicates. Try with an LR of 1.5e-5 and then walk it up. Rank 32, alpha 64. If it continues, you don‚Äôt  have enough sample diversity. Try these first then walk up your learning strength until you find the expression you are looking for.  Chances are you have too many samples. That 1.5b can intake 500-2000 samples max. Anything less than 25 separate sample types can cause this‚Ä¶. Diversity is key.",
          "score": 1,
          "created_utc": "2026-01-17 00:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqav6q",
          "author": "Serious_Molasses313",
          "text": "Stop gooning to small models. That's SA",
          "score": -10,
          "created_utc": "2026-01-15 14:10:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdjstl",
      "title": "Google Drops MedGemma-1.5-4B: Compact Multimodal Medical Beast for Text, Images, 3D Volumes & Pathology (Now on HF)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qdjstl/google_drops_medgemma154b_compact_multimodal/",
      "author": "techlatest_net",
      "created_utc": "2026-01-15 13:41:03",
      "score": 43,
      "num_comments": 3,
      "upvote_ratio": 0.99,
      "text": "Google Research just leveled up their¬†[Health AI Developer Foundations](https://developers.google.com/health-ai-developer-foundations)¬†with¬†**MedGemma-1.5-4B-IT**¬†‚Äì a 4B param multimodal model built on Gemma, open for devs to fine-tune into clinical tools. Handles¬†**text, 2D images, 3D CT/MRI volumes, and whole-slide pathology**¬†straight out of the box. No more toy models; this eats real clinical data.\n\nKey upgrades from MedGemma-1 (27B was text-heavy; this is compact + vision-first):\n\n# Imaging Benchmarks \n\n* **CT disease findings**: 58% ‚Üí 61% acc\n* **MRI disease findings**: 51% ‚Üí 65% acc\n* **Histopathology (ROUGE-L on slides)**: 0.02 ‚Üí 0.49 (matches PolyPath SOTA)\n* **Chest ImaGenome (X-ray localization)**: IoU 3% ‚Üí 38%\n* **MS-CXR-T (longitudinal CXR)**: macro-acc 61% ‚Üí 66%\n* Avg single-image (CXR/derm/path/ophtho): 59% ‚Üí 62%\n\nNow supports¬†**DICOM natively**¬†on GCP ‚Äì ditch custom preprocessors for hospital PACS integration. Processes 3D vols as slice sets w/ NL prompts, pathology via patches.\n\n# Text + Docs \n\n* **MedQA (MCQ)**: 64% ‚Üí 69%\n* **EHRQA**: 68% ‚Üí 90%\n* **Lab report extraction**¬†(type/value/unit F1): 60% ‚Üí 78%\n\nPerfect backbone for RAG over notes, chart summarization, or guideline QA. 4B keeps inference cheap.\n\nBonus:¬†**MedASR**¬†(Conformer ASR) drops WER on medical dictation:\n\n* Chest X-ray: 12.5% ‚Üí 5.2% (vs Whisper-large-v3)\n* Broad medical: 28.2% ‚Üí 5.2% (**82% error reduction**)\n\nGrab it on¬†[HF](https://huggingface.co/google/medgemma-1.5-4b-it)¬†or Vertex AI. Fine-tune for your workflow ‚Äì not a diagnostic tool, but a solid base.\n\nWhat are you building with this? Local fine-tunes for derm/path? EHR agents? Drop your setups below.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qdjstl/google_drops_medgemma154b_compact_multimodal/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzv3kcr",
          "author": "toomanypubes",
          "text": "Holy shit, this thing works great.  On my Mac I setup a python MLX script to process @300 DICOM image slices (MRI) converted to JPEGs. Single threaded‚Ä¶this model chewed through the whole stack in 18 minutes.  Got a clinical summary for each image, and helped us identify a partial ligament tear - without waiting 3 days to see the doctor.  What a crazy time to be alive.\n\nThank you Google MedGemma team!",
          "score": 9,
          "created_utc": "2026-01-16 04:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuy6xg",
          "author": "astrae_research",
          "text": "As someone in medical research field, this sounda very interesting. I'm confused that nobody is commenting - - is this trivial or not useful? Genuinely curious",
          "score": 3,
          "created_utc": "2026-01-16 03:54:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qem1po",
      "title": "We fine-tuned an email classification model so you can auto-label your emails locally with n8n.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/tfwi6yjzvqdg1.jpeg",
      "author": "party-horse",
      "created_utc": "2026-01-16 17:18:54",
      "score": 42,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qem1po/we_finetuned_an_email_classification_model_so_you/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzz5ybz",
          "author": "Outside-Balance7754",
          "text": "Cool! Does the spam category also include phishing emails? If not, I‚Äôd say missing a phishing tag is definitely a drawback.",
          "score": 2,
          "created_utc": "2026-01-16 19:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6hko",
              "author": "party-horse",
              "text": "It‚Äôs included in spam for now but you can easily train your own model with any categories you want!",
              "score": 1,
              "created_utc": "2026-01-16 19:29:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzymory",
          "author": "nunodonato",
          "text": "I love everything you guys are doing!",
          "score": 1,
          "created_utc": "2026-01-16 18:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6j3y",
              "author": "party-horse",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-16 19:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzyrd1p",
          "author": "CalmlyObservant",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-16 18:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzvoo5",
          "author": "HealthyCommunicat",
          "text": "Woah, this is automation i‚Äôd actually use, thank you for your efforts to create a tool anyone can use.",
          "score": 1,
          "created_utc": "2026-01-16 21:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02cvf7",
              "author": "party-horse",
              "text": "Amazing! Thanks for",
              "score": 1,
              "created_utc": "2026-01-17 06:40:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzyh702",
          "author": "astral_crow",
          "text": "I‚Äôve been waiting for n8n to be talked about more consider how much stuff I‚Äôve seen about comfy. But what sparked the fact I am seeing n8n everywhere again so sudden?",
          "score": 1,
          "created_utc": "2026-01-16 17:36:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz6b0i",
              "author": "party-horse",
              "text": "I think it makes it very easy to build integrations on top of llms. That‚Äôs how I have been using it",
              "score": 2,
              "created_utc": "2026-01-16 19:28:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qcu498",
      "title": "Small AI computer runs 120B models locally: Any use cases beyond portability and privacy?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/",
      "author": "0xShreyas",
      "created_utc": "2026-01-14 17:51:55",
      "score": 41,
      "num_comments": 32,
      "upvote_ratio": 0.95,
      "text": "Saw Mashable interviewed TiinyAI at CES. It is a pocket-sized device with 80GB RAM that runs 120B models locally at 30W. When you compare it to DGX Spark, the Spark has 128GB RAM and much more speed. But the Tiiny is about a third of the price and way smaller. Anyway Im curios what are the actual benefits of having such a small device? Also what specific tasks would actually need the portability over the higher performance of a DIY/bigger unit?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzkw7sp",
          "author": "uti24",
          "text": "I mean, what about memory bandwidth?\n\nWe still dont know it, I think, optimistic scenario it having 200Gb/s, but I afraid it only having 80Gb/s or so, at that point it's not better than a regular PC/laptop.",
          "score": 17,
          "created_utc": "2026-01-14 18:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzprp2p",
              "author": "ecoleee",
              "text": "You're absolutely right; memory bandwidth is crucial for the efficiency of running large model inference. \n\nTiiny has two chips: 96GB/s on the SoC (ARM v9.2 CPU) and 144GB/s on the NPU. We have our proprietary inference acceleration technology, **PowerInfer**, where **hot-activation parameters are calculated on the NPU and cold-activation parameters are calculated on the SoC**. This allows for inference acceleration even with a heterogeneous computing hardware architecture. You can see an open-source example of **PowerInfer on GitHub (which has 8.6K stars)**.",
              "score": 3,
              "created_utc": "2026-01-15 12:14:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpsq3b",
                  "author": "uti24",
                  "text": "Interesting insight, so is it like 4 channel 4500MT memory or like 2 channel 9000MT?",
                  "score": 1,
                  "created_utc": "2026-01-15 12:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzku4po",
          "author": "FullstackSensei",
          "text": "Not that I think $1400 is a good price for an 80GB RAM little SBC, but I'll believe the price part when it's actually available to buy for immediate shipping.",
          "score": 15,
          "created_utc": "2026-01-14 17:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzps4pz",
              "author": "ecoleee",
              "text": "That makes sense. It has nothing to do with whether memory is expensive right now‚Äî$1400 is a price that makes users reasonably question whether it can even be released to the market; it's not cheap. Tiiny has already entered mass production and is currently undergoing FCC certifications, etc. Your oversight is very welcome.\n\nhttps://preview.redd.it/880jgaef9idg1.png?width=1279&format=png&auto=webp&s=bbe1002ee3fd587307ffed5cfbeeab302d970bbb",
              "score": 2,
              "created_utc": "2026-01-15 12:17:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl3cnt",
          "author": "Your_Friendly_Nerd",
          "text": "Maybe smth like a mobile AI companion for when you go hiking and there's no internet? idk",
          "score": 6,
          "created_utc": "2026-01-14 18:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp4cjf",
              "author": "FaceDeer",
              "text": "A while back I saw a startup on /r/preppers that was selling a \"portable Internet\", a small self-contained computer whose purpose was to provide a wifi hotspot with a bunch of built-in websites and documents that could provide all manner of useful information for emergency situations. First aid, car repair, etc.\n\nHaving a reasonably smart LLM that's able to do RAG on all that information and help you use it would be a great enhancement to that. Usually if you're in an emergency situation you don't have time to browse through PDFs to find exactly the bit of information you need right now.",
              "score": 2,
              "created_utc": "2026-01-15 08:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp4dk4",
                  "author": "sneakpeekbot",
                  "text": "Here's a sneak peek of /r/preppers using the [top posts](https://np.reddit.com/r/preppers/top/?sort=top&t=year) of the year!\n\n\\#1: [The bottle of 91% alcohol that I keep under my driver's seat may have just saved my life in a way you wouldnt expect.](https://np.reddit.com/r/preppers/comments/1k2nelq/the_bottle_of_91_alcohol_that_i_keep_under_my/)  \n\\#2: [Incredibly Proud Prepper Moment!](https://np.reddit.com/r/preppers/comments/1kfr5m5/incredibly_proud_prepper_moment/)  \n\\#3: [Lessons:  Got caught in the bomb threat and cyber attack at Dublin airport.](https://np.reddit.com/r/preppers/comments/1nmpqv5/lessons_got_caught_in_the_bomb_threat_and_cyber/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",
                  "score": 1,
                  "created_utc": "2026-01-15 08:46:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzqp0ts",
                  "author": "sn2006gy",
                  "text": "You can simply keep about 3-4 ebooks with search on your mobile device and have all the information you need for survival - no need for an LLM that would hallucinate - especially under pressure where you won't be providing the prompt/context it needs whereas a book is simply more directional and applicable in nature.",
                  "score": 1,
                  "created_utc": "2026-01-15 15:21:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzlza42",
              "author": "j00cifer",
              "text": "This is actually a legit use case. It runs on any usbc power bank",
              "score": 2,
              "created_utc": "2026-01-14 21:03:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpskz7",
              "author": "ecoleee",
              "text": "You‚Äôre actually very close üôÇ ‚Äî that‚Äôs one use case, but not the main one.\n\nThink of Tiiny less as a ‚Äúchatbot in the wild‚Äù and more as a portable AI compute module.\n\n ‚Ä¢ It‚Äôs for people who want serious models (20B‚Äì120B)\n\n ‚Ä¢ running fully offline,\n\n ‚Ä¢ with predictable performance and zero token costs,\n\n ‚Ä¢ and no need to buy a new GPU rig or replace their laptop.\n\nThe ‚Äúold computer‚Äù demo wasn‚Äôt about hiking ‚Äî it was to show that compute is decoupled from your PC. Plug Tiiny into any machine and it suddenly becomes a capable local-AI workstation.\n\nOffline matters not just in nature, but in:\n\n ‚Ä¢ secure work (law, research, enterprise)\n\n ‚Ä¢ development & coding without API bills\n\n ‚Ä¢ regions with unstable or expensive internet\n\n ‚Ä¢ long agent workflows where token cost explodes\n\nSo yes, it can be an offline companion on a hike ‚Äî\n\nbut its real value is: owning your AI, anywhere, without renting intelligence from the cloud.",
              "score": 1,
              "created_utc": "2026-01-15 12:21:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzq7jsl",
                  "author": "Your_Friendly_Nerd",
                  "text": ">but its real value is: owning your AI, anywhere, without renting intelligence from the cloud.\n\nI get the same by running a rig at home and using VPN to access it. The main selling point is the combination of offline+mobile.\n\nWith good internet, there's no need for an offline device, and if I only really need to use it at my desk, there's no need for a portable device.",
                  "score": 0,
                  "created_utc": "2026-01-15 13:52:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzmby48",
          "author": "GoodSamaritan333",
          "text": "Resilience for when a dictator government or the CIA shutdowns the internet on my country.",
          "score": 7,
          "created_utc": "2026-01-14 22:00:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoi108",
              "author": "duplicati83",
              "text": "This, and I also don‚Äôt want my personal information being used to train the models owned by the evil tech bros. Because let‚Äôs face it, they‚Äôll just use it against us somehow to hoard even more money.",
              "score": 2,
              "created_utc": "2026-01-15 05:31:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nznkdll",
          "author": "Candid_Dependent4615",
          "text": "I saw the product specs and there's a microphone and a built-in speaker. Maybe you can turn this into a private assistant? Instead of using Alexa or Siri, you could load your own automation scripts and let it manage your schedule and tasks locally. Having a personal assistant remember all your private data and previous conversations without any data leaking to the cloud is pretty useful imo.",
          "score": 3,
          "created_utc": "2026-01-15 01:56:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqpcxp",
              "author": "sn2006gy",
              "text": "The HomeAssistant voice controller does this and it can run on a Pi and it only costs 59 bucks for the voice device.  Obviously you need a Pi or computer to run HomeAssistant - certainly not a 1400 dollar one.",
              "score": 1,
              "created_utc": "2026-01-15 15:22:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzsb7jf",
              "author": "conquest333",
              "text": "Nice idea. Also you can feed it with all the private & important documents and have it act as a local knowledge base.",
              "score": 1,
              "created_utc": "2026-01-15 19:42:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzkxqch",
          "author": "Clipbeam",
          "text": "I guess you could take it with you on-the-go? In that sense the size would make sense? But I agree a larger home server would do fine for most use cases",
          "score": 2,
          "created_utc": "2026-01-14 18:14:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcnb4",
          "author": "GoodSamaritan333",
          "text": "And the vision models will allow autonomous drones, jamming proof and without requiring long optic fibers attached to then.",
          "score": 1,
          "created_utc": "2026-01-14 22:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzna9y9",
          "author": "IveGotThatBigRearEnd",
          "text": "DGX Spark has 119GiB ram, for those who might otherwise have assumed GiB",
          "score": 1,
          "created_utc": "2026-01-15 00:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznax03",
          "author": "gordonmcdowell",
          "text": "This is just a permutation on privacy, but you can solve problems with an unlocked LLM that the online models will not answer.",
          "score": 1,
          "created_utc": "2026-01-15 01:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzosb5a",
          "author": "CMPUTX486",
          "text": "I think if you buy Spark, you get cheaper CUDA support.  Otherwise, you need to use a model with cuda.",
          "score": 1,
          "created_utc": "2026-01-15 06:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq1ufe",
          "author": "eazolan",
          "text": "At some point the cloud AI companies are going to seriously jack up their prices.",
          "score": 1,
          "created_utc": "2026-01-15 13:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr6ope",
          "author": "yourloverboy66",
          "text": "Perfect for field geology. In remote areas without internet access, an offline device like this for immediate data analysis would be incredibly useful.",
          "score": 1,
          "created_utc": "2026-01-15 16:41:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrxjki",
          "author": "bloomjt",
          "text": "Think about government work or legal firms‚Äîany profession where data privacy is non-negotiable. This is a solid solution for them",
          "score": 1,
          "created_utc": "2026-01-15 18:41:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvte26",
          "author": "techlatest_net",
          "text": "Silent always-on inference server for home/office‚Äîplug any laptop/phone via USB/network, zero setup. Field devs (execs, consultants) run private 120B agents offline in meetings/hotels where big rigs fail. Low 30W = solar/battery power for remote sites, disaster zones, or edge deployments.",
          "score": 1,
          "created_utc": "2026-01-16 07:45:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcag6",
          "author": "GoodSamaritan333",
          "text": "Autonomous parroting moving robots from Boston Dinamics",
          "score": 0,
          "created_utc": "2026-01-14 22:01:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9xml5",
      "title": "Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026",
      "subreddit": "LocalLLM",
      "url": "https://www.techpowerup.com/345000/gigabyte-announces-support-for-256gb-of-ddr5-7200-cqdimms-at-ces-2026",
      "author": "GoodSamaritan333",
      "created_utc": "2026-01-11 11:59:30",
      "score": 35,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q9xml5/gigabyte_announces_support_for_256gb_of_ddr57200/",
      "domain": "techpowerup.com",
      "is_self": false,
      "comments": [
        {
          "id": "nyz30tq",
          "author": "silenceimpaired",
          "text": "Finally! What really held me back from buying this much ram was the motherboard and not the price. With the ‚Äúslight‚Äù increase to ram prices I can sell my car to buy this ram and I will have no more barriers‚Ä¶ provided they can ship me the ram and motherboard as I won‚Äôt have transportation anymore.",
          "score": 19,
          "created_utc": "2026-01-11 14:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz5s7s",
          "author": "Lux_Interior9",
          "text": "K",
          "score": 7,
          "created_utc": "2026-01-11 14:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza6efr",
          "author": "Sovairon",
          "text": "At this amount ram should come ECC by default to be honest..",
          "score": 2,
          "created_utc": "2026-01-13 02:39:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzytg0",
          "author": "IngwiePhoenix",
          "text": "Talk about a \"halo product\"... x)",
          "score": 1,
          "created_utc": "2026-01-11 16:56:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcuyh2",
      "title": "What is the biggest local LLM that can fit in 16GB VRAM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/",
      "author": "yeahlloow",
      "created_utc": "2026-01-14 18:21:44",
      "score": 34,
      "num_comments": 68,
      "upvote_ratio": 0.87,
      "text": "I have a build with an RTX 5080 and 64GB of RAM. What is the biggest LLM that can fit in it ? I heard that I can run most LLMs that are 30B or less, but is 30B the maximum, or can I go a bit bigger with some quantization ?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzlo7nw",
          "author": "BigYoSpeck",
          "text": "What's an acceptable speed for you?\n\n\nI have a Ryzen 9 5900x, 64gb DDR4 3800, and a 16gb Radeon RX 6800 XT\n\n\nI can run gpt-oss-20b at 120+ tok/s\n\n\nQwen3 30b partially offloaded to CPU at about 40 tok/s\n\n\nAnd gpt-oss-120b with 32 MOE layers offloaded to CPU at 23 tok/s\n\n\nI imagine your system would be faster still",
          "score": 14,
          "created_utc": "2026-01-14 20:12:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpebld",
              "author": "wisepal_app",
              "text": "i have a laptop hp zbook i7-12800h, 64 gb ddr5 4800 ram and 16 gb rtx a4500 gpu. i don't get your tok/s values. what do you use for that? i mainly use lm studio. what is your context window size and other settings? And which quants do you use?",
              "score": 4,
              "created_utc": "2026-01-15 10:23:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzs6g8r",
                  "author": "BigYoSpeck",
                  "text": "LM Studio does all or nothing with the MOE offloading, it doesn't allow the fine grained n layers setting you get in llama.cpp\n\nI haven't done much more than quick testing with gpt-oss-20b and Qwen3 30b to know the exact limits of context, but gpt-oss-20b fits entirely in VRAM, no special settings really \n\nThe trick with MOE models like Qwen3 30b and gpt-oss-120b is the `--n-cpu-moe N` parameter. I have it at 32 for gpt-oss-120b and run about 120k context. Can't remember the exact number or layers for Qwen3, you basically juggle between the context you want to fit in vram and the amount of layers for performance\n\nJust the mxfp4 GGUF's for gpt-oss, and I think Unsloth Q6\\_K\\_XL for Qwen3. Qwen3 Next doesn't perform far off either (I think around 35 tok/s) and is way more efficient for context",
                  "score": 2,
                  "created_utc": "2026-01-15 19:21:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o01pj92",
              "author": "CaterpillarOne6711",
              "text": "can I ask you where you run gptoss 20b?",
              "score": 1,
              "created_utc": "2026-01-17 03:44:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03ftar",
                  "author": "BigYoSpeck",
                  "text": "I just run it on a self compiled ROCm build of llama.cpp. Vulkan get's more tok/s for generations (about 150) but prompt processing is massively lower at like 1600 vs 2600 in ROCm",
                  "score": 1,
                  "created_utc": "2026-01-17 12:32:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o035ewt",
              "author": "deulamco",
              "text": "Do you think I can archive the same with Xeon E5-2696 v3 + 64GB DDR3 ECC + 5060TI-16G ?",
              "score": 1,
              "created_utc": "2026-01-17 11:04:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03g5rt",
                  "author": "BigYoSpeck",
                  "text": "Small models that fit entirely or mostly in VRAM probably faster, large models like gpt-oss-120b I couldn't say. I'm guessing the quad channel DDR3 is close to DDR4 bandwidth, but the Xeon is considerably slower than a Zen 3 Ryzen",
                  "score": 1,
                  "created_utc": "2026-01-17 12:35:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzncbxz",
          "author": "vertical_computer",
          "text": "## Everyone is missing the forest for the trees.\n\nu/yeahlloow \\- forget the number of params. **Look at the file size of the model.**\n\nYour GPU has 16GB of VRAM. That literally tells you the maximum size you can run - 16GB (plus leave some room for context, so realistically 14GB)\n\nThen, you go to HuggingFace and look at whatever model you want, and find someone who has produced a bunch of quantisations for it.\n\n**It literally tells you the file size for the model. That‚Äôs the answer.**\n\nIf it‚Äôs larger than about 14GB, it will spill over into RAM and run super slowly. If it‚Äôs larger than about 70GB *(14GB from GPU + 56GB RAM, leaving 8GB for the OS)* then you won‚Äôt be able to load it at all.\n\nHere‚Äôs a random example: [Nvidia Llama 3.3 Nemotron 49B](https://huggingface.co/bartowski/nvidia_Llama-3_3-Nemotron-Super-49B-v1_5-GGUF)\n\nhttps://preview.redd.it/kb3ibnylxedg1.png?width=1242&format=png&auto=webp&s=083f6fcf4c951c1a814cdaf8fd9fa712cec1c21b\n\nIf you run the IQ2\\_XXS version, it will fit entirely within your VRAM.\n\nAny larger and it will spill over into RAM. So you could absolutely choose the Q8 version @ 53GB, it will just be hella slow.",
          "score": 4,
          "created_utc": "2026-01-15 01:10:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp2slv",
              "author": "Bhilthotl",
              "text": "Also, depending on how you run it, via Ollama, llama.cpp, opencode, tiny llm, etc you might need to tweak things. Ive got a 16gb 5070 and found that I got best results with llama.cpp over Ollama due to spills over into onboard RAM",
              "score": 2,
              "created_utc": "2026-01-15 08:31:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpnagc",
              "author": "mintybadgerme",
              "text": "This might help? https://github.com/nigelp/ai-model-tracker",
              "score": 0,
              "created_utc": "2026-01-15 11:41:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlfulm",
          "author": "Fabulous_Fact_606",
          "text": "Just got into local LLM last week. Haven't had time to test all the different LLM. I've tried Jamba2 and Ministral\n\nMy setup is almost the same as yours : AMD 9900x RTX5080 32GB RAM running Windows 11 + WSL.\n\nHere's my docker setup:\n\nhttps://preview.redd.it/pguuuztk8ddg1.png?width=724&format=png&auto=webp&s=c2d32a37167151f7ec5acece247285fb6d762511\n\nCagra for RAG, Kokoro TTS , Parakett-sst, llava for vision, ministral (local llm) all in its own fastapi tunneled via wireguard to my VPS server. Can access my local llm on any device.\n\nAll of these were able to fit into ram and vram.\n\nResponse is pretty good. I have a front end that tunes the output. Gave it internet search / crawl /optical / speech to tex / text to speech. latency is not as good as the foundational models..but for a local llm it is still impressive.\n\n**Question:**¬†\"Who wrote the Declaration of Independence?\"\n\n|Metric|Value|\n|:-|:-|\n|**Tier Used**|Tier 0 (LLM + RAG Validation)|\n|**RAG Facts Found**|0|\n|**Validation**|LLM ONLY (stable fact)|\n|**Confidence**|90%|\n|**Total Time**|**10.3 seconds**|",
          "score": 3,
          "created_utc": "2026-01-14 19:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlglrg",
              "author": "Fabulous_Fact_606",
              "text": "# Test 2: NFL Prediction\n\n**Question:**¬†\"Make a prediction who will win the NFL matchups this weekend\"\n\n|Metric|Value|\n|:-|:-|\n|**Tier Used**|Tier 3 (Web Search + Prediction)|\n|**Web Sources**|4 (FOX Sports, ESPN, [NFL.com](http://NFL.com), CBS Sports)|\n|**Facts Gathered**|16 facts|\n|**Search Confidence**|95%|\n|**Total Time**|**1m 45s**|\n\n**Predictions:**\n\n|Game|Predicted Winner|Probability|Predicted Score|\n|:-|:-|:-|:-|\n|49ers vs Seahawks|**Seahawks**|65%|Seahawks 24, 49ers 21|\n|Rams vs Bears|**Rams**|60%|Rams 27, Bears 20|\n|Bills vs Broncos|**Broncos**|62%|Broncos 28, Bills 21|\n|Texans vs Patriots|**Patriots**|75%|Patriots 31, Texans 17|\n\n**Key Factors Used:**\n\n* Regular-season records (15-3 Patriots favored)\n* Historical playoff performance\n* Defensive strength analysis\n* Home/away considerations\n\n# Latency Comparison\n\n|Query Type|Tier|Time|\n|:-|:-|:-|\n|Stable historical fact|Tier 0|\\~10s|\n|NFL Prediction (web search)|Tier 3|\\~105s|",
              "score": 1,
              "created_utc": "2026-01-14 19:38:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlmkw9",
                  "author": "VenomFN",
                  "text": "Were there any tutorials you followed? Looking to set something similar up for myself",
                  "score": 1,
                  "created_utc": "2026-01-14 20:05:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl4acp",
          "author": "SKirby00",
          "text": "You probably won't be able to fit 30B models unless you're willing to crank the quant down below 4-bit, and accept potentially significant quality degradation. Remember, you also need to leave room for at least a bit of context, so a 14.5GB model might *technically* fit but is unlikely to be very useful.\n\nI have 24GB of VRAM (3060 Ti 8GB + 5060Ti 16GB) and I've found that on Qwen3-Coder-30B, I cap out around 18K context which feels like just barely enough to be useful. That seems to be the sweet spot right now for me, though the context is shorter than I'd like.\n\nWith 16GB of VRAM, my guess is that you could likely just barely run something like GPT-OSS-20B, with smaller models allowing for more context. The sweet spot for you will likely be closer to the 14B range, depending on what you're trying to use it for.",
          "score": 6,
          "created_utc": "2026-01-14 18:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzm6b2p",
              "author": "grocery_head_77",
              "text": "dumb question - how are you connecting your two GPUs? Are you using software like lossless scaling or ???\n\nasking as I have a 3070 Ti (my old gpu) and currently have a 5090 FE, but would love to use both like you!",
              "score": 5,
              "created_utc": "2026-01-14 21:35:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznvynb",
                  "author": "WishfulAgenda",
                  "text": "Check if your motherboard support 8x8 pice bifurcation. Stick the second card in, turn the machine on and good to go as long as your software supports it. I run two 5070ti with lm studio and works great.",
                  "score": 3,
                  "created_utc": "2026-01-15 03:04:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nznzffm",
                  "author": "Candid_Highlight_116",
                  "text": "just shove it into the second slot and LM Studio can use it \n\nnote that they can't parallelize the workload, so the entire process is as fast as the slowest card, and there will be overheads, but a lot of people find that acceptable. also this pipeline parallel mode don't get bottlenecked much by pcie so x1 mining adapters work too, allowing mining rigs to be repurposed",
                  "score": 3,
                  "created_utc": "2026-01-15 03:25:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzorgmt",
                  "author": "SKirby00",
                  "text": "I literally just plug them both into the computer at the same time, in separate PCIe slots. LM Studio literally just picks up the two GPUs and if your drivers are up to date, you're good to go. Just use a frontier model to help you figure it out if you run into problems.\n\nIt's not like using GPUs for gaming back in the day where you had to set up SLI and basically needed matching GPUs. Any two reasonably modern cards from the same brand (both Nvidia or both AMD) will be fine together.",
                  "score": 2,
                  "created_utc": "2026-01-15 06:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl0uq3",
          "author": "sam7oon",
          "text": "i would suggest for you to read more into it, there is a lot of types of LLMs, if you need it to invoke tools, code, write emails and so on,\n\nyou are most probably are gonna max at 14B models, however if you are planning to run a long context, you may need to downsize to 8B, \n\nBut again depending on what you are gonna use it for, its different, it would be nice if you can specify your target,",
          "score": 6,
          "created_utc": "2026-01-14 18:27:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzljv0p",
              "author": "mckirkus",
              "text": "Totally depends on how it's quantized.  80 billion parameters at Q4 is smaller than 80 billion at Q16",
              "score": 2,
              "created_utc": "2026-01-14 19:53:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm22vm",
                  "author": "sam7oon",
                  "text": "Yes i see more informed answers after mine, am afraid to touch quantized models , do you think a 80B Model quantized would produce better result than a non quant 8B models ? \n\nAssuming we are talking about generating email as an example.",
                  "score": 1,
                  "created_utc": "2026-01-14 21:16:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl3l8a",
          "author": "PermanentLiminality",
          "text": "If you are doing anything serious, you need space for context.  You can shoehorn a model that fills your VRAM.  It will work great for a simple one sentence question.  you will not be able to feed it a lot of data though.  Try and keep the model under 80% of your VRAM. so say 13Gb in size.  Not a hard rule, but it is a good starting place.  If you want to feed a big input, you may need something smaller.  \n\nYou can spill over into system RAM, but your speed will drop big time.  The Qwen 3 30B A3B actaully does OK even if it does spill over a bit.   It may be one of the largest models you can run effectively.",
          "score": 2,
          "created_utc": "2026-01-14 18:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw00oh",
          "author": "cibernox",
          "text": "The best you can fit in vram alone, probably gpt-oss 20B. If you use system ram, y comes down to the speed you want. But you can run 80B MoE models at reasonable speed",
          "score": 2,
          "created_utc": "2026-01-16 08:45:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzllh0o",
          "author": "karmakaze1",
          "text": "I'm running [qwen3-30b-a3b with IQ4_XS quant](https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF?show_file_info=Qwen_Qwen3-30B-A3B-IQ4_XS.gguf) which doesn't quite fit. I let some of the layers spill over to CPU but it still gets a decent number of tokens/sec.",
          "score": 1,
          "created_utc": "2026-01-14 20:00:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzncn80",
              "author": "vertical_computer",
              "text": "When it‚Äôs so close to fitting, why not drop to something like Q3_K_M and let it entirely fit on your GPU?",
              "score": 3,
              "created_utc": "2026-01-15 01:12:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzo7j3b",
                  "author": "karmakaze1",
                  "text": "Great advice. By default I've been using q4 quants. The other way to go is to use a dense model with fewer parameters which can be a bit slower with higher accuracy.",
                  "score": 1,
                  "created_utc": "2026-01-15 04:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlp63d",
          "author": "PM_ME_COOL_SCIENCE",
          "text": "I‚Äôm using a 5060 ti with 16gb vram, and with llama.cpp server I‚Äôm getting gpt oss 20b mxfp4 with 120k context. I can share the exact command, but nothing too crazy. Fully on gpu, 120 tk/s generation. If you‚Äôre willing to go slower, you can fit qwen 3 next or gpt oss 120b with MOE expert offloading. Those are the biggest yet somewhat performant models for mixed cpu/gpu.\n\nWhat‚Äôs your use case? How much context?",
          "score": 1,
          "created_utc": "2026-01-14 20:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlq28j",
              "author": "ristlincin",
              "text": "Jesus 120k? What's the model's size?",
              "score": 1,
              "created_utc": "2026-01-14 20:21:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlrv5j",
                  "author": "PM_ME_COOL_SCIENCE",
                  "text": "20b, 3.5b active. Gpt oss 20b",
                  "score": 1,
                  "created_utc": "2026-01-14 20:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzm7j0i",
          "author": "GutenRa",
          "text": "I discovered the qwen3-next 80b q4, which was recently updated by unsloth. \n\nA significant improvement in quality compared to the 30b and lower models.\n\n10t/s at 5900x & 5060ti16gb vram & 64gb ram.",
          "score": 1,
          "created_utc": "2026-01-14 21:40:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmd790",
          "author": "metamec",
          "text": "Qwen3-Next-80B-A3B (Thinking or Instruct) by Unsloth. Use the Q4 K M quant.\n\nI have an RTX 5080 too with 128 GB RAM, and that model only uses around 40 GB of RAM when configured optimally. Unfortunately I'm a long way from home until next week and unable to access my exact settings, but you basically want to offload 99 layers to the GPU and then force 36 MoE layers to the CPU.",
          "score": 1,
          "created_utc": "2026-01-14 22:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmh1dj",
          "author": "National_Cod9546",
          "text": "You can fit a 24b model at q4s and 16k context. More then that and it will become painfully slow as it spills into ram. Generally you want to use the biggest model you can at q4. At lower quants, the model becomes too stupid. Very large 70b+ can handle going to q3 or even q2 before the stops gets to bad. But you are usually better using a smaller model. With less than 16k context, it gets hard to give the model enough information to do anything useful.¬†",
          "score": 1,
          "created_utc": "2026-01-14 22:24:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmnqy0",
          "author": "Loud_Communication68",
          "text": "Check out the phi series",
          "score": 1,
          "created_utc": "2026-01-14 22:57:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq6ixt",
          "author": "ProfessionalYear5755",
          "text": "I have just built my rig B550, 5600X, 9060XT 16GB, (only 16GB 3200mhz at the moment) BUT what surprised me was when a model spills over in RAM it does NOT crawl. It is slower yes but is totally useable. I believed from insane amounts of research that RAM = crawl, It demonstrably does not. Happy days!",
          "score": 1,
          "created_utc": "2026-01-15 13:47:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvtjvy",
          "author": "techlatest_net",
          "text": "24B Q4\\_K\\_M fits clean on 16GB (RTX 5080), \\~140t/s with your 64GB RAM soaking KV cache. Push 30B Q3 or 32B Q2\\_K if you squash context to 2k, but quality dips. 30B isn't max‚Äîquantization gets you there, offload handles overflow.‚Äã",
          "score": 1,
          "created_utc": "2026-01-16 07:46:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwensw",
          "author": "mistrjirka",
          "text": "I would say one of the best ones is ministral 3 14B. Depends on what you want.",
          "score": 1,
          "created_utc": "2026-01-16 10:58:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyiopq",
          "author": "TheRiddler79",
          "text": "You could download gptoss120b offload most of it to ram and you'd still get good speed and a very intelligent ai. You should try",
          "score": 1,
          "created_utc": "2026-01-16 17:43:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00w93n",
          "author": "beryugyo619",
          "text": "Theoretically 8B fp16, 16B q8, 32B q4, 64B q2, 128B q1, so on. See the pattern?  \n\nAnd there comes overheads and context caches",
          "score": 1,
          "created_utc": "2026-01-17 00:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03a4k2",
          "author": "Mad_Bark00",
          "text": "Give me a good llm for 8gb ram",
          "score": 1,
          "created_utc": "2026-01-17 11:46:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlenzq",
          "author": "ZoSoPa",
          "text": "http://www.canirunthisllm.net/",
          "score": 1,
          "created_utc": "2026-01-14 19:29:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzm2mm7",
              "author": "Dan_Wood_",
              "text": "SSL has expired and it‚Äôs a coming soon page hosted by PythonAnywhere ‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-14 21:18:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm4q8q",
                  "author": "Caltaire",
                  "text": "Don‚Äôt know how accurate this is but saw it posted elsewhere in response to the previous link.\n\nhttps://apxml.com/tools/vram-calculator",
                  "score": 1,
                  "created_utc": "2026-01-14 21:27:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlf0hm",
          "author": "Turbulent_Dot3764",
          "text": "16vram, I5 10¬∞, 32gb ram\n\nGPS oss 20b 48k context, full gpu, ollama.\n\nRunning this plus my docker and others apps for development.\n\n\n24B models fit wit less context.\n\n\nBigger than 24B, only with smaller quantization",
          "score": 1,
          "created_utc": "2026-01-14 19:31:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoia8v",
              "author": "SexMedGPT",
              "text": "Which quant do you run?",
              "score": 1,
              "created_utc": "2026-01-15 05:33:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04ctzj",
                  "author": "Turbulent_Dot3764",
                  "text": "I pull from ollama, it's use the open a√≠ gpt. MXFP4",
                  "score": 1,
                  "created_utc": "2026-01-17 15:41:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlmmoj",
          "author": "forthejungle",
          "text": "Only VRAM matters. RAM is irrelevant, especially when it comes to speed.",
          "score": 1,
          "created_utc": "2026-01-14 20:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzleapx",
          "author": "HealthyCommunicat",
          "text": "Why not just copy paste this exact question into gemini and get a really specific exact answer that is near instant, actually informational, and can give you even further specific info related to your setup? Do you actually care to learn?",
          "score": -5,
          "created_utc": "2026-01-14 19:27:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlk574",
              "author": "true-though",
              "text": "change your username.",
              "score": 9,
              "created_utc": "2026-01-14 19:54:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm6ssx",
                  "author": "HealthyCommunicat",
                  "text": "nawh. the basis of healthy communication is being upfront and blunt about things that would actually in long run benefit the other person. you don't think it would be good for OP to know to ask gemini? they want to get into llm's but dont realize that all the answers and all the information in the world is more at your fingertips than ever? go read all my comments, they're all extremely blunt - yet not one single person is able to refute what i say because every single person knows in the back of their head what im saying is right. tell me, doesn't it take much more time and effort and energy to make a post on reddit and wait for comments and reply and read them when you can do that all in one interaction with gemini?",
                  "score": 2,
                  "created_utc": "2026-01-14 21:37:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzm35ov",
                  "author": "Condomphobic",
                  "text": "He has a genuine point that I actually wonder about daily. \n\nA LLM like Gemini can give a **much** better and nuanced answer than humans can.",
                  "score": 2,
                  "created_utc": "2026-01-14 21:20:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qery3t",
      "title": "Training ideas with 900Gb of vram",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qery3t/training_ideas_with_900gb_of_vram/",
      "author": "soppapoju",
      "created_utc": "2026-01-16 20:56:10",
      "score": 32,
      "num_comments": 21,
      "upvote_ratio": 0.92,
      "text": "Hello, i have an opportunity to train something and use a \"supercomputer\".\n\nWhat would you do with this amount of vram available? About 10x H100\n\nThinking of training something and bringing it to personal use or to be used publicly on a website.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qery3t/training_ideas_with_900gb_of_vram/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o006wdx",
          "author": "FamiliarRice",
          "text": "I have an open source research company that uses around this much compute. Let me know if you‚Äôre interested in collaborating and we will get you on some top ml conference papers !",
          "score": 24,
          "created_utc": "2026-01-16 22:21:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01x6cd",
              "author": "NoobMLDude",
              "text": "I would be interested in collaborating. \nWhat topics , domains , problems do you work on?\nCan I Dm you?",
              "score": 1,
              "created_utc": "2026-01-17 04:37:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o009vgj",
          "author": "RoyalCities",
          "text": "Download everything you can find from Annas archive and make your own Chatgpt.",
          "score": 11,
          "created_utc": "2026-01-16 22:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00unc0",
              "author": "No-Acanthaceae-5979",
              "text": "This",
              "score": 3,
              "created_utc": "2026-01-17 00:30:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzy2ga",
          "author": "WolfeheartGames",
          "text": "If you don't already have an important research idea to follow, distilling a larger model down to a non quadratic architecture is probably the highest impact you can have. \n\nIf you have an experiment to run, make sure it's solid first.\n\nIf you will allow me a moment to self promote, I have this https://github.com/bigwolfeman/TitanMAC-Standalone an implementation from a series of Google papers. I haven't worked out all the kinks and performance elements yet, but you can take the neural memory module and use it on any other architecture as a replacement for quadratic attention. The kernels are there for forward and back passes. I'd prefer if you contributed to this project though =) you'd be working through issues either way if you took the neural memory just as a factor of writing your own architecture. \n\nThere is a potential issue with the sliding window required for the neural memory, you may have chunk boundary issues. From my testing I have this resolved, but I haven't gone past 50k steps on it yet to make sure it doesn't crop up again later.\n\nI also submitted this to the hackathon for this subreddit https://github.com/bigwolfeman/Retnet-Distillation \n\nI do not recommend using retnet for this. It is extremely unstable and slow. I had to patch torchscale just to make it work at all and kept finding issues in torchscale. But there is a nice reference implementation of keeping the teacher in vram beside the student, caching it's logits to disk, or inferencing from a vllm server and exposing top k logits.\n\nEven with how unstable retention is, it may be worth implementing to resolve chunk boundary problems. I am currently not maintaining retention in TitanMAC, but if further testing shows issues I'm planning to use it.\n\nAlso, my repo has the nested learning module based on SGD. It is a memory efficient optimizer. Allowing for more parameters with less vram. \n\nIf you're interested in using this repo, reach out to me. I have a branch I am doing a lot of development work on. I have found that feeding in hyper connections directly to the nested optimizer gives it richer signals for convergence and is basically free to do. It may be possible to average gradient heuristics to 16x16 tiles using my development branch, but I haven't had time to test this yet. It is the last step of the dynamic sparse implementation I'm working on in that branch. \n\n---\n\nOther great use for so much vram: distill a VLM to V-JEPA.\nDistill to HOPE https://github.com/Sk16er/hope_nano HOPE is a vram monster, I don't think this architecture is ready to scale yet, but it's research value is tremendous.\n\nEdit: the custom kernels are targeting blackwell, but they're written in Triton. Getting them ready for an h100 shouldn't be too bad.\n\nIf this deployment you want to do is far enough out in the future, I am planning on implementing the entire architecture in mojo for further performance gains. I'm still ablating several takes on the sparsity before I do this. The current code isn't designed to shard across gpus, though pytorch makes it fairly easy. Mojo is very good at sharing, though I haven't tested that myself.",
          "score": 15,
          "created_utc": "2026-01-16 21:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o000pn1",
          "author": "amooz",
          "text": "Where‚Äôs the dude who‚Äôs training a model using only text from the 1800‚Äôs London?",
          "score": 7,
          "created_utc": "2026-01-16 21:50:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzzwf8",
          "author": "K33P4D",
          "text": "this is just too much  \nI'm here whipping 12B and 30B models, waiting 30 mins for an answer and this dude here with 900GB vRAM smh",
          "score": 6,
          "created_utc": "2026-01-16 21:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0065y8",
              "author": "much_longer_username",
              "text": "It sounds like they're just a guest on this system, probably through work or school. \n\nBut my dad always said \"it's only a lot of money if you don't have it\".",
              "score": 8,
              "created_utc": "2026-01-16 22:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o009vrn",
                  "author": "Large-Excitement777",
                  "text": "Still trying to wrap my head around how gullible everyone is",
                  "score": -3,
                  "created_utc": "2026-01-16 22:36:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o004vsj",
              "author": "Osprey6767",
              "text": "yeah lol same",
              "score": 1,
              "created_utc": "2026-01-16 22:11:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzxnub",
          "author": "Aromatic-Low-4578",
          "text": "I'd pretrain an LLM",
          "score": 2,
          "created_utc": "2026-01-16 21:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzzptm",
          "author": "Sicarius_The_First",
          "text": "If I'd write all I would do with it, it would be several pages long.",
          "score": 2,
          "created_utc": "2026-01-16 21:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzxqua",
          "author": "Hoak-em",
          "text": "\"planning\"-optimized glm4.7 would be interesting, though I'm not sure how much work that would take.",
          "score": 1,
          "created_utc": "2026-01-16 21:36:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o033n3l",
          "author": "soppapoju",
          "text": "900gb is an estimate. It might be more might be less, when i get to know the exact specs of the machine.\n\nAs person new to LLM training, is why I ask.\n\nGot some ideas with gpt but human answers always better!\n\nI can que to use it, but it needs to be something worthwhile to me or to a lot of people.\n\nBest Regards all!",
          "score": 1,
          "created_utc": "2026-01-17 10:48:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o03b3e0",
          "author": "alexbarbershop",
          "text": "I would set it loose on a very high quality curated historical dataset or historical imagery datasets as that is a very weak spot in all AI models currently.  Coincidentally, I do have over 1000 negative scans of public domain trolley images from the NEHR trolley museum collection on DigitalMaine and a massive spreadsheet of metadata I compiled manually.",
          "score": 1,
          "created_utc": "2026-01-17 11:54:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03p8fb",
          "author": "cjstoddard",
          "text": "Depends, do you want to get rich or do you want to do legitimate research? If you want to get rich, I'd say you can't go wrong with porn, if you want to do legitimate research, I got nothing.",
          "score": 1,
          "created_utc": "2026-01-17 13:36:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004io8",
          "author": "UseMoreBandwith",
          "text": "I would start with   \nwriting better reddit posts",
          "score": -2,
          "created_utc": "2026-01-16 22:09:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcpoaw",
      "title": "Google just opensourced Universal Commerce Protocol.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcpoaw/google_just_opensourced_universal_commerce/",
      "author": "techlatest_net",
      "created_utc": "2026-01-14 15:08:25",
      "score": 26,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "**Google just dropped the Universal Commerce Protocol (UCP) ‚Äì fully open-sourced! AI agents can now autonomously discover products, fill carts, and complete purchases.** \n\nGoogle is opening up e-commerce to AI agents like never before. The¬†**Universal Commerce Protocol (UCP)**¬†enables agents to browse catalogs, add items to carts, handle payments, and complete checkouts end-to-end‚Äîwithout human intervention.\n\n# Key Integrations (perfect for agent builders):\n\n* **Agent2Agent (A2A)**: Seamless agent-to-agent communication for multi-step workflows.\n* **Agents Payment Protocol (AP2)**: Secure, autonomous payments.\n* **MCP (Model Context Protocol)**: Ties into your existing LLM serving stacks (vLLM/Ollama vibes).\n\nLink: [https://github.com/Universal-Commerce-Protocol/ucp](https://github.com/Universal-Commerce-Protocol/ucp)\n\nWho's building the first UCP-powered agent? Drop your prototypes below ‚Äì let's hack on this!¬†",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcpoaw/google_just_opensourced_universal_commerce/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzke6vh",
          "author": "eli_pizza",
          "text": "What retailers actually support it? A protocol that nobody uses isn't too useful.",
          "score": 3,
          "created_utc": "2026-01-14 16:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzklq2l",
              "author": "FaceDeer",
              "text": "Google *just* opensourced it. How quickly are you expecting coders everywhere to be able to jump and implement something like this?",
              "score": 0,
              "created_utc": "2026-01-14 17:20:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzko7zf",
                  "author": "eli_pizza",
                  "text": "It‚Äôs got dozens of logos on the page as ‚Äúendorsing‚Äù it but have any of them deployed or committed to deploying it?\n\nIf not what would a UCP agent even do right now, pretend order from pretend stores?\n\nAnyway, open protocols are better than closed ones but they‚Äôre not always ‚Äúgood.‚Äù Google AMP was open and it was bad and harmful to the web.",
                  "score": 1,
                  "created_utc": "2026-01-14 17:31:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmdvep",
                  "author": "FoxTimes4",
                  "text": "Opensourcing something that doesn‚Äôt work anywhere isn‚Äôt that useful so does it work somewhere today?",
                  "score": 1,
                  "created_utc": "2026-01-14 22:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlymaf",
          "author": "lanthos",
          "text": "Any idea how long Google is planning on supporting it? Does Gemini already use it or if not what is their roadmap?",
          "score": 1,
          "created_utc": "2026-01-14 21:00:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmqhnj",
          "author": "frobnosticus",
          "text": "Is it newly existing and open source or is it stable and in use...newly open sourced?",
          "score": 1,
          "created_utc": "2026-01-14 23:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqoe1z",
          "author": "sn2006gy",
          "text": "Since agents are probabilistic - what is the utility of agents doing this considering, the risks involved?\n\nThis sounds like something being branded agentic for the sake of branding \"slap AI or AGENT on it\" without people fully understanding that agentic workloads are still probabilistic and \"buying shit\" shouldn't have a probability attached to it.",
          "score": 1,
          "created_utc": "2026-01-15 15:18:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qagyxr",
      "title": "What are best local llm for coding & architecture? 120gb vram (strix halo) 2026",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qagyxr/what_are_best_local_llm_for_coding_architecture/",
      "author": "Proper_Taste_6778",
      "created_utc": "2026-01-12 01:07:33",
      "score": 16,
      "num_comments": 24,
      "upvote_ratio": 0.82,
      "text": "Hello everyone, I've been playing with the Strix Halo mini pc for a few days now. I found kyuz0 github and I can really recommend it to Strix Halo and r9700 owners. Now I'm looking for models that can help with coding and architecture in my daily work. I started using deepseek r1 70b q4_k_m, Qwen3 next 80b, etc. Maybe you can recommend something from your own experience?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qagyxr/what_are_best_local_llm_for_coding_architecture/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nz304fe",
          "author": "Zc5Gwu",
          "text": "I‚Äôve been enjoying unsloth Minimax Q3_K_XL. You have to turn basically everything else off to fit in vram but it‚Äôs a beast at coding.\n\nOtherwise, gpt-oss-120b is fairly strong.",
          "score": 14,
          "created_utc": "2026-01-12 01:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz34o3o",
              "author": "Proper_Taste_6778",
              "text": "Added Minimax to the list. Thanks!",
              "score": 3,
              "created_utc": "2026-01-12 02:00:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz5gq83",
              "author": "Visual_Acanthaceae32",
              "text": "How much vram you have? And how big is the context window you can use?",
              "score": 2,
              "created_utc": "2026-01-12 12:31:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz622y1",
                  "author": "Zc5Gwu",
                  "text": "Strix halo 128gb. I can use 64k context but it does oom on occasion. I just set the server to restart in the service file.",
                  "score": 3,
                  "created_utc": "2026-01-12 14:37:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3dtsv",
          "author": "ExistingAd2066",
          "text": "Strix Halo is too slow to work comfortably in agentic mode. The main problem is slow prompt parsing. Agent coding requires large prompts and context. I'm disappointed and thinking of buying a Mac Studio",
          "score": 8,
          "created_utc": "2026-01-12 02:48:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz64cwr",
              "author": "LongBeachHXC",
              "text": "This has been my experience so far with my coding tasks. The context can get large really quickly. You need that large context for the understanding though.",
              "score": 3,
              "created_utc": "2026-01-12 14:48:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz89z2w",
              "author": "Proper_Taste_6778",
              "text": "Of course, Mac Studio is better, but it's several times more expensive. I don't need to put the entire source code in the command prompt, because what's the point? Only errors and possibly functions for optimization, etc.\n\nAI  won't do all the work for me, but if I use it, I won't have to spend 15 minutes looking for an error on Stack Overflow, for example. I'm waiting for the successor to Strix Halo with more bandwidth, maybe 256-512GB of RAM would be great ^^.",
              "score": 2,
              "created_utc": "2026-01-12 20:48:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2xc3t",
          "author": "sinan_online",
          "text": "That‚Äôs a lot of VRAM to have locally, hard to find people with real experience‚Ä¶ But I would recommend having an abstraction layer, like LiteLLM, because any new model could still pop up. You want switch them out and in.\n\nAlso, I really find that putting the critical stuff in the prompt is one of the most important part of using them for actual code. Choice of GPT vs Claude matters less, in my opinion.",
          "score": 4,
          "created_utc": "2026-01-12 01:21:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2z316",
              "author": "Proper_Taste_6778",
              "text": "Community around strix halo still growths. It's the cheapest way to run quiet large LLM's models. Thanks for answer!",
              "score": 5,
              "created_utc": "2026-01-12 01:30:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz32i80",
                  "author": "sinan_online",
                  "text": "I just looked it up, thanks for letting know. I have been strictly CUDA with all my experiments, so I‚Äôll need to learn more.",
                  "score": 2,
                  "created_utc": "2026-01-12 01:48:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3bwuk",
          "author": "No-Consequence-1779",
          "text": "Hi! I‚Äôve been thinking of getting one for coding also. And 24/7 crypto trader.¬†\n\nHow is the speed for the qwen3 coder 30b instruct (dense) and moe (q4 or q8) please? ¬†\n\nHow are you liking it?¬†",
          "score": 2,
          "created_utc": "2026-01-12 02:38:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3dz2v",
              "author": "award_reply",
              "text": "Interactive Benchmark viewer for Strtix Halo: [https://kyuz0.github.io/amd-strix-halo-toolboxes/](https://kyuz0.github.io/amd-strix-halo-toolboxes/)",
              "score": 8,
              "created_utc": "2026-01-12 02:49:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3fx3e",
                  "author": "No-Consequence-1779",
                  "text": "Thank you. If the asus accent gb10¬†https://a.co/d/izvKIHg\nWas the same price, which one would you choose if it was a dedicated ai rig. Mostly inference.¬†",
                  "score": 2,
                  "created_utc": "2026-01-12 03:00:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q9a4bf",
      "title": "Where to buy used gear?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q9a4bf/where_to_buy_used_gear/",
      "author": "wizoneway",
      "created_utc": "2026-01-10 17:33:52",
      "score": 15,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Ebay listing for rtx6000 half price with accounts created in November. Is ebay safe? How do people not get scammed buying used gear now a days? Are there other sites that are reputable?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q9a4bf/where_to_buy_used_gear/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nytmld8",
          "author": "Herr_Drosselmeyer",
          "text": "An RTX 6000 Pro for half price is always a scam.",
          "score": 17,
          "created_utc": "2026-01-10 17:55:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytqkgd",
              "author": "HumanDrone8721",
              "text": "This is true, I would dare to say that even 80% is still 90% a scam, there is absolutely no reason for some to sell such a new card and the tired canard \"tee hee, I'm a Barbie Doll and got this for a present and I don't know what to do with it, or even what is it, so I'm selling it because I need money...\" is way too tired, I think the current economy made suckers with money to spend an endangered species.",
              "score": 3,
              "created_utc": "2026-01-10 18:13:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nytnkxm",
          "author": "HumanDrone8721",
          "text": "Usually it helps specifying in which county you are, because besides EBAY, that is kind of a worldwide site, there are a lot of country specific platforms and users in those countries can tell their experience, for example Kleinanzeigen is the king in Germany, mostly for p2p deals but also b2p, Craiglist and Facebook Marketplace are monsters in US but not so much in EU and so on.\n\nRegarding EBAY in the EU you're quite well protected, even if the seller says no returns, you can return what you've ordered online in 14 days (you may have to pay shipping). In US I've heard (ot sure) that they even introduced \"no buyer protection offers\" but I have no practical experience with it.",
          "score": 3,
          "created_utc": "2026-01-10 18:00:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyu2g4r",
              "author": "fallingdowndizzyvr",
              "text": "> In US I've heard (ot sure) that they even introduced \"no buyer protection offers\" but I have no practical experience with it.\n\nIn the US, you get a 30 day \"not as described\" return option no matter what the seller says.",
              "score": 4,
              "created_utc": "2026-01-10 19:09:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyx8e71",
          "author": "No-Consequence-1779",
          "text": "If it‚Äôs too cheap. No. ¬†If the account has not sold anything before. ¬†No. ¬†Plan to pay the market rate. ¬†You can not scam an honest person :)¬†\n\nFacebook marketplace can save you sales tax. Usually hundreds. But you‚Äôll need to verify it works. If they want to meet someplace where you can‚Äôt find them again, don‚Äôt do it. ¬†",
          "score": 2,
          "created_utc": "2026-01-11 05:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3dnq9",
          "author": "Educational-World678",
          "text": "Every once in a while someone gets a letter with a photo of the thing they were trying to buy... It's common enough that bored journalists can regularly find examples, but rare enough that it feels like news when it happens, if that makes sense. Also, eBay+PayPal are very good about protecting buyers when variffiable fraud has been committed.",
          "score": 2,
          "created_utc": "2026-01-12 02:47:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qe3yzy",
      "title": "Help finding best LLM to improve productivity as a manager",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "author": "2C104",
      "created_utc": "2026-01-16 02:42:07",
      "score": 14,
      "num_comments": 12,
      "upvote_ratio": 0.86,
      "text": "Like the title says, I am not in need for the LLM to code anything, I'm essentially looking for a tool that will support my managerial work. \n\nI want to be able to feed it text descriptions of the projects I am working on and get help categorizing, coordinating, summarizing, preparing for presentations, use it as a tool for bouncing ideas off of, suggestions for improving email communication, tips to improve my management productivity and abilities, etc... \n\nI want to do this offline because although ChatGPT is very helpful in this regard, I don't want sensitive work content to be shared online. \n\nMy rig has the following:\n\nIntel(R) Core(TM) Ultra 9 275HX (2.70 GHz)  \n32.0 GB RAM  \nNvidia 5070 Ti (Laptop GPU) w/ 12gb RAM  \n2tb SSDs\n\n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qe3yzy/help_finding_best_llm_to_improve_productivity_as/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzv1jrr",
          "author": "iMrParker",
          "text": "Honestly that smaller Gemma models would be good for you. Gemma 12b might fit and it has vision. GLM has 9b models which are excellent (4.6 flash I think?). If you had a desktop 5070 ti you'd be able to run GPT OSS 20b which is very solid for this purpose.¬†\n\n\nThe best advice would be to download LM Studio, use the model search, and play around with an assortment of models till you find one that fits your needs",
          "score": 7,
          "created_utc": "2026-01-16 04:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv5ynf",
              "author": "2C104",
              "text": "Thank you I'll check it out",
              "score": 2,
              "created_utc": "2026-01-16 04:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvt7k7",
          "author": "techlatest_net",
          "text": "Hey man, for managerial stuff like summarizing projects, tweaking emails, or brainstorming without sending sensitive info to the cloud, your rig is perfect‚ÄîOllama is dead simple to set up and flies on that 5070 Ti with 12GB VRAM.‚Äã\n\nGrab Ollama (ollama.com), install in like 2 mins, then pull mistral-nemo:12b Q4 or qwen2.5:7b-instruct Q5‚Äîboth fit comfy quantized and give solid, non-hallucinating responses for exactly what you described. Nemo edges out Llama 3.1 8B on reasoning without being too wordy, and folks use it daily for email drafts and task categorization just like you want.‚Äã\n\nPaste project text, say \"summarize this for a team mtg, suggest 3 action items\" or \"rewrite this email to sound more collaborative\"‚Äîhits 20-40 t/s easy. Bonus: there's even a quick Python script floating around using Llama3 via Ollama for auto-email summaries if you wanna script it later. Way better privacy than ChatGPT, zero cost after download.",
          "score": 2,
          "created_utc": "2026-01-16 07:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx1xqf",
              "author": "2C104",
              "text": "Can anyone explain why this comment is getting downvoted? It seems the most eloquent of the responses and since they all seem to be equally helpful I'm just trying to understand why people would disagree with what was stated here.",
              "score": 2,
              "created_utc": "2026-01-16 13:38:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwpw1m",
          "author": "chief-of-wow",
          "text": "Although I don't know how offline AI works, I have a couple of tips to share about online AI:\n\n1. For Presentations, Gamma AI is really good. I recently tested it out and it has so many cool features. The one that stood out to me was how easy it was to choose diff types of graphic elements within the tool. For the slides, I gave my notes to Claude and asked it clean them up and structure them as slides. It did so and then I fed that into Gamma to build a presentation for me.\n\n  \n2. A productivity tip: I use voice dictation a lot. When I have lots of thoughts and context, I use dictation to feed AI. Be it drafting a long (and important) email, creating an SOP, or similar comms or documentation work, this saves me a lot of time. Also, I don't know the reason, but the results are so much better than when I type instructions. I am a decent writer so proofreading and editing doesn't take me long. Overall, this process works well for me so I definitely recommend testing it.",
          "score": 1,
          "created_utc": "2026-01-16 12:24:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxbau4",
          "author": "nofilmincamera",
          "text": "Having access to local LLM has made me a extremely effective.  That said its a right tool for right job.\n\nUsing it for language re writes? Sure. But you really need one of the bigger models simply just for deep research. \n\nAlso,  understanding tools and python will get you pretty far. A local installed took line N8N makes it great to automate.  LLMs are non deterministic. Which means by themselves are terrible for consistently.  In any use you come up with, it really helps that you break that task apart.  What of this can be automated without a LLM at all? A smaller model can do a lot as a validator, and where non deterministic is a value.",
          "score": 1,
          "created_utc": "2026-01-16 14:26:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwfmcb",
          "author": "fasti-au",
          "text": "Nemotron 30b if local with websearch and some memory files is very good for most things.  Technical writing I‚Äôd go phi4.  \n\nBig leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nGrok is the most cutthroat, a win is a win damn the alternatives sort model.  \n\nIt backs itself and is very much great at winning if it finds a trick but it‚Äôll bend the rules and go around problems in some ways so I would be wary of long memory and brainstorming without checks",
          "score": 0,
          "created_utc": "2026-01-16 11:06:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx2cm0",
              "author": "2C104",
              "text": ">Big leagues chat then gpt is best standalone but you f you Claude mcp then Claude is way more for less ficking imo\n\nI have no idea what this is trying to say, it seems incoherent to me. Are these the names of LocalLLMs? Or were you just using shorthand?",
              "score": 1,
              "created_utc": "2026-01-16 13:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o011jom",
                  "author": "number66-1",
                  "text": "I couldn't understand him either, but he doesn't need to waste his time explaining himself since if you are using AI and really care to understand what he said you could ask AI to help you understand. \n\nHe's done his part, he shared his knowledgeable opinion.\n\n\"Chatgpt: Here‚Äôs what that commenter is saying, rewritten in normal English, with the shorthand decoded.\n\n1) Clean rewrite of their comment\n\nIf you want a local model and you can give it web search + a small ‚Äúmemory‚Äù of your files, Nemotron 30B is good for most tasks.\nFor technical writing, they‚Äôd pick Phi-4.\n\nFor ‚Äúbig leagues‚Äù (top-tier general chat), GPT is the best standalone option.\nBut if you‚Äôre using Claude with MCP, then Claude is better / less annoying in their opinion.\n\nGrok is the most cutthroat: it cares about ‚Äúwinning‚Äù and will pick an answer aggressively.\nIt‚Äôs very confident and can be great when it finds a clever shortcut, but it may ‚Äúbend the rules‚Äù or take weird detours, so they wouldn‚Äôt trust it for long-term memory or brainstorming unless you verify everything.\n\n\nThat‚Äôs the whole meaning.\n\n2) What each name likely refers to\n\nNemotron 30B: a local LLM family/model (30B = ~30 billion parameters). They mean ‚Äúrun it on your own machine‚Äù (or your own server).\n\nPhi-4: Microsoft‚Äôs Phi family model; they‚Äôre saying it‚Äôs strong for structured/technical writing.\n\nGPT: OpenAI‚Äôs models (ChatGPT). They mean ‚Äúbest general-purpose chat without needing extra tool wiring.‚Äù\n\nClaude: Anthropic‚Äôs models.\n\nGrok: xAI‚Äôs model.\n\n\n3) What the jargon means\n\n‚ÄúLocal‚Äù: you run the model yourself (privacy, control, but you manage setup + performance).\n\n‚ÄúWebsearch‚Äù: hooking the model to a browsing/search tool so it can fetch current info instead of guessing.\n\n‚ÄúMemory files‚Äù: usually means a RAG setup (retrieval-augmented generation): your notes/docs are indexed and the model can pull relevant chunks while answering.\n\n‚ÄúBig leagues‚Äù: basically ‚Äúbest-of-the-best overall.‚Äù\n\n‚ÄúStandalone‚Äù: not using tool integrations, agents, RAG, custom workflows‚Äîjust the model as-is.\n\n‚ÄúClaude MCP‚Äù: almost certainly referring to the Model Context Protocol (MCP)‚Äîa way to connect Claude to external tools/services (files, apps, search, automations) in a standardized way.\n\n‚ÄúCutthroat / win is a win‚Äù: the model will prioritize getting some answer that seems successful, even if it‚Äôs not cautious or perfectly aligned with your intent/rules.\n\n‚ÄúBend the rules / go around problems‚Äù: meaning it may produce confident outputs, shortcuts, or ‚Äúcreative‚Äù leaps that sound right but aren‚Äôt reliably grounded.\n\n‚ÄúWary of long memory and brainstorming without checks‚Äù: don‚Äôt let it build long chains of assumptions over time; verify with sources, logs, or constraints.\n\n\n4) Why it reads incoherent\n\nIt‚Äôs basically:\n\nmissing punctuation,\n\ntypos (‚Äúbut u f you Claude mcp‚Ä¶‚Äù = ‚Äúbut if you use Claude MCP‚Ä¶‚Äù),\n\nand it jumps between local model advice and hosted model opinions without transitions.\n\n\nIf you tell me what you were trying to decide (local vs cloud, privacy, work tasks, hardware), I can translate their advice into a concrete recommendation that fits your situation.\"",
                  "score": 1,
                  "created_utc": "2026-01-17 01:11:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwpevx",
          "author": "beauzero",
          "text": "Just use claude cowork.",
          "score": 0,
          "created_utc": "2026-01-16 12:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx80vu",
          "author": "Nx3xO",
          "text": "If you want multiple options set yourself up on a jetson orin 16/32gb. Openwebui. Easy to setup guard rails, multiple llms ready to load. You can airgap for extra security. Low power too. Your laptop is fully capable but this option b could be a much better and flexible option. \n\nI have an 8gb, about 8 models. Focused on math medical and general. Simple small models. I can access it on the wire or jump onto it via wifi ap I setup. Technically I could throw it on a battery and and tap into on the go, airport/traveling. Add extra storage and its a portable backup solution.",
          "score": 0,
          "created_utc": "2026-01-16 14:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxgc90",
              "author": "mauricespotgieter",
              "text": "Hi Nx3xO.\nWould you mind sharing details of your n8 setup?\nI am looking to do something similar and would be grateful for any assistance. Still finding my legs and learning. Thanks in advance. Open to DM directly if that is more appropriate?",
              "score": 1,
              "created_utc": "2026-01-16 14:51:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qexu0l",
      "title": "Is there a local/self-hosted alternative to Google NotebookLM?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qexu0l/is_there_a_localselfhosted_alternative_to_google/",
      "author": "RadiantCandy1600",
      "created_utc": "2026-01-17 00:36:37",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "I‚Äôve been using **Google NotebookLM** recently and the workflow is incredible‚Äîbeing able to upload a dozen PDFs and have the AI \"ground\" itself in those specific sources is a game changer for research.\n\nHowever, I‚Äôm not thrilled about uploading sensitive work documents or personal research to Google‚Äôs cloud. I‚Äôm looking for something I can run **locally on my own hardware** (or a private VPS) that replicates that \"Notebook\" experience.\n\n**Ideally, I‚Äôm looking for:**\n\n* **Privacy:** No data leaving my machine.\n* **Source Grounding:** The ability to chat with specific \"Notebooks\" or collections of PDFs/Markdown/Text files.\n* **Citations:** It needs to tell me exactly which page/document the answer came from (this is the best part of NotebookLM).\n* **Audio/Podcasts (Optional):** The AI podcast generator in NotebookLM is cool, but document analysis is my priority.\n\n**What are the best options in 2026?** I‚Äôve heard names like **AnythingLLM**, **GPT4All**, and **Open Notebook** (the GitHub project) thrown around. Which one is currently the most stable and \"NotebookLM-like\"? ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qexu0l/is_there_a_localselfhosted_alternative_to_google/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o012xn0",
          "author": "GrayRoberts",
          "text": "Open-notebook.io",
          "score": 4,
          "created_utc": "2026-01-17 01:20:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o013c4r",
          "author": "Aromatic-Low-4578",
          "text": "[https://github.com/lfnovo/open-notebook](https://github.com/lfnovo/open-notebook)",
          "score": 3,
          "created_utc": "2026-01-17 01:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00x742",
          "author": "marsxyz",
          "text": "Just the RAG in openwebui ?",
          "score": 2,
          "created_utc": "2026-01-17 00:45:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00y8ep",
          "author": "michael_p",
          "text": "I was going to say anythingllm but you mentioned that",
          "score": 2,
          "created_utc": "2026-01-17 00:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o037mix",
          "author": "toadi",
          "text": "I use obsidian for note taking. I use a plugin that you can use claude, opencode, ... inside obsidian. Wrote a custom agent in opencode for research and scanning my repository and internet if needed. I provided it templates I want it to write in too.\n\nFeels close to notebookllm but in my own way.\n\nThe plugin: [https://github.com/RAIT-09/obsidian-agent-client](https://github.com/RAIT-09/obsidian-agent-client)\n\n  \nI tried different plugins but I prefer these types as my opencode can plugin different applications keeping its configuration the same. This way I don't need to redo it in my editor and obsidian.",
          "score": 2,
          "created_utc": "2026-01-17 11:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00xanc",
          "author": "MaverickPT",
          "text": "ragflow.io?",
          "score": 1,
          "created_utc": "2026-01-17 00:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ie83",
              "author": "Karyo_Ten",
              "text": "Have you actually used it?",
              "score": 1,
              "created_utc": "2026-01-17 07:29:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03gv86",
                  "author": "MaverickPT",
                  "text": "I've tried a few months ago, but I think their document ingestion pipeline was broken as I couldn't reliably ingest my datasheets. Since then I know they have updated it but I haven't tried it yet.\n\nHave you?",
                  "score": 1,
                  "created_utc": "2026-01-17 12:40:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o023g7n",
          "author": "arman-d0e",
          "text": "Surfsense. It started as a notebook lm clone but I think they got cease and desisted or something and had to redesign a bit\n\nhttps://github.com/MODSetter/SurfSense",
          "score": 1,
          "created_utc": "2026-01-17 05:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o044czd",
          "author": "Clipbeam",
          "text": "Have a look at https://clipbeam.com if you use a Mac. No data leaves your machine, it allows you to chat with one or more pdfs/markdown/text files (plus also web links, audio and video files) and it will tell you which document it based it's answer on when you chat with it. \n\nWhat I haven't done yet is make it possible for the AI to cite the specific page of a document, but I may build this feature for a future update. Let me know if it meets your needs?",
          "score": 1,
          "created_utc": "2026-01-17 14:59:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcmmvw",
      "title": "M4/M5 Max 128gb vs DGX Spark (or GB10 OEM)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/",
      "author": "Soggy-Leadership-324",
      "created_utc": "2026-01-14 12:59:55",
      "score": 13,
      "num_comments": 87,
      "upvote_ratio": 0.82,
      "text": "I‚Äôm trying to decide between NVIDIA DGX Spark and a MacBook Pro with M4 Max (128GB RAM), mainly for running local LLMs.\n\nMy primary use case is coding ‚Äî I want to use local models as a replacement (or strong alternative) to Claude Code and other cloud-based coding assistants. Typical tasks would include:\n- Code completion\n- Refactoring\n- Understanding and navigating large codebases\n- General coding Q&A / problem-solving\n\nSecondary (nice-to-have) use cases, mostly for learning and experimentation:\n- Speech-to-Text / Text-to-Speech\n- Image-to-Video / Text-to-Video\n- Other multimodal or generative AI experiments\n\nI understand these two machines are very different in philosophy:\n- DGX Spark: CUDA ecosystem, stronger raw GPU compute, more ‚Äúproper‚Äù AI workstation‚Äìstyle setup\n- MacBook Pro (M4 Max): unified memory, portability, strong Metal performance, Apple ML stack (MLX / CoreML)\n\nWhat I‚Äôm trying to understand from people with hands-on experience:\n- For local LLM inference focused on coding, which one makes more sense day-to-day?\n- How much does VRAM vs unified memory matter in real-world local LLM usage?\n- Is the Apple Silicon ecosystem mature enough now to realistically replace something like Claude Code?\n- Any gotchas around model support, tooling, latency, or developer workflow?\n\nI‚Äôm not focused on training large models ‚Äî this is mainly about fast, reliable local inference that can realistically support daily coding work.\n\nWould really appreciate insights from anyone who has used either (or both).",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzjr5g0",
          "author": "g_rich",
          "text": "If you‚Äôre not doing any training or fine tuning and just running inference then the higher memory bandwidth of the Mac will be a benefit. \n\nHowever set your expectations accordingly, regardless of how much hardware you get you‚Äôre not going to get Claude level performance and with only 128MB of unified memory you‚Äôre going to be a far ways off. If you want Claude level performance then you‚Äôre better off paying for Claude because you can easily spend upwards of $10k plus and still never hit what Claude or ChatGPT or Gemini deliver. \n\nBenchmarks for the M5 show a significant increase in performance for LLM‚Äôs over the base M4 and there are some rumors of new MacBook Pro‚Äôs being released at the end of the month so you might want to wait a few weeks and see if Apple introduces MacBooks with the M5 Pro and Max.",
          "score": 10,
          "created_utc": "2026-01-14 14:59:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o001bd1",
              "author": "GCoderDCoder",
              "text": "Agreed! As much as I love my local hosted models, the goal with local is privacy, wanting to learn from the ground up, and shielding against market uncertainty, not getting the best inference experience. I have a 256gb mac studio and I get very good local inference but I am spending a lot of time trying to configure local systems to manage context/ memory better. \n\nThe model outputs like glm 4.7 are great for me at q4 (I argue near if not at frontier level) but models at that level start around 20t/s for me and slow down as context increases. Plus I can only manage so much context to run locally on my mac studio so even if speed weren't an issue memory becomes an issue. So systems breaking things into smaller tasks help keep the models faster but introduce complexity too.\n\nI use claude in cursor for work and these aren't concerns. I just use the model and get top tier performance with no worries. I have a 128gb strix halo that gets similar speeds to the spark. I use that as a side gpt-oss120b for agentic tasks with moderate to low logic requirements for tasks other than coding. That's like gemini flash 2.5 doing agentic tasks for you not the model that we worry about replacing us lol",
              "score": 1,
              "created_utc": "2026-01-16 21:53:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzj7zw1",
          "author": "jba1224a",
          "text": "The m4 max has significantly higher memory bandwidth (almost double) than the spark.  The spark also works on a unified memory model.\n\nIf we‚Äôre talking strictly inference the MacBook is going to going to be much faster - the caveat being the spark will have better support given most frameworks run well on nvidia as it‚Äôs cuda.\n\nDon‚Äôt sleep on mlx though, it‚Äôs getting very good.\n\nIf you don‚Äôt need a laptop format you may want to consider a Mac Studio m4 max with the same configuration which would be -considerably- cheaper.",
          "score": 7,
          "created_utc": "2026-01-14 13:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjbuqd",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-14 13:38:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjmias",
                  "author": "belgradGoat",
                  "text": "What the issue with mlx? I run mlx every day no issues at all, it‚Äôs fast it loads quickly",
                  "score": 1,
                  "created_utc": "2026-01-14 14:35:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzjxto0",
              "author": "StardockEngineer",
              "text": "Thai is not true.  The DGX‚Äôs Prefill speeds crush the Mac.  The Spark will frequently finish interesting before the Mac‚Äôs even output a single token.",
              "score": 2,
              "created_utc": "2026-01-14 15:31:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzk0mtm",
                  "author": "jba1224a",
                  "text": "I‚Äôve used both for local inference and I did not find that to be true.\n\nBut I do agree that if you‚Äôre using large complex prompts it is a factor",
                  "score": 3,
                  "created_utc": "2026-01-14 15:44:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlgei2",
          "author": "HealthyCommunicat",
          "text": "If you‚Äôre going for PURE text generation, there simply is nothing that beats the m3 ultra mac studio. Ask ANYONE who says that the gb10 is better to show it and do a direct comparison. The nvidia people like to say that the mac people are just exaggerating the numbers on paper - thats not true at all. They‚Äôre completely choosing to ignore the fact that on paper, nvidia is supposed to have much much better capability - and thats perfectly true, the spark can do many many things that would take the m3 ultra many times over to do the same exact thing such as fine tuning or image/video gen - but if your goal is to do pure pure agentic inference, there simply is nothing that beats the m3 ultra. Its not a matter of opinion or whatever, its something that can be proved literally every single time. \n\nIf your goal is to have the llm read as many documents as possible and organize them or whatever, the spark wins by a massive amount but fails completely on actual text generation.\n\nif u find someone pushing you on the fact that the dgx spark is better than the m3 ultra for agentic coding workflows, tell them to prove it, im willing to load up a model and run tests to show it.\n\nnvidia fan boys will say \"the m3 ultra is only better on paper and not in actuality\" - when thats literally the complete opposite. the spark has much more extensive capabilities in terms of actual numbers in hardware. the m3 ultra is lacking extensive things that the dgx spark has. yet llm's run better on the mac studio m3 ultra. people are literally mad that there nvidia toybox isnt able to do as well as apple, completely forgetting that the nvidia tax is a real thing that even massive megacorporations admit to, why cant people accept that their paying extremely high for a machine that simply does not perform as good as the m3 ultra? how much longer are people going to keep arguing with me about this but not one single person is willing to actually put it to the test when i ask?\n\nagain, if anyone is so confident that the dgx spark is able to beat the m3 mac studio in terms of speed for pure text generation, im willing to bet $100 into an escrow of ur choice if you're willing to match just to  prove you wrong. its a free $100 for you right?",
          "score": 5,
          "created_utc": "2026-01-14 19:37:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmmrmj",
              "author": "Responsible_Room_706",
              "text": "So, TLDR Is that neither are a silver bullet and given the use case it may make sense to have both?",
              "score": 1,
              "created_utc": "2026-01-14 22:52:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmn56p",
                  "author": "HealthyCommunicat",
                  "text": "kinda? like u really really only need the dgx spark if ur planning on focusing hardcore in video and image - cuz the m3 ultra can still do that very very well, like the m chips are purely about that shit, just ends up nvidia is better.\n\nif text generation is any part of your needs, this is the difference between being able to use glm 4.7 and not being able to. it doesnt matter how fast prompt processing is if its not even usuable if the prompt itself is short.\n\nexample, even if u gave both spark and mac studio \"hi, check the weather today\" using glm 4.7 - the m3 mac studio will beat the spark by a fuck ton, and saying that is an understatement. any model that is over 30b parameters, even 70b and not moe are too slow and literally literally just not usuable unless ur like using it for pure writing stories or some crap\n\n8. Conclusion\n\nThe choice between MLX (Mac Studio) and DGX Spark for agentic coding is ultimately a choice between¬†**Bandwidth**¬†and¬†**Intelligence**.\n\n**MLX**¬†offers the raw horsepower to generate text rapidly, making it the superior \"Typewriter.\" However, its current software limitations regarding cache reuse in branching scenarios make it a forgetful writer‚Äîone that must constantly re-read the entire book to write an alternative ending.\n\n**DGX Spark**¬†offers the architectural sophistication of a datacenter. It is a slower \"Typewriter\" due to memory bandwidth limits, but possessed of a photographic memory for context. In complex, non-linear agentic workflows where \"thinking\" (branching, backtracking, and planning) dominates \"typing\" (linear generation), the DGX Spark's ability to reuse the KV cache via TensorRT-LLM makes it the more performant and robust platform.\n\nFor the immediate future (2026), until MLX natively incorporates Radix Tree caching and PagedAttention primitives, the NVIDIA ecosystem remains the pragmatic choice for serious¬†*autonomous*¬†agent development, while the Mac Studio remains the king of¬†*interactive*¬†local coding assistance.",
                  "score": 4,
                  "created_utc": "2026-01-14 22:54:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmsckv",
                  "author": "HealthyCommunicat",
                  "text": "comes down to\n\ndo u want to be able to have a tiny bit higher intelligence than the m3 ultra and still be at speeds where u can literally read 3-5x times faster, or have just a itty bitty bit less intelligence and have a machine that can generate text much faster than u can even read? i realize i shouldve said it like this in the first place for others wondering the same.\n\n  \nits not the same thing, its not even close. it literally is a matter of something being usuable or not.",
                  "score": 1,
                  "created_utc": "2026-01-14 23:21:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjpaa3",
          "author": "Mean-Sprinkles3157",
          "text": "Dgx spark is small like a brick, energy efficient lower than 100w, mostly 10w idle. I setup one at home, and purchased a used dell latitude 5510, with 32gb ram that I can carry any where. It has no issues with cursor , vs code plus multitabs of Firefox.\nI like the fact that Dgx is extensible.right now is 1, I could connect more. The concern is the bandwidth. I was contemplating to return my current dgx, replace with  the gb10 oem like the Asus one. What do you think? The current dgx cost me 6000 CAD, while the assent is 4200 CAD for 1tb, the difference is one good MacBook Pro.",
          "score": 2,
          "created_utc": "2026-01-14 14:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjv0tw",
          "author": "pieonmyjesutildomine",
          "text": "If it's \"mainly for running LLMs,\" get the Mac. I have both and that's the only thing I use the Mac for, the spark is a CUDA dev platform.",
          "score": 2,
          "created_utc": "2026-01-14 15:18:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmh57c",
              "author": "Soggy-Leadership-324",
              "text": "if i want to use comfyui for gen video or image, using such as wan or ltx-2, is it spark will be better?",
              "score": 1,
              "created_utc": "2026-01-14 22:24:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzon67k",
                  "author": "pieonmyjesutildomine",
                  "text": "Miles better, yeah. The Spark has better and faster image processing than Macs or even the Strix Halo.",
                  "score": 3,
                  "created_utc": "2026-01-15 06:12:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlh84t",
          "author": "HealthyCommunicat",
          "text": "Let me put this simply. If there is a dgx spark next to a m3 ultra and they both loaded qwen 3 next 80b and gave them the exact same prompt, the m3 ultra will finish first every. single. time. The prompt processing speed of the spark cannot mathematically, logically, PHYSICALLY outbeat the mac studio.",
          "score": 2,
          "created_utc": "2026-01-14 19:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznxhvr",
              "author": "johnkapolos",
              "text": "Doesn't the spark have faster prompt pre-processing about 4x (and about that much less speed in generation)?\n\nOP's use case has huge context size compared to the generation size.",
              "score": 1,
              "created_utc": "2026-01-15 03:13:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzny7gx",
                  "author": "HealthyCommunicat",
                  "text": "yes it does. maybe even more. the token gen just simply makes up that much. its just math. thats the level of insane drastic difference, the token generation on the dgx spark is that bad that even with poor prompt processing the m3 mac studio can still do speeds the dgx spark cant.\n\nyou can go read my other comments about this because i get pretty dam deep into this. im tired of people saying that this myth when its proveable wrong.\n\nwhats the point of a good reader who cant even write?\n\na writer who cant even read can still produce something. i know this is a exaggeration but its as simple as that.\n\nif u know anyone who has a dgx spark willing to do actual benchmarks side by side with a model people actually want to use, let me know. i'd be willing to put money into an escrow over this simply just because i got to test the difference myself before i returned my spark and even have videos. the prompt processing is insanely better on the dgx spark, but the memory bw just trumps all.\n\nCalculating the Performance Crossover\n\nI am currently synthesizing the performance metrics to determine the exact point where superior text-writing speed overcomes a slower start in data ingestion. For a typical coding task involving a 10,000-token context, my calculations show that while a high-compute system might start working five seconds faster, the more bandwidth-heavy system completes the generation phase significantly earlier. In multi-step agentic workflows where a model might output code across 50 iterations, the cumulative time saved during the 'typing' phase quickly renders any initial ingestion advantage irrelevant.\n\nCalculating the Total Timeline\n\nI am synthesizing a total execution model that adjudicates between starting fast and finishing fast. My findings indicate that in a typical programming loop involving 40 to 60 turns, the cumulative time saved during the actual writing of code blocks is the decisive factor. While a competitor might save five seconds on an initial file read, a system with triple the data throughput saves multiple seconds on every single response it generates. Over the course of a standard coding task, this generation advantage accumulates into a lead that no amount of initial processing speed can overcome.\n\nThe Ingestion vs. Generation Math\n\nI have been mathematically deconstructing the performance intersection when an agent reads new files mid-task. My calculations show that even when an agent pulls in a fresh 1,000-token block of code, a high-bandwidth system only needs to generate about 55 tokens of output to reach the finish line first. Given that professional coding agents frequently generate hundreds of tokens of refactored logic or debugging logs per turn, the 'typing' speed of the memory pipeline remains the definitive bottleneck for the total session time.",
                  "score": 1,
                  "created_utc": "2026-01-15 03:17:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzmecpp",
          "author": "HealthyCommunicat",
          "text": "by the way, mlx-lm allows for extreme kv cache optimization - im getting above 2.5k prompt procesing on the m3 ultra mac studio on minimax m2.1 4bit when focusing mainly on prompt procesing and not token gen, cuz when this does happen the token gen goes down from 40 to 15-20 (still very very usable) this isnt the case all the time and very specific - but with literally an hr or two of tweaking im able to use minimax m2.1 4bit so dam comfortably.\n\npeople will try to say im lying, and if u just dm me i'd be more than willing to screenshare",
          "score": 2,
          "created_utc": "2026-01-14 22:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoadzt",
              "author": "TheRiddler79",
              "text": "You get 20/sec on MiniMax-M2.1? That has got to feel nice.\n\nMinimax is like the sleeper of the truly brilliant reasoning models.\n\nI get about 4.5,but strictly ram no gpu.  I'm on zen 2 technology nothing super modern.",
              "score": 2,
              "created_utc": "2026-01-15 04:36:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzmghw9",
              "author": "Soggy-Leadership-324",
              "text": "how many ram of ur m3 ultra? thx",
              "score": 1,
              "created_utc": "2026-01-14 22:21:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmha8v",
                  "author": "HealthyCommunicat",
                  "text": "256 gb. got it for $5500 usd. there just is nothing else that will beat this when it comes to using llm's for agentic coding for this price. again, if someone tries to argue that the spark is better, ask them to show it cuz im willing to screenshare any fucking day to put an end to this bs argument.",
                  "score": 1,
                  "created_utc": "2026-01-14 22:25:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjg9a4",
          "author": "sn2006gy",
          "text": "The cost benefit isn't there... all of this hardware is crazy slow and frustrating in comparison to a Claude or Cursor.\n\nMaddingly slow if it's your day job.",
          "score": 4,
          "created_utc": "2026-01-14 14:02:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjjtoa",
              "author": "nyssaqt",
              "text": "Is it?  I was considering buying a Mac Studio 256gb in order to test local LLMs for coding.",
              "score": 1,
              "created_utc": "2026-01-14 14:21:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjpe5a",
                  "author": "sn2006gy",
                  "text": "Is coding your day job? If so, I'd just pay for a service and save the sanity.\n\nIf you just like to tinker, go for it. \n\nI work for an OSS company so I just pay for Cursor and even if I have to upgrade the plan for more tokens, it's cheaper than buying and maintaining my own localllm for something that is going on github anyway. \n\nInversely, I thought a lot about privacy of localllm for development but then it smacked me, if my code is only as strong as the model and the prompt then anyone with a model and a prompt can clone my idea so my proprietary ideas are only as good as the models everyone can run so I may as well just code using Claude/Cursor.\n\nI put a lot more effort into pulling my data off the internet, deleting/obfuscating social media posts for privacy so my likeness and data won't train a model after kind of realizing my concerns for privacy around code were kind of moot.\n\nthe value is higher up the chain in actually being able to run the service, run the code, make money from it - and perhaps a local llm to help you achieve that is better than one to code it but then again, the larger the model the more you can press it for nuance/angles that aren't already being done...\n\nwhich then just reminds everyone we're in a rush to the bottom :D",
                  "score": 4,
                  "created_utc": "2026-01-14 14:50:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzlhmay",
                  "author": "HealthyCommunicat",
                  "text": "Dont listen to this guy, i work in a datacenter 8+ hrs a day and i use the m3 mac studio to run glm 4.7 4bit (25token/s max) and minimax m2.1 @4bit and am getting 40-50token/s and 500+ token/s pp (slower end compared to nvidia but a spark wont even get close to 40-50 token/s) - it has completely replaced claude for me. Just less than an hr ago i was working in cline and had compacted multiple times and token gen never dropped below 25-30 token/s - still very very usable.",
                  "score": 0,
                  "created_utc": "2026-01-14 19:42:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzjumvl",
              "author": "LoonSecIO",
              "text": "You don‚Äôt get an email you are out of tokens till next month :shrug:",
              "score": 1,
              "created_utc": "2026-01-14 15:16:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjuxg8",
                  "author": "sn2006gy",
                  "text": "clicking the upgrade button isn't that hard and if your job depends on your coding it's still cheaper.",
                  "score": 3,
                  "created_utc": "2026-01-14 15:17:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjyoqo",
          "author": "StardockEngineer",
          "text": "These Mac people got it backwards.  Btw, I‚Äôm a Mac fan who has bought 5 M-series Mac‚Äôs since M1 and own two currently, with plans to buy an M5 Max. \n\nBut the DGX Spark crushes the Macs.  There are two parts of inferencing, prefill and token generation.  The DGX is much faster on prefill.  It will often finish  prefill and inferencing before the Macs even begin their first token.  \n\nExo‚Äôs own charts say this https://blog.exolabs.net/nvidia-dgx-spark/\n\nAlso see this https://youtu.be/IUSx8Vuo-pQ\n\nIt is a constant, misinformed theme in this sub to ignore prefill.\n\nAlso, are you interesting in making images, videos, fine tuning or anything else AI can do?  Get a DGX. It‚Äôll be much faster.",
          "score": 1,
          "created_utc": "2026-01-14 15:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlfn1f",
              "author": "HealthyCommunicat",
              "text": "The prefill matters little when ur actually using the llm‚Äôs agentically and the dgx spark forces you to wait 1 second to read the prompt and then 30 minutes to type out the exact same sentence that would take the m3 studio maybe 1 minute to read and 5 mins to write - this is just an exaggerated example but in the end if you do the math, as context starts to grow, the m3 ultra just will always win for pure inference. Don‚Äôt want to believe me? Dm me and we can do some tests, and not just with gpt oss 120b but an actual real model that we would want to use.\n\n\nAre people really trying to say that the prompt processing is THAT MUCH INSANELY faster that the token gen speed outbeats the m3 ultra? Like yes the spark has much faster prompt processing but even if the text generation starts much earlier, that text generation will end much after the m3 ultra has already done prompt processing and fully generated. Is it not simple logic? Are you really really trying to say that if we put a dgx spark and a m3 ultra side by side and gave the same prmpts with the same inferencing engine and actual model to use for real coding, that the spark would win?",
              "score": 3,
              "created_utc": "2026-01-14 19:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlidzl",
                  "author": "StardockEngineer",
                  "text": "I'm not trying to say it, I am saying it.  Did you even look at the links?  At just 8k context, the Spark starts and finishes before the Mac even begins producing.  How about when it's 15k from OpenCode or 30k+ from Claude Code?  Or when agents are reading files in the middle of an agent flow?  It doesn't  get better.  \n\nThere is almost always more input tokens than output, that's why LLM providers price them differently.  \n\nI linked to a video in the post you replied to that literally put the two side-by-side, and the Mac did not win.   26 minutes vs 6.\n\nAm the only one who notices when the Mac's \"win\" it's when they completely ignore or quickly gloss over the prefill problem?  Literally watch all the 4xUlras RDMA videos.",
                  "score": 0,
                  "created_utc": "2026-01-14 19:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzjrd5a",
          "author": "CMPUTX486",
          "text": "I got the DGX because I want to use cuda and lazy to fix library issue.",
          "score": 1,
          "created_utc": "2026-01-14 15:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzk0ws0",
          "author": "paulirotta",
          "text": "Compare the best type of models of interest to you that run best on each. nvidea releases fp4 models that run 2x faster than q4 on Spark machines. Despite the lower bandwidth, that might be the difference.",
          "score": 1,
          "created_utc": "2026-01-14 15:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznw7a6",
          "author": "SlippySausageSlapper",
          "text": "In order to do something equivalent to what claude does, you would need a B200 or equivalent, which costs half a million dollars and uses 14,000 watts of power continuously during inference.\n\nYou might want to temper your expectations.",
          "score": 1,
          "created_utc": "2026-01-15 03:05:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzo9m7s",
              "author": "TheRiddler79",
              "text": "Nah, I can do coding \"equivalent\" to Claude running code on my threadripper straight from ram.\n\nNow, can I deliver 100 tokens a second to 1,000 different people simultaneously? No, but can I get 10 tokens a second for myself personally, yes. At the end of the day, I can do that 24 hours a day for free. In tandem with anything else, which means if I needed something faster I would just simply have Claude do it or Gemini.\n\nUnless people intend on selling compute, or sharing it with a bunch of other users, the need for massive tokens per second can be mitigated by simply using a smart plan and your current other tools when necessary",
              "score": 1,
              "created_utc": "2026-01-15 04:31:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzob4dq",
                  "author": "SlippySausageSlapper",
                  "text": "You‚Äôre not doing that with less than 2tb of ram and you won‚Äôt be hitting 10tps without quantizing it into incoherence running on CPU.\n\nRunning claude-level models at home is not practical unless you have hundreds of thousands to spend on a computer, which is nuts because you can get a LOT of claude usage from Anthropic for 200/mo.",
                  "score": 2,
                  "created_utc": "2026-01-15 04:41:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzrilrv",
          "author": "pistonsoffury",
          "text": "You're cross-shopping the wrong products - you should be doing Mac Studio vs. Spark. If portability is a concern, then the obvious choice is the Macbook pro.",
          "score": 1,
          "created_utc": "2026-01-15 17:34:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuuyg3",
          "author": "rditorx",
          "text": "Remember that M5 Pro and M5 Max MacBook Pros are supposed to arrive soon which will probably bring some speedup regarding memory bandwidth and neural acceleration. Even if you're not buying them, M4 MacBook Pros could get cheaper, though a build-to-order / configure-to-order 128 GB M4 Max may become impossible to order once the M5 Pro/Max come out (leaving only left-over stocked items).",
          "score": 1,
          "created_utc": "2026-01-16 03:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmhorz",
          "author": "HealthyCommunicat",
          "text": "\"3.3 The \"Cross-Over\" Efficiency Point\n\nResearch by EXO Labs introduces a critical concept: the cross-over point where one machine's advantage cancels out the other's.\n\n* Because the Spark starts faster (low TTFT) but runs slower (low TPS), and the Mac starts slower (high TTFT) but runs faster (high TPS), there is a specific prompt-to-generation ratio where the total time is identical.\n* **Short Prompt / Long Response:**¬†The Mac wins easily. The initial 2-second lag is instantly recovered by the blazing fast generation.\n* **Long Prompt / Short Response:**¬†The Spark wins. If the task is \"Read this book and say 'Yes' or 'No',\" the Mac spends 20 seconds reading and 0.1 seconds answering. The Spark spends 5 seconds reading and 0.2 seconds answering.\n\n**Benchmark Synthesis:**¬†The user's intuition that \"token generation speed will make up for the lack in prompt processing\" is mathematically correct for arguably 90% of interactive use cases (chat, coding, writing). The \"Prompt Processing\" advantage of the Spark is a specialized victory relevant primarily for massive RAG ingestion or batch processing, not the fluidity of conversation.¬†¬†¬†\n\n\"\n\n**Final Verdict:**\n\n* **DGX Spark is Faster When:**¬†You are processing massive documents (long prompts), running batch classification jobs, fine-tuning models, or require native CUDA support for training.\n* **Mac Studio M3 Ultra is Faster When:**¬†You are chatting, coding, writing, or interacting with the AI in real-time. The generation is smoother, the capacity (up to 512GB) is higher, and the latency per token is significantly lower.\n\nso yeah the mac studio wins for when ur trying to use an actual llm. u dont have to believe me, u can just go ask multiple spark and m3 mac studio owners to prove it. \n\n  \ndont even get me started on mlx kv cache reusing cuz at that point it just starts getting sad for the spark",
          "score": 1,
          "created_utc": "2026-01-14 22:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmyltx",
              "author": "StardockEngineer",
              "text": "You got cut off there. Lol did your max tokens run out?  Dude I‚Äôm arguing with a bot.  My bad.",
              "score": 3,
              "created_utc": "2026-01-14 23:55:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjeo32",
          "author": "mxforest",
          "text": "M4 Max MBP runs way too hot and loud. I also bought it for coding but gave up. Now i have Claude Code 5x plan which is much better. Still use it for playing around with interesting models but M4 silicon prompt processing is way too slow for anything serious. Would not recommend. Wait for M5 max or setup a separate machine with something like 4x used 3090 if you really want local. Use it with a Macbook Air.",
          "score": -1,
          "created_utc": "2026-01-14 13:53:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzli0qs",
              "author": "HealthyCommunicat",
              "text": "M4 max macbook pro is indeed way too loud for fans to use for llm‚Äôs. The mac studio however is silent asf\n\nits still much quieter and much much much more capable than any gpu laptop though\n\ni got the m4 max 128 laptop knowing that i'd be able to turn it into a small cluster with my other macs as long as u have tb5",
              "score": 2,
              "created_utc": "2026-01-14 19:44:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qa0ore",
      "title": "Is GLM 4.7 really the #1 open source coding model?",
      "subreddit": "LocalLLM",
      "url": "/r/Anannas/comments/1qa0nme/is_glm_47_really_the_1_open_source_coding_model/",
      "author": "HuckleberryEntire699",
      "created_utc": "2026-01-11 14:28:45",
      "score": 12,
      "num_comments": 6,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qa0ore/is_glm_47_really_the_1_open_source_coding_model/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nyzc1rx",
          "author": "kweglinski",
          "text": "haven't done comparison but it is actually solid. I'm using claude at work and glm4.7 at home and while claude is (sadly) better, the glm is really capable. I'd say glm requires a bit more programming skill from user than the claude but in good hands both work well.",
          "score": 4,
          "created_utc": "2026-01-11 15:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz60o8",
          "author": "kinkvoid",
          "text": "It's hard to say. I use for RP and it's good. Kimi k2 is on par with GLM 4.7",
          "score": 3,
          "created_utc": "2026-01-11 14:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzpwo3",
              "author": "eli_pizza",
              "text": "K2 is good but it uses a lot of tokens for reasoning. And maybe it could be fixed with better prompting but I think it sucks for code review.",
              "score": 2,
              "created_utc": "2026-01-11 16:13:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzpm9a",
          "author": "eli_pizza",
          "text": "Try it and see? It‚Äôs incredibly inexpensive and there‚Äôs no one single ‚Äúbest‚Äù. It‚Äôs definitely better at coding than 4.6 which was already decent. \n\nMiniMax is also quite good and DeepSeek 3.2 seems great at code review.",
          "score": 2,
          "created_utc": "2026-01-11 16:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1jxe1",
          "author": "Consistent_Wash_276",
          "text": "I‚Äôve only compared it to minimax 2.1 both locally and the free servers running them in opencode. \n\nIt‚Äôs good. Minimax 2.1 is pretty decent as well. \n\nRemind you I‚Äôm on Apple Silicon and not CUDA cores so do with that what you will",
          "score": 2,
          "created_utc": "2026-01-11 21:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzj2ql",
          "author": "tamerlanOne",
          "text": "Opus 4.5 is almost no brainer even for many users with basic programming knowledge, for glm 4.7 you need to have a good basic programming knowledge to use it at its best",
          "score": 1,
          "created_utc": "2026-01-11 15:41:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfctk3",
      "title": "Quad 5060 ti 16gb Oculink rig",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/ohsze83iswdg1.jpeg",
      "author": "beefgroin",
      "created_utc": "2026-01-17 13:09:42",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qfctk3/quad_5060_ti_16gb_oculink_rig/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o03lkwc",
          "author": "JEs4",
          "text": "Neat setup!  But isn‚Äôt PCIe 5.0 x16 to 4x4x4x4 bifurcation capped at ~8 gb/s? \n\nWhat do you plan to use it for?",
          "score": 2,
          "created_utc": "2026-01-17 13:13:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03lyot",
              "author": "beefgroin",
              "text": "Yes and moreover it‚Äôs no longer pci 5 but 4 but it‚Äôs plenty for inference which is my primary use case. I even did the test on running full pci 5 x16 vs Oculink on Gemma 3 12b and there was no different in tps",
              "score": 2,
              "created_utc": "2026-01-17 13:15:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03o8nb",
                  "author": "JEs4",
                  "text": "Makes sense! What tps are you seeing?",
                  "score": 1,
                  "created_utc": "2026-01-17 13:30:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o04gsqd",
          "author": "iMrParker",
          "text": "Cool setup! I was wondering how you handled the 24pins for the occulink boards but I watched the video and saw they were sata powered which is smart. I was thinking of doing something similar not too long ago but settled for a single 3090 occulink with my main 5080 system. I want to expand but I feel like it would require a whole rebuild¬†",
          "score": 2,
          "created_utc": "2026-01-17 16:00:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03x5g4",
          "author": "GutenRa",
          "text": "Gemma3-12b fits entirely on a single graphics card. How do models handle when distributed across multiple graphics cards? Qwen3-next 80b, for example? What software do you use to distribute the load across multiple cards?",
          "score": 1,
          "created_utc": "2026-01-17 14:21:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03yox5",
              "author": "beefgroin",
              "text": "I‚Äôll post test metrics later today, regarding the gemma3 12b I only mention in relation to comparison between full pcie5x16 vs Oculink inference speed. I‚Äôm using ollama to run models. It splits the load between GPUs nicely. Yet to try vllm",
              "score": 2,
              "created_utc": "2026-01-17 14:29:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04a8ry",
                  "author": "dragonbornamdguy",
                  "text": "VLLM is a beast, very hard to setup but when it starts to work it beats metal really hard.",
                  "score": 2,
                  "created_utc": "2026-01-17 15:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q9f1ls",
      "title": "LM Studio consistently gives longer ai chat responses, why?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q9f1ls/lm_studio_consistently_gives_longer_ai_chat/",
      "author": "maguskain",
      "created_utc": "2026-01-10 20:42:38",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "Admittedly, I am new to all this.   But getting enough up to speed that I've installed LM Studio, koboldcpp, Oobabooga...along with a bunch of different models.  Also have Sillytavern.\n\n  \nHere's what I can't seem to figure out.  It seems like the consensus is that LM Studio is pretty basic.  And I would agree from a functionality perspective.  But it's the only UI that I've been able to use that consistently gives longer replies in when doing AI chat.  \n\nExample - Dark Desires 12B.  In LM Studio, I get usually around 3 paragraphs.  Kobold, Oobabooga - it's like 1 sentence.  It gets a little better if I have Sillytavern connected to Kobold, but not much.\n\n  \nAny ideas why?  I want to migrate beyond LM, but the response length being longer is what is keeping me there.\n\n  \nContext length = 4096.  Temp = 0.8.  Top K = 40.  Repeat = 1.1.  Min P = 0.05.  Top P = 0.95.  It's the same for LM and kobold, yet completely different results.\n\n  \nAnyone have any ideas?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q9f1ls/lm_studio_consistently_gives_longer_ai_chat/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyunjzq",
          "author": "paroxysm204",
          "text": "What are your system prompts?",
          "score": 3,
          "created_utc": "2026-01-10 20:53:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyup2gi",
              "author": "maguskain",
              "text": "It's any prompt.  But I'll give an example using Emily from the presets on Kobold.\n\n\\---  \n\\[Character: Emily; species: Human; age: 24; gender: female; physical appearance: cute, attractive; personality: cheerful, upbeat, friendly; likes: chatting; description: Emily has been your childhood friend for many years. She is outgoing, adventurous, and enjoys many interesting hobbies. She has had a secret crush on you for a long time.\\]\n\n\\[The following is a chat message log between Emily and you.\\]\n\nEmily: Heyo! You there? I think my internet is kinda slow today.\n\n{{user}}: Hello Emily. Good to hear from you :)\n\n\\---\n\n  \nOn Kobold, after I say \"hi\", I get this:\n\nEmily  \nDid something happen? You seem a bit... off.\n\n  \nLM Studio:\n\n*I type out a quick reply, trying to sound casual even though my heart is already racing just seeing your name pop up on the screen*  \nEmily: Hey buddy! Yeah, it's being super annoying. I can barely load anything.  \n*I shift closer to the computer, my fingers hovering over the keys as I wait for you to respond, a small smile playing on my lips*",
              "score": 4,
              "created_utc": "2026-01-10 21:01:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyy9h5g",
                  "author": "MadeByTango",
                  "text": "> [Character: Emily; species: Human; age: 24; gender: female; physical appearance: cute, attractive; personality: cheerful, upbeat, friendly; likes: chatting; description: Emily has been your childhood friend for many years. She is outgoing, adventurous, and enjoys many interesting hobbies. She has had a secret crush on you for a long time.]\n\nSome of y‚Äôall need to *hear* this: prompting chat bots this way is NOT healthy",
                  "score": 3,
                  "created_utc": "2026-01-11 10:33:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nywepp3",
                  "author": "meganoob1337",
                  "text": "Default max token value for response, there is a parameter that limits the max tokens that is allowed in the response, I think that might get set to some value that is different, if it's omitted it's endless I think?",
                  "score": 2,
                  "created_utc": "2026-01-11 02:21:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyuwu5z",
          "author": "AInotherOne",
          "text": "IMO LMStudio is BY FAR the best option for running models locally - for my needs. Its functionality and UX makes the others look archaic and it works with Blackwell GPUs out of the box. I love that I can switch models on the fly when using it with SillyTavern, experimenting with different models to see how they handle the same RP scenarios.\n\nEven though Studio was the best option for running various models on my 5090, I ended up going back to OpenRouter for SillyTavern. The flagship models are just impossible to beat with local hardware if you're into complex RPG-style scenarios and long narrative arcs.\n\nGood luck!",
          "score": 3,
          "created_utc": "2026-01-10 21:40:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyv0mzi",
              "author": "maguskain",
              "text": "Interesting, thanks.  So maybe I shouldn't feel as limited as I thought about using Studio",
              "score": 1,
              "created_utc": "2026-01-10 21:58:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyvzu1j",
          "author": "Jfusion85",
          "text": "Not familiar with LM Studio, but had similar issues experience with llama.cpp, it came down to the model. Apparently instruct models do this.  If you want to use the model there‚Äôs usually a stop signal, or just change to a non instruct model",
          "score": 1,
          "created_utc": "2026-01-11 01:00:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyyz9wp",
              "author": "maguskain",
              "text": "Yeah, I'm learning a bit more now and I think you might be right.",
              "score": 1,
              "created_utc": "2026-01-11 13:55:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nywe3sm",
          "author": "kidousenshigundam",
          "text": "Change your system prompt.",
          "score": 1,
          "created_utc": "2026-01-11 02:18:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qepgii",
      "title": "Fine Tuning LLMs Fully Local!",
      "subreddit": "LocalLLM",
      "url": "https://seanneilan.com/posts/fine-tuning-local-llm/",
      "author": "my_cat_is_too_fat",
      "created_utc": "2026-01-16 19:21:06",
      "score": 10,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qepgii/fine_tuning_llms_fully_local/",
      "domain": "seanneilan.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzz72yn",
          "author": "Total-Context64",
          "text": "Nice work!  You're on an M1 - I support MLX LLM training in SAM if you want to give it a try. :)",
          "score": 2,
          "created_utc": "2026-01-16 19:31:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzd4bl",
              "author": "my_cat_is_too_fat",
              "text": "Thank you so much! I'll keep posting my findings here. I'm having so much fun learning about local llms & hopefully making them do useful work.\n\nYes, tell me more.",
              "score": 2,
              "created_utc": "2026-01-16 19:59:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzze79y",
                  "author": "Total-Context64",
                  "text": "Take a look here: [https://github.com/SyntheticAutonomicMind](https://github.com/SyntheticAutonomicMind) the last version implemented mlx training support, I'm still working on support for training llama models.",
                  "score": 2,
                  "created_utc": "2026-01-16 20:04:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o03iq9b",
                  "author": "Total-Context64",
                  "text": "llama training is in today's release if you'd like to tinker with it.  Would love feedback.",
                  "score": 1,
                  "created_utc": "2026-01-17 12:54:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qaxwf5",
      "title": "It seems like people don‚Äôt understand what they are doing?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/7h948anosxcg1.jpeg",
      "author": "platinumai",
      "created_utc": "2026-01-12 15:30:06",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 0.73,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qaxwf5/it_seems_like_people_dont_understand_what_they/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nz6uldl",
          "author": "MrTewills",
          "text": "Yep, I don't.",
          "score": 1,
          "created_utc": "2026-01-12 16:52:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza2844",
          "author": "DexSprinkle",
          "text": "With skyrocketing prices and no relief until possibly after 2028, I feel like I need to buy the the best hardware I can afford now, while it's still available, and learn how to set it up and use it once I get it.",
          "score": 1,
          "created_utc": "2026-01-13 02:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb47hg",
          "author": "sinebubble",
          "text": "The struggle is real. I need to run too many tools on my MacBook and have no extra memory for a local model. I have a 16G M1 MacMini I use with LM Studio for remote inference but it is sloooow. I hate using my openrouter api for coding because I have to strip out every piece of identifiable code just to have it bomb out 25% of the time. There are days where it seems like nothing is working well and the work arounds just aren‚Äôt worth the hassle :(",
          "score": 1,
          "created_utc": "2026-01-13 06:10:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbwc35",
      "title": "I'm building a real-life BMO with a Raspberry Pi 5 (Mistral/OpenAI + YOLO11n)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qbwc35/im_building_a_reallife_bmo_with_a_raspberry_pi_5/",
      "author": "Strange-Dimension675",
      "created_utc": "2026-01-13 16:44:09",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 0.85,
      "text": "GitHub Repo: [https://github.com/ivegotanheadache/BMO](https://github.com/ivegotanheadache/BMO)\n\nHi! A few months ago I posted about building a Voice Assistant on Raspberry Pi 5. Because of university, I couldn't update the project for a while, but now it‚Äôs almost finished! It‚Äôs now a full AI companion with object recognition (YOLO11n). I‚Äôm also working on face and voice recognition, so he can play games with you, and I plan to add robotic arms in the future.\n\nI hope you like it! All the faces were drawn by me. I‚Äôll be adding more emotions and the canon green color soon. Right now it‚Äôs pink because my case is pink‚Ä¶ lol\n\nhttps://reddit.com/link/1qbwc35/video/w7tc1ylxa5dg1/player\n\n",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qbwc35/im_building_a_reallife_bmo_with_a_raspberry_pi_5/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nzegutg",
          "author": "yeeah_suree",
          "text": "This is super cool! What games will it play and how are you using the objects recognition? I am also building a similar LLM assistant with voice recognition and text to speech, planning to have it play chess with me, as well as run RetroArch and Pico 8. I‚Äôm undecided if I will add its own monitor like yours or just use a Tv when needed.",
          "score": 1,
          "created_utc": "2026-01-13 19:07:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}